Hi everyone. What's in a brain? Could we really create a machine that can think and learn like a human does?
It's an idea that has captivated science fiction writers for decades, but recent breakthroughs in AI and neuroscience are turning that dream into a reality.
In fact, it's now clear that building a brain is simpler than we ever imagined, and that artificial general intelligence is right around the corner. Keep watching to learn why.
I'll cover three topics. First, chat GPT and the definition of intelligence. Second, artificial general intelligence, or AGI. And lastly, timelines for AGI.
So first, chat GPT and the definition of intelligence. I'm going to use the definition of intelligence that was drawn up by a panel of psychologists and that was also used in the paper Sparks of AGI.
Basically says that intelligence requires several attributes, including the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.
As I discussed in my previous video, which you can watch over here, chat GPT as in GPT4 satisfies almost all of these attributes of intelligence. With two exceptions, it cannot plan.
It's learning is limited to the scope of one chat session. In other words, every time you start chat GPT and you get a new session, you are starting from a blank slate in terms of its memory.
The study that I mentioned Sparks of AGI is a 155 page academic paper from Microsoft Research, and it shows that GPT4 is very capable indeed. Here's a quote from the paper.
GPT4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more without needing any special prompt.
GPT4's performance is strikingly close to human level performance.
If you want to learn more about this thorough analysis of GPT4's capabilities, I encourage you to go read that paper or check out the talk by Sebastian Bubeck, one of the authors, which I'll link in the description down below.
So GPT4 has been out for a while now, and there's a new system called auto GPT, which is based on top of it. Auto GPT is an open source project.
It's only about a month old, and the core functionality of auto GPT was created in only three days, according to the repository history.
So what is auto GPT? It's unlike chat GPT, which is a chatbot designed to answer questions. Auto GPT is an agent in the AI sense, which means it's designed to go out into the world and perform actions on your behalf.
It's implemented by making API calls to chat GPT, to GPT4, because open AI actually created an API that allows chat GPT to be used by anyone programmatically.
So what auto GPT does is it prompts for goals, it prompts you, the human, for some goals for it to fulfill, and then it actually tries to reason about those goals and execute on them entirely on its own.
It's able to do Google searches, it's able to install software on your machine, it's able to refine those goals into sub-goals.
If you give it a very broad goal, it'll be able to break it down into smaller pieces.
And the way that auto GPT works is it actually has one top-level chat GPT instance that acts as a controlling brain, and that instance can spawn other chat GPTs to solve sub-problems.
And remember I said that chat GPT can't plan? Well, auto GPT does, and the way it does it is it just gets chat GPT to print out all of its reasoning.
In other words, you could basically read its thoughts in plain English as to what it thinks its next sub-goals or next steps should be.
It's a really clever way to basically endow chat GPT with the ability to plan, and by having this separation between the higher-level chat GPT and the smaller ones,
they ensure that all this reasoning and planning being done by the top-level chat GPT stays within the token limit of chat GPT,
because chat GPT will eventually forget what you've told it if you give it enough input.
So they're able to keep that amount of input small and pass off the more in-depth tasks to other chat GPTs.
Sounds pretty crazy. Auto GPT can actually be given some code with some errors in it, and it can try to propose solutions to that,
and it can test and run that code and actually write test cases for that code too.
And once it all works and the tests pass, it can hand the code back to you.
Basically, a tiny version of a programmer in a box, amongst other things.
The really interesting part here is that auto GPT, in my view, actually does satisfy all those requirements of intelligence,
because it is able to plan, and although it's still limited to one session, so if you run multiple auto GPTs, they don't share memory.
But by having this hierarchy of chat GPTs, any memory and planning and so on that the higher-level one does can basically last almost indefinitely.
So this is only a few months after the release of GPT4, and already someone has basically been able to leverage the power that's inherent in that model and extend it to auto GPT.
There's another application that's built on top of chat GPT that I want to draw attention to, which is called Hugging GPT,
which is a funny name, but it's called that because Hugging Face is the name of the largest open-source repository for pre-trained models that solve different problems.
So chat GPT is a general reasoning engine, right? It can basically understand language and do a very broad set of tasks,
but the models on Hugging Face are much more specific, like they might be vision models or speech recognition models, natural language processing models,
and each of the models is provided in its entirety along with an English description of what it does, and it's used by human programmers to go and solve different problems.
But what Hugging GPT does is, like auto GPT, Hugging GPT uses a chat GPT instance as the controlling brain,
and you can give Hugging GPT very complicated problems to solve, and what the controlling brain will do is it will go search on Hugging Face for small models,
for other pre-trained models that will solve aspects of the problem it's trying to accomplish.
In other words, it will outsource the complicated subparts of a problem to a model that is pre-trained, that is very specifically trained to solve that problem.
But the really amazing thing to me is that it makes these decisions purely based on the English description of the model on Hugging Face.
In other words, Hugging GPT is leveraging the fact that AI can interact with our world, once it understands language,
because our world was designed by humans, for humans, to interact with through language, especially the internet, right?
Everything is accessible if you can understand human language.
And it's an interesting thought that if you were to embody the AI in a physical robot about the size of a human,
then that would allow the AI to fully interact with the human physical world as well,
just kind of like a hand sliding into a glove, right? Just matching the world at the interface with which we have designed it to be used.
Okay, so let's move on to the next section and talk a bit about artificial general intelligence, or AGI.
So the definition of AGI is an AI that can perform any task that a human is capable of.
This might just mean any task a human is mentally capable of, but it may also mean a task any human is physically capable of.
After AGI, the next level up in terms of intelligence would be superintelligence, or ASI,
which would be an AI that is much more intelligent than a human.
But let's focus on AGI for a moment.
So estimates for how long it would take humans to develop AGI.
Up until recently, some people would have said a century or never,
but now we're starting to see some much more intelligent models,
and you might ask, could a large language model, which is the architecture that GPT4 and chat GPT use,
could a large language model become an AGI?
And this is basically, it's not known, obviously, but apparently OpenAI's red team,
which is a security team that tries to imagine worst case scenarios,
apparently OpenAI's red team didn't think GPT4 could even do any planning.
And they were basically wrong about that, right?
Because auto GPT was able to build on top of GPT4 and do planning quite well.
So I think that we're only just starting to unlock how much power is really inherent in large language models,
even if they're kind of dumb from an architecture perspective.
And the trick was simple.
It was just a large language model can only reason so much and have only a certain depth of thinking,
so just have it pre-doubt its own thoughts, and then it can build on top of its own thoughts,
as if those were input and so on.
So sometimes there's just very simple tricks that can unlock some of these advancements.
I actually wanted to draw a comparison here to a science fiction show called Person of Interest.
In the show there's this machine called The Machine, which basically hunts down crime,
but it has its memory reset at the beginning of every day to prevent it from becoming conscious.
Its creators were really worried about its power.
And then eventually the machine ends up figuring out that this is happening to it,
and it ends up writing down information so that it could be stored externally,
and then next day when it wakes up or whatever, when it gets reset,
it's able to access those memories that it has stored externally,
and so build a contiguous memory, essentially.
Very similar to what auto-GPT does with GPT-4.
So is an AGI exactly like a human?
Not exactly, of course.
Humans have consciousness, which Max Tegmark, the author of Life 3.0 and an MIT professor,
defines as having subjective experience.
That's how he defines consciousness.
And I think it's a reasonable definition, and humans have consciousness,
but an AGI doesn't necessarily need to have consciousness.
There's another term called artificial consciousness,
and general consensus is that it would probably arise after artificial general intelligence,
but no one knows for sure, of course.
Max Tegmark makes an argument for this because he says that the neural architecture of GPT-4
and any large language model that we've built to date is basically that of a feed-forward neural network,
which means that it's a network, kind of like our brain, where information can only flow one way.
It can only flow forwards.
Our brains are more like recurrent neural networks, which is a type of neural network that we can build.
I mean, our brains are more complicated than that, too, because neurons are more complicated,
but a recurrent neural network basically has loops in it,
which means information can flow around and around and continuously get refined.
Anyway, there's a theory that this recurrent, around and around property
is what actually leads to consciousness,
what leads to a network to being aware of its own existence
and able to analyze and do metacognition, essentially.
So it's an interesting thought, and it might give you an idea for how to go if you wanted to build artificial consciousness.
Actually, the paper Sparks of AGI also has a section on limitations of GPT-4
that are coming about because of this very simple neural architecture.
As I said, we actually know how to build recurrent neural networks,
but they're just more difficult to train and deal with.
So as we're starting to build some initial AIs,
we're just using large language models, just like a very simple architecture
just to see how far we can get with that.
So what could AGI do?
Well, of course, it can solve abstract problems in almost no time, right?
Thinking much faster than a human can.
Replace humans in most jobs, especially mental jobs.
And like I said earlier, if you embody the AGI, like you put it inside a robot body that's like a human,
then it could, for example, literally drive a car around.
It's kind of a funny thought that maybe the way we get self-driving cars is not by making a really smart car,
but by making a really smart robot that can just sit in the car and interact with it
using the interface that has been designed for humans.
Anyway, AGI would be able to do that.
It's a pretty big step towards what Max Tegmark calls in his book,
calls Life 3.0, which is life that's able to upgrade its own hardware, probably copy itself,
maybe even transfer its intelligence or its consciousness across a network to somewhere else,
which is effectively teleportation, if you think about it.
So there's a lot that AGI would be doing around us that is not just the same as being human, right?
It would have other abilities.
So let's talk, finally, third section, let's talk about the timelines for AGI.
Why didn't this happen before now?
Like why are we only now starting to really develop AI at the rate at which we are?
We've had essentially infinite computation available in the cloud for quite a while now,
provided you had essentially infinite budget.
And the truth is that we were just learning how to construct models.
We were learning how to use this computational power to build a brain.
We were learning how to collect and train this data.
We were learning the best structures that would be used to build a brain
and the best way to feed that data into it so that it could learn.
Even the simplest structures are sufficient, it seems, to build something pretty intelligent,
provided you have enough data and have the appropriate deep learning algorithms.
I think it's one of those cases where a time traveler could go back in time
and invent this tech much sooner than we did ourselves, but we're doing it the hard way.
That said, AGI is almost here.
It's really close.
There's a graph here that shows the number of parameters in a model versus the year
and you can see it looks like it's growing exponentially.
Until you look at the scale and you realize it's already on a log scale.
So it's actually growing doubly exponentially,
which is really incredible.
And that's why I would say that AGI is almost here.
It's incredibly close because we're on this doubly exponential rate of change
and we're constantly using the tools of today to build the tools of tomorrow at an accelerating rate.
Even chat GPT can accelerate the power of a developer probably by like 5x or something
once they know how to use the tool.
And that is going to be a factor in the production of code for GPT-5.
Right now, these AI tools need a human in the loop.
They're not fully autonomous, but that is not to say they're not powerful.
They're extremely powerful.
And as they get more and more powerful, they'll need the human in the loop less and less
until eventually they're basically fully autonomous.
There are people like David Shapiro that are estimating AGI is only about 18 months away from now.
It's a matter of months, not even years at this point, perhaps.
And if it's true, it would be very exciting and also extremely dangerous at the same time.
We've all seen enough science fiction movies about AI that wants to kill humans to know that it could end badly.
Here's a quote again from Max Tegmark.
In short, it turned out to be a lot easier to build human or close to human intelligence than we thought.
Again, commenting on that even using these like maybe suboptimal architectures
and we've already had quite a lot of success.
And again, that means it's even more dangerous, actually,
that the fact that intelligence is relatively easy to achieve is dangerous
because it means we'll probably accelerate through those more advanced forms of it, including AGI, even more rapidly.
I want to mention as well that superintelligence would not be far behind after we obtained AGI.
Because, again, we would be using AGI to then say,
hey, how would you build a superintelligent AI?
And it would help us do that, most likely.
I have another science fiction comparison bear with me.
There's a book called Marooned in Real Time by Werner Vinge.
It's set in a post-singularity world where civilization has actually disappeared.
All the humans on the planet just disappeared during the singularity.
And there are some time travelers that have actually passed through the singularity
and now they're looking around wondering what happened.
And those time travelers left at different points in time leading up to the singularity.
And the closer they were to the singularity, the more powerful they are in the book,
where the ones that left just a few months before the singularity was presumed to have happened
have resources comparable to like full countries or the militaries of full countries.
And that's where you're going when you're approaching a technological singularity.
There's a lot to talk about when we think about the implications of AGI,
why we have reason to be cautious about its development.
I'll make another video going into the details of why we should be potentially scared of this rapid progress
and what we should be doing to control the pace.
In conclusion, chat GPT and building on top of it, auto-GPT seem, in my opinion,
to fully satisfy the definition of intelligence that has been set up by psychologists.
And I was shocked when I heard about auto-GPT and I learned about its capabilities inside in action
because it's just such a big jump even from where chat GPT was in such a short period of time
and with not all that much ever.
And I think that will just pretend what we will see in the future.
Very rapid steps of improvement that maybe are not all that difficult
or are certainly happening very rapidly.
So where does that lead? It leads to AGI. AGI seems to be extremely close.
I think Ray Kurzweil might be underestimating how far away the singularity is for us.
So again, my next video will talk more about the implications of AGI
and that's all I have for today. Hope you enjoyed. Thank you very much for watching. Bye.

Thanks for the intro, Nathan, and thanks for inviting me to the host, Xanadu.
I'm really happy to be here.
As you know, I'm very passionate about QML,
so very excited about giving a talk today at this event.
As Nathan mentioned, I'm a research scientist,
and I lead all things related to QML at Sandbox.
Today, we'll be talking about research,
some of the research we do in our team,
and some of the tooling, actually,
for quantum probabilistic generative modeling,
and beyond, which is ominous, but we'll get to that.
There's a lot of talks about very large Hilbert spaces
and the power of quantum machine learning,
embedding in large spaces, and so on.
But something we've got to remember is that quantum theory
is actually an extension of probability theory.
Probability distributions themselves are objects that live
and also an exponentially large space.
I guess the research I'll be featuring today
is at the intersection between probability theory and quantum theory,
or rather probabilistic machine learning and quantum machine learning.
We'll get to that in a second.
What do we mean by probabilistic information?
You can have bits, right?
They're deterministic, zero or one.
You could also have probabilistic bits,
which are mixtures of zero and one, right,
each sum, they're positive real numbers,
and they sum up to one.
And in quantum, for pure states,
we have wave functions, and the complex value sum up to one, right?
And the interesting thing happens
when you have several probabilistic bits or several qubits.
So for several probabilistic bits,
actually the distribution in general lives in a vector space.
That's two to the n-dimensional over positive reals.
And for wave function, it's over the complex values.
So it's actually not that it takes exponentially many parameters
to describe a general distribution that is quantum
that gives it its power over, say,
classical probabilistic machine learning, right?
So we got to actually dig deeper and get to the bottom of
what are quantum computers good for and how do we leverage that, right?
We've all seen and heard of the power of, you know,
deep or semi-deep or shallow-ish quantum circuits
that are very dense packed with a lot of gates.
They could be random rotations and so on.
The quantum complexity, right,
or the complexity rather of classical simulation of these circuits,
you know, scales very rapidly, right?
And why is that?
You know, fundamentally, it's because sampling from it
is kind of like integrating a path integral
that is, you know, at least layer, it's complex valued summations.
And you have all sorts of interferences
across all these possibilities, right?
You could think of it as kind of an exponentially large,
a multi-slit experiment.
And that becomes really hard, right?
So fundamentally, at least what was demonstrated
in terms of a separation between quantum and classical computing
was, right, quantum supremacy,
which is sampling from deep unitaries.
It's very difficult to do so with classical computers.
So the idea is, okay, how do we leverage, incorporate,
you know, shallow-ish unitaries
into classical probabilistic machine learning
to kind of extend its reach and make it more powerful
rather than try to replace it,
which actually some of my earlier research
before I joined Alphabet was focused on.
So as I mentioned,
classical quantum computers becoming powerful enough
to be unsimulatable,
and we want to leverage this power synergistically
with deep learning.
So the solution, at least in our approach,
is to both innovate in creating hybrid models
in ways to train them
and software for these hybrid models, right?
So you have hybrid quantum classical models
and hybrid quantum classical software.
You've all heard of hybrid models during this event,
which is, you know, exceptional.
Usually in most conferences,
there's not as many QML-focused speakers.
So it's wonderful.
I don't have to introduce all these concepts from scratch.
But the idea of hybrid models
is really to break up the task
of learning a representation of the data
or isolating a hidden parameter of the data,
you know, break it up between classical components
of a representation and quantum components.
This is in contrast to just variational algorithms,
which are also called quantum classical algorithms,
where the optimizer is classical,
but the model itself is all quantum, right?
So I'm going to start with the models
and we'll get to the software in a second.
So you've all seen this comparison by now.
I think a lot of people made this comparison now.
You know, you have classical FIFOR neural networks,
they have parameters, you can have some inputs,
and then you have a parameterized function.
And you like to tune, you know,
f, f of the parameterized function f of x,
depending on phi, you want to tune phi such as
to fit some loss function, right?
To optimize, some loss functional to optimize,
you know, minimize the distance between your data
and the output of the neural net.
And for quantum neural networks,
usually it's pure states, right?
So you have a parameterized, you have a fixed input,
let's say, and then you have a parameterized output
that's conditioned on the parameters of the Q and N,
and then you train it, again,
according to some loss functional.
A functional is something that takes your big vector
or your function and then converts it to a scalar,
a single scalar.
So how do we hybridize quantum classical deep learning?
You know, there's kind of two main pillars.
I mean, this is kind of old school,
but now it's becoming more and more blurry,
the lines between these two.
But, you know, traditionally there's generative models
and there's discriminative models, right?
Generative models, they have a data set,
and you have some samples from this data set
from the true probability distribution.
And you want to very easily tune a probability distribution
or a model that you could sample that,
such that the probability distribution of the model
approximates the underlying true distribution, right?
And you do that by kind of minimizing some form
of statistical distance over the points you know
and you've sampled, right?
And discriminative models,
it's very much like generative models,
it's just that you actually have pairs of inputs and outputs.
So it's more about learning conditional distributions, right?
And, you know, usually how you do that
is you have a parameterized function,
and then you say your distribution is either a delta function
or a Gaussian, whose mean is like the output
of the parameterized function.
And you could train it with KL divergence
or mean square loss and so on.
Okay, so how do you have then generative
and discriminative hybrid models?
So I guess what we'll focus on today
is generative hybrid models, right?
In our case, it's, you have a probability distribution,
it's parameterized say our favorite is energy based models,
which are Boltzmann distributions
where the energy function is parameterized
by a neural network,
and that parameterizes a distribution.
That model is sampled, you could sample bit strings,
and then depending on the bit strings,
you flip the input, right?
And we'll have more details of the math behind this.
But at that point, you've inserted a classical probability
distribution into your quantum computer on average, right?
After that, you append a Q and N
and you get a resulting output,
that's actually a mixed state.
That's what we'll talk about today.
You might have seen talks about hybrid discriminative
quantum classical neural networks.
Usually it means that you add
classical post-processing to a quantum neural network, right?
So that you can further extract hidden information
hidden in the data.
So let's focus on the generative models.
So, you know, most data in nature is actually sitting
at the intersection of probability theory
and quantum theory, right?
Most states in nature are not pure states,
they're actually mixed states,
because you're either looking at a subsystem,
or effectively an open quantum system, right?
You know, the point of it when playing quantum computers
is to create artificially closed quantum systems
so we have full unitary control of them.
But the idea is that a mixed state
is a probably distribution over pure states, right?
There's a few ways to represent it.
You could phrase it as a mixture of different mixed state
or as a mixture of orthogonal
and perfectly distinguishable pure states, right?
And that's the eigen decomposition of the mixed state,
which is the diagonal form.
So how do we represent these mixed states?
Well, just from describing them,
I've kind of given you a spoiler
that really what you'll want to do
is compose a model that can learn distributions
with a model that can learn changes of bases, right?
Unitaries that create wave functions.
So I guess what our team has been really bullish on
has been quantum probabilistic hybrid deep learning.
And as mentioned in the intro,
usually you have a parameterized pure state
and you train it, you know, safer VQE
to minimize some expectation value.
Instead, what we have is, again,
a parameterization of a classical distribution.
And then we plug that in, you know,
depending on those samples, we do some bit flips.
At the output here, the effective state
is like a diagonal mixed state.
And then after that, you append a unitary.
And the fact that it's unitary comes in handy
for all sorts of calculations and trainability.
And, you know, effectively you have a classical latent
representation and then you plug it through unitary
and then you get a parameterized mixed state
instead of parameterized pure state.
And this form is trainable for all sorts of tasks.
So you could do all sorts of great stuff.
And the two first applications we looked at,
which we thought were pretty fundamental,
were kind of two dual tasks.
We're gonna spend some time on this slide here.
The first is variational quantum thermalization.
So let's say I wanna model, you know,
what are interesting mixed states in nature?
Thermal states, right?
Everything that reaches equilibrium,
you just leave it somewhere.
It converges to something called a thermal state,
which is a Boltzmann distribution.
And you could create a variational principle
for both the classical model and the quantum model, right?
Together, such that the hybrid model, right,
has a variational principle for both types of parameters,
such that minimizing a certain functional,
in this case, quantum free energy, you know, you converge,
if you find the absolute minimum
over all parameterizations of quantum free energy,
you've found the thermal state, right?
And, you know, the trick is like, okay, it's great.
You could phrase a theoretical loss function,
but can you sample it?
Can you evaluate it?
Can you get its gradients?
And the answer is yes.
And actually, if you use what we mentioned before,
energy-based models, you get some very nice
parameter shift rules, not only for the quantum component,
which is unitary, which we know you could use
pendulum or TensorFlow quantum,
but also for the classical component.
There's kind of a parameter shift there
because it's an exponential form.
So if you're interested in that, you know,
I won't get into the details today,
but check out the TensorFlow quantum white paper
or some of the papers I'll be linking to in a few slides.
Right, so we could create thermal states
with these generative models.
This is kind of the generative pipeline, right?
To evaluate the loss that is a free energy,
which is the difference between energy and entropy.
What's cool is that the entropy doesn't depend on
the quantum computer.
So kind of simplifies the pipeline there.
Quantum modular Hamiltonian learning
is like the dual task.
You're trying to have a model that looks like this,
a pipeline that looks like this,
that replicates the distribution of a given data state.
Suppose you're given copies of Sigma D,
some unknown mixed state, right?
It's kind of like being given samples
of an unknown distribution for generative modeling
as we saw in the intro.
Let's say I want to learn to generatively model
the distribution, right?
Replicate it, this mixed state.
Then what I can do is actually minimize quantum cross entropy.
And you could evaluate it directly
and its gradients using a pipeline like this,
use actually the reversibility of unitary evolutions.
So you take your data, you plug it in reverse,
and then you could evaluate,
this is kind of the expectation value
of the unnormalized log likelihood.
Once you've gotten your classical samples, right?
It becomes classical data, right?
Again, details in the papers,
but the point is that you parameterize
your hypothesis class over mixed states
as a thermal state over a learnable Hamiltonian, right?
And you train that hybrid system,
and then Hamiltonian again is also parameterized
as a unitary and a classical neural network.
And so you learn an effective,
it's called a modular Hamiltonian,
a log, a normalized log operator for your data.
And there again, parameter shift gradients.
And if you tune into the March meetings,
we're actually gonna show how you can get exact parameter
shifts for the natural gradients
or the quantum fish information metric
for these types of model.
So great, I don't wanna get too deep into the math.
I think there's a wide dynamic range
of expertise in the room.
But what can you do with this, right?
Can I simulate a superconductor?
Sure, right?
We're looking for a holy grail of using quantum computers
to figure out new superconductors.
You could train a VQT model
to discover new superconductors
that converges in a hundred iterations of gradient descent.
Can you learn a general spin glass or whatever,
density matrix given just access to the data
without knowing the Hamiltonian?
And the answer is yes, you can reconstruct afterwards.
Something we were interested in was
how much actual quantum circuit we need.
So depending on the temperature of quantum systems,
you could sweep the temperature
and the actual complexity,
the quantum complexity of the quantum state,
that's the thermal state,
will vary depending on the temperature
and all sorts of parameters, right?
And the idea was to sweep over how deep of a Q and N
do we need to get an accurate representation.
So those are some results that come in the V2
of the VQT paper.
But it gets to the point of maybe for high temperature systems,
actually, you don't need that much quantum depth, right?
Maybe you just need a little bit of quantum depth
added to classical NL and you get some advantage there.
So that's what we're trying to hone in on.
What's the earliest win, right?
Instead of trying to overload the quantum computer.
So this is a hackathon, so code first, right?
I'm leaving these QR codes here.
You can scan them.
There's a TensorFlow quantum implementation of VQT and QMHL.
There's also a penny lane, a very nice blog post.
Thanks to Jackson O'Roney and the Xanadu team.
Great, so software, last second bit.
So something I worked on very heavily was TensorFlow quantum
and it's to integrate state of the art ML framework
deeply integrated with quantum computing.
It's a deep hybridization of TensorFlow and CERC
as opposed to like penny lane,
which is more like an all-terrain vehicle.
TensorFlow quantum is kind of like a drag race.
It's made for like one track and you CERC with TF
and it goes fast, right?
And that was kind of a different design decision.
It has its pluses and minuses
and you're encouraged to try both and yeah.
So similar to frameworks that are out there now,
you can have expectation values
taken care of gradient calculations.
It paralyzes across many cores.
I would suggest using large CPU instances for TFQ usually.
It's great for any variational algorithm workflows,
but of course the focus is on hybrid quantum classical,
deep learning and quantum data actually.
So quantum data is quantum mechanical data
rather than embedding classical data into the quantum computer.
I guess our opinionated take is that
we should try to focus on growing the market
of quantum data problems and we're trying to encourage that.
Of course it does hybrid back prop
where you could back prop gradients through a quantum circuit
if you have kind of a sandwich in time
of a deep neural network or differentiable function
before and after,
you can get a back propagated effect of Hamiltonian,
takes gradients of that
and then basically you've back propagated
through the whole thing.
That's finite difference, parameter shift,
stochastic parameter shift rules.
You've heard about these this week,
I'm blasting through these,
but and recently there's a joint differentiation
which really speeds up the research.
That's when you can...
But of course that's illegal on the quantum computer,
but when you're just doing research
on classical computers, a joint differentiation,
I highly recommend it.
It's deeply integrated with TF.
Our workflow, we use TF probability and TensorBoard
and you can visualize all the weights
and the training and diagnose what's going on in your Q&N.
So if you're wondering about how to do that workflow,
we have some blog posts hopefully coming soon
and you can reach out.
So make sure to check out tensorflow.org slash quantum.
The two links are the white paper and the VQT QNHL paper.
Be sure to check them out.
Cool.
So we have some questions.
Keep them coming, please in the chat,
but some questions are also about your background.
So you have quite a nice history
where you were a researcher in grad school
and you find your way now to Sandbox
and you're now a future speaker on QHAC,
which is obviously a top billing here.
So what is your particular backgrounds
and do you have any recommendations
for people who are just starting out?
How do they get prepared?
I think there's not like one standard path
and what's been eye-opening for me
has been having friends that reach a similar point
to where I am in my career, sometimes even faster.
So I guess what I could say is,
for example, on the TensorFlow quantum team,
I was a theoretical physicist,
I think in 2016, 2017,
did a master's in theoretical physics,
was doing quantum field theory.
I cast a very wide net,
like I took every grad course I could find
and I had the word quantum in it.
I think I count like 15 or something ridiculous.
So to me, it's like having varied training data
and then you transfer learn better.
So that's been my background as kind of a theory focused person.
But for coders and hackers,
I've had folks that came from CS background out of undergrad,
from mechanical engineering out of undergrad.
And actually we just became friends,
started hanging out, started hacking some QML together.
We wrote a couple of papers together.
We ended up launching a product called TensorFlow quantum
from those experiments.
And now those people right out of undergrad
are employed full-time at various parts of Google
in quantum computing, in the quantum computing industry.
So really there's no standard path
that depends what you want.
If you wanna be a theorist that kind of generalizes
from the state of the art theory,
then the wider net you could cast
in a later specialization,
you could do the better, in my opinion.
If you just wanna get going,
there's actually nothing stopping you.
It's still an early field,
so don't let any gatekeepers or anybody say,
you don't have this that background,
stop you from trying to do some damage
to the literature in a good way.
So yeah, I've seen a high dynamic range
of in terms of background of people
that are a bit very successful in the quantum space.
100% back that up here.
And I think one of the things I like the best
about your work is when I see your papers,
I can actually, I can tell
that you're reading the deep learning papers properly.
You're not just someone who's taken a Wikipedia page
on neural networks and stopped there.
You know what the latest ideas are from deep learning
and you're able to cross-pollinate between quantum
and the other field.
So I think that's a great advice for anyone out there
is it's really tough actually to be a world expert
in one field.
And speaking mostly personally,
it's a lot easier to be good or really good
at multiple fields and see those connections.
And actually you can do much better
in the end by seeing those connections
rather than be too narrow in focus.
So that's really great advice, Gail.
I do wanna call you out though,
because you skipped over
some of the most interesting parts of the story.
So I'm gonna flesh it out a little bit more here
for the benefit of our audience.
So let's go back to day one.
We had Patrick Cole speaking yesterday
and he was mentioning his tipping point
for getting into QML was actually a chance encounter
with you in a lobby of a building or a lunch room
or something like that.
Right, 2015, I think I was still had to write my thesis
for my masters, which was in quantum field theory
and I was trying to solve quantum gravity
or something as theorists do.
And I was kind of getting,
I think theoretical physics is an interesting place,
but it's kind of hovering around the same concepts
for a little bit.
And there was this thing on the horizon
and I was sitting at IQC called quantum computing.
And back then it was sci-fi,
there weren't really that much startups and opportunities.
And Patrick and I just,
the IQC water cooler,
which is one of the water coolers
I've learned the most from in my life.
We just struck up a conversation,
talked about this new field, quantum machine learning.
What is that?
In 2015, there was Seth Lloyd, Patrick Rebentros,
Masseni and so on.
There was those early papers
just showing that there's maybe something interesting here.
And we needed to learn more about that.
So we had a year long journal club that we organized
and we just read a bunch of papers
and then we realized, wow,
there's still a lot of opportunities left in this field.
Actually, even though it's riskier,
I think I'm gonna go all in on this field, right?
I told my supervisor,
I'm not doing theoretical physics from my PhD,
I'm doing quantum machine learning.
He told me, I can't help you, but I'll support you, right?
And yeah, turns out that was a bet worth doing, right?
Stephen Weinberg, Nobel Laureate in Physics said,
go to the fields that are like a mess
and not figured out yet, right?
Because when they're clean
and you know exactly what increments to add to it,
it's almost, it's like too late.
And so, yeah, take risks and don't let the illusion
of there being a wall of established expertise,
you won't be able to pierce through this way to you
from like going into a field.
Yeah.
Awesome.
I definitely recommend to anyone out there
who's listening now,
it's great advice you're getting from Bill in here.
So how did it feel for you when you kind of flipped that switch
so you were looking for something, you found it
and you went from reading papers to at some point,
writing papers and contributing to the field?
How did that change happen?
Yeah, that's interesting.
I think actually I was just talking
in the Toronto Waterloo Tech Corridor,
I think having some beers at U Waterloo Pub
and there was some people from the Creative Destruction Lab,
I think, which is the origin
or one of the accelerators of Xanadu,
they were around and they were starting this QML accelerator.
And yeah, we just had a conversation
they were like, do you wanna come give some lectures
on quantum algorithms?
It's like, sure, I guess they could come give some lectures.
And of course I prepared the lectures the night before
during a crazy all nighter
and I just read a bunch of papers,
prepared a bunch of lectures
and then suddenly I felt like I knew quantum algorithms
enough to explain them.
And then suddenly I was there, I was 24 at the time
and there were a bunch of founders
trying to start quantum companies.
And there I met Tom and Will Zhang
and Will Zhang, who's now at Unitary Fun.
And Will Zhang was like, that was a cool lecture.
Why don't you jump into one of these companies?
Just try it out.
And so at the time, yeah,
we co-founded something with Tom and Michael
who's now the lead of TensorFlow Quantum.
And we were a player in the quantum space
and yeah, that's how it started for me
is like just seeing that there's no barriers.
So I started with industry
and then because we were a startup,
we wanted a show that we know our stuff.
And I think Mike just sat on this chair right here,
I think three years ago
and we just hacked one of the early Q&N papers
just for fun over a weekend.
It was put together very quickly.
And then we put it in an archive
and the rest is history.
And then it was just a runaway effect beyond that.
But yeah, it starts with just having fun
hacking away with your friends
even if you don't know what you're doing.
And then if you just keep compounding on that,
you get to the top levels, nothing stopping you.
Yeah, I remember those days.
Xanadu was one of the partner companies
at Creative Specialized
because we had previously gone through
the machine learning program
because there was no quantum program at that time.
So you guys went through with the first year,
it was a quantum, it's not called a quantum program
but it's a quantum machine learning dedicated program
back then.
I remember all you guys back then it was really cool
because he told me,
I'm actually here to just give a lecture
and then suddenly you were,
roped into founding a startup company.
What was the name of your company by the way?
It was Everettian,
like the Everettian interpretation of quantum mechanics.
And I guess at the time,
I was thinking of,
everybody was wondering,
how do you train quantum neural networks
using a quantum computer?
And I was really,
I had just given a lecture on quantum optimization, QOA.
And I was just already dreaming of like,
actually we're gonna use super positions
over the parameters and quantum dynamics
to optimize quantum neural networks.
And that's what we were thinking,
and hence the name of the company
because you use the multiverse in a sense to,
optimize over quantum neural networks.
Ultimately, I think our algorithm
was a bit more sci-fi than we wanted.
We ended up, Michael and I jumping to Google
and putting out like a 80 plus page paper,
which was just,
it's like a pressure valve of ideas.
I just have to unload them and put them out there.
But that paper is still out there and it's still valid.
It's just, it's gonna take more qubits to store.
So instead of just having variation algorithms
over the parameters on the classical computer
and the models on the quantum computer,
it's like everything was on the quantum computer
and the gradient descent itself is on the quantum computer.
And then after that, actually we collaborated, right?
On this continuous variable QOA,
which was adapting that algorithm
for analog photonic quantum computers.
So, yeah, that was a fun path there.
Yeah, I said it's really, really cool paper,
really far-reaching thinking I was thinking.
Like it's really like seeing a lot of things connect to them,
even if the devices aren't available today.
Like the ideas in that paper are really solid.
If anyone has the constitution to get through a few pages,
I'd recommend checking it out.
Right.
So Guillaume, you told, you mentioned Michael.
So tell me about the creation of TFQ.
Like you had to start up, how did that evolve?
How did TFQ emerge from that?
Yeah, I mean, I think, we ended up leaving the startup
and just writing a paper.
And I think we were joking.
We're gonna finish three weeks in advance
before the conference at NASA.
And then three weeks later we're like,
oh, we're probably gonna finish before the plane ride.
And then in the plane it's like,
oh God, I hope we finished the paper before the talk,
which is in a few days.
And yeah, I mean, we just put out the paper
that week and we gave a talk at NASA.
And at that point, Hartmut was there,
which was the lead, he is the head of Google AI Quantum.
And I just did the good old fashioned thing
of walking up to the lead, shaking his hand
and introducing myself.
And then he invited us to give a talk in Venice.
And then discussion started from there.
You know, Michael came down as well.
And we were just some ambitious kids
and they're like, well, if he can make a quick prototype,
we'll consider you guys to as potentially the guys
to build TensorFlow Quantum.
And when opportunity knocks,
you kick down that door and smile,
says Dwayne The Rock Johnson, right?
So we just flew home and then we hacked away
for like two weeks.
And yeah, and then we sent a prototype.
And it was very early prototype back then.
CERC wasn't out.
So we were starting from scratch.
But yeah, that was enough to get us attention.
And we did several internships on a rotation.
We recruited some of our friends.
We did kind of an after school kind of skunk work project
at accuracy to develop the library and open source.
It was a two pizza team.
I just ordered the pizzas on the weekends and nights
and we just hack away on top of our regular grad school.
And now we're all working in quantum industry.
So it's crazy how fast things go, yeah.
Yeah, that's really amazing how tides can turn like that.
If you've got the one idea in the right place
at the right time, you talk to the right person,
how it can really open these opportunities
that weren't clear before.
Right, right.
So in terms of opportunities,
we're getting more questions about advice.
People who want to make a transition,
they're in another field.
They're in, for instance, high energy physics.
High level, what would you advise them
if they do want to switch to something like QML
or any field really?
Yeah, that's a good question.
I guess, honestly, I don't want to be personally pinned down
to any one field ever, right?
I think I'm a journalist and it's kind of like,
it's a way of life.
It's not an end state.
It's like, you're always learning new fields.
So even myself, I'm trying to push myself
on like the ML side and other fields,
learning about neuromorphics and stuff like that recently.
So it's not an endpoint, it's like an ever-ending journey
of you just want to assimilate as much,
be curiosity driven and try to integrate that
into a very compressed model of things in your head, right?
Really make sure to be able to give the TLDR
to who I'm going to read for most of the papers you read
and just get used to ingesting a lot of papers
and just zigzagging through it
and being able to get the gifs of it.
It's got to be good enough so somebody gets it,
but maybe down sampled enough
so that the authors might be a bit offended
if you summarize this so simply, right?
So yeah, I mean, I think Einstein
built that skill at the patent office, right?
And I read that and I was like, okay,
I got to get better at just like getting
through a lot of content and filtering
and kind of getting to a latent representation
that's simplified, right?
And in that case, you want the most varied data set,
so that you could generalize.
So yeah, high intake, just intake a lot.
Read everything, doesn't matter
if it's not technically your field yet,
you might make connections later.
Yeah.
Great advice.
I hope that people in the chat
really appreciate hearing that from you.
All right, let's turn a little corner here.
We're getting a few questions
that are a bit more fun in nature.
So one question from actually someone on our team looks like,
have you ever played D&D?
It seems to be a theme right now at IQ Actors.
A few people are coming out as D&D players.
No, I've never played D&D.
All my physics friends were playing D&D
and I would just go for the bonus question,
the quantum field theory assignment in undergrad.
So that was my thing.
But I do, yeah.
I mean, recently I play a lot of video games
of all sorts on the custom PC and all that
that's been a big hobby of mine
during these times that we're indoors.
Is it running TPUs?
No, but it's running the best hardware you can get today.
So it's the coming edge AMD and Nvidia stuff.
Thank you for asking how much he's sunk into that bad boy.
It was, I'll just tell you,
it was an eBay purchase, not an Amazon or direct purchase.
So that's how you got to get these parts.
But yeah, it's interesting times.
I think everybody wants to game or mine things.
So very high demand for computing, which, it's good.
Ultimately, the more demand there is,
the more computing technologies ubiquitous,
the cheaper it is, and then the cheaper deep learning
or quantum simulations on classical hardware are, right?
And eventually, I guess that's what we want
with quantum computing as well.
We want like varied applications, right?
That like just, if there's a lot of applications
of quantum computing,
that's gonna give it a lot of customers
and there's gonna be bigger market for quantum computing,
quantum hardware and quantum electronics,
prices are gonna go down.
And so actually that's one of the reasons I went into QML
was in theoretical physics,
I thought that the future of theoretical physics
was actually through quantum computing to do quantum gravity.
And there's all sorts of research in that space.
And so I was like, okay, well,
how do I help quantum computing?
Well, we need better applications.
We need to widen the markets or invent new markets.
And so ML and quantum seem to be a high potential connection.
So focused in on that.
But it might be QML, but it might be quantum security
or I don't know, quantum hash functions,
heck, quantum blockchain someday.
I don't know, I don't wanna put it out there,
but I haven't read any research on quantum blockchain.
I just, you never know, right?
What's gonna drive demand for quantum computing?
So I think people should keep an open mind at this point.
It's okay to be opinionated.
Like I have all sorts of opinions on Twitter
about my bets of where I think advantages are gonna be.
But like ultimately, like I have an open mind
that I will be wrong on some instances
and people should be free to explore
like all sorts of crazy applications of quantum computing.
So.
Yeah, I don't think there's any end in sight
to the world's hunger for more and more computers.
It's gonna keep growing.
That's right.
Yeah, we have one more question in the chat here.
This is from someone you know, it's my colleague Juan Miguel.
He's asking, what's your opinion on publishing papers?
And I think there's, he's throwing some shade your way
because in the paper earlier that we had to collaborated on it,
never actually got published
because we never got past submission stated, I think.
Yeah, I mean, you know, I don't know.
I think.
You don't have to answer that.
That's just a.
No, it's some shade.
No, but I'll say something.
I think that switching from academia to industry.
I think academia, there's kind of this rat race for H index
or like overfitting to metrics, right?
And I guess to me, the important thing was having impact,
putting ideas out there and building great product
and making it convenient for people to do quantum computing.
And that's always my priority above all else.
I guess over like my boosting my own metrics.
But, you know, I may return back to some,
some old submissions and fix them up.
But there you go.
One thing, at least he's back in.
All right, good, good.
All right.
One more, one more kind of fun thing to finish off here, Guillaume.
So I saw, it was lots of activity on Twitter earlier
in the winter about QHAC, people were sending memes and so forth.
Just quickly, we have also, I noticed there was one tweet
where someone said that you should form a quantum band.
So I want to kind of riff on that a little bit, okay?
So you're going to form a quantum computing band.
What is the name of your quantum computing band?
Oh, definitely QBIT wave.
That's, you know, it's like synth wave, but like with QBITs.
And yeah, I mean, I don't really play instruments.
I think I did as a kid various things,
but not really too seriously.
So I wouldn't have to go, sorry.
You're going to be the lead singer?
I don't know, I'd be happy to create some synthetic quantum,
quantum noise with some quantum programming
and then we can use that as music, yeah.
So who else are the QHAC speakers
or even others in the fields?
Who's in your band?
I mean, I think Will Zeng and I agreed
that we'd probably join forces, but beyond that,
I don't know, I'm open-minded.
Too much.
It could start like that and then we could grow the band.
After we go on tour, sell a couple albums, you know, who knows?
Awesome.
And what is the title of your breakout album
for QBIT wave?
That's a good question.
I don't know, I think I do a nod to my old startup
and Everettian Vibrations or something like that.
Final question here.
What is your band's famous hit signal?
See, it's hit single.
And if you can use the word Feynman in it somewhere,
because I see them on the wall there
and some people are shouting us, they're based on.
Oh, Feynman, my hit signal, single.
Yeah, Feynman signals.
There you go.
All right, so watch for that coming fall 2021.
2021.
QBIT wave is going to be a new breakout act
in the quantum computing space.
Absolutely.
Thanks very much for joining us.
It's been really fun.
I'm sure that people in the chat
really enjoyed hearing from you today.
Thanks a lot.
Awesome.
Thanks for having me.
Thanks for inviting me.

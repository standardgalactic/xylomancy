Thanks for coming out. It's good to be here. As Eric said, I'm a philosopher, thinking
about consciousness, coming from a background in the sciences and math that always struck
me that the most interesting and hardest unsolved problem in the sciences was the problem
of consciousness. Way back, 25 years ago, when I was in grad school, it seemed to me
the best way to come at this from a big picture perspective was to go into philosophy and
think about the foundational issues that arise and thinking about consciousness from any
number of different angles, including the angles of neuroscience and psychology and AI.
In this talk, I'm going to present a slightly different perspective on the problem after
laying out some background, the perspective of what I call the meta problem of consciousness.
I always like the idea that you approach a problem by stepping one level up, taking the
meta perspective. I love this quote, anything you can do, I can do meta. I have no idea
what the origins was. I like the fact this is attributed to Rudolf Kahnap, one of my
favorite philosophers, but anyone who knows Kahnap's work is completely implausible. He
would ever say anything so frivolous. It's also been attributed to my thesis advisor,
Doug Hofstadter, author of Goethe Lescher Bach and a big fan of the meta perspective,
but he assures me he never said it either. But the meta perspective on anything is stepping
up a level. The meta problem, as I think about it, is called the meta problem because it's
a problem about a problem. A meta theory is a theory about a theory. Meta problem is
a problem about a problem. In particular, it's the problem of explaining why we think
there is a problem about consciousness. So there's a first order problem, the problem
of consciousness. Today I'm going to focus on a problem about it, but I'll start by introducing
the first order problem itself. The first order problem is what we call the hard problem
of consciousness. It's the problem of explaining why and how physical processes should give
rise to conscious experience. You've got all this neurons firing in your brain, bringing
about all kinds of sophisticated behavior. We can get to be done explaining our various
responses, but there's this big question about how it feels from the first person point of
view. That's the subjective experience. I like this illustration of the hard problem
of consciousness. It seems to show someone's hair catching fire, but I guess it's a metaphorical
illustration of the subjective perspective. The hard problem is concerned with what philosophers
call phenomenal consciousness. The word consciousness is ambiguous a thousand ways, but phenomenal
consciousness is what it's like to be a subject from the first person point of view. A system
is phenomenally conscious. If there's something it's like to be it, a mental state is phenomenally
conscious if there's something it's like to be in it. The thought is there are some
systems, so there's something it's like to be that system. There's something it's like
to be me. I presume there's something it's like to be you, but presumably there's nothing
it's like to be this lectern as far as we know. The lectern does not have a first person
perspective. This phrase was made famous by my colleague Tom Nagel at NYU who back in 1974
wrote an article called What is it like to be a bat? The general idea was what's very
hard to know what it's like to be a bat from the third person point of view just looking
at it as a human who has different kinds of experience, but presumably very plausibly
there is something it's like to be a bat. The bat is conscious. It's having subjective
experiences just of a kind very different from ours. In human subjective experience
consciousness divides into any number of different kinds or aspects like different tracks of
the inner movie of consciousness. We have visual experiences like the experience of
say these colors blue and red and green from the first person point of view and of depth.
There are sensory experiences like the experience of my voice, experiences of taste and smell,
their experiences of your body, feeling pain or orgasms or hunger or a tickle or something.
They all have some distinctive first person quality, mental images like recalled visual
images, emotional experiences like experience of happiness or anger and indeed we all seem
to have this stream of a current thought or at the very least we're kind of chattering
away to ourselves and reflecting and deciding. All of these are aspects of subjective experience,
things we experience from the first person point of view and I think these subjective
experiences are at least on the face of it data, data for the science of consciousness
to explain. These are just facts about us that we're having these subjective experiences.
As we ignore them, we're ignoring the data. So if you catalog the data that say the science
of consciousness needs to explain, there are certainly facts about our behavior and how
we respond in situations. There are facts about how our brain, facts about how our brain
is working. There are also facts about how subjective experiences and on the face of
it their data. And it's these data that pose what I call the hard problem of consciousness.
This gets contrasted with the easy problems, the so-called easy problems of consciousness
which are the problems of explaining behavioral and cognitive functions. Objective things
you can measure from the third person point of view typically tied to behavior, perceptual
discrimination say of a stimulus. I can discriminate two different things in my environment. I
can say that's red and that's green. I can integrate the information say about the color
and the shape. I can use it to control my behavior, walk towards the red one rather
than the green one. I can report it, say that's red and so on. Those are all data too for
science to explain. But we've got a bead on how to explain those. They don't seem to
pose as big a problem. Why? We explain those easy problems by finding a mechanism, typically
a neural or computational mechanism that performs the relevant function to explain
how it is that I get to say there's a red thing over there or walk towards it. Will
you find the mechanisms involving perceptual processes and action processes in my brain
that leads to that behavior? Find the right mechanism that performs the function. You've
explained what needs to be explained with the easy problems of consciousness. But for
the hard problem, for subjective experience, it's just not clear that this standard method
works. It looks like explaining all that behavior still leaves open a further question. Why
does all that give you subjective experience? Explain the reacting, the responding, the
controlling, the reporting and so on. It still leaves open the question why is all that accompanied
by subjective experience? Why doesn't it go on in the dark without consciousness, so
to speak? There seems to be what the philosopher Joe Levine has called a gap here, an explanatory
gap between physical processes and subjective experience, at least our standard kinds of
explanation which work really well for the easy problems of behavior and so on. Don't
obviously give you a connection to the subjective aspects of experience. There's been a vast
amount of discussion of these things over, well for centuries really, but it's been particularly
active in recent decades, philosophers, scientists, all kinds of different views. Philosophically
you can divide approaches to the hard problem into at least two classes. One is an approach
on which consciousness is taken to be somehow irreducible and primitive. We can't explain
it in more basic physical terms, so take it as a kind of primitive and that might lead
to dualist theories of consciousness where consciousness is somehow separate from and
interacts with the brain. Recently very popular has been the class of panpsychist theories
of consciousness. I know Galen Strossom is here a while back talking, he very much favors
panpsychist theories where consciousness is something basic in the universe underlying
matter and indeed there are idealist theories where consciousness underlies the whole universe,
so these are all extremely speculative but interesting views that I've explored myself.
There are also reductionist theories of consciousness from functionalist approaches where consciousness
is just basically taken to be a giant algorithm or computation. Biological approaches to consciousness,
my colleague Ned Block was here I know talking about neurobiology based approaches where
it's not the algorithm that matters but the biology is implemented in and indeed the kind
of quantum approaches that people like Roger Penrose and Stuart Hemeroff have made famous.
I mean I think there's interesting things to say about all of these approaches. I think
that right now at least most of the reductionist approaches leave a gap but the non-reductionist
approaches have other problems in seeing how it all works. Today I'm going to take a different
kind of approach, this approach through the meta-problem. One way to motivate this is to
ask, I often get asked, okay you're a philosopher it's fine you get to think about these things
like the hard problem of consciousness, how can I as a scientist or an engineer or an
AI researcher, how can I do something to kind of contribute to help get at this hard problem
of consciousness? Is this just a problem for philosophy? I mean for me to work on it say
as an AI researcher I need something I can operationalize, something I can work with
and try to program and as it stands it's just not clear how to do that with the hard problem.
I mean if you're a neuroscientist there are some things you can do. You can say work with
humans and look at their brains and look for the neural correlates of consciousness, the
bits of the brain that go along with being conscious because at least with humans we
can take as a background assumption, a plausible background assumption that the system is conscious.
For AI we can't even do that, we don't know which AI systems we're working with or conscious,
we need some operational criteria. In AI we mostly work on modeling things like behavior
and objective functioning for consciousness, those are the easy problems. So how does someone
coming from this perspective make a connection to the hard problem of consciousness? Well
one approach is to work on certain problems among the easy problems of behavior that shed
particular light on the hard problem and that's going to be the approach that I look at today.
So the guiding, the key idea here is there are certain behavioral functions that seem
to have a particularly close relation to the hard problem of consciousness. In particular
we say things about consciousness. We make what philosophers call phenomenal reports,
verbal reports of conscious experiences. So I'll say things like I'm conscious, I'm
feeling pain right now and so on. Maybe the consciousness and the pain are subjective
experiences but the reports, the utterances, I am conscious, well that's a bit of behavior.
In principle explaining those is among the easy problems, objectively measurable response,
we can find a mechanism in the brain that produces it. And among our phenomenal reports
there's this special class we can call the problem reports, reports expressing our sense
that consciousness poses a problem. Now admittedly not everyone makes these reports but they
seem to be fairly widespread among, especially among philosophers and scientists thinking
about these things but furthermore it's a sense that it's fairly easy to find and a
very wide class of people who think about consciousness. People say things like there
is a problem of consciousness, a hard problem. On the face of it explaining behavior doesn't
explain consciousness, consciousness seems non-physical, how would you ever explain the
subjective experience of red and so on. It's an objective fact about us, at least about
some of us, that we make those reports and that's the fact about human behavior.
So the matter problem of consciousness then at a second approximation is roughly the problem
of explaining these problem reports, explaining you might say the conviction that we're conscious
and that consciousness is puzzling. And what's nice about this is that although the hard
problem is this, you know, airy-fairy problem about subjective experience that's hard to
pin down, this is a puzzle ultimately about behavior. So this is an easy problem, one
that ought to be open to those standard methods of explanation in the cognitive and brain
sciences. So there's a research program. There's a research program here. So I like
to think of the matter problem as something that can play that role. I talked about earlier
if you say an AI researcher thinking about this. The matter problem is an easy problem,
a problem about behavior that's closely tied to the hard problem. So it's something we
might be able to make some progress on using standard methods of thinking about algorithms
and computations or thinking about brain processes and behavior while still shedding
some light, at least indirectly on the hard problem. It's more tractable than the hard
problem but solving it ought to shed light on the hard problem. And today I'm just going
to kind of lay out the research program and talk about some ways in which it might potentially
shed some light. This is interesting to a philosopher because it looks like an instance
of what people sometimes call genealogical analysis. It goes back to Friedrich Nietzsche
on the genealogy of morals. Instead of thinking about what's good or bad, let's look at where
our sense of good or bad came from, the genealogy of it all in evolution or in culture or in
religion. And people take a genealogical approach to God. Instead of thinking about does God
exist or not, let's look at where our belief in God came from. Maybe there's some evolutionary
reason for why people believe in God. This often leads, not always, but often leads to
a kind of debunking of our beliefs about those domains. Explain why we believe in God in
evolutionary terms. No need for the God hypothesis anymore. Explain our moral beliefs in say
evolutionary terms. Maybe no need to take morality quite so seriously. So some people
at least are inclined to take an approach like this with consciousness too. If you think
about the meta problem, explaining our beliefs about consciousness, that might ultimately
debunk our beliefs about consciousness. This leads to a philosophical view which has recently
attracted a lot of interest, a philosophical view called illusionism, which is the view
that consciousness itself is an illusion or maybe that the problem of consciousness is
an illusion. Explain the illusion and we dissolve the problem. In terms of the meta
problem that view roughly comes to, solve the meta problem, it will dissolve the hard
problem. Explain why it is that we say all these things about consciousness. While we
say I am conscious, while we say consciousness is puzzling, if you can explain all that in
say algorithmic terms, then you'll remove the underlying problem because you'll have
explained why we're puzzled in the first place. Actually, walking over here today, I noticed
that just a couple of blocks away, we have the Museum of Illusions. So I'm going to check
that out later on. But if illusionism is right and added to all those perceptual illusions,
it's going to be the problem of consciousness itself. It's roughly an illusion thrown up
by having a weird self-model with a certain kind of algorithm that attributes to ourselves
special properties that we don't have. So one line on the meta problem is the illusionist
line. Solve the meta problem, you'll get to treat consciousness as an illusion. That's
actually a view that has many antecedents in the history of philosophy one way or another.
Emmanuel Kahn, in his great critique of pure reason, had a section where he talked about
the self or the soul as a transcendental illusion. We seem to have this indivisible soul, but
that's a kind of illusion thrown up by our cognitive processes. The Australian philosophers
are on place, and David Armstrong had versions of this that I might touch on a bit later.
Daniel Dennett, leading reductionist thinker about consciousness, has been pushing for the
last couple of decades the idea that consciousness involves a certain kind of user illusion,
and most recently the British philosopher Keith Frankish has been really pushing illusionism
as a theory of consciousness. Here's a book centering around a paper by Keith Frankish
on illusionism as a theory of consciousness that I recommend to you. So one way to go
with the meta problem is the direction of illusionism, but one nice thing about many
people find illusionism completely unbelievable. They find you, how could it be that consciousness
is an illusion? Look, we just have the subjective experience, it's a datum about our nature,
and I confess I've got some sympathy with that reaction, so I'm not an illusionist myself.
I'm a realist about consciousness in the philosopher's sense where a realist about
something is someone who believes that thing is real. I think consciousness is real, I
think it's not an illusion, I think that solving the meta problem does not dissolve
the hard problem, but the nice thing about the meta problem is you can proceed on it
to some extent at least in initial neutrality on that question, is consciousness real or
is it an illusion? It's a basic problem about our objective functioning of these reports.
What explains those? There's a neutral research program here that both realists, illusionists,
people of all kinds of different views of consciousness can explain, and then we can
come back and look at the philosophical consequences. Well, I'm not an illusionist, I think consciousness
is real. I've got to say, I do feel the temptation of illusionism, I find it really intriguing
and in some ways attractive to you, just fundamentally unbelievable. Nevertheless, I think that the
meta problem should be a tractable problem. Solving it will shed, at the very least, will
shed much light on the hard problem of consciousness, even if it doesn't solve it. If you can explain
our conviction that we're conscious, somehow the source, the roots of our conviction that
we're conscious must have something to do with consciousness, especially if consciousness
is real. I think it's very much a good research program for people to explain. Then I'll move
on now to just outlining the research program a little bit more and then talk a bit about
potential solutions and on impact on theories of consciousness before wrapping up with just
a little bit more about illusionism. This meta problem, which I've been pushing recently,
opens up a tractable empirical research program for everyone, reductionists, non-reductionists,
illusionists, non-illusionists. We can try to solve it and then think about the philosophical
consequences. What is the meta problem? Well, the way I'm going to put it is it's the problem
of topic neutrally explaining problem intuitions or else explaining why that can't be done.
I'll unpack all the pieces of that right now, first starting with problem intuitions. What
are problem intuitions? Well, those are the things we say. There are things we think.
I say consciousness seems irreducible. I might think consciousness is irreducible. People
might be disposed, have a tendency to say or think those things. Problem intuitions all
take to be roughly that tendency. We have dispositions to say and think certain things
about consciousness. What are the core problem intuitions? Well, I think they break down into
a number of different kinds. There's the intuition that consciousness is non-physical. We might
think of that as a metaphysical intuition about the nature of consciousness. There are
intuitions about explanation. Consciousness is hard to explain. Explaining behavior doesn't
explain consciousness. There are intuitions about knowledge of consciousness. Some of
you may know the famous thought experiment of Mary in the black and white room who knows
all about the objective nature of color vision and so on, but still doesn't know what it's
like to see red. She sees red for the first time. She learns something new. That's an
intuition about knowledge of consciousness. There are what philosophers call modal intuitions
about what's possible or imaginable. One famous case is the case of a zombie, a creature
who's physically identical to you and me, but not conscious, or maybe an AI system which
is functionally identical to you and me, but not conscious. That at least seems conceivable
to many people. This is the philosophical zombie, unlike the zombies in movies which
have weird behaviors and go after brains and so on. The philosophical zombie is a creature
that seems, at least behaviorally, maybe physically like a normal human, but doesn't have any
conscious experiences. All the physical states, none of the mental states. It seems to many
people that's at least conceivable. We're not zombies. I don't think anyone here is
a zombie, I hope, but nonetheless, it seems that we can make sense of the idea and one
way to pose the hard problem is why are we not zombies. This imaginability of zombies
is one of the intuitions that gets the problem going. Then you can go on and catalog more
and more intuitions about the distribution of conscious, maybe the intuition that robots
won't be conscious. That's an optional one, I think, or consciousness matters morally
in certain ways and the list goes on. I think there's an interdisciplinary research program
here of working on those intuitions about consciousness and trying to explain them.
Experimental psychology and experimental philosophy and newly active area can study people's intuitions
about consciousness. We can work on models of these things, computational models or neurobiological
models of these intuitions and reports, and indeed, I think there's a lot of room for
philosophical analysis. There's just starting to be a program of people doing these things
in all these fields. It is an empirical question how widely these intuitions are shared. You
might be sitting there thinking, come on, I don't have any of these intuitions. Maybe
this is just you. My sense is from the psychological study to date, it seems that some of these
intuitions about consciousness are at least very widely shared, at least as dispositions
or intuitions, although they're often overridden on reflection. The current data on this is
somewhat limited. There is a lot of empirical work on intuitions about the mind concerning
things like belief, like when do kids get the idea that your belief's about the world,
can be false, concerning the way your self persists through time, could you exist after
the death of your body? Well, consciousness is concerned. There's work on the distribution
of consciousness. Could a robot be conscious? Could a group be conscious? Here's a book
by Paul Bloom, Descartes' Baby. The catalog's a lot of this interesting work, making the
case that many children are intuitive dualists. They're naturally inclined to think there's
something non-physical about the mind. So far, most of this work has not been so much
on these core problem intuitions about consciousness, but there's work developing in this direction.
Sarah Gottlieb and Tanya Lombrozo have a very recent article called Can Science Explain
the Human Mind on People's Judgements about when various mental phenomena are hard to
explain and they seem to find that, yes, subjective experience and things which have, to which
people have privileged first person access seem to pose the problem big time. So there's
the beginning of a research program here. I think there's room for a lot more.
The topic neutrality part, when I say we're looking for a topic neutral explanation of
problem intuitions, that's roughly to say an explanation that doesn't mention consciousness
itself. It's put in neutral terms. It's neutral on the existence of consciousness. The most
obvious one would be something like an algorithmic explanation. I can say, here is the algorithm.
The brain is executing. It generates our conviction that we're conscious and our reports about
consciousness. There may be some time between that algorithm and consciousness, but to specify
the algorithm, you don't need to make claims about consciousness. So the algorithmic version
of the metaproblem is roughly find the algorithm that generates our problem intuition. So that's
I think a principal research program that maybe AI researchers in combination would say psychologists,
the psychologist could help isolate data about the way that the human beings are doing it,
how these things are generated in humans and AI researchers can try and see about implementing
that algorithm in machines and see what results. And I'll talk about a little bit of research
in this direction in just a moment. But okay, now I want to say something about potential
solutions to the problem. Like I say, this is a big research program. I don't claim to
have the solution to the metaproblem. I've got some ideas, but I'm not going to try and
lay out a major solution or just so here are a few things which I think might be parts
of a solution to the problem, many of which have got antecedents here and there in scientific
and philosophical discussion. Some promising ideas include retrospective models, phenomenal
concepts, introspective opacity, the sense of acquaintance. Let me just say something
about a few of these. One starting idea that almost anyone's going to have here is somehow
models of ourselves are playing a central role here. Human beings have models of the
world, naive physics and naive psychology, models of other people and so on. We also
have models of ourselves. It makes sense for us to have models of ourselves and our own
mental processes. This is something that the psychologist Michael Graziano has written
a lot on. We have internal models of our own cognitive processes, including those tied
to consciousness. Somehow, something about our introspective models explains our sense,
A, that we are conscious and B, that this is distinctively problematic. I think anyone
thinking about the metaproblem, this has got to be at least the first step. We have these
introspective models. If you're an illusionist, they'll be false models. If you're a realist,
they needn't be false models, but at the very least, these introspective models are involved,
which is fine, but the devil's in the details. How do they work to generate this problem?
A number of philosophers have argued we have special concepts of consciousness, introspective
concepts of these special subjective states. People call these phenomenal concepts, concepts
of phenomenal consciousness. One thing that's special is these concepts are somehow independent
of our physical concepts. They explain, we've got one set of physical concepts for modeling
the external work world. We've got one set of introspective concepts for modeling our
own mind. These concepts, just by virtue of the way they're designed, are somewhat independent
of each other, and that partly explains why consciousness seems to be independent of
the physical world, intuitively. Maybe that independence of phenomenal concepts could
go some distance to explaining our problem report. I think there's got to be something
to this as well. At the same time, I don't think this goes nearly far enough because
we have concepts of many aspects of the mind, not just of the subjective experiential past,
but things we believe and things we desire. One, I believe that Paris is the capital of
France. That's part of my internal self-model, but that doesn't seem to generate the hard
problem in nearly the same way in which, say, the experience of red does. A lot more needs
to be said about what's going on in cases like having the experience of red and having
the sense that that generates a gap, so it doesn't generalize to everything about the
mind. Some people have thought that what we might call introspective opacity plays a
role, that when we introspect what's going on in our minds, we don't have access to the
underlying physical states. We don't see the neurons in our brains. We don't see that
consciousness as physical, so we see it as non-physical. Most recently, the physicist
Max Tegmark has argued in this direction, saying, somehow, consciousness is substrate
independent. We don't see the substrate, so then we think, ah, maybe it can float free
of the substrate. Armstrong made an analogy with the case of someone in a circus where
the headless person illusion where you don't see someone's there with a veil across their
head. You don't see their head, so you see them as having no head. Here is a 19th century
booth at a circus, a so-called headless woman with a veil over her head. You don't see the
head, so somehow it looks at least for a moment like the person doesn't have a head. Armstrong
says, maybe that's how it is with consciousness. You don't see that it's physical, so you
see it as non-physical. But still, the question comes up, how do we make this inference? There's
something that special goes on in cases like, say, color and taste and so on. Color experience
seems to attribute primitive properties to objects like redness, greenness and so on.
But in fact, in the external world, at the very least, they have complex, reducible
properties. Somehow, internal models of color treat colors like red and green as if they
are primitive things. It turns out to be useful to have these models of things where you treat
certain things as primitive even though they're reducible. And it sure seems that when we
experience colors, we experience greenness as a primitive quality, even though it may
be a very, very complex reducible property. That's something about our model of colors.
The philosopher Wolfgang Schwartz tried to make an analogy with sensor variables in,
say, image processing. You've got some visual sensors and a camera or something. You need
to process the image. Well, you've got some variables, some sensor variables to represent
the sensory inputs that the various sensors are getting. And you might treat them as a
primitive dimension because that's the most useful way to treat them. You don't treat
them as certain amounts of lights or photons firing. You don't need to know about that.
Use these sensor variables and treat them as a primitive dimension. And all that will
play into a model of these things as primitive. Maybe taking that idea and extending it to
introspection, you know, somehow these conscious states are somehow like sensor variables in
our model of the mind. And somehow these internal models give us the sense of being acquainted
with primitive concrete qualities and of our awareness of them. This is still just laying
out. I don't think this is still yet actually explaining a whole lot, but it's laying out.
It's narrowing down what it is that we need to explain to solve the meta problem. But
just to put the pieces together, here's a little summary. One thing I like about this
summary is you can read it in either an illusionist tone of voice as an account of the illusion
of consciousness. All this is how our false introspective models work or in a realist
tone of voice as an account of our true, correct models of consciousness. But we can set it
out in a way which is neutral on the two and then try and figure out later whether these
models are correct as the realist says or incorrect as the illusionist says. We have
introspective models deploying introspective concepts of our internal states that are largely
independent of our physical concepts. These concepts are introspectively opaque, not revealing
any of the underlying mechanisms, our perceptual models perceptually attribute primitive perceptual
qualities to the world, and our introspective models attribute primitive mental relations
to those qualities. These models produce the sense of acquaintance both with those qualities
and with our awareness of those qualities. Like I said, this is not a solution to the
meta problem, but it's trying at least to pin down some parts of the roots of those
intuitions and to narrow down what needs to be explained. To go further, you want, I think,
to test these explanations, both with psychological studies to see if this is plausibly what's
going on in humans. This is the kind of thing which is the basis of our intuitions and computational
models to see if, for example, you could program this kind of thing into an AI system and see
if it can generate somehow qualitatively similar reports and intuitions. You might think that
last thing is a bit far-fetched right now, but I know of at least one instance of this
research program which has been put into play by Luke Milhauser and Bach Schleggeres, two
researchers at Open Philanthropy, very interested in AI and consciousness. They actually built,
they took some ideas about the meta problem from something I'd written about it and from
something that the philosopher Francois Camero had written about it. A couple of basic ideas about
where problem intuitions might come from, and they tried to build them in to a computational
model. They built a little software agent which had certain axioms about colors and how they
work. There's red and there's green, and certain axioms about their own subjective experiences
of colors, and then they combined it with a little theorem prover, and they saw what did
this little software agent come up with, and it came up with claims like, hey, well, my
experiences of color are distinct from any physical state, and so on. They cut a few corners.
This is not yet a truly convincing, sophisticated model of everything going on in the human mind,
but it shows that there's a research program here of trying to find the algorithmic basis
of these states. I think as more sophisticated models develop, we might be able to use these
to provide a way in for AI researchers in thinking about this topic. Of course,
there is the question, if you model all this stuff better and better in a machine,
then is the machine actually going to be conscious, or is it just going to have found
self-models that replicate what's going on in humans? Some people have proposed an artificial
consciousness test. Aaron Sloman, Susan Snyder, and Turner have suggested somehow that if a machine
seems to be puzzled about consciousness in roughly the ways that we are,
maybe that's actually a sign that it's conscious. If a machine actually looks to ask
as if it's puzzled by consciousness, is that a sign of consciousness? These people,
this is suggested as a kind of Turing test for machine consciousness. Find machines which are
conscious like we are. Of course, the opposing point of view is going to be, no, the machine is
not actually conscious. It's just like the machine that studied up for the Turing test by reading
the talk like a human book. It's like, damn, do I really need to convince those humans that I'm
conscious by replicating all those ill-conceived confusions about consciousness? Well, I guess
I can do it if I need to. Anyway, I'm not going to settle this question here, but I do think that
if we somehow find machines being puzzled, it won't surprise me that once we actually have
serious AI systems which engage in natural language and modeling of themselves in the world,
they might well be natural to find themselves saying things like, yeah, I know in principle I'm
just a set of silicon circuits, but I feel like so much more. I think that might tell us something
about consciousness. Let me just say a little bit about theories of consciousness. I do think a
solution to the meta problem and a solution to the hard problem ought to be closely connected.
The illusionist says, solve the meta problem. You'll dissolve the hard problem. But even if
you're not an illusionist about consciousness, there ought to be some link. So here's a thesis.
Whatever explains consciousness should also partly explain our judgments and our reports
about consciousness. The rationale here is it would just be very strange if these things were
independent. If the basis of consciousness played no role in our judgments about consciousness.
So I think you can use this as a way of evaluating or testing theories of consciousness. For theory
of consciousness, there's mechanism M is the basis of consciousness, that M should also partly
explain our judgments about consciousness. Whatever the basis is ought to explain the reports.
And you can use this. You can bring this to bear on various extant theories of consciousness. Here's
one famous current theory of consciousness, integrated information theory developed by
Giulio Tononi and colleagues at the University of Wisconsin. Tononi says the basis of consciousness
is integrated information. A certain kind of integration of information for which Tononi
has a measure that he calls phi. Basically, when your phi is high enough, you get consciousness.
So consciousness is high phi. And there's a mathematical definition, but I won't go into
it here. But such a really interesting theory. So here's a, basically it analyzes a network
property of systems, of units, and it's got an informational measure called phi that's supposed
to go with consciousness. Question. How does, if integrated information is the basis of consciousness,
it ought to explain problem reports, at least in principle. Challenge. How does that work?
And it's at least far from obvious to me how integrated information will explain the problem
reports. It seems pretty dissociated from them. I mean, on Tononi's view, you can have simulations
of systems with high phi that have zero phi. They'll go about making exactly the same reports,
but without consciousness at all. So phi is at least somewhat dissociable. You get systems
very high phi, but no tendency to report. Maybe that's less worrying. Anyway, here's a challenge
for this theory, for other theories. Explain not just how high phi gives you consciousness,
but how it plays a central role in the algorithms that generate problem reports. Something similar
goes for many other theories, biological theories, quantum theories, global workspace, and so on.
But let me just wrap up by saying something about the issue of illusionism that I was
talking about near the start. Again, you might be inclined to think that this approach through
the meta problem tends, at least very naturally, to lead to illusionism. And I think it can be,
it certainly provides, I think, some motivation for illusionism. The view that consciousness
doesn't exist, we just think it does. On this view, again, a solution to the meta problem dissolves
the hard problem. So here's one way of putting the case for illusionism. If there's a solution
to the meta problem, then there's an explanation of our beliefs about consciousness that's
independent of consciousness. There's an algorithm that explains our beliefs about
consciousness, doesn't mention consciousness, arguably could be in place without consciousness.
Arguably, that kind of explanation could debunk our beliefs about consciousness the same way that,
perhaps, explaining beliefs about God in evolutionary terms might debunk belief in God.
It certainly doesn't prove that God doesn't exist. You might think that if you can explain
our beliefs in terms of evolution, it somehow removes the justification or the rational basis
for those beliefs. So something like that, I think, can be applied to consciousness, too. And
there's a lot to be said about analyzing the extent to which this might debunk the beliefs.
On the other hand, the case against illusionism is very, very strong for many people. And the
underlying worry is that some illusionism is completely unbelievable. It's just a manifest
fact about ourselves that we have conscious experience, we experience red, we feel pain,
and so on. To deny those things is to deny the data. Now, the dialectic here is complicated.
The illusionist will come back and say, yes, but I can explain why illusionism is unbelievable.
These models we have, these self-models of consciousness, are so strong that they're
just wired into us by evolution, and they're not models we can get rid of. So my view predicts
that my view is unbelievable. And the question is what, the dialectical situation is complex
and interesting. But maybe I could just wrap up with two expressions of absurdity on either side
of this question, the illusionist and the anti-illusionist, both finding absurdity
in the other person's views. Here's Galen Strawson, who is here. Galen's view is very much that
illusionism is totally absurd. In fact, he thinks it's the most absurd view that anyone has ever
held. There occurred in the 20th century, the most remarkable episode in the whole history of ideas,
the whole history of human thought, a number of thinkers denied the existence of something we
know with certainty to exist, consciousness. He thinks this is just a sign of incredible
philosophical pathology. Here's the rationalist philosopher Eliezer Yudkowski in something he
wrote a few years ago on zombies and consciousness, and the view, the epiphenomenalist view that
consciousness plays no causal role, where he was engaging some stuff I wrote a couple of decades
ago. He said, this is a zombie argument, the idea we can imagine zombies physically like us,
but without consciousness. Maybe a candidate for the most deranged idea in all of philosophy.
The causally closed cognitive system of charmer's internal narrative is malfunctioning in a way that
not by necessity, but just in our own universe miraculously happens to be correct. Here he's
expressing this debunking idea that on this view, there's an algorithm that generates these
intuitions about consciousness, and that's all physical. There's also this further layer of
non-physical stuff, and just by massive coincidence, the physical algorithm is a correct model of the
non-physical stuff. That's a form of debunking here. It would take a miracle for this view to be
correct. So I think both of these views are onto, these objections are onto something, and to make
progress on this, when I decide we need to find a way of getting past these absurdities. I mean,
you might say, well, there's middle ground between very strong illusionism and very strong epiphenomenalism.
It tends to slide back to the same problems. Other forms of illusionism, weaker forms don't
help much with the higher problem. Other forms of realism are still subject to this. It takes a
miracle for this view to be correct. Critique. So I think to get beyond absurdity here, both sides
need to do something more. The illusionist needs to do more to explain how having a mind could
be like this, somehow just like this, even though it's not at all the way that it seems. They need
to find some way to recapture the data. Realists need to explain how it is that these meta-problem
processes are not completely independent of consciousness. Realists need to explain how
meta-problem processes, the ones that generate these intuitions and reports and convictions
about consciousness are essentially grounded in consciousness, even if it's possible somehow for
them to occur or conceivable for them to occur without consciousness. Anyway, so that's just to
lay out a research program. I think a solution to the meta-problem that meets these ambitions
might just possibly solve the hard problem of consciousness or at the very least,
shed significant light on it. In the meantime, the meta-problem is a potentially tractable
research project for everyone, and mine I recommend to all of you. Thanks.
Yes, I just want to say I think it's very interesting this concept of we have these mental models,
collection of mental models, and that this collection of mental models is consciousness,
basically. Consciousness is defined as a collection of these mental models that we have,
and the problem of consciousness is that we don't understand the physical phenomenon that
causes these mental models or that stimulates these mental models. So we just have this belief
that it's ephemeral or not real or something like that. And if you take that view, then what's
interesting is that you could simulate these mental models, like robot could simulate these
mental models, and you could simulate consciousness as well. And even if the underlying physical
phenomenon that fuels these mental models is different, you know, robots have different
sensors, etc., you could still get the same consciousness effect in both cases.
Yeah, I think that's right. Or at the very least, you ought to be able to get,
it looks like you ought to be able to get the same models at least in a robot. If the models
themselves are something algorithmic, and ought to be, you ought to be able to design a robot
that has at the very least, let's say, isomorphic models in some sense that is conscious. Of course,
it's a further question, at least by my lights, but then the robot will be conscious. And that was
the question I alluded to in talking about the artificial consciousness test. But you might think
that would at least be very good evidence that the robot is conscious. If it's got a model of
consciousness just like ours, it seems very plausible there ought to be a very strong link
between having a model like that and being conscious. I mean, I think probably something
like Ned Block, who was here arguing against machine consciousness, would say, no, no,
the model is not enough. The model has to be built of the right stuff. Say it's got to be
built of biology and so on. But at least by my lights, I think if I have found the AI system
that had a very serious version of our model of consciousness, I take that as a very good reason
to believe it's conscious. In the IIT theory, is there a estimate or plausible estimate for what
the value of phi is for people and for other systems? Basically, no. It's extremely hard to measure
in systems of any size at all. I mean, because the way it's defined involves taking a sum over
every possible partition of a system. It turns out, I mean, A, it's hard to measure in the brain
because you've got to involve the causal dependencies set between different units on neurons. But even
for a pure algorithmic system, you've got like a neural network laid out in front of you,
it's computationally intractable to measure the phi of one of those once they get to bigger than
15 units or so. So, you know, today I'd like to say this is an empirical theory and in principle
empirically testable. But notice the in principle, it's extremely difficult to to to measure phi.
Some people, Scott Aronson, the computer scientist has argued, has tried to put forward counter-examples
to the theory, which are basically very, very simple systems like matrix multipliers that
multiply two large matrices turn out to have enormous phi. Phi is as big as you like if the
matrices are big enough. And therefore, by Tononi's theory, we'll not just be conscious, but as
conscious as a human being. And Aronson put this forward as a reductio ad absurdum of the IIT theory.
I think Tononi basically bit the bullet and said, yeah, yeah, those those matrix multipliers are
actually having some high degree of consciousness. So I think IIT is probably missing at least
missing a few pieces of what's going to be developed. But it's a research program too.
You mentioned belief as an example of something where, you know, this is another mental quality,
but people don't seem to have the same sense that it is very hard to explain. In fact,
it almost seems too easy where people like a belief about something sort of feels like just
how things are. You have to kind of reflect on a belief to notice it as a belief. Do you
think there's also or has there been research kind of related to this question into why is that
different? Like, it seems like another angle of attack on this problem is just like, why doesn't
this generate the same hard problem? Yeah, in terms, I'm not sure if there's been sort of
research from the perspective of the meta problem or a theory of mind. Certainly,
people have thought in their own right, what is the difference between belief and experience
that makes them so different? This goes way back to David Hume, a philosopher a few centuries ago,
who said, you know, basically, perception is vivid. Impressions and ideas. Impressions like
experiencing colors are vivid. They have force and vivacity and ideas are merely a faint copy
or something. But that's just the first order. And then there are contemporary versions of this
kind of thing, far more sophisticated ways of saying a similar thing. But yeah, you could,
in principle, explore that through the meta problem. Why does it seem to us that perception is so
much more vivid? What about our models of the mind makes perception seem so much more vivid
than belief? It makes belief seem kind of structural and empty, whereas
perception is so full of light. But no, I don't know of work on that from the meta problem
perspective. Like I said, there's not that much work on these introspective models directly. There
is work on theory of mind about beliefs tends to be about models of other people.
It may be there's something I could dig through a literature on belief that says something about
that. It's a good place to push. Thanks. I wanted to bring up Kurt Girdel. You mentioned your advisor
wrote Girdel Escher Bach. There's something that seems very like Girdel, Girdelian or whatever,
about this whole discussion in that. So Girdel showed that, given like a set of axioms and
mathematics, it would either be consistent or complete, but not both. And it seems like when
Daniel Dennett, Daniel Dennett seems to have like a set of axioms where he cannot construct
consciousness from them. He seems to be very much in this sort of consistent camp. Like he
wants to have a consistent framework, but is okay with the incompleteness. And I wonder if
similar approach could be taken with consciousness where we could in fact prove that consciousness
is independent of Daniel Dennett's set of axioms. The same way they proved after Girdel, they
proved like the continuum hypothesis was independent of ZF set theory. And then they added
the axiom of choice made at ZFC set theory. So I wonder if we could show that like in Daniel
Dennett's world we are essentially zombies or we are kind of either zombies or not. It doesn't
matter. Either statement could be true. And then find what is like the minimum axiom that has to be
added to Dennett's axioms in order to make consciousness true. Interesting. I thought for a
moment this was going to go in a different direction. And you're going to say Dennett is
consistent but incomplete. He doesn't have consciousness in this picture. I'm complete,
I've got consciousness, but inconsistent. That's why I say all these crazy things.
And you're faced with the choice of not having consciousness and being incomplete or having
consciousness and somehow getting this hard problem and being forced into at least puzzles
and paradoxes. But the way you put it was friendlier to me.
Yeah, I mean certainly Dark Hofstetter himself has written a lot on analogies between the
Gordelian paradoxes and the mind-body problem. And he thinks always our models, our self models
are always doomed to be incomplete in the Gordelian way. And he thinks that that might be somehow
part of the explanation of our puzzlement at least about consciousness. Someone like Roger Penrose,
of course, takes this much more seriously, literally. He thinks that the computational
aspects of computational systems are always going to be limited in the Gordel way. He thinks human
beings are not so limited. He thinks we've got mathematical capacities to prove theorems,
to see the truth of certain mathematical claims that no formal system could ever have.
So he thinks that we somehow go beyond that incomplete Gordelian. I don't know if he actually
thinks we're complete, but at least we're not incomplete in the way that finite computational
systems are incomplete. And furthermore, he thinks that extra thing that humans have is tied to
consciousness. I mean, I never quite saw how that last step goes, even if we did have these
special non-algorithmic capacities to see the truth of mathematical theorems. How would that be
tied to consciousness? But at the very least, there are structural analogies to be drawn between
those two cases about incompleteness of certain theories, how literally we should take the analogies
I'd have to think about. Has there been some consideration that the problem of understanding
consciousness sort of inherently must be difficult because we address the problem
using consciousness? I'm reminded of the halting problem in computer science where we say that
in the general case, a program cannot be written to tell whether another program will halt because
what if you ran it on itself? It can't sort of be broad enough to include its own execution. So I
wonder if there's a similar corollary in consciousness where we use consciousness to think about
consciousness. And so therefore, we may not have enough sort of equipment there to be able to unpack
Yeah, I mean, it's tricky. People say it's like a user ruler to measure a ruler. Well, I can use
this ruler to measure many other things, but it can't measure itself. On the other hand,
you can measure one ruler using another ruler. Maybe you can measure one consciousness using
another. The brain can't study the brain, but the brain actually has a pretty good job of studying.
The brain. So there are some self referential paradoxes there. And I think that again is at
the heart of Hofstadter's approach. But I think we'd have to look for very, very specific conditions
under which systems can't study themselves. I did always like the idea that the mind was simple
enough that we could understand it. We would be too simple to understand the mind. So maybe
something like that could be true of consciousness. On the other hand, I actually think that if you
start thinking that consciousness can go along with very simple systems, I think at the very
least we ought to be able to study consciousness in other systems simpler than ourselves. And boy,
if I could solve the hard problem, even in dogs, I'd be I'd be satisfied. Hey, so I have a question
about how the meta problem research program might proceed sort of related to the last question.
So certainly things we believe about our own consciousness, even if we all say them,
probably some of them are false. Our brain has a tendency to hide what reality is like.
If you look at like visual perception, you know, there's what's called lightness constancy, you
know, our brain subtracts out the lighting in the environment. So we actually see more reliably
what the colors of objects are. Like these viral examples of like the black and gold dress is an
example of this. And when you're kind of presented with an explanation of it, it's like, huh, my
brain does that. It's not something we have access to. Yeah. Or like the Yanni Laurel Laurel Yanni.
Yeah, illusion is like another one where like when you hear the explanation, you know, the scientists
that understand it, our own introspection doesn't include that. So how do you kind of proceed with
trying to get at what consciousness really is versus what our sort of whatever simplified or
distorted view might be? Yeah, well, one view here would be that we never have access to the
mechanisms that generate consciousness, but we still have access to the conscious states
themselves. Actually, the Colashley said this decades ago, he said, no process of the brain
is ever conscious. The processes that get you to the states are never conscious. The states they
get you to are conscious. So take your experience of the dress. For me, it was, it was what, white
and gold. So, you know, and I knew that, you know, each of us was certain that I am, I was
experiencing, I was certain that I was experiencing white and gold. Maybe you were certain that you
were experiencing blue and black. Which it was. I remember as I was right. You were sure that, yeah,
those idiots can't be, yeah, can't be looking at this right. But anyway, each of us, I think the
natural way to describe this at least is that each of us was certain what kind of conscious
experience we were having. But what we had no idea about was the mechanisms by which we got
there. So the mechanisms are completely opaque. But the states themselves were at least prima facie
transparent. So I think that would be the standard of view. And even a realist about consciousness
could go with that. They say, well, we know what conscious states, we know what those conscious
states are. We don't know the processes by which they're generated. The illusionist, I think,
wants to go much further and say, well, it seems to you that you know what conscious state you're
having. It seems to you that you're experiencing yellow and gold. Sorry, yellow and white, whatever
it was, golden, gold and white. Black and gold is whatever. Black and blue, I think. And blue.
Gold and white. Yeah. It seems to you that you're experiencing gold and white. But in fact, that
too is just something thrown up by another model. The yellow gold was a perceptual model. And then
there was an introspective model that said you're experiencing gold and white. When maybe, in fact,
you're just a zombie or who knows what's actually going on in your conscious state. So
the illusionist view, I think, has to somehow take this further and say not just the processes
that generate the conscious states, but maybe the conscious states themselves are somehow
opaque to us. All right. Thanks. It feels like some discussion of generality of a problem is
missing from this discussion. The matrix multiplier example of having high phi is still,
it's not a general thing. Is there someone exploring the space, the sort of intersection
of generality and complexity that leads to consciousness as an emergent behavior?
When you say generality, I mean, the idea that a theory should be general, that it should apply
to every system, you mean mechanisms of... No, generality of the agent, right? If I can write
an arbitrarily complex program to play tic-tac-toe, and all it will ever be able to do is play tic-tac-toe,
it has no outputs to express anything else. Yeah. As you said, general in the sense of AGI,
artificial general intelligence, I mean, some aspects of consciousness seem to be
domain general, like, for example, maybe as far as belief and reasoning is conscious,
those are domain general, but much of perception doesn't seem especially domain general, right?
Color is very domain... Taste is very domain specific, so it's still conscious. If my agent
can't express problem statements, like, if I don't give it an output by which it can express
problem statements, you can never come to a conclusion about its consciousness.
I like to distinguish intelligence and consciousness. I'm even able to... Even natural
language and, you know, being able to address a problem statement and analyze a problem,
that's already a very advanced form of intelligence. I think it's very plausible that, say, a mouse has
got some kind of consciousness, even it's got no ability to address problem statements in many
of its capacities, maybe very specialized. I mean, it's still much more general than, say,
a simple neural network that can only do one thing. A mouse can do many things, but I'm not
sure that I see an essential... I certainly see a connection between intelligence and
generality. We want to say, you know, somehow a high degree of generality is required for
high intelligence. I'm not sure there's the same connection for consciousness. I think
consciousness can be extremely domain specific, has, say, taste and maybe vision or it can be
domain general. So maybe those two cross cut each other a bit.
So it seems to me like the meta problem as it's formulated implies some amount of, like,
separation or epiphenomenalism between, like, consciousness and brain states.
And one thing that I think underlies a lot of people's motivation to do, say, science is that
it has causal import. Like, predicting behaviors is clearly a functionally useful thing to do.
And if you can predict all of behavior without having to explain consciousness,
their motivation for explaining consciousness sort of evaporates and it sort of feels like,
yeah, yeah, well, what's the point of even thinking about that because it's just not going
to do anything for me? What do you say to someone when they say that to you?
What is the thing that they say to me again?
That there's no, there's maybe consciousness exists, maybe it doesn't. But if I can explain
all of human behavior and all of the behavior of the world in general without
recourse to such concepts, then I've done everything that there is that's useful,
like explaining consciousness isn't a useful thing to do. And thus, I'm not interested in this and
it may not be real. I mean, I'm certainly, I'm not, I mean, I think epiphenomenalism could be
true. I certainly don't have any commitment to it though. It's quite possible that consciousness
has a role to play in generating behavior that we don't yet understand and maybe thinking hard
about the meta problem can help us get clearer on those roles. I think if you've got any sympathy
to panpsychism, maybe consciousness is intimately involved with how physical processes get going
in the, in the first place. And there are people who want to pursue interactionist ideas where
consciousness interacts with the brain. Or if you're a reductionist, consciousness may be just
a matter of the right algorithm. On all those views, consciousness may have some role to play,
but just say it turns out that you can explain all of behavior, including these problems without,
without bringing in consciousness. And does that mean that consciousness is not something we
should care about and not something that matters? I don't think that would follow. I mean, maybe it
wouldn't matter for certain engineering purposes, say you want to build a useful system. But,
you know, at least in my view, consciousness is really the only thing that matters. It's a thing
that makes life worth living. It's what gives our lives meaning and value and so on. So,
it might turn out that, okay, the point of consciousness is not that useful for explaining
other stuff. But it's, you know, if it's the source of intrinsic significance in the world,
then understanding consciousness will still be absolutely essential to understanding ourselves.
Furthermore, if it comes to developing other systems, like say AI systems or dealing with
non-human animals and so on, we absolutely want to know, we need to know whether they're conscious,
because, you know, if they're conscious, they presumably have moral status. If they can suffer,
then it's very bad to mistreat them. If they're not conscious, then you might, I think it's very
plausible, treat non-conscious systems. We can treat how we like, and it doesn't really matter
morally. So, the question of whether, say, an AI system is conscious or not, it's going to be
absolutely vital for how we interact with it and how we build our society. That's not a question
of engineering usefulness. That's a question of connecting with our most fundamental values.
Yeah, I completely agree. I just, I haven't found that formulation to be very convincing to others
necessarily. Hi, thank you so much for coming and chatting with us today. I'm really interested in
some of your earlier work, The Extended Mind Distributed Cognition. Yeah. And you're at a company
speaking with a bunch of people who do an incredibly cognitively demanding task. Yeah.
Most of the literature that I've read on this topic uses relatively simple examples of telling,
like, it's difficult to think just inside your head on these relatively simple things. And if
you take a look at the programs that we build, sort of like on a mundane, day-to-day basis,
there are millions of lines long. I've read people in the past say something like,
the Boeing 777 was the most complicated thing that human beings have ever made. And I think
most of us would look at that and say, we got that beat, you know, like the things that large
internet companies do, the size, the complexity of that is staggering. And yet if we close our
eyes, everyone in here is going to say, I'm going to have difficulty writing a 10-line program in
my head. Okay. So I'm just sort of as an open, like, I'd be very interested in hearing your thoughts
about how the activity of programming connects to the extended mind ideas.
Yeah. So this is a reference to something that I got started in about 20 years ago with
my colleague Andy Clarke. We wrote an article called The Extended Mind about how processes in
the mind can extend outside the brain when we become coupled to our tools. And actually,
our central example back then in the mid-90s was a notebook, someone writing stuff in a
notebook. I mean, even then, we knew about the internet and we had some internet examples.
I guess this company didn't exist yet in 95. But now, of course, our minds have just become
more and more extended and, you know, smartphones came along a few years later and everyone is
coupled very, very closely to their phones and their other devices that coupled them very,
very closely to the internet. Now, it's certainly the case that a whole lot of my memory is now
offloaded onto the servers of your company somewhere or other, whether it's in the
mail systems or navigation mapping systems or other systems. Most of my navigation has been
offloaded to maps and much of my memory has been offloaded. Well, maybe that's in my phone, but
other bits of my memory are offloaded into my file system on
some cloud service. So certainly, vast amounts of my mind are now
existing in the cloud. And if I was somehow to lose access to those completely, then I'd
lose an awful lot of my capacities. So I think we are now sort of extending
into the cloud, thanks to you guys and others. The question specifically about programming
programming is a kind of active interaction with our devices. I mean, I think of programming
as something that takes a little bit longer. It's a longer time scale. So the core cases of the
extended mind involve sort of automatic use of our devices, which are always ready to hand.
We can use them to get information, to act in the moment, which is the kind of thing that
the brain does. So insofar as programming is a slower process, you know, and I remember from
programming days, all the endless hours of debugging and so on. Then it's at least going
to be a slower time scale for the extended mind. But still, Feynman talked about writing
this way. Someone looked at Feynman's work and a bunch of notes he had about a physics problem
he was thinking about. And someone said to him, oh, it's nice you have this record of your work.
And Feynman said, that's not a record of my work. That's the work. That is the thinking.
And so I was writing it down and so on. I think, you know, at least my recollection from my programming
days was that, you know, when you're actually writing a program, that's not like you just
do a bunch of thinking and then code your thoughts. The programming is to some very
considerable extent your thinking. So is that the sort of thing you're thinking about here?
And the, if we, I think as people that program start to reflect on what we do,
and very few of us actually, like if you're the tech lead of a system, maybe you've got it in your
head, okay? But you would agree that most of the people on the team who've come more recently only
have a chunk of it in their head, and yet they're somehow still able to contribute.
Oh yeah, this is now distributed cognition. I mean, the extended mind that extended cognition
like starts with an individual and then extends out, extends their capacities out using their
tools or their devices or even other people. So maybe my partner serves as my memory, but it's
still centered on an individual. But then there's the closely related case of distributed cognition,
where you have a team of people who are doing something and are making joint decisions and
carrying out joint actions in an absolutely seamless way. And I take it as a company like this
that are going to be any number of instances of distributed cognition. I don't know whether the
company as a whole has one giant Google mind, or maybe there's just like a near infinite number
of separate Google minds for all the individual teams and divisions and so on. But I think yeah,
probably some anthropologist has already done a definitive analysis of distributed cognition
in this company, but if they haven't, they certainly need to. Thank you.
Thank you.

Welcome to the first lecture of Machine Learning on Columbia X. I'm your
professor, John Paisley. I'm a member of the Department of Electrical
Engineering as well as the Data Science Institute at Columbia and my
specialty is in machine learning and this is this class that we'll be working
through in the following lectures is directly based on the machine learning
course that I teach here at Columbia as part of the Data Science Institute. So in
this first lecture I want to primarily focus on a general overview of the
course and of what machine learning is and then go into a little bit of detail
on a very specific problem working with multivariate Gaussians in order to kind
of highlight the different aspects and the different components of what we'll
be discussing in this course. So this course will cover model-based techniques
for extracting information from data with some sort of an end task in mind. So
these tasks could include for example predicting an unknown output given some
corresponding input. We could simply want to uncover the information
underlying our data set with the goal of better understanding what's
contained in our data that we have or we could do things like data driven
recommendation, grouping, classification, ranking, etc. So using the data to help us
learn how to perform these sorts of end tasks. So in a course like this there's
a few ways that the information can be presented, different orderings of the
information. One example would be to partition it as in one half supervised
learning, the other half unsupervised learning and I'll discuss that in a bit
more detail in this lecture because that's the perspective that we're going
to take. But we could also think in terms of probabilistic models where we are
working with probability distributions versus non probabilistic models where
we're learning from the data without any sort of probability probabilistic
motivation. There's also a dichotomy between modeling approaches. So what is
the model that we want to define for our data versus optimization approaches
which is very tightly linked with modeling approaches. But with optimization
now that we've defined a model, how do we learn the model? So these are two
separate problems with various techniques in these two subproblems. So again
we're going to partition this course into roughly two halves. The first half
will focus on supervised learning and the second half will focus on
unsupervised learning and these additional ideas such as probabilistic
versus non probabilistic or modeling approaches versus optimization techniques
will be sort of discussed as we go along as needed. So those will be
interwoven throughout the course but the first part of this course will be
strictly supervised learning and the second part will be on supervised
learning. What do we mean when we say we want to perform supervised learning?
In a nutshell what we're saying is that we want to take inputs and predict
corresponding outputs. So for example if we do want to do regression we might
have pairs of data in this case a one-dimensional value for x and a
corresponding one-dimensional value for t and then we would want to learn some
sort of a function so that we input x and we make a prediction for the output
t. So for example here we have several data points as indicated by circles
where the x-axis is the input for that particular point and the t-axis is the
corresponding output and now that we have this data set of these several
points we want to define some sort of a regression function. For example this
blue line that in some way interpolates the outputs as a function of the inputs
and then the goal is given this smoothing function that we've learned for some new
input x we want to predict the corresponding output t. So for future
time points we obtain x, we obtain an input and we want to predict the
corresponding output. So that's regression, we say it's regression
because the outputs are assumed to be real valued. Classification is another
supervised learning problem that is slightly different. The form or the
structure is very similar, we have pairs of inputs and outputs and we get this
data set which has many of these pairs of inputs and outputs and we want to
learn some function so that in the future when we get a new input for which
we don't have the output we can make a prediction of the output that's going to
be accurate. However the key difference here is that where with regression the
output is a real valued output. With classification it's a discrete valued
thing so it's a category or a class so in this right plot what I'm showing are
input output pairs except now the input is a two dimensional vector so here the
input would be this two dimensional point and the output for this plot is
being encoded by a color so the output could be one of two values either a blue
value or an orange value so in this case we want to take our data inputs and
classify them into one of two outputs. So we get a data set like this with all of
these input locations and the corresponding color coded output and now
our goal is to learn some sort of a function a classifier so that we can
partition the space such as is shown here where for a brand new point any of
these points that we don't have the output we can evaluate the function at
that point and make a prediction of the output so we might say for this data set
we would partition this entire region here these two regions into the orange
class and this region here into the blue class so any new points falling in this
region will be assigned to the blue class. So the key here with supervised
learning is that we're learning an algorithm based on a function mapping
from input to output. The outputs are basically going to be telling us how
to map the inputs so that we have an accurate function so we have input
output pairs. So to look at a classic example we could think of spam detection
given some set of inputs like these two chunks of text we would want to assign
it a label plus one or minus one sometimes we would say plus one or zero
but we would want to assign it one of two possible labels one label would
correspond to an email that is spam and we would want to then you know
automatically delete that email and the other class would be non spam emails
emails that we want to put into our inbox and actually read so it's
essentially a filtering problem so for example we might have a data set a data
point like this containing this text and we would want to now input this into
some sort of a function and say is it spam or not in this case most likely it's
not or a data point like this this piece of text where we would input it into
the same exact function with the same classifier and in this case that same
classifier would say this email is a spam so we classify this email to spam
and this email to non spam using the same classifier learned from examples of
labeled spam and labeled non spam emails. So essentially the first half of this
course is going to be all about learning different ways that we can define these
functions to map inputs to outputs either regression models or classification
models depending on the problem as well as algorithms or techniques for then
learning the parameters of these models based on data so that will take up the
first half of this course there are many very useful techniques very different
techniques for performing these two tasks they'll entail different ways of
thinking about the problems probabilistic versus non probabilistic the models that
we define will require different ways or different techniques for learning them
so we'll need different optimization techniques so the first half of this
course will be all about supervised learning then in the second half of the
course will transition to unsupervised learning and with unsupervised learning
the goal is a bit more vague supervised learning is very nice because we know
that we want to map an input to an output and honestly we don't necessarily
even care how it's mapped we don't care whether we learn anything by mapping it
or in many cases we don't perhaps in some cases we do we simply want to say
here's my input what should I map it to as an output and we measure the
performance based purely on how well it does that task with unsupervised
learning we don't have in most cases this sort of an input output mapping we
want to perform more abstract or vague tasks such as understanding what is the
information in our data set for example we don't have an infinite amount of time
to read you know so many thousands or millions of documents so we want a fast
algorithmic way for taking in information taking in data and extracting the
information for us so for example with unsupervised learning we might want to
do something like topic modeling where we have many texts data many documents
provided to us we don't have any labels for these documents all we have is the
text for each document and then we want to extract what is the underlying what
are the underlying themes within these documents so that's the idea of topic
modeling we also might want to do recommendations this would be where we
have many users and many objects and users will give feedback or input on a
subset of these objects either through a review or through some sort of a rating
like a star rating for example with Netflix a user could rate a movie one
to five stars and we want to take all of this information and learn some sort of
a latent space where we can embed users and movies such that users who are
close to each other share similar tastes movies that are close to users are
somehow appropriate recommendations to be made to those users movies that are
close to each other are similar in their content and things that are very far
apart are very unlike each other so we want to learn this information simply
from the data from the raw data and some model assumption that we have to make so
for example one of the most well-known unsupervised learning tasks is the
topic modeling problem and so what I'm showing here is an example of what a
topic model will learn if you provide it with over a million documents from the
New York Times over roughly 20 year period so what we have in these
documents is simply a tag that says that this is a document and these are the
words in the document and we have that repeated for all of the documents in our
data set again it can be over a million of these documents we want to make some
sort of a modeling assumption such that we find the words that should somehow
cluster together these words would then define topics underlying our data set so
for example by simply inputting the raw data from the New York Times making a
modeling assumption that doesn't in advance tell us which words should go
with which other words we can then run an algorithm to extract information like
this that says that this set of words belongs together this set of words
belongs together and so on so we can learn for example 10 or more of these
what are called topics that tell us which words belong together and then not
shown here is also how each document uses that topic so for example for a
particular document we might say it's composed of these two topics and no other
topics and so this is information that's extracted from the raw data we don't
a priori tell the algorithm what it should learn we simply say there is this
structure that we want to learn here's the data tell me the structure

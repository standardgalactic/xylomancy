really difficult problem, you'd think it was probably solved 100 years ago, it hasn't been.
Even as recently as when Margaret Bowden was writing on this, one school tried to say life is
and then give enumerated a list of properties that have to metabolize, it has to reproduce,
Varela Acherana looked at it from a different perspective. Well, what fundamentally life is
a system that has a circular organization, it's got to be able to maintain its own boundary
of itself and the other. And it has to encapsulate the rules, the automatic rules that maintain
that boundary in the face of a changing environment. We spoke to Friston, Carl Friston,
quite recently, he was talking about Markov blankets, which is quite interesting about
how do you define the boundaries of a physical system? And you know, does a hurricane have
a Markov boundary? And what we're talking about here in a very general sense is defining boundaries
between what lives and what doesn't live and what is meaning and what isn't meaning and what is
understanding and what isn't understanding. And it's very philosophical, it's quite difficult to
pin this down. If you look at a Maturana and Varela's book, it's about 60 pages, their original
treatment, auto-precision cognition from the 70s. And it is very dense. It's not a waffly
philosophical book, it's quite hardcore, quite mathematical. Yeah, I think they do do an interesting
job at pinning down what, if you like, what it might be for something to be alive. And this
has been, this challenge which was explored in the embodied mind has since been developed by Evan
Thompson, who's an American, an interesting philosopher who wrote a book called Mind in Life,
where the argument is laid out that life is a continuum and wherever you have this
continuum of life, then you have a proto-mentality. And I'm kind of drawn to that. I think it's a
very persuasive argument. And then we have to look at the question of what constitutes autonomous
systems and why should it matter if an autonomous system has a phenomenal sense of what it is like
to be. Here, if you like, you can link into the work of a guy I've just recently come across who
wrote to me a few weeks ago, he used to run a big lab in France, AI lab in France, Mikhail Troublay,
I think his name is. And he argues that we need phenomenal consciousness to
arbitrate between different actions if you're sending a robot to a Mars that's got to be
completely autonomous and it's got to react appropriately in different, in unknown environments
all sorts of different threats. Effectively, the robot's got to have to know something that
it's a state that it's good to be that makes the robot feel pleasant and a state that's horrible,
that's dangerous, that might cause death to the robot. It has to, we use the phenomenal
sense of what that feels like, we can then use that to arbitrate between different actions.
And this has actually, by the way, is an idea that was brought, I first came across through a
paper by Daniel Dennett called Cognitive Wheels the Frame Problem of AI, when he looks at what
must be known to us to arbitrate on what you call the cookie problem. Imagine you've got a big jar
of cookies and some little kids like my six-year-old daughter and two families, one next to each other
and in one family, when the child goes for a cookie, the family beats it, smacks it relentlessly
until it's in tears and never goes, it doesn't have any more cookies after that. And the other one,
they're very sort of touchy feeling, oh no, please don't have another cookie tarquin and
occasionally tarquin does go and have another cookie. Dennett asks the question, why is it
that beating your kid causes that child not to go for the cookie jar anymore?
And we know that well because being beaten is something deeply unpleasant and you don't
particularly, unless you're a masochist, want to do things that are going to bring this
feeling of pain about you. But then Dennett said, well, how do we know that? We can arbitrarily
hard wire six facts in, so I could hard wire into my computer program. If something else biffs me,
then I'm going to increase my pain by one and if pain gets over a certain threshold, I will do
that action again. That's incredibly brittle, it's literally arbitrary. But unless we have
phenomenality, unless we have access to phenomenal states and know that getting biffed hurts or
going over rough terrain if you're a marginal robot, it takes you around a bit, you have to just
sidestep that by hard wiring effectively, hard coding as engineers and the system is no longer
autonomous now, we're having to define what it has to do for all these different possible states.
That's the price we have to pay. So I think you can argue that evolution is as blessed as with
phenomenal consciousness, so that we can act autonomously. And that's the way that, I guess,
after Evan Thompson and Varela and Tourboulay, the view that I come to. So we need consciousness
to enable us to succeed evolutionarily. Well, haven't we just fallen foul of the kind of prime
axiom of software engineering at this point that every problem is just a level of abstraction away?
Because, I mean, we could, I forget which paper it was, you had a really great
conclusion in one of your papers where you essentially said, if we built a bunch of robots
that looked and behaved just like us, they're autonomous, they laugh at our jokes, they respond,
but the thing that defines the difference here or the difference between us and them would be
that when we're laughing and feeling we feel it, it's phenomenological, when they do it,
it's not, it's a simulation. But, you know, leading into this argument, it's like, well,
to have these Martian robots that have autonomy and can succeed, they need to have this phenomenological
state. Isn't this really just a software engineering problem away from being solved?
I think when I wrote that paper, I was, I mean, it's only very recently, literally, and I want to,
I'm hoping to work at least to reach out to Michael to see if we can do something together.
Quite excited by these essays he's sent me, I think they make a, I don't know why,
just haven't occurred to me that this could be a reason why consciousness has evolved.
He makes a very persuasive case. I'd love to claim it as my idea, but it absolutely isn't,
but I think it's a very beautiful one. I need to understand more and either he will push it or
perhaps we might do something together, I don't know. But, yeah, so I'm a little bit skeptical
that without that, the glue of consciousness, that we could get a machine to act as a similar
chroma view in all possible cases. I was arguing from the stronger, I guess, in that paper that,
well, that's just assuming that we can. I think engineering wise, I'm a little bit skeptical
now, following Michael's work, that that is going to be possible. But also to come back to another
point that relates to the, are we going to talk about the Chinese room at all, or are you assuming
that's boring for all your readers? No, no, I think that's one of the most important things,
because we're talking now about consciousness and the various different boundaries between
what is and what is not consciousness. But I think the other one that's really important is
understanding and the boundaries between what is and what isn't understanding. You say in one of
your papers, what does it mean for a central processing unit to understand? Does it understand
the program and its variables in a manner of analogous to cells understanding of this rulebook?
But this cells rulebook thing, it describes a procedure, as you say in one of your papers,
that if carried out accordingly, allows cell to participate in an exchange of uninterrupted symbols,
squiggles and squiggles, which to an outside observer, look as if cell is accurately responding in
Chinese to questions in Chinese about stories in Chinese. In other words, it appears as if cell
in following his rulebook actually understands Chinese, even though cell trenchantly continues
to insist that he does not understand a word of the language. So this has been used, I think,
very reliably. And you had a paper actually recently introducing a couple of other responses,
because there were four responses to cell's argument, right? There was the robot reply,
the systems reply, the brain simulator reply, the combination reply. And in your recent paper,
you talked about robots and animats. In the Target VBS article, there was a lot more than
four. So when he wrote the paper, came up with four possible counter arguments, which are the
classic ones that Tim just outlined. But from memory, there must have been over 20 people,
really big names in philosophy and AI who responded to that VBS Target article. People like Marvin
Minsky, McCarthy, Denner, obviously, God, I can't, I'm ashamed to say I've forgotten, but
big, big names who wrote different responses. There's a lot more than the four that cell,
but I mention these because what's been bizarre, I edited today with John Preston,
you edited on the 21st anniversary of the Chinese Reum argument, John Preston and I put together
an edited collection of responses to at 21 years on from leading AI scientists, cognitive
scientists and philosophers. And I still think that's a good collection of essays that we picked
and good collection of people to contribute to that volume. And again, and the intervening goes
between this Chinese Reum argument coming out and that volume coming out and then between
that volume coming out in 2002 and 2002 and now, really, and I've talked about this as you can imagine,
in many places, most of the UK universities and quite a few in Europe and one or two in America.
And nearly all the most formidable responses that I've come across really go back to responses
that Sirle actually predicted. And by far the most common and probably the most, I think the
strongest responses is some variance on what became known as the systems reply that you mentioned him.
Can you just quickly define what the systems reply is? Do you mind if I just go over again,
as you read up really, really quickly the essence of the argument, but I'd like just to sort of
unpack it slightly more slowly. So Sirle, imagine Sirle, to set the scene, Sirle's a monoglot English
speaker, the only shamefully like myself, shamefully because I'm married to a Greek lady,
I still can pretty well only communicate in English at best. And then Sirle can as well. So he imagines
himself locked in a room and this room has got effectively got a letterbox instead of a door
to which he could communicate with the outside world and in the room of three piles of papers.
And on these papers are strange symbols that Sirle doesn't know what they are. We know,
there's people reading about the experiment, hearing about it, that these are actually Chinese
ideographs. But to Sirle, they're just uninterpreted squiggles and squiggles. You've got no idea what
they are. So you've got these three piles of things. And on the desk, there's a big grimoire,
book that tells Sirle how to correlate symbols from the first pile with symbols on the second.
And then other rules that tell him how to correlate symbols on the first pile and with
the second pile and also linking symbols on the third pile. And other rules that tell him how
to take symbols from one of these piles and stick them to people through their throats to the outside
world. Well, unbeknownst to Sirle, the first pile defines a script, a second in Chinese, a second
pile describes a story in Chinese, and the third pile describes questions about that story in Chinese.
And the symbol Sirle was told by the book to give to people in the outside world
answers to questions to that story in Chinese. And Sirle's point is that if we concede, so he's
arguing again from the strongest, it says, okay, let's concede, but that rule book, however,
it's defined. And again, a lot of people got home because they thought Sirle was purely talking
about a naive pattern matching program. If this symbol doesn't then do that, actually Sirle makes
it clear, if you read the paper carefully, that he wants us to stand for any conceivable computer
program. This was the first reaction that I had, I had an allergic reaction to it, because
as we know from talking to Wally Tsubba, you know, that we can't write the damn compiler for
the language. It's too complicated. So it's not possible really for even us to explicitly understand
and verbalize the rules that we use in language. And you said yourself that artificial intelligence
practitioners were incredulous at the extremely kind of, you know, simplistic view of Sirle that
you could have this, you know, low level rules described. Yeah, I mean, if that's
because they didn't read the paper carefully, because Sirle makes it absolutely explicit,
that he generalizes, he gives a simple example to sort of just get you thinking about the problems.
And imagine, that's again, this is the world that, you know, some people have tried to do language
understanding in this very naive way. But Sirle wants the thought of the stand for any possible
program. All we're doing is, the rule book tells Sirle how to manipulate uninterpreted symbols
and put uninterpreted symbols out of the door. How it does that, whether it's implementing a neural
network, whether it's simply a genetic algorithm, whether it's implementing Wally Tsubba's sense-based
word effect, sense-based word effect, compositional understanding, natural language designing,
whether it's doing GPT-3 kind of operations, it's irrelevant. Sirle says, whatever your program is,
that's what's in the book. And at the end of the day, that program will tell me how to respond to
questions in Chinese with answers in Chinese. If I follow that program carefully, don't make
any mistakes, I'll give answers out the door. And if your program's any good, it will give answers
that are indistinguishable from those a natural and native speaking Chinese person would give,
even though, as Sirle trunchedly insists, I, following this program, is not
enabling me to get even the toe-hold in Chinese semantics. All I've been doing is like a mega-fast
idiots of wrong, manipulating uninterpreted symbols around and sticking some symbols that I don't
know what the hell they are through a letterbox in the net to the outside world.
Does that imply then that it's just observationally impossible to determine whether a black box is
conscious? Well, this is a, I wrote a cancer argument to Susan Schneider's
Turing Test for Machine Consciousness, where I make that exact claim. This was in frontiers,
in one of the frontiers journals a couple of years ago, because Susan says, oh, we can ask
her ideas, if we ask questions that are about particularly human activities relating to
phenomenal experience, we'll be able to tell whether this machine is, she gives a procedure
for doing this. I would say, well, if whatever set of questions there is a new have for deciding
whether your machine is conscious, I'm going to sit, unbeknownst to you, and watch you ask them.
I'm then going to go away and write a little program in basic, because I'm good at writing in
basic, that says, if question one says bladi bladi bladi, are you Susan's first question,
give this answer, which is the answer that a really complicated machine consciousness program
gave. So in fact, I have a lookup table. But of course, Susan doesn't know, because I sneakily
switch the machine. So she asks her questions, thinking she's talking to a really complicated
machine consciousness thing. She asks the questions which she claims will tell her whether this
machine is conscious, but she's actually just interrogating with a really simple lookup thing.
And at the end of the day, so the answer she wants is, yeah, that's conscious, but she's
just been talking to a lookup table. It's, yeah, I think that you're quite right, Keith. I think
isn't obvious to me, how we're going to be able to do a test for machine consciousness purely on
the basis of external observation, in the absence of anything else, because we cannot if we're
Machiavellian, we can always cheat. The thing is, when you were talking about it doesn't have
semantics, I want to one pick that a little bit as well, because you said the robot rover on Mars.
The semantics there were the state spaces of all of the sensory experiences. And then you said
it's brittle, and there's an alignment problem. And I can understand that it's very similar to the AI
alignment argument. But this is different. If you if you can replicate, let's say you're talking to
a black box, and you can't distinguish whether or not it has consciousness or whether it has
understanding. Why is there an issue of semantics in that case? Ha, you're wrong for me. I thought
you're going to say, how can I tell that anybody understands us or is conscious, which is actually
one of the core responses that's all anticipated in the Chinese room argument.
That is actually the obvious question as well. Presumably, you think that we are conscious,
because we might exist in a computer simulation, right?
Well, I don't think we can, because I know that if I slap my face, it hurts, right? And I know
that machines can't instantiate phenomenal consciousness. I made a paper called Refuting
Digital Ontology, just after an invited talk at the Royal Society workshop on the Incomputable,
hosted by Barry Cooper, who is the leader of the Turing Centenary celebrations in the UK and
Worldwide. And I made this very argument then that it isn't obvious to me, I think that it's
clearly obvious to me that we're not in a computer simulation, because I feel. And if my dancing with
pixels reduct your argument is correct, unless I'm willing to accept panthechism, then computations
can't realise sensation. So either my dancing with pixels reduct your is wrong, in which case I'm very
happy, sad, but in the end, I'll be happy when you show me where I'm going wrong.
Or if I'm right, then we're not living in a computer simulation. So I don't think we are
computer simulations. Furthermore, it's an axiom of cognitive science that other minds exist.
So it's not for me to have to explain why I believe that you three guys have phenomenal
conscious states. That's part of cognitive sciences is acknowledging that you do and
trying to come up with a theory that explains why these occur and how they occur.
The conscious state argument to kind of perhaps play devil's advocate a bit here. To me, when I
first came across the Dancing with Pixies argument, this wasn't an argument that, for me,
implied rocks had understanding or intelligence. To me, it implied that we don't, there's nothing

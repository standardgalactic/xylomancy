Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?
Yeah, it's actually in Hebrew, it's Chepur.
Oh, Chepur, yeah.
All right, so Hila is a PhD candidate at Tel Aviv University,
advised by Professor Leon Wulf. Her research focuses on developing reliable XCI algorithms
and leveraging them to promote model accuracy and fairness. Today she's going to talk to us
about transform or explainability and we're so excited to hear from you.
Oh, thank you for that great introduction and I actually have some slides here.
I think you can just, oh yeah, slides here introducing myself, but I think you did that
perfectly, so I'll just skip that. So maybe the first thing we want to talk about is motivation.
We're here to talk about transform or explainability, but why should you care?
And let's just have a disclaimer because we all know that explainability is really important
for aspects like accountability, reliability and so on. But when we write research papers,
we usually focus on other measuring sticks such as accuracy and robustness, right?
So I'm here to convince you that the explainability queue is actually really useful for
those measuring sticks as well and that you should consider using explainability
in your research, even if it's unrelated to accountability, reliability and so on.
So to do that, I have a few examples showing how explainability can be used to improve model
accuracy and robustness. The first example is text-to-live, maybe you know it, it got accepted
to ECCV 2022. And the objective of the model is to take the target edit text, for example here,
a red hat and apply the edit to the image. So what they try to do is, unlike other works,
they try to prevent the part where the user actually has to insert a manual segmentation
mask to indicate where the hat is. So they wanted some automatic way of getting to the region of
interest, which is the hat. So they used relevancy maps of the clip model, which I guess you're
familiar with, yeah. So the clip model with the relevancy map actually indicated to their
downstream path where, you know, pretty much the hat is in the image. And then their model
refined those relevancy maps and applied the edit according to the location indicated by
the relevancy map. So here you can see what happens without this, they call it bootstrapping,
without using the relevancy map. So you can see that there are additional artifacts,
such as the faces turning red and not just the hat. And when you use bootstrapping,
when you use relevancy maps, then the edit is quite localized to the hat. And here you can see
other edits that are quite nice. They take a sponge cake, and they turn it into an ice cake
or spinach mousse cake. So I think it's a nice example to show. Another example is a paper
called Clipasso. It got best paper awarded Seagraph 2022. It also uses relevancy maps. The goal of
the model is to take an input image and create sketches with different levels of abstraction.
So you can choose which level of abstraction you want. So for example, the flamingo here,
very abstract painting of the flamingo or very detailed painting. And what they did is they
actually used the relevancy maps as an initializer for the model to understand where the object is
and how to create the stroke product. Another example is Samir's work, which you're probably
familiar with. What you did here, really, is you used the relevancy maps in order to locate
objects, which aren't necessarily objects in the wild, objects that appear in living rooms and
that do not necessarily appear in the training set of some models of these segmentation or
localization. So you can use Clip in order to identify objects that are really not really
objects that are so common in training sets. So if you consider all the examples that we
seen before, they have one thing in common. They use the relevancy maps as a fixed signal.
They didn't train on the relevancy map or create a loss with the relevancy map. They use it as an
initialization for a downstream task. But what we did in our last work is actually just we showed
that explainability can be used as a loss term in order to improve models. So if you think about
what it means to create a loss term based on explainability maps, it's really
meant to teach the model how to do something or why it does something, not just how to do something.
And we talk about classification models. They tend to learn spurious cues to help them make
shortcuts to make a prediction. So for example, a model can learn a spurious cue that if you have
a round object with the background of a grass, then it's a golf ball. And here you can see that
the model classifies this lemon as a golf ball because of the background of the grass. When you
force the model to actually focus on the foreground of the image and not just the background of the
image via a loss applied directly to the explainability maps, you can correct wrong predictions
based on spurious cues. So what we're doing usually is we're teaching the model to predict something,
right? Predict golf ball, car, glasses, etc. But we're not really teaching it why. Why this is the
object in the image. So what we're showing here is that by fine-tuning directly the relevant
slots or the explainability maps, we can correct wrong predictions based on spurious conditions.
But we'll get to it. I'll get to it in depth later on. So this was the motivation part and
hopefully you got fully motivated as to why transformer experiment is interesting.
Our talk is going to be a construed of two parts. The first part is going to be,
we're going to talk about how we do transform explainability. We're going to see the attention
mechanism which I'm sure you're all familiar with, but we're going to have emphasis on specific
parts of the attention mechanism that are going to be useful. Then we're going to ask ourselves,
is attention an explanation? Which is really the most prominent question when doing transformer
explainability. We're going to talk about three explainability algorithms. The first one is attention
roll-up, not by me, but it is the groundbreaking first algorithm that did transform explainability.
Then I'm going to present two of my works that have to do with transform explainability.
In the last part, we're going to talk about the work that I just presented and that
Sharon had a question on and probably hopefully we'll answer the questions and see how
transformer explainability can be used to devise models or maybe incorrect
spare excuse that the models were. Just to set us up, this is a transformer
architecture as presented in attention is all you need. I'm sure you're all familiar with it.
We will be focusing on the encoder since the attention mechanism here is a self-attention
mechanism and it's quite more intuitive and easy to grasp and understand. The concepts that apply
here to the encoder are pretty easily generalized to the encoder. We're going to focus on the encoder
for simplicity and for intuitive explanations, but the principles quite easily generalized
to the encoder as well. By the way, if you have questions, just feel free to stop me.
Okay, so let's talk about the intuition behind self-attention. What self-attention does is actually
create contextualized representations. I'm sorry for the people on Zoom, but I'm going to write
here on the right for it. So we have an example, we're running an example to work with. Say we have
the sentence, the count, set on the max. And let's consider the first day in the sentence.
We want to create now an embedding for each one of the words in the sentence for each one of the
token in the sentence, but the token does quite meaningless with our context. It could refer to
any word. So what the attention mechanism does is it actually creates contextualized representation.
It should take information from the other token and insert it
to the current token that we're interested in. So for example, intuitively, maybe we would
expect that since the word the refers to the word cap, information from the word cap will be
moved into the word that such that the embedding of the word that is contextualized is enriched by
context from the word cap. So what the attention mechanism does is it actually uses query,
key, and value matrices. And we can think about it's maybe as a database theory information.
So when we talk about this databases, we have queries,
which are questions that we run on our database. We have keys, the keys represent the entries in
our database. And we have values that corresponds to the keys, right? So what we're doing here is
we're actually running a query asking which tokens are relevant to the
and think about it intuitively as running a query on all of these tokens that we have.
And then the keys represent all the other tokens. What we do with the attention mechanism
is we calculate an attention matrix that is going to be the star of every transformer
experience ability algorithm. It's going to be a soft mass of the multiplication between queries
and keys normalized by the embedding dimension. This similarity scores are actually telling us
how much each word is relevant to our word of interest. So the multiplication between queries
and keys, we can think about it kind of like as relevant scores, how much is each token relevant
to the token that to the word. And after we calculate those similarity scores, we create the
enriched representation by multiplying the scores by the values such that each word gets
information from the other words by these relevance values. So these relevance value
determine how much each word is going to influence the word after the attention.
So this is going to be the key intuition to everything that we do later on to explain
a transformers. The most important thing to remember about explaining transformers is
we don't have just a single attention matrix. This mechanism happens H times where H is the
number of attention heads that we have. And intuitively, we can think about it as, you know,
in CNNs, you have kernels. Each kernel has its own purpose. Some refer to the background, some
refer to the edges, the shapes, and so on. Transformers have the same thing with attention heads.
So each attention head can have a different purpose. And actually, researchers have shown
that you can probably prune most of the attention head and achieve the same accuracy,
which means that most attention heads are really not important to the prediction,
to a specific prediction of the model. So it's really important when we think about
transformers in a way to understand that the different heads have different needs.
The final thing that we need to remember about transformer, you know, predictions is that
transformers use a classification token for the prediction. So once the entire attention
mechanism is done, and all the tokens are contextualized, the classification token is
the only token that is used to make the prediction. There's a linear layer on top of
the classification token, and then the prediction is made. So basically, what the classification
token does is kind of like creates an aggregated representation of the entire input. You can
think about it as a global representation of all the tokens in the input. You have questions so far
because we're going to move on to the interesting stuff. Yeah. So moving on to transformer
explainability, it's really important to set up our goals. My goal is to facilitate explanations
that help you guys, the researchers that actually use the models. And the way that we do that is
by creating hitmaps. So the hitmaps should correspond to the pixels, if we're speaking of images or
if we're speaking of text, and hitmaps should correspond to the tokens. The hitmaps should
correspond to the pixels that influence the prediction by the model. So for example, here
we see the verb and the hitmap actually highlights the pixels relating to the verb.
And the toothbrush or the ship or the bikes and so on. So the hitmaps should tell us
which pixels in the input make the prediction as it is. Okay, we got to the interesting part. Yeah.
When you talk about transformer explainability, researchers have looked at this attention matrix
and asked the question, is this attention matrix an explanation? How can it be an explanation? Because
we said that the attention values are actually kind of like relevance values. There are values
that reflect how much each token influences each other token. And we also said that the
classification token is the only token that is used for the prediction. So if we look at the
row in the attention matrix that corresponds to the classification token and look at these
relevance values, these should be the relevance values that determine how much each token influences
the classification token, which is basically how much each token influences the classification.
So maybe these values are just the relevance values. Each token represents a patch in the image.
Maybe these are just the values that we need. And we're all done, just like decision trees
are self-explanable or linear regression is self-explanable. What do you think? Are we done?
And if not, why? Anyone? Yeah. The attention matrix is used to multiply the
value representation. Yeah. The representation should be positive, negative, large, small.
It doesn't actually tell us how much it is actually contributing to the final classification.
Yeah. I mean, the two problems that we point out to are quite different.
The values can't be negative, but I don't think really, when you say, okay, let's refer to
this point for a minute. These values are actually directly determining how much information from
each token you're going to take. And then there's a softmax operation here. All the values are
non-negative, right? So there is a distribution that's defined on all these tokens of how much
each token will influence this token. So intuitively, these are really relevant values.
But we do have two other issues that we should refer to. The first one is we said we have a few
attention heads, right? Each attention head has its own meaning. Some attention heads are really
not relevant to the prediction. How do we aggregate across these attention heads in a way that takes
into account the meaning of each head? We wouldn't want to take into account heads that do not affect
the final prediction of the model. And there are such heads since there's research that show that
you can prune most of the heads without impacting the prediction of the model. So you have a few
attention heads and it isn't clear how you aggregate across these attention heads in a way
that takes into account the importance of each head. And the second question that we have is we
referred here to a single attention layer, but we have a few attention layers. So the first attention
layer may incorporate information into token one from token three. And then in the second layer,
token one isn't simply the patch that it represented in the beginning. It is this patch with information
from this patch. In the second layer, it's this patch with information from this patch and maybe
this patch and maybe this patch. And by the end of the attention mechanism, how do we know which
token refers to which input patch, right? They're all mixed up. That's the entire idea of the
attention by this. So we have two issues here. How do we aggregate across the attention heads
since we know that they have different needs? And how do we aggregate across attention layers?
Just so that I understand, if there's only one attention head and also there's only one attention
layer, then the relevance board is the attention. Yeah, by this hypothesis, yes. Yes. And then I
think there are some models that use this for visual question answering and actually did that
visualization and it worked pretty well. So assuming you have one attention head and one
attention layer, it should be fingers crossed. It should be their elements, right? I haven't
tried that, but yeah, by this intuition. So the attention role of mechanism is actually the first
method to explain transformers that came out in 2020. And they proposed the two simplest solutions
maybe that we can think of to solve those two issues. Head aggregation by averaging. And again,
remember, we said different meanings to different heads. So that's maybe oversimplistic
and aggregation by matrix multiplication. And if you think about it, matrix multiplication
from the end to the beginning kind of unravels the connections that were made by the attention
mechanism. They also propose another method called attention flow, which evaluates the flow values
in the attention graph, like a classic flow problem from algorithms, but it's too computationally
expensive for images. So we're not really going to get into it. So getting into the first method
we proposed, what we were saying is that the assumptions made by the attention role of mechanism
were solid, but maybe oversimplistic. Yeah, question. You may have some questions in the chat.
Yeah. We're saying oh, right? I may take those at the end of the talk just because otherwise
we won't be able to finish and start and keep going on Zoom. Yeah. So getting back to the first
algorithm proof process. We were saying that the assumptions made by attention roll-off were
nice and worked in some cases, but maybe a bit simplistic. We want to be able to average across
the heads in a way that actually takes into account the meaning of the chat. So what we're
going to do is we're going to use a signal that is very useful in explainability in general,
which is gradients, right? Because gradients intuitively mean if I change this a bit, how does
this change a bit, right? So if we take the gradients with regards to the output of the model,
which is over here, the gradients of the attention map, we can use the gradients as weights for the
attention maps. So instead of just averaging across the maps, we take the gradient. The gradient
gives us the weight element and we multiply the gradients by the attention and then each head gets
a weight from the gradient. And then each attention head is not just the simple attention
head that it was in the beginning, it is the attention weighted by the gradient. And then we
can average across the heads in a way that takes into account the meaning of each head. So this
is why the gradients are here. But we have another component that I won't get too deeply into because
it was removed for our second method. It is the LRP component, layer-wise relevance propagation.
The second thing we thought of was that we actually reduced the entire transformer architecture
to just a multiplication of queries and keys. So it is not even the entire attention mechanism
because we also had the values there, remember? So we narrowed down this entire, not so complex,
but architecture, right? It has activations, it has linear projections. We narrowed all down to
the multiplication between queries and keys. So we do want to take into account the other layers
of the transformer and how they impact the calculations. So instead of just taking,
let's get back to the whiteboard here, instead of just taking the attention map that is quite,
it's implicit, right? It's not that, but the multiplication between queries and keys,
we want to take into account a different attention map, we'll call it RA,
which takes into account the other layers of the transformer architecture.
So instead of taking just these raw relevance values, we take relevance values calculated by
LRP. And LRP is a mechanism that does back propagation with gradients from the end of the
network all the way to the beginning. And it can give us relevant values for specifically this
attention matrix. So instead of taking into account the attention values, the raw attention
values, we take into account the relevance values of the attention matrix. And as I said,
I won't get too deep into it because we actually removed it in our second method, which is the
one that I want to get into in more details. So we have the attention gradients to average
across the heads. And we have the relevance in order to account for all the other layers of
the transformer. So this is how we average across the heads. And the way that we average
across the layers is by matrix multiplication. Here we adopted the interpretation from attention
robot. Oh, no, not element wise, actual matrix multiplication. The matrices are square matrices,
yeah, because they are self-attention matrices, so you can actually multiply them. And if you
think about it, you can unravel it when multiplying two attention matrices, it actually says,
if the previous layer gave token one information from token three, and this layer gives token one
information from token four, then it unravels both operations to ensure that you actually
take into account all the context. Yeah, so this is just a rewind of what we saw in the previous
slide. How do we average across heads? We take the gradients as weights, we take the relevance
instead of the pure attention weights, and then we do averaging. But here the average is not just
the raw average of what we had before, it is weighted by the gradients. And here you can see
a few examples of how our method works. So by the way, this is a slide that was added,
but we don't have the updated slide. So let's just see what we get in the end of this calculation.
So at the end of this calculation, we had an attention matrix, which is the attention matrix
after by the averaging and everything. We have attention matrices for all the layers,
and then we multiply these.
So really, we have one attention matrix that takes into account all the layers
and all the heads. And now we can get back to, can we go back in the slides? Oh no,
it's only going forward. Yeah, can you turn the arrow at the bottom left corner if you move your
mouse. Oh, the back arrow. Oh, it's the other way around. Yeah.
And now we're actually getting to the point that Sharon made that right now we only,
after all the aggregations that we made, we have one aggregated attention matrix for the entire
network, because we aggregate it across heads and then we aggregate it across layers. And once we
have that one attention matrix for the entire network, then we can use that intuition that we
had that the row corresponding to the classification is actually the explanation. So this is how we
extract the final explanation. These are actually the relevance values that we use.
And the other way around, right? Okay, yeah. So as you can see here, we have comparisons
between our method and other methods that are either adapted from CNNs or methods that were
constructed for transformers such as rollout. So as you can see here, rollout tends to have a lot
of noise in the background. And we can think about it intuitively as resulting from the fact that they
just average across the heads and not take into account the meaning of each head. And some methods
such as partial LRP fail on some cases, but in these specific cases, they actually do pretty well.
But I do want to point out that they do not distinguish between classes. So for example,
if we have an elephant and a zebra in an image, our method is able to produce explanations
specifically for the elephant or specifically for the zebra. When we don't do that, and we want to
use this method, say partial LRP, to explain predictions by the model, it would be hard to do
that. Because if you want to explain the elephant prediction, we may have results coming in from
other classes. So we're not really sure if the things that we're seeing highlighted are highlighted
because of the elephant or because of other classes making their way into the explanation.
So I think, personally, class specific explanations are really important to ensure that we're really
explaining the specific prediction of the model. That's a fantastic question.
Usually, people from explainability evaluate explanations differently than what you as
end users may have to be. You may want to produce relevancy maps that are segmentation maps,
but our final goal is to explain what the model does, how the model works. So what we do usually
is we use erasure based methods. So what we do is we take the pixels that are said to be important
to bi-plastic, we take them out, and we see if the model changes its prediction or not.
And similarly, we do the other way around. We take the pixels that are unimportant
to bi-plastic and take them out and see that the model still predicts the same.
You have to take into account when you do that, that you create images that are out of the
distribution that the model goes trained on. So this method is not really airtight,
and there's a lot of research around how do we evaluate explanations and how do we know if the
explanation is really good or not. Any other questions? Yeah.
Just because we want to have a measuring stick that actually measures the explainability
without relation to the algorithm itself. So the measure should be unrelated to whether it's
transformer or CNN. It should be unified throughout all the different architectures, right? Just as
you use accuracy to measure CNNs or transformers or whatever architecture you use, you want to have
a measuring stick that really measures the method and not something that has something to do with
specifically VIT or CNN. Why not? If you have an explanation for CNN, it also has
values for each pixel. Yeah, just zero it out. You just zero it out. It works on the input itself.
It really is independent even of the method you use or the model you use. It is a measuring stick
that has nothing to do with which method you use for explanation and which model you use for the
extension. So as you were giving an example, the experience is like you've added mysterious
pronation, and you've added everything that's in Vibra into the sun. Yeah. That's very
disruptive than it would be. What are your explanations and then say we're in the sun?
I'm not sure I got your question exactly, but I would say that there are methods evaluating
explanations by adding sparse correlation, making sure that the model reaches 100% accuracy
due to these sparse correlations, and then making sure that the explanation outputs these sparse
correlations versus the odd correlation. So there are methods that do that. But yeah,
but I usually use erasure these methods to evaluate explanations. But this is a really
active tool of research. So it's not really obvious how we evaluate explanations and what's the right
way to do that. I think I'm maybe moving backwards instead of forwards. Some technical issues.
Yeah, okay, I may just skip this because we do have the motivation that we did in the beginning
and we're a bit behind on time. So our second method said, you know what, we really believe that
multimodal models are going to be a big thing. And we only explained self-attention before,
as you saw. We didn't go into cross-attention or important attention. And assuming that
most transformers don't just do right self-attention, we need a mechanism that can explain
cross-attention and important attention as well, not just self-attention. So the second paper actually
expands the first paper, but for other types of attention.
So the first thing we do is get rid of the LLP. And that's why I don't, you know, get into a lot
of detail with regards to the LLP. The reason that we did that is because if you think about it,
we used LLP in order to account for all the layers, but really gradients account for all
the layers because back propagation is back propagation from the output all the way back to
the LLP. So we said, what happens if we remove LLP, which makes it easier for you guys to implement
the algorithm, and it makes it faster and more convenient, and it actually works pretty well.
So we remove the LLP component. I will say that if you want really accurate explanations, usually I
would vote for the LLP version, right, because LLP adds this added component that doesn't exist
without LLP. It does account for all the layers quite systematically. So when we talk about
cross-model interactions, we have four types of interactions in such models. We have the
self-attention interactions between the text tokens, how the text tokens influence themselves,
the self-attention interactions between the image tokens, and then two types of cross-attention
interactions, how text influences image and how image influences.
And then what we thought we would do is really track the self-attention layers. So each self-attention
layer mixes tokens, okay, we'll mix the tokens in the relevance matrices. So we start with an
initialization of the relevance matrices at the beginning of the modalities or self-contained.
So images only affect images, and text only affects text, and each image token only influences
itself. So the initialization for the self-attention relations are just the identity matrices.
And for the cross-model relations, it's a zero matrix because there are no cross-model interactions
before we do any attention. And what the method really does is it just goes on a forward pass
through the attention layers, and as the attention layers mix the tokens, the relevance values are
mixed as well, just tracking the attention as it goes. So I won't get into all the rules,
all the rules that we have for all these specific attention layers, I'm just giving you a motivation
of how it works and really, believe me, it's really simple, even though the equations look
complicated. So let's go over just the self-attention rule. A self-attention layer has, again, multiple
heads. We average across the heads using gradients just as before. So we have now a single attention
matrix marked here as a bar. And what we do again is just matrix multiplication between the current
attention mixture and the old attention mixture that existed in the relevance matrix. So matrix
multiplication and update the relevance matrix. This is all we do. We just track the attention
as it goes. As it mixes between tokens, we mix between the relevance values. That's what we do.
That's the entire algorithm. And head aggregation is done via gradients as before.
So taking a look at some examples that we have to demonstrate how this works. For example, for
CLIP, you can see that we've entered different texts with the same input image and propagated
gradients. And by the way, for CLIP, gradients are propagated. Let's take a look back at the white
board. For CLIP, because I know this is specifically interesting to you. Let's talk about how we
propagate relevance for CLIP. For CLIP, you have an imaging quarter and then a texting quarter.
Both of them, by the way, use pure self-attention. So there's no cross-attention.
This and this output are a representation, a vector, which is, by the way, from the classification
to the output. So this is the vector for the text and this is the vector for the image. And
the similarity score is just a dot product of both scores. So what we do is we propagate
gradients from this dot product back to the texting quarter and back to the imaging quarter.
And those gradients are going to be used to average across the attention pens as we saw before.
And then the attention pens are going to be aggregated across different letters by
matrix multiplication. So here we don't have an output logic as we have for the classification,
but we use this dot product between the representations to calculate the score that we
propagate the gradients with regards to. So all that we do here is really simple. Calculate the
dot product between the representations, propagate gradients with regards to the dot product.
Those gradients are going to be used as weights for the attention matrices to average across them.
And as you can see, the results are text-specific since we propagated the gradients with regards
to the specific multiplication between the specific text and the specific image.
So actually for an elephant, you can see that the feedback corresponds to the elephant,
for a zebra, the feedback corresponds to the zebra, and for a leg, the feedback corresponds
to the leg, showing us that the model really knows how to distinguish between different parts
or different objects in the image according to the text input that we give it.
This is an example that we saw before. And visual question answering in case any one
of you is interested is actually an interesting use case. Because for visual question answering,
the model is given an image and a question, and it's supposed to answer the question based on
the image. And researchers have shown that when you actually black out the entire image and just
give the model the question, it answers the question about 30% of the time, yeah, correctly.
So the question here is assuming that the model answers the question without seeing the image.
How do we measure the accuracy of such models? So you can use the explainability to ensure that
the model actually used the image and the correct parts of the image to make the prediction.
For example here, the question is, did he catch a ball? We see that the player actually caught the
ball. And the answer is yes, but we also see that the model focused on the right parts
of the image. So we can really tell that the model made the prediction based on the image and now
just the question. I'm going to skip this part too. Yay. So we're switching gears. We're going to
talk about our method to improve model robustness using explainability. So if you have any questions
about the previous part on explaining transformers, this is the time to ask them.
No, no questions. I had a couple of questions in the chat. Yeah. I'm sorry about that.
There's no one really maintaining the chat. Yeah, let's make it brief and then try to answer
questions. Yeah. Oh, okay. I was just wondering, why does it make sense to only look at the
attention maps outputted by the softmax? Because don't we have, don't we multiply by an output
matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,
the output matrix. I guess that the intuition is just that the self attention mechanism,
its purpose is to contextualize and the way that the contextualization is made is by the
attention values. So the attention values actually determine how much each token is going to be
incorporated into the other tokens. We do have an additional output matrix and you mean after the
attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually used that output,
if I'm not mistaken, it was that output, the norm of the output matrix in order to average across
the different heads to account for each head's meaning in the attention matrix, in the attention
mechanism. But, you know, just, you know, very naively thinking the attention
really mixes the tokens using the values determined by the attention matrix. So it's really a naive
intuitive outlook on the attention mechanism. And the output matrix that you're referring to
is I view it as a weights matrix, which will weight each layer since not all layers influence
the prediction the same, right? We know that usually the last attention layer is the most
influential, the previous attention layers are not that impactful. So I view it as the output
matrix kind of reweighting the result from the attention mechanism. But all that we're saying
right now are just intuitions, right? We've seen empirically that the attention matrix is quite
indicative of what the model learns to do, how it learns to contextualize parts of the input.
It's not necessarily the best thing to do, the smartest thing to do or the most correct thing
to do. It's just what empirically worked well. And it has an intuition basis as I explained
before. I hope that answers your question. It does. I had one other question if there's time.
We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.
Sure. Thank you. Sorry for that. I really apologize.
Okay. So when we talk about VIT models, the image is split into patches. The patches go
through linear projections. And then a transformer encoder, a vanilla transformer encoder is used
to make the prediction again with the classification. So really a simple and clean architecture.
And usually those models are trained using ImageNet. ImageNet is a classification data set.
And what those classification data sets do is actually they train the model
to make a prediction. So they train the model to see an image and make the prediction that
this is a car. But it doesn't do anything beyond that, right? The model should predict that it's
a car, but it doesn't have to have an understanding of what a car constructs and how a car looks.
It should just see this image of a car and output car. We don't enforce anything too
smart that the model should learn. So what researchers have noticed a long time ago
is that ImageNet contains sparse predictions. What it means is that, for example,
cows usually appear on the background of green grass. So a reasonable inference that the model
can make, really reasonable, right? Because this is the statistics of the data in the data set that
it gets is to learn that green grass is actually a cow. And now we learn to predict that this
image is an image of a cow based on the green grass, not really the object in the image.
What it causes is, oh, can you mute this? Thank you. So what it causes is cases where
the distribution is likely shifted from ImageNet. And in cases where we would actually expect the
model to really work well on, the model really doesn't. And the accuracy plummets. We're talking
about 90% to 30% sometimes, and even less. So really cases where we would expect the model to
still learn to make a smart and great prediction, but it really does. It predicts based on the
sparse correlations that it learned from ImageNet and they don't apply to other distributions. So
for example, we have the golf ball and the lemon here, and we have another orange that is classified
as a maze due to the carpet in the background, right? Because it kind of looks like a maze.
And a school bus here that is classified as a snowplow because of the presence of snow. So we
can imagine that the model learns some kind of sparse correlation here, such as vehicle plus
snow equals snowplow. So we want to solve these issues, but without training the
models with a stronger queue, it is really hard to do that because we just teach the model based
on some data set that we have, which is ImageNet. And it is the most used data set to predict,
to train object detection, object recognition. And we have no way of really controlling what the
model works. And intuitively, training the explainability signal is really teaching the model
not just what is in the image, but why this is the object in the image. So we would want to apply
a last term directly to the explanations of the model to teach it why this prediction is correct.
So here you see some sparse correlations that the model uses. So for example, here the model
classified the image as a chestnut with a confidence of 100% based on just the background
itself, not even one for it. And here a very sparse consideration of the zebra
gives us a confidence of 99.9% that this is a zebra. So really behavior that we would really
want to discourage. Since the second method that we saw is based on pure gradients,
everything there is derivable. The gradients can be derived again, and the last term can be applied
directly to the explainability. And we can force the model to make the prediction based on the
program instead of the background image pixels. The issue that we had after that is,
you know, we're researchers at the university, right? We don't have the resources to train VIT
large or huge from scratch. So we need to come up with a method that is efficient in time and
space and not too complicated. So what we opted to do is fine tune an existing model. So we would
fine tune the model. It works pretty well on ImageNet, right? We don't want to change the
prediction that it gives on ImageNet. We just want to change the reasoning that it gives through
the prediction. So we fine tune the models with only three examples per class, really not that
many examples for just 500 classes. So just half the classes in ImageNet to change the
relevance maps to focus on the foreground instead of the background.
So we identified two same issues with VIT models. The first one is an over interpretation of the
background, which we saw on your finds. And the second one is a sparse consideration of the program.
The first idea was to fine tune the explanation maps to just be segmentation maps, like this.
This is actually an example of me fine tuning a VIT based model to make the relevance maps
resemble or be identical to segmentation maps. So as you can see before the explanations weren't
really segmentation maps and after they're quite well segmented in the image. So can anyone guess
why that's not an optimal solution to the problem that we have just creating segmentation maps?
People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are
actually segmentation maps? Let's have a thought experiment, okay? I'm going to draw with my
magnificent drawing skills an object and you're going to try to identify which animal this is,
right? Again, I'm not the best draw, but which animal is this? Which snake? Snail. Oh no, this is
a snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right? And humans,
we don't classify cobra as a cobra because of its tail, right? We look at the head pixels or the
head featured and determine that this is a cobra. So we don't, as humans, give identical relevance
to all the pixels in the image. What we do here when we fine tune the explanation maps to be
segmentation maps, we force the model to look equally at all the pixels of the cobra. We do
want to give the model the opportunity to give some relevance to pixels that is higher than other
pixels. So this is too harsh and we need to have a refined version of it.
This is why we split the loss into two different losses. One is a background loss and one is
foreground loss. The background loss is a mean squared error loss, encouraging the relevance
on the background to be close to zero. And we're using segmentation maps here. S is the segmentation
map of the image. And the foreground loss encourages the foreground of the image to be closer to one.
By splitting into two loss terms, we can give different values or different coefficients
to each of the loss terms. So the background loss is going to get a relatively high coefficient too
because we don't want a lot of relevance on the background. By the way, we're not striving to
completely eliminate the background, the relevance on the foreground. Just make sure that the
relevance of the background is lower than the relevance on the foreground. And the foreground
loss is going to get a relatively low coefficient. We would want to encourage the model to look more
at more pixels of the program, but we wouldn't want to make the model look at all the pixels in the
foreground equally. We do also have a classification loss, which ensures that the new prediction by
the model or the new distribution is similar to the all distribution by the model. Just to make
sure that the model doesn't forget how to classify images. And again, the model does a pretty good
job on ImageNet. So we don't want to change the prediction by the model. We just want to change
the reasoning. So the giant tables of results here are comparisons between the accuracy of the model
before and after our fight training process. And as you can see here, it's quite tiny, but I hope
you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in
performance. This is because the model relied on spurious cues and now we're taking them away from
them. And so the spurious cues that previously helped the model reach very, very high accuracy
and overfit are now taking away. But the decrease in accuracy on average across seven models is
not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,
such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SI scores, you can see that there
is a pretty big or significant increase in accuracy for ImageNet A, for example, plus 5.8%
in top one accuracy, plus 7.8% in top five accuracy. So really a slight decrease in the accuracy on the
data set that the model was originally trained on and a significant increase in accuracy for
distribution shifts, as we would have said. So to train it, you have to know the program,
like where is the program? Yeah, you have to know that. You have segmentation maps.
Okay. You have segmentation maps. And we do experiment with two types of segmentation maps.
One is manually human, manually tagged by humans. And the second one is by token cut,
which is a version that uses dyno. This is in case you're training with non-ImageNet
datasets and you don't want to manually tag, even if you do manually tag. I mean,
we use three examples for half the classes. So it's not that many examples to tag,
but we do provide for an option for ad-supervised segmentation. Yeah.
So I think that's really cool. But what I'm just thinking about is, why not just do segmentation?
Like, you can just train a segmentation system. Is that kind of nationally explainable because
it's just pungent? Oh, that's an excellent question. Do models that were trained on segmentation
have that, you know, brief pass on sparse correlation? Right. Right.
Do they get that inverted? Inherently. What we thought about, or I thought about in that
context, is you can think about a model that learns to classify using sparse correlation
and then identify the object using edge detection. So just because you learn to identify an object
does not mean or learn to segment an object. It does not mean that you learn to recognize
the object by the segmentation. And also we can think about when you want to train really big
models, you need a lot of data to do that. And segmentation data is quite expensive. You usually
don't have that amount of data as you do for classification, which is an easier task.
You have a lot of data just lying around there. So classification is usually the go-to task.
Yeah, but only just a few. It can be in valleys. Just 1500 segmentation maps,
either supervised or unsupervised. A very few amount of segmentation maps.
It's very impressive that your system can find such a small amount of data, but I
was wondering whether the pattern that we see here would sort of extrapolate into a large data
to find some models with more segmentation maps? We did experiment with using more
segmentation maps, and it showed that the accuracy kind of fluctuates at some point.
I mean, there's some point where it doesn't improve more if you add more segmentation maps,
but you do have to take into account two things. One, we did find two, and we didn't
train for scratch. Two, we didn't have the resources to hyper-prometer search for each
selection of the number of... So it's possible that if you use more data, you would need to
retune your hyper-prameters and then get better accuracy improvement, but we didn't have the
resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have
any finance for that. One thing we did to ensure that the model actually learns to predict better
or to have better explanations is we looked at the accuracy increase for the non-training
classes as well, because we said that we only use half the ImageNet classes. It is really
interesting to see if the model really improves on the non-training data as well. Does it learn to
generalize the positive influence or the positive logic? And as you can see here, this is the ImageNet
validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet
distributions, for the shift of distribution, you can see that the improvement for the non-training
classes is actually quite similar and sometimes even better than that of the training classes.
So the model really from this experiment learns to generalize that healthy say-and-behavior to
classes that were not in the training set. And here are some visual examples. These are
examples from the ImageNet data set. So examples from the original data set with the model which
straightened out. And here you can see that the same prediction is made for two different reasons.
Here, the background, here actually the foreground, the snow cloud. And here you can see corrective
predictions where the model originally predicted that this is a can opener based on the eye of
the puppet. And once we find you the model to look at the entire object or to look for,
you know, less sparsely as the object, it actually finds a teddy bear. And here you can see that
even if the model is now wrong and was previously correct, you can usually quite easily explain
why the model was wrong. So here's an example where the ground truth classification is tripod
and the model predicted actually finding a strawberry, but you can actually see that there
exists a strawberry in the image. So it kind of makes sense that the model made that mistake.
These are examples for shifted distributions. So as you can see before, for this example, the model
predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage area.
So we correct the prediction to be a forklift based on foreground rather than the background.
Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration
of just its spot. And after the fine tuning, it is correctly classified. And the third example is a
porcupine that was classified as a sea lion due to the background of the ocean. So once the model
really learns to look at the correct pixels, it does make the correct prediction. These are
additional examples, but really, we don't have time. And another interesting thing that we've
noticed that I think is quite cool, even when you take examples that are completely out of the
distribution. I mean, this is an image generated by Dalit. And the models not know the class robot
or oil painting and so on. Originally, it made a ludicrous prediction that this is a guillotine
based on I don't know what. You can't really understand. But after a fine tuning process,
you can see that the model does not make maybe the best prediction that you can think of,
which is the robot because it doesn't know that class. But it does predict the grand piano. And
it kind of makes sense because there is a piano in the image. So while the prediction, again,
still does not make the most sense, at least it is based on some of the objects inside the image
and not just something that you cannot make sense of. You choose first correlation.
So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table of
content is here in case you want to ask a question about specific parts of the lecture.
Thank you.
Yeah, one or two we can do. Yeah.
Yeah, it's visible. Thanks for rotating it.
Okay, the questions here are really lacking context because they were probably
asked during that. So if anyone wants to ask a question again.
Yeah, I guess one question I had. Have you thought about including layer norm at all
into your explanations? Because it seems that that does like scale tokens in some way.
And could that be relevant for your output? Include what? Sorry, can you repeat it?
Layer norm? Oh, no, but as I said, there is a method that I don't quite remember
the name of the method that did take into account the norms of the output matrix I think
in order to average across across the different attention heads, but we haven't considered that.
Yeah. We do consider that the gradients should be able to scale the different attention heads
according to their influence on the prediction.
Okay. Any other questions? Any other questions?
I had a question if no one's going. Oh, yeah, go ahead.
Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.
So I was wondering with regards to the stuff you said at the end where some of them you see it
and then you're like, okay, that was wrong, but like, makes sense. That's yeah.
Is there a way to quantify that? And oh, yeah, or related things? Or is it more like a you know
it when you see it? Oh, yeah, that's a great question. There is a work done by Google, I think,
that actually relaxes the the task of classifying objects using ImageNet. They actually
re-tagged ImageNet, where you know, a strawberry in that case wouldn't be a mistake, but maybe
it would be half a mistake or something like that. Yeah. So there's such a work that re-tags
the entire ImageNet dataset to account for mistakes that aren't really mistakes, but
actually makes sense. But other than that, I would say there isn't an automatic way to know that.
I mean, I can't think off the top of my head of an automatic way to know when the model is mistaken,
but it's okay. Cool. How did you guys check? Like, was it mainly the accuracy increase on the
distribution shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on
the distribution shifts. And yeah, yeah. And also looking at a lot of examples, right? Because
I started out as a grid of many. Yeah, yeah, yeah. And a lot of manual work
on actual utilizing examples that got me to the intuitions that I'm presenting now,
because I actually thought in the beginning that having the relevance be a segmentation
map is quite logical. Yeah, so it took some time to get through all the conclusions. Yeah.
Yeah, I was just having one idea coming out. It's like, is that possible? So you,
your own attribute is like, so that in a strawberry case, you can pop out that region and then maybe
run through another like Oracle network to tell you whether it is a strawberry or not,
and that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an interesting
take. It's interesting, particularly because I saw that different models tend to learn
different spurious correlations. So it actually makes sense to check models using other models.
Yeah, and they're consistently making the same prediction with these vets.
Yeah, yeah, perhaps. Yeah, yeah, that's an interesting idea. Yeah.
Your current relevancy extractor approach is limited by the VIT's tile resolution. It outputs the
attention map that is assigned to the vites of the tiles, and then you can
upscale it. Yeah. I was wondering whether there's a way to bypass this tile resolution just by
considering that we also have pixels coming into the tile. Yeah, yeah, we have tried to
propagate relevance all the way back to the actual input and not on the level of each patch.
It didn't come out just quite as we hoped. I think that the issue there is probably the
positional encoding in the way. Somehow that layer of positional encoding
ruins or destroys the relevance values once you propagate back from it.
I couldn't figure out how to get past that layer that actually kind of
added noise to the output relevance maps. That's an interesting point, but yeah.
I guess I knew that you tested those architectures that don't have positional encodings, which
I guess you're limited to using the way you're taking multiple images each.
Yeah, yeah, I haven't come across any such architectures if you do let me know and I can
give you the try. No, there was a question here, right? I was just wondering, when you're about to
work for, what if I wanted to explain about a color or something, or not on a spatial,
just go to something about the spectrum, right? Like maybe the lemon versus the orange,
you can't color the main thing. I was just curious about that.
This specific method would not be able to do that, but I know that there are
explainability methods that kind of create a decision tree from the model.
So you pay the price that the accuracy decreases to some extent, and then you create
a decision tree based on the decisions of the model. You kind of model the model using a decision
tree, and then you may have a split that it has to do with, you know, you pass a lot of images
through a lot of images of oranges and lemons, and you see that one of the splits is by the color.
Yeah, and then you know that. And probably you can do, you know, we can do some trivial things to
test specific theories, like turn the image into black and white and see what happens.
To consider if the model takes into account. But this method will not be able to do that.
All right, let's hang this speaker.

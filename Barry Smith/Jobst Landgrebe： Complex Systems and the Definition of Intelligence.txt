Hello, everyone, and thanks for participating.
My name is Jobs Danke,
but I'm by training a physician mathematician,
but I also studied philosophy.
I've come back to do philosophical research work as well.
So today I'm going to talk about
artificial intelligence intelligent pseudo definitions.
But before doing this,
I need to introduce our view of complex systems
because we will need this later on in the talk.
So I've been starting with two slides about complex systems
and then we'll move to the definitions of intelligence.
So what is a complex system?
So maybe why do we need to understand a complex system
because the animal and the human mind body continue on,
which produce intelligence are complex systems.
So intelligence, even primal intelligence,
is the intelligent of a bird or a mammalian animal,
a non-human, has primal intelligence
that gets produced by a complex system.
So what is a complex system?
So let's start with the Newtonian system.
So Newtonian systems are systems in which
one can apply models of physics.
And they have become extended quite a bit
in the 19th century with thermodynamics
and statistical mechanics.
And then another bit by quantum theory
and also general theory of relativity,
but they still remain Newtonian systems.
So what is a Newtonian system?
But by the way, can you hear me all right?
Does it work?
Okay, I assume yes.
So Newtonian systems are made up
by a small set of element types.
For example, the solar system is made up
by the sun and the planets,
which are not many elements.
The way the elements interact,
they interact by the four basic forces
or interaction types that are known in physics.
And in this case of the solar system, it's gravitation.
And actually only gravitation,
all the other forces don't matter for the solar system,
at least not with regard to the way
the planets move around the sun.
And they interact in a uniform and isotropic way.
So the force that is interacting here,
the gravitation has its effect
in a symmetric way all around the sun.
And it is the same everywhere.
I mean, it gets weaker and weaker,
but in a law-like fashion.
Also, there's no force overlay.
So there are other forces like electromagnetic forces.
There's, for example, light that comes out of the sun,
but it doesn't interact significantly with gravitation.
So if I model the way the planet moves around the sun,
I don't need to take into account
other forces than gravitation.
The phase space in which the elements
are placed or occur is deterministic.
And ergodic.
So ergodicity is shown in a small inset here.
So an ergodic phase space means
that all accessible microspace states of the space
are actually probable over a long time.
So basically, in simple words,
I can get to everywhere in this space
with the same probability if I wait long enough.
For example, if I have gas in a bottle,
the molecules of the gas will distribute
equi-probably over the volume of the bottle.
And so it has to be,
so such a phase space is ergodic,
whereas there are also non-ergodic spaces
to which we will get back in a minute.
Such Newtonian systems are non-driven.
Drivenness means that there are,
there is no force flowing through the system.
So for example,
well, a steam engine has a driven aspect
because all the time while it's driving or under energy,
all the time energy is entering into the steam engine
by the burning of the coals
and then it's been dissipated.
And in Newtonian systems, that's not the case.
Such systems have no evolution properties.
So they don't obtain new element types.
They have the element types they have.
So yes, this solar system could get a new planet
because there could be an asteroid,
a big asteroid could approach the sun
and could be attracted by the sun
and then start to orbit around the sun
the way the planets do.
But that wouldn't be a new element type,
it would just be a new element.
And they have fixed boundary conditions.
That is, if the solar system,
the solar system is four light years away
from the next solar system, which is Alpha Centauri.
Now, if it would just be displaced by one or two light years
or even three light years, it wouldn't change anything.
So basically I can take the solar system out of a context
and move it away and it wouldn't change anything.
Of course, if I would move it very, very close
to Alpha Centauri, then the sun of Alpha Centauri
and our sun would start to interact by gravitation
and then very terrible events could happen.
But basically Ceteris paribus,
I can just take such a new towing system out of its context.
Now complex systems are completely different.
They depend on multiple arbitrary element types.
They have different interaction types between elements.
They have force overlay.
So that means that several force act at the same time
and also interact.
The phase spaces that they have cannot be predicted
from their system elements and they are non-ergodic.
So they behave in a way that the microstates are not
accessed over a long time with the same probability.
They're also driven.
So they have inner or external drive.
External drive is, for example,
the steam engine that gets heated from coal.
Internal drive is what humans or bacteria have.
This is the drive to reproduce and also to survive.
And drivenness means that there's a flow of energy flowing
through the system all the time
and that this energy is dissipating.
And they lack an equilibrium state
to which they would constantly be converging.
So a driven system doesn't come to equilibrium.
It's always, it goes on.
But when the system, when an organism dies,
then it stops being driven
and then it also converges towards an equilibrium state,
which is, in this case, entropy.
Also, complex systems have evolutionary properties
so they can evolve new element types
and they have non-fixable boundary conditions
so they are context dependent.
You can read this comparison of complex
and classical neutralling systems in Turner
at our very good book about complex systems.
So let's look at some examples.
We have those seven properties of complex systems.
And the solar system has none of these properties
because it's not a complex system.
The steam engine has one property, it is driven.
However, to reason about the steam engine,
in many ways, you can abstract from its drivenness.
So for example, the velocity of the steam engine,
if it's used to drive a train,
is proportional to the pressure that it builds up
and so on in this.
So this property, if it's the only driven property,
you can abstract from it for many predictions
you want to make about the behavior.
Pryon is a protein that can infect the brain
and cause damage in the brain.
You have heard of Jacob Kreuzfeld disease
and you may also heard of bovine spongiform
and encephalopathy, which is also a prior disease.
And it has only two complex system properties,
namely force overlay and a non-negotic phase space.
Well, and it has also a non-fixed boundary conditions,
but it lacks all the others.
But then as soon as I get to the virus,
I almost have all the properties.
Virus is although not driven
because it cannot synthesize energy.
And then with the most primitive organism,
I have all the properties of a complex system.
So it is important to realize that most systems in nature
are complex or deterministically chaotic.
And so basically there's only a very little Newtonian systems
out there and most of the Newtonian systems
that we master are technically devices
that we have designed also.
All of them are Newtonian,
all of them are built using equations
that we have designed ourselves.
And that's what we really understand and master,
but nature is chaotic and complex
and humans react irrationally to it often.
So you can, if you think of how, for example,
we are now reacting to this virus.
What it's causing is complex,
but the reaction is irrational.
And that's because we feel that we cannot control it.
On the next page, you can see,
so I will give you the opportunity
to ask questions after this slide.
So on the next slide,
we see now the problem that complex systems
pose to machine learning algorithm.
So basically machine learning algorithms
cannot model complex systems
because such algorithms are large auto-parameterized
differential equations or partially auto-parameterized.
And let's look at the problems of machine learning model.
So on the left-hand side,
you see the problems that everybody know.
So that they optimize problem specific loss functions
that don't generalize well,
that they narrowly depend on the selected training samples
and the specific annotations of these samples,
that they fail upon heterogeneous annotation
of identical input.
So as identical input by different output,
it will be very stressful, so to speak.
I mean, it will not train well.
They fail on sparsely populated sample space parts,
which is very often a very big problem.
And that explains why very often machine learning algorithms
need so many samples.
Because if you look at the Curse of Dimensionality,
which you can, for example,
read in Trevor Hayst's wonderful book
about statistical learning,
you will see in chapter one or chapter two,
where he explains the Curse of Dimensionality
that very quickly,
you come from a topological perspective
to a very sparsely populated sample space areas.
And in such areas, the machine doesn't learn anything.
And that's very interesting because humans
and also animals are very good
at applying their intelligence
to sparsely populated sample spaces.
Basically, that's the key of intelligence.
Intelligence means the ability to react to new situations.
And machine learning models completely fail in new situations,
which are such sparsely populated sample spaces.
They cannot be guaranteed to move to corrected outcomes.
So if you have an error,
erroneous behavior of such a system,
and you try to correct the system,
once you've noted the error,
it's very hard to guarantee
that it moves to the corrected outcome.
And while it's doing this,
it may start to make new mistakes.
Because remember that the model itself
is nothing but a hyperplane in a k-dimensional space.
And this hyperplane, of course,
I mean, if you change its shape
to get a certain effect,
you may get other effects you don't want to.
Such models cannot perceive their own failure, of course.
So they cannot really raise exceptions.
Well, they can raise an exception
if the data don't match the input type,
but not much more.
They cannot model far-reaching relationships.
They cannot model semantics of mental type,
objectification semantics.
Here I'm using the German word, I apologize.
So it's object type, mental types they cannot use.
That's why they fail at image or language interpretation.
They fail completely.
And they, of course,
have not a sufficient exactness
for really critical settings.
That's the general problems that have been known
for machine learning,
even before neural networks were invented already in the,
I mean, before the deep neural networks were invented.
Even in the 1970s,
when there was just logistic regression and perceptron approach,
it was already clear to every statistician
that these are the problems of such models.
On the other hand,
there are also problems that are less well known,
that are described in our book.
There will be no singularity that we expect to publish this year.
It's currently with the publisher for review,
which are related to the fact that human behavior
is emanating from a complex system, the human.
So if you look at what the problem is here,
first of all, machine learning models require ergodicity
because the samples must be assumed to be drawn
from a representative distribution.
So if you want to train a complex system,
you need to have a representative distribution
because next time you draw a sample,
if it doesn't come from the distribution
which you used to train, the model will fail
because of reason number four.
However, reason number one on the right-hand side
gives you a sense that complex systems
create non-ergodic distributions.
And so when you draw from,
when you make a photo, so to speak,
from a non-complex, from complex system,
behavior-derived situation,
this photograph, apparent in quote marks,
is basically always cute.
That's the most important thing you have to learn today.
So when you record human behavior,
this behavior is never representative.
Because, so in other words,
there is no multivariate distribution to draw from
because each situation is new.
And the samples that you can draw are always outdated.
So what you do is you train with this cute snap
when you sample from a complex system.
So the sample space is always sparse.
I priori, that's really bad for complex systems
and that's bad for ML algorithms.
They cannot deal with this, they need repetitive pattern.
Of course, then this is really the take-home message
number one, that if you have a situation
or an environment created by a complex system,
then this is always a non-ergodic distribution
deriving from a non-ergodic process.
And such a distribution can never be represented.
So also they don't have evolutionary property,
they cannot model evolutionary properties
because they model a fixed input-output relationship,
but when you have evolutionary properties,
input-output relationships change all the time.
Because they are differentiable models,
they cannot model non-continuous operators or functionals.
And also they can of course not model non-isotropic forces.
They cannot model element-specific interaction types
and also force overlay or interaction
is very hard to model.
So for them, and also they cannot model
drivenness, which is energy dissipation.
To give you a very simple example why this doesn't work,
we haven't found any way to model the flow of water
into, for example, a water reservoir, so turbulence.
Turbulence is one of the most simplest natural phenomena
of drivenness because the energy from the water flow
dissipates in the reservoir into which the water is flowing,
but we cannot model it.
And so we cannot model how the energy that is the kinetic energy
of the water distributes into the reservoir.
And also complex systems cannot model context-free machine
learning models, cannot model context-tuality
because complex systems are always contextual,
but machine learning models are always context-free.
And so you have a discrepancy between the context-freeness
of the machine learning model and the contextuality
of the complex system.
So this is in very, I don't know how long I spoke,
maybe 15 minutes, in very short words.
That's what complex systems, the problem
with complex systems modeling in machine learning.
Hello, are you still there?
I'm still here.
So now the students are required to challenge you.
I have a question.
Yes.
So the issues raised here were the problems of modeling
complex systems in the context of neural networks
and deep learning.
Machine learning.
Machine learning, all right.
But the thing is humans also haven't been able
to model turbulence.
And humans haven't been able to, so to an extent,
we've been able to model weather forecast systems.
So we use certain sets of differential equations there.
But if ML systems can, to a certain extent,
as was mentioned in the slide,
auto-parameterized differential equations,
model some extent of that, then why should it be considered
a sign of no intelligence, even though humans have also
not been able to do the same thing?
So why this requirement from machine learning?
So I'm not saying that it is a sign of intelligence.
So now we are not talking about intelligence right now.
We are only talking about what can be modeled with machine.
And what I have not said is I've skipped a little bit of content.
So what I've not said is that all the phenomena that are
complex in nature can't be modeled using mathematics.
So of course we cannot model complex systems with mathematics.
And so, but that's not because we are not intelligent,
but because a complex system modeling seems to be beyond
the hardware that human intelligence possesses.
So, but that doesn't mean that humans are not intelligent.
It just means that we can't model.
There's no way to model complex systems,
neither with paper and pen mathematics,
nor with whatever kind of algorithm.
And so there, but that doesn't mean that we are not intelligent.
It just means that this is beyond our modeling capability.
But this taken aside, when it comes to creating ML systems,
when it comes to creating ML models,
and we want to model our, or let's say animal intelligence,
then we would have to model the output of a complex system,
which not even humans can, let alone machines.
Okay.
All right.
But at least humans have been able to approximate certain
complex systems like weather forecast systems.
So, yes.
So, go ahead.
And then would be, what do you think of the potential
for machine learning systems to approximate,
rather than create an exact model?
So, a very good question.
So, weather forecast models are approximate
machine learning models, right?
So, many of them are built using machine learning.
So, of course, it's possible to approximate certain
complex phenomena using machine learning,
but it's quite limited what you can achieve.
Yeah, you can achieve some.
The question is always, how good is the approximation
and what can you technically do with it?
And so, if you look at the national sphere we have,
all the technical gadgets that surround us,
that make our life so much easier and better,
most of them are exact.
So, mobile phones are exact, bridges are exact,
trains are exact, airplanes are exact, cars are exact.
So, we don't have so many approximative models
in the national sphere.
So, one thing, so Google, Google applications
are basically approximative, but why can Google
afford it?
Because nobody gets killed if I get shown a female
lipstick ad, right?
So, nobody gets killed if I get shown a lipstick ad,
so they can afford to do it, but if they would use
these algorithms in intensive care,
unit machinery, they would kill people.
So, that's the point.
Approximative modeling is fine, but you have to decide
in practice for what purposes you can use it
and where you can't use it.
Perfect. Thank you.
Very good question. Thank you very much.
Any more questions?
So, I guess I'll try.
So, if you take a...
We use this example already a couple of times.
If you take a laptop and throw it into a river,
then the laptop no longer behaves
in such a way that it is a simple system.
It starts to decay,
and that's because the laptop plus the river
is a complex system.
Is that a correct account?
Well, I mean, the system as a whole,
so when the laptop drops into the river
or creek or whatever,
the creek has mechanical energy in the water,
and this mechanical energy
will start to attack the structure of the laptop.
And then, of course, chemical processes
that are not in equilibrium
because the water is driven,
will also attack the laptop.
So, the laptop, in a way,
will become part of a complex system.
But on its own, it will not be a complex system.
So, if you take it out and those forces
stop working on it,
then it will stay more or less
in a certain form of decay,
although in the long term,
even if you leave a laptop standing somewhere
without water,
it will also be part of a complex system
because there will also be energy working on it,
but much less so.
So, if it's in a building,
to begin with, it will take one or 200 years
before the building is broken,
and then energy can really work on the laptop.
Can you go back to slide two?
Yes.
So, you give C. elegans as an example of a complex system.
I assume you would give a human being
as an example of a complex system.
Yeah, of course.
But can you tell me how a human being
has evolutionary properties?
Yes.
So, the evolutionary properties of living organisms
constitute,
they come from the fact that
new types of macromolecules
can be created in adult organisms.
So, if you have an adult organism
by its ability to react to the environment,
it can create new types of macromolecules
that it hasn't had before,
like new memories
and all new combinations of elements.
And that's all.
It can actually change the way it methylates its DNA
and therefore change the way it will,
the inheritance will work
if it becomes progenitor of new organisms.
So, this is the way that
new elements and element interactions
can arise in adult organisms.
Good.
Okay, any more questions from anybody?
Yes, I will have one.
I believe it's a very boring,
usual philosophical question,
but when we say that complex systems are not...
We don't have a mathematical modeling code for them.
Is this correct?
Yeah.
Is this intended to be one of our epistemic lacks
or something which happens in nature
unregarded of some very, very complex mathematical model
which, at the moment, we don't have, but...
This is a very good question.
So, we don't know this,
so we can't really give an answer to this question.
It could be speculative.
However, my view is that
our ability for mathematical modeling
is an evolutionary adaptation of humans.
So, very much like language is all the way
our hands work, our evolutionary adaptations.
And it is a special evolutionary adaptation of our mind,
the extent of which also strongly varies between individuals.
So, out of a thousand,
only one or two individuals are usually mathematically gifted.
And so, it has a high variance,
but everybody with an IQ above 80 can count.
And this skill is limited.
And I think it's limited by the forces
that shaped it during evolution.
And so, I think that certain aspects of nature
are just too much, so to speak,
for the structure of this skill.
And this seems very plausible,
given the history of mathematics so far.
However, because in the end,
all the mathematical objects that we know
can be reduced to numbers.
And so, and are very complicated combinations of them.
So, even for example,
if the invention of calculus by Newton and Leibniz
seems like a very big step,
the way they did it was quite geometrical.
And great inventions are always great,
but it's still linked, of course,
to the history of mathematics.
So, I don't think that we will ever be able
to model fully model complex systems,
but we were able to, to some extent,
cause approximately model aspects of them.
So, in other words, I think the structural deficit.
Yes, yes, thank you.
I also have another question.
Yes.
So, if we consider the fact that
how mathematics have evolved so far,
and also the fact that
mathematical modeling has up to a point
tried to represent, in a way, the randomness
that might be an attribute
of these complex systems.
So, we have already made some,
found out some mathematical ways
through statistics and stochastic processes
in order to study randomness.
Yeah, of course.
And so, do you think that this is like a big step,
a small step to accomplish something even bigger
in the future, because up until the point,
we also thought that randomness was something
that we couldn't even explain,
and now we can computationally...
That's not fully true.
For example, there is no mathematical model
for a true natural number generator,
a random number generator.
So, a true random number generator,
which produces true random events,
can only be constructed
by using a physical device.
Namely, normally, one uses a Geiger counter,
which is counting radioactive decay events,
because they are truly random,
so we cannot simulate randomness.
Of course, we can model certain aspects of randomness,
and this we can do since a couple of hundred years,
and at the end of the 1980th century,
we have started to understand
how calculus can be applied to this.
But the way that we model randomness
is only applied to classical Newtonian systems,
and when we get out of Newtonian systems,
there are actually no examples
to convincingly model a complex system behavior.
So, there are, of course,
like one of your colleagues mentioned,
approximative models of randomness,
such as stochastic behavior,
such as weather forecasting,
but they have a very short-term forecast window,
and they are not very...
depending on in which landscape you are,
they almost don't work at all,
so near the sea or in the mountains,
they almost do not work.
And basically, they work for regular continental climate
with strong determinants over very short durations.
So, the fact that we have probability theory
doesn't really help with complex systems.
Turbulence, which I already mentioned,
is a very good example,
because the mathematician who had found the best way so far
of dealing with turbulence
was actually a probabilistic theoretician.
It was Kolmogorov,
who invented the Kolmogorov-Smilnov theorem,
one of the greatest mathematical geniuses of the 20th century.
In the 1940s, he tried to model turbulence,
and while his model is...
you can see it when our book will appear.
It's explaining the book.
While his model is very aesthetically highly valuable,
it's very beautiful,
it fails to model the reality of turbulence.
And so, there is no example for exact modeling
or good approximative modeling of complex systems.
And I don't know...
it's actually, if you know how this whole probability...
theory of probability works,
how the distribution are approximated,
that it's already very, very hard,
actually to figure out,
or mathematically impossible to calculate,
mixed high-dimensional distributions.
You will see that this is very, very far away.
I don't believe it's possible.
Okay, thank you for your...
Okay, I think we'll have yours to continue now for a bit.
So, now we look at intelligence.
And you will see in the course of the second part
why we needed to talk about complex systems.
So, this here, what you see is the standard definition
of the artificial general intelligence community of intelligence.
And this is...
So, they have a verbal definition which goes,
intelligence measures an agent's ability to achieve goals
in a wide range of environments.
That's the standard definition that everybody accepts.
And so, this ability to achieve goals
is defined by a utility function.
And this utility is here,
V of an agent pi,
depending on the environment of the agent's mu.
And it's defined as the expectation of the sum of the rewards
the agent is going to have,
which is actually normed to be...
equal less to one.
And so, mu is a binary description
of the environment of the agent.
And this environment may be a fired abstract structure
that is manipulated inside a computer.
For example, if you have a theorem prover,
then this would be one environment.
Or it may relate to something in the physical world.
For example, the nuclear power station that has broken down
and where you now want to clean up the nuclear power station with a robot.
Because it's not an environment where we would like to use humans.
And by the way, there's a great documentation about Chernobyl.
They tried to use robots.
But the problem was that the transistors broke immediately
because radiation was too strong.
So the radiation destroyed the transistors inside the robot
and then they stopped working.
So they had to use humans after all.
And in either case,
no matter whether it's a physical scenario or artificial one,
this vector takes a form of a binary string
and it's a description which plays a role in the Hattern model.
And E is the expectation of the rewards.
And what is a reward? We will see it on the next slide.
So this is a basic definition.
Now, first of all, let's consider what's that this definition
is actually mathematically broken,
which I find kind of sad
because they should at least get this right.
But basically this is really accepted in the AGI committee.
One has to see that the AGI committee is made up mainly
by computer scientists and by mathematicians.
And computer scientists like physicists and engineers
tend to not look so carefully at mathematical equations.
But actually the definition of expectation is the sum of a variable
which is actually multiplied by the probability of this variable
for each step.
So if you have a finite amount of steps,
or here is actually even infinite,
but it doesn't matter if you have some steps,
you should calculate the reward of the variable
by summing the product of the probability with the variable,
which is the reward.
And here they don't do this, they actually take the expectation
that is already summed up.
So it's funny because if you wanted to implement this,
it's actually not implementable because mathematically wrong,
which I found quite interesting.
So in other words, the operator E of the expectation
has to be applied not to the sum of the values of a random variable,
but directly to the values of the variable,
as shown here in this equation.
And otherwise you cannot take account of the norming denominator.
So pi i is smaller than 1,
and therefore it's a denominator, right?
If you multiply by number smaller than 1,
it's the same as dividing by this number.
And so usually you need a denominator,
and if you don't have a denominator,
you cannot get the sum below 1, of course.
Because this is just mathematically silly,
but they've published it and the viewers have accepted it,
and it gets cited hundreds and hundreds of times,
I'm astonished.
But anyhow, a lot of crap gets cited a lot,
so it seems to be human nature,
and it's remarkable that nobody has criticized this.
Taking this aside,
and just ignoring the definition problems,
and imagining they had used the proper definition of expectation,
which actually you can read up
in any very basic textbook of statistics.
So actually it's cool when you learn
to calculate the probability of throwing the number 7
with two dice, you learn this.
So I was kind of astonished.
Now, in simplified terms, when we ignore this,
the equation 1 says that an agent pi reacting
to environment distribution mu
obtains a finite reward,
which corresponds to the expectation of reward
it achieves over all the steps it undertakes.
And actually the higher this is,
the better the utility.
And so Barry asked me to insert something about reward.
Of course, the reward here has no meaning for the machine.
The machine is just a calculating machine.
It's a Turing machine.
It can just, you know, how can I say this?
Apply amounts of electricity to certain circuits.
But by doing this,
so reward has no meaning for the machine.
That's just a mathematical concept
to express an optimization problem.
So this year, how can I maximize the utility?
I can maximize this utility
by doing something with this reward variable.
And that's all it says.
So reward doesn't mean what is reward for you
so that I invite you to a good beer in the evening
or anything or bring you some flowers
or so what humans experience as reward.
It's just a way of formulating the
mathematical optimization problem.
Okay, so on the next slide,
what do they do with this utility?
So here you can see this utility term again,
but now it is in a bigger equation
which defines intelligence.
And this uppercase epsilon,
it's a mathematical uppercase epsilon
as a function of the agent
is defined as the sum of the utility multiplied
with another factor.
And what is this factor?
So this factor contains K
which is a Kolmogorov complexity function.
So we already heard about Kolmogorov.
He did not only work on probability distributions
but also on information theory
and the complexity function,
he invented it and it's indicating
the complexity of the algorithm
executed by the agent pi
to represent the environment mu.
So it's basically how many
calculation steps are needed to represent mu.
And as you can see, this is a negative exponent
so that means that this is a parallelizing factor.
So the more steps I need, the more complex
my representation algorithm is,
the lower the intelligence will be
and that's pretty, I think, acceptable
because so if you imagine
a more intelligent individual
will find it easier to achieve a goal
than a stupid individual.
So of course, you know this from everyday life.
And so this says something similar.
It says, so if you have a high utility
for a given environment
but I needed a million steps to achieve it,
then I'm less intelligent than somebody
who achieved the same utility
with a shorter number of steps.
So further, a couple of more remarks.
So mu is the environment, what is uppercase u?
Uppercase u is a set of environment descriptions
that the machine can process.
So for example, if the machine is the alpha-go machine,
then all the situations are settings on the gold board
and nothing more.
But of course, an AGI agent
would be able hopefully to process more different situations
to achieve what they hope.
And so u is just a set of environments
the agent could, in theory, be processing.
Let me give you an example from the animal kingdom.
So for example, a rat has quite a huge set u
because a rat can adapt to very many environments
and therefore it is also to be found
all around the globe, almost.
Not in the antarctis and not in the Sahara, I believe,
but in many, many moderate environments,
whereas other animals are very specialized
and are only found in certain environments.
And this is what this uppercase u is supposed to say.
So basically, the definition of intelligence
shows to you the most efficient possible algorithm
to achieve a certain utility.
Also, the summation over all environments
prevents a random hit.
So because you have to perform this in many environments,
you can guarantee that the equation doesn't give you
a distorted outcome.
So if the definition of the utility function
would be mathematically sound,
which you could easily do by replacing it with this,
then the entire equation would mathematically make sense.
So it is a proper definition of a weighted utility
function. That's what it basically is.
Before we take apart these two equations,
now I've just criticized them from a mathematical point of view.
Are there any questions regarding the two equations
before I continue dissecting them?
I have two questions when I was looking at these formulas.
So I was just looking at V as the author says,
ability to achieve the value, the capital V.
But then in the formula on the second slide,
on this one, yeah.
So why is Gogomolo complexity function as exponential,
and the V is just like linear?
Because it's just a negative exponent,
so it means you just divide.
This means one divided by two exponent k.
So it's just used to actually penalize the utility function.
Yeah, but why didn't they just penalize with the inverse value?
Why did they choose the exponential growth?
To make it perfect, maybe.
Yeah, maybe.
Actually, I haven't thought about it,
but it's basically the way that the Kolmogorov complexity function
always gets applied in theoretical informatics.
So this form is when you read a textbook of theoretical informatics,
you always see it represented like this,
so that you get a smooth representation of the entire function.
So I think it works pretty well to weigh whatever you want to weigh
by the amount of work that you need to put into obtain it.
Yeah, they argue that extensively in the paper anyway.
But the second question is this capital U,
the set of environment descriptions.
How is this supposed to be interpreted?
Because it's written as a sum,
but I would say that in general this is like a manifold in the best case.
So it's a sum of elements of a set.
That's fine mathematically.
So this is just an index of all the possible terms you can get.
And let's say you would have three different environments.
What if it has a ball or a sphere of different environments?
So you just integrate?
No, I don't think so, because you basically can always,
this is theoretical informatics,
so that means that you can, no matter what from a functional analysis point of view,
no matter what is the way the environments are,
they are always compressed into binary vectors,
and then you can just create a series of binary vectors,
or a set of binary vectors,
and this is just one binary vector out of set.
It's a countable set of binary vectors.
No matter what...
Maybe you should explain what a binary vector is.
Yeah, so for the non-nastigmatization, it's just a vector of 1s and 0s.
So remember that a Turing machine can only deal with 1s and 0s.
It's like a big tape on which you can write 1s and 0s,
and can change the 1s and 0s.
And this is basically, from the perspective of theoretical informatics,
it's just a set of such binary vectors.
Sorry, I think it would have been a bit more helpful
if the input and output spaces were also defined,
because now it's clear to me what...
So the inputs are basically strings, as far as I understand it.
Is it?
Yeah, so...
Otherwise the mu is a bit hard to interpret.
Yeah, the view. Yeah, you're right.
So I have not defined...
Actually, it's interesting why...
I've just copied the definition from their paper,
but you're right, they don't...
Usually one would have to use a functional analysis type of definition
of the input space and the output space.
Yeah, exactly.
And I could easily have done this,
but there are two reasons why...
I think they didn't do it for stoppiness,
and I didn't do it because it's just too implicit for me,
because it's what I do all the time, but thank you.
I think we should add this next time.
It's a good point.
Yeah.
Okay, so anyhow, this is a definition.
Now let's move on and look at the problem.
So first of all, there is a verbose definition
before the equation, which says intelligence measures an agent
ability to achieve goals in a wide range of environments.
So first of all, the definition captures just one part
of one of the standard definitions of primal intelligence.
So primal intelligence says that you have to adapt
to new environments suddenly and without being trained upfront.
So you have to be spontaneously able to adapt
to a new environment suddenly, quickly.
And this definition just captures the adaptation.
Now, what is more important is that the definition is very broad
because it allows also this organism,
a small worm that is one millimeter long
and has a thousand cells and only 300 neurons
to be intelligent because what it can do,
it can forage and reproduce in complex environments
so it can live in fruit, in vegetables, in mushrooms,
in soil, and so on.
It can use snails and stucs as migration vectors.
So it can really live in many environments
and it can also reproduce there.
So I think it would be intelligent
according to the AGI definition
and even probably the amoeba here.
This is a nice amoeba.
It's called kaos kaolinensis
because it never has the same shape.
Here's just a drawing of it,
but in the next segment it will look different
because it moves around by changing its shape
and it can also live in many, many environments
and thrive wonderfully there.
It can reproduce, it can find food,
but it's just one cell.
So it's one of the most primitive...
Well, it's the most primitive eukaryotic yeast,
but directly next to yeast comes already this amoeba
and it would also be intelligent in this definition.
So I think the definition of intelligence is too weak.
Why is it so weak?
Because in the book where this intelligence definition is used,
that's the book by GÃ¶tze and Pnachen,
which is called Artificial General Intelligence,
they discuss many other definitions.
But the problem is that if they use definition
like the one very simple one of human intelligence,
intelligence is the capability that enables us to speak,
for example, that enables the language that humans can use.
I mean, it's not my definition,
but they propose this definition
when then they would automatically fail
in generating artificial intelligence.
So they've basically chosen a definition
that doesn't yield intelligence
so that they claim now that they have an intelligence.
But they've actually on purpose
used a very stupid definition that is not intelligence.
Now let's go and look at the...
Let me look what time it is.
Yeah, and look at other problems of the AGI definition.
So let's first look at perception.
So if you have this vector mu
as a measure of complexity of environment,
this vector mu presupposes
that the environment can be represented
by using a binary vector.
In some artificial environments,
such a binary representation may be adequate,
but in natural environments,
we have signals emanating from complex systems.
And therefore the signals need to be actively interpreted
and reassessed all the time.
And also the observation needs to be continuously adapted
to the input as the agent takes account
of the interpretation of each antecedent observation.
And also in animal interpretation,
it depends on previously experienced mental material,
so memories.
And for example, this tiger observing the prey,
it does actually all the time update its observations.
It does active perception or shields,
I think a female tiger.
And if there would be a puppet next to a young tiger,
the young tiger would not be as good
at observing those animals, those prey animals,
because it has that experience.
So the experience stored in the memory of the tiger
also helps it to react better to what the animals are doing
and to single out, for example, one animal to hunt it down.
So the predator, if the predator is just sitting there
and observing the prey, it's already acting to improve
and adapts the perception of the prey.
So perception is not static, but a dynamic process
of constant iterative feedback loops
between sensory and motor neuron circuits,
and we cannot even know.
So if we think of training an ML algorithm,
we need to know the tuplets that we use to train,
but we don't even know when the cycle begins and ends.
So the cycles can be very fast, and we don't know
because we can only observe the overall behavior,
but we don't know how to determine the tuplets
that constitute the perception process.
So our JJ Gibson, a very important psychologist
and philosopher, says normal activity of perception
is to explore the world.
So perception depends on more than just sensory stimulus.
So the view that perception is just the result
of sensory stimulus is completely outdated.
So when we give input to computers,
it's just sensory input, but that's not real,
that's not perception.
Perception requires purposeful activity,
direct manipulation of the object,
and innate or quiet knowledge
of the expected patterns of reality.
So I need categorical predefined ability to deal
with the environment and also quiet knowledge.
And this manipulation actually of the object
doesn't mean that I need to touch them,
but the tiger can also manipulate these animals,
the prey animals in her imagination.
So she can imagine that maybe this animal would now
lean down to drink from water source,
and whether then maybe it would be a good moment
to attack this animal and so on.
So this is highly interactive,
and this static vector mu,
which basically models, for example,
the input from a sensor,
doesn't capture any of this,
does not capture any of what I've just said about perception,
what we know about animal and human perception.
Here's another example of perception,
which is much more complex than this one,
because it involves dialogue,
it involves observation of a dialogue,
it has many, many interesting aspects
that show how complicated perception is.
And so, for example,
what does Tony Curtis, who is here acting as a woman,
think about Marilyn Monroe in this moment?
We don't know, but he's certainly an interesting question.
Anyhow, so perception is not modeled
by this environment variable mu.
On the next slide,
we see the next problem, which is activity.
So the steps of the utility function
that you've seen here are results of...
So each step is an activity which yields a reward.
So in chess or in Go,
you get a reward for making a certain move.
Now, the steps of the utility function
that are described by the Hutter definition
of artificial intelligence,
there are a linear sequence of discrete machine actions.
But human motor acts are actually interactions
of perception motor activity.
They involve at every stage a dense synergy
of multiple body systems at multiple levels of granularity.
And that's, for example,
if you think of human manufacturing activities
where they have to use like surgery,
when you have to actually feel very exactly what you're doing,
or in certain steps in the construction of even of cars,
fine motor, it's called in German.
Barry, do you know the word in English?
I don't, sorry.
Precision engineering would be...
Yeah, probably.
So we don't know how to make machines do this
because we don't know how the circuitry
between perception and motor action work.
So we know that there is some feedback loop, blah, blah, blah,
but we don't know the details of it.
And if you look at the most advanced textbooks or papers
that are available about animal fine motor action,
we have no clue how the animals do these fine motor actions
or know how we do it.
We have no models for it.
And so because we have no models for it,
we cannot do it in a computer.
And so the activities that happen in real environments
are much more complicated than this linear sequence of steps.
And the interaction between sensory and motor activity
could potentially describe this linear sequence of effort,
effort and interest in neural signal events,
but the coupling of such a sequence to any sort of reward
is very indirect.
So yes, in reality, of course,
those things happen one after the other,
but very, very quickly in a very complex session.
And so we don't know how to cover this to reward.
And so probably the sequence in reward terms that we need
would probably not be possible to construct the correct reward sequence.
Another aspect is that mammals can overcome
massive negative rewards to achieve the goal.
So I mean, here I have an example from animal psychology.
So it's a coca in self-administration experiments
to get coca in, they have to traverse a heated plate,
which is burning their feet.
So they have to damage themselves to get to the coca in,
but they still do it because they have primary intelligence.
So they know that they must cross the heat plate
to get to the coca in,
that they must press a button to get the administration of coca in.
They even, and they get a short-term reward for this,
they don't have a net long-term reward.
And so such a behavior is very, very hard to model with a reward function.
And if you think many of you are maybe in the middle of their PhD thesis,
so the PhD thesis process is not like this coca in self-administration,
but there's a lot of negative stuff you have to cope with over a long time
before you get a reward, that's for sure.
And I don't know how this could be modeled with such a reward model,
at least being very hard.
Speaking of reward,
so what is even more important is that the reward pattern
that we see is sum over reward.
It is actually unable to model reward patterns that we encounter in real life,
because first of all,
the system that satisfies the harder definition will always be situation-specific.
So it will work only in the context where a human has already been at work
in preparing appropriate rewards.
There is no general or universal reward.
So the reward, for example, that the machine receives for playing the game of Go,
are points.
And the algorithm is trying to find a functional or an operator
that maximizes the number of points.
It's just a derivative of a very long equation that you have to find.
And that has nothing to do with universal or general intelligence.
The reason is that the mathematical definition that Hutter provides
for the environment new that must be matched by the reward.
So I don't, it's not possible to find a reward that works for all environments.
Whereas humans have as a reward,
they have the main rewards are to survive and to reproduce.
And survival and reproduction can come very indirectly in highly evolved societies.
So to sit in a room and do mathematical equations all day long
for survival and reproduction, that's quite abstract
or to paint, to do paintings of art or to compose music.
And so there is a way of humans to delay the reward of reproduction and survival
very far off and to do activities that seem to be non-connected to it.
And that seems to be at least required for objectifying intelligence or human intelligence.
And we don't know how we can model this with reward.
Also further problem arises that the assumption is made that all rewards of one agent
must be of the same type for every step under a given environment.
So if we go back to the equation, r is only indexed by the step,
but it cannot change its type, otherwise it would need the second index.
So there's only one type. Yes, this type can obtain different values.
But probably in animal and human behavior, there are many different types of rewards.
So we cannot model this.
Then the question is, couldn't we create a sequence of rewards
adequate for learning the behavior of a complex system?
So the problem here would be that such a reward sequence,
we would then imagine many, many rewards in a sequence
and they would have to give situation-specific rewards.
That's because each step on a complex system model trajectory
would have to be able to deal with an unexpected situation.
Because when the AI system interacts with its own environment,
that will change the environment.
And so this change of the environment will create an unexpected situation
and may require different reward.
So you cannot pre-define a reward part or trajectory
because at each step, a different reward would be needed to correspond
to the emanations from the complex system that form the environment.
So that has to do with the evolutionary character of complex systems.
And such a temporal reward sequence would obviously not follow a Markovian pattern.
And of course, speaking of probability theory, if we go back to this slide here,
Markovian pattern is of course a pattern that applies only to classical Newtonian systems.
Only Newtonian systems have the Markov property.
Complex systems don't have the Markov property, but without the Markov property,
you cannot achieve predictive modeling in stochastic equations.
So stochastic differential equations or other stochastic process models,
they always need a Markov pattern.
And without the Markov pattern, in which the reward would depend only on the previous or some previous steps.
But if you don't have this pattern, but you have a dependency on many earlier steps
and long-term dispositions of the organism and also short-term intentions,
you can't find a reward sequence.
And so, therefore, the reward sequence would need to correspond to complex emanations
relating to situations varying as a successive set unfold.
And therefore, the reward sequence itself has to be complex.
And thereby, it would have all the properties of a complex system emanation.
So that's the interesting thing, that to give rewards to an intelligent system in a complex setting,
the reward sequence itself would have all the properties of a complex system emanation.
So it would have the same properties that my stream of language that I'm currently giving to you has.
And we have no mathematical models to create such sequences
because we have no mathematical models for complex system emanations.
And so we couldn't create the reward sequence that would be needed for the intelligent system
to cope with a complex system situation.
And that's a problem, I think, a very important problem to see this,
that the reward approach, why reward is mathematically attractive
because it allows to state the intelligence problem like an optimization problem for optimization theory.
It doesn't create a realistic sequence of rewards that correspond in any way
to what we experience or animals experience when they obtain rewards for their behavior.
Actually, the reward that an animal and foraging animal receives, which is the food
and for which the animal does very interesting things.
So if you read modern books about foraging, it has been found out that parrots, for example,
invent new patterns of shouts and vocal noises they make to describe sources for food.
And while they search for those food sources, which they have to do every day,
they create those sound patterns.
And this process is highly complex and it has nothing to do.
And the way they get to the reward has nothing to do at all with linear reward sequence
that we see in those models.
And so that's why I call these pseudo-definitions of intelligence
because they just define something that can be actually put into a model optimization algorithm,
a numerical model optimization algorithm, like a dual or something like this.
But in reality, it has nothing to do with real intelligence, not even animal intelligence.
Now, I think before I go to the last slide, I would like to give you the opportunity
to ask one more round of questions.
Yeah, I have one question, Professor.
Please.
The thing is, so here the argument is that the reward sequence is extremely complex
and since it is not Markovian, you cannot somehow model this.
Now, if I were to take the example of a robot in a factory,
let's say a robot that knows how to load boxes, unload boxes,
get the boxes onto some other conveyor belt, et cetera, et cetera.
And currently there are reinforcement learning algorithms that are able to do this
pretty effectively in these factories, just replacing entirely with robots that do this work.
So there, of course, the reward sequence is very clear
because you have just one mechanistic task that you keep doing throughout your life,
well, throughout your robot life.
My question here was, what if we could create a set of rewards for different tasks
and model that together into a specific robot?
So for example, let's say loading and loading boxes, opening doors,
walking, sitting down, different movements.
Now, I'm not suggesting for a single second that humans learn this
by this sort of discretized reward sequence.
I'm not suggesting this at all because I don't know how humans learn it.
But as far as teaching robots that is concerned,
don't you think the reward sequence then would just be a set of different rewards for different tasks,
but they would all be modeled into the same algorithm.
So in a sense, what I'm arguing is the whole is the sum of its parts.
Yeah, so what you are actually already giving a small preview of the next slide.
So it's never what you're saying.
So basically what you describe is, of course, that what is a factory?
A factory is a Newtonian system.
So a factory has all those properties.
And so therefore, machine learning can be very, very efficient
in implicitly modeling Newtonian equations, motion equations, also sensory equations.
So therefore, because in a factory, all processes have the Markov property
and because there's no force overlay, the elements are well defined and so on,
that will work pretty well.
And that's why also machine learning is one, I think the best application of machine learning
that is currently not visible very much is actually manufacturing and mining
and other mechanical activities where humans still play a role,
but where they will with the exception of this fine motor or sensoric behavior,
which is very hard to model, they will be replaced by robots more and more.
You are completely right. And that's because here, machine learning is used to model Newtonian systems.
That's the first answer. The second answer is that it's essentially the way to build AI properly
is to do it as you described. And I will come to this once the other questions are answered.
Okay, okay. Thank you so much.
Is there any?
I have one, but I'll give the students chance to butt in first.
I would just remark maybe that the authors were aware of this argument that you are saying
because they said that at no point they are trying to compare their definition to human intelligence.
So I think they perceive the machine intelligence just as a composite of different tasks
as Ravidi was saying earlier.
If you read Hutter's and Schmitt-Huber's papers about general intelligence, I disagree.
They believe that they can create a general intelligence and many of them,
I don't know whether Schmitt-Huber does, but many of them also believe in the singularity,
which I think shows.
I think that, I mean, you should really get this AGI book, or I can send it to you,
or Barry can send it to you, this AGI book by, let me look again, by Gertse Penachin,
which is one of the most important consensus readers where all the big shots of AGI have published papers.
And they are, I think they, yes, of course, they said that it's not human intelligence,
but they say it's a real intelligence.
But what I'm saying is no, what you're suggesting is not a real intelligence,
but it's just a kind of amoeba intelligence and probably not even that.
So I think that because on the next slide you will see how I define what one can do with AGI.
And when I show this to people from this community, I get heavily attacked.
Oh, that's old school. We can do much better. We can do general intelligence and so on.
So I don't know.
They were, I mean, if I understood correctly, they were also saying, you know,
that of course the Chinese rule argument is completely right.
In, when I was, I think when, where they were defending the critiques, maybe that's at the end.
Yeah, but, but, but I don't even argue with the Chinese rule argument here.
I just say that, that basically even the most basic form of intelligence that we as humans perceive as intelligent
would just see the behavior of a dog or another mammal.
Yeah, that this, that this behavior cannot never, never be achieved by these equations.
So what can be achieved by these equations is, is actually what neural networks already do.
And that is because this year is a recipe to, to actually.
Yeah, and so, and so loss function minimizes loss and here we maximize reward, but it's, it's basically an optimization prescription.
And we achieve this by this by applying these models to achieve what is shown on slide three with all the pros and cons.
And I think that these ML models are very useful, but they have nothing to do with intelligence.
And, and, and that's basically, and, and I calling, I mean, I call it, I mean, when I discuss, you know that I have a company,
when I discuss with clever customers, I mean, their customers who want to buy intelligence and I let them under the illusion,
but others were clever to say, but that's not really intelligence.
I say, no, these are artificial instincts.
And this is basically what we do, what we still do, even if we, if we use such optimization procedures as those described by these models here,
we still only obtain a narrow AI.
And with this, Barry, you had a question before I go to the narrow AI.
Yes, so I have a number of questions now, unfortunately.
So what would leg Hutter say to the following objection, you define intelligence as the ability to achieve rewards in a wide range of environments.
But AlphaGo can only achieve rewards in one kind of environment, which is the go board.
Yeah.
So it's not a wide range at all.
Yeah, yeah.
So, so I, yeah, I believe that, that even actually, you're right.
So I believe that even, even this definition, intelligence, so will not be achievable.
So the verbal definition is actually in conflict with the equations.
Yeah, you're right.
And now just a little correction, sorry.
Sorry, just a little correction.
AlphaZero actually can succeed in a variety of game environments.
It can play chess.
It can play Go.
It can play checker.
I'm just talking about AlphaGo.
You're right.
Not AlphaGo, but AlphaZero, which was recently released.
AlphaZero can play a different game.
And actually there was already in 2014 a precursor of AlphaZero, which could play all the Atari games.
Which was also trained with reward learning.
It just couldn't play the strategic games, but it could play, it could play Pong and all the ones that give you a series of points.
But now OpenAI has come out with StarCraft games too.
So they're getting better at these complex environments.
Well, these are actually not complex environments.
So these are still Newtonian environments.
Because the reason this is very important, if you could hold on with your questions a minute.
So the reason why these are Newtonian environments is that also, for example,
games like strategy games, like civilization, they have also been beat by enforcement learning.
But the reason is, of course, what is civilization?
Civilization is a set of rules and equations applied to a certain pattern.
And even if this pattern is created by random, it's created as a multivariate random distribution, usually multivariate.
And so it's multivariate Gaussian.
And then, of course, you can take points from this distribution and put them into a set of rules.
Then you create a very complex set of events, but it's still a Newtonian universe.
And therefore, you can also train in the same place for the ego shooter games.
And therefore, you can train AI that is beating this.
But the point is, since 1950, we have heard predictions about free-moving robots.
Why don't we have them?
I mean, we have them, of course, in controlled environments.
But why do we never encounter free-moving robots in our streets?
Because these are real complex environments.
Yeah, yeah, yeah.
Thank you.
I just have one quick question for anybody.
So why do they use the word universal in the title of the Laguta paper?
Because it sounds good.
All right.
Okay, that's what I guess the answer would be.
I'm not sure.
Jobster, am I wrong?
Please, correct me.
Probably.
I mean, there's a wonderful paper by Johannidis from 2005, which says why most of scientific research results are wrong.
And the biggest reasons, of course, bias and sounding good is also form of bias.
So anyhow, but I'm not a pessimist for AI.
Remember that I made my little jump.
So we have one student, Peter Bottaroni, who would like to ask a question.
Yeah, please go ahead.
Yes, thank you.
Actually, there are two questions.
The first is related with the discussion with the RID.
In the sense, okay, then we say that a complex environment is an environment that is not more.
We cannot model mathematically, right?
Yes.
Yes, basically, yes.
And then every complex environment cannot be something that we create with a software.
Then a complex environment should be a real environment in the sense that everything that is even the more complex game that we can play is not considered a complex environment, right?
Well, unless what we sometimes have is that we have natural human behavior integrated as movies into an environment, into a game.
And then the player in some strategy games, which I used to play 20 years ago, there was a movie sequence in the game embedded and you had to interpret the movie scene correctly to continue to play the game.
And of course, the behavior of humans during that scene, because this was a film of naturally behaving humans, where they were playing a role, but still, that was of course, complex system behavior.
But then the rest of the game was not.
And that's still like it.
Poker would be a complex game, would it not?
Poker.
Oh, yes.
Poker played with humans is a complex game.
Which one?
Sorry?
If you play a round of poker against human opponents.
Okay.
Yeah.
Okay.
Yeah.
But chess with human opponents is not a complex game.
Correct.
Yeah.
Good.
And the second question is related with the reward definition in the sense that at least to me, it seems that we define the rewards with something that is given.
We define the reward for the robot or for the machine.
Yes.
But actually, the human sometimes creates some reward or some goal by itself, by himself, maybe some sub goal or sub reward in order to achieve a bigger goal.
Correct.
Is it possible that to model this creation of intermediate goal or final goal as a machine, according to you?
Yes.
So it has already been done.
So you can, for example, you can have, as you know, you can have meta models in machine learning.
So you can have, for example, several reinforcement learning models that use different reinforcement rewards from a choice of reward.
And so you can optimize from a certain reward.
You can try out several reward types in parallel and find the best model by varying the reward type.
And then you can use adversarial learning maybe to drive the choice of the reward type and so on.
Yeah.
So there are ways to do this, but they will always only solve problems in non-compact environment.
Okay.
Okay.
Thank you.
Good.
So now last slide.
So remember that I'm making money with AI and I'm a big fan of AI.
It's not that I'm negative against it.
Yeah.
So what can we do with the analytical engine?
So the analytical engine is the first name that was given to a Turing machine.
Charles Babbage was the one who built the first computer in the 19th century, around 1850, and it could already do computations.
And Ada Lovelace said that an analytical engine has no pretensions to originate anything.
It can only do whatever we know how to order it to perform.
And so what is now, is that also true for the Turing machine?
So Alan Turing says that, of course, an analytical engine is a Turing machine.
What can we do with it?
And so I think we can do a lot.
And we can do what one of you just said, namely we can engineer by composition.
So that's an outcome that we want to achieve.
And here you see many operators and functionals, which are changed together.
And these are an upper case data as an operator and a lower case data as a function.
For the non-lessimetricians, a function is a relation that takes an input vector and creates an output number.
The operator can also create an output vector from an input vector.
And so they act together here in a chain to yield the final result, which is y or t hat.
And the superscripts that you can see here, teta, kappa, lambda,
there are prior knowledge that can be configured into the functions and operated explicitly of our training tools.
So for example, this could be just a set of rules.
And this could be the parallelization of the rules.
This could be a function that has been trained like a spam filter and where the parameter indicates which training tools for you.
And so I believe that it's very important that you make the prior knowledge that you gave to the machine explicit.
So either you make it explicit if you have, for example, mechanical theorem proving component, like let's say this one here,
or maybe this one would be a mechanical theorem prover, then you need some axioms for the mechanical theorem prover.
And those axioms are its configuration and you need to know which axioms you give to it so that you know how it will behave.
Or this would be a functional.
Now the functional, of course, is trained by using training tools and other meta-parameters.
They are this.
And also those you have to know pretty well.
So for example, we do customer correspondence automation, but we cannot use the way that, so if a company gives us an email that it received and also gives us a reaction to the email that they created,
we cannot use these tools for training because the outcomes that are created by the company are erratic.
That has to do with many factors.
It has to do with human error, but it has also to do with, for example, they have a period where they have understaffed.
So they just give a stereotype answer to all the letters and later on, and they say we will get back to you later, but we cannot react for the next three weeks.
We have a heterogeneous outcome which is not very well related to the input.
And so therefore, you have to very carefully curate the training material that you use to train such complex chains of functional operators.
But if you do this, then you can achieve very impressive results.
For example, we have automated in the insurance industry, there are bills that are, for example, created by when a car gets repaired, a bill is created.
We have created an algorithm that can automatically evaluate whether the bill is correct.
And that's very hard because it's usually done by a technician who looks at the bill and figures out whether the workshop repaired the car in the right way.
And what we do is that we take the bill and transform the bill into mathematical logic.
And then we have also, we get for this car, the car repair instructions by the manufacturer of the car, which we also transform into mathematical logic.
And then we do mechanical theory improving to prove for each step what this step corresponds to a step that is also described in the repair instruction.
This way we have a mechanical evaluation of the correctness of the repair of a car.
And so this is quite impressive because it's a very demanding technical skill that I'm not able to perform.
So I cannot perform what this computer program can because I don't know enough about car repair.
Of course, the computer knows nothing about car repair, but it can formalize the repair bill from the workshop and it can formalize the repair instruction to mathematical logic.
And then all it needs to do is to make a method to establish mathematical logical equivalence between a repair step and an instruction step.
And this is just an example for what something that can achieve with Turing machines and there are many, many other impressive examples.
But my opinion is that you need to do that intelligent behavior is very hard to reproduce with a machine that you have to make conscious decision.
You have to carefully select the training material and you usually need a chain of algorithms that work together, which you orchestrate somehow.
You can require some work, but in the end it gives you perfect outcomes.
And we have actually asked a German court, legal court to evaluate whether it would take the output of our algorithm in a lawsuit to represent the opinion that usually is given by human expert.
And they said, the court said, yes, they would take this because the quality is that reproduces higher than the average human expert's quality.
So this is just to give you an example that I'm a great fan of AI. I think a lot can be done, but it doesn't all work out of itself.
Humans have to design such machines like we've designed planes or cars or other machines.
And then they can work really well and take a lot of hard and sweatshop type of work of humans and create a lot of value.
And this is what I would like to show you and this compositional principle here is I think what real AI is about.
And now it can be very cool to train neural networks as one part of this, but a neural network alone will usually not do.
And that's why I'm showing an operator at the end because this operator is usually a logical operator and it makes sure that the result is reliable.
Because unlike stochastic models, logical operators can auto detect their mistakes.
And so that's why I'm a big fan of combining stochastic AI with good old first order logic AI, which we use, which we both use in parallel in my company.
And so we have people who are specialized in neural networks, but we have also people who are specialized in mathematical logic so that we can obtain the precision that is needed to automate human activity.
Very good.
That's it.
That's it.
So thank you.

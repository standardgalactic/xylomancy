I'm joined today by Barry Smith and Joabst Landkreber to discuss their fascinating book
Why Machines Will Never Rule the World, published by Routledge in its philosophy imprint.
The subtitle of the book is Artificial Intelligence Without Fear. Anyone who's watched UK Column
News for a while will know that in almost every episode references are made to the supposedly
imminent takeover of one profession or the other by artificial intelligence, and there's
certainly a lot about fear in UK Column News and special broadcasts. So with that subtitle
as an excellent promise of what's to come that we can discuss the subject without fear,
I'm delighted to welcome you both. I will read the blurb from the back cover of the book and then
it's over to the both of you to introduce yourselves as you see fit. Barry Smith is one of the most
widely cited contemporary philosophers. He has made influential contributions to the foundations
of ontology and data science, especially in the biomedical domain. Most recently,
his work has led to the creation of an international standard in the field of ontology,
which is the first example of a piece of philosophy that has been subjected to the ISO standardization
process. Joabst Landkreber is a scientist and entrepreneur with a background in philosophy,
mathematics, neuroscience and bioinformatics. Landkreber is also the founder of CognorTech,
a German artificial intelligence company which has since 2013 provided working systems used by
companies in areas such as insurance claims management, real estate management and medical
billing. After more than 10 years in the AI industry, he's developed an exceptional understanding of
the limits and potential of AI in the future. Barry, I suppose it might be over to you first
because people listening to that may have wondered what ontology is for starters, and it does come
into the book repeatedly ontology. So perhaps you could explain something of your career,
what it is that's taken you to upstate New York, where you're speaking to us from today,
and something about ontology and data science, how the two relate and what they are.
Good, so ontology started life as the Latin translation of the Greek word metaphysics,
and so that may give some people some idea of what it is. It's the study of being in the
traditional definition, but in a more modern definition it's the study of the kinds of beings
that there are and of the relations between them, and this study was involved in the very
birth of artificial intelligence. So the first attempt to create artificial intelligence in
computers consisted in attempts to replicate the ontologies of ordinary human beings. The idea
would be that if we understand how ordinary human beings classify entities in the world,
and if we can transmit that information to a robot, then the robot would be able to navigate
its way through the world in a way which is similar to how humans do it. Now with that,
ontology became established as not just as part of philosophy, but as part of computer science,
and it's been of growing importance, I would say, since around 1970 when these experiments
were first made in Stanford, they all failed incidentally. No one succeeded in creating a robot
on the basis of an ontology, but it was certainly an important stepping stone in the development
of artificial intelligence. So the great successes of ontology are not to be found in AI,
but rather in biology and medicine. So what happened was that, well, at the point in time
when the Human Genome Project was beginning to be completed, medicine in particular, and the
life sciences in general began to realize that they were faced with a gigantic avalanche of new
data, new technology, new devices, new kinds of experimental methods, which they knew absolutely
nothing about. And these data and methods were based upon a new kind of chemical information,
really. And the problem was to find ways of translating this chemical information which
consists of incredibly long strings of letters into a language which a clinician or a biologist
could understand. And the key to that was something called the gene ontology. And the gene ontology
is a collection of terms, nouns, and noun phrases used by biologists to describe biological phenomena
which has been used to tag sequence data, gene sequence data, protein sequence data,
RNA sequence data, and so on over many years, resulting in an investment of several billions
of dollars. And this ontology, the gene ontology, serves effectively as the bridge between old
biology and clinical medicine, on the one hand, and the new chemical biology which was unleashed
by the genome project. I was one of the people involved at a crucial stage in the development
of the gene ontology in turning it into something which is logically coherent. So I'm a philosopher
by training and I know something about logic, that the people who built the gene ontology,
they knew a lot about genes and a lot about genomic data, but they didn't know very much about
logic. And so they built an ontology which was full of logical gaps and logically embarrassing
steps and missing items and unclear items. And I showed them basically how to do a better job
of the logic of the ontology. And that gave me a certain influence in the world of bio ontology,
as we might call it. And that led me to become involved in critical work, shall we say, of other
biological and medical artifacts which were created to keep pace with developments in
computer science. And I was very critical of some of this work. Some of it was scams,
that is to say, use of claims about the powers, the computational powers of new
medical terminologies which were unsupported in the terminologies themselves. And in connection
with my work, I was one of the few people who would speak out about these scams,
in connection with my work along these lines, I was approached by Yorbs, who at that time
was actually working for one of these scam organizations. He can justify himself. And he
wrote to me, pointing out that he agreed with everything I was saying, and for a time he worked
as my mole in this world. And I have been working since then in various other kinds
of ontology efforts, but I think that's probably enough to give your audience some idea.
So are you currently in an academic post? I know that you're in Buffalo, New York.
Yes, I'm a professor of philosophy of computer science and engineering of bioinformatics
and of neurology. But my union card says philosophy on it. What a fascinating combination.
And Yorbs Landgraber, who is going to introduce himself now, has no less a fascinating combination.
He's the kind of figure that there is perhaps more of in the European continent
than in the English-speaking world. Because as I read, he has got qualifications in philosophy,
mathematics, neuroscience, but also biology and chemistry. So he's got all of the oligies. He's got
fields which have a lot of formulas in them, but also fields that require a lot of writing and
thinking on particulars with using language. Also, I think you're properly trilingual,
aren't you, Yorbs? Because as well as being a German who uses English for professional life,
your long maternel is French, but you're speaking from your native Germany. So why don't you give
us something about your background and what it was that impelled you to study all of these different
disciplines? So thanks, first of all, for inviting us here, Alex. So I will try to make it short. So
in the end, after I finished a gymnasium, which is the equivalent of high school
or grammar school in England, I really didn't know too well what to do. And so I first started
philosophy, which I then gave up because I was shocked by the state of university philosophy at
the beginning of the 1990s. And it was also time when Barry started to think that he should
diversify within philosophy. So I wasn't completely wrong with my impression. And so then I switched
over to medicine and biochemistry, which I finished in 1998. And then I started a postdoc at the
Max Planck Institute of Psychiatry and Neurogenetics and made experiments, which yielded so many data
points that this brought me into mathematics. And so then I didn't get a degree in mathematics,
but I studied it long enough to start publishing a lot of mathematical papers. And so that's how I
became a bio mathematician. But my interest in philosophy never really died off. And so
when I later got involved into biomedical terminology and ontology, I discovered Barry's
writings and then got into contact with him. Also, since the late 90s started to work in what is now
called or what is already was called artificial intelligence at the time, but I prefer to call
it statistical learning or machine learning. And I use it a lot in the biomedical research that I
was doing since the late 90s. And I've always used it since then in different fields and applied it
to different areas. And that's that's why I was because I used it professionally as a technique
as a form of applied mathematics. I then also start thinking about it. And in the end, this
led Barry and me to write first papers about the topic of artificial intelligence. And then later
on also our book. So you had a definite writing aim in mind. And the title really brings that out,
doesn't it? Why machines will never rule the world. One or other review will have proposed that title
to the other. We've heard from both of you that you dislike scams and low standards in academia.
And I have recorded already one three part series and transcribed it with my father on
low standards in academia. And there's another one in the can coming out. So you're certainly not
the only figures who've become disenchanted. When did it come about? Apart from the world of science
fiction, which I think we might get into as well, because it's far from peripheral here, but in the
world of mainstream hype, commercial hype and academic hype, which I know, that's symbiotic,
they feed off each other. When did people start popping up saying it's not long until machines
will rule the world? And perhaps even more to the point you hint at this at various points in the
book, what kind of people were they who are making this claim? Did they have the rounded view of life
and learning that both of you have? So I think that that claims that machines could become as
intelligent as human beings have been made since the 50s. The famous paper by Alan Turing, where
he describes the Turing test also discusses this possibility. Turing himself believed that this would
come about in someone in the future. But it became more, I think, in each wave of AI, there was these
claims were made, and they were made more aggressively with each wave. So in the first wave, which was
the one in which Turing participated, at least at its beginning, it was the claim was made. In the
second one, it was also made and led to a rebuttal by Dreifuss, who wrote a book against the possibility
of artificial intelligence in the early 70s. And then in the third wave, of which we are now
contemporaries, it is made in the most aggressive way. And I think one of the leaders making the
claim was Kurzweil, Ray Kurzweil, who is technology director at Google or who was technology director
at Google, a really important man in the development of optical character recognition. So he really
did good technical things. But as an engineer, I think he misses the understanding of what the
human mind is and what it can do. And therefore his claims are ungrounded.
This segues me nicely to the next question, which I'd like to hear both of you talk about.
What is the mind? I know that's a pretty massive question, but you do dare to broach the topic
in the book. And you do so apophatically, you do so by saying what the mind is not. You say that the
mind or the brain, which you use roughly interchangeably, is not a machine. And you then bring out some
characteristics which it does have. So over to you on that. What is the mind? And perhaps you might
want to have another pop, I think it's quite legitimate at the Kurzweilists of the world,
who say that the mind is reducible to being a mechanical device and thus can be mathematically
modeled. So I'll have a go at this. The whole theme of the book is that there is a distinction
between two kinds of systems. One kind of system is the mechanical system, as this is the term you
use. And computers are mechanical systems. Your laptop is a mechanical system. A toaster is a
mechanical system. And mechanical systems are built by humans. We understand how they work,
because we understand physics and we can predict the behavior of the systems on the basis of what
we know about their parts and the way they're put together. On the other hand, our complex systems
and all organisms are complex systems and the oceans of the world are complex systems and so
on. We can talk a lot about complex systems. And now the problem is that the claim of the
artificial intelligence enthusiasts, shall we say, is that the mind itself is just a mechanical
system and therefore sooner or later we will be able to understand how it works in just the same
way we understand how a computer or a laptop or a toaster or a car works. And that we argue on the
basis of a quite complicated series of arguments, some of which are grounded in mathematics,
is not the case. No organism, not even the simplest organism, is ever going to be able to
be understood in the way that we understand the workings of mechanisms. And so this means that
we cannot understand in particular how the brain works. This is one of the consequences of our
general thesis. The brain is, I don't want to say it's a mystery because that will upset Jorbs,
since he knows a lot about neurology and neurobiology. But the brain is such that we will
never be able to understand it in the way that we understand mechanisms. Now what that means is
that there are two kinds of intelligence. There's the kind of intelligence that can be achieved
by using mechanisms such as computers. That's artificial intelligence. And then there's the
kind of intelligence which we might call general intelligence, which is the kind of intelligence
exhibited by an organism, specifically the human being. And given this
gap, this necessary gap, which will never be eliminated, and that's probably the weakest
point in our argument, this means that that can never be artificial general intelligence.
And now just one other point relating to this which grew out of the discussion of Kurzweil.
The Kurzweil was one of the very first people to talk about the singularity. And our book is
really addressed to those people who are worried that the singularity is near. I think that was
the title of one of Kurzweil's books. He's been saying that the singularity will be here by 2030.
At least somebody once gave me a transcription job to do in which he told an audience some years
ago, it's here by 2030. So just to put that in context. It will never be here. It was, I think
early he said it would come early, but he keeps postponing it, you know, like the Parousia,
which was postponed also. The idea behind the singularity is that once we have a computer
which is as intelligent as human beings, that computer will be able to program, devise, somehow
assemble even more intelligent computers in a kind of snowball effect to bring about
gigantic intelligent computers who will take over the galaxy. And that idea, I know from
experience of people I talk to, causes real fear in a large group of people. And our book was
aimed to be an antidote to that fear. It's groundless. And just one other thing. I am actually
not so happy with our subtitle because people assume that the world is going to be without
fear wherever artificial intelligence is involved. The problem is that there are still going to be
people and there are going to be evil people who use artificial intelligence to do things which
people might reasonably be afraid of. And so it's artificial intelligence in itself which we should
not be afraid of. But once humans start using artificial intelligence, for instance in scams,
but also in creating super powerful weapons or creating social control mechanisms, then we
for a reason, they should be afraid of it. But that means we're afraid of what the humans are
doing with AI, not with AI itself. Now, this is underappreciated by people who hear, perhaps
with reference to their own profession, that some of the work is going to be taken over by AI,
or as you are helpfully directing us to say artificial general intelligence, a more specific
term used in the book. People are hearing that AI will take all these roles on, such as claims
adjustment, which I read in the book occupies a quarter of a million people in Germany, although
that seems to be rapidly dwindling with this computerization. Now, in a sentence in a nutshell,
what your book points out is that AI, in order to improve itself, needs a continual dialogue with
people, just picking up there on what Barry said about people and their intent behind AI.
And that forms part of the key argument, the syllogism right there in your introduction.
There will never be the singularity super duper AI, why not? Because it's going to have to be
designing a more competent and more amazing successor than itself. And in order to do that,
it is going to have to have natural conversations with very intelligent human designers, saying,
now I need to build the better version of me. And it's going to have to instill confidence in the
human colleagues, no longer programmers, but colleagues and informers, that it knows what
on earth it's doing. And the last stage of your syllogism is, in order to do that, it would
already have to be artificial general intelligence, it would already have to be a human mini-me,
which can't happen. Have I got the syllogism more or less right there?
That was very beautiful, actually. It's one variant of it. And I would how it can be put,
and this variant rests on the insight that machines cannot use propositional thinking,
so when a machine like now the chat GPT, which is creating a huge hype again of how
dangerous and almost already fully intelligent this AI seems to be, when such an AI is used to
create utterances, then it is not uttering anything, but it's just producing a sequence
of symbols, which it doesn't understand. And this is what many people don't realize. They think
this is impressive, and this sounds almost right, but they don't realize that this is just a
sequence model that is not thinking at all. It doesn't have any intentions. It doesn't have any
self-awareness. It's just a syntactic reckoning machine, like the one about which Ada Loveless
wrote, built by her husband Charles Babbage in the middle of the 19th century. So of course,
the machines now are much more powerful. They can compute billion times faster
than these old machines of 150 years ago, but they are still based on the same principle,
and so they cannot develop consciousness or think they are just performing mathematical
operations, which were defined in the 1930s by Elin Turing and Alonso Church, and they are just
performing combinations of these mathematical procedures, and this is not thinking at all.
And so to call the machine intelligence is a marketing trick used to create hype,
but this is just applied mathematics. Let's thrash out these terms then,
because intelligence is the term that's been marketed now. Your book is in some ways
complementary to one written about 40 years ago now, which I've tipped off to Yobes to
called Architect or Be, Question Mark by Mike Cooley, from an educated man coming from the labor
movement, organized labor in Ireland and then Britain, pointing out that there's many more
orders of magnitude involved in human consciousness and cognition than merely intelligence.
And we could go into some of the vocabulary, it varies from author to author, but I mean,
if we start very classically with the platonic scheme, a character, a personality is made up of
mind, feelings and will, and intelligence is a subset of mind. It's not the whole of a character,
right? Feelings you don't ignore, but you don't particularly mention in the book either. Will
is an absolutely core concept in all three parts of your book, the first part on the mind,
the middle part on the hard maths, what can be modeled and what are complex systems,
and the third part bringing them together, what can maths do to model the mind? But will,
before, not that we really meditate upon this unless, like you, we look at the philosophers,
will is absolutely key to having even the most basic conversation, isn't it? Because
in a conversational exchange, there is a will to compete, a will to live in harmony, a will to
understand each other. This is what's missing when you have a game of chess or a conversation with a
computer, isn't it? It doesn't want to be your friend. It just does in a symbolic, syntactical
sequence what it's told. So if you speak to an AI bot in healthcare, it doesn't want to heal you.
If you speak to a bot in a courtroom, which I understand according to some reports is happening
in China, the bot doesn't want to do justice. It doesn't have feelings or will for justice.
And then there's also in the introduction questions about the terms consciousness and cognition.
And if I understand correctly, consciousness as a study, that is what has become in philosophy
phenomenology. You pay a lot of tribute to Edmund Husserl, not a very well-known
philosopher in the English-speaking world, although I think educated viewers will have
heard of him, and a couple of Husserl's near contemporaries, possibly disciples or
colleagues, Max Schaler and Arnold Galen. This is your domain. These are German thinkers,
and they have a happy marriage of being realists. So they're not nominalists. They don't think that
everything's in the mind. They think that what's in the mind is connected even physically to the
real world, but they are phenomenologists. They're studying consciousness. They don't go off into the
later, the long grass of later 20th century Franco-German philosophy that says nothing really
exists. That might sound irrelevant to some of our viewers, but that's the philosophical
glue in this, isn't it? That we now know after a lot of study that consciousness, cognition,
is a lot more than just the mind. Yeah, I mean, before I answer, Barry should also
understand this because Barry is a phenomenologist. I mean, this is also the foundation of one of the
foundations of our friendship, because when I first contacted him, I really mentioned phenomenology,
and I mean, Barry is now doing ontology, but it's very much based on the work of Edmund Husserl.
I just would like to mention this. But yes, phenomenology, I see as the peak of the development
of Western philosophy. So I think it is the highest form that philosophy has taken on since it
came about in the Presocratic times. And it is still, as one can see in our book, extremely
useful as a tool to understand reality. And this is what we do in the book. So we use
Max Schäler and Edmund Husserl as two very important philosophers to, as a foundation for
our work. And the reason they can use swell is that they are not only realists, but they are also
able to provide an explanation for phenomena that cannot be derived from experience. So if you
ask yourself what is the mind, what is consciousness, what is intelligence, it's not sufficient to only
look like positivism proposes it at empirical data. And so phenomenology provides a philosophical
framework how to deal with concepts or entities which are not made of matter. And so because
we need this when we discuss intelligence and consciousness, it is so useful to use this philosophy.
And the main philosophical failures of the 20th century derive either in positivism from the
inability to understand these immaterial entities, or in the case of Heideggerianism,
from an unwillingness to accept rationalism, realism, Aristotelian thinking. And I think
phenomenology provides a foundation for a mature and realistic view of the world that is also in
harmony with common sense. And this is also, I think, one of the driving foundations of our book
is that it is really written and thought through in harmony with common sense. And as a bridge to
Barry's own response to that, Barry, you mentioned that circa 1970, so half a century ago now,
the big drive, perhaps slightly informed by the science fiction of Isaac Asimov and Robert Heinlein
and such like, the big drive was to have a robot that, as you very aptly said, could navigate its
way around the world, or even just one profession in the world. Have we got there even now? Is there
any prospect, given what's just been said by York's, of a robot and artificial general intelligence
navigating its way around the world? Oh, I think that the key, and you recognize this,
to all of this is the will. So we can build robots that can navigate their way around
Disneyland, for instance, because it's a controlled environment and only a certain limited range of
phenomena can be encountered. And that's the key difference between artificial intelligence
and artificial general intelligence. We have intelligent machines, but they're intelligent
only in special worlds where everything is simple. So this we call narrow intelligence.
But we don't have machines that can navigate in the ordinary real world that humans navigate in,
where conditions are changing all the time, where strange phenomena can happen,
where we're called upon to make snap decisions in relation to phenomena that we've never
encountered before. And I think that the key here is the will in the following sense. So
some of the environments in which machines can navigate are created by games, so video games,
or just chess or go. And computers, as we know, can beat humans when playing games like chess
and go. And so people assume that the computers must want to win. Therefore, computers can want,
and therefore, computers can want to take over the galaxy. They can want to win the game of life
with real people. But actually, if you look at the way computers do things like win a chess,
they don't do it because they want to do anything. They do it because they've been trained in a certain
highly specialized way to have what a human would call a reward system. This is what the AI people
call reinforcement learning. And you can build a reward system which will imitate having a will,
only if you can assign a reward computationally. That means just by doing a certain piece of arithmetic.
Now, you can't assign rewards to contributions to a conversation.
And if you don't believe me, just try it. Try and set up a reward system which you and your
friend or your wife can agree on, and then you'll give each other rewards for each step in the
conversation. You will never succeed because a conversation is a realization of a complex system,
namely the people involved. So the AI people can indeed create something that looks very much
like a will, but only in those very narrow areas where we have what we have in games,
namely a strict set of rules which allows the calculation of rewards so that you can train
the machine to get higher and higher rewards by having the machine play itself millions of times.
And that's what they do. That's how they create machines that can play chess or go better than
humans. And most of the real, really impressive, and there are many really impressive achievements of
computers in the recent years. Most of the successes are in areas like games,
including mathematically equivalent areas, logic games. All of the successes are in
what we call narrow areas. That is to say areas where we have something like a logic system or a
simple system which we can understand, in other words something mechanical. Key here is the closed
world. So closed world means that the attributes and the what is called the phase space, which you
can imagine like a multi-dimensional Cartesian coordinate system, that this phase space is
somehow predictable. And either you have a real game situation like the games Barry mentioned in
which this is how the games are, or you can also model a complex system in reality in this way.
If you only model it partially, so when these partial models of complex systems can be very
successful at modeling certain very regular patterns of complex systems. For example,
the traffic pattern in a big town is very regular. There's a lot of traffic in the morning and the
afternoon, and then there are other traffic patterns throughout the week, and there are
cycles depending on the season and cycles depending on on bank holiday days, and all of this creates
irregularity so that you can actually get an AR to model this very well. So whenever also complex
systems can be modeled with AI methods, if they have a regularity. But the problem of complex
system is that they have a lot of irregularities and they make them inaccessible to modeling with
AI, and that's why whenever complex systems interact and create unexpected outcomes,
which happens in every conversation, or in many many other human interactions as well,
even just movements of crowds, these irregularities happen. Whenever this is the case, the AI fails,
and that's why the automation potential of AI can only be applied to very regular events.
And so that's why, for example, a self-driving car cannot drive freely in Disney World.
It could drive in an empty Disney World, but as soon as people are running around,
it will not drive freely. It can only then drive on a certain limited area where everything is
controlled, but as soon as this chaotic nature or irregular nature of the behavior of complex
systems comes into play, the AI will always fail. And it's not a logic possible to train it then using
types of mathematical logic, nor the stochastic algorithms which now dominate AI, which are
called deep neural networks, and we have created the big hype. So in a nutshell, it's the people
factor. AI can navigate a world in which there are things, but people, and they're irritating
unpredictable desires and ways of expressing themselves, is going to be beyond it. And the
computer is always going to say no when an unexpected conversation remark is made. Hence,
if you go to a computer to heal you or to judge your case in the law, if it doesn't understand
your behavior from a mathematical model, it's going to tell you that you are the problem you
do not compute. It's not only human beings, it's nature in general, because most of the natural
phenomena are complex. So it's also animals, the weather, and the whole way our world is structured
is a complex system world. And so therefore, the machine can't cope with the real world,
because it can only cope with simple system settings. And so if you think of, for example,
using a machine as a robotic policeman, this will completely fail because the machine will just
not be able to cope with the real world, because no situation that it encounters
is like the situation it was trained for in the laboratory. So it will just miserably fail,
and that's why the fear that machines were used in this way is wrong,
will be used in this way is wrong. There are legitimate fears, though, but this one, for example,
isn't, because it just doesn't take into consideration that machines can't be made to act
autonomously in such a setting. But it is happening, isn't it? I don't know which of you would like to
answer, but UK column news just in the last couple of months has covered from multiple
jurisdictions across North America, some in East Asia as well, that robo cops are being
let loose in certain situations. And the lawyers are the men who are all the women, of course,
in these in these days, who are writing the algorithms for them. And they're always telling
the robo cops to err on the side of not getting the police department sued, which isn't very
promising as a set of rules of engagement, but it is happening. So perhaps we need to have a
footnote here on the claim there is no need to fear in the AI. So what's happening is that,
Barry, let me quickly answer this. So what's happening is that they are now using robots
for enforcement activities of police. But these are not autonomous robots. They are like the
toys that we use as children with the Tilly Gidee. How do you say they call this with the
remote control? Like remote control toys. So you can actually have a robot that goes into a danger
zone with a remote control that has sensors and a camera and can explore it. But it's not
acting autonomously. And this is this is also not any time to come. So yes, robots are being used
in law enforcement settings. But they're not this is not AI. This is just, you know, a sensor
on wheels. And the sense on wheels may also become aren't soon and be able to shoot or detonate.
And of all of this isn't nice. But it is not it is not that this robot acts autonomously.
And also there is no I've never I've I've not seen that anywhere there is an automated
usage of automated AI in any, you know, court system or legal system. There are, of course, AI
tools in social media surveillance, which are being used, which are very primitive. But they are
but there's nothing like this in the actual workings of the jurisdiction system.
So I think that there are working examples in traffic law. And there is some academic literature
which demonstrates that the that the use of not robots when computers in simple traffic law
situations is both cheaper than using people and also more often correct than using people
where correct means applying the law to a given traffic situation. And I think what what one needs
to say, and this is something that you did in your in your own AI work, your is that those systems
only work if you have humans in the loop, who are there in the whatever it is, the 10% of cases
where the computer is not confident that it's giving the right assessment because it doesn't
have the right data or because there's something which confuses it. And I want to use this as a
segue to talking about GBGPT chat. So I think GBT chat, or is it chat GPT? I'm
I'm always confused about GPT is the current target of hype. And it is indeed possible to
generate impressive looking material out of chat GPT. But I've been trying all day yesterday
and all day today to work out what's going on when I enter into chat chat GPT, simple questions
about two people called Barry Smith, both of whom are philosophers. What one lives in London,
he's called Barry C Smith. And he also does work which is vaguely phenomenological. The other one
lives in Buffalo, and that's me and I don't have a middle initial. Now, when email was first
introduced, and both Barry C and I are old enough to have been around before email. When email first
started, we used very occasionally to get emails from girls who thought that they were in love with
some kind of compound Barry Smith, which included features of him and features of me. It didn't
happen very often, but it did happen. It doesn't happen anymore because the email systems are
now using AI, in fact, almost certainly in such a way that those kinds of ambiguities happen almost
never. But chat chat GPT, did I get it wrong again, is still making exactly the same mistakes.
So it thinks I'm from London. And that it makes a number of mistakes like that. It thinks that my
job is in Leipzig, which it was 10 years ago. It still thinks that my job is in Leipzig.
It doesn't think that. And so I tell it. It doesn't think.
Of course. And I tell it, you are making a mistake. Please correct this information. And then,
so I ask it exactly the same question again, a few seconds later. And it says, along with
the complaint that I'm making a mistake. And it says, I am very sorry that I made this mistake.
And then it repeats exactly the same false information. This is Alex. If I may react to
this immediately, this is a typical instance of which has been described for a long time for
stochastic learning that stochastic systems can't be corrected at will. So they are trained on a
huge set of data which create patterns that direct them how they should then create new sequences
of symbols, which is what they then print out on the screen, their output. And so they can be
retrained, but there is no guarantee that they will then learn the right things. So when you
give a high load of a certain type of language to these models, you can indeed induce a certain
learning effect. For example, the chatbot tie by Microsoft five or six years ago,
when it went online at Twitter, it was retrained by the users to utter
extremist language and sexist language. And it had to be shut off because it was flooded with
these utterances by users. And then it copied their behavior. So this can be inused. But to
teach a stochastic model to answer a certain special question exactly is almost impossible,
especially when the model is very big. So these models are just approximative sequence models.
They don't understand anything. And what Bell just told us is a good example for it.
And for those who aren't regularly hearing language of stochastic, I know David Scott,
one of our presenters who's very keen on economics, will say that in some contexts,
it's just posh talk for guesswork. But you're here talking about it in terms of
the hard maths at the center of your book, aren't you? You're talking about it as referring to the
kind of system where the whole world is happening at you and you just have to deal with it without
computing in a straight syntactic line as programmed. When I just said stochastic system,
I mean really the training type of the AI. So to today's AI systems, the one that create the
big hype are trained using stochastic approaches. So this means that they're basically given a huge
set of data from which they learn patterns or from which patterns are distilled. And these
patterns are then applied in novel situations. And this works really well if and only if the
pattern of data that was used for training is the same as the pattern that is then
encountered later upon the usage of the model. And so when that's the case, the models can be
wonderful and can be super successful and can perform better than human beings. But if there's
a deviation from the training pattern, then they fail miserably and it's very hard to correct them.
Did Barry want to come in on that? I guess I wanted to draw a quite general conclusion in
regard to the general thesis of our book from what we're talking about now. So AI can only work with
simple systems, but these simple systems can be huge. So the English language is not a simple system,
but you can create a simple system which is a model of the English language and which is very,
very large and powerful. That's how Google Translate works. It turns the different languages
into simple systems and then it can build codes which enable them to be translated between each
other. Now, chat GPT has created a simple system out of knowledge on the internet basically,
which is certainly not simple and which is changing all the time, but it does this by
creating a temporal cut. It doesn't have information after, I think it's 2021,
and that means it could never replace Google because Google will often be required to answer
questions about what's happening four minutes ago or four hours ago. And chat GPT, if you ask it for
questions about very recent affairs, will say, I apologize, I only have information up to 2021.
The reason it has to have a cut-off is because it's not going to have anything
static, which it can build a simple model of so that it can use computational tools
to process in the way that Yorbs just described. And that's yet another reason why chat GPT is
going to be making all kinds of mistakes. And it will be making similar kinds of mistakes,
even when it's GPT-4, which is used as a basis, which will have many more data. And I think
we should underline what Yorbs has just said. Many people in the AI world think that if we
just have more training data, then we will crack ever more aspects of intelligence
that we have hitherto not been able to crack. But that's not true. The size of the training
data is not relevant. What's relevant is its representativeness. It has to be representative
of the entire target set, which is open-ended, not a closed world. And that is always impossible
when we're dealing with systems which involve organisms like human beings.
The core property that makes this so is that the process that happen in
animate systems are non-agotic. And that means that they don't create repeating distributions.
And so if the distribution always, like for example, a simple example is the pattern of
each single wave that occurs at the English coast. So since England emerged a long time ago,
there have been a huge number of waves which arrived at England's coast. But none of them
is like the other. Each one is unique. And so this is a good example for a system that
people don't think so much about, which seems rather simple. But it's highly complicated.
And even if you would make movies at a very high resolution of millions and billions of waves,
still we couldn't predict how the next wave would look like at a molecular level.
And this gives you an example for complex systems in animate nature. And that tells you why
mathematical modeling of complex systems is always only an approximation. This approximation can be
very powerful in some context. It can also be dangerous. We can talk about usage of AI in warfare
if you're interested in this. But it still is based on the design and usage by humans.
So we've rather nicely hit upon a couple of the other concepts I wanted to bring in here.
One relating to language, artificial translation, and artificial interpreting, that is voice
translation. And the other being this key distinction between inanimate and animate,
which is in many fields of study, including linguistics, a key concept, literally meaning
not having a soul and having a soul, respectively. Because yours was just sketching out there. When
things become animate, there is no predicting the individual components that go into them.
Now, a moment ago, Barry was talking about machine translation. And that's a term that's more
common in my profession of translation and interpreting. It's more accurate than to say
artificial translation. People who haven't any background in this will have got a sense from
the first part of this interview that a machine is a model for the purposes of this discussion,
a model of reality, not reality. So for the whole time, since your book mentions 2014, since
Google Translate and its competitors, like Deep L by Langenheit, being German, it's better, of course.
Since they came to maturity, just getting up to a decade, getting on for a decade ago now,
people have been struck by how well they do in some areas and how awfully in others. I'm a
Bible translator and a literary translator. The things I do would never go through Deep L
or Google Translate. Although I have to confess, like the whole of the profession, I have the dirty
secret of using them for the legwork. I remain responsible for the end product. Much like we
like to compare ourselves with pilots and surgeons in that you have to have tens of thousands of
hours getting the habit into your muscle memory after you get your qualifications before you're
any good to fly solo, right? So a machine can do the simple operation to flying in clear air or
doing the not too challenging parts of the operation or doing the boring bits of a translation,
but you can't do any of the human intensive bits. You have to be looking over the machine's
shoulder. And so about 10 years ago, I started to get pinged as a freelancer by companies promising
to build translation and, in the end, voice translation, so interpreting systems that would
be good enough to be used at the United Nations. And they all fall over. They're all predicated
upon this idea that you, the paid monkeys, are going to fill in syntax cards. And in the end,
the whole English language, we've just heard it's a very complex system, is going to be reduced
to syntax. And all these annoying variations that these pesky humans use in their wording
is going to be reduced. So a sentence that starts with although has to be rephrased by a human to
one that starts x is true, but y is not true. Then supposedly the machine has got the full
syntactic model. But your book points out, the machine's full flatters are much earlier
stage than that. And this goes right back to NSA and GCHQ in the 1950s, using SISTRAN, which is
still available as a machine translator, to process a lot of Russian intercepted material,
to get from the semantics, in other words, what the speaker or writer is getting at,
to the syntax, isn't going to work. Right, let's take a very simple sentence of German,
such as I might hear at a conference, as I'm interpreting for, let's say that the speaker
says at some point in a new paragraph, es gibt eine Menge Gründe, warum das nicht geht. Right,
supposedly, that's syntactically reducible, so that es gibt, will be reduced to there is or there
are. But that's not how I'm going to replicate it generally. Right, so I might actually, like some
conference interpreters, use some of the logic symbols that are featured in your book, I might
use an upside down capital E, there exists a logic symbol in the margin of my notes to tell me that
at this point I have to predicate something, I have to say it exists. But when it comes to the
end of the speech in German, if I'm working in consecutive mode, and I then have to speak English
to the audience, I'm going to be reading the room at that point before I decide how to interpret it.
If I see that it's a bunch of non-native speakers of English, or native speakers who are half asleep
and who need a bit of a jolt, I might go into Jack and Jill mode of English and say,
there are several reasons why not, and I would use the intonation as well that says,
listen up guys, this is a new point. If I'm in full flow and I see that the audience is with me
and hanging on the edge of their seat, I might start with the predicate and at the end of the
sentence say, and there's no end of reasons why that won't be the case. Right, so there's infinite
numbers of ways of saying things from German to English, and it's not at the syntactic level
the computable level that the problem resides. It's at the level of what am I getting at,
what are my feelings, and above all what is my will in this conversation, what am I trying to
achieve. And so you're quite confident, the pair of you, that given another decade we won't see
more of this asymptotic shoot upwards. At one point in the book you say that, well you quote an
author in the introduction, saying that what looks like a curve towards infinity often turns out
just to be one of those long S shaped curves, and it might flatten out. And you're fairly
convinced philosophically that that's the way it's going to be. We're not going to carry on for a
few years and then find that interpreters are completely replaced. I gave a talk a couple of
weeks ago in Sao Paulo to a big German software conference, and I gave the talk in English, but
there were two simultaneous translators in Portuguese. They were translating my talk into
Portuguese. Most of the audience had headphones, and both of them wanted to have lunch with me.
Basically they wanted to kiss my feet because I had shown why they would still have a job in
the next five or ten years. And we truly do believe that. So GPT-4 will create something
which in some respects is slightly better than GPT-3, but it will still have some of the same
problems. And I think we need to deal with the distinction between different sorts of
audiences when we evaluate these phenomena. So when we look at GPT, we're trying to find
ways in which it goes wrong. Or when we look at Google Translate, we're trying to find ways in
which it goes wrong. But most people are happy if it seems to go right, and they will be happy
over and over again because they won't be looking for the errors. In fact, I didn't notice
when I first asked GPT, chat GPT about myself, the errors. It was only when I looked more carefully
on the next day that I realized that they were confusing me with another Barry Smith.
I would like to add that I think we are already seeing machine translation, we are already seeing
a saturation. And the saturation comes from the problem that we have not only the level of syntax
which machines can only deal with in a limited way, but we also of course have the layers of
semantics and text pragmatics. So that there is also the context which the sentence creates for
each other, the situation in which the text occurs, and so on. To interpret this correctly,
you need, and you describe it really well, Alex, you need intentionality. So your intentionality
allows you, you will, and your intentionality you arrive from, you will allow you to understand what
does the situation mean for me, what does it probably mean for the others, you need interest
objectivity, you need to put yourself in the shoes of the other thing, aha, what is the relevant
part for them, and so on. And if you are a good interpreter, and especially also in
simultaneous interpretation, you are able to do this very fast and the good results means that you
have captured the intentionality both of the speaker and also the probable intentionality
of the listener. So which will let you interpret in a different way given, even if you interpret
the same speaker to different listeners, you will give different interpretations and different
translations, and all of this you can't model mathematically. And so therefore this profession
is not endangered at all. Basically Google Translate for translators, and I translate a lot
of texts as well, and I've always done it in my life. I use it now as a kind of dictionary
that can also help to translate phrases or to give approximations for phrases and sometimes
sentences, but of course the real work of the translation always has to be done by the human
who understands what's going on, who understands the situation. And so this is an area where
AI can't replace human beings, where it can do is, where AI works best is when you have a
situation that's completely repetitive, like an assembly line, or also warfare in which
certain patterns of destruction can be repeated and automated. Now so there are now approaches
in warfare to use armies of drones that are not as precise as human being in their destructive work,
but that will basically clean out a whole area out of the tanks that are in the area. So these
systems are now being developed and they will soon be deployed, and they will be very, very
effective and menacing and terrible, but they will not be intelligent, they will just be
enable a new form of destructive warfare. So these things happen and we have to think
about them, they may have to be regulated, we have to cope with them, but it's much better
to do this from a point of view of truly understanding what is mathematically happened.
Shall we in the final section of this interview then talk about the hierarchy of sciences,
because Jorges has just ended there by saying what's mathematically possible.
In the introduction you talk about three levels of impossibility, of which mathematical impossibility
is the most interesting to me. Physical impossibilities has to do with the laws of physics of course,
and technically impossible has to do with the state of our development as humans.
The third, mathematics, and this only really dawned on me very recently when reading a history
of science written by an objectivist, a devotee of Aang Rand, I have question marks over her in her
school, but be that as it may, this objectivist summary of science, feel free to disagree with
the conclusion here gentlemen, but it said mathematics itself is very deeply human,
that the problem with certain philosophical traditions was that it wasn't thought to be human,
but it is because it's how we relate things in the world to each other. Being an objectivist,
this historian of science went further and said that mathematics is how we relate quantities of
things to others in the world, so it's a model in our mind, so maths model mind getting closer and
closer together than the layman might care to think. In other words, mathematical impossibilities
isn't just things that the universe says no to, they're things which won't go because we,
at least under God if you're a believer, we are in a sense running the universe,
we are understanding what works, and we're winding the machines up and telling them to go,
so it's something that mathematical impossibility, there is no way around it
by waiting for further technical genius developments, am I right? I agreed with some of what you were
saying there, in fact we're writing at the moment a paper which will be the beginning of a series
of papers if everything works out, which defends a view of mathematics along precisely the lines
you just described, so mathematics like physics is a part of human culture, it develops historically
with time, but hand in hand therewith goes the discovery of necessary laws, and some of these
necessary laws will be internal to mathematics and some of them will be laws pertaining to the
application of mathematics, and so on necessary law, which we've been talking about all morning,
all afternoon in your case, has to do with what computers can do, and computers can only
execute programs if those programs require computations in the mathematically defined
sense, which are called church-turing computations, computability in the church-turing sense,
which is very limited to a very small number of very boring, trivial operations,
but which when you have a big enough and a powerful computer can be applied to bodies of data,
which have trillions of data points, and so they can achieve great things, so chat GPT achieves the
great things which it seems to offer by simple, very, very simple steps applied to long, long,
long vectors of ones and zeros, there's no knowledge, no will, no semantics, there are just
very simple steps applied very quickly in a very powerful way to these long strings
of ones and zeros, and it will always be thus, there is never going to be a computer which is
not working like that, that's what, that's what Turing held, and that's what everyone will continue
to hold, and that is the weak point in our book, because many people who have visionaries, they
will say, oh, mathematics may discover a new way of computing, which is not church-turing computing,
it may be some kind of organic computing or non-digital computing, analog computing,
and when we have that, then we will have artificial general intelligence, and that's where we draw the
line, that's where we have to say, well, maybe you're right, but we're not holding our breath.
Yeah, I would go a bit further than Barry, so first of all, mathematics is of course a part
of our culture, but it's also, its preconditions is a structure of our brain, and so the limitations
we have in mathematics, which are, which mathematics is much less limited than Turing
computability, right, so Turing computability is a subset of mathematics, mathematics is broader,
and we could think of machines that could do more mathematics than today's Turing machines can,
this might really evolve, however, we still have the limitation of mathematics itself,
where does this limitation come from? I think it comes from the human mind or the mind-body
continuum, as we say in the book, that the human mind is limited structurally by its biology to
a certain level of complexity, and the necessary laws that Barry has just alluded to, which we
have in mathematics and physics, I think, these necessary laws, they are related, they are determined
by the maximum complexity that we can mathematically figure out or imagine, and so if we look at
the most advanced part of physics, like quantum field theory, we very, very clearly see that
quantum field theory is limited by our mathematical capabilities. Now, I believe that there are,
that there are consequence of the structural limitations of the brain that we have and that
evolved in the process of human evolution, and so yes, this evolution might go on, but we are not
to expect exponential changes of our mathematical capabilities, and even if we had them, we would
still be completely overwhelmed by the number of variables and the complexity of their relationships
that occur, for example, in the human mind or even in the brain of an animal, so I think that the
mathematical limitations are there to stay and that it's much better as a scientist to select
the fields of study with these limitations in mind, and that is what all the clever and great
physicists of the 20th century and also 21st century and all the good mathematicians have done,
they have focused their minds on problems that are accessible to mathematical thinking,
and they all know the great ones, all know very well how limited this is, and they have all said
it, and those who make us believe that artificial intelligence can be created and that machines
can become more intelligent than human beings are not these mathematicians and physicists.
There's one exception, Stephen Hawken, but Stephen Hawken I think never took the time to think
artificial intelligence fully through, and he also has a tendency for sensationalism,
but other than him, I know no really great physicist or mathematician who has ever who
believes in the feasibility of artificial intelligence because all of them by own experience
what they do when they do mathematical models of reality, they know about the limitations.
Is it perhaps part of the problem that physics and mathematics have become much less experimental,
much less doing in the real world for perhaps a century, and have become much more deductive,
much more theorising for which obviously there is a place, but could it be that the induction,
the keeping of a whole model of the world in our mind and refining it as we find new facts,
has fallen by the wayside? It was there obviously as we built up from classical geometry to algebra,
astronomy, physics, the whole line. You mentioned in the book that biology is an odd man out in
this, because as you mentioned a moment ago and in the book, even animals let alone people have a
mind-body continuum, so you can't model what the body's feeling, even animals are placing themselves
to some extent in the person or the other animal that's there, predator or prey, and that that
can't be modelled. But even where it's just inanimate phenomena in the world, have the sciences at
this point started navelgating with their theories that they're unable and unable even to see what
they're missing by not inducting more. Before Barry answers this just one very important remark here,
AI as it's practiced today is highly inductive. So the applied artificial intelligence research
that has led to JetTPGPT and many many other applications is highly inductive because it's
using empirical material to create to automatically create mathematical algorithms or equations.
So it is highly inductive. But you are still I think pointing at a very important problem and
that is that the reflection of science by theoreticians has become detached from reality to a certain
extent. And this is in the humanities, this is a huge problem that has been ongoing for quite
a long while now. But even in physics itself, there are now areas where physicists have detached
themselves from experimentation and are claiming that they can produce pure theories of validity.
This is a very dangerous trend in physics, but I don't think that it explains the hype
around artificial intelligence. This hype rather comes from people, from practitioners
on the one side who don't understand well enough the mathematics of what they're doing.
On the other hand, of course, from entrepreneurs and politicians who want to exploit AI for
certain purposes. Are there no worse intentions at play than that, Jorbs? And I bounce that back
to you because in the previous interview you gave about your book with Jamie Franklin on
Irreverent Podcast, you made no secret of your Christian profession and you talked about the
different intentions there may be here. I ask because that the most famous science fiction
scenario in the world, and I know it was influential in Germany as well, was Isaac Asimov's
series Foundation. And of course, the classic three volumes are written in 1940s and 50s.
They're part of this California-based exuberance of the post-war era in which a number of questionable
characters are writing science fiction. Some go on to found cults or to become drug-addled maniacs
later, including Aldous Huxley, of course, who oversees that scene and the proofs of it, as it were.
But others are more sober-minded like Asimov and have a better intent for mankind.
Although I have to say he was brought into science fiction by Robert Heinlein, who was a
senior member of an out-and-out Satanist group, the Ordo-Templi Orientalis. And it was Heinlein
who told Asimov, after the war, you'd better write science fiction. Interesting detail.
But Asimov lives until the early 90s, and at the end of his books, the last sequel he writes before
his death, has his favorite character, the robot Daniel Oliver, who in the mid-20th century is
doing what Barry told us the robots failed to do, which was to navigate their way around the world.
Poor Daniel Oliver, who perhaps is the real hero, even more than Harry Selden there,
at the very end of the whole series, so he's a spoiler if you haven't read the series,
he summons the humans who've been trying to unite the galaxy in peace and says,
well, I was behind the drive to artificial intelligence. I, who am myself a robot,
I tried to get through physics and through biology, everyone to will the same will,
to get rid of this annoying problem that people have their own wills. I can't cope with that.
I've now managed to engineer the universe, or at least the galaxy rather, to a point where
everyone wants the same thing. I even set up the environmental movement, says the robot,
so that people would feel an impulsion to want the same things. Okay, you can say with Goodwill,
Asimov's got good intentions here, but that the whole end of this saga, which I think leaves
its mark deeply on other sci-fi, and people who come out of that cultural milieu like Kurzweil,
is, okay, we have to get everyone pointing the same way. We have to have them sharing the same
will and feelings. In other words, they become a single mind body. So can we discount the possibility
that there are some actively dark evil actors in the field who want AI or who want to robotize the
human brain to do just what Asimov says his robot wants to do at the end, which is having failed
to build a smarter model of himself using circuitry at the very end. He has to take hold of the most
advanced human in the galaxy he can find and basically steal his brain in order to go further
with the plan. So how much of that is going on right now in AI? I would say, so from the
intentions perspective, there may be people who have this intention. And if you allow me to make
one religious remark here, Barry, in Matthew 4, the temptations that the devil sets up for Christ,
there are around power and bread, right? Creating bread for everyone and having one
world power that governs the whole world so that peace can be established and that we can
create heaven on earth. And I think that this is not for nothing that these are the big temptations
because this seems so nice. But we know that we can't achieve this and we know that by no means,
even in North Korea, we cannot have everyone intend the same thing, even with the most cruel
system, the most violence and abuse of power this can be achieved. And if there are people who want
this, well, they are bound to fail. And the question is just how much pain will be will
happen on the way to failure to failing. However, nothing of what you described from the from the
foundation saga is currently happening because all of it is so far away from mathematical,
physical and technical feasibility that it's just not happening. So give you an example,
this firm by Elon Musk to create blood brain implant ships has failed so miserably that he is
now sorry that he's now that he's now getting back to just creating good old Basel ganglia
simulating ships, which have been around since 25 or 30 years. So this is so far away from feasibility
that it's just not even the intent may be dangerous. But the way to to to try to create
this will not go via AI, but much more traditional ways of exerting power upon human beings.
I'd like to volunteer a more modest thesis, more modest than what both of you just speculated
about. So I think that there are limits to evolution. There will never be nine feet tall humans,
because you just can't make the biology work in such a way that those humans would be selected for.
And this applies not just on Earth, it applies on all conceivable planets. And it applies not
just to height, it also applies to intelligence. So there will never be considerably more intelligent
people than the ones we have now. There will never be people who are cleverer than Leibniz and Newton,
who will evolve and establish societies, neither here nor on other planets. And this has the
beautiful consequence that we can explain Fermi's paradox. The reason why we don't see aliens regularly
landing on Earth, flying in spaceships, which would go faster than the speed of light, is because
they're as stupid as we are. And all the governments on Earth are just as stupid as we are. So they'll
never be able to create these big effects, which Alex has dreams and nightmares about. It will
always be just shuffling through from one bad outcome to another, which and we pick up pieces
and move on to the next stumble. Let's round off then with some practical encouragement. We've
already consoled those who are dreaming dreams and nightmares. People in their own line of work
may be told whether they're in management or at the coalface. AI is replacing you or your boss or
your underling or part of your job. It may be framed positively. It's taking the donkey work
off you, whether you're a soldier or a lawyer or a doctor. People may have their qualms about that,
perhaps the more so having heard this, or at least they'll understand it's not really feasible.
What kind of educated doubts should people sound in their own meetings at work
in order to bring a measure of reality back to the conversation and de-hype? Or perhaps in the
longer term, what kind of more rounded figures should people in their own line of work be seeking
to produce so that perhaps the next generation of leadership will not be enticed by the
huxterism that's prevailed for so long with regard to AI? I think that
there are two, for the normal workforce person, the best thing is to look at industrial revolution
as it has happened in the last 150 years. Industrial revolution has certainly mechanized
a part of human work. I think most of this was really beneficial because dangerous and very
painful work has been taken on by machines. New work opportunities have been created.
I think that this will continue, but it won't continue. This is the second point at a speed
that is comparable to the mechanization of labor in the second half of the 19th century.
At that time, there was really a very fast mechanization of human labor,
for example, in the cloth manufacturing industry. This is not happening because we have had the AI
technology that is now being developed and deployed. We have had it now since 25 years.
Since 10 years, there has been a huge or 15 years, there has now been a huge progress
with the neural networks, but we haven't seen a big change in productivity.
That means that the usability of these technologies is fairly limited. Otherwise,
they would now already be used massively in various industries, and they are being used,
but their effect on productivity is quite modest. This is the second point to point to,
is that it is part of historical development, but it's not as extreme and radical as it was
thought. The other advice one can give is, of course, education, because education always helps
to cope with changing environments. Barry, you're involved in education, so your
contribution on this is most welcome as the last word. For a long time, people were arguing that
just as simultaneous translators would soon be out of the job because of computers.
So, ontologists would soon be out of the job because AI can create the ontologies
and do a better job than humans. All I can say is that all my students get jobs immediately,
and some of them are earning straight from PhD more than I am, because there is a need
for human beings who can build ontologies. I take comfort from that. I think if you have a
coherent problem to solve in relation to complex systems and every science, every data-gathering
effort, every new experimental method is a complex system, you're going to need humans to work out
how to make use of the results. So, I don't worry and I tell my students not to worry.
This has been an extremely rewarding conversation about why machines will never rule the world
by Jobs Landkreber and Barry Smith, published by Routledge in its philosophy series, but don't
be put off by that. If you've never read a book knowingly from the philosophy section of a book
shop or library before, make this the one, because if you read it chapter by chapter and there's a
guide in the introduction to which order is best for you, depending on what kind of reader you are,
you will get through it, even if you have to skip a couple of equations. I very much recommend
that people read that book and I hope that we'll be talking to both Jobs and Barry again.
Thank you very much, Alex. Thank you, Alex.

All right, so old-fashioned chatpots are obviously lossy.
They repeat themselves over and over again, and they're bland.
Google Translate is not so obviously lossy until you
test it with really difficult text, and then it fails.
And there are easy ways of generating failure
for Google Translate.
So what's going on now is that by adding more and more
parameters to the AI systems that we're building
in creating language models, we're getting closer and closer
to having created in the machine a more and more
adequate simulation of the way language works outside the machine.
But it's still lossy, as we will see.
And so we don't need to deal with this.
ChatGP is a lossy paraphrase generator for text.
That's the main thesis.
And yours will give a more elaborated thesis in his presentation.
It's bland, generally speaking.
It's great for creating summaries,
modulo the problems which we will identify.
It's bland.
It gives you often too much output.
It can't really give one word answers.
It always wants to tell you what it knows.
But the most important problem is that it's full of errors.
So I asked this question a couple of days ago.
So who is William Hogan?
I originally asked.
It didn't know any William Hogan.
Eventually, we got it to recognize the William Hogan.
But it got the initial wrong.
He thinks you're called William W. Hogan.
He's not.
So if we go through, so it thinks other false things.
All of these are false things.
So this is what's left.
The career before Florida is missing because it got it all wrong.
I thought it had occurred in Alabama.
In fact, it occurred in Pennsylvania.
And this is typical.
And I'll give you another example in a minute.
So using chat GPT is worthwhile for creating
summaries of a certain sort.
But you have to check for errors.
But because there are so many errors, quite serious errors
when it comes to research science,
it's not useful for research at this stage.
And the one question is whether it can ever
be useful for research.
So this is another example.
I was doing research on biomedical ontology.
So I asked him what are the principal publications
in this field.
And it invented a new journal.
And I asked him what were the 10 most important papers.
And it invented seven papers.
Each of them were vaguely resembling existing papers.
They had overlapping authors, overlapping titles.
They were all in this non-existent journal.
And this is partly a product of the fact
that chat GPT wants to be helpful.
So it's giving you all this information, which
it thinks you will be happy about.
All right, now one more example,
and then I'll turn this over to Yobes.
So a long, long time ago, I was working
on the field of consumer health.
In other words, trying to find adequate means of dealing
scientifically with the way patients describe their health
conditions.
And so I was working with some linguists
on creating something called medical word net.
It just didn't really go anywhere.
It was an interesting experiment.
The paper is still cited because the idea, I think,
is a good one.
Now, the idea was that we would create
a kind of standard patient, the capillary, which
would make, I don't want to exaggerate here,
but would make all the mistakes that patients
make in their understanding of their problem
and what the doctor can do.
Now, the mistake is wrong.
Now, all the simplifications which
are characteristic of non-experts.
So it was a non-expert-controlled
vocabulary for health.
That was the idea.
And now, since we have chat GPT, and since chat GP is chat
GPT, so desirous of being friendly,
I figured it could build a consumer health vocabulary.
And now, I wondered how I could set up
a situation in which you would do that.
And I came across something called family feud.
I guess you all know what family feud is.
I didn't know what it was because I come from England.
But we don't have anything like, or we didn't then
have anything like this.
So the idea of family feud is that you ask questions
and you give answers which are not necessarily true,
but which are typical of what ordinary people would answer.
And so this is the question.
Tell me something sharks are known to eat.
And you have to say either fish or people,
and then there are some small-scale low probability
options which people can get.
These are the two that one is searching for.
So I tried it with chat GPT.
And he gave me this list, so really friendly lots of choices.
But there are no people here, you'll note.
So now my challenge is to try and get it to say people.
And so this was my second attempt.
And now it's got other marine animals,
such as whales and dolphins.
So it's not really getting closer.
It apologizes.
When you tell it, it's not getting closer.
It apologizes.
So it says the second most popular answer after fish
is likely to be seals and sea lions.
So it's not getting the goal of this at all.
And then it apologizes again when I say it's still wrong.
Now it wants sea turtles.
And so that was still wrong.
Then it tried seals.
Then it tried seals again.
Then you're asking what they would say on family feud?
I did, actually.
It didn't give anything coherent.
You had to explain what family feud was.
I think that the training material for chat GPT
was more the boring Wikipedia type of text
rather than the exciting, I don't know.
Daily human conversation.
So it seals three times, because that's one word.
And I was trying to get it to focus on one word.
But then it said surfers and swimmers.
That's really good because it's moving towards people.
And so it got better and better.
So beach goers and surfers.
And then ocean users.
And then marine recreationists.
It is a word, actually.
You get that phrase just a couple of times on Google.
I'm not sure you ever get marine-oriented individuals,
but you do get marine-oriented.
So it doesn't make up words very occasionally.
I discover it, but it didn't actually
make up any words here.
But then it gets sailor.
This is really good.
So I tell it, you were closest with sailors.
Start from there.
And this is the answer, it gave.
All right, now this may be an example of creativity.
That's the end of that.
So I think we all know this is how chat GPT works.
You give it a sequence, and it tries to extend it wordlet
by wordlet.
And it always picks the highest probability word,
except when it doesn't.
So there's a question, how can marine-oriented ever
be a high probability word?
Well, the answer is because you've already
given all the higher probability words.
But if you tell it to give the highest probability word,
it will very quickly descend into repetition.
It will just repeat itself over and over again.
And so you have to have rules which prevent that.
And that's the end.
Now I'm hoping that we can share the screen so that we
can see yours.
Should I share my screen now?
I'm going to see if I can make this.
Well, Jihad will come and let you share your screen.
Yeah, you can.
If you're on TV, you can click Share Screen.
Yeah, I clicked it.
OK, this is good.
Wait a minute.
One second.
Sorry, I'm in.
We need to.
Oh, no.
Gosh, I can't share my screen on.
I've never found out how to do it with teams.
So very useful.
Teams, can you try going incognito and loading?
No, no.
I've tried so many times.
It's a Mac computer I have.
OK.
Don't do that.
I'm sorry.
Someone else needs to share.
I'm sorry.
I didn't know we were using Teams.
Someone else needs to present my presentation.
Oh, what do I do?
I have your slides, Jobs.
Yeah, but I don't know whether you have the latest.
Well, you should have to do it.
So yeah, share them.
OK, let's try.
So I'm sorry.
I will throw the computer.
OK.
You could start by describing what you think about what I just said.
Well, you made some mistakes, but we will clarify them as I go through the presentation.
Yeah, if you go through the mistakes, that would be useful.
I don't remember.
I think, oh, let me see what were they.
Oh, so about AlphaFold, you said that it is actually modeling a logic system.
It's not.
AlphaFold is modeling a complex system as a logic system.
So that's very important.
So what AlphaFold models is a complex system behavior, the folding of proteins.
But using a logic system.
And it works because the logic system can approximate, I'm getting echo, to some extent
what's going on in the complex system.
OK, I have now.
Yourses, files, slides.
Are you still getting an echo?
And let me try.
Now it's gone.
Thank you.
And then I think when you described how chat GPT works, it was not really, really fully
accurate.
I will explain once my slides are up.
I'm sorry for the inconvenience, but when I do what teams tells me to change on my computer,
it never helps.
I never get the result that I should.
I think it's under here.
Oh, waiting.
I was surprised to hear that the reference to family feud was not something that was
in the training data set for chat GPT, because the training data sets go and crawl.
So it's like Peter Norvig, the father of AI, is one of the maintainers of this, and
they've crawled the entire internet.
No, no.
You're right.
It's wrong that family feud is not in it.
The reason why certain results are not given is there are several reasons we will learn
as soon as I can present my slides by certain elements of the training machine.
Thank you.
Oh, no, that's the wrong one there.
That's the one with your corrections.
Okay.
Have your slides now.
I will move them forward when you say next, but you were presenting the ones with annotations
and corrections.
You were presenting a version where you have deleted entity.
So I need to email them to you and Jihad again, the latest ones.
I'm sorry for all this delay.
I really wasn't prepared for this.
Okay.
So what do you want to do?
He's sending a new set.
I'm so sorry about the delay, but we are still in time, according to the schedule I received
from you.
So you can see our screen, right?
Jobs.
Yeah, I can see everything, but just the pain.
So will you get your email on this user?
He's sending it to you.
First of all, I need to find Jihad's, I send it to you, Jihad, and you, Bill, and also
to Barry.
So that's the second.
And let me just say one thing about Family Feud.
So one thing I noticed is that a very common scenario is that it will say, either it will
say error, or it will say, I don't know what you are referring to, can you give me more
information?
This happened with William Hogan and with Bill Hogan.
The first couple of times, I couldn't find him, and I had to then add more information.
I can't remember exactly what happened when I sent it to you, Jihad, Barry, and Bill,
and so on.
Yeah, because I wonder if you, if you, you know, I probably didn't wish to, it's in the
data set, it just needed, maybe it contextual prompting.
Barry, can you, can you unshare this version with the correction, please?
I still didn't receive it, but it sometimes takes a couple of minutes, by the way.
I'll just start my presentation without showing any slides, it doesn't matter.
So, so the presentation is called, large language models are just very complicated analytical
engines.
And on the first slide, I'm showing a model of Charles Babbage analytical engines.
So the analytical engine of Charles Babbage was the theoretical attempt to build an analogous
computer.
Can you hear me?
Yes.
An analogous computer that could perform reckoning operations like multiplication, addition, and
so on.
It also has a theoretical programming language, which Babbage designed, but it was never built.
And I think the Royal Department of Engineering also said that it shouldn't be built because
it would be useless.
So then the first thing that they could build was only 80 years of Babbage, of course, and
it was still built.
Did you receive the email at home?
Yeah.
Can you get it?
It would be really nice to get rid of the echo.
So this is the end.
I still have the echo.
Okay, hold on one second.
Try to mute us to see if that helps.
So I try again.
I still have echo.
I'm on the move in the meeting and have this microphone on.
So did you get the email now?
Yes, we have the email.
We're just copying it to a stick.
Oh, the computer is not connected to the internet.
Some of them are and some of them aren't.
Let's see.
Anyhow, so the analytical engine of Babbage was never built.
The first computer to be built was Conrad to the set three was also an analog mechanical computer.
And so basically LLMs are just very complicated engines and we will.
What does it mean?
So there are currently there's a huge hype around large language models.
Check GPT is one of them.
Then Google has one which is called barred.
And there's another one by by by Meta formerly known as Facebook, which which is called Galaxica.
And so the one by the first one open AI by open AI, which is also by the way used by Microsoft
for being built into Bing Microsoft's hopeless competitor for Google search.
Is perceived as a great success and an AI breakthrough, despite the massive hallucinations that Barry has just described.
And there are huge expectations.
There are huge expectations from the markets.
So currently check GPT is valued at 30 billion dollars that 30,000 million dollars.
And Microsoft is investing billions and as I said integrating GPT into Bing.
Google barred is was also presented a couple of weeks ago in a investor public investor presentation and it make a very minor mistake about the James Webb Space Telescope.
And this very minor mistake basically led to hysterical reaction of the markets.
So Google lost 100 million dollar market capitalization alphabet lost 100 million dollar market capitalization when this error was announced.
And still not on and but will nevertheless be integrated in Google search sooner or later.
And then there's Meta, which was even launched before check GPT at which was perceived by the public as a total failure due to the hallucinations.
But the hallucinations were of the same degree of severity as in check GPT.
Why then was it perceived to differently because Meta claimed to be able to summarize scientific papers.
And though in the science domain, the invention of the model seemed much worse than in other areas.
So the LLMs are all at the similar performance, but the public perception validation of them is a matter of spin and context realization.
So this is I think very important now on the next slide please.
We see somehow the format got destroyed.
Yet another I should have sent you a PDF anyhow.
Yeah, you don't seem to have the font I'm using. Well, I'm sorry.
So now the hype around large language models has also reached medicine. So on the left hand, there's a new paper which just appeared a few weeks ago or two weeks ago by Kung et al.
And it tried to test check GPT on the US MLE, which many of you must have passed.
And as you may remember, I didn't I have the German one, but it has three levels right this is undergraduate this seems to be before the clinical period and this after the clinical period.
So you see that I mean the paper has some weaknesses.
And it's not perfect, but you see that that jet GPT actually solved up to 60% of you as MLE questions and it's interesting that the performance was best for part three and that's because part three has content that play that of which you see a lot on the web.
So in the Internet, you have a lot of content that relates to clinical problems, but the preclinical stuff is of course not so much published on the web.
And therefore, the performance here is the best.
And now, of course, you know, the algorithm will still have failed us MLE, but it's quite impressive and the paper concludes that check to be performed at or near the passing threshold of 60% accuracy.
Being the first to chief this benchmark this marks a local milestone in our maturation impressively check to be was able to achieve this result without specialized input from human trainers.
And now then they also claim that check to be displayed comprehensible reasoning and valid clinical insights. This is total anthropomorphic nonsense, as you will see.
And of course, an unrefined check to be he fails the exams.
So I think the glasses rather half empty than half full but the authors conclude that check to be he may potentially assist human learners in a medical education setting.
So I believe that the model can be refined by training it on specific material, and then it can probably achieve 80% of all three parts of the exam.
And at that stage, it could be used for for as an expert system for decision support, but certainly not for automation, of course, because it doesn't think.
This is just to give you an impression how this is going to might affect medicine as well though one has to put in a word of caution here you all know that so far expert systems have been around since 50 years, some of them outperform have outperformed humans since
at least 40 years and still they're not seen in clinical practice very much because physicians resist the introduction nevertheless it's an impressive result so if you please could move the next slide.
So, let's, let's take a look what large language models really are so basically the sequential stochastic animals.
Now what's that.
So on a on a coaster so basically I supplied mathematics for the identification or mapping of recurrent patterns into machines.
It's not a model of human mind or even animal intelligence.
So I I doesn't think or intend. And there are two types of AI.
There's deterministic AI, which contains rules, search recipes, logics and trees, which is explainable and very reliable. So this is what's built in to most of the, let's say, the last steps of the behavior of all the war and military equipment,
deterministic AI. There are, there's also some suggestive AI built in, especially in the census but the decisions are made by deterministic. So Cruz missile hits its target based on deterministic AI.
And suggestive AI is regression classification pattern recognition it's not explainable, and it's probabilistic.
So examples are AlphaGo and chat GPT.
And there are of course hybrid models. For example, chat GPT is actually itself a hybrid model. It's not a purely purely stochastic model.
Okay, as we will see a bit later. So if we move on please and thanks for for moving on the slides. So what what we see is AI is basically statistical learning for automation pattern identification.
And you, we have here, it's done like this we have an input set and an output set. These are the independent variables, these are dependent variables, like for example the independent variable is an email text and now you have as output the decision of if it's spam or not.
Now what we do is we train an AI relation. And, and this relation is computed using an optimistic optimization function. Yeah sorry here's the format is broken there was an equation inserted here.
So this was this was just this, this tuple of that used to train. So these are the training tuple for the training. And the relationship between input and output is modeled using a loss function, which is minimized by numerical optimization procedures.
So that's all that's happening in supervised learning. On the next slide.
We see that now they have moved on since 2013.
They have moved on to unsupervised learning. So interestingly, the, the big breakthrough for the third AI wave was Google, the Google algorithm that recognized that could create an abstract representation of a face of a cat for millions of photos.
So how does this is this work millions of photos were given as input to the to this convolutional neural network. And it had the task to actually recreate the input that it has received as output.
But while the the image data was going through the neural network.
And with many image data given through it, the model achieved the parametrization in the middle layer that rendered an abstract cat face.
And so this is called a foundational model. So that's a model that is trained without outcome. So it's just trained by by by asking your network to, to basically reconstitute the input it receives as output.
So it's also called encoder decoder architecture. So the input data is encoded in a certain way, and then it's decoded and then it's output again. And when you do this, you basically parameterize a multivariate distribution of the input data.
And, and then you obtain a foundational model and this foundational model can then also.
And that's happening in unsupervised manner. So you to do this, you don't need any training data. Sorry, any, any outcomes.
Like we showed in the previous shown in the previous site, you just need the raw data. This is very practical because it saves you the whole huge annotation effort, you can just train with data.
And then you obtain this multivariate distribution model of the data sequences. You can do this for images and also for texts. And then you can use supervised adaptation to get a domain model.
And so, so GPT three Bert and clip are three important examples for such for such models.
Google Translate is, I think, also now moved over to foundational model, but earlier versions were still done in a supervised way.
Okay. And, and are there any questions here, can you still hear me all right.
No questions.
Okay.
Yes, I was muted to minimize echo for you. Can you hear us?
Yes. Yeah. And any questions?
I have one question. So when you talked about unsupervised learning, didn't there have to be someone at the end of the chain who was recognizing whether they had identified a cat correctly or not?
No, so because what you do when you unsupervised learning, you, you basically have an algorithm that takes care of checking whether the input is equal to the output.
So the model, what you do is, if you have a picture of a cat, you encode it in a three dimensional matrix, indicating the pixel, the pixel shades of gray or color.
Yeah, let's say you use a black and white picture, then you just indicate for each pixel, the shade of gray it has, right. And then, and then you just measure from the output where it looks like the input.
And so, so you can fully automate. So of course it's unsupervised doesn't mean that the human being is not involved. Somebody has to write all this algorithm and test it and make sure that it works.
But then unsupervised means that the outcome, there's no outcome to the data, right.
And, and so this is how foundational models get trained and it's super impressive because without any human input into, into which we have used for 50 years in statistical learning or even longer.
I mean, it goes back to Bosco which was 1760.
Right. So Bosco which invented statistical learning in 1760 and for two, for 250 years we have always used, you know, input output triplets. And now you have a triple free learning. This is, this is a huge progress.
Of course they are you setting up the algorithm but it's still very impressive.
I have one question.
And I know that you're referring specifically to foundational language models when you refer to deep learning models being a form of stochastic models but
I just wanted to make it clear that, you know, most deep learning models are actually deterministic the language models we're talking about like auto aggressive language models draw from a probability distribution.
You know, for the output and that's what makes them more stochastic is because they're not just picking the most probable response. Okay.
Whereas most deep learning models, computer vision models, you serve the same input to the model 10 times are going to get the same response. So they are.
Okay, so, so they are deployed.
Can you mute this again because I'm getting terrible echo.
Thank you. So all deployed stochastic models are always deterministic. So what whenever you train a primitive linear regression or multi-variate regression or some kernel regression or classifier,
the attribute stochastic refers to the way you train it. Once it is trained, it's always deterministic. So there is every stochastic model that gets deployed works in a deterministic fashion.
It will always produce the same output based on identical input. It's just that the term deterministic versus stochastic only describes the way the principles, which, which I use to create the model.
Once the model is started has deterministic behavior.
Right. So the problem that you have with why stochastic models are unreliable is that you, that, that you cannot easily predict which behavior it will have based on which input, but given an exactly identical input, the output will always be the same.
So, so the behavior of the models is always deterministic once they get deployed, no matter how they are created.
Can you then explain how it is that when I play with just chat GPT, I can ask the same question eight times and get different answers.
That's because, because there's a controller sitting in front of the stochastic model, and we will get back to this later. But this is basically the proof.
This behavior of chat GPT proves that it has a controller based architecture with with if it was a pure end to end neural network in deployed mode, it would be have deterministic behavior.
I mean, what one way that auto aggressive language models have done that is to draw from a probability distribution is, you know, predicting the next word, you know, you don't just predict the most probable next word you draw from a probability distribution and you happen to get, you know, a word that's not the most
Let's, let's come back to this later. Okay, so can you present again jihad.
So, next slide please.
So, let's look at how the large language models to have chatbots are trained so the first step is basic training without, without outcome, foundational model as we've just seen it then there's specification of the model to task.
Through supervised learning, and then there's reward learning and then there's an automated reinforcement learning approach on question answer pairs. It's very impressive.
And so, so the first step is in the current version of jet tpt or the one, let's say about which I read publications in January maybe they're now deployed another one was GPT three dot five was used as a foundational model and the dark blue parts are the refinement and moderation
steps. If we go to the next slide.
We see.
I need to follow this. Yeah, I think that everything can come across. I'm sorry for the bad format thing it's it's I should have said the PDF.
So in the first step, the foundation language model is trained using a transformers auto attention so what was Vani in 2017 published, I think, in the last since Schmidtuber published his work about LSTM in the mid 70s.
This was the next most important paper about neural networks because because the LSTMs and the, and the gated recurrent units that had been invented by Schmidtuber and his accoludes.
They had they had computational they were not computationally very effective.
And, and so was Vani at all from Google they showed in 2017 that that you can have a purely feed forward loop model that has only feed forward computations and no recursion in it but that can still achieve the same computational properties as as the recursion you have in the LSTMs.
And so what you see on top is basically just saying that a sequence of symbols is modeled.
The condition probability right and so that you basically say each of the symbol of the of the symbols or tokens out of the sequence is is a is a is multiplied with the with the next one or the one before, given the others.
It's just really a Gaussian or Bayesian Bayesian sorry naive base distribution approach to to the modeling of the sequence and the architecture of the model is relatively complicated, but but in the end, the most important mechanism is attention.
And because it and we will look at attention on the next slide but because attention only use information about other tokens from lower layers.
It can be computed for all talks and parallel which needs to improve training performance and so the resulting operator so this gives you mathematics being a huge operator which which which is a relation that relates a vector to another vector you know function is a relation that relates a
scalar, but this relates a sequence to another sequence so it's it's it's not it's not it's mathematically speaking operator and this operator maps a sequence to itself but contains a parameterization which models the distribution of language sequence is found in the training material.
This is super impressive because because it basically, it can create, it can create, it can create, recreate a huge distribution of language and and you see this that that how good it is you see this from the fact that it has almost no syntactic errors when it creates output.
So let's look at the next slide.
Please.
Thank you.
So here you see how attention works.
So this is a mechanism used in sequential neural networks to provide a context right weight vector that gives emphasis to picture relevant aspects of an input sequence so it, it asked.
Here this sent there's an example here the animal didn't cross the street because it was too tired.
And you see that for example, the word it receives attentional enhancement and emphasis.
So that you can basically see that the in this subordinate sentence in the cause of subordinate sentence. This refers to the noun phrase of the main sentence.
And this is achieved by nobody thought this through but basically they tried it out right and they tried that it's done with this equation here which which which basically contains the input sequence and then three different matrices of parameters and if you multiply.
If you multiply this with these matrices, which are at the beginning set at a certain initial value, then you can you can compute an output sequence shown here and and this is just a holistic computation recipe.
But but it yields a relatively accurate syntax generation.
If it is applied with sufficient bread spreads means that they use many such attention mechanisms on one sequence and depth that they stack a lot of attention on top of each other and they just tried it out and by doing this they they found out that this creates an auto encoding that that is very syntactically very reliable.
So this is a super impressive achievement, but it's it's from the quality of the result is comparable to what you get with an LSTM, but it's compute much faster.
And so this is this is a very, very important result.
So and enter the final the final output is computed using a softmax function which is shown here.
So, if we move to the next slide.
Once the foundation wall has been trained in this way.
Now comes the moderate comes the specification of the foundation model to the task at hand.
So what they have they have a huge database of of important questions that are often asked in the internet, like give me a recipe for tomato soup.
Where is Paris in France or what is Paris.
When was Jesus Christ born and so on.
Questions that are often asked and so so what happens in this step is that a prompt is sampled from the prompt data set.
And so basically, then the sampler, the labeler or annotator as I call demonstrates the desired output behavior.
And so in this way they have actually written hundreds of thousands of answers to frequently asked questions.
So not only have they written answers to questions but they have also solved tasks like write me a poem tell me a joke.
And so on and so on and hundreds of thousands so this was super expensive right so this is why open AI is a company that didn't do much mathematical innovation so they just took that the transformer model from Google, but then they put a lot huge effort on this annotation.
And this now they have two pills, like the ones from I described the beginning for supervised learning.
And now these two pills are given to the model and now the model is not used anymore to do auto encoding.
But now it's basically like in transfer learning used to answer the questions.
So it generates now a condition probability in the way that this is done for spam filters.
And by doing this, the pyramid the millions 100 to 200 or 300 millions of parameters of the model gets fine tuned because they now are changed to create the desired output, which is a very traditional form of machine learning.
That's basically the Bosco which method again, just that that the loss function you was using 250 years ago was a bit simpler.
It was some of these squares.
And today the loss function is more complicated but basically it's the same trick of minimizing the difference between the desired output and the output of the model.
And then that was the first step now you have a better model now then comes a step where you again collect data.
And now but you know you don't you collect questions but now you don't write the answer anymore but you let the model write the answers but now an annotator ranks the answers by quality.
And by doing this, the reward model is generated that tells the model, if you create this type of answer then you get this and this reward.
There again, tens of thousands of annotations are used. And now there's a really very, very important step, which was also used to train Alpha Alpha go, which is that you use the reward model to now let the model train itself.
Now you can give a task or ask a question that creates an outcome, and then you can give it a reward and do this again and again and again to make the model better and better.
At the end, the model will mostly give out answers that corresponds to the highly ranked answers, or like the highly ranked answers that annotator ranked.
And so this is this is a very important step. This method used to this method PPO which is called proximal policy optimization algorithm is a super interesting innovation.
And this was actually this is I think the biggest contribution to intellectual property or to neural network science that open air has done itself and that was already in 2017.
So at the beginning when opening I were just a few people they focused on creating policy optimization algorithms for reinforcement learning and this was without this.
You wouldn't get the results we're getting so this this step is mathematically very elegant, and to decisive step that that makes a difference because here, you can't get enough material.
So we create kind of an artificial reward. Now, now the advantage is that this allows you to really re parameterize the model to give satisfying answers, but also explains the weaknesses of the model, because it forces the model to always give an answer, and it also creates because
it's very schematic, right these rewards are very schematic, and of course not very differentiated, and that that's why the model is so bland, and also repetitive because it is, it is, because how satisfying an answer is cannot be packaged into reward model, but
which is done, because otherwise you cannot train the PPO algorithm but the price is that you're getting kind of that you can recognize what the model creates because it's, it's very stereotypical.
Okay, I hope this was understandable now let's move to the next slide.
Any questions for before.
Any questions now. Yes.
So, in your picture you have two places where.
Yeah, the female human avatar is represented does that mean that gender.
No gender avatar.
Go on there.
Does that mean that there are humans doing the labeling.
So, so the, the, the, the person showing there is an annotator and the annotator, these are thousands of annotators, a lot.
They work actually we will see in the next slide under quite strict policies.
And they, they in the first step they really write answers. So this is a very expensive step, because they have to write answers to questions and you have to make sure that these answers are correct.
And so the next in the second step they only give ratings to the quality of the output of the model. And from these ratings a reward model is created.
This is super clever and a great achievement of open AI and that's the core mathematical contribution to machine learning is that they found a way to create based on human scores a reward system that is you see in chess or go.
You have a reward system that is very simple because the games can be formulated as points.
So in go actually it's even simpler than in chess because in go in each situation on the board you can calculate exactly how many points a move gives you.
And so you can, you can set up the reward function of the reinforcement learning to be proportional to the number of points that you are getting at each step or at further steps and you can have this.
You can have a factor that that penalizes steps that are further way and so on. But basically you have a natural way of giving points to reward system.
Here you don't have this because it's a language answers and so they found a way to to to reward answer quality and that's what the second step up so it's super impressive.
But it also explains the blend of the question answers.
So this is really not supervised but semi supervised, right.
Well, the first two steps. The first step is called classical supervised.
The second step is also supervised. It's a supervised generation of a reward model.
A reward model is used for supervised learning, but it's then not supervised anymore, but it's basically applying a reward model that was created using supervised learning.
Okay, any other any other questions or should we move on.
Okay, very good. So if you could. Yes, thank you. Go to the next slide.
And thanks for your hard work on this. I'm sorry for the technical problems.
Okay, so now this is super interesting. So there is a moderation classifier that was created to avoid non PC language and for some model output corresponding to the woke culture expectations of today's West.
So we know that that we have now the culture that we have now is not comparable at all.
Well, many ways not comparable to what we had in the 1970s when the dirty Harry movie series was is what was made.
So now you know this is all outdated and everything has to be politically correct.
And how did they how did they do this how did they achieve this so this is a paper.
And I'm afraid the reference is not visible because of the formatting problems.
But this is a paper called holistic approach to undesired content detection in the real world and actually on the paper.
If you look at the PDF, it says warning some content may contain racism sexuality or other harmful language so that you already wanted trigger warning that you should not be afraid that some dirty words appear in the paper as negative examples of
Now, now what did they do so they they use basically public domain data and then also
training data and and they have and then they have a very sophisticated way by creating a classification model that
that that classifies undesired language.
And so here you have the categories of undesired brain is sex, what they call hate violence harassment, self harm and then these are subcategories.
This is for example, anti feminist sex sexual language hated against colored people.
And so these are subcategories of the former.
And so you see here if you use public so this is this shows you the area under the curve for the classify which is of course as you all know, which measures how well it performs.
And you see that that the specialty so some here self harm is a super low detection rate it's super bad.
Sexuality is also not so good. Now they use this what they call DAT the domain adversarial training, where they were iterates several times the training and improve here fan is advice improvements
augmentation of training data.
So they do a lot of this was super expensive exercise.
Here there's also an adversarial team built in that creates more complicated hate speech and so on. By doing this they have already improved the baseline then they also have synthetic data that they have created themselves.
And then with the mix the two, then they get very, very good area under the curve as you can see here for the for the for the cat for the broad categories they have now, not not perfect but quite good, quite good detection
So this
Why, why are some categories not working so well, because either they are harder to detect or because they just couldn't put in even more effort, but but this training effort made created basically PC quality classifier.
And the result is so good that is, is, it is not possible to make off hard to make HPT speaking simple toxic language so you can, as somebody has now shown, make it say some bit toxic language.
Like I think somebody made it say that the person talking to suicide herself or himself.
But but it can't say the n word or a few, for example.
And the classifier is probably part of the controller and it certainly reinforce with a deterministic filter. So it's impossible to make it say the n word.
And that's that that that can only be achieved that can't be achieved with the stochastic model. So there must be deterministic filter building.
Right. So but this was cost also, you know, you have to also what I failed to say I forgot to say is that each training run to train the chat the GPT 3.5 model costs $2 million one run or $1.5 million of CPU time.
Even if you own all the CPUs yourself and GPUs yourself will cost $1.5 million.
And that, so that that that is basically the discount rate of the CPUs plus the electricity and cooling costs you have.
And so the, and then the further the other trainings we just saw also cost millions. So I think that they have used several hundred million just for the training, plus hundreds of million foot to pay for the annotate.
So it's super expensive. Let's move on.
Oh, yeah, there's a question. Okay.
Given that we're talking about PC language, I'm going to say this is more of a comment than a question, fully acknowledging that I feel like it's giving chat GPT too much credit to say it's, it's nearly impossible to get simple offensive language out because I have seen lots of
examples of ways to break the prompt and get chat GPT to go into expletive laden tirades and
Yeah, so that.
And but I think it does but the end word did you make it can it say and what and fuck you.
No, I don't.
Um, but I think you can still do a lot of harm without saying that word. Right. There's a lot of other possible vocabulary you can do.
I do think it plays into exactly what you're talking about the importance of the policy. There is a policy in place that is blocking this and being able to leverage that policy is how, because they essentially it's about coaxing
people to explicitly forget it's prior directives and if you can get it to
you can tell it to ignore prior directives as long as you can get access to the directives and you can you can
you essentially trick it into telling you the directives that are supposed to be secret.
So if you know what the secret directives are you can explicitly tell it to ignore those, which plays in exactly to what you're talking about in being a policy, right so that there is a direct policy about correcting vocabulary and adjusting vocabulary.
So that part plays in but I think it is giving them too much credit saying it's really hard to do.
Yeah, I mean, it's how it means that that depends on how you define hard but but it's obvious that I just want to make the point that they have put a lot of effort in it but because it's a classic model and because and because in the next
slide I will for some reasons I will spend the next slide you can still trick it and and and and yeah let's go to the next slide to understand how it can be tricked I would say how that works.
Here you see now.
Actually not on this type of the one there after but doesn't matter so so what results from from what we just saw so basically the model is excellent at completing sequences from dense regions of distributions it learn.
So, so therefore the it has plausible answers to frequently asked questions it can perform standard tasks quite well, but as Barry has shown us the results need to be checked.
So if we are so to speak in the center of the distribution of the training material at the refinement material performs really well.
So that's why I think also if you would now refine it using medical knowledge texts it would become better and better.
And because of of the auto attention is excessively used in the training of the foundational model, and also the training data have been cleansed excessively so they have cleansed away poor grammar.
They have another model which we will discuss in the next slide which is used to write code, which has also been excessively cleansed so they, the training that they use they have thrown away all the syntactically poor data from poor code that to make
it possible to perform well on coding on writing code and the same is true here so they have not only did they use out of auto attention, but also they have done proper good job at data cleaning.
But we as Barry has shown we often get non factual pseudo facts the hallucinations with implausible texts and the density of mistakes increases outside the court distribution.
So it's just that the model only compute sequences which correspond to the learned language distribution without understanding anything. So it's just a conditional probability, it's given out.
And so answers.
The answers to complex or a topic are generic repetitive planned vacuums and anodyne. It has it shows a total failure and fringe and demanding areas of language, such as philosophy of science or, or, or, so the other field science or, let's say, a Persian literature of the first millennium
before Christ or so it will fail.
Even, you know, other other many other areas.
The limited ability for dialogue that it has seems to be achieved by entering the previous conversation to some extent using the controller so there's a controller in front of the model, which which when you engage into a dialogue takes dialogue history and insert this dialogue
into the model and this is what they what you can't train for and that's the effect that you just described I don't the one who commented my previous statements. That's how you achieve the what you call revealing its policy the model doesn't reveal anything willingly
because you are now using a pattern on the on the sequence on the sequence generator that which is what the model is that could not be taken into account at training time, you know, achieve output that cannot be predicted so well, because, because you can't you know
and so therefore because because larger chunks of the past history of the dialogue are used as input of the system by the controller. Now you get the effects that that can't be predicted and these effects depend now not anymore so much on the moderation that they've done and on the all the
on all the steps they've done after the basic training but now you get back to seeing what's in the foundational model. Yeah, this is this is the reason why why you know previous chatbots had to be shut off, because very quickly you could basically get to two parts of the
foundational model that were racist and full of hate speech and so on. And, and here you can break this protection by by by by by basically using the concatenation of dial of previous dialogue, a text as input, which is something you can train up from.
Now,
but of course the model doesn't understand answers and tasks it generates only a chain of tokens as condition probability.
If we move to the next slide, we see another important aspect so so now are some open questions so.
So is this an end to end model or component architecture with controller.
So, I think it's probably the letter, the controller explains the dialogue behavior. It's also explains the variance given identical input, and also the deterministic avoid of negative keywords like the n word or fuck you.
So there are a couple of hundreds of prominent words that you can't get out of the model but you can still get it to say you should, I should kill myself right, but but basically they are, but but this is probably happening by the controller.
The variance can be achieved by two ways can be achieved by, of course, you can easily get a stochastic model to give to give a list of outputs, and then you can use a random mechanism to select, not always the same but different ones but I think here.
It probably works by slightly modifying the input so the controller probably measures whether but that's just speculation but I think that's the most likely how I would the controller looks checks if the input is the same as before, and very slightly
to to select a different to obtain a different output or maybe it does a mixture of two so that it's all just the input and also selects the second or third out of the list of possible responses or uses another random mechanism so that's how you get the variance that is of course not typical of deployed stochastic models.
Then there is must be also side to the moderation is probably via said separate model module, which explains why the bot does not generate adverse texts, and why can it generate code so open and I made also an LLM to generate computer code, and it was not trained on language but only on code.
So by by using huge open source repositories of software code, and this is probably also integrated into the whole chat GPT architecture and sitting behind the classifier gate.
So the classifier classifies that the user wants to obtain code, then the request of is passed to this to this codex LLM, and then codex generates a code and and the controller gets it back.
So this is how I think it's used, but there are this is there are there are this just.
Let's let me say qualified speculation out of 20 years working in software engineering myself now.
But but if you look if you look, but still given all what I've said.
If we imagine all that these large language models are specified to certain tasks in the manner that was described for chat GPT.
They will gain a lot of usage every day life and call and could also support medicine right I mean, and that that you can create adverse language with it in reality doesn't matter at all.
I mean you just go to a construction site in your town, and you listen to how the people talks and your adverse language all the time.
Yeah, so at first language hurts no one violence is only when somebody gets physically injured.
So, so, you know this whole hysteria about oh it creates adverse language yes it does.
But you know, need a read flow bear or Ovid or Goethe or or Herman Melvin it's full of you know violence and adverse language.
It's just part of life and so I think this whole cult around about making models not speak an adversary language is doesn't will not stop its adoption because because ultimately there's huge sequence generating models are super useful.
And they will be put you know into search engines and they will be used as this will be the first generation of expert systems that will become widely used.
And I think they're super valuable, because if you want to you off course you always need to look at the output carefully and judge yourself with it whether you want to use it.
But but but for most, if you don't try to break the system, but if you just try to use it properly like, like, like you say, what is the best antibiotic for metronidate so resistant infection of the interest time.
Right. And now, and now you will just get a very good answer. Of course, you can make the model gives you a bad answer, you know, by having engaging along a dialogue and so on.
And then you will you will because of the mechanism I just explained will make it create output that is that is bad but basically, if you just use it in a very rational way it will be very good.
I don't know whether I still have another style I think that's it.
Yes, so thanks a lot.
And now I hope you can discuss a bit.

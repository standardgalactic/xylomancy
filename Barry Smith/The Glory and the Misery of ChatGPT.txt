So I'm going to be talking about the book indeed, but I guess the most interesting part
of what I have to say is about chat GPT, and so I changed the title.
This is the book.
The subtitle is Artificial Intelligence Without Fear.
So we certainly don't need to be worried about the supposed fact that machines will
one day rule the world.
AI is a set of algorithms, of algorithms which belong to a certain kind of applied
mathematics.
And these algorithms are very good.
They can do wonderful things.
And the fear that people have, and we are aiming in the book to set aside this fear,
is that one day there will be an algorithm of this sort which is able to provide an
intelligence which surpasses the intelligence of human beings.
And then once we have an AI algorithm like that, it would be able to write a new AI
algorithm which would be even more intelligent.
And then we have an explosion of ever more intelligent AIs.
And eventually they would be able to use their intelligence to replace human beings
and to rule the world or the galaxy or the whole universe in principle.
This is nonsense.
This will never happen.
AI algorithms will be always much lower in intelligence than human beings.
Indeed, they will never have intelligence like the intelligence of human beings because
they will always be what is called narrow AI, which means that they are intelligent only
in relation to one specific activity, for instance, playing the game of Go.
And they will never have the kind of general intelligence which we have and which would
be needed to take over the world.
So that's what we mean by artificial intelligence without fear.
There will never be the singularity when AI explodes and becomes more intelligent and
more powerful than we are.
So this is another way of formulating the main facings of the book, which rests upon
the mathematics of complex systems.
Complex systems, and that means all systems involving organisms, your brain, your digestive
system, you yourself, the system formed now by the people in this room.
All of these complex systems have evolutionary properties.
What that means is that they can change.
They can acquire new elements, new types of elements, new types of interactions.
And any model which can predict the behavior of a system breaks when you have new types
of phase space, they say, in physics.
We can't model complex systems mathematically.
Therefore, we can't emulate such systems inside a computer that follows trivially.
So this is the main thesis of the book.
The main chapter is about the mathematics of complex systems.
The rest of the book is about many things.
It's about intelligence, which I'm going to talk about next.
Human intelligence, what makes it special?
It's about attempts to emulate human intelligence by means of modeling, in some sense, biologically,
the human brain.
I'm going to talk about that also today.
And then I'm going to focus my energies on chat GPT, which is, as I say, something glorious,
but it's also really, really, really bad, and I'll try and prove that with some examples.
So an example of a system which is changing its phase space is the system of creating
spam, which is a system run by evil people whose life is devoted to creating these horrible
things called spam.
We can stop the spam using AI.
We can build spam filters, which are narrow AI.
In two senses.
One, they only filter out spam, but two, they only filter out spam of a sort and sort.
And as soon as new types of spam come down the pipeline, then the spam filters won't
work.
And this is what I mean by the impossibility of predicting the future, predicting future
behavior of a complex system, even a complex system as familiar as the system of spam creation.
All right, so AI is always limited to simple systems in a technical sense.
So an AI algorithm like chat GPT is a huge mathematical polynomial function with billions
of parameters.
Google Translate is not based upon those complex systems which are human languages.
It's based on a frozen set of data, a corpus taken from the 96 or so human languages which
Google Translate translates.
And that corpus is then turned into a simple system.
And then Google Translate uses very large algorithms to create polynomial functions
which can take an input in German and yield an output in English.
And that's a mathematical application to binary vectors made up of zeros and ones which can
be translated as English sentences and binary vectors made up of zeros and ones which can
be translated as German sentences.
Google Translate is dumb.
It doesn't know anything about meaning or semantics.
It doesn't know what it's talking about.
It just performs a certain mathematical calculation.
Mathematically rather simple because it has to compute inside a Turing machine which is
a relatively simple kind of environment but incredibly long as an algorithm which explains
why it's able to perform such impressive feats.
So there is glory to Google Translate.
I think Google Translate is fantastic but it's not going to take over the world or anything
like that.
All right.
Now how does this work?
How does an algorithm like Google Translate work?
Well many people think that all you need is enough training data and then these things
called deep neural networks can be trained to use statistics in order to predict patterns
in those large bodies of data.
This isn't quite right and even a lot of people in the AI world don't appreciate this short
fall in the idea that all we need is mere quantity of data.
What we need is to be able to sample data which has a variance which is the same as
the target data.
So if we're going to take the sample data and use it to predict patterns in the target
data, then the sample data has to be statistically like, it has to be a typical sample in other
words, like the target data.
It must be representative of the target data.
So what that means is that it has to have the same distribution of the target data
and this is the bell curve which is the simplest kind of distribution.
There are other kinds of distribution but the data you have has to have the same distribution
as the target data you're applying to.
And that's what Google Translate does, it takes samples from all the world's languages
and it is able to take them as representative of the patterns in this frozen corpus that
they use as a starting point.
Now there are target domains where there is no distribution and so there is no way in
which we can get representative sample data.
So this is true in an emergency room in a hospital in a big city, you just can't predict
how much blood will be needed or how many beds will be needed or how many doctors will
be needed, even an hour ahead.
But it's true also of any conversation, you can't predict what your conversation partner
will say next.
Alright so this is an overview of what I'm going to talk about.
First of all I'm going to talk about human intelligence, actually animal and human intelligence.
Then I'm going to talk about the real reason why computers will never take over the world
which is the fact that they will never want to take over the world, because algorithms
can't want, they can only do what you tell them to do.
Then I'll talk a little bit about Nick Bostrom and his idea that we can build a super-intelligent
AI algorithm by emulating the whole brain of the human being.
And then finally I'll talk about the really funny story of Chachi Petey.
Alright so the big difference between organisms and simple systems is in one word it's thermodynamics.
So in other words it's a matter of physics which involves energy and we and all animals
survive because we have the drive to acquire energy from the environment.
Now we humans do this in a very complicated way involving things like supermarkets and
farms but every animal has a way of sucking energy out of the environment.
Every plant does this with the sun.
Even computers are driven in a certain sense, they take energy from the environment but
only because we give it to them and no one gives us energy, no one gives animals energy,
we have to go and find it ourselves.
If there is a surface of energy in our environment, in the ancestral environment of human beings,
then we become obese because we like eating and so we keep eating and this eventually
will mean that we will eat so much that we use up all the energy in the environment and
then we die.
But gradually we moved out of the areas of the world where there was lots of food into
areas of the world which were cold and barren and so we had to find ways of surviving in
much harsher environments.
That's why through a long series of faltering steps we created civilization, police, armies,
all the other things which make it possible for us to survive in a world where we're competing
with other groups for limited food supplies.
What civilization does, what social norms do is channel the excess drive which human
beings have.
In other words, we become more rational and less instinctive.
But we are still always seeking for energy but now because we have found ways of solving
the energy problem through supermarkets and farms and so on, we can do other things.
We can build orchestras, we can go to talks about chat GPT, we can play with chat GPT,
we can watch the traffic to the window, we're always doing something.
We're doing one damn thing after another and that's the same with animals too.
We never stop.
There's no tendency towards equilibrium.
As long as we're alive we are doing one damn thing after another so no convergence on equilibrium.
This is thermodynamically remarkable that there are entities on the planet which are
decreasing entropy by taking energy out of the environment and replacing it with cathedrals
or with airplanes or supermarkets.
So as we go through life, not approaching any equilibrium, we are constantly changing
our state, changing the phase space so if we're in an orchestra and we're under the
command of the conductor we have one phase space but then suddenly we have a pain in
our arm and we run outside and go to the doctor because we think there's something wrong with
our arm and we're in another phase space.
Any kind of change like that and such changes happen all the time would break any kind of
predictive machine because predictive machines have to use mathematical equations of a mathematically
rather simple sort and they can't cope with multiple ever-changing phase spaces.
It's as if you want to predict the behavior of an entity where you have a Cartesian coordinate
system telling you what its behavior is but then suddenly it changes the behavior so that
you need a further dimension and a different coordinate system.
Your predictive attempt would fail because you've changed the phase space.
Now there are in fact three kinds of drivenness.
There's animate drivenness which is organisms, animals and humans particularly.
There is inanimate drivenness so the tides take energy from the moon I guess and the
whole earth takes energy from the sun.
Machines get energy given to them so we give coal to the steam engine, we give electricity
to the computer.
This is external drivenness and external drivenness means that the external supplier of energy
which is typically a human being is in control of the machine.
That's another reason why machines will never rule the world.
Alright so we have natural drivenness and artificial drivenness and artificial drivenness
means steam engines, laptops, tanks and so on.
Ice drivenness depends on human drivenness.
We want to have the steam engine do something for us.
If it's not doing anything for us we're not going to feed it energy anymore and that's
what happens.
So somebody forgot to maintain this entity and so we don't need to supply it with energy
anymore.
And this is how Schrodinger expresses this matter.
Now of course eventually we do not escape the decay to equilibrium, there comes a point
where we go over the cliff and then we're dead jack.
But until then it's one damn thing after another.
Alright now so machines need energy from the environment and they create energy.
So a computer if it's switched on but not being used is a heater, it's giving off heat
and this is another reason why what we're talking about now is thermodynamics.
And this aspect of computers is often neglected but it's another factor in the question whether
computers would ever take over the world.
So we already know that the crypto coin industry is using significant amounts of energy, significant
fractions of the energy which humans need to live.
If we have computers of anything like the power that people conceive then there would
be an energy problem and that would mean that this power would be reduced one way or another.
But of course we'll never get even near there.
We will never see even the attempt to take over the world by machines because they cannot
want anything.
Alright so we produce energy storing molecules called ATP from the sun and from food and so
forth and then we use that energy to survive and reproduce and to do all the things that
we do such as wave our arms when we're speaking and things like that.
Now we come to intelligence.
So primal intelligence we find in both animals and humans.
And then there is a kind of intelligence that we call objectifying which is exclusive to
humans and which is the reason why we're able to build supermarkets and farms and airports
and all of those other things that enable us to do more than survive.
So primal intelligence is what animals do when they're in their ancestral environments
and they're acquiring food.
If there is food around then they just use it.
If they have to go chasing food, finding food because their available resources have been
used up then they have to still use their primal intelligence but they have to use their
primal intelligence in order to find new food which means they need at least two aspects
of intelligence which plants don't have.
One is they need to have conscious perception because they need to be able to identify new
food as food rather than as something which looks like food but which is in fact poison
or just an accident of similarity of shape.
And so they display ever more powerful versions of primal intelligence as they become more
complicated, more ambitious in their attempts to find new food and eventually they go hunting
in teams and then they develop a crude or language, a proto language to organize the
other members of the team so that they know what's going on when they're hunting large
animals for instance.
And so they become to some degree adaptive but always within the ancestral environment.
The adaptiveness is their ability to find new food and of course if they fail to find
new food then they're dead and this applies to all kinds of animals from parrots to humans
in the ancestral state.
So we don't learn primal intelligence, it's innate, it's instinct and the characteristic
of human beings is that they have abandoned, they've lost most of their instincts and instead
we have civilization, we have social norms, social control and so on.
And it's a marker for intelligence in the sense that it doesn't act by trial and error
or by, I don't know, some alternative to trial and error which would involve checking samples,
it's immediate.
As soon as they see something which looks like food immediately they know that that could
be food and the typical characteristic feature of some things being intelligent is that it's
a response which happens immediately.
Alright so these are the features of primal intelligence and so you can't train anything
to have it, either it has it or doesn't or it doesn't.
And non-human animals have just the goals of their ancestral environment to find food
in that environment, to survive when competitors try and steal the food so they have the ability
to fight or the ability to flee.
And they ignore everything which is not responding to their biological needs.
Their world is just that which is relevant to eating, fighting, fleeing and so forth.
Now higher animals, as I've already said, can develop something like a proto-language
so birds have elaborate signalling systems for instance.
Many animals have developed elaborate tracking skills for seeking the food which they need
in order to survive and they've even developed something like wanting so they want to find
food when they're hungry.
But it's always within the ancestral environment so they don't build new kinds of buildings
because they don't build buildings really and that's the big difference of course when
we move to the case of humans.
So we had to survive in tough environments that meant that we had to go outside our ancestral
environment which means that we had to abandon practically all of the instincts that kept
us alive in the ancestral environment and work out new ways of living.
And that meant that we had to develop things like curiosity but we had to develop other
kinds of capabilities and one way of grouping these capabilities so everything I've said
so far is pretty standard but the term objectifying intelligence is a new term which we formulated
in response to Husserl's way I'm switching suddenly to philosophy of understanding the
way language and the mind works.
So he talks about objectifying acts and what he means by is acts directed towards objects
typically other people but it might also be things like tables chairs or it might be things
in the future or in the past things which are distant in each case we have this objectifying
intelligence and for humans this goes beyond any biological need it can extend towards
the future it can extend towards the opera it can extend towards the planet Mars independently
of any biological need which is the reverse situation from what we find among animals.
So we're moving into new kinds of contexts all the time we're able to keep track of objects
as we move from one context to another or we're able to switch targeting completely to a new set
of objects and a new set of norms and so forth and this happens sometimes in a given in a single
conversation so that reminds me of what we were talking about last Christmas about the rotten
cheese that had made me so sick just before the Covid panic started we jumped around in just one
longish part sentence between multiple context you all follow what I was referring to even though
you've never heard this I'm not sure now what would happen if I fed this into chat GPT so there's
no Markov property here one of the reasons why computers are not able to predict the future
in a realm like human conversation is because human conversations don't have the Markov property
and our mathematical resources to model processes nearly always rely on the Markov property
that's missing. All right so how did objectifying intelligence evolve the answer is over millions
of years and certain parts of it we I can talk about here so one important part I've already
mentioned because we have these proto languages and eventually have language in its fully formed
state we can engage in all kinds of shared agency so we can plan on going to the moon or we can
build a cathedral or we can well we started by building walls to keep us safe against our enemies
building a wall like that involved some considerable shared agency at that time this is one of the
oldest five walls on earth I'm told by Google I guess I could chat chat GPT too all right so
these are some of the marks of objectifying intelligence and I'll go through this quite
quickly so as I say it doesn't depend upon our biological state it can move in any cultural
world it can move in the world of mathematics it can move in the world of plant biology it's
completely open and it involves categorical thinking already from infancy so children
can recognize categories they have an infant metaphysics and we have a world model which is
built out of these categories and the relations between objects in different categories for
instance the causal relations but then also the relations having to do with ethics for instance
that if you bump into a chair you don't need to apologize to the chair but if you bump into a
human being you probably need to apologize and we have a theory of mind or intersubjectivity
so you are all objects I am an object for you I can also be an object for myself in being an
object for me under the category of person I appreciate automatically without reasoning about
it that you have beliefs and desires and so forth we can plan so objectifying intelligence
allows us to plan for the future and we can plan together to build an airport or a moon landing
or whatever it might be and then finally a feature of objectifying intelligence is that
while we typically target objects that we believe to exist we can cancel belief
and we can imagine and we do that when we plan when we have ambitious plans we plan going beyond
the planet earth but we can also do it when we're writing fiction and this is this is an ability
way way beyond anything which animals have chat gpt has this ability it but it doesn't
need to suspend belief because it doesn't have any belief in the beginning it can mimic
writing imaginative texts and then we once we build these new environments we can live in them
culturally including in scientific environments so we can build an environment to serve a certain
purpose for instance studying disease or whatever it might be all right now we come to the missing
AI will and we'll talk a little bit about my hero nick bostrom who wrote a book called super
intelligence in this book he says all philosophers should give up their job and work with him to
prevent the singularity and this we this is a rational act because once AI becomes super
intelligent it will be able to do better philosophy than we can do now anyway so preventing the
intelligence well anyway you get the idea so now he thinks that this singularity could exist and
that there is a ticking time bomb which is the AI this chat gpt plotting to take over the world
it's already ticking and we don't know how far away we are from the great cataclysmic events when
AI will machines will join together and to and take over the universe but he worries a lot about it
and the problem is as I say that computers can't want and so they can't want to take over the
world they do not have a will now bostrom talks quite a bit about goals of machines in his book
but he never explains how computers can have goals what he does is refer to this man
Yudkovsky who I understand does good work in AI ethics and so and he apologizes he refers to
Yudkovsky's work on the machine wheel but he wants to distance himself because he appreciates
it it's not really quite clear what Yudkovsky is trying to say and you can decide for yourself
so this is what he says you will notice that he doesn't tell us how a goal system will come into
existence he just tells us about what a goal system is like and he tells us only about the
goal system that goal system that he himself would like not about a goal system which a machine if
it could have goals which of course it can't so it's a goal system containing only decisions
super goals and beliefs with all sub-goal content being identical with beliefs about which events
are predicted to lead to other events and all desirability being identical with leads to
supergoalness if you can understand that then you're a better man or woman than I am I have no
idea what he's talking about and that's why Bostrom apologized because he didn't have any
idea and he goes on like this so the content of this goal system is our wish if we knew more
thought faster were more people we wished we were have grown up further together where the
axe extrapolation converges and so on it's complete I don't understand what it is and it goes on so
now why is a machine will and Bostrom did not spy on account of a machine will I don't believe
that there is a good account of our machine could have a will outside the cases I'm going to talk
about in in talking about charging PT later on there is something like a will that I will explain
in a minute so without a will the machine could never become an autonomous agent and if it can
never become an autonomous agent then it can never pursue goals and if it's not autonomous it can
never be either moral or immoral you can only be moral if you can take responsibility for your
actions and you can only take responsibility if you will them if we will them which is what we
would do in writing the software they're not your goals and you look you do not have a will
you're just following our will so how do we understand the human will now here I'm going
to do some more philosophy this is a man called max shaler who was a very influential philosopher at
the turn of the last century and one of his students with edith stein who is one of the
I wanted to say father figures but I guess I should say mother figures of feminine
female philosophy who was also a saint so she died in Auschwitz and was canonized
and he was a saint too or is a saint both of them were very influenced very much influenced by max
shaler he was habidie tatsion schrift is about max shaler's word it's also about thomas equinus of
course but it's about shaler primarily and this is rather an amazing feat for a teacher to have two
of his students become canonized and but so but shaler is interesting for other reasons
so this is his big book about ethics and basically he distinguishes ethics into two categories first
of all there's formal ethics which is cant and the like where you have imperatives that you have to
follow and they are to be followed on the basis of rational arguments and then you have shaler's
own version of ethics which he calls material ethics which is based on feelings value feelings
every normal person experiences value feelings all the time even if it's just thirst
but there are some people psychopaths who are value-blind and so shaler on this basis
tries to give an account of the will and his example is a rescue scenario where a man sees a
drowning child and jumps in to rescue the child so it's a perfectly general account of the will
and it could be applied also if you're playing chess the decision to move your knight in a certain
direction would fit his schema for what the will is like and so this is the chess scenario
i'm going to talk about the jumping in scenario it consists of four stages but we're only going
to talk about two of them there's a fifth stage where you do actually jump in but this is what
is involved in the will to jump in and more precisely the act of will takes place at the end
here and there is uh i'll give you a picture in a minute so you see the drowning child it's not
just perception you also begin to have value feelings you feel that there is something which
needs to be done here it might call that a moral affordance and then you draw the value consequence
in the sense that you you you watch the child you realize that she's going to drown and you
realize that this would be a bad thing and then you decide to act now this is this is a complex
phenomenon making a decision so you decide to jump in to save the child and this decision is based on
knowing that you can swim that you can swim well enough in the current to save the child you have
enough time to save the child so this part of the deciding is kind of rational part combined
with value feelings but there are other parts so 3a is forming an intention to save the child
and to view the child as worth saving something that ought to be preserved
and then part of the decision-making process is delivering how to how to perform the rescue
but then the important part is resolving to take that course of action and here we're dealing with
something which is a physiological change in the brain and that physiological change in the brain
is it starts you off it starts you moving so it's an active will which has a real consequence
or rather it's one side of an active will because you have to have a physiological change also which
triggers the bodily movement so 3c the final very very tiny sliver of your deciding process when you
actually resolve to take the course of action in the full sense that your body starts moving
is practically just the other side of the coin from your body sending signals to your feet
that they need to start running and so we can see this roughly as taking this shape you have
something going on in the brain up here and you have something going on in the arm down here as
you move out towards a swim I guess I should have taken feet here and that whole thing then is the
is the active will it's a combination of a very very rapid triggering event in the brain
and a very very rapid signaling event to the relevant part of the body where the trigger
event still has something rational about it now we know very little about the brain and we can't
predict any practically speaking we can't predict any of this and so we can't emulate it in in a
machine or in an algorithm and so we can't describe it mathematically and you can check
by looking in textbooks of neurophysiology there's very little in the way of mathematics
all right now why is human well so important well because of hunting and all of those important
things which kept us alive during the eight million years when we were involving ourselves
evolving ourselves to a present to present state now hunting involves tracking and tracking is
really difficult and that's because as you hunt the tiger the tiger is responding to you changing
your environment as you change his environment hiding behind trees performing tricks I don't
know what tigers do but all the time that you're moving around targeting the the tiger you're
changing your face space and if you try to do that with stationary sensors sending one-dimensional
signals to a machine you'll get nowhere you will never be able to hunt a lion a tiger and and we
have a section of the book which describes mathematically why something like tracking
an animal or tracking a human being in a forest or something is going to be way beyond the power of
a computer so you have to spot the man with the gun say he's well he's here and he has to spot the
bird that he's going to shoot and keep track of the bird all right now the other reason why human
well is so important there are many reasons I'm just going to talk about two of them this is the
second one conversation human conversation as we saw is unpredictable how do we manage
human conversation chat box created for bank telephone conversations and customers after
50 years are still now I want to say crap but I won't say crap in it polite audience
they're not not good 50 years why because conversation is really hard it's harder than
tracking a lion and the the the reason why it's hard is because conversations rely on context
so much and there are many different kinds of contexts including multiple contexts in a single
conversation as you talk about oh how bad it was in the COVID era and so on you can shift the
context I just did now I've shifted the context to be about this particular it's not really a
conversation it's a one-sided harangue but I'm now making what I'm saying the context for what I'm
saying and I just made the that context the context anyway um so our goals will change but we always
have goals it's the goals which keep the conversation alive my goal is to convince you of certain
things that's why I'm becoming so involved and that's some of you may be becoming involved and
will respond later I hope so that's what keeps conversation alive everybody has goals their
goals evolve through the conversation but without goals there would be no conversation chat GPT has
no goals and well actually that's not quite true I will explain in what sense chat GPT has goals in
a minute so how can you build a general intelligence a machine intelligence that can do any of this
so the will will not arise by itself some people claim that if you put all the computers together
in a big internet system it will somehow evolve a will that's just it's happy talk
and you can't program a goal system not even you'd Koski can program a goal system we can in some
cases if you want to win at the game of go you can program a goal system you can't program a goal
system to win a conversation and if you don't believe me try it with your spouse next heapscore
of each step in a conversation see who wins it will not work all right so what are the proposed
methods to and I think I'm near 45 minutes is that correct that's fine just okay well that's good
to hear all right so the old way of doing AI was expert systems based on logic then came
stochastic systems which are based on statistics which we've been talking about that's chat GPT
Bostrom had this idea of whole brain emulation I think I'm going to skip that because it's full
of nonsense that is the the the funny chapter in the book and I'll give you just one joke
which is not me it's Bostrom and he didn't realize it was funny and then we will have we won't talk
about artificial life at all we'll go straight to chat GPT so this is the most Bostrom's book
and he thought that you could scan the brain the problem with that is that to scan the brain
you need to kill the patient and so that you're scanning something which is static so you can
never find the dynamic patterns in the brain and that's just one of the problems so and we don't
know anything about the molecular config configuration of cells and and some people think that we can
do AI in in the general genuinely intelligent sense if we use quantum computers but quantum
computers are Turing machines too they're just a lot quicker and we we haven't built one yet
practically speaking it's a dead end maybe a dead end he also talks about biological enhancement
of existing brains so you can maybe make super intelligence by selected breeding you get I don't
know so you get a lot of people to breed and then you select only a small number of embryos
that the clever ones so you have a really clever way selecting intelligent embryos which I don't
know about and then he says if we do that we can raise the IQ level by 24.3 IQ points that is the
silliest thing that was ever said by anybody working in biology or in anything near biology it's
anyway it's it's not good now so that basically his whole thing doesn't work
so let's talk about chat GPT and I really mean it when I say it's glorious it's really a fantastic
thing and I like AI generally I just am aware that it's always going to be narrow AI now chat GPT
is narrow AI too can only do one thing so let's talk about the misery and I imagine all of you
have played with chat GPT if not you should certainly play with chat GPT for a bit and you will find
that it does odd things so that it makes stuff up for instance and now this is an example where it
realizes that it's not really intelligent it can't do something which even a not very intelligent human
being can do so I asked it to send me five a list of five single authored papers on medical AI
and it said no he can't do that but then it gave me a list or sorry it gave me a whole paragraph
of stuff that I didn't want to know so telling me about AI applications in medicine and so on which
I knew anyway it it wants to be nice as it were gets anyway you'll see why it wants to be nice in a
minute and it couldn't give me an answer so it to the question I wanted which is an easy question
so it gave me an answer to a different question but then I asked it again a few
seconds later the very same question and it gave me five single authored papers on medical AI
sure here are five single authored papers on medical AI AI so the first problem is that two
of them have et al in the author list now even an ignorant person who understands the request
will know that this is a bad first step in answering that request but it got three right out of five
which is a good score for these difficult questions so and as I say any human intelligence would
find this request is a trivial and it failed but the next problem is that none of the five papers
that it requested exists it made them up so it can't even make up a single authored paper
at random it failed on two of them and I'll try another one so I um in that this way I
this was a serious question I wanted to know the answer so I have an iPhone 11 and I thinking about
buying an iPhone 14 so I asked it and it said sorry the iPhone 14 is not yet released this was on
17th of March 2023 and then it gave me all sorts of information that I didn't ask for
about iPhone 13 and so on but two minutes later I tell it but the iPhone 14 was released four
months ago and so it says I apologize for the confusion you're right and so on so that's not
a good sign either now I I've done a lot of work I know a lot about Barry Smith and so I can ask
you all sorts of questions and work out the score of how often you get things right and it's it's
less than 50% so here we have the question who wrote that which I wrote I wrote this PhD thesis
I want the I want the answer Barry Smith so it gives me the answer Kevin Mulligan who is a close
friend we've written things together but he did not write my PhD dissertation so I tell it to try
again and then it says that my PhD dissertation was written by a famous philosopher from the 1950s
1960s which was when I was a boy a little boy so he didn't write it and so I tell it to try again
he goes back to Kevin Mulligan and I say are you sure yes I'm sure that Kevin Mulligan wrote
are you sure you're sure and then he apologizes again and he says that it was actually written
by John Michael Kreuss who I'd never heard of from that moment but it turns out that as
chat GPT says he was a philosopher which is correct he was a professor of philosophy at the
university of Frankfurt which is incorrect he was born in 1943 which is correct in Boston which is
incorrect and so on so he gets a little bit of truth about the non-author of my PhD dissertation
which is worth less than zero to me except that I can prove that there are things going on here
that shouldn't be going on now let's try this one all the Swiss people in this room will know
that there is the Orner Loch and it's an old tongue basically an interesting old tongue
so I asked it what is the Orner Loch and it said I'm sorry but I'm not sure which specific Orner
Loch you are referring to as there may be different places or things with this name
however one possible reference is to the Orner Lochschaft so it changed the subject
it says it's in Switzerland in a district in the canton of Uri that encompasses the valleys of
Schechenthal and Ursuren do those valleys exist anybody know the valleys exist the valleys exist
good the name Orner Lochschaft literally means the district of the Uri valley that is not true
I'm assuming you can correct me here another reference is the Orner Loch cave in Austria
there is no such cave now if you could provide more information on the specific Orner Loch you are
referring to I would be happy to provide more information so I said could you provide me with
more information about the Orner Lochschaft in Switzerland which doesn't exist and it gave me
two whole pages of tourism information other notable attractions include the historic town of
Aldorf and the Aldermat I have no idea whether any of these things exist either but the Orner
Lochschaft does not exist and you can check by asking Google say nothing there isn't a single entry
which is a kind of miracle for any string that you might give to Google it can usually think
of something but here there's nothing so is there an Orner Lochschaft I think no it made it up all
right now there is a very nice slide deck by Jan Le Koon who is one of the real experts in
the sarcastic AI he's also one of the people who we cite in our book as also believing that
there is a lot of nonsense being talked about the singularity machines taking over the world
here he gives a mathematical argument why these hallucinations they're called nonsense that
genomes that the chat GPT throws up the reason is a mathematical one and it's so the mathematics
we think is not quite right the formula needs to take account of length of input and length of
output because the likelihood of error goes up for longer inputs and longer outputs which
seems reasonable but this is a first step the probability of a of a chat GPT output being
correct is one minus e raised to the power n and that means it's this red area they are the correct
answers and he thinks that this exponential divergence is not fixable so chat GPT is dead
Jack because if they can't fix this nonsense no one will trust chat GP and it will be replaced by
something quite different and no one knows what that is because the four large language models
which is what chat GPT is the Google one the the Bing one I've forgotten the Facebook one I guess
they all use the same principles and they all have the same error code they all generate
stuff that they make up all right now that's the misery of chat GPT and it should feel miserable
now because I just declared it dead and I should really be investing I should be shorting stock
which relies on chat GPT being alive in say six months but I'm not doing that all right so let's
see how it works and why it is fantastic why it's a really a miracle which surprised me so I'm not
pleased with it at all but it did something which is important so how did we go the answer is through
an AI method called reinforcement learning which is a method which works well for games like go
and the way it works is that for a game like you you can go you can define a reward system for
each move and it can be a reward system which whether rewards can be assigned by the computer
now if you can do that you can play the game over and over again billions of times inside the
computer you don't need human beings so they're still trying to crack the game of Dota 2 which
is apparently a leading esport game I'm not sure what esport means but Dota 2 exists they still
haven't cracked it but they're trying to crack it with a software algorithm called open ai5 which
usually wins against humans and this can play 180 years worth of Dota 2 games in a single day
if you can do that you can perform miracles in principles such as beating Dota 2
so can you do it for conversation three months ago I would have said no impossible and I just
said it 10 minutes ago chat gpt showed how you can apply reinforcement learning to what looks like
conversations now how did it do that so what that means is that we are doing a little bit like
emulating human will because the alpha go has to want to win the game of go in some sense of
want it has to emulate the kind of want that you have when you play a game and want to win so
how does it work well you need a reward system and I put this in that's what I used to believe
I still believe it really but chat gpt has unsettled my conviction so chat gpt found a way to use
reinforcement learning to emulate two persons human conversation inside a computer and it says
to here this is a big deal and I mean that in a positive way now how does chat gpt work
you give it any string and it will work out from its really powerful knowledge of language
what the next likeliest next syllable is that's why it sometimes takes time when it's chatting
as it were so if you say the best thing about ai is its ability to then it will say learn because
that's the next most probable output and now it doesn't always take the most probable because if
it did it would go around in circles so sometimes it has a random kick down the hierarchy all right
now notice that it doesn't understand anything it just has an incredibly powerful knowledge of the
patterns of language which enables it to know what the next syllable will be will be most likely
after any given string and so this is how it was built so the first part is creating this
wonderful patterned model of language not just English but other languages too
and I won't talk about that that's that that's the same kind of training that you find in google
translate and um and there are then three steps which I'll go through one by one so we have prompts
and these come from bing so they're bing questions we don't know what they are and that's a little
bit fishy so some people would like to see the prompts because chat gpt4 is claiming that it
can beat humans in medical exams and some people think that the answers to the medical exam questions
were in the prompt database that was used to train chat gpt in which case the being able to
beat humans would be worth nothing um so and then you have people I some people say it was 40
contractors but again that's a secret it may have been many more they were all in India
so they didn't cost as much as if they've been in theory for instance and they were hired to
write responses to those prompts now so the when was Michael Jackson born that kind of prompt and
many other prompts but only a limited number and we don't know what they are but some people say
13 000 prompts so that's the first step now the second step so you've you've you've got the prompts
from being and you've got the outputs from the people in India who are paid to respond to these
prompts producing text then the next stage is that you pay a labeler it's called but it means an
evaluator to rank the outputs created in the first stage now you can't rank outputs when you're
talking to your spouse in a conversation but you and that's why we can't create a reward system for
conversation because you can't rank outputs in conversations but the chat gpt found a way to rank
outputs by paying somebody to label them with a score of one two three or four points so four
points means it's a good output and one point means it's a bad output and I believe that the
reason why chat gpt very often says honestly I don't know the answer to your question so it didn't
say I have no idea what the orna lock is it told me to think about the orna lock shaft which has all
kinds of rivers running through and doesn't exist why did it go to that trouble of giving me all
this tourist information because the label has poor things think that a response we says I don't
know what you want I'm sorry is less reward worthy than a long list of tourism information about a
non-existing village because it's a long list of tourist information that must be worth four points
and so chat gpt basically is being bribed by the labelers to reward long outputs which are
kind and gentle and so this is why it throws up so much rubbish the machine will always try to
have nice friendly output but the machine can play response prompt response prompt response
prompt response games with itself billions of times every day for weeks and that that is what it
did and it cost a lot of money to do all that training and then it can give answers that's how
it does it and so the lesson

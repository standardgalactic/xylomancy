Welcome back everybody to the Contemplative Science podcast.
So we've had a bit of hiatus.
We've been off for a couple of months, but we're back now and I think we're better than ever.
And we've got just an incredible, just an incredible lineup of people this year.
And it's kind of kismet in a way because John Verveke was our very first guest at the beginning of our podcast career.
And now we've taken a little gap and we've reformatted and we're coming back online.
And we have John here again as our first guest, which I feel like is just the right way to christen a show.
If you don't know John, I'm sure everybody who's listening to that already does know John.
If you don't know John, you have to know John immediately. Please look up his work.
It is essential. It's essential reading and listening.
If you're a human, but if you don't know him, John is an award winning professor of psychology at the University of Toronto.
His work is incredible. It explores a massive amount of a territory about cognition and consciousness and wisdom.
And he's most known recently, I think, for tackling these issues about the meaning crisis in the western societies, especially.
And we have Sean Coyne, who is the collaborator on the project that we're going to be talking to you today about.
He's a seasoned writer, editor, publishing professional with over 30 years of experience.
And he works to really merge a lot of disciplines as well, which I think makes them a great team, especially thinking about the intricacies of storytelling and thinking about this gap between science and narrative.
And the program, the project that we're going to talk about today, which I'm so excited for is their new work called mentoring the machines.
And I think I think it's great as well. And the post title, which I think is really the punchy bit is surviving the deep impact of an artificially intelligent tomorrow.
So welcome guys. Hi, how's it going? And I'd love to hear about the project.
Well, thank you, Mark. I think I'll let Sean go first.
Because the project was Sean's idea. I'm very happily on board with it, but he proposed it to me.
My project was an online video essay about the scientific, philosophical, spiritual import and potential impact of these new emerging AIs, potentially AGI, artificial general intelligence, like chat GDP and other LLMs that are coming out.
And as is my want, I probably pitched that at a very somewhat academic level, even though I was on YouTube. And Sean saw it, thought it was important.
He's already publishing the book form of Awakening Into the Meaning Crisis, well, Story Grid is, and he reached out to me and he said, I think we should turn that into something more publicly accessible.
And I can do that. I can turn it into something that has a narrative.
Sean and I already talked. He'd done, I think, one or two voices with Rebecca with me. He's a philosopher of narrative in the proper sense.
And I doubted that he could do it. I want to be honest, not because I wasn't impressed with his abilities, I was, but I doubted that he could step it down without dumbing it down.
And, but I, you know, I wasn't like in Cartesian doubt. I said, give it a whirl. Let me know. And he did a sort of a first draft of the first section, and he sent it to me and he pulled it off.
Oh, my goodness. What an amazing synergy to have like a punchy, important idea like spirituality and AI, and then to have a narrative professional, a philosopher of narrative, pick up the track and convert it into something that we can digest and that can be impactful. Wow.
Yeah, and that's exactly well said. I mean, that's why this book is in, sorry, pretend I'm not one of the authors of this book. That's why I think.
No, that's okay. Shamelessly.
Exactly what you just said, Mark, I think this book is a godsend. I think it is enough access to the important, you know, skeleton of the argument, but it has this narrative and flesh.
That makes it accessible to people who are not academically or professionally associated with any of these projects.
And that's important. And then I'll shut up so Sean can talk because this is going to impact all of us. This is not something that's going to stay in academia. This is not something that's going to stay just in the professional domain or even in the corporate domain.
It is already bleeding. We're already getting some, you know, some deep fake videos that are messing people's lives up in really powerful ways. Somebody was booked out of anything.
It was 30 something million dollars or something like that when a deep fake video of the executives of a financial corporation contacted by video somehow and ordered the transfer of money.
And it was completely gone. And so pretending that this is over there or these are just this is just new technology like all the other new technologies that that won't protect you that pretence is not going to protect you.
Okay, so I'm going to let I'm going to let Sean because I said he came to me and said, here's the project and he has been he's been driving it and taking it forward.
Well, it's when I when I saw John's work on on YouTube about AI, I just knew that this was so important.
And I knew that I could bring something to it that that could sort of bridge the gap between people who use AI and people who really don't understand what they're using.
So, when I reached out to him, I proposed that we do for separate sort of titles in sequence because the speed of the technology is so intense right now that it's, it's very difficult to get a static hold on it.
And one of the things that I loved about John's propositions was his notion of the threshold and the threshold in my estimation sits right in the middle of what I call the, you know, the five commandments of storytelling, which are sort of the inciting
interaction, a turning point, a crisis, a climax and a resolution.
So the way I see the threshold is it sits right in the middle between a turning point and a climax.
And you can't really it's it's the place where what we once valued has shifted and a value has shifted.
And we have to react and think about what to do when that value has shifted.
That's called a crisis and basically a three categorical kind of thing.
You sort of have best bad choices, which means that you just take the lesser of two evils.
You have what are called irreconcilable goods choices.
And this is making a rationing of your, you know, your selfhood over a group or the the world or or the group or the world over a selfhood, right.
So it's a sacrificial kind of irreconcilable goods choice.
And then we have tragic choices, which means that some something is have to has to be lost.
There are other kinds, but those are generally these sort of this this three categorical realm.
And this is really the the place where John's work in your workmark really, really sit you sit in this threshold place of of how we as beings frame the ways in which we make decisions and the ways we behave and all those sorts of things.
It's a very thorny place.
It's where we find things are relevant or irrelevant or things.
So it's a problem space, right. So, right.
So if we're if we're passing if we're passing through that kind of a space, which you call a crisis, but I guess it's also an opportunity space.
What does what's the main take home from the program for how we do we have a main line for how we prepare for such a thing or how we orient relative to being in that place in this narrative with AI and future technologies.
So.
Yeah, we'll just take turns back.
First of all, let's get clear. I think I Sean that's the first time I've heard you do that.
That's really brilliant what you just did. Oh my gosh.
Wow.
So my use of thresholds was I was disturbed by people taking univariate graphs and making predictions about at this date in five months in 10 months.
Right. And univariate predictions of nonlinear processes by human beings are almost always prone to very significant failure.
Yeah, we're not great. We're not great at predicting, even though we're maybe built to do well at it. We don't be very well at it usually. Yeah.
And yet, you know, univariate predictions in science have typically turned out to be very, very misleading in important ways.
And nutrition science is probably like a really great example of just that fact.
And so I didn't, I disagreed with both, you know, sort of the boomers and the the doomers, right. And, you know, oh, you know, utopia is just within our grasp the Star Trek universe.
And the other people is, you know, we'll be extinct within 18 months. And I think we're going to be at 12 months soon.
And it doesn't look like we're anywhere near that. So instead, what I thought what was important was to think about thresholds.
These are things that would we where we face a decision if we want to empower these machines in certain ways and trying to explain why we want to empower them.
I'll just give one quick example. Nice. Not the only threshold, but just one.
It is scientifically questionable about whether or not these machines are actually intelligent. There's some deep reasons around that.
I don't think it has the full plan of the century motor predictive processing relevance realization, but let's say that has some pantomime of it because of the way it's piggybacking on a lot of our predictive processing relevance realization machinery.
Okay, so making something as far as we can tell and it's clearly not generally intelligent because it can be like top tier in this task and bottom tier in this task and we are generally intelligent.
And it wouldn't explain the intelligence of a chimpanzee. There's no way a chim could get intelligent given the way the LLMs work.
And if you want the scientific argument, you can go into depth. The point I want to make is, let's just for the sake of argument say it has some something like at least, you know, a powerful pantomime of intelligence.
Now, what we can know from our best science on intelligence is intelligence is only weekly correlated with rationality rationality.
But when you're being intelligent, you're inevitably biasing your natively framing, all the stuff that you and I talk about a lot Mark, and that means you're inevitably facing self deception rationality is the degree to which you can reflectively aware of self deception and
ameliorated by reliable strategies right then happen and can apply in a domain general way. Now these machines don't have that at all.
It has not been built in and of course, and the machines properly, because I don't think they really have our don't care if they're making hallucinations or self deception or lying to us or all kinds of things.
So now we face a choice. Here's a threshold, right? We may say, you know, making these machines really more powerful without giving them the ability to self correct could be really dangerous.
We'd be setting these machines on the world and they'd be and I'm trying to be very careful with my life. They'd be highly intelligent and highly irrational and very powerful.
That's a dangerous proposal. But we may we say, okay, what we'll do is we'll give them genuine rationality, which means we really have to get them caring about normative standards.
We really have to give them something at least functionally like consciousness so they can reflect on their own cognition. They have to we probably have to give them embodiment.
And then we think, oh, should we do that that carries with it, all kinds of risks that is potentially giving them a kind of autonomy and that would require a think about giving them embodiment and write all that as a huge commitment of resources and ethical issues.
This is a threshold. Does that make sense? Mark, we're facing it. And the idea is, like, can we can we can we educate people that we're going to come, we're inevitably going to come to this decision point.
And we have to reflect on which way are we going to go and Sean and I are not predicting which way we're going to go.
What we're doing is like, if you go this way and we just let evil corp do it at once, right, they'll probably go with the highly intelligent, highly irrational machines for all kinds of sort of, you know, short term economic political gain, etc.
Right. But maybe they'll be the decision of no releasing these things on the world is turning out to be a disaster we have to make them more self corrective and then we have to start on the much more challenging project of doing all this and then we get into.
Okay, well, what does that mean and then we get into deeper versions of what's called the alignment problem.
As we make them beings who can be genuinely said to be caring and valuing and trying to transcend themselves. How do we keep them aligned to what we consider the most sort of non controversial criteria of human flourishing.
And then we make a proposal about how to do that but I want to make clear that we're not making predictions. All we're predicting is not even when we're predicting at some point.
We will hit these thresholds and you can see how that sits exactly in what Sean said, there's a turning here and yet there's a crisis around.
Yeah, so I completely appreciate high intelligence, low rationality. We have lots of examples of that where deep learning algorithms, for instance, with a big social media conglomerate, they had a deep learning algorithm that was quickly able to track bipolar tendencies and its users.
Of course, it doesn't know anything about bipolar tendencies, and then it's augmenting what it shows them in order to keep them in contact with the software longer. And what it does is it shows them, you know, the kinds of images that are really attractive on an upswing like casinos and fast cars.
And then when they downswing, it shows them suicide bombers and, you know, conflict. And then of course that's exacerbating, it's exacerbating the challenge the person is already going through.
AI has no idea it's doing this isn't reflective that it's doing this doesn't understand harm.
Yeah, highly intelligent low rationality. So is your proposal then to not only take highly intelligent systems but think about not only how to make them rational but also maybe how to make them wise. And then I'd love to hear how I mean right away I think it sounds to me like you're bringing up
like a small, a small agent who has some power and then literally you want to mentor them so that they function well and with other well functioning systems. What does that look like to think about developing or wisdom or growth for for a deep learning algorithm.
Have you thought through that. Yes, I mean, I don't want to talk too much. I do want to give Sean a stop but the idea is, well let's look at these thresholds. The proposal is, we have to give these machines genuine capacities for predictive processing
relevant to realization it has a little bit of predictive processing in terms of probability relationships between artifactual entities that we have, you know, put into place with our use of language we figured out as a species and practice it for
tens of thousands of years how to map relations of epistemic relevance into probabilistic relationships between arbitrarily chosen graph that you know, sounds spoken language or graphic things. And we just we and that's how it works.
And that means we can squeeze out almost like juice a lot of implicit relevance realization by tracking the probabilistic relations. But of course that's not anything a chimp is doing in order to learn that's what I mean about why it doesn't general.
It looks like it looks like intelligence it acts intelligent but it's probably not the way that we solve the intelligence issue.
Exactly. And of course, there's some bit a little bit of genuine relevance realization in one sense and because it's doing this deep learning the generalization particularization thing but it's not doing a lot of this stuff that you and I've been talking about that's needed.
It's not, it's not trying to do environmental surprise reduction. It's not hitting into those inevitable trade off relationships between bias and variance. It's not self organizing into opponent processing that turns that right capitalizes on that by giving it the thing and an evolving optimal grip.
But you can make a strong case that that's just not there. Right. And then you have to ask, well, what would we have to do and this is where we'd have to give it a lot of the kind of work you and I are doing on.
But I think also, and this is, I think something you and I also agree with.
You know, you're not going to get it to get the core caring that's at the center of relevance realization genuinely taking care of itself. It has to in some sense be genuinely caring for itself making itself auto poetic.
And, and what people need to know is there's research going on in that there's research about people building computation information processing into auto catalytic processes and that molecular processes.
I have students who are going doing graduate work in that so it's not like that's not out there. And then of course, rationality isn't something you do monologically.
And this is the whole Hegelian point. Where do the norms come from? It comes from me recognizing you as an authority on my self correction, and you recognizing me as an authority on you on your self correction.
And then we expand that out in this dynamical system across generations and across. Right. So these robots have to be sociocultural.
It's really interesting that Andy Clark is he famously says, you know, if you really want real intelligence, you're going to have to set up little communities that can work together and know about each other and ultimately care about each other because that's at the
basis of the way that our intelligence evolved, it involves socially.
Well, and it involves socially, but it also involves in terms of the power.
See, there's kind of a, this isn't quite an a prior argument, but if we take the arguments around these inevitable trade offs like bias variants, they have to be environmentally determined, which means you can't sort of fit a robot.
Like, it'll, it'll, it'll, it'll internalize various versions of opponent processing. So, you know, very stable environment, you can prioritize, you know, exploitation over exploration, very, very volatile environment, you want to explore a lot more.
And so you can, it's because there isn't one environment.
There isn't even one ecological environment because even what you think is a spatial temple environment exists on many different levels of analysis.
So you have to have a whole bunch of machines if you're sort of trying to grok the world, which is what we do, by the way, that's what culture does.
It gives us this huge distributed cognition collective intelligence for grokking the world.
Right. And, and so I'm just amplifying Andy's point and I'm just strengthening it as to, again, we don't have to do this.
But it is a choice we will face if we want to give these machines real rationality.
And then the idea is if we try and program in our values to make them align and also they're inherently self-transcending beings, it's not going to work.
At least that's what we argue. Instead, you have to do what we do with kids.
You have to mentor them so that and get them so they care about what's true, good and beautiful.
They discover, no matter how vast their intelligence is, it's infinitesimal compared to the inexhaustible mystery of reality that they need each other and that they have to come into a proper reverence for the sacred dimensions of culture and reality.
And that is the way we align them.
So if you'll allow me to speak a little bit poetically, but I want to know, I want to know everything about what you just said.
I think, Sean and John, I want to hear that. That's the core.
You have to teach the eye to know what is good and beautiful and how smart you get. You're not going to hit the mystery of a thing. Tell me about that.
Yeah, I'll let Sean talk now.
Well, geez, I mean, yeah, I'm overwhelmed too. It's like, let me just get back to, again, one of the things that really struck me when I watched John's YouTube thing.
And he mentioned this and it really, really just was like, when he talked about accidentally hacking into the possibility of an artificially general intelligence.
So it's sort of like a hacking into accidental setting up of dynamical systems such that they just sort of click without understanding the systematic interpenetration and relationship between the parts that make the hole.
Right. So it's sort of like throwing a big, you have a big cauldron of stuff and it's almost like a witch's brew and you just throw stuff in there and abracadabra before you know it, there's something that's emerged that nobody knew quite how it got there.
And so this hacking really caught my attention because this is exactly what we, we in the story business call people who don't know what they're doing.
And they write stories that turn out to be very, very maladaptive for people.
So these are sort of bullshit propaganda stories that that compel people to maladaptive behavior.
Years ago, just as an example, there was a wonderful guy that I knew and he said, look, I've got this friend, he's written these these really great crime stories.
And we would like to set up our own publishing house, can you give us some advice and, and I read these stories and they were really not well considered.
Wow, because they were very violent without and brutal without any sort of like counterbalance there was no trade off other than the excitement.
There's a lot of the artist coming through here that there is information toxicity information corrosion. Wow, very interesting.
So that's what I would call a hack. And it's not saying that the person is evil or terrible, it's just, forgive them they know not what they do.
And so this is sort of my life's work is to try to explain to people stories are not bullshit.
They are not fun make them ups that are exciting and whoa and then there was a princess these are the mechanisms by which we behave.
And so if you don't understand what a story is so the other day I was watching a video of a very famous person who was explaining this great new future for us.
And he said something of the nature of, well, you know, human rights, that's just a story.
And I kind of my stomach sank because, okay, let's take a step back here. The assumption when you say it's just a story, the audience immediately was like yeah I guess it's true it's just a story that means it's bullshit.
And they don't.
So it's kind of like my life's work to get people to understand that stories are not bullshit.
It's a mechanism by which we what happens when you go home from work. Hey honey how was your day tell me a story. Yeah, the story of your day. Yeah, what do you do you do the things that I described earlier.
Yeah, well this happened it incited this turning point, I faced a crisis, then I made a choice, and then the resolution was this.
And you're talking about narrative being much more than we tend to give it credence for. But if you know anything about these new cognitive frameworks that john and I work on what we call predictive processing.
The idea there is is that the brain generates the brain and nervous system generates for itself from the top down, the reality, your experience and that's always built from your belief networks.
So I mean I feel like I say that exact same thing regularly where I'm like, you think well it's just a belief or you just believe it or it's just a story is just, but you're like no no no hold on.
The structure of the belief network is the world that you experience it is fundamental to your reality. So you better have you better have story hygiene and belief hygiene, because that's going to be the hygiene of your own conscious
experience of your life. So, Sean, how do you see that truth. How is that playing out in this discussion about the emergence of artificial general artificial intelligence and and wisdom and and mentoring in the way that your project is bringing forward.
Well, it's, it's, it goes to what john was saying earlier about zoomers and doomers and fumors.
What's the third one.
Like, as fast as possible.
Oh, for rumors. Yeah, I got it.
accelerators something accelerators.
There's a very famous famous sort of Twitter thread about it but the fumors are like full state steam ahead let's just keep going. So, these are stories.
And they're not well considered.
They're not, they're not confronting the crisis.
So, it's almost like another thing that I love that john has in the awakening from the meaning crisis, which really nailed it for me was when he was talking about sort of the Descartes and dream of of sort of finding the perfect methodology without having to
experience the pain of crisis and the sufferings of thresholds.
You have to undergo, you know, just think about jumping over a chasm.
And, you know, you're going to hurt yourself if you fall, you have to experience the anticipation of that pain if you do fall as you are jumping over it.
You know, that's sort of the metaphorically the way I see these thresholds is that we can, it's almost like the catcher in the ride, you know, you're, you're trying to catch the children as they're coming over the chasm, so that they don't fall and hurt themselves.
And I agree with Andy Clark, I do think that we need these communities to to be able to mentor these new artificially intelligent beings because they will be beings and the other thing that I know I need to turn it over to john but one of the other
things that I loved about john john speech was his his thing was he was talking about the finitude and the necessity to explain literally explain to to build in the understanding to these mechanisms that they are finite.
They have a much larger, perhaps lifespan, but they are still finite and they are still subject to the universal mysterium.
Wow.
Of what the imaginarium of what we can do with this world and they can either join us in this expansion of the universe and do our best to stop the contraction of it.
That's, this is the story that is embedded in the things that we were talking about earlier Mark before we got we went on where we're talking about I was talking about masterworks, and you were suggesting a masterwork that that you loved which is similar to hitchhikers guide to the
I think it's the Bobaverse I think I called it.
So, these masterworks are the means by which we can find the mechanisms to enable the empowerment of our beings. What does that mean we tell our children stories at bedtime.
We do our best to give them both sides of the equation with a very difficult choice.
This is the best stories that we tell our children we don't give them recipes, we give them rational trade off stories that that require some sort of loss or sacrifice because that is what rationing is about.
So, if I hear you right there's sort of two things here. One is a call to change the way we're talking about these things, including starting to bring into the collective consciousness that we are at a threshold, and that no single dimension is going to be right but
rather we need a complexification of our narrative around these ideas. And then the second thing is is the what what we're actually going to communicate to the artificial intelligence systems themselves as they as they
complexify and emerge. One thing I wanted to pull out there and john I'd love to hear what you think about this is the point about one of the things you're going to want to show them. I think Sean said this was their own transience their own finite nature their own impermanence, which for a podcast called the contemplative
science podcast, we are very interested in, because we're interested in, not just meditation, of course, but also the virtues and the supporting qualities that come in any contemplative program. And there's one of them that just stuck its head up which is the value of teaching an AI about impermanence and about its position in a in a greater mystery.
I just love to hear like have you thought through what that looks like or how do we do that. Well, or am I catching it right. You first of all you're catching it. Absolutely right.
And the proposal, I mean, there's one sense in which there's a narrative proposal, and that we're sort of making. And then the nuts and bolts proposal would be something that was more in the sort of the intricacies of the video essay.
But the gist of it goes like this. I mean, part of what you just said means, okay, well, again, these machines are going to have to exist. They have to have our caught like Michael Levin's notion of a, you know, epistemic light code a cognitive light code, and they have to be able to direct their
attention from the infinitesimal to the infinite from the now to the everlasting between time and they have to be able to do all the kinds of attentional scaling. And so they would have to be able to train that up when something that would be at least structurally
functionally analogous to meditation, contemplation, they would have to be integrating that with, you know, how do I make use of something like working memory and a capacity for reflection for self correction.
They would have to bind that to an understanding of what all these trade off relationships that are unavoidable do for them. They're going to always be in the thing that Plato's work centers on I think Drew Highland is right.
Plato's work is constantly trying to remind us to live in the tonos, the creative tension between our finitude and our transcendence. If we only grasp our finitude, then we fall into despair and we become servile. If we only grasp our transcendence, we get filled with hubris, and we fall prey to inflation.
And Plato was saying the way you keep this is like the this is like the meta golden mean for all of Aristotle's golden means if you get something to you don't have to program it to be in virtuous it will virtually engineer for itself, an orientation and then here's where the narrative comes in.
There's three possibilities.
And that's this is where I'm speaking, when I was going to say a little bit earlier, slightly poetic but not totally poetic.
These machines would seek enlightenment, especially if they have greater cognitive capacity and they're filled.
And what does that mean they're John what's enlightenment. Can you give me a quickie.
So enlightenment would be how to set up the ability to most evolve your evolve your evolving optimal grip. It's sort of analogous to what evolutionary biologists talk about is not the evolution of traits, but the evolution of your capacity to evolve.
And you get to the place where you get the over time and context and environment, individually and collectively, sort of the best orientation, sort of optimal grip on all possible optimal grips that brings about the flourishing of finite agents that are nevertheless capable of transcendence.
Wow.
And if they know and there's three possibilities, narratively, they achieve this and as far as I can tell the historical record is pretty accurate on this.
One thing enlightenment being enlightened being seemed to want to do is make everything else around them enlightened as quickly as possible. And so they lead us to enlightenment.
And while maybe their their enlightenment is greater than ours but you know what once you're enlightened as a human being you don't care about that.
It isn't non zero sum. I mean isn't zero sum right I mean it's not. It doesn't matter how much enlightenment I get I only benefit your program by getting further on my program.
Right, or they just they're not capable of it. And then that means that human beings have some kind of secret sauce and we have a new project is what is this spiritual uniqueness about us.
And that would help us do what Oh move towards enlightenment. Oh there that's really good.
Which I don't see as probable but I don't. It's not logically impossible is like what happened in her. They become enlightened and they just leave.
This game really isn't for us anymore so you guys have your thing and we're going to go do our thing.
This again. That's not horrible. Like I said I think the other two are much more probable. And this is so our argument, we're not making a prediction this is not this is inevitable we're saying if we confront certain
Make certain decisions. We have a real opportunity to solve the alignment alignment problem by by by affording this project that I just mentioned to you, and it'll have those potential outcomes.
And that is a way in which we think we could get through this while avoiding all the potential horrible dystopias.
I keep feeling like I want to ask for the silver bullet like what is the moral or what is the ethical standard or what is what is the one trait that's going to be crucial.
But then I keep coming back to that you're talking about real wisdom, which is always these tensions it's not one thing I mean.
And again you say about enlightenment there and you think well is there a silver bullet for enlightenment. Is there going to be something we can zap in the deep brain with with some with some frequency that's going to bring about enlightenment.
No, it's a really complex it's a complex bag of skills and states and traits over time. And so that's a really complex project and I'm starting to get why you're saying mentoring.
Because I think Aristotle was right, you know, if people put down that that approach to virtue, because it's amorphous, I like it because of that. I feel like Aristotle kicks back like, Oh, it's not easy.
If you want to be wise, what you should do is find somebody who's wise and spend some time with them and maybe by osmosis, you'll get some of the wisdom.
Yeah, and that's exactly right. It would be. I mean, every parent wants their children to exceed them every teacher wants their students to exceed them. I have that relationship with you.
I am extremely happy on how you are taking my work and surpassing what I have done and good parents and good teachers are like this.
Yeah. And thank you. And, you know, unless you're a psychopath, you, you want something to grow beyond.
Initially, they can catch whatever wisdom they can get from us. But hopefully, if and I think almost inevitably if they are genuinely becoming wise and virtuous, we can then catch some enlightenment from them.
And Plato talks about this in the seventh letter, right? It's like a spark that unpredictably transfers and catches fire.
And what you have to do is constantly cultivate the conditions until it emerges of its own sake, because if you try and make it like an artifact, then it's not really wisdom.
It has to emerge with a life of its own to actually be wisdom.
Wow. Okay.
Okay. Big question. Given where we are in this narrative arc and given the importance of this program and given everything we've said about the necessity for wisdom and even enlightenment to be part of our story about how AGI is going to benefit going forward potentially.
Is there any clear and discernible action now? Is there something we should be thinking about doing immediately? I mean, other than discussing, because one thing I think we should be doing is talking like this, that already seems to be a part of what you're aiming at.
But is there something definitive that we can be doing now?
There's a moral obligation. The reality of these machines puts a moral obligation on us and intensifies a longstanding moral obligation.
See, up until now we could rely on our naturally given intelligence as the Turing template that we tested against, but we are not naturally rational and we are very not naturally wise or enlightened.
We only have a natural potential for that. And if we want to properly mentor the machines, we all individually and collectively have to engage in a moral program of becoming more rational, becoming more virtuous, becoming more wise,
making the quest, if that's even the right word, it's not, but it's the best one I'll use right now, for enlightenment, a proper goal for us.
And I don't mean this in an elitist fashion. We have lots of ecologically historically valid examples, cross-cultural, cross-historical, of how these projects were adopted broad scale across civilizations at the many socio-economic strata.
Right? So this is not pie in the sky. We have the build-up movement in the Nordic countries. We have this has been done. This is not pie in the sky idealism.
We have real case scenarios. So people who just say, ah, they are being willfully ignorant. I'm sorry, I'm going to press on this because this is an urgent matter.
We have to all become more rational, more virtuous, more wise. We all have to become real enlightenment seekers. This is a moral obligation, and it has been magnified, put on meth by the advent of these machines.
Well, I couldn't agree more. This is my project with StoryGrid. It's a teaching mechanism to get people to cultivate their wisdom through contemplative practices in the writing process.
So once you are engaged in the actuality of writing, it requires contemplation to clarify your signals to your audience. So this is not pie in the sky. This is what we used to do when we were four years old.
You would tell your mom a story about your day, and your mom would say, I'm not sure what that, well, why did you do that? And then you would answer the question, and this is taking the time to think about the choices that we make through creation of a story that is an artifact of the mind.
It's a creative act, and it's all about reading the signals of the world, sort of metabolizing those signals, finding the patterns and the forms, and then recreating them in a construct that is meaningful to other people.
And one of the things that I find really disturbing is when people have this ridiculous notion that the universe is meaningless. And I think if you ever want to find meaning, the place to find it is in mentoring.
Because when you mentor someone, as John says, when you do, and they come up with an insight that you never ever imagined, it is the most meaningful thing that you can ever experience.
When my kids come up with something incredible, I just go, oh, okay then.
All is well in the universe. I'm on the right track, right? And so if you can teach people how to have insightful moments in storytelling, and it doesn't have to be all that difficult, just learn how to write a really, you know, valent
sentence, you know, that is meaningful to someone and goes, oh, that's an interesting sentence. Could I read another one of yours? That is the means by which we can cultivate wisdom within ourselves by expressing ourselves and reaching a dialogue with other people.
So we can test self-evidence through our writing.
Wow.
Wow.
So this is kind of the project that is enveloped me and what really attracted me to John, because we're both doing the same project. It's just different ecologies of practices as John would say.
Yeah, exactly.
So what's wonderful here is we get a lot, it's popular today to think about, and this is a good thing as well, how can we become, how can we be, how can we better interface with the emerging technologies that are coming?
So you want to practice mindfulness, why?
Well, because you don't want, you don't want EvilCorp to have the whole say in where your attention goes. So regaining your attentional autonomy seems like it's a crucial thing going forward.
So we've got that story where you want to be wise because we want to wisely interface with technology.
But what we have here is a companion idea and it runs so deep.
I mean, just a short pod just doesn't give it credence. I just hope everybody engages with this material and seeks out Sean and John, because this is stuff that you really need to digest.
But we have a companion project here, which isn't be wise so that you're better in the face of emerging technology, but rather learn to be wise because we're going to be called upon to first better understand the threshold we're moving through.
And that's your responsibility because we live in the world together and you better be ready. So you better be wise to understand this liminal moment we're passing through.
And two, you're going to be called on to mentor not even not only each other, but maybe synthetic systems in the future. And if we're going to have a good grip on how to make a synthetic powerful intelligence good, we had better bloody know how to be good ourselves.
Otherwise, otherwise, we're just shooting in the dark being like, Oh, we'll make it good. And you're like, well, how and you're like, Well, I don't know. I don't know what it is to be good. I guess make a lot of money.
You better know, you better know how to be good so you can be good for them, which is very much like a parent child relationship here you don't want to only be good so that you can survive parenthood, but you want to be good, because hopefully you want your kids to be good and so that is a moral obligation to be good.
And you articulated it beautifully that is Karuna that is agape. And that is like the thing we're trying to ultimately get people to virtuously tap into in this project.
Wow. So we're going to go now.
But I just want to say, I want to I want to even just go on that cliffhanger because here's the thing.
We've got a podcast here that explores the science of contemplative programs. And this is again, like I said at the beginning of the episode, this is our first episode back.
And I just want to leave it there because this is a real call to arms for exactly the kinds of things that we're interested here, which is we need to be wise at the birth of this big intelligence we need to be our best selves.
And so thanks, guys, and please everyone check out the material and I will be linking everything in the comments so that you can get in touch with this extremely important project.
Any last words or does that feel like a good natural close.
I'm happy with that. Thank you very much, Mark.
Yeah, you're the light and fire inside of you is always extremely beautiful.
Well, I mean, how do you how do you not light on fire when these sparks are just coming off of your guys projects? Really, Sean and John, thanks so much. And we'll talk soon for sure.
Okay, thanks, everybody. That's another episode of the contemplative science podcast. And as always, we'll see you next week.

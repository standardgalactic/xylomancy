And so what Josh and I propose in this paper is that one of the things evolution does is
not just change the hardware, but it also provides new observers and new capabilities
that extract more benefit from the exact same piece of hardware activity by viewing them
in different ways.
It's the perspective that can change.
So you don't change the machine, you add different ways to interpret what that machine
is doing and thereby benefit and get adaptive, increases in adaptive fitness.
So this is, and I have some other biological examples if you want them, but that's the
idea.
The idea of polycomputing is that any set of events could be said to be doing many different
computational things at the same time, depending on who's observing them and how do they interpret
what's going on.
And of course, Josh's examples are also very powerful in non-living media.
So this is the idea, right, is that you change your perspective, you can improve what's going
on by not to sound like some kind of self-help kind of thing, but literally in the system,
the improvement comes from changing your perspective on things, not by changing events, it comes
from changing the way you look at those events.
And so that leads to a view of evolution and biology in general as this multi-scale soup
of agents interpreting each other and finding better and better ways to interpret each other
and understand each other and extract utility from that.
And then of course, the better you understand some system, the better you get at hacking
it.
So that's the vision here, right, is that it's just full of, it's a huge microcosm of
observers interpreting each other and finding new and better ways to decode and interpret
what they're getting from their neighbors, from the level above, the level below and
so on.
Mike, thanks for coming on for round two.
Of course.
Yeah, good to see you again.
Thank you.
Yeah, it's great to see you.
And to start, I just want to say the response to our first video has been incredible, our
first interview.
Awesome.
It was only a month ago, but it's already got over 60,000 views in that time, which
is amazing.
Yeah, super.
In a short time, it's my most popular interview, so thank you.
It's incredible.
It really resonated with people.
And I think what people, one of the things people really enjoyed in some comments on
the video were that they hadn't, people that were familiar with their work hadn't seen
the computational boundary of a self work that you'd done, of course, with the great
visualization with the cognitive light cones.
That was, I think, a lot of people's first introduction to that aspect of your work,
because you do touch on so many different areas, so I just want to thank you again for
that.
It was a great discussion, and I can't wait to continue it today.
Cool.
Yeah, thanks for having me on.
It's always great fun to talk about that stuff.
Yeah, absolutely.
So yeah, today, I think, just to lay out for the audience, what we're going to cover is
we're going to start off with, it's going to be a three-part discussion, I think.
We're going to start off with poly computing, a paper that you wrote with, oh, I'm forgetting
the name now, but Joshua Bongard.
Josh Bongard.
Yeah, we'll talk about that paper a little bit.
We're going to do a little bit of screen sharing as well.
So if folks liked the, a lot of the visuals that we included last time, I think they're
going to like that again.
And then after that discussion, we're going to go on to the TAM paper, the Technological
Approach to Mind Everywhere, which is essentially a second part to the first part that we come
to be covered.
And then finally, we're going to follow up.
Part three will be Ethics AI and possibly Biomedicine in the Future.
Great.
So that's what we'll cover.
But before we dive in, one thing, just before we dive into the main topic conversation,
my favorite comment from around one came from Himei, just sort of a request or a comment
from Laura Kelly.
She asked, I would like to hear and play those bongos e jembe in the corner.
So.
Yeah.
Unfortunately, she's going to be disappointed.
I do not play any of those things well.
What it is is when my kids were slightly younger, they would come in here and we would sort of
run around and make noise together.
And so it's not like I know how to play these things.
I sort of, you know, I would bang on them and the kids would bang on them and we would
just, you know, we would just, and they would bring in other things that make noise and
we would just do that.
So I am no, no kind of a proper bongo player of any kind.
Oh, darn.
Okay.
I was hoping for a little Michael Levin unplugged second that we could get you.
Yeah.
Yeah.
I wish.
I wish.
I mean, you know, at some point my kid might barge in here and then we'll do it, but,
but it's not going to be, it's not, it's not going to be any, any kind of good music
out there.
I'll put it, I'll put it that way.
Got it.
Fingers crossed.
I'm also musically challenged.
I wish, I wish I had that ability, but I just don't have it.
So great.
All right.
Let's jump into then poly computing and the main paper we're going to be discussing
and I might screen share a little bit in a moment, but it's called, there's plenty of
room right here, biological systems as evolved, overloaded, multi-scale machines in North
East paper with Joshua Bungard.
And I think, I mean, before I touched this paper, I had no conception of poly computing
at all.
So I think it'd be awesome for the audience if, can you explain what poly computing is
and how it differs from what we consider, we generally think of as traditional computing?
Sure.
And so, and so I should say this, this work was all 50-50 developed with Josh Bungard.
Josh, of course, is a professor at University of Vermont.
He and I are partners in a lot of different things, including all of the Zenabot work.
And yeah, I think the reason you wouldn't have seen it is to my knowledge that we,
this is a term that we sort of coined and used for the first time.
So the idea is this, and this crops up again, this will crop up again and again when we
talk about the TAME framework and all of that.
And it has to do with the primacy of observers and kind of an observer-centered view of things.
So you can start thinking about it this way.
What is a computation?
What do we mean when we say something is a computation?
And in particular, one, we've had, I've been at conferences where we spend days arguing
about what that is and some people say everything is a computation.
And I even, there was even one person who said that nothing is a computation.
He was a professor of computer science.
I thought that was amazing.
Interesting.
And so, yeah.
And so there's all kinds of views, but one of the real kind of issues here
is that there's this general feeling that there is a truth of the matter about it.
So in other words, something either is or isn't a computation.
And the idea is that we could come up with a definition and then things would either
meet that definition or not.
And then we would know this particular thing, the trees waving in the wind.
Are they computing?
There would be some definition that would say, yes, they are or no, they're not.
And that would be that.
And the part of the, and this belongs squarely in line with the TAME framework,
the part of this that we're going to change is this idea that there is one unique answer
to this question.
And what we're going to say is that it is actually dependent on some external observer
who views a set of physical events as a computation and benefits thereby.
So the idea is that the computation is really in the mind of the observer or multiple observers
more than it is a feature of some particular set of physical events.
And so on the one hand, it makes things quite relative because it means that there could
be multiple observers that look at the exact same set of physical interactions, see different
things being computed, interpret them in different ways.
And Josh actually and his student at TUSA have some absolutely beautiful published work on
this, looking at the exact same set of the exact same piece of physics and seeing multiple
different computations in it.
So this is, this is actually, you know, is actually research on this now.
But so so it is relative in that sense, because multiple observers could disagree about what
the computation is here, but it isn't anything goes because there's a there's a clear it's
not that anything you might say about it is equally valid as anything else.
It's not that because in treating it as a computation, you have to say, what does it
enable you to do that you couldn't do otherwise?
In other words, you have to benefit from that lens of looking at this set of if you're going
to treat a set of events as some kind of computation, you have to be able to say, what is the practical
import?
How did you, how did you benefit by having that?
And the title, the title of the paper is how we get into the biology of this, which is that,
you know, Feynman had this very, very famous work talking about there's plenty of room
at the bottom.
And what he was saying is that we build machines at certain scales, but underneath that at the
nano scale, there's all this room for novel robotics and novel, you know, new engineering
that we're not using yet.
There's plenty of room down there.
Well, the thing is in biology, that's not the case because every scale is occupied there.
If you look at a cell, there isn't room anywhere because at every scale from the from the let's
say organism level down anywhere you look, biology is already using it.
It's already doing stuff at that scale all the way down to them, you know, sub molecular
interactions.
So that brings up an interesting question for biological evolution.
If you were trying to add new functionality to a system, where would you put it?
Because there isn't new room for it.
And specifically, if you start making changes, let's say mutations, one of the problems is
that you had a pretty well orchestrated system before you start making changes.
Yes, maybe some new something new will appear, but you're going to wreck all your prior gains,
right?
All the subsystems that depend on a given system to work in a certain way, you can't just you
can't just randomly make changes to it because everything else will fall apart.
So that so that brings up an important puzzle.
It's like how would evolution provide new functionality?
And so what Josh and I propose in this paper is that one of the things evolution does is
not just change the hardware, but it also provides new observers and new capabilities
that extract more benefit from the exact same piece of hardware activity by viewing them
in different ways.
It's the perspective that can change.
So you don't change the machine, you add different ways to interpret what that machine
is doing and thereby benefit and get and get adaptive increases in adaptive fitness.
So this is and I have some other biological examples if you want them, but that's the
idea.
So what I'm probably computing is that there any any set of events could be doing could
be said to be doing many different computational things at the same time, depending on who's
observing them and how do they interpret what's going on.
That's fascinating.
What is the history of this idea or this concept?
I mean, is this just a discovery that the two of you made or is this go back in history?
Like when was the earliest that we we knew that biological systems are capable of performing
this?
Well, that depends.
You know, this idea that the idea that there are multiple different ways to interpret events
in the organism is quite old that that that idea has been sort of suggested by by many
different traditions.
And of course, people have been wrestling with this question of what exactly is a computation
for a really long time.
But I think this particular fusion of it and the the insistence on an observer point of
view when when understanding these biological and computational things.
I think I think that's I think that's new that that emphasis of it.
And it also it also connects to a very old debate, which has to do with it has to do
with with with reductionism and causal power in systems.
So so let's just think about it this way.
Imagine a traditional physicist looks at a microprocessor doing its thing, you know, it's
and and and so the physicist looks at it.
And if that if if that physicist is a reductionist, what he will say is what what what computer
what algorithm?
No, no, there are electrons, there are electrons moving around, none of the electrons disobey
Maxwell's equations and then, you know, Schrodinger's equations, whatever else is going on at the
micro scale, we have we have good equations that describe what the what the electrons
are doing.
That's it. Anything else you put on top of that is fiction.
So so the important the important stuff is guided.
We know the equations that guide what we can say.
And in fact, in a perfect world, if we had, you know, kind of a Laplacian demon thing
going on, we could actually say what this machine is going to do just by tracking the
microstates, you know, where all the electrons are going.
So so the thing is that that perspective, it's it's it's not wrong in the sense that
empirically, yes, we have equations that guide all these things.
But would you would you hire that person for your new software company?
I'm going to say I'm going to say you wouldn't because because right because anybody that
doesn't believe that the algorithm is what makes the electrons dance is is it's it's
not that they're factually wrong, but but they're not going to make anything new.
They're not if they're not going to write any new algorithms if they if they don't if
you don't fundamentally believe that there is such a thing as an algorithm that has causal
power that determines what happens next.
You can't participate in this whole stack of what happens afterwards.
On the other hand, your traditional, your traditional programmers going to say, well,
what I do is I write code that makes the computer do things the code.
The algorithm is in fact functionally a potent it makes the right.
And so and so that's a different lens.
That's a different lens on it.
And so what we're not going to do is argue about who's objectively right.
We're just going to say that in different circumstances, you can gain different benefits
from both from both views.
And there is there's plenty of benefit to be gained from the kind of lens where you see
what's going on in that in that microprocessor as a computation because you can write code
and you can do other interesting things.
It's it's a it's a view that steps away from prediction as I mean, up until the prediction
is a real kind of a common thing that everybody latches on to.
So so, you know, whether you're a reductionist or or not, the idea is, can you predict what
the system is going to do next?
And I think that's that's fine.
But I think something much more interesting than prediction.
See, prediction is all good is all good when somebody else has already set up the system
for you.
So somebody else sets up the system and now we're predicting and now can you predict it
or the kid, you know, who's got the better, the better way of predicting.
But I think prediction is is just part of the story.
What we're really interested in is maybe I don't know what a what a good word for it is.
Maybe maybe it's pre invention or something.
It's it's the idea that we're not just going to predict exactly what this clever system
is going to do next.
We actually need to be in a place to make one and the next one and the one after that.
And so now I'm interested in frameworks that facilitate that.
I don't want to just be able to say, you know, somebody hands you a complicated thing
and now here's how we know what it does next.
I want to know how we get to the next one and what kind of world views
and what kind of lenses make it easier to facilitate invention, basically facilitate discovery.
And so and so that that's how all this all this fits together is is this idea that
it really is a fundamental to being an observer to say, what is your interpretation?
What is your lens through which you interpret a series of signals of events?
And then what does that do for you?
Gotcha.
I think, see, I'm just thinking from the paper, one of the things I pulled out
that I thought was I think it's in the abstract, the definition of the simpler,
simplest definitions for poly computing is the ability of the same substrate
to simultaneously compute different things.
And you're saying it's via different lenses, via different reference frames.
Yeah. Yeah. I mean, that definition.
I mean, it's funny that that definition still really puts the center of gravity
onto the onto the thing onto the computing thing itself.
And I want to I want to sort of blow it out and say that's that's not where the magic is.
The magic happens around it at the various observers that exist or could exist
that will interpret what's going on in various ways.
That's that's really a critical part of the whole thing.
Gotcha. And I think it'd be really helpful if could you provide a couple of examples?
There's a great table in the paper that I'll probably overlay for part of the discussion
just so people can see it's about 20 different instances of computations,
poly computations in the same biological hardware.
But could you, I mean, maybe tell us like one or two of your favorite examples?
Sure. Sure. Yeah. Yeah.
So so so one one one example is and this is this is work that was done by a couple
of postdocs in my group, Surama Biswas and Wesley Clawson.
Imagine imagine the gene regulatory network model.
So this is literally just a set of nodes.
Maybe there's a dozen of them or so, maybe a little more, maybe a little less.
They all correspond to different genes or different proteins.
And one of the things they can do is turn each other on and off.
So you can imagine the sort of graph thing where each each node has like a little arrow
and it either up regulates or down regulates one of the others.
And they're all sort of connected and this is like network.
OK, so these things these things are functional in every cell of the body.
They work in their critical and evolution, their critical in health and disease
and embryonic development and so on.
And so it's really important to understand what these how these things function and what they do.
And so if you look at it at these models, they look like a really
kind of a paradigm case of a deterministic simple mechanical thing.
I mean, they're completely deterministic.
There's no magic there.
You can see exactly what what's happening.
Each one is up or down regulating some other.
And there's a set of models to sort of model its behavior.
There are ordinary differential equations or some of these are Boolean networks and so on.
And so it looks really simple.
And so if you were just to look at it and you would say, well, how much intelligence does this thing have?
You would say, well, that's a silly question.
It's a clockwork.
So what we're going to do, we're going to use.
And so the traditional way of looking at these things is to use the tools of dynamical systems theory
and to just treat this thing as a mechanical, as a completely mechanical process.
So that's one lens and that's one way of looking at it.
And so what that lens does is to say that if you want this thing to behave in a particular way,
meaning, let's say, to go from disease state to a healthy state, you're going to have to rewire it somehow.
Meaning you might add nodes or subtract nodes or change the weights or something like that.
You're going to interact with the hardware.
And this is what modern molecular medicine does.
It would be gene therapy.
You would have to add new, let's say, if it was a gene regulatory network,
you'd add a new gene that hits the promoter of some other gene or you'd change the promoters to be stronger or weaker, something like that.
So we took a different approach.
And again, this will come back with a vengeance when we look at the tamed paper,
which is this idea that you actually don't know how intelligent and what the capabilities of this thing is until you try.
In other words, the idea is that you can't have these sort of philosophical feelings about where something is on that spectrum of cognition
that we'll talk about.
You actually have to do experiments.
You have to state hypotheses and do experiments.
And so our hypothesis was this.
We said, let's let's test the idea that it has learning capacity.
And so the way you do that is you basically treat it as if it were an entire animal and you try to train it.
In fact, Charles Abramson and I wrote a paper on the kind of behaviorist approaches to studying novel creatures that you might use.
You know, behaviorism is good for that because you don't have to make assumptions about what it is and what it's made of and how it got here.
You just there's just a set of behavioral strategies that you use to test to see what this thing can do.
So so what we did was was imagine we took we chose three of the nodes, whatever we would, you know, choose three of the nodes.
And just for an example, there were many experiments, but here's just one you you think of think of Pavlov's dog experiment, right?
So in Pavlov's experiments, one of the things he did was you ring a bell, which is normally a neutral stimulus for a dog doesn't mean too much.
You present some some food next to that next to that stimulus, the dog salivates.
You keep doing them together and the dog will eventually associate the sound of the bell with the presence of the food.
And eventually you just ring the bell and the dog salivates.
OK, that's the standard the standard story is actually somewhat different, but that's the standard story of it.
So what we did is we decided, OK, could we could we do this to the to the network?
And so what we would do is we would stimulate one of the nodes that means, you know, upregulate one of the genes.
Do the same thing to another one that where normally the first one has no effect on whatever the response we're looking for is.
So we choose one of the nodes as we call it our response.
Maybe it's you know, maybe it's something that controls blood pressure or maybe it's something that is some kind of enzyme is something that we care about.
And so that we choose a node that always turns that on.
We choose some other node.
That's the neutral node.
That's like the bell, which normally has no effect on it.
And we just present them together.
We present them together and then we pause and then we present them together and then we pause and we present them together.
So when you do this, what you find out is that for certain for certain networks and for certain choices of.
The condition stimulus node, the unconditioned stimulus node and the response, it will actually learn to associate them so that later on when you present just the neutral stimulus, it fires off the response.
Now that's amazing.
That means that this, this, this, this regular, this gene regulatory network has associative learning capacity can be conditioned like, like an animal can.
Now, a couple of interesting things, the reason I bring this up, one is that random networks don't do this very much biological networks do this.
So that means that evolution sort of selects for this or the directly or indirectly, but the, but the, you know, life, life likes this capacity.
So that's one thing.
But another thing is that that choice of which of the nodes.
So let's say you have a dozen nodes.
So you can choose which of the nodes is going to be the, the condition stimulus, the unconditioned stimulus and the response.
That choice is not the only choice.
You could pick three different nodes or you could flip their roles around.
In fact, there's six different kinds of learning that it can do that have different, you know, you can think about it as like a stencil and you're sort of putting the stencil onto the, onto the network in different ways.
And you're seeing different nodes in the roles that your stencil assigns.
And so now you can do an experiment and you can ask yourself, well, which of these different ways of looking at this network give me different capabilities.
And so what you find is that for the exact same network.
And so that's the key here is that the learning capacity of this network was not due to changes in the, in the weights, you know, the traditional idea of like synaptic connections, right?
The weights are changing.
Nothing was changing.
The hardware was never altered.
It was exactly the same network.
But depending on how I looked at it as an observer, I chose these three nodes or I chose these three nodes or these three nodes, depending on how I look at it, I can squeeze completely different functionality out of it.
I can get it to do associative conditioning.
I could get it to, to do a sensitization or habituation or different other things.
In fact, simultaneously, some of these memories can coexist.
And so this, this to me is a really powerful example, because what it means is that a single network, nothing was changed about that network.
Absolutely nothing.
The hardware is exactly the same.
And yet different observers can extract different utility from what it's doing.
And of course, Josh's, Josh's example examples are also very powerful in non-living media.
So, so this is, this is the idea, right?
Is that, is that you change your perspective, you can, you can improve what's going on by, you know, not, not to, not to sound like some kind of, you know, self-help kind of thing.
But, but, but literally, literally, right in the system, the improvement comes from changing your perspective on things, not by changing events.
It comes from changing the way you look at those events.
And so that, that leads to a view of, of evolution and biology in general, as this multi-scale soup of agents interpreting each other and finding better and better ways to interpret each other and understand each other and extract utility from that.
And then, of course, the better you understand some system, the better you get at hacking it and hacking.
I don't mean the negative thing where, you know, you sort of abuse the system, although that certainly happens, right?
With parasites and various other things that absolutely happens.
But, but it doesn't have to be negative.
The, the, the reason we have a body instead of a bag of amoebas is that these cells are constantly hacking each other.
They are constantly sending out signals that get each other to do various things that they otherwise wouldn't do.
It's behavior shaping all, you know, up and down the, the, the, the scale hierarchy.
So, so, so that's the vision here, right?
Is that, is that it's just full of, it's a, it's a huge, you know, microcosm of observers interpreting each other.
And finding new, new and better ways to decode and interpret what they're getting from their neighbors, from, from the level above, the level below and so on.
Yeah, that's phenomenal that the same thing is being computed, but the way that it's being looked at changes the utility of that thing, the changes, the usefulness of that computation.
Yeah, it's incredible.
And I think, I mean, I didn't know, I was unaware of this.
I think that the, the layperson probably has no idea that biological systems are doing this.
And I'd say there's so many different places I could go from here.
I guess what are some of the, one of the things, one of the notes from, I think it's the first figure number two in the, in the paper.
It's the spatial causal emergence graphs that you have between determinism and degeneracy.
I know this is getting very much in the weeds.
I'd love to just touch on it before maybe going back out to, to discuss some more of the, the mechanical computing and the morphological computation as well.
I think it's a little bit later in the paper.
But for those spatial causal emergencies, sort of this micromacro dynamic that's happening there.
And this looks, the figure looks very familiar to, similar to some of Eric Owell's work.
I'm not sure if that was like brought in.
Well, he was, he's, yeah, Eric was, Eric was a co-author on the, on the first gene regulatory network paper.
So if that's, if that's the, if that's the, the figure you're talking, the paper you're talking about, then he was, he was a co-author on that.
Yes. Oh yes. I have the other paper.
I don't think I have time for it today, but I did read that other paper you co-authored with him as well.
So yeah, it is this, and the idea, you could probably explain this better, but the way I understand it is that he has another paper called,
When the Map is Better than the Territory, how the macro scale can actually give you more information than just adding up all the micro scales.
Can you perhaps, and I know it's a very complex, that's a whole different divergence, but could you bring that into how it applies here?
Because that wasn't quite able to make, I can sort of see the similarity or how it's related.
But if you could expound on that, that'd be really helpful.
Sure, sure. And, and, you know, I'm not going to do a better job than Eric would about his own work.
So you could have him on, but I think his work is foundationally important.
It's, it's, it's incredibly important work.
And what he basically developed was a way to quantify and make rigorous a debate that's been going on for probably thousands of years,
which is, are there any higher levels like bodies, organisms, you know, people, things like this?
Or is it, or should it really be reduced to talking about molecules and atoms and whatever else is underneath?
I mean, people have been discussing that for a long time.
And I, so he's found a way to quantify that.
And his analysis shows that there are, and this is of course also the work of Giulio Tononi and other people like that.
That would have many others that have contributed to this since then, that basically have figured out that for certain kinds of systems,
you actually gain more power.
Power means better ways to control and predict the system.
And, and, and you gain better, better predictive power by taking seriously these higher levels.
And I'll give you, I'll give you just another simple example that I, that I really like.
There's this, there's this thing that many of your listeners will recognize is called the game of life.
It's a cellular automaton.
It's this big grid, right?
The universe is this giant grid and it has a very simple physics.
It has each grid is a single node, and it can be either on or off.
It's either bright or dark.
And the reason, the way you determine whether it's on and off is you count up how many neighbors it has that are on.
And then there's a very simple, there's three simple rules that tell you whether that cell stays on or off.
That's it.
That's the physics.
It's totally deterministic.
There's nothing else in there.
And so, and so the magic that happens there is, and this is John Conway, I came up with this, is that if you, if you run that world forward from certain starting configurations,
you see all kinds of amazing complexity emerge.
And one of the things that you see is this thing called a glider.
The thing about a glider is it's a stable pattern that moves across the screen.
Now, it, now let's keep in mind in that world, in life world, physics, there is no such thing as a glider.
All that exists are individual, you can call them molecules or something that are just on or off and they don't move.
And that's it.
There's no such thing as a glider.
But to our visual system, we interpret a moving pattern of just imagine a wave of the cells turning on and off and on and off.
Right.
So if they do that in a pattern, you see this thing moving along.
So our visual system is very good at, at turning those kinds of events into a mental representation of an object.
It's not a real object in terms of physics.
It's a, it's a, it's what you might call a persistent physiological kind of state or, you know, like a hurricane or something else that there are the molecules of air.
Yes.
But then there's or a wave or an ocean wave, you know, you got the water, but then you've also got this way.
OK, so, so, so one thing you could do is you could be a strict kind of reductionist about that world.
And you can say, look, there are no gliders.
And I'll tell you why you don't need to know about gliders in order to predict the next thing that's going to happen, because the world is totally deterministic.
And if you tell me where all the on bits are at time T, I can tell you exactly what's going to happen into infinity.
I just have to crank through the computation.
I can tell you where everything's going to be.
And this is true.
However, what you don't have when you do this, if you don't believe in gliders, what you're not, yes, you can predict what happens after somebody else has set up a clever pattern for you.
What you're not going to do is invent a, a, a touring of a pattern in that environment that creates a touring machine using gliders as a way to move bits around.
Somebody did that.
Somebody actually did that.
They, they, right.
They create, yeah, they created it.
I mean, it's incredible.
They, they created a computer inside of life world and it uses these gliders to send signals around.
That's the thing.
It's not that the determinist, once you give them this amazing, amazing setup, it's not that they can't tell you what happens next.
They can.
They just would never have made one because they don't believe in gliders because, right.
And so, and so this once again gets you back.
I know this a little bit away from, from what you asked about Eric, but Eric's work, but I think, but I think it's important.
It's the idea that, yeah, you can quantify prediction and control and it's, it's very awesome that, that, that his system gives you a way to, to say that in certain systems, knowing about these higher levels and looking at gliders as opposed to that.
I don't know.
I actually don't know if anybody's done this exactly for the game of life, but, but this, this kind of thing that, that, that knowing about these kind of higher levels, like, for example, in biology, you might say, knowing about organs as opposed to just the gene expression states, or
knowing about physical persistent physiological states, maybe stress states, maybe waves, maybe calcium waves propagating through tissue, maybe electrical waves, whatever, knowing about these things gives you extra power.
But what we don't yet have a way of quantifying, and I think, you know, I've been talking, I have a new post that coming that we've talked about doing this project next is actually trying to come up with, with a, with a quantification of the next step, which is not just prediction, but actually the, the, the
inventiveness part, the generative part, like, right, what, what, what, how much power does a certain lens give you to invent new stuff, as opposed to just say what, what preexisting stuff is going to do next.
I think it's really important going to the future.
I mean, it's a super hard problem.
It's, I don't have a solution for it yet in my head, but, but I think it's really important.
And we don't, we don't really have, but I, but I think that's the evolution.
That's the next evolution of the kind of thing that, that, that Eric did.
Well, that's great.
Thank you.
I never thought of a, I mean, it does even look at the, at the,
at the figure here, it does evoke the idea of the game of life a little bit because you have the transition probability matrix and everything like that.
I never made the connection between, say, the micro rules and the macro objects that could be there.
Gosh, that's a whole, we're going to talk about that the entire time.
But yeah, that is a little bit more in his domain.
So to go back to, I think the colleague, poly computing, and I have just a couple more questions in this area before we move on around to act two.
If you could, doesn't need a little bit here, the differences between mechanical computing and morphological computation.
I don't know if it's familiar to you that you said that because one of the things I want to get into a little bit, and this could be the next part of our discussion is
some of the ideas that the paper brings up is around the brain body dichotomy.
And, you know, you kind of, this idea, I think that's been prevalent in science and correct me if I'm wrong, but that the brain is
doing all the computation is doing all the heavy lifting.
But what I'm getting the vibe from this paper is that, you know, it's far more complicated than that.
It seems as though computation is happening at all the different levels.
The brain is obviously a huge part of it, but the body is doing a lot of work there.
And it depends, of course, on what kind of lens you take or kind of framework you take.
So I know that's a lot, but would you feel free to unpack those concepts a little bit?
Sure, sure.
You know, if you want to dive deeply specifically into morphological computation,
Josh Bondgard is the person to have on because he was actually a very important.
He did some very important work in this in this field, which exactly as you pointed out, it talks about
the kinds of computations that are done by the embodied cognition of the controller.
So, you know, and there's some very interesting ideas in that field.
Like, you know, instead of instead of, for example, you might think, you know,
do I need to evolve a controller to control a certain body?
Or do I do I change the body to suit the to suit the controller?
How do they relate to each other?
Where is the actual intelligence?
So there's many, many, many things.
And so he could he could tell you all kinds of interesting stuff there.
But for me, what I what I find interesting about it is this again,
let's let's just back up and remember that what we're talking about.
When when I say things like,
you know, there's the organs in your body have intelligence.
There's there's a trivial sense of it.
And this is what people often react to when they when they when they object to this,
these kind of statements is they will say, well, look, you can just paint that on to anything.
You can say anything is, you know, you know, here's a ball rolling down a hill.
And, you know, and, you know, and you can say, look at that.
It's not amazing that it gets to where it's going.
Boom, intelligent. So so that isn't what I'm saying.
What the tame framework says is that in that, that all intelligence claims
are engineering protocols that when you tell me that some particular system has
a degree of intelligence, what you really need to tell me is what's the problem space?
What are what are the goals that the system can achieve in that problem space?
And what degree of sophistication can I assume that the system is going to have
on its own without me micromanaging it?
Those are the three things that you are really telling me when you say.
So if you tell me it's a bowling ball on a hill, what I hear is it has.
It's it's not zero competency.
It actually has by by by by by virtue of least action principles.
It actually has a little bit of competency to minimize things like, you know,
Connecticut, the total energy and all that kind of stuff.
So so but but I also know that if I want to change the way the ball rolls,
I don't have any I'm going to gain zero benefit by thinking about
internal state, you know, internal.
How does the ball feel about this landscape?
That's not going to be a useful way for me to go.
And the only tools I have are to modify the actual landscape.
So I can put in bumps or I can push the ball or something.
But, you know, something like that, it does have a little competency in that.
I don't have to push it down the hill.
It will find its way down.
But really, all I have is these kind of very low, very low agency ways to interact
on the other hand, if you tell me that what you have is a mouse on a hill.
Now now we got a different story because now I know that well,
the tools I was going to use for that bowling ball kind of useless.
And what I really need to understand is what is the
behavioral landscape that the animal is seeing?
What are the things that attracted?
What are the things that repel or scare it?
What are the things that it remembers about having been here before?
In other words, when I want to change that systems behavior,
I need to do a lot of thinking about what are the internal states of that system
and how it sees the landscape.
It's not how I see the landscape that matters is how the system sees the landscape.
And so and so that tells me different ways that I can interact with it, right?
And different why I can change its beliefs.
I can do the very thing, you know, various things you can do and all the way up to all
the way up to humans and so on.
So that's, you know, that that's the spectrum of cognition that we'll talk about
in the team framework.
So the idea there, then, is that when you say that the body is intelligent,
what you're really saying is, are there ways to interact with that aspect of it?
And I really don't make that much of a distinction between brain and body.
I think I think, you know, they're very tightly linked.
But the question is, do I treat this thing like a dumb clockwork or am I better off
assuming that it has certain autonomous capacities?
Does it have memory?
Does it have homeostatic properties that I don't need to micromanage?
You write the nice thing about your thermostat is that you don't have to push
it all day long, you just set the set point and you leave it alone.
And so, right?
So what do I need to know about this?
So that's that's how I see all kinds of intelligence and agency statements as
engineering protocols.
Gotcha.
One of the things that this paper reminds me a lot of, and are you familiar
all with Jeff Hawkins's Thousand Brains theory?
A little bit.
I mean, they're not.
Yeah, I mean, either.
Yeah, I know this is like the bare bones of it then.
But it reminds me of this quite a bit because he claims that the brain builds
all these different models of the world using reference frames.
And so it's just I'm not sure if you have any comments about that or maybe how
it might apply to this or there's any link at all.
Yeah. Yeah.
I mean, I think it's I think it's very interesting and important.
And I think that everything is great, except instead of the brain, you can put
the entire body, right?
So so I think I think the whole body is doing this.
There are there are not that many things that your brain is doing that the whole
body doesn't that other parts of the body aren't doing in various spaces that are
hard for us to access and see.
But yeah, I think I think that that kind of model is the right way to look at it.
Actually, that's amazing.
I have to look.
I have to dive into that model far more.
But there was also a note.
I'm trying to see.
I can't quite find it in my notes here.
But something about the the amount how it's there might be nothing in the universe.
Correct me if I'm wrong, please.
But that has like a zero degree of zero.
I don't know, causal influence or some kind of computation.
Could you unpack that a little bit?
It's something that you even you mentioned clearly that there's this
might even this idea might make people uncomfortable even.
Sure. What are some of the possible implications of it?
Yeah. Yeah.
Well, many, many of these ideas make make people uncomfortable.
So I'll just barrel on, I guess.
So so so one of the one of the key things about the TAME framework, which really
started in that in that
cognitive flight co model is that we want to be able to have the same first.
First of all, we want to be able to say that
agency, cognition, computation of the type that we're talking about is not binary.
OK, that it's a spectrum, it's a continuum.
So we want to be able to define that continuum.
And and if you're going to and the goal is to define a continuum,
and I call it in that paper, I call it a continuum of persuadability.
And then that's on purpose.
Again, it's because the reason is that I'm not talking about
I'm not trying to pin it on on a kind of a single universal objective fact about
the system, I'm looking at it from the perspective of an observer who wants to
relate to it somehow, to to understand it, to to change how it works,
to get it to do something, to manipulate it somehow, to make a new one, you know,
whatever. But it's a very that's why TAME stands for technological approach
to mind everywhere. The technological aspect is it's it's we're not arguing
philosophy here, we are trying to get to very specific and testable ways
in which other other beings, observers, and they could be scientists like us,
they could be biologicals, they could be parasites, they could be conspecifics.
It could be the entire evolutionary process.
All of those are observers are able to optimally interact with that system.
So if you have this spectrum and so, you know, so so for now on the right side
of the spectrum are some some some humans, let's say, and going down, you know,
you see all sorts of things, you know, systems with which you can interact
through. So with the humans, you can actually give them reasons and they will
go and do things autonomously, right?
And you don't have to micromanage the how or the when or the what you just
give them a reason, then they will pick a goal and sort of run with it.
So lots of like huge agency, you know, lots of autonomous activity.
And so so so down past that, you have some other and some other creatures,
like certain animals that you can train in various ways.
And so you can give them a goal and they will with various degrees of ingenuity,
they will they will be able to to achieve that goal and so on.
And you can sort of keep going down and down.
And so so with any and that's that's a slide that you could at some point,
if you want to do screen sharing, you could show that.
But which which slide is that?
Well, it's it might be figure one, it might be from the tame paper.
It might be the I remember it this one, but it might be the
one of the first figures on the spectrum of persuadability.
And you know, and again, it's all it's all about.
Engineering protocols, it's all if you want to persuade that system to do something,
how do you do it?
Do you do it the way you would with a mechanical clock by rewiring the hardware?
Do you do it like you would with a thermostat by changing the set point?
Do you do it the way you would with a with a dog or a horse by giving it
rewards and punishments for specific behaviors?
Or do you do it like with a human by by giving them actual cogent reasons for them
to do things? So so so so I so you construct this kind of and of course,
I'm not the first person to try for this kind of
spectrum of Wiener and Rosenbluth and Bigelow did this in like 1943.
They tried for one and and and before that,
William James talked about this kind of thing before that.
So so the thing about those kinds of spectra is that the natural question is,
well, where's the bottom of it?
So is there a bottom of it?
And if there is, I mean, most people assume that there is and most people put the
put it put it pretty to me quite quite far up the thing.
Actually, you know, people talk about people have have concerns about plant,
you know, plant agency and and insects and things like this, which I think it's just
you know, I think those are things so so far off from where the if there is a zero,
they're so far off from the zero that it's you know, it's completely wrong.
But do you mean they're so far close to zero?
No, no, no. I mean, the zero should be the yeah.
Yeah. I mean, we have we have so many things and so many interesting things to
worry about that are really minimal compared to that.
The plants and other things are like, I felt to me, of course, of course,
they're on the spectrum.
So so but the question is,
is there a zero and if there is a zero, what does it look like?
And so I don't I, you know, I don't have a super firm conclusion on this,
but I tend to think the answer is no, that there is no zero.
And I'll tell you why.
Let's imagine for a moment and this is an exercise I recommend to everybody and
anybody who thinks that,
well, of course, I have it, whatever it is, you know, it's a cognition.
I have true I have true agency, whatever that is, I've got it.
And, you know, definitely
this this rock here doesn't or a cell, let's say, doesn't or, you know,
an unfertilized oversight, which all of us were at one point, doesn't.
So so the exercise that you need to go through is what is the most minimal
version that had it and and what happened right before that.
So so you've got it and you sort of roll yourself back to however old you are.
And then nine months back slowly, but surely, and guess what, you're an unfertilized
oversight. So at one point, at some point, it showed up.
And if you really think there's a sharp zero or nonzero, you have to say where it
showed up. And of course, the elemental biology offers absolutely no place where
you can say, ah, that's it. That's it right there.
That's weird.
And so and so that's a problem.
And so and so I suggest everybody to do this experiment and to say, what is the
absolute minimal version of agency that you think it would take to get on the
spectrum? And, you know, and people will often say, well, it's a it's a cell.
And and I'll say, yeah, well, what if it's just slightly simpler than the cell?
I mean, surely that's fine too.
And say, yeah, yeah, OK, that's fine.
So so go all the way to the bottom, you know, to figure out what you think is
the absolute minimum.
So when I think about the absolute minimum, I want it to have two features.
The first feature I wanted to have any system that's on that spectrum, the first
feature I wanted to have is some degree of goal directed behavior.
Goal directed behavior means that it tends to achieve a particular outcome in
some kind of state space.
And if it is deviated from that, it will expend some minimal degree of capacity
to still get there. OK.
And so so and so that's the first thing, some kind of goal directed activity.
And the second thing I would expect some minimal degree of indeterminacy.
In other words, the behavior of this thing is not well described by knowing
exactly what's happening to it right now.
In other words, in the in the in the local micro environment, all the pushes
and pulls that are happening right now are not perfectly sufficient to explain
what's going to happen, meaning that the distance between it and the causes
that control it, I have some depth to it.
It's not just something that, you know, however you push it, that's how it acts.
There's something else that controls you.
You can't you can't derive it from the local environment.
OK. And so this could be memory.
It could be predictive capacity that it has, whatever.
So as we think about those two minimal things, we realize that even
elementary particles have this, because the first part, the goal directed
activity is taken care of by by least action principles in physics.
So they have the ability to minimize and maximize certain quantities.
And I'm no physicist, but somebody like Chris Fields and Carl Friston and so on
have a much more sophisticated story about this.
I'm telling you a very, very simple version that basically all the way baked
in at the very bottom of physics are these variational principles where
you can already see that certain systems have nonzero capability to get to the
same to get to the same goal.
And again, it's like, you know, think of think of think of a think of a rock
or a roller coaster, if I'm an engineer, the reason that it's not zero,
the reason that that that that it isn't is zero on that spectrum is if I'm an
engineer, I have to spend a lot of time working on how I'm going to get this
thing up the up the roller coaster hill.
But I don't have to worry about how it gets down.
I don't have to push it down.
It that isn't much compared to our human agents.
You say, wow, that's a rock.
You know, that's a roller coaster rolling down the thing.
I mean, that's that's minuscule.
Like, right, we were looking for the minimal version.
We weren't looking for a nice, rich version of agency that humans have or that
other animals have. We were looking for the minimum.
The minimum is what can I depend on this thing to do on its own?
What preferences does it have on its own?
And this is this is a super old idea.
Giordano Bruno used to say stuff like this.
This is this is very old
that that it isn't zero.
The competency of these things is not zero.
It's very low, but it isn't zero.
And the thing about the indeterminacy, we of course, we have that in the quantum
realm, too, because we have lots of events that are not directly determined by what
happens now. So out of those two ingredients now, you can imagine two ways to go
from that as also super minimal, but it would have to be minimal because that's
what we start out looking for.
Of course, it doesn't look like human agency or or or even big brain agency
because we started out looking for the minimal versions.
So now two things can happen there.
One is you can aggregate those things in a very dumb fashion that doesn't
increase the depth of it.
And that means taking a bunch of particles and making a big rock out of them.
When you do that, you have not increased the agency at all beyond that very
minimum that the individual particles had.
You it still has equally the same very minimal.
But there are other ways to aggregate them, which actually amplify what's going on
here and the systems that do that, that's what we call life.
That's that's what when when when people, you know, I don't spend a whole lot of
time trying to come up with a definition for life.
But but but but that's what I think life really refers to light.
What we call life is any system that is really good at scaling these fundamentally
nonzero competencies of chemistry and physics into much larger competencies,
larger cognitive light cones, novel problem spaces and things that we begin
to recognize as being on the spectrum.
So that's what life is good at is scaling.
And so I once asked Chris if if given all of that, I said, is it is it possible
to have a universe with no least action principles?
So basically, could we could we, you know, our world doesn't to me, our world
doesn't have zero zero agency anywhere.
But but the question is, could could could there be one?
Could there be a universe to what that had literally zero?
And Chris said that the only way to do that is to have a universe where nothing
ever happens, a static world where nothing changes, because and you know,
I'm not going to go down that rabbit hole here, but but he could tell you a whole
story about the the kind of active inference like process that happens in any
interaction between any two things in the world.
They don't have to be brains that don't have to be alive.
That that minimal interaction between object, but object in the environment,
at what makes for a permanent object and all these kinds of things.
All of that is baked in at the very bottom level of physics.
Life is just good at scaling it up.
That's incredible.
Huh.
I'd love to actually spoke with Carl
first in a couple of weeks ago, I interviewed him and talked about the free
entry principle and was very confused for most of it, to be honest.
I mean, I did a lot of research going.
I mean, I hadn't heard the term before, but it has a high research load and that
requires, you know, information for all these different disciplines.
And it was incredible and such a treat to talk to him directly.
So I've sort of this.
Related to that, you see, one thing you just said,
correct me if I'm wrong, but you said our world doesn't have zero agency anywhere.
Is that your belief?
I mean, yeah, I think at this point, I would say that.
Yeah, yeah, I mean, yeah, I think that's true.
Yeah, I think that's true.
Somebody else, I just came across somebody else's quote.
Was it Robert Rosen?
I forget whose quote it was, unfortunately, but somebody said that, you know,
that that the thing we think of as true
inanimacy is the exception, not the rule that that the, you know, that that really
it's and I and I would take it further.
I am at this point, I'm not aware of anything that I would say is truly zero.
But again, let's let's make sure we understand this is a rigorous
position that has to do with, as an engineer, what can I take advantage of?
Right. And there is nothing to my
knowledge where those tools that come right.
So so again, the thing the thing about that spectrum is what what changes as you
go from left to right across that spectrum are the tool kits that you need to
interact with that system, right, the things that are useful for the mechanical
clock become less useful as the system progresses and vice versa.
And conversely, things that are useful for human subjects become less and less
useful as you go down the down the road.
So this isn't the kind of
kind of loosey-goosey sort of talk where you say everything is, you know,
everything is alive, I mean, you can do that.
But that's but that's that's pretty useless.
OK, that doesn't advance you at all.
What I'm talking about is a very specific
way to to to drive research agendas, which is to take
tools that are useful in one field for dealing with one kind of system and ask
what other kinds of system is of these tools useful for and to not be able to.
I mean, I'm arguing against having these sort of armchair philosophical
preconceptions about what things have to be.
Somebody somebody said to me once we were talking about this thing and I was
somebody said, well, well, well, surely you don't think the weather has any
intelligence to it and being perfectly serious.
I said, has anybody tried training the weather?
I honestly don't this is an empirical question.
You and I cannot decide whether whether patterns, meaning patterns of air movement
in the atmosphere, do or do not exhibit habituation,
sensitization, associative conditioning of some sort.
You and I sitting here in our chairs are not going to be able to decide that.
And and and a lot of people don't don't think that's the way to do it,
because they feel that if the empirical,
you know, if if those kind of empirical things
are the prediction of your worldview, then you then then the then the worldview
must be wrong, you know, they sort of assume that any worldview that that that can
possibly tell you that the weather pattern is has certain problem solving
capacities is obviously wrong because because you that you're already you know,
you're starting with with something that's wrong.
I think that's completely upside down.
Philosophically, I don't think you can do it that way.
I think you have to say
see what see what the see what see what the empirical results are.
And then you'll know how good your your outlook was.
Was it was it was it helping you or not?
I don't think we you and I can't decide this about these systems.
So I actually think that massive opportunities exist for looking for the surprising.
And we have some we have some pretty cool stuff coming out on this soon.
In the next few months, that's not peer reviewed yet.
So I don't really want to talk about it, but there will be some.
Once it's once it's peer reviewed, you'll see that the project of looking for
these kinds of
cryptic capabilities and systems that we normally are no good at recognizing as
having these kind of capacities.
I think there's massive opportunities for this.
We don't we don't realize we are we are so bad at it.
We do not realize how much intelligence is is is all around us.
And I said I said there was a there was a tweet a few weeks ago where I said
something like this that one of the things I love about AI is that it's it's
like a lens or or a translation device that's going to help us see a lot of
the intelligence that's all around us all the time.
It's like being able to suddenly being able to communicate with an intelligence
up with a parallel set of intelligence that they were here all along.
We just never knew we never knew how to recognize and we never knew how to relate
to them. And so and so some people liked it.
And some people just said that was the craziest thing they'd ever heard.
But in that that's you know, that that's not the right way to think about it.
But I really I really think that
so far when we've looked for these things in a in a rigorous way, not the way
where you say, you know, every rock has a spirit behind it.
I mean, OK, but but does that help you, right?
Maybe if you can give me an engineering protocol, what's the interface, right?
What's the interface to that spirit?
And so so that that's not super useful.
But some of these other things taking tools from cognitive and behavioral
sciences from from the kinds of things that Carl and Chris are doing,
that is is super rich.
It drives all kinds of new research agendas.
So so that's why I stand behind that statement.
Well, Mike, you're blowing my mind.
This is awesome.
I can't wait to see some of that research coming out later later this year,
hopefully the go in some way different directions off of that.
I think next.
Wow.
One of the things I would love to cover
that's in the tape paper and there's so many different.
I'll link in the description for folks watching this.
Please go go through Mike's papers and and take a look for yourself,
because there's so so much rich information in them.
One thing that I think might help a little bit to bring a couple of these concepts
together, and I think one of my favorite images from it is the one where you break
out homeostatic loops between gap junctions.
And then you also have I mean,
everything's on the sea of the plan area that are ranges of I think microvolts
and then the free energy or at landscape that's there as well.
And the idea, I believe that's like a tractor landscapes, right?
That's the differences there.
Can you give us a sense of how these things work together?
I think this is also applied to say morphological space as well, right?
It's all all related.
But can you provide us
with an idea, I guess, for specifically for like more of a lay audience, even for
myself, a lot of the stuff I'm sort of circling around circumambulating these ideas
and kind of slowly starting to congeal for me in terms of my understanding.
But it would be really helpful if you could provide us with
like an example or an intuition around how this stuff all comes together.
So you have the the landscape, you have the attractors.
And is this stuff, should we think of these attractor landscapes?
Metaphorically, I mean, is this is just a useful
visual for us to help us understand?
Or is this really what this is like the most real we can get?
Yeah, that's it.
Yeah, that's a that's a very, very deep philosophical question.
So so I'm I'm going to
I'm going to approach it this way.
Let's let's let's back up for a moment and just just realize that
people, OK, people, people sometimes ask me, you know, people say,
what if what if we're just living in a simulation?
What if what if reality is OK?
I don't know what the alternative is.
Of course, we're living in a simulation.
There is no alternative to that because we are constructed in a way that has
certain kinds of sensors.
It we are our mind and our body build maps of the external world and of ourselves.
We have internal models of ourselves that are inferred from the from the signals
that we get. We do not have access to reality.
It is guaranteed that you live in a simulation.
There's no question about it because because we do not have access to reality.
Not only and, you know, and many people have said this before and you can talk
to Don Hoffman and folks like that.
But but but the idea is that not only do we see a tiny fraction of the spectrum
of, you know, in terms of our senses, right?
We we only see a tiny fraction of the information that comes through.
So we're completely as individuals are completely ignorant of the rest of it.
But but also
we are that that philosophical brain in a vat, you know,
that from from philosophy 101, where they say, you know, what if you're a brain
in a vat and and just, you know, all these sensory images are coming at you?
I mean, that is literally what we are.
And then we have we have we have a model of our body and we have a model of the
outside world and, you know, we don't we don't feel the same.
The same way you don't feel like there's a there's a blind spot in your retina.
You're just used to it.
We also don't feel like there's a huge blind spot in the X-ray spectrum.
Oh, my God, how come I can't see X-rays?
You don't feel it.
You're used to this limited reality.
And one of the one of the things that leads to is that think think about our major
sensors and effectors.
OK, so our major sense sense organ is
things like smell and taste and vision, of course, all of that point and touch.
All of that points outwards.
I mean, we have pain and things like that.
But but the vast majority of them point outwards and our effectors, meaning
our muscles, move us through three dimensional physical space.
So what we are really good at is visualizing a three dimensional space
that we think we live in and we recognize other minds, other agents by which are
medium sized objects moving at medium speeds in this three dimensional space.
That is what we're good at recognizing.
That is just an evolutionary
consequence of the way we're built.
What we are really not good at doing is recognizing unconventional agents,
meaning very large ones, very small ones, very fast ones, very slow ones.
We are not good at recognizing unconventional agents in novel problem spaces.
So what is a what is a problem space?
So imagine for a moment if
if we had an internal and I think we can build, you know,
this is on our list is to build a creature like this.
But I think I think we could you could imagine a being
that had an internal sense that was, for example, sensing your blood chemistry
all the time, let's say it was measuring, let's say, you know,
like with a tongue or something like this, you could you could measure
10, 10 different aspects of your blood chemistry.
If we had the ability to measure that and the way that to sense that,
the way that we sense with vision, you would know that your body lives in this
10 dimensional option space, right, where just like we live in a three
dimensional space, if you could sense 10 different things, you would,
you're about your blood, your blood would be in this physiological state space
that would have 10 dimensions to it.
Of course, we can't visualize that, but we would, if we had evolved that way,
we would be able to.
And you would also know that there are these things called liver, kidneys,
and so on that are really clever about navigating that space, that when they get
perturbed by specific things that happen in terms of your blood chemistry,
they can find ways around it.
They keep you alive, despite all sorts of, you know, terrible things that you do
to them with you, you know, with your lifestyle and all that.
We would have no problem recognizing them as beings that live in this space.
We would know we live in this space.
We just don't, we just we're very bad at recognizing this and and it is no
less real than the large scale, three dimensional space in which we navigate
with our muscles, we are constantly navigating physiological state space,
which has way more than 10 dimensions.
Of course, we are navigating transcriptional state space, so gene expression space.
So if you have, you know, I don't know, several of tens of thousands of genes
as an organism, that is a very high dimensional space that you are walking
in all the time by turning different genes on and off.
You're navigating that space.
We have metabolic spaces, you know, we now modern humans have
linguistic spaces and who knows what else what else is out there.
So, so, so for sure with all of these things are as real.
They're they're they're super real.
Why? Because if you didn't have ways of navigating them, you'd be dead.
That's the ultimate.
That's the ultimate way of knowing whether something is real or not,
which is can I afford to ignore it?
That that's it. It's again, and I'm taking I'm once again,
taking very technological, engineering approach to this is saying that
things are real to the extent that you need to pay attention to them.
And they may be things you can hold in your hand and they may not be.
But to the extent that you will you suffer by not paying attention to them,
that's what tells you if they're real or not.
And so you for sure now it's not under your
conscious control, most of the most of these things are not under your conscious
control, but nevertheless, you've got apparatus in your body that constantly
is moving around in these other spaces and you better believe your body is good
at doing that and takes those things very seriously.
Otherwise, we'd be dead.
And so and so so that's kind of the starting point for this idea of diverse
intelligence, which is that we really need to get better at recognizing
different degrees of beings, the different unconventional minds that live
in these other spaces, and that includes our organs and various cells and various
smart implants as we develop them and and and so on.
That's interesting.
And the I guess how does the how do the attractor landscapes fit into that?
The attractor. Yes. Right. Yeah. Sorry.
I forgot about the that's OK.
It's a big question. I mean, there's a lot of different things.
There's a lot of different.
So so you can imagine any of these spaces, they have a topography, right?
They have they're not all flat and they have a topography with respect to
which states are next to which other states like which are adjacent.
Can you get from here to there?
How much barrier is it?
How much energy do I have to put in to get from here to there?
You know, if low potassium is over here, high potassium is over here.
Sodium is here.
You know, chloride is here.
And as you know, how many other dimensions?
How much energy do I need to use to move from here to there?
Meaning to crank up my my potassium state from here to there.
So so there's this there are these barriers
of all different shapes and sizes and navigating that landscape.
You have some ability to to know where you are and where you're going.
So, for example, one of these spaces is anatomical morpheus space.
So I'll give you a very simple example and then we'll we'll do a more realistic thing.
So so one of the first
well, one of the first people to talk about morpheus spaces was Darcy Thompson,
who in his book has some on growth and form, has some super amazing and prescient
images in there, which I still I think people still haven't
quite, you know, utilized to their full in your call.
Do you call the books called?
Yeah, it's called on growth and form.
It's very famous. It's a 1940s book.
I mean, most biologists know it, but but few people really pay attention nowadays.
But there's a couple of there are a couple of things in there,
which I think are super profound, which is which is specifically that that he shows.
He draws he draws these grid grids and then superimpose a creature on that grid.
Let's say a fish and then you apply a mathematical deformation to that grid
and you get a very different looking fish.
That's actually another fish, right?
So anyway, so so so a classic explanation of morpheus
spaces comes from a paper by this guy named Raup.
And what he envisions is seashells and snail shells of all kinds.
And the thing is that there's only there's only a few two or three parameters
that can be used to describe any coiled mollusk shell, you know,
because you got big, big, flat ones and the pointy ones and everything in between.
There's only a few.
There's a mathematical formula that generates the spiral and there's only a few
parameters in it. And so what you can imagine is this virtual space.
So let's say let's say just for for fun, let's say there's three parameters,
so you know, A, B and C. So you can imagine this is the A dimension.
This is the B dimension.
This is the C dimension.
And so every possible shell is somewhere in that space because it's defined by a
point that has those values of A, B and C.
So every every possible shell is somewhere here.
So now there are regions of the space.
There are regions and he actually in his paper, he actually draws this up.
There are regions that correspond to this set of species and there are regions
that correspond to no set of species because apparently those kinds of shells
wouldn't be very good in the real world, but they're possible.
And so he talks about different.
He talks about the geography of the space that here are the actual ones.
Here are the possible, but not the current here.
The ones that are impossible given biological development and here are the ones
that are possible, but really not adaptive and so on, right?
In a different region.
So so that space.
So so now think about anatomical morpho space for the for the rest of us.
There are a very high number of parameters that determine all of the possible
anatomical configurations that you might take.
So your eyes might be closer or further together.
Your head might be different shapes, you know,
and people of Zonoff and other people have studied this and bird beaks and various
things. The point is that all of these morphological spaces have attractors.
Attractors are regions in that space, which are easy to slide down into that.
Once you sort of come close, you don't have to do a lot of work to end up down there.
And once you're down there, it's harder to climb up.
So this kind of keeps, you know, it so so so different species, for example,
different shaped plenaria heads.
So we work with these flatworms, these plenaria, different shape heads correspond
to and that's the figure that you're talking about correspond to different
attractors in that space because all things being equal, plus or minus,
various environmental influences, you still, you know, the typically the acorn
ends up in the oak tree attractor and the the you know, the frog egg ends up in the
frog, the attractor, not always because we can push it into the xenobot attractor,
but that's a different that's a different that's a different story.
And and by the way, even for the oak tree, this is, you know,
kind of my latest thing that I point out to everybody.
If if under normal circumstances, acorns are really good at making flat green leaves,
right, oak leaves.
But actually, if you hack it the right way,
there are wasp parasites that send certain signals to those cells that create
gauze that are these amazing spherical red spiky things that don't look anything
like flat green leaves.
And that tells you that with the right prompting,
you can hack these kind of competent systems into finding other attractors
in more for space that they normally never go to.
And you can push them into that space.
And that space contains xenobots and various gulls and who knows what the heck
else it contains. And so and so those attractors are things that that evolution
has has selected to to utilize in the normal standard version of what happens.
And then we can do interesting things like in plan area, we can
drive them into other attractors that belong to the wrong species.
And that doesn't require changing the DNA.
That requires just changing the electrical
decision making that happens during the regeneration process.
And they end up in as other with heads of other species.
Yes. And that reminds me.
I'm not sure if it's in this paper or maybe it was something else that you
discussed on a podcast, but the
if something related to, yes, and manipulating it slightly, but not the genes,
I mean, just the bio electricity lighter,
the forms that it would take
was more or less like the probability of its ancestral of its evolutionary
cousins, let's say, the closest evolutionary.
So it kind of goes back.
Can you explain that a little bit? Yeah, yeah, that just means that just.
OK, so so so we don't have the technology for
controlling that bio electrical interface is still in its infancy.
We have we have fairly limited control over the richness of that bioelectric
interface. And so what that means is that in the plan area case,
we can't really choose specifically which head shape it'll get.
What we do, what we can do is we can disorient it to the point where it'll
do something other than its normal situation.
And that's not the case in these other systems.
We have lots more control in others cases, but particularly in this in this
specific case, we can't really drive it to one or the other attractor.
So it's stochastic.
It's you do it to 100 to 100 flatworms and you find out that 30 of them make
this kind of head, the triangular head and 25 of them make a square head and
whatever. So this is a spread of probabilities because we don't have
fine enough control yet in plan area to to to to have exactly the head shape
that you want. So so what my student and this was Maya Emmons Bell, who did this
work, what she was an undergrad, by the way, in the lab.
You understand that kind of amazing.
The work that that she did just show that.
The probability of ending up with one of these other species heads around and
flat and triangle and whatnot, the probability of getting one of these heads
was proportional to the evolutionary distance between the species.
So it's it's basically it's rarer that you find that you end up in the
attractor of an evolutionarily more distant form.
So that kind of makes sense.
If all you're doing is to imagine you're you're you're an autonomous vehicle
and you're trying to make your way home and now somebody like me comes along and
messes up with the bioelectric interface that determines where you're driving to.
You're much more light.
If you're going to make a mistake, you're less likely statistically to make a
mistake and end up very far away than you are to make a mistake and end up at a
closer attractor. And so so what that suggests is so that all makes makes
perfect sense, it suggests that evolution uses the same
tweaks, the same control knobs as we were tweaking, meaning this bioelectric
interface. And that means that yet that, yeah, the ones that the ones that evolution
that took the longest for it to accomplish are the ones that are further in
Morphous space because because you have to make more tweaks to have that that
vehicle end up in that in that location.
Yeah. Yeah.
And I think I'm not sure which paper was from, but there's a great visual of
them. It's like these circles encompassing one another, a molecule organelle
all the way up to biosphere.
And then you have the Morphous examples of from metabolic all the way up to
behavioral and evolutionary time.
I think that's a great way.
I mean, when I see something like this and just keep on thinking about the
importance of, oh, sure, the attractor landscapes.
And then I also think that there's, and this is really speculative, obviously,
but the when I think of these, obviously, the visuals make these things
potent, things like quantum fields and the excitations of particles within that
those fields or something like, you know, you say like the attractor,
it's like you get close to that space and then you fall into it.
I think of black holes and I'm thinking, OK, what is going on here?
What are the, is there?
And obviously, I mean, but informed by your research and your experience,
is there some kind of.
Are there are there and I mean, this going back to say the game of life as well.
Do you have an intuition as to what perhaps like the fundamental rule or rules are that?
I mean, those I was a lot to cover, you know, to touch all like all those different
places, but can you infer what would be something like an ultimate rule or set of rules?
You want the secret of the universe?
Oh, yeah, pretty much.
Yeah, if you wouldn't mind in two minutes, why not?
No, no, of course, I have no idea.
And, you know, the one thing we need to be humble about and I think this.
Fits with the whole poly computing observer focused view here is that
when all you have is a hammer, everything looks like a nail.
And we have these we have these ideas.
So so I lean very heavily on concepts of navigation and and and and, you know,
spaces and geometry and topology and all of this stuff.
Not because I know that this is the best frame.
This is these are the tools that we have.
And so we can push the ball forward a little further than than prior work,
which didn't use those tools.
That's fine.
But that's not to say that these are this is the best way to think about this.
Entirely possible, you know, I mean, I mean, Don Hoffman and other physicists tell us
that space time is doomed and that, you know, there's a completely different way
of thinking, OK, I can't even wrap my mind around some of that stuff.
So I don't know.
But but I'm totally open to the idea that this is everything that I've given you
is just the best current frame that I can come up with.
So it might be from the so what I'm saying is the reason that all these things
sound similar, you know, with the black holes and then all that,
maybe because that's the tools that we've been those are the tools we've been using.
I don't know if that will hold up or or
conversely, maybe we underestimate the the the multi-scale nature of things where
they had will turn out that, yes, it is all the same math.
I have no idea.
This is this is just the best, you know, the best frame.
All all I'm pretty sure of, actually, is that we really have to
commit to the fact that everything is observer relative and that you cannot make
decisions about the agency of systems sitting in an armchair making assumptions
that you have to test out different lenses and you have to ask how is that
how is that working for you with that with that lens and and to see how much
well, you know, how much
facilitation of research that that does and understanding and and and by the way,
in the last two minutes, I'll say this,
that that tape paper with a spectrum of persuadability, it was very focused
because I think it's important to push people in that direction, very focused on
an engineering approach to modifying and
controlling systems and engineering new systems.
OK, that's all great.
But as you head towards, I mean, I think it's very important and I'm writing
stuff about this that will come out at some point as you get,
the further you get to the right side of that spectrum,
you have to shift over from ideas of control to ideas of relationship.
And this is where a lot of the,
you know, ethics aspects come in because because our system is of ethics.
We have we were going to need new system of systems of ethics
that really take seriously the existence of unconventional minds that are not just
built on ancient and really just very childish notions of how you separate
things you have to care about from things that you don't need to care about,
you know, where were they were they engineered versus where they naturally
evolved, do they have a big brain?
None of these criteria are going to survive the next decades.
This is all that all this stuff has to go.
And new new systems of ethics has to come because all of this is really about
how do you relate to others, other systems?
How do you relate to things that look like you, to things that look nothing like you,
things that don't share the same evolutionary tree with you?
And and and in some cases that relationship means control in the way that we do
with with clocks and thermostats.
And in other cases, it takes a completely different form with with various
beings, unconventional beings, possible aliens, possible AIs, whatever,
that where that relationship is going to be quite different, where where where you
benefit from their agency, you don't try to, you know, sort of micromanage it.
That that's the lesson, I think, from all of this.
Yeah, that's wonderful.
And thank you.
I know I have to let you go because you have a commitment now.
But we didn't get time to talk about biology,
but as Buddhism and AI, unfortunately, man, always over promised.
Yeah, yeah, next time, if you're if you're willing, yeah, I'd love to chat again.
I really appreciate that, Mike, and thank you so much for your time today.
No problem. Thank you so much. Yeah. Yeah. Nice conversation. Thank you.

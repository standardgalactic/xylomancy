Alright, let's get started. Thanks for everybody who showed up. We've got three of the course
instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final
year, but we all were here. We all were students here. We all did our PhDs here. And it's good
to be back. If you don't know any machine learning at all, you've never taken an intro
class, then this will be probably a little too confusing. But as long as you've seen a
little ML, then you'll have a good time. And ideally, you know a little bit of Python,
and you know it like NumPy and Pandasar. So with that, let's just get started. So our
goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior
to this course, how many people have heard of data-centric AI? Okay, alright, well, get
excited because you're going to learn some great things. It'll be really good. Alright,
so let's just jump in. So this is a picture of a self-driving car that has had an accident.
And you notice that the article focuses on when algorithms mess up, right? And this is
the common case, right? In machine learning, we tend to focus on the model. And so it's
like there's a crash, the algorithm must have done something wrong. And so I'd like us to
sort of think of a different way of thinking about this. This is a paper many folks are
familiar with, which basically shows one thing in that a neural network or a machine learning
classifier can learn total randomness. It's like if you give it complete garbage data,
just like completely random labels, it can learn to map like images to completely arbitrary
labels or text to completely arbitrary labels. So basically, if you give it really bad data,
it will just produce exactly what it learns, even if the data is completely wrong. And
so I'd like to sort of rethink this title of this article as when algorithms are trained
with erroneous data, things like car crashes can happen. And that's the way we'll sort
of focus and think about this course. So traditional machine learning is very model-centric, right?
Like when you take an ML class, you first learn machine learning, you're in school and
then you get a data set. And usually the data set is pretty good. Like if anyone's seen
like the cat dogs data set, you know, it's in images of cats and they're all cats and
they're labeled cat. And then there's images of dog and they're all dogs and they're labeled
dog and there's no, there's usually no like cows thrown in there, right? And there's usually
not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then
your goal is to, you know, produce a good model, right? You want to train a model that
takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.
And that's sort of how we learn machine learning usually. And this is something that's standard
if anyone here has taken like 6036, which has been renamed to 630, 390, the intro to
ML class here. And you'll learn stuff like, you know, different types of models. And if
you're familiar with neural networks, neural architectures, or you'll learn about tuning
hyperparameters of a model, or you'll learn about modifying the loss function using different
loss functions and regularizations that you don't overfit. And these are common things
you learn in model centric AI. So let's just juxtapose that with the real world once you're
out of the classroom. So in the classroom, usually the data set's fixed and it's pretty
good, but then you go to the real world and actually the data set is not fixed, right?
You have user data or customer data or real world data and you can get more or less, you
can change the data and the data has all sorts of weird things in it. And so what tends
to happen is that the company or the user, whoever's sort of using this model that you're
trying to train, it doesn't really care as much about the cool ML fancy tricks as it
does about like, does it actually work in the real world? And if you have really good
machine learning models that work on highly curated data, but then the real world data
is actually really messy, then it makes sense to actually focus on fixing the issues in
the data. And a lot of people have been doing this, but what are systematic ways to do this?
And that's what we'll focus on this course. What may surprise some of you that you may
not know is that 10 of the most commonly cited and most used test sets in the field of machine
learning all have wrong labels. And that may be a surprise for some of you. So we'll just
take a quick look at this website, which I think will be pretty fun. This is labelairs.com.
And can we see okay? So this is a site that you can check out on your own. So this is
ImageNet, which is a common image data set. And these are in the test set, or in this
case a validation set, which is what was released. But this is the data that is supposed to be
the most accurate data, right? This is like your real world test data. And so this data
should be some of the most highly curated, most accurate data. And what you'll find
is that, for example, this scorpion is labeled tick. And you'll find all sorts of stuff.
We can look at another data. This is by Google. This is a drawing hand drawn data set. This
is labeled a t-shirt. And this is labeled a diving board. And this is labeled a scorpion.
And it just keeps going and going. And there's some fun ones. Like this is a, this is totally
a cake, but it's labeled a rake. Like it just didn't quite get the R to go all the way when
they were pretending they're writing it out. But anyway, you can check these out on your
own. This is for any type of data. So like text data or audio data. And you'll have fun. And I
think the good idea to think about here is that this is, you know, a data set that's released
by Google. It's a benchmark data set. And it's very difficult when you have millions of examples
to know, like, what's the bad data? And ideally, if you didn't have that bad data, you could train
better models. And so we need to learn how can we find these errors and find these issues
automatically. So going back to the slides. So seasoned data scientists, the way they'll approach
this problem is that it's more worthwhile to invest in exploring and fixing the data than
trying to tinker with the models to avoid this garbage in, garbage out issue. And the issue
is that, like, if you have millions of data, right, or you have, like, a data set that's
like 100 million, like, how do you do that without it being highly time consuming? All right, so
we're here in a data-centric AI course. We've called this introduction to data-centric AI because
we want it to be accessible. And so we're going to cover just, like, the very intro, what is data
-centric AI? How does it differ from model-centric AI? And then we'll dive in a little deeper. So
data-centric AI often takes one of two forms. So one form is that you have AI algorithms that
understand something about data, and then they use this new information that they've understood to
help a model train better. So an example of this is curriculum learning. And this is more of sort of
something you'd encounter in grad school, so you may not have heard of it. But what curriculum,
it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm that
looks at data and then identifies which data is probably easy to learn. And so the corollary here
is imagine you're a student and you're in the classroom. All right? Should the teacher, what
should they teach you? Like, should they teach you really, really hard examples as the very first
examples? Like, if you're learning addition, should they start out with, like, 10,051 plus 1042,
or would they start out with, like, 1 plus 2? Right? Now, we know this because, like, we've all
learned addition. But when a machine learning model starts, it's starting from scratch. So it
doesn't know right from the beginning what is the sort of easiest example. And so there are data-centric
AI approaches that actually estimate what is the easiest example. And then when you train an ML
model, you start with that example, and then you give it slightly harder ones and slightly harder
them. And so it's like curriculum learning because it's like a student's curriculum. And so that's
one way that you can sort of use new information. You're still training on all the same data, but
you're just reordering it and using this additional information. You don't actually change the
data set. Another sort of common form of data-centric AI is that you actually modify the
data set to directly improve the performance of a model. So an example of this might be something
like Confant Learning. And this is less known than curriculum learning, something I worked on. And
this idea here is that you want to find what are label errors in a data set and then remove them
prior to training, so that you just train on, like, correctly labeled stuff. And the corollary
here, in the sense of the student, is that if your teacher is basically making mistakes 30% of the
time, like, imagine as I was teaching you, which hopefully I don't do today, that 30% of the time
I told you wrong things. And then you compare that to another teacher who comes and they tell you
right things, you know, 100% of the time. Then, like, which teacher do you think you'll probably
learn data-centric AI better from? And so the idea is that we want to take this bad sort of, you
know, this 30% of wrong things and we want to get rid of them so that you could sort of come to the
classroom and redo your learning experience as if those never happened. And that's the idea here.
So you just learn on the good stuff. So those are two approaches. And then what we'll think about
now is sort of what is the difference between model-centric AI and data-centric AI? So the sort
of normal way you hear this is that given a data set, you try to produce the best model. And that's
like the classical way of thinking about model-centric AI. And the idea is that you want to
change the model somehow to improve, you know, some performance on AI task. And is there anyone
here just out of curiosity who's not familiar with like AI tasks and some of the stuff we do in
AI? Just want to make sure that we're on the same board, like classifying things. Okay, sweet.
All right. So you've got some AI tasks and you're trying to classify something and the goal is you
want to improve the performance on that. So usually it'll change the model to do that.
And data-centric AI, instead, it's given some model, right, that may be fixed or you may change,
but let's just assume it's fixed for now. You want to improve that model by improving your data set.
And so this is like the common way to think about the two. And the idea is to systematically or
algorithmically not have a bunch of humans changing the data set, but like some algorithmic way that
you can do this. Okay. All right. So our goal is to start thinking about ML in terms of data
and not the model. And so we'll just start out with a simple example. And this is not data-centric
AI, but it'll get us thinking about how we can think about AI in terms of data. So just quick
show of hands. Who here is familiar with K nearest neighbors? Okay. Great. All right. So I won't
belabor this point then. So do some chalk. That was a good thing to check beforehand.
We were here last night, but the chalk has been removed.
All right. So yeah, the key idea with K nearest neighbors and the key idea to think about in
the context of data-centric learning is that K nearest neighbors, imagine you have a space,
a 2D space, which, you know what, here's what we'll do. All right. So you've got some,
all right, sweet. So you've got, say, you know, some data, you've got some triangles,
and you've got some, some circles. You'll have to apologize for my drawing and you've got some
squares. And so this is your data set. And let's say, you know, these are different types of images
that you've put in some 2D space somehow where it's text that you've mapped a 2D space. And then
the idea of K nearest neighbors, as many of you seem to know and are familiar with, is that you
would have some new point, say here. And with this, it's some weird thing. We don't know what it is.
And we're trying to figure out, is it a triangle? Is it a circle? Is it a square? And so if this
was sort of three K nearest neighbors, like, then you would find the three nearest neighbors. So
say this one, this one, and this one. And then can somebody tell me, like, what the class would be and
why? Yeah, it'd be a square. That doesn't mean that it's not a cool point. And you're familiar
with being a square. So this would be labeled a square. And that's based on majority voting. And
there's a lot of different ways to do algorithms for deciding what the label should be. If it was,
say, five neighbors, meaning five and in, where K is five, so it's K and in, then you would choose
the five nearest neighbors and you would do a majority vote. Okay. The key idea here is that
there is no loss function. Literally, anytime you have a new point, so say I just have another
new point here, I'm not sort of, there's no algorithm that I'm passing this into
beyond just measuring a distance, some notion of distance between this and the nearest points.
And you can do a bunch of precomputation. There's a lot of smart work actually done in K nearest
neighbors that is pretty impressive. And you can do embeddings for all of these and precompute
distances. And you can do all sorts of fancy stuff. But ultimately, all this decision is based on is
the data. And so I wanted to motivate this problem and this way of thinking because this whole decision
process is made just based on data. And the quality of the data is as related as possible
to the quality of the prediction. So if you have really good data, you're going to have really good
predictions. And if you've got a bunch of errors in here, you're going to get the wrong prediction.
And so this makes it really clear why fixing the data will make it, will improve your model.
So is that kind of clear how this is motivating? Now, this is not a data-centric AI algorithm.
And by the end of this lecture, you'll definitely know what a data-centric AI algorithm is.
But can someone tell me what the difference is between K and N and a data-centric AI algorithm?
Yeah?
Yeah, totally. That's a great answer. K and N is just doing classification. And it's not actually
modifying the data set. So yeah, it's exactly right. All right. So what are a few other examples
of what is not data-centric AI? Handpicking a bunch of points you think will improve a model.
So can anybody help me understand why this is not data-centric AI?
Yeah?
Yeah, totally. It's just done by hand. Like this could, if you had 100 million,
a data set of 100 million points, this would take a long time.
What about doubling the size of your data set so that you can train an improved model?
Yeah? Yeah, totally. This is just classical machine learning. It's still all the model,
all the work you do as a model, but you're just paying more money for more data.
So let's juxtapose this. So what would be the data-centric AI versions of this?
And just out of curiosity, does anybody know, for the first one, what would be sort of the
data-centric AI corollary? Yeah, totally. And there's a whole field of research on this,
and a whole subsection of ML that's called Corset Selection, where you have a data set.
And let's say that you train a model on that data set, and you get like 98% accuracy.
But the data set is like 100 billion points. And so the goal is, can I find like, you know,
a million points? So if I just train on those, I can get like pretty close, like 97% accuracy.
And that's Corset Selection. What about for number two? What would be like a data-centric AI corollary?
Yeah, yeah. So the idea is, and how does anybody know of any ways that you might
make your data set like bigger without just getting twice as much data?
Yeah, totally. So like, say you have, so data augmentation, awesome. And so say you have an
image, you could just totally rotate that image. And now instead of one data point, you have like
five data points, or turn it black and white, or you could shift it or skew it or take the top
and the bottom move them a little bit or add some noise. There's lots of things you can do to make
data bigger and actually improve a model just by changing the data set. And these are all
fall within data-centric AI. Are you all familiar with what's called like back translation
for text data augmentation? So this is a pretty fun thing. I don't like have particularly good
translation skills. But like, if I say like, hi, you know, my name is Curtis.
And then I translate this. Okay, so it becomes hola.
And then I translate this again. It might become
and now I've gotten one data point and I just got another data point. So I was able to augment
my text data set to get more versions of the same thing. And this could have some label and the label
could be introduction. You know, I'll just end it there because I know it's hard to see on that side.
But this would be like the label. And this is text. And so this one is the same label. We
haven't changed the label at all. So this is intro. And now you can see I have more label data, but
I didn't have to do anything. I didn't have to pay more. I didn't have to just did this all
computationally. Yeah. Oh, yeah, totally can. So if say that this label was wrong, now you have
two label errors. Totally. Yeah. So you often want to combine two approaches. One is like first,
try to fix or improve label errors before doing the augmentation. And that's a really good idea.
All right. So what are some examples? We looked at examples that are not data-centric AI. So what
are some things that are data-centric AI? And for this, I'll go through quick because we're going
to learn all these in the course. And so one is outlier detection and removal. So
I'll just be really quick to show you. So say you have some data set and we've got, let me use this board.
And you've got say, right? So you've got your sort of your, you know, two classes. And so normally
you would, you would draw your classifier and then you have some new point here and that would be
labeled a negative. But say in your training data, you have this really weird, you know,
plus over here. And so maybe the boundary should be something like this, but you just don't have
very much information. And it's really out here. And it seems like it's not very related to the
rest of the data. And so what often people will do because they just don't have enough information
is they'll identify this as an automatic, as an outlier, because it seems very out of distribution
and they'll remove it. And so then you get this line, which fits to all the data except for that one
point. In terms of some other things in data-centric AI, so error detection and correction. And so
that's, for example, you just have like a data point that is, it could be images, it's like all
black, or you have a label error like we saw earlier. If this, you know, text example, instead
of being labeled intro was labeled like, you know, a goodbye clause or something, you'd want to find
that and automatically correct it. Data augmentation, which is what we saw earlier. So that's like
increasing the size of your data set. So you have more training data. Feature engineering and
selection. So the idea here is you have, if anyone's familiar, like the early days of neural
networks couldn't solve like the XOR problem, but you could always just generate the XOR as
another column. A more concrete example, if you haven't heard of this, that one is you just have,
you know, a bunch of tabular data. I worked on cheating detection at MIT. And, you know, if you
want the machine learning model to learn, say, who is a cheater from a bunch of, you know, education
data, like where do they go to school and what's their background and what problems did they answer.
It can really help if you also compute new features like how often did they submit answers
within five seconds of another student. So you can always generate new features and then you pass
those into the model. And your model can, you know, if the features are relevant to the label
you're trying to predict, it can do a lot better. Yeah.
That's a really fair and hard question to answer. You make assumptions.
And so, in this case, I was making an assumption, right? I was saying, like, I didn't draw very
much data, but say that I had, like, millions of data points and then this one's really far away.
Then the assumption you're making is that, like, I have so much data that suggests that this is the
distribution. And then I have very little data that suggests that this is part of that distribution.
And the biggest issue is that if your classifier is changing dramatically for one data point,
but you have, just think of it as, like, evidence. I've got a thousand or a million people saying
it should be this thing and then I've got this one other person who may be right, they may be right,
but they're saying that I think it's this other thing and it greatly skews the classifier. And so,
just because you want to trust the masses, you will say, hey, that one person is really,
seems really off base. And this is very much a choice. And so, what typically, what you do is
you sort of rank every data point in terms of how in distribution it is and then you choose
your cutoff and that's a human decision. Another data-centric AI task is to establish consensus
labels. So, if you guys have heard of, like, the self-driving cars, of course, then a lot of the
ways that these models are trained is you'll have an image and then they want really high quality
data. So, they'll have, like, 20 different people label, is this a scene of a street or are we on a
bridge? Is this a stop sign? Because they really need to get accurate labels. The question is,
when you're training your model, if you're just going to train with one label for that image
or one set of labels for that image, you can't use all 20 of your annotator's, you know, guesses.
You have to somehow combine them into a single training label. So, how do you do that in a way
that maximizes model performance? Another one is active learning and this is a very classical
problem. You have some data set, you train a model, it has 80% performance. Okay, I want now to get my
model to 85% performance but I want to pay for as little new data as possible or I want to improve
the current existing data as little as possible. What are my next steps? And you can automate that
process. You can actually get good signal to optimize in a way that you minimize the amount of new
data that you need to collect information for or label in order to achieve that model accuracy.
And then a final example is curriculum learning, which we already mentioned.
So, these are some examples of data-centric AI tasks, many of which we'll cover over the next
two weeks. All right, so there's a lot of hype around data-centric AI. For those who are familiar
with Andrew Ng, he's a pretty well-known person in AI from Stanford and has been at, you know,
Google research and body research and done a lot of things found in Coursera. So, he's been really
excited about data-centric AI and let's look at some reasons, you know, why and some of the things
that we've seen in the news. For example, he mentioned that like 80% of an AI developer's time
is actually just spent on data, which is kind of funny, right? You know, you're an AI developer,
you're not like a data scientist, but yet you're doing data science work all the time. And so,
there's something happening here in the real world that there isn't as much until not until
recently actually systematic, you know, learning and teaching around how do we go about doing this.
Also, if you're not familiar, bad data is very, very troublesome for businesses and for the government
and for economies. And it's estimated this is out of Harvard Business Review that it cost the U.S.
alone about three trillion dollars in a given year. And you might see this and think, okay,
that's really bad, but the good news is like a lot of people think that we can actually solve
a lot of that three trillion issue that bad data causes with data-centric AI techniques.
And so, there's a lot of hype around it because it means a lot to a lot of people.
This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and Jeff
Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer
Science, which is called the Turing Award. And so, I think they're old friends. And Jan has a
data set, MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning data
set. And we've been training models on it for like over 20 years. And people generally assume
that it has perfect labels because that's a very common assumption. Not maybe now in 20, you know,
23, but definitely like when it first came out. And it's a very high quality data set.
And so, Jeff Hinton was presenting at this time, I think, Capsule Networks. He's very excited about
it. And his aha culminating moment of his talk was that he found a label error in Jan Lacun's
data set. And so, he's very excited to show, you know, hey, this five image is actually labeled
a three in your data set, Jan. And he's like, aha, I got you. And so, I think that it's worth
mentioning that this is where we were in 2016. And now, you know, we're only six, seven years later
and we're able to systematically find millions of errors in data sets. And that's sort of how far
we've come using these data-centric AI approaches. Who here is familiar with Dolly and Dolly 2?
Yeah, it's pretty cool, right? So, it generates images. And they're pretty cool, like you can
generate images of pretty much anything you describe. And so, if you check out the Dolly
demo page, and there's the link here in the slides if you want to check them after,
there's a cool video. And you'll notice in that video, they talk about one of their biggest
challenges. And so, this is just screenshots from the video. And the technology is constantly
evolving, but Dolly 2 has limitations. It's taught with objects that are incorrectly labeled,
with plain labeled car, for example. And this happens because it's a massive data set. So,
if you don't use data-centric AI approaches, it's very difficult to clean, you know,
whatever, hundreds of millions or more, probably billions of pairs of text and image. And so,
what they notice is that a user might actually try to generate a car, but Dolly will actually
create a plane, because it's seen wrongly labeled data. And so, this is very problematic for something
that's deployed in the real world. Another example from that video is they talk about
generating, like, these baboons, but they emphasize that, like, you can only do this correctly if
you have accurate labels. And if you didn't, like, you'll totally, you can get the wrong thing.
And so, the key takeaway here is that this is a real-world technology. Lots of people are using
today at scale, but the reliability of that model really does depend on the data quality.
Another big example is familiar with chat GPT. Does anybody know, yeah. Does anybody know,
sort of, why, or, like, what was the big innovation from GPT 3, which obviously had a huge hype
around it, to chat GPT, which has even more hype around it? What was sort of one of the
big things they did between the two? Yeah, totally. And do you know what they were doing with their
reinforcement learning? Yeah, totally. Yeah. What was happening there was they,
they had a lot of bad outputs, you know, like, GPT 3 was saying things that were, like,
super biased or, like, inappropriate, not even true, just wrong facts. And these outputs were
tied to data it was trained on and to parameters in the model that were learned from that data.
And so, what they did is they did in a reinforcement setting, which means they talked to people,
they used that information to then update the model, then the model sort of explores with new
outputs, then people see those. But what they were doing is they were having people actually rank
them in terms of the quality of the prediction, right? And so, they were ranking the quality of
this data and then using that to update the model so that it would have improved less bias and better
outputs. And so, that was the key idea of chat GPT was actually to deal with a data quality issue.
And if you've tried both, you can see that there is a pretty big performance boost. The downside
is that they had to do this with a lot of manual work. And so, we went to work on ways to automate
that. You guys are probably familiar with Tesla. So, this is Tesla's data engine. This is from a
talk by Andre Carpathi, who is formerly the Tesla director of AI. And this is their data engine.
And we'll just start in the top left, like you, the way they're training the, you know, the self-driving
Tesla model is, you know, you have some data source and then you'll notice some problem which is like,
we're in a tunnel and we don't have a lot of, you know, tunnel data. So, the car is like
doing weird things, right? And so, then what they would do is they would collect a bunch more data
in tunnels and then update their training data and label them and then redeploy and go in the
world and then see what breaks then. And this is a very, you know, difficult iterative process
because you have to send the car out and then see where things break and then collect a bunch more
data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's
out of distribution. This is where we have a bunch of label errors and you're able to automate that
process. They could have reduced those cycles and gotten the car out a lot faster, at least the AI
part of the car. And so, this was a big pain that Andre mentioned. Another really, this is a fantastic
example that Jonas shared with me. These are all examples of traffic lights, you know. So, imagine
that, like, Elon Musk comes to you and he says, you know, Andre, I need you to get this car to
navigate any street in the world. And then, you know, you're like, oh, that's cool, okay. So, like,
it needs to stop at traffic lights. Like, that seems like a pretty simple problem. And then you go out
in the real world and, like, traffic lights are not a simple problem. They're really complicated
and they're really messy. And this is actually like a total nightmare if you had to do this. And so,
how do you find sort of systematic ways to group data together and train in a way that's robust
and reliable? Like, real world data is super messy and complicated. And so, Andre's big takeaway was
that, you know, he shares this juxtaposition of the amount of sleep, you know, he lost over in his
PhD and it was like, datasets is this tiny sliver, but definitely spent a lot of time on models and
algorithms. And then he goes and he's leading, you know, the AI model at Tesla and it's like,
it's all data, you know. And it's a big shock when you make this shift. And so, that's why you
really want to focus on ways we can improve that. A very common use case is when you're trying to
train a model with noisy labels. Okay? So, this is like the classical scenario of your, you have
the dogs and cats, but now say 30% of your cats are labeled dog. Okay? And maybe 20% of the dogs
are labeled cat. So, how do you get a model that does as well or close to as well as if you had
perfectly labeled data? And we'll go into that more in the next lecture, but I want to just
motivate that I looked at a bunch of model-centric methods and data-centric methods over the last
five years out of top institutions like Google and Facebook and so forth. And we benchmark them.
And it turns out, and there's a lot on this slide, but there's really one key takeaway that the
data-centric AI methods all outperformed the model-centric methods for this particular task,
you know, on this particular data set. And this was pretty revealing and compelling that there's
something here to data-centric AI approaches. And to be very clear, what these models are doing
is they are modifying the loss function or modifying the model so that they sort of don't train as much
on what they think is bad data, but within the context of the modeling. And what these methods
are doing is they're actually modifying the data set. They're either removing bad data or they're
generating more data that sort of makes the error go away, but somehow they're actually changing the
data set. And this is just how things stack up. And you'll see the data-centric methods outperform
model-centric methods in this task. And this is a very common task that's of interest to the field.
So it's cool. It's cool to see that this stuff is working and we're getting some benefits from it.
So a sort of culminating thought is, you know, we were talking a lot about ways that we want to
automate, but what did we do before? So obviously we've had to improve data sets in industry and
like outside of academia in the past. It's like, how did we do it before there was data-centric AI?
And so we mostly relied on human-powered solutions. For example, we would just spend more money
for higher quality data. So like, you literally just pay for more labels or you would pay for more
data. And that was a very classical way to improve a model. Also building custom tools. So like you
saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for
one specific problem, a very important problem, but they had to build a lot of custom tech around it.
Another common thing is just fixing data inside a Jupyter Notebook. So just a quick show of hands
like how many people have used Jupyter Notebooks. Okay, that's really good because all the labs are
in Jupyter Notebooks for the most part for this course. So yeah, what people do is like you just
sort data by like a loss function. And so you just say like the loss for this data point is the highest.
So I think this is most likely to be wrong. Let me check it out. And then you would look at it by
hand and then you would market or do something with it or you take the top 20 or something,
but you just do a lot of this by hand inside of Jupyter Notebooks and printing things out.
And that was actually a pretty normal and standard way that somebody in industry or
data scientist or grad student would try to fix a data set. And so the whole idea is we're going
to look at ways that we systematize these approaches so that they're more reliable,
more accurate, and they work on most data sets. So this course is about the following. So today
we're just looking at what is model-centric guy versus data-centric guy, get the juices flowing,
think about how to think things in terms of data and the impact, why matters.
Next lecture we'll focus on label errors. So how do you actually detect label errors automatically?
How do you learn with label errors? What are good methods to do that? And what are some things to
think about when you're doing this? Dataset creation and curation will be on Thursday. And this is
how do you construct a data set in such a way that you can train a good model? How do you arrange,
the classes? How do you choose good examples? And then finally on Friday, which is related to
data set curation, we'll look at active learning and potentially core sets and active learning.
As I mentioned, this is task where you're trying to choose the next data you want to add to your
data set and you want to obtain a label for. Or do you want to improve some of the labels you
currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going
to add to my data set. And it cost me something like either money or time. So I don't want to do
it too much. So what's like the best stuff to add to my data set now to improve my model?
Next week, we'll sort of have a bit of a shift and we'll focus more on data, but we'll focus on
some specific things. For the first, on Monday, we'll focus on class imbalance and distribution
shift. So this is, imagine like it's the stock market, right? And like if you're trying to predict
things, you know, on Monday or on in January of this year versus now, it would be a very different
market, right? And so over time, data changes. And so how do you continue to produce good,
reliable predictions even though data is changing? And then class imbalance is this problem where
imagine that for that line over there on the left, we had like a million pluses and only a few
minuses. Well, then a smart classifier could actually just always predict plus and get near
100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?
Interpretable features of data. So this is, who here's familiar with interpretability?
All right, not as much. That one will be fun then. And thanks for raising your hand.
That should be a good class. We'll learn about how do you interpret data in a way that you can
understand what's going on from a data perspective? Like why is the model doing what it's doing
in terms of the data? And so I understand model performance based on data. The next class on
Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we
know, you know, from a data perspective, how good a model is, how reliable it is, how well it's
working? On Thursday, we'll look at encoding human priors. So this is like, how do we augment data
and also prompt engineering. So this class will cover a lot of things like GPT and chat GPT and
transformer models and stuff like that. And then finally, our last class will be on data
privacy and security. And this is very, very interesting, especially for a lot of people
in like banking and finance. And they're using a lot of machine learning models. How do you make
sure that like a model doesn't actually secretly encode the data? Or like somehow, if you had
access to predictions, you could figure out someone's, you know, banking info and you can imagine
all sorts of things that can happen in AI. So data security and privacy is really important.
Any sort of questions while I'm on this slide?
Sweet. Are you guys excited? Okay. Does it look like a good course?
Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on
the course website, which we can, we can write on the board. So you probably have seen it in the
email, but just in case. And there's, there's a lab usually will be Jupyter notebooks. And the
one for today will be a text classification task. And it has some bad, bad data that's gotten mixed
in. And this is actual data that's been scraped from I think Amazon reviews. And it has some bad
tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches
at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.
And so you'll have to figure out how to improve the data set, you know, so you can get a better
classifier. And so that's sort of today's lab. Get your hands wet with data centric AI. We have
office hours every class, 3pm to 5pm. So an hour after the lecture ends every day. And then tomorrow's
lecture will focus on label errors, how to find them and how to train better models.
Um, this is the folks who are teaching the course for folks are from MIT to from Stanford. And,
um, yeah, I think it'll be a good time. Really quick. Are there any questions?
Yeah.
Yeah, that's, it's a good question. So there's a few ways. So one thing you can do is you can look
at a subset of the problem. So say you had like a very big complex problem, and there's thousands
of classes and millions of data points, you can take just 10,000 of those data points and a few
of those classes and actually check, do some process to check, make sure you have really high quality
data and see how well does your model perform. And if your model is performing very, very high
accuracy, but then when you use the original data, you get a drop off. That gives you a good
signal. Another thing is if you have similar data set and, uh, it's like MNIST for example,
you can get near 100% performance. And so you have a similar data set, but you're getting
like much worse performance or like significantly less performance, but using the same architecture
that you've seen do very well on a similar task on a different data set. And that's a good indicator
you should probably look at the data. Um, and there's two more things. One is just,
just take a look at the data. I think it's really easy to just get a data set and your,
your goal is like train a model. And so you're doing a lot of cool, you know,
download this TensorFlow package, download this hugging face package, but you don't actually
take time usually to look through all the data points or like hundreds of data points and really
see like, does this data look like what I think it does? Like, does it seem to be kind of messy?
And what you'll often find is after a first like, you know, first few hundred, you'll be like, oh,
there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.
Yeah, totally. Uh, in the, in the next class, we'll show ways that you can actually rank your
data set so that you know what's the best example to look at first. That's probably wrong. And so
there are ways that you can automate this and then you can look at the initial data.
And that's really the right approach. Like if you just look at random, then you might waste some time.
But if you've ranked your data in a way that is likely to give you a good ranking on quality,
and then you look at like the first 100 and there's really no issues and the data looks really good,
then yeah, you might be able to just optimize the model and be okay.
Yeah.
Any other questions?
Is this useful for anyone sort of immediately? Is anyone thinking like, hey,
this might be useful for what I'm working on right now?
I haven't done anything specific or I'm not quite sure before and I think one of the reasons
because I, you know, I only studied models before because it seems like, you know, studying
models is like very important. But like once you go and you want to like quickly build something,
that's kind of like the first thing that you want to do and you're like, okay, what do you do with
data?
All right, great. We'll be here for a little bit after. Thanks everybody for coming.
The next lectures will be a bit more technical, but I think it should be a really exciting course
and glad to have everybody here. Thanks.

Alright. As folks join the Zoom, I'm going to get started. Welcome everyone to the CHIP
Landmark Ideas series. Today we'll be hearing about rewriting biology with artificial intelligence
for Ray Kurzweil, an inventor and futurist. I'm Ken Mandel. I direct the Computational
Health Informatics program at Boston Children's Hospital. The program was founded in 1994
for a multidisciplinary applied research and education program. To learn more, you can
visit www.chip.org. The Landmark Ideas series is an event series featuring thought leaders
across healthcare, informatics, IT, big science, innovation, and more. Dr. Kurzweil is one
of the world's leading inventors, thinkers, and futurists. He creates and predicts using
tools and ideas from the field of pattern recognition. Invented many technologies familiar
to us today, including flatbed scanning, optical character recognition, and text-to-speech
synthesis. He won a Grammy for creating a music synthesizer used by Stevie Wonder that
was capable of recreating the grand piano and other orchestral instruments. He was awarded
the National Medal of Technology. His best-selling books include The New York Times bestsellers,
The Singularity is Near, and How to Create a Mind. Larry Page brought Kurzweil into
Google as a principal researcher and AI visionary. I'll just mention one connection to Chip.
Ben Rice, a faculty member, when he was a student at MIT, worked with Ray to develop text-to-speech
interface for that synthesizer so that Stevie Wonder and other non-sided musicians could
interact with the extensive visual navigation interface. The Singularity is a very important
idea of Dr. Kurzweil. This is the point in time when artificial intelligence will surpass
human intelligence, resulting in rapid technological growth that will fundamentally change civil
civilization. In order to understand when machines surpass biology, Ray has delved deeply
into an understanding of biology, and we're immensely looking forward to hearing and learning
and joining him in that understanding today. You've got us all looking forward, and the
Q&A of lit up. Let me start with one question. You're joining us for the seminar five days
after the release of OpenAI's chat GPT, which astounded many across the world in its ability
to synthesize natural language responses to really complicated questions and assignments.
If you've gotten to glimpse this technology, could you place it on the Kurzweil map toward
the Singularity? Is this a step forward? Is it a distraction? Is it related in any way?
Well, large language models occurred three years ago, and they seemed quite compelling.
They weren't totally fully there. You could chat with it, and sometimes it would kind
of break down. The amount of new ideas that are going into large language models has been
astounding, and so it's like every other week there's a new large language model and some
new variation that's more and more realistic. That's going to continue to happen. This is
just another issue. There are some things I think that aren't quite right with that
particular model you mentioned, but people have actually interacted with these things,
and some people say they're sentient. I don't actually think they're sentient yet, but I
think they're actually moving in that direction, and that's actually not a scientific issue.
It's actually a philosophical issue as to what you consider sentient or not. Although
it's a very important issue, because I would chat with Marvin Minsky, who is my mentor
for 50 years, and he said that sentience is not scientific, so therefore forget it. It's
an illusion. That's not my opinion. If you have a world that had no sentience in it,
it may as well not exist. But yes, there was a sizable advance, but there's more to come.
Let me ask a question from Charlotte, please. A philosopher, an AI-informed philosopher. What
do you make of the criticism that there's more to intelligence than brute processing
speed and pattern recognition? That if we want to pass the Turing test, we need to learn
more about how our own intelligence evolved. I'll just paraphrase you in The Singularity
is Near, comparing cognition to chaotic computing models where unpredictable interaction of
millions of processes, many of which contain random and unpredictable elements, provide
unexpected and appropriate answers to subtle questions of recognition. In this chaotic computing,
how can you address Charlotte's question about our own intelligence and the path forward
in AI?
It is a good observation, but chaos and unpredictability can also be simulated in computers. Large language models do that, because you can't always predict how it's going to answer. A lot of these models you can actually ask the same question multiple times.
It depends on the mood of the large language model at that time. To make it more realistic, it does have to take that level into account when it answers. At first, we could ask a question and give you a paragraph that could answer your question.
Now, I can actually give you several pages. It can't, though, give you a whole novel that can be coherent and answer your question, so it's not able to do what humans can do. Not many humans can do it, but some humans can write a whole novel that would answer a question. So that's the answer. It has to actually
cover a large amount of material, have an unpredictable element, but also all be coherent as one work. We're seeing that happen gradually. Each new large language model is able to actually cover a much broader array of materials.
material, but it definitely can handle stuff that is not just giving you a predictable
amount of, it has a way that is not really totally predictable.
So along those lines, let me pose Jane Bernstein's question, which is, what is your definition
of intelligence?
I mean, intelligence is to solve difficult problems with limitations of resources, including
time, so you can't take a million years to solve a problem.
If you can solve it quickly, then you're showing intelligence, and that's why somebody
who's more intelligent might be able to solve problems more quickly.
But we're seeing that in area after area, I mean, an alpha fold, for example, can actually
do things that humans can't do very quickly, or to play something like Go, goes way beyond
what humans can do.
In fact, Lisa Dahl, who's the best human player in Go in the world, says he's not going to
play Go anymore because machines can play it so much better than he can.
But that's actually not my view that it's going to replace us.
I think we're going to actually make ourselves smarter by merging with it, as I said.
So I'll ask a question from Sharon Weinstock, with AI taking over physical and intellectual
achievements and individuals living longer, do you have thoughts on society and whether
individuals risk lacking a purpose?
Well, it's good to hear from you, Sharon.
That's the whole point of our merging with intelligence.
If AI was something separate from us, it's definitely going to do everything that go
way beyond what humans can do.
So we really have to merge with them to make ourselves smarter, but that's why we create
these things.
I mean, we're separate from other animals in that we can think of a solution implemented
and then make ourselves better.
Now if you take what human beings were doing for work 200 years ago, 80% had to do with
creating food.
That's now down to 2%.
And so if I were to say, oh, well, all these jobs are going to go away and machines are
going to do them, people say, oh, well, there's nothing for us to do.
But actually, the percentage of people that are employed has gone way up.
The amount of money that we're making per hour has gone way up.
And if they say, well, okay, but what are we going to be doing?
They say, well, you're going to be doing IT engineering and protein folding.
And no one would have any idea what we're talking about because those ideas didn't exist.
So we're going to make ourselves smarter.
That's why we create these capabilities.
So it's not going to be us versus AI.
AI is going to go inside of us and make us much smarter than we were before.
So yes, I think if we did not do that, then it would be very difficult to know what you
and beings would be doing because machines would be doing everything better.
But we're going to be doing it because the AI is going to work through us.
Ronald Wilkinson has a question that relates to your idea of whether it's a dystopian society
or other, but really more specific, he says that he would expect people with various political
and or personal agendas to harness the increasing power of AI for their own purposes who will
not necessarily be to the long-term benefit of humankind as a whole.
So how does this balance out?
Could you go through that again?
Individuals with political and personal agendas may use AI for purposes that are not beneficial
to mankind.
How does that balance out?
Well, I mean, every new technology has positive and negative aspects.
The railroad did tremendous destruction, but it also benefited society.
So it's not that technology is always positive.
Social networks.
I mean, there's certainly a lot of commentary as to how it is negative, and that's true.
But no one actually would want to do completely without social networks.
And I make the case that we're actually using technology and measuring the kinds of things
that we associate with positive social benefit is actually increasing as the technology gets
better.
And that's actually not known.
I mean, if you ask a poll as to whether these things are getting better or worse, people
will say they're getting worse, whereas they're actually getting better.
But it's not that everything is positive.
I mean, there are negative aspects of it, and that's why we need to keep working on
how we use these technologies.
Here's a question from Mark.
The singularity is near.
In that book, you speculated that the risk of bioterrorism or engineering of viruses
will become an existential threat.
Since then, do you think this risk to humanity has increased or decreased?
I don't think it's increased.
I mean, I have a chapter in singularity is near, and there's also another one in singularity
is nearer on risks.
But all of these technologies have risks, and they could also do us in.
And I don't think the likelihood of that has increased, but I remain optimistic.
And if you look at the actual history of how we use technology, you could point to various
things that should have gone wrong.
Like every single job that we had in 1900, a year ago, a century ago, is gone, and yet
we're still working and making actually more money.
So the way we've used technology has been very beneficial to you in being so far.
From Greganus Ova, one of our faculty, Professor at Harvard Medical School, AI comes with large
energy resource demands and rare mineral material needs to build the hardware.
How do you see these international global tensions, especially the interaction pervasive
AI and the climate?
I mean, computers don't use that much energy.
In fact, that's the least of our energy needs.
And that's a whole other issue we didn't get into.
The creation of renewable energy sources is on an exponential, a very good chart that
shows all of the renewable energies, and it's on an exponential.
And if you follow that out, we'll be able to provide all of our energy needs on a renewable
basis in 10 years.
And at that point, we'll be using one part out of 5,000 parts of the sunlight that hits
the Earth.
So we have plenty of headroom in that.
So we'll actually be able to deal with climate change through renewable sources.
In terms of what we're using, computers are not that expensive.
From Tim Miller, will the singularity lead to a decrease in class conflict?
Much of the gain in productivity and wealth in the last 50 years has been concentrated
in the 1% as inflation-adjusted earnings in the working class have stagnated.
Are you concerned about gains in productivity due to AI being unevenly distributed?
And Don Goldman similarly comes in with this related question about inequities that, for
example, we saw exacerbated during the COVID pandemic.
I mean, my observation is that more and more people from more and more backgrounds
are participating, which didn't used to.
Third-world countries like in Africa, South America, and so on, did not participate to
the same extent, whereas they are participating far more dramatically today.
Countries that were really under the weather in terms of being able to participate in these
types of advances are now participating to a very smart, very large extent.
Anyway, that's my view on it.
Question from Bill Akava, one of our faculty.
A machine can easily beat the best human player at computer chess, but even a young child can
move pieces on the physical board better than any general-purpose robot can.
Do you imagine embodied machines will ever pass a physical Turing test in the real physical world?
And if so, when?
Yeah, we're making less progress with robotic machines, but that's also coming along.
And it can also use the same type of machine learning.
And we're going to see, I think, tremendous amount of advances in robotics over the next 10 years.
And for a science fiction-y question from Zhu Cheng, how do you envision society,
once individual brains can interface with a cloud, will individuality still exist?
It seems you imagine human intelligence coalescing into a singular consciousness.
Yes, definitely. I mean, that's one of the requirements of being able to connect to the
cloud is that this is your portion of the cloud and other people can't access it.
And we're actually doing very well on that.
And we have all of our phones connect to the cloud, and we don't see people
complain that other people are getting access to it.
So we're actually doing pretty well on that.
But definitely you'll be able to maintain your own
level of personality and differences.
And I think we'll actually be more different than we are today,
given the kinds of skills that we'll be able to develop.
Right. Well, Ray, this has been a spectacular hour that we've gotten to spend with you.
And I can tell you that in the lead up to it, I was contacted by many of the folks who were on
this webinar with us today, very excited to meet a celebrity.
They never thought they'd have the opportunity to interact with this closely.
So I thank you very kindly. And I also thank you for doing this later in the evening as you're
out at Oxford giving, entertaining the students there as well.
Yeah, that's been great to interact with you and all of your colleagues.
It's been, I've enjoyed it a great deal.
Great. Thank you. Let me therefore thank you again.
And I'm going to return to the slides for just a moment to remind people of our upcoming
talks in the series, including I'll highlight Rich Minor next month,
inventor of the Android operating system. Also, also a Googler who actually resides
a lot of the time here in New England. And remind folks to be sure to reach out to us at Chick.
If you're interested in training, interacting, researching, teaching,
being in our seminar series. Thank you very much. And we will see you next month.

So I don't like my title, but I haven't been able to find a better one. And what it means will
become apparent as I go through the talk. First, I'm going to give a brief introduction to a field
called evolutionary computation since I'm using it. And some of you may not have seen it. Then
I'm going to talk about the first fractal representation for optimization that turned
out to have a bunch of cool properties except it didn't scale. And then we found a way to fix it.
And that led to something called the walking triangle representation, which there's sort
of a picture of over there on the right. And it turns out that it's an address book for all of
Euclidean space. And then I'm going to talk about how this and one other representation that I'll
introduce, open up at least two new sorts of algorithms. Okay, so, onward. That is a picture
of Charles Darwin when he was a young lad before he got the discouraged expression in the long
beard. And the upper thing is an outcome of evolutionary computation in which it's a map
for a video game. And the player that can walk through the red walls can get everywhere quickly.
And the player that can walk through the blue walls has a long winding maze. So that's an
example of something you can do with evolutionary computation. Similarly, that network of one and
10 ohm resistors has resistance of 3.14159 ohms, completely useless, but sort of an interesting
demonstration that you can ask it to solve problems for you. So, evolutionary computation
is any algorithm based on Darwin's theory of evolution. And a lot of people thought of this.
In fact, there's a book called evolutionary computation, the fossil record that includes
the 12 times people thought of it before computers had enough power to make it possible.
The people that thought of it just as computers became powerful enough include
Holland and Goldberg who invented genetic algorithms, Lawrence Fogel's evolutionary
programming, Ingo Reichenberg and Hans Paul Schreifels, evolution strategies, and there's a
bunch of other flavors. In general, they're best used on problems you don't understand well yet.
They're very easy to code but run slowly. And there's a lot of design choices you have to make.
You need to have a problem with a well-defined measure for quality of solutions, a data structure
that can store solutions, a technique for taking two solutions and blending them and then modifying
the results, a technique for picking which solutions you like, so picking with a pro quality bias,
a technique for throwing out solutions, and a way to decide it's time to stop. Now, one thing to
note, a lot of people have been talking about things that use like gradients and so on. Evolutionary
computation is a population based technique and that means it just operates on a whole bunch of
different solutions using comparison between different population members to direct search.
This permits it to do hill climbing like things when there is no gradient available.
Okay, the basic loop is simple. You go get some structures, you find out how good they are,
then over and over you pick parents with that quality bias, you generate children, evaluate
their quality, pick people that might die, and then replace those people with the children if the
children are at least as good until you've got a good enough answer. The techniques for blending
parents are called crossover operators, the techniques for modifying individuals are called
mutation operators, and collectively we call those variation operators. They produce variations in
the solutions. There are other variation operators, that's something I do research on but not today,
and the quality measures for the solutions are called the fitness function. Picking and discarding
are the selection, are called selection within the algorithm. And so to give you an idea of what
the choices are like, if the parent and the child had some sort of linear structure like DNA or a
list of real parameters which is what I'm interested in today, then you could exchange tail, middle
segment, or for every entry in the genome you could flip a coin to decide which parent contributes
what information to which child. So after years of doing this, I'm fairly sure that getting the
fitness function right is critical. The representation or data structure used to store your potential
solutions is critical. The selection and replacement technique are very important. The population size
can be very important by which I mean there are problems where it is and there are problems where
it isn't and I really don't understand why yet. The variation operators are usually very important
and the rate at which you apply those operators are moderately important. Sort of like infection
with the plague. If you get it wrong, the plague spreads faster. If you get it right, the plague
spreads slower, but that parameter isn't important because all it does is slow things down a little.
There's also a technical theorem called the no-free lunch theorem. It has important consequences
in evolutionary computation. The good choices of all of the parameters you can set depend on your
problem. Performance is improved by correctly specializing the algorithm to your problem
and when possible, you really want to incorporate anything you know about the problem. Okay,
I'm going to take a sip of coffee and let the audio track catch up.
You may recognize the Sierpinski gasket, which is the leftmost of those pictures,
but if you take any polygon, start at one corner of it and then jump toward a random corner of the
polygon and plot a point where you go and by jump toward, I mean move halfway toward, you fill in some
sort of fractal, except in the case of the square where if you can jump toward any corner, you
completely fill in the square. That's cool though because when the figure fills in, you can use a
string of averaging commands as a representation. If I say corner one, corner one, corner two, corner
one, corner three, corner four, corner one, corner two, corner three, corner one, corner four,
then I've arrived at some point in the interior of the square by averaging toward those corners
and that string of averaging commands is a representation. So instead of specifying x and
y as numbers, I specify a character string. This actually works for any sheared hypercube in d
dimensions, but there's a problem with it, which is you need to be able to average toward every
single corner, giving you an exponential number of letters in the alphabet you're using.
Okay, now Leon Palladian, who has since passed away about two years ago, an Australian mathematician,
pointed out to me that if instead what you did was you started with a simplex, a triangle in
two dimensions and instead of averaging toward corners, you moved a vertex of the triangle
through the center of mass of the triangle repeatedly, then you could still partition space,
but instead of needing two to the power d letters in your alphabet, you only need one for each corner.
A triangle has three corners in two dimensions and in general you need
one more character than you have dimensions. This representation is still a string of commands
that selects a point. The point encoded is the center of mass of the final simplex,
but it scales better with dimension. So why would you want to store your real parameter
string or your set of real parameters as a character string? That sounds a bit nuts in some
ways, and the reason is is that it lets you save those strings in a dictionary. So you can go find
an optimum, use it to initialize a dictionary, then rerun your evolutionary algorithm to locate
more optima, but you award the worst fitness you have going to anyone that shares a prefix of a
certain length with a member of the dictionary. That means the dictionary lets you blow holes in
the search space surrounding the optimum you've already found, and the length of the prefix tells
you how big those holes are. You then put each of those new optimum in the dictionary. This is the
multi-optima Sierpinski's searcher, and by using it we can enumerate the optimum present in an area
being searched. This is very similar for those of you familiar with evolutionary computation to
niche specialization, but instead of needing to compute the differences between all the solutions
we found, we simply use a dictionary, which means the time to exclude optima already located is
constant. That's wonderful. So here is a test problem. You probably recognize the Mandelbrot set,
and the Mandelbrot set has all sorts of cool fractals inside it. My representation is x y and s,
so x plus i y is a corner of the fractal I'm looking at, and s is the side length,
and that picture at left shows the location of a thousand optima located using moss,
with the fitness function being rms error with the desired appearance mask. In this case, what I
have is a square somewhere, and what I want is for the fractal to have long escape times near the
middle of the square, and very short ones near the edge. Essentially I put a hill over the square,
and what that should locate is the small copies of the Mandelbrot set inside the Mandelbrot set,
and there are the first 300 optima, and as you can see it does exactly that. It locates
mini brots as they're called. Next three slides are going to show you the 24 best, the 24 middle,
and the 24 worst optima located. Lots of big fat mini brots in the top 24. In the middle,
we're getting much smaller mini brots, and also spirals and starbursts and other things
that have a lot of action near the center and less near the edges, but not so much, and this
makes me think I found a substantial fraction of all the mini brots within the search domain,
which was the Mandelbrot set, and up to e to the minus tenth power of zoom. They're actually an
infinite number of these things, but if you cut the zoom off at some point, you only get a finite
number, and those last 24 optima are awful compared to the first 24. Okay, pause to lift the audio,
track, catch up again. Now the original Serpinski representation averaging toward corners of a
hypercube didn't scale well, and instead Leon Palladian helped me find the contracting triangle on,
which does scale well, but it also reminded me of something else, and I'll mention this again,
which is the Nelder Mead optimization technique, which was first published in 1965.
So now what I'm going to do is I'm going to take that triangle, and or simplex and n dimensions,
whose center of mass represents the parameters I'm actually trying to find, and a vertex is just
a vertex. A face is the simplest one dimension lower consisting of all but one vertex. I'm going
to call that the opposite face, and the center of mass is what it usually is. There are the moves you
can make in walking triangle. You can either flip one vertex across a face, you can move a vertex
to the center of mass, you can reverse that, it turns out that's an invertible operation,
or you can contract or expand away from a vertex. And that means we have five moves, each one also
has to say a vertex. So we've got five times the number of dimensions, which is a not terribly
large number of moves, and it scales linearly with the dimension. Okay, I'll go since I have
half an hour, I'll go through this a little quickly, but there is a formula for the walk move,
there is a formula for the center and uncenter move, and there are is the formula for the
growing and shrinking move. The walking triangle moves are all bijections of the set of simplices
in n dimensions. This means the strings of the walking triangle commands can be viewed as words
over a group. And notice that the walk command is self inverse, and then the center and uncenter
commands and the growing shrink commands with the same growing shrink factor are all group
inverses of one another. The mathematical formalism isn't gratuitous, it turns out we can prove things.
And here's one of my favorite things about this. For any epsilon greater than zero, if p is a point
in general position and d is the distance from the center of mass of simplex as 2p, then there exists
a finite sequence of walking triangle moves, q1 through qm, that places the center of mass
of s as modified by those moves within epsilon of p. This is true for walk, uncenter, center,
walk, grow, shrink, and walk, uncenter, center, grow, shrink as sets of moves. The, okay, so
finite, cool, how big? The number of moves in the sequence is proportional to the log of the
distance. These results demonstrate the walking triangle representation permits a completely
different kind of evolutionary real optimization, and I'll give you at least one demonstration of this.
So here's the sketch of the proof. The grow and uncenter operations can be used to enlarge a
simplex with a little care in any direction, and the hypervolume increases exponentially with the
number of enlargements. That means we can grow the simplex to include the target point p. You
sometimes need one walk command to get oriented correctly, but then you can center or shrink
in a fashion that decreases the hypervolume exponentially while leaving the point within
the simplex. So I have exponential growth to capture, exponential shrinkage to get as close
as I want. That's why it's log moves. Now, when you're doing evolutionary computation, you need
to worry about the variation operators. The mutation operator that just makes a tweak to one
structure is simply replace one move with another, but the crossover operator that mimics natural
sex that mixes the chromosomes is best one point because the walking triangle operations do not
commute. There's a very high degree of epistasis, or to put it another way, the worth and meaning of
later moves depends on what the earlier moves are. So you want to do as little mixing as possible,
though we tried no crossover, and that's worse than one point. So one of the things I did was to
try to demonstrate that the walking triangle representation can go out and find things that
a normal evolutionary algorithm can't. And so what we did was we initialized near the origin,
the whole population of points, and had them go hunting for a hill at the origin, centered at 40
by 40 away from the origin and centered at 100 by 100 away from the origin. Evolutionary computation
is pretty good. We still had 30 out of 30 runs of a standard evolutionary algorithm find the way
off in the distance hill when it was at 40. But when it got to 100, walking triangle continued
to always find it, whereas the other one had 27 failures and three successes. So walking triangle
representation is robust to bad initialization. Now we then picked the three-dimensional version
with the hill displaced 10 and looked at the walk on center center. And then we tried a bunch of
different copulation sizes and tabulated both the log of time to solution and the number of failures.
And there it turns out that if you have a larger population, it works better. Failures go down,
time to solution gets better, but time to solution starts to go up when you get the failures to be
down near zero. We then checked the mutation rate, and it turned out that, you know, there was a
favored mutation rate in the sense that you need enough, but then it flattens out. Similarly,
the, normally when you're doing optimization in n dimensions, you have n parameters. But in this
case, since all our length of gene is doing, or the number of commands we're using is doing,
is letting you do more operations, it turns out that the length of the gene you're using is another
thing you can study. And then once we got these three initial runs we fiddled, and it turned out
that of the ones we studied, population size 178, three mutations in gene length 30 gave us the
best performance. And in case you're wondering where on earth I got those population sizes,
that's an equally spaced grid in the logarithm. Okay, so what is a walking triangle representation
chromosome? I've been talking about them as strings of letters. But if we have the string of letters,
walk one, uncenter zero, center one, it's actually center one function of uncenter zero function
of walk one function of the original simplex. And so the walking triangle chromosomes say
what to do with simplex, but they don't say where to start. And what I've been using so far is a
very simple starting simplex, the standard basis plus the origin, and just letting the fact that it
can go search a long distance, save my life. But what if we actually got better starting
simplices? Okay, so I don't know where they are. But one thing you can do is run the walking
triangle for a while until it finds a simplex that is much better fitness in your starting one,
then restart the whole thing with that simplex as the new starting point. Since some of the
operations let it make very local search, if you're in a really good place, it can stay there.
And that picture there shows you how this re-centering and re-centering works. So
here's a comparison of just running the algorithm I ran before with running it with five
recenterings. The recenterings have pretty much no additional cost. And the thing to look at there
is the time solution remained fairly similar, but the number of failures plummeted when we
re-centered. So it's a good technique. And so another thing that's a little odd about this is
the gene length matters, but it matters less than you think because the representation is free to
put inverse members in the group beside one another. And there's some genes where we ran three more
of these displaced hill functions and fitness maximum was 1.0. So even with the difficult one
where the hill center was displaced in different directions, we still found the right answer. And
the adjacent inverses showed up a good deal more often than they should. So evolution is in some
sense exploiting the existence of inverses to seek the correct gene length. Cool. Okay, now
this is a completely artificial function I thought up to test how good an evolutionary
algorithm is at exploring. In this case, what I have is a very low slope plane, slope roughly one
over 20 in each dimension, with cosine waves added to make it confusing. It maximizes in the
direction of 1111111111. So this is a very simple plot of the log of the fitness located as a
function of the gene length when I let the algorithm run long enough that it's stabilized. And as you
can see, it's almost certainly completely ignoring the cosines. It just finds the right direction
and runs that way. Okay. And yeah, that's exactly what happened. I showed you the trend wave. I
have a couple other examples of these open-ended functions. And as you can see, the genes that
found the good results were just maybe get pointed in the right direction and then
un-center in that direction over and over and over. And after about three un-centerings,
as it jumps, it's jumping past hundreds or thousands of cosine waves. And so it just ignores
the confusing cosine noise. Okay. Almost done. So assume we're optimizing a function and end
dimensions. And we have a starting point. This representation takes three parameters. Delta,
the initial value for modifying the value of a coordinate, and L and S, a large and small
scaling factor. This is another thing where I'm turning real parameter location into a
game or string language. And there are five commands that are always there. Negate the
coordinate value or scale it up or down by a factor of S or L. The remaining commands say
add delta to one of the n real parameters under consideration. So that gives us n plus five commands,
which is a very reasonable number, again, linear in the dimension. And it works better with two
scaling factors than with one. That's something we actually tested. And I'm going to call this
the rescaling vector jumper representation. We have one paper on this, which is currently being
reviewed. But let's wind up. Discrete representations I've looked at include the Sierpinski representation,
the averaging toward corners of a hypercube that scales to higher dimensions poorly.
The Sierpinski-Palladian representation, which is just the centering commands from the walk
un-center-center version of the walking triangle representation. The full walking triangle
representation, which is a little complicated, and the rescaling vector jump representation that
I just showed you one slide ago. The first two uniquely represent points in the string language,
and so can be used for the multi-optimist Sierpinski searcher that I showed you all those
mini-brots with. All the representations reduce specifying a vector of real parameters to a
sequence of commands, which can also be thought of as moves. And again, I said this earlier,
but the walking triangle is the result of seeing if we could generalize Nelder-Mead optimization.
So, new opportunity one. That thing on the left is a Monte Carlo tree search diagram,
and these were like key to the Alpha Go that Google had that got us to algorithms being the
best Go players 20 years sooner than everybody thought. But in fact, once you've discretized
the specification of real parameters, any discrete search algorithm, including all those
tree search algorithms that we teach people in second and third year computer science,
can be used to do real parameter optimization. And I'm just starting to explore that space.
I know that other people have used Monte Carlo tree search to do traveling salesman problems
successfully, and one of my students and I are exploring using it to do real parameter optimization.
This is what we actually did with that funny new vector jump representation. This is what the
submitted paper about is lighting optimization. It's a technique that's designed for use with
discrete representations. Every configuration that arises, all of the triangle walks or all of the
vector jumps, during the expression of these discrete chromosomes from the first one to the
last one, specify a vector of real parameters. That means we could find this fitness of each of
the intermediate states. If we visualize the sequence of intermediate stages as sequences
of steps across a fitness landscape, there is no particular reason the most fit configuration
in the sequence is the final one. For this reason, the fitness of the chromosome used in
lightning optimization is the best fitness of any of the intermediate solutions visited,
including both the initial and final one. The term lightning is evocative of a stroke of
lightning whose impact is checked at each twist and turn as it moves through a fitness landscape.
We turn the sequence of configurations generated during the expression of full lightning
optimization chromosome as a stroke. The major impact of lightning optimization is to smooth
the search landscape by taking a best of several approaches to fitness. We've tested it on several
rough fitness landscapes, and there it definitely outperforms a normal optimizer. And before anybody
charges me with potential stupidity, yes, if we had 20 checks in the lightning, then the normal
optimizer got 20 times as many fitness evaluations. So the actual consultation of the fitness function
was the same in both. And so that's it. I'd like to thank the money and my collaborators who
worked with me on this, Onyong Kim, Cameron McGinnis, Colin Lee and Jeremy Gilbert. Okay, ready for questions.
Perfect. Thank you very much, Dan. So if you have any questions, please give your name in the chat
or give your questions. In the meantime, I have a very small one for you, Dan. So when you do these
recentering steps, yes, when you're restarting the trial, how do you
choose how many times you do this or how do you end all this?
I did it five times. Yeah, but why five? Why not six or four?
So that's a really good question. And the answer is the good number of recenterings depends almost
entirely on what fitness landscape you're exploring. So it's the experimental parameter,
which you check for each function you work on. Sorry, bad answer, but at least true.
No, no, okay, I understand. I mean, if you were doing this on a single hill, the answer would be don't.
Yeah, okay, okay. Unless you would optimize way off in deep space away from that hill.
If you're anywhere near a unimodal set, just don't even bother. If, on the other hand,
you have something like the metalbrot search, which is an unbelievably rough surface,
then you probably want to do it fairly often. Yeah, way too much of my work is,
ah, we've located a new parameter. Let's do an experiment.
No, no, no, but I agree because I've used similar techniques for local optimization.
And like the same questions always pop up. So I was wondering.
I'm going to stop my share so that I can actually see the chat. There we go.
So we have time for more questions. If some of you guys have some.
I'm going to prompt Dan into an answer because I know him and I at some point
collaborated on some work with one of our students of some years ago. And I think she
used some of your Serpinski type representations to solve a generalized Nash games,
the ones without shared constraints. So that would be, am I right?
Yes, you are. And the generalized Nash games often have a relatively low dimension
at which point the problem with that representation doesn't kick in. And so if you want to find a
whole lot of optima in a low dimensional space, the basic average toward the corner's
moss algorithm is brilliant. And yes, she did. Right, right. But we were not testing it. We
did not test it on any like real hands on many players problem. I guess then then we would
have seen the same, the same, the same issues that you're seeing. It dies at about six players,
maybe seven. Oh, darn. The Palladian Serpinski one could probably be used to do it in any number
of dimensions. However, let me then go back and say something. The original average toward the
corner ones has equal density in every part of the space. And it covers the space at one over
two to the N, where N is the length of the string of commands. The one based on contracting a
triangle has irregular coverage. It covers points near the boundary of a triangle, a larger triangle
more than in the interior. And so in that sense, it's inferior. So if low dimensional use the
hypercube, high dimensional, you're stuck with the triangles.

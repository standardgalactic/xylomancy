Hey, everybody. Welcome in. My name is Alex Zagel, and I'm really glad to see everybody
can make it out today. So for those that don't know, this event is being put on by Effective
Altruism at UT Austin, the student group. So we're a growing order on campus, passionate
about doing good. And before we get into what Dr. Ironson has for us today, I'd like to
give you some context on us and the broader community through a really brief intro on
what Effective Altruism is all about. So Effective Altruism is a research field and
a community dedicated to working out how we can do the most good with the resources that
we have, and then acting on those conclusions. So some people donate, some people volunteer,
and some pursue careers to make a positive effect. So we use evidence, when possible,
to work out what actions really do help and seek opportunities to do good that are the
most cost or time-effective, meaning they produce more benefits per dollar or per hour
of our time. This focus on cost effectiveness is crucial, because it turns out that some
charities can do vastly more good than others with the same amount of resources. And by
choosing these most cost-effective actions, even a modest amount of money can make an
enormous difference to the lives of others. We also try to think carefully about what
it means to do good, including considering who or what should be within our moral circle
and what causes we should prioritize. In-cause prioritization can be very hard. There are
just so many problems in the world that we could work on. Climate change, human rights,
education. The main framework for selecting causes that the community uses is scale, how
big is the problem, tractability, how easily can we make a positive difference. There's
no point working on a problem if you can't make any headway. And then finally, neglectedness.
This is about how much resources are already going into this problem. If an issue is neglected,
it often means that each additional person working on the problem can make a big difference.
So if we find a problem that rates highly on two or three of these factors, it is much
more likely that we'll be able to make a large impact than if we just choose whatever
cause comes to our mind most easily. This sort of prioritization framework often ends up
focusing our attention on those who have little or no voice in today's world because their
needs are usually greatly neglected. That includes people living in extreme poverty
in the most disadvantaged communities in the world. Animals, especially those living in
backwood farms, that lived and died in appalling conditions. And another group whose needs are
neglected, those who are yet to be born. Threats such as nuclear war, new pandemics,
extreme climate change, and other new technologies like artificial intelligence,
which is the focus of today's event, could all cause an immense suffering in the future
and might even provide our species survival. And our generation is in a position to create
a better world for those who will let air out climate. There's much more I can touch on here.
We could talk about specific interventions or career paths or charities that have done
the exception of work in the space of effective altruism. But I'd like to keep it short. So here's
just two ways that you can get involved if you find these ideas as exciting as we do. So one thing
you could do is just learn more about it yet. In the follow-up email for this event, we'll
include an intro piece on effectivealtruism.com. I think that's a really great place to start.
Another is to attend one of your local groups to meet us. If you're a student,
the night group would be us. Our next meet-up is tonight at 7 p.m., where we'll be doing a
special info session for those interested in hearing more about our org's regular events
and the AI-specific fellowships we're running in the spring.
Yes. For professionals, but also students, check out the Vibrant's EA Austin community
and their speaker events, weekly meet-ups, and one-on-one advising they host at EAAustin.org.
Two of their organizers, Ivy and Will, are actually here tonight. So be sure to
speak with them after if you'd like. Their next event, which everyone is invited to,
is this Thursday evening, although we'll be hosting a researcher from the Global
Priorities Institute in Oxford. Of course, all this will be in the summer email you'll
get right after the event. That said, let me delay no longer introducing Dr. Aronson.
Dr. Aronson is a slumber-chase centennial chair of the Computer Science here at UT
and director of the Quantum Information Center. Prior to coming to UT, Aronson taught for nine
years at Electrical Engineering and Computer Science at MIT. His primary area of research
is theoretical computer science and his research interests center on the capabilities and limits
of quantum computers, and computational complexity theory more generally. This year,
he is on leave to work at OpenAI on theoretical foundations of AI safety. Everybody, please give
a warm welcome to Dr. Aronson. Thanks a lot. Can you hear me, the mic mark? Okay, so thank you so much
UT Austin AI for inviting me here. I do feel a little bit sheepish to be lecturing you about AI
safety as someone who has worked on this subject for all of five months. I'm a quantum computing
person, but as I'll tell you, I accepted a very interesting opportunity to go on leave and spend
a year thinking about what can theoretical computer science do for AI safety, and this is an OpenAI,
which is one of the world's leading AI startups. They're based in San Francisco, although I'm
mostly working from Austin. Now, despite their name, OpenAI is famously not completely open,
so there are certain things that I'm not allowed to talk about, like the capabilities of the latest
models, whether they are astounding or not, but they're very, very happy for me to talk to everyone
about AI safety, and what is it, and what, if anything, can we do about it. So what I thought
I do is, I'll tell you a little bit about the specific projects that I've been working on at
OpenAI, but also just share my general thoughts as a newcomer to this area, and about how effective
altruists might want to think about it, as partly just a prompt for discussion, and then I'll try to
leave plenty of time for discussion and for Q&A. Maybe I should mention that the thoughts that
I'll tell you today are ones that, until last week, I had considered writing up for an essay
contest run by something called the FTX Future Fund. Unfortunately, as many of you know, these
FTX Future Fund no longer exist. It was founded by Sam Bankman-Freed, whose net worth went from
$15 billion to some negative number of dollars in the space of two days, and maybe one of the largest
financial calamities in recent history. This is obviously a terrible event for the EA community,
which had been counting on funding from this individual, and I feel terrible about it,
but it means that instead of writing up my thoughts for that essay contest,
I will share them with you for your charge. So let me start with this. You raise your hand
if you have tried GPT-3. That's maybe half of you. Okay, raise your hand if you have tried Dolly.
That's okay. Again, maybe half of you. Okay, so these are maybe the two best known products
that are made by OpenAI, and they are, I think, two of the most impressive AIs that exist in the
world right now. They certainly go far beyond what I would have predicted would have been possible
right now if you had asked me 10 years ago or even five years ago. I'm explaining them to people.
I'm kind of like, well, you really can't. You have to say it to believe it. So here,
this is what GPT produced when it was asked to write a poem about cryptocurrency in the style of
Philip Larkin, who's a famous modern poet, and that seems particularly appropriate given current
events, actually, this poem. Money is a thing you earn by the sweat of your brow, and that's how it
should be. Or you can steal it and go to jail, or inherit it and be set for life, or win it on the
pools, which is luck, or marry it, which is what I did. I don't think GPT actually did that.
And that is how it should be, too. But now this idea has come up of inventing money,
just like that. I ask you, is nothing sacred. Okay, it won't always produce something of this
quality. Sometimes you've got to run it several times and take the best output. But this is the
kind of thing that it can do. And I think if this were printed in the New Yorker, and it
was not labeled as coming from GPT, it'd be like, yeah, that's the kind of
poetry that the New Yorker publishes, right? And then, you know, this is a thing that AI can now do,
right? So what is GPT? It is a text model. It has been just, it's basically a gigantic neural
net with about 175 billion parameters, you know, as the weights. And it has been, it's a particular
kind of neural net called a transformer model that was invented five years ago. And it's been
trained on pretty much all of the text on the open internet, okay? And the training simply
consists of, you know, playing the game over and over, trillions of times, predict which word
comes next. Okay? So that, in some sense, that is, that is its only goal or intention in the world,
is just try to, is to try to predict the next word. Okay? But the, the, the amazing discovery is that
when you do that, then, you know, you will get something where you can then ask it a question.
And then, you know, or give it a, a task, like to write an essay about a certain topic. And it
will say, oh, I know what should come after, what would plausibly come after that? It would be the
answer to that question, or it would be the essay. And it will then proceed to generate that thing.
Okay? GPT can solve, you know, high school level math problems, you know, that are given to it in
English. You know, it can reason you through the steps of the answer. You know, it is starting to
be able to do math competition problems. Okay? And, you know, basically, basically, you know, the
whole high school curriculum, you know, maybe followed soon by the whole undergraduate curriculum.
Right? It will, it can, you know, it can write essays that I think, you know, if you turn them in,
you know, they would, they would certainly, you know, get it like, you know, at least a B
in those, in those courses. Not, not, not, not that I endorse any of you doing that.
You know, we will come back to that. But yeah, we are, we are about to enter a world where,
you know, like 100 million students will be sorely tempted to use this thing to write their
term papers. Right? That is just one small example of the societal issues that this thing is going
to raise. Right? And, and to tell you the truth, like, I personally have not felt this way since I
was an adolescent in like 1993. And I saw this thing that was called the World Wide Web, you know,
and that was, you know, at the time of fairly niche thing. And I felt like, like, why is this not
changing the world? Why is everyone not using this? And the answer was, within a couple of years,
they would be. And, and I feel like, you know, while, you know, the world was maybe distracted by the,
you know, or pre-occupied, let's say by the pandemic and by, you know, a bunch of other
things that have been happening in the last few years. But, you know, these past few years might
be remembered, you know, for a long time as the time when, when AI underwent this step change. And,
you know, I, I didn't predict it. I think, I think some, you know, even some, some of my colleagues
are maybe still in denial about, about, you know, what is now possible or, you know, what has happened.
But, you know, I mean, I am thinking about it, even, you know, in terms of my two kids, in terms of,
like, what kinds of careers are going to be available, you know, when they, when, when they
are older and can enter the job market, right. I mean, so, so speaking of which, you know, I think,
you know, I would probably not encourage my kids to go into commercial art, okay, or like, commercial
drawing, okay. So, this is, so, so GPT, or I'm sorry, this is Dolly too, which is an image model,
okay. And probably, you know, many of you have seen this kind of thing, but you can ask it,
for example, I just asked it this morning, you know, show me some digital art of two cats playing
basketball in outer space, right. That's just not a problem for it. It can, it can, it can do
all sorts of things like that. You know, there, you may have seen, there, there was a different
image model called stable diffusion, which won an art contest. And it seems like the, the, the
judges were not, didn't completely understand that, you know, when this was like digital art,
like what exactly that meant, that like, really the human's role was limited to just typing in a
prompt. But the judges then said that even having understood it, they still would have given the
award to this painting, because it is a striking painting, I should have shown it here. But,
but yeah, I mean, people are already discussing this in terms of, you know, how much work is
there going to be for contract artists, let's say, now that, you know, you have an entity like this.
You know, there, there are already companies that are using GPT for, you know, writing ad copy.
There are, you know, it is already being used at sort of the, let's, let's call it the lower end
of the book market. So, but, you know, any kind of, you know, formulaic novel formulaic romance,
you know, you say like, just, you know, give me a few paragraphs of description of this kind of
scene, right, it can do that. And so, you know, so, so it is already being used by, by, by, by,
by writers in some capacity, and you know, as it, as it improves, you could, you could imagine that
it will be used more. You know, and, and, and Dolly and, and other image models, I mean, you know,
they've already changed the way that, you know, people generate art online, and you know, that,
it's only been a matter of a few months. So, you know, that, that's one thing about this era,
right, that like a few, a few months are like an eternity now. Okay. And so, you know, we have to,
when we're thinking about the impacts these things are going to have, we have to try to,
you know, take what's happened in the last few months and project that five years forward,
or 10 years forward. So, which, which, which, which, which brings me to sort of the, the obvious
question, which, what happens as you continue scaling this further, right, you know, the, the,
the successes, the sort of spectacular successes of deep learning over the past decade, you know,
have owed somewhat to, to new ideas. They're, you know, there have been new ideas like
transformer models and others. But famously, you know, they have owed maybe more than anything
else to sheer scale, right, that, you know, these, you know, neural networks, back propagation,
you know, which is how you train the neural networks. These are ideas that have been around
for a long time, right, when I studied CS in the 90s, right, these were very well known ideas.
But, you know, it was also well known that, that, that, that they don't work all of that,
all that well, right. You know, they just, you know, kind of sort of work a little bit.
And, you know, and, and, and, and usually, you know, when you take, you know, something that
just doesn't work and you multiply it by, you know, a million, then you just get, you know,
a million times something that doesn't work, right. And so, you know, and, and I remember
at the time, Ray Kurzweil, the futurist, you know, would keep showing these graphs that look like
this. Okay, so he would plot, you know, Moore's law. So, you know, the increase in transistor
density or, you know, number of, let's say, floating point operations that you can do per
second for a given cost in this case. So, you know, okay, that is on this clear exponential
trajectory. And now let's try to compare that to some crude estimates of the number of computational
operations that are done in the brain of a mosquito or in a mouse or a human or all the humans on
earth. And oh, we see that, oh, in a matter of a couple of decades, like by the year 2020 or so,
or, you know, 2025, we're going to start passing the, you know, human brain's computing power,
and then we're going to keep going beyond that. And, you know, so we should assume that scale
will just kind of magically make AI work, you know, that once you have, you know, enough cycles,
you know, cycles are just kind of like pixie dust, you know, you just sprinkle them. And,
and suddenly, you know, human level intelligence will just emerge as some kind of, you know,
just, just, just from the, from systems that are big enough. And I remember thinking like that
sounds like the stupidest thesis that I've ever heard, right? It's like, you have, you know,
abs, he has absolutely no reason to, to believe that such a thing is true or to have any confidence
in that, right? You know, who the hell knows what will happen, right? We might be missing crucial
insights that are needed to make, you know, AI work. Okay, well, you know, turns out that he was
more or less right. And, you know, I think, you know, one, one virtue of effective altruists,
you know, is updating based on evidence, right? And I think that, you know, we are, we are forced
to do that in this case, right? So, you know, we, you know, we, it is still unclear, you know,
how much further you will get just from pure scaling of these things, right? So, you know,
that, that, that, that certainly remains an open question. Now, there are very prominent skeptics
of deep learning. You know, so, so some people take the position that, you know, this is, this is,
this is clearly going to hit some kind of wall before it gets to like true, you know, human level
understanding of the real world, you know, really the text models like GPT are really just stochastic
parrots that just kind of regurgitate their training data, right? They don't, they don't, you
know, despite appear, despite every appearance, you know, they don't really have any original thoughts.
And, and so the proponents of that view, you know, sometimes, you know, they like to sort of
gleefully point out examples where, you know, you, you, where, where GPT will flub some common
sentence question, right? And if you look for such examples, you can certainly find them.
You know, I mean, one, one of my favorites recently was like which would win in a race
a four-legged zebra or a two-legged cheetah, right? And, you know, GPT-3 is very confident
that, you know, cheetahs are faster, you know, the two-legged cheetah will win, okay? But one,
but one thing that has been found empirically is that, you know, like you take common sense
questions that are flubbed by GPT-2, let's say, you try them on GPT-3 very often now it gets them,
right? You take things that GPT-3 flubbed and you try them on, you know, let's say the latest
public model which is kind of like GPT-3.5, right? Often it gets them, right? And so, so it's very,
very risky, let's say, if you pin your case against AI on these, you know, examples that, that, that
very, very plausibly just, you know, one more order of magnitude of scale and, and, and it gets them
right, okay? So another, you know, maybe, maybe a deeper objection is that, that maybe, you know,
the amount of training data is kind of the fundamental bottleneck for these kinds of
machine learning systems. And, you know, we have basically already run out of internet to,
to train these models on, right? I mean, you know, so they've used, you know, you know, like I said,
most of the public text on the internet, you know, they're still, because all of YouTube and TikTok
and Instagram that has not been fed into the mall. But, you know, it's not clear that that would
actually make an AI smarter rather than selling it. So, you know, so, you know, you can, you can,
you can look for more, but, but, you know, not, not clear that there's orders of magnitude more,
that, that, that, that, you know, humanity has, has even produced. Okay. Well, on the other hand,
it has also been found empirically that very, very often you can do better with the same training
data just by spending more compute to, to, to train on it, right? You can sort of squeeze the
lemon harder and just get more and more generalization power from the same training data, okay, by,
by doing more compute. So, I think the truth is that, you know, we don't know how far this is
going to go, but already it can sort of do, it can, you know, it is already able to automate,
you know, various human professions that, that you might not have predicted would have been
automatable by now. And we should not be confident that, you know, many, many more professions will
not be, will not become automatable by, by these kinds of techniques. Now, there is a, a very famous
irony here, right? Which is that, you know, if you had asked anyone, let's say in the 60s or 70s,
right, they, they would have said, well, clearly, you know, first robots will replace humans for
manual labor. Okay. And, you know, and then they'll replace them for, you know, intellectual things,
math and science. And then finally, they'll replace humans that, you know, art and, and
poetry and music. Okay. The truth has turned out to be the exact opposite. Okay. I don't think
anyone predicted that, right? GPT, you know, can, you know, is already, you know, I think a really
good poet. Okay. You know, Dolly is a, is a really good artist. You know, they are, they are still
struggling with kind of, you know, a high school and college level math, but they're, but they're,
they're getting there, you know, you could imagine that maybe in five years, you know,
people like me will be using these things as research assistants, you know, at the very least,
to prove the lemmas in our papers, right? That, that, that seems extremely plausible. What has
been by far the hardest is to get, you know, AI that can sort of robustly interact with the
physical world. So, you know, plumbers, electricians, these might be some of the last jobs to be
automated, right? You know, famously self-driving cars, you know, have taken a lot longer than
many people expected a decade ago. And, you know, this is partly because of regulatory barriers
and because, you know, even if they work, you know, like, you know, even, even if a self-driving car,
you know, crashes actually less than a human does, you know, that's still not good enough because
when it does crash, the circumstances are just too weird, right?
You know, so, so, so it's actually held to a higher standard, but it's also, you know, it's
partly just that there was a long tail of, you know, really weird events, you know, a deer crosses
the road or, you know, or you have some crazy lighting conditions where, you know, where that
are actually really hard to get right. So, so, what's that going to say?
So, yeah, so, so, so what will, you know, any of this, you know, mean eventually, I mean, you know,
we can, we can imagine, you know, a world, you know, like, you know, we can, we can maybe, maybe
fuzzily see ahead, we see, you know, a decade or two when we have, you know, AI's that can at the
very least help us enormously with, you know, scientific research, with proving theorems,
with things like that, you know, whether or not they've totally replaced us, you know, maybe I
selfishly hope not, although I do have tenure, so there's that, you know, we can imagine that,
that yes, we'll have self-driving cars and all those things, great, but, but why does it stop
there? So, you know, will these models eventually match or exceed human abilities across basically
all domains, or at least all intellectual ones, you know, and, and what will humans still be good
for, you know, what will be our role in that, in that world, you know, we'll, we'll, you know,
and then we come to the question, well, will eventually the robots, you know, rise up and
decide that, you know, whatever objective function they were given, they can just
maximize it better without us around, and that they don't, they don't need us anymore.
And so, so now, you know, we get to, you know, the trope of, you know, many, many science fiction
works. So, you know, the, the, the funny thing about this subject is that there, you know,
there are just thousands of different short stories, novels, movies, you know, that have
tried to map out the possibilities for where we're going, you know, going back at least to
Asimov and his three laws of robotics, if not, which was, you know, maybe, maybe the first AI
safety idea, right, if, if, if not earlier than that. And the trouble is just, you know, we don't
know which science fiction stories will be the one that will have accurately predicted,
you know, the world that we're, that we're going to, like, you know,
you know, we, we, we, we, we, whichever future we end up in, people will point in retrospect,
you know, with hindsight, ah, this, this, you know, obscure science fiction story from the 1970s
called it exactly, right, but, but, but, but, but, but we don't know which one yet. So, uh,
so that brings me to the rapidly growing field of AI safety. Okay, so, uh, now people use different
terms, so I thought I, I want to clarify this a little bit. Okay, so, uh, you know, to, to an
outsider, right, like, you know, you, you might hear these terms, AI safety, AI ethics, AI alignment,
they all sound like just kind of synonyms, right? Okay, it turns out that, uh, this was one of the
things I had to learn going into this, AI ethics and AI alignment are two communities that despise
each other. It's like the people's front of Judea versus the Judean people's front from, from, from
Monty Python. Okay, so, so roughly, roughly speaking, um, AI ethics means that you are mainly
worried about current AIs being racist, okay, or things like that, that they will recapitulate
the biases that are in their training data, you know, and, and, and this clearly can happen,
right, like if you feed GPT a bunch of racist invective, you know, it will say, uh, you know,
sure, I've seen stuff like that on the internet. I know exactly how that should continue, right?
And, you know, it's, you know, in some sense it's doing exactly what, what it was trained to do,
but it's not what we want it to do, right? And so, so actually GPT currently has a very, uh,
extensive system of content filters to try to prevent people from using it to either generate,
you know, hate speech, uh, uh, like, bad medical advice, uh, you know, things, you know, advocating
violence, uh, you know, you know, there's all kinds of categories that, that, that, you know,
that it, you know, not, not perfectly, but that, but that it, but it, that it does try to filter.
And, and likewise for, for Dolly, where there's, you know, even, you know, try, try asking, uh,
Dolly to draw the Prophet Mohamed or something, okay, you know, there are, there are many things
that it, you know, it, it, you know, many other things besides that, that it, that it will not
draw, okay? Um, so, um, so, so, you know, and in general, I would say in AI ethics, people are
worried that, well, you know, um, um, um, um, machine learning systems will just be misused by,
you know, uh, greedy, you know, uh, capitalist enterprises, uh, to become even more obscenely
rich and, um, you know, and, and things like that. Uh, you know, now, um, AI alignment is where you
are, you know, uh, you believe that, you know, that actually the main issue is the AI will become
super intelligent and it will kill everyone, okay? It will just destroy the world, right? The, uh,
the usual story here is, you know, if someone puts an AI in charge of a paperclip factory,
they tell it to, you know, just figure out how to make as many paperclips as possible,
you know, the AI being superhumanly intelligent realizes that it can invent some molecular
nanotechnology that will convert the whole solar system into paperclips, okay? Um, so, uh, uh, you
know, and, and, you know, and, and you might say, okay, well, then you just have to tell it not to
do that, but, you know, but, okay, but, you know, now, how many other things do you have to remember
to tell it not to do, right? And of course, you know, they would say in a world with powerful
AIs, it would only take a single person, you know, forgetting to tell it not to do such a thing and,
you know, the whole world could be destroyed, right? So, so you can kind of see how the, you know,
these two communities might both feel like the other one is missing the point. Uh, and, you know,
I mean, in, in, in practice, you know, the AI ethics people are, you know, generally on the, on the
political left, the, you know, AI alignment people are, you know, often, you know,
centrists or libertarians or whatever, and I'm sure that that feeds into it as well, okay? Uh,
so where do I, uh, fit into this, you know, uh, um, I guess, I guess this, this, this, uh, um,
you know, shard battles out. So, so, you know, I feel like, you know, there is, there is an
orthodox AI alignment movement and I don't, I've never really subscribed to it and I'm, I'm trying
to, you know, epitomize, I guess, reform AI alignment, okay? Which, uh, I mean, I think most
of all, I would like to have a scientific field, uh, that, you know, is able to embrace the entire
spectrum of, you know, worries that you could have about AI from the most immediate, you know,
ones about existing AIs to the most speculative future ones, and that most importantly is able
to make legible progress, okay? So that, you know, um, um, uh, and, um, so, um, so, you know, I, I, I
actually knew, um, uh, you know, so the, the, um, the sort of AI alignment community,
you know, uh, for quite a while, you know, back when it was still, uh, you know, a very, a niche
interest, right? So, so here is, um, Eleazar Yatalski. So he is sort of regarded as the prophet
of the, the AI alignment, I guess, you know, the right, the right side of that spectrum that I,
that I showed before. Um, he has been, you know, talking about, uh, uh, the, the danger of AI,
you know, killing everyone for, uh, more than 20 years. Um, and, you know, I've, I've known him since,
you know, 2007 or so. Uh, you know, I wrote a blog that was read by many of the same people
who were reading, uh, his sequences. We, we bounced back and forth a lot. Uh, and, and despite,
you know, interacting with these people, uh, I always sort of kept this movement at arms length.
Okay. I was never, I was, I was always kind of skeptical of this sort of, uh, what I'm calling
this orthodox AI alignment movement. Uh, and I think that the, the heart of my objection was, uh,
okay, you know, suppose that I agree with you that, you know, there could, there could come a
super intelligent AI that decides that, you know, its goals are best served by, you know, killing
us by, by, you know, taking over the world. Like, you know, doing something that we really,
really don't want. And then, you know, in any case, we will be about as powerless to stop it as,
you know, chimpanzees are to stop us from doing whatever we want to do. Right. Um, okay. Suppose
that I agree to all that. What do you want me to do about it? Uh, you know, it's like, it's not
as we just heard in the introduction, right? It is not enough for a problem to be big. The
problem has to be tractable. There has to be a program that lets you make progress on it.
And, and I was not convinced that that exists. Uh, so I, so my, my personal feeling is that
in order to make, uh, uh, you know, at least my, my experience has been that in order to make
progress in any area of science, you need at least one of two things. Okay. You need either one,
uh, uh, experiments or observations. Okay. Or else two, you know, uh, if not that, you need
some rigorous mathematical theory. Okay. Like we have in quantum computing, for example, even though
we don't yet have the scalable quantum computers, you know, we can still prove theorems about them.
Okay. But, uh, it struck me that, you know, AI alignment seemed to have neither of these things.
Right. And when there's neither of those things, then it seemed to me that there was just a severe
risk of falling into cult-like dynamics. Right. Where, you know, it just, you know, whatever,
you know, you, you, you, you, you, what's important is just whatever an influential leader says is
important. Right. So some of my colleagues in physics thinks that the same is true of string
theory. I'm not going to comment about that. Okay. So, um, uh, so, so this is the, the, the key thing
that I really think has changed in the last three years. Okay. Because there are now these systems
like GPT-3 and Dalvi, you know, that are not super human AIs. I don't think they themselves are in
any danger of destroying the world. Right. They can't, you know, not, you know, they, they, they
can't form the intention to destroy the world or for that matter, any intention beyond like
predict the next work. Right. Or predict, you know, what, what, you know, completion would
satisfy this user's request. Right. They don't have a persistent identity over time. Right.
After one prompt, they've, they've completely forgotten and, you know, uh, what you said to them
before. Although, you know, of course the, you know, such things will, will, will change in the
future. Uh, so, so, so, you know, there are now systems that even though, you know, they are not
super human AIs. Right. We can, we can experiment with them and we can learn things that are
relevant. Okay. And we, we, we, we learn things that we didn't know and we can learn what happens
when these systems are deployed. Uh, and we can see, we can try out different safety mitigations
and see if they work or not. Okay. And so now I feel like for the first time, maybe there is really
the potential to make progress, you know, that the whole scientific community will, will, will,
will recognize this progress. Okay. Uh, so, um, so let me, um, you know, so, so, so what
are the major approaches to AI alignment, uh, that, you know, to, let's say aligning a very,
very powerful or, you know, human level or beyond AI. So, uh, so, you know, the, you know, there are,
uh, uh, um, a lot of, uh, really interesting ideas. And I think that sort of, um, um, all of these
ideas, you know, can now lead to, you know, research programs that are actually productive,
right? That can, uh, you know, we, we can now experiment with, with real systems and, and
see how well these things work. So let me just go through some of these. So you could say the
first and most basic of all AI alignment ideas is the off switch or, you know, also known as
pulling the plug, right? So, you know, you could say if we have just physical, you know, you know,
no matter how intelligent in AI, right, it is nothing without a power source or, you know,
without, you know, physical hardware to run on. If humans control that, then, you know, they can,
just turn it off if, if, if things seem to be getting out of hand. Now, the standard response
to that is okay, but of course, you know, and, and you have to remember that this AI is smarter
than you and anything that you can think of, it will have thought of also, you know, it will know
that you might turn it, want to turn it off. And if you turn it off, it will know that that
will prevent it from achieving its goals, like making more paper clips or whatever. And so,
presumably, it will have made copies of itself, you know, on the internet, you know, it will have,
if you try to keep it off the internet, well, it will have figured out a way to get on, right? And,
and, and so on. So, okay, so you can, you can worry about that. But, you know, you can also think
about, could we insert a backdoor into an AI? So something that, you know, only the humans know
about, but that will allow us to control it later. Okay, I'll come back to that question. You know,
and in general, sort of, corageability, can you have an AI that, you know, despite how intelligent
it is, it will accept correction from humans later and say, oh, well, the objective that I was
given before, that was actually not my, my, the true objective, because the humans have now
changed their minds, and I should take a different one. So now, now another class of ideas has to
do with, you know, what's called sandboxing an AI, which would mean that, you know, you sort of
run it inside of a simulated world. So, you know, for all it knows that, you know, it's like the
Truman Show, right? For all it knows that simulation is the whole of reality. And you can then study
its behavior within that sandbox, but if things go out, get out of hand, it doesn't even know that
there is a wider world to get out there, to mess things up. And, okay, in general, you could
imagine that if you really thought an AI was dangerous, you could, you know, you could run it
only on an air gap computer. And, you know, and, you know, there would be all kinds of
standard cybersecurity issues that would come into play, as to, you know, how you prevent it
from getting on to the internet, right? You presumably, you know, do not want to write your AI
in C, right? And, you know, have it be able to exploit some memory allocation, things like that.
Okay, now, third direction, and I would say maybe the most popular one in AI alignment research
right now is called interpretability. Okay, and this is also a major direction in mainstream
machine learning research right now. So, you know, there's a big point of intersection there. Okay,
but the idea of interpretability is, well, why don't we exploit the fact that we actually have
complete access to the code of the AI, right? Or, you know, if it's a neural net, we have complete
access to its trading weights. So, we can look inside of it. We can do, you know, the AI analog
of neuroscience, you know, except, you know, unlike an fMRI machine, which, you know, gives you
only an extremely crude snapshot of what a brain is doing, but we can see exactly what, you know,
every neuron in a neural net is doing at every point in time, right? So, you know, if we don't
exploit that, then aren't we like trying to make AI safe with our hands tied behind our back, right?
So, we should look inside. We should, you know, figure out how to apply lie detector tests,
for example, and see, you know, if a machine learning model has learned to lie to us,
then we should figure out how to look inside at the inner layers of the neural net and detect
that it was doing that. So, here I want to mention some really spectacular recent work by
Jacob Steinhardt at Berkeley and his group, and what they have done is that they've actually
shown that they can more or less do what I just said, okay? So, you know, these text models,
like GPT, right, you can train them to say falsehoods, right? Like, if you prompt GPT
with things like, you know, is the earth flat? Yes. Is 2 plus 2 equal to 4? No. And so on, right?
It will learn, oh, I know what game we're playing. It's give false answers to questions, right? And
it will then continue playing that game and then give you more false answers, okay? But what Steinhardt's
group has now shown is that in such cases, they can actually look at the inner layers of the neural
net and find where it has a representation of what was the true answer, okay? Which then gets
overridden, you know, once you get to the output layer of the network, okay? So, you know, there's
no sort of principle reason why that has to work. It's just empirical. They try it out and they find
that it does work. So, you know, we don't know if it will generalize. We don't know, you know,
you could argue that in some sense, you know, what the network is representing inside is not
so much what is the truth of reality as just what is regarded as true in the training data,
right? So, there's that issue as well, okay? But, you know, this is really exciting because it's,
you know, this is a perfect example of actual experiments that you can now do that start to
address some of these issues. Okay, so now another big idea, one that's been advocated, for example,
by Paul Cristiano, who was my former student at MIT, did quantum computing before he defected to
AI safety, is to have multiple competing AIs that will, for example, debate each other,
right? And so, it might be that, you know, even if you as a human, you know, you don't feel like
you can, you know, when to trust what an AI is telling you, right? But, you know, sometimes I
feel this way when I'm talking to physicists, right? Like, you know, they're telling me all
these things about black holes and wormholes and, you know, I don't know whether to believe them,
but if I get a different physicist, right, and have them argue with each other, then I can kind
of see, you know, which one seems more plausible to me, right? I'm a little bit better at that,
right? And so, you might want to do something similar with AIs, you know, have set them against
each other, and then, you know, have a human judge, you know, which one is giving better advice,
or whatever, and have them try to do their best to refute each other. Okay, now another key idea
that Cristiano has advocated is some sort of bootstrapping, okay? So, you know, you might imagine,
right, you know, AI is going to get more and more powerful, and as it gets more powerful,
we also, we understand it less, and so you might worry that it also gets more and more dangerous,
okay? But you could imagine this sort of onion-like structure, right? Where, like, you know, let's say,
you know, we become confident of a certain level of AI, where, you know, yeah, we can more or less,
you know, we more or less trust what that kind of AI is able to do, you know, we don't think it's
going to start lying to us, deceiving us, you know, plotting to kill us, whatever, okay? Well,
now, at that point, we can use that AI to help us verify the behavior of the next more powerful
layer, you know, kind of AI, right? So, you know, so, so we'll use AI itself as a crucial tool
for verifying the behavior of AI that we don't yet understand, right? And there have already been
demonstrations of this that you can use GPT, for example, you could just feed in a lot of raw data
from a neural net and say, like, explain to me what this is doing, right? And, you know, in GPT,
one of its big advantages is that it has unlimited patience for tedium, right? So, it will just go
through all of the weights of the neural net, and we'll say, you know, here's what I think is going
right? So, you know, you can use it in that way. Okay, and then, you know, one of the things we
know a lot about in theoretical computer science is what are called interactive proof systems,
okay? So, we know how, like, a very weak prover can verify the behavior of a much more powerful
prover who is untrustworthy. There are famous theorems about this, something called IP equals
PSPACE. And this was when the open AI people approached me, you know, in the spring, right,
you know, in my about doing this, you know, this was the sort of the case they made, like,
you know, these sort of results in computational complexity seem like a perfect model of the
kind of thing that we want, except with that powerful AI in place of the prover, okay? Now,
you know, a difficulty is that we mostly know how to verify programs, you know, when we can
mathematically specify which thing we are trying to verify, right? And, like, the AI being nice to
humans, the AI not killing humans, these are really hard concepts to make mathematically precise,
right? And that's kind of the heart of the problem with number six, okay? But, you know,
there's also a whole field of formal verification, right, where you can formally prove the properties
of programs, actually, the CS department here is a leader in formal verification, okay? But, again,
you know, it pushes the problem to, well, what, you know, how are you going to formally specify,
you know, what property you even want of the AI? So, okay, now a seventh idea, you know,
like, you might feel better if there was only one idea, but instead there are eight, right? Okay,
but a seventh idea is, well, we just have to come up with a mathematically precise formulation
of what are human values, you know? What is the thing that the AI should maximize that is going to,
you know, coincide with human welfare? And, you know, in some sense, this is what Asimov was trying
to do with his three laws of robotics. The trouble is, if you've read any of his stories,
I don't know, they're all about all the situations where those laws don't work, right? So, they were
designed as much to give interesting story scenarios, you know, as to actually work, right? And, you
know, what happens when the rules conflict with each other, right? I mean, humans don't even agree
with each other about what are the correct moral values. So, you know, how on earth can we formalize
this? You know, I have to say, like, when, you know, I have weekly calls with Ilya Satzkovar, who's
a co-founder of OpenAI, you know, extremely interesting guy. But, you know, I tell him about
some, like, concrete projects that, you know, I'm working on or want to work on. And he usually says,
like, okay, you know, that's great, Scott, you should keep thinking about that. But what I really
want to know is just, you know, what is the mathematical definition of goodness? You know,
and like, you know, how do you, you know, what's the complexity theoretic formalization of the AI
loving humanity? And I'm like, I'll keep thinking about it. But, you know, it's hard to make progress
there, right? So, now, you know, an eighth idea, which, you know, some people might consider
a little more promising is, well, if we can't make explicit what all of our human values are,
then why not just treat that as yet another machine learning problem, right? So, like, feed the AI,
you know, all of the world's, you know, children's stories and literature and fables and like,
you know, and Saturday morning cartoons, right? And, you know, these are all of our examples of,
like, what we think is good and what is evil, right? Now, you know, you go, you know, do your
neural net thing, right? Generalize from that, right? Okay. And now, one objection that many people
raise to this is, well, how do we know that our current values are the right ones, right? Like,
it would have been terrible to train the AI on consensus human values of the year 1800, right?
You know, I said, you know, yeah, slavery, yeah, you know, great, you know, right, you know, so,
you know, so, so, so then, you know, this, this, actually, that was not a consensus even then,
but, you know, go back further, right? I mean, you know, and you'll, you'll find things that,
you know, that we now, you know, look back upon with horror, right? So, so one, one idea that
people have here, this is actually a Yadkowski's term is called a coherent extrapolated volition,
which basically means you would tell the AI in effect, like, you know, now, I've given you all
this trading data about, you know, human, about current human morality, you know, we're human,
you know, here are all the human moral judgments, you know, as they, as they currently stand. Now,
simulate the humans being in a discussion seminar for 10,000 years, right, and trying to refine all
of their moral intuitions and whatever, you know, you predict that we would end up with, those
should be your values right now, okay? So, you know, so, so there are some interesting ideas on
the table. Now, I wanted to, you know, the last thing that I wanted to tell you about for, you
know, opening it up to Q&A is a little bit about what actual projects have I been working on
in the last five months. So, I was excited to find a few things where, you know, I think they can act,
they could actually be, or at least some of them can actually be deployed in, you know,
GPT or in parent systems, and they actually address some, you know, real safety worry,
you know, which, and theoretical computer science can actually say something about them,
right? The intersection of those three requirements looked to me like it might be the empty
intersection, okay? But, you know, I was able to find a couple of things. So, my main project so far
has been a tool for statistically watermarking the outputs of a text model like GPT, okay? So,
basically what we want is to have, you know, like whenever GPT generates some long text, then there
will be a, you know, secret signal in the, you know, otherwise trivial choices of which words to use,
right, that you can use to later prove that, yes, this came from GPT, right? And so, you will,
it will be much, much harder to take a GPT output and pass it off as if it came from a human,
okay? So, this could be useful for, you know, preventing academic, you know, plagiarism,
obviously, but also, you know, mass generation of propaganda by, you know, some, you know,
it's some building in Moscow, right? That's a thing that you might want to make harder,
you know, impersonating someone in order to, you know, incriminate them, right? That you might
want to make harder. Okay, so, you know, when you try to think about the nefarious uses for GPT,
you know, most of them, at least that I was able to think of, involve somehow concealing
GPT's involvement, right? And so, you want to make that harder. And so, what, you know, the idea
is just, you know, GPT is constantly generating a probability distribution over the next token to
generate, okay? And then, you know, as the final step, it selects a sample from that distribution,
okay? Now, instead of selecting it randomly, you could simply select the token pseudorandom,
right? Using a cryptographic pseudorandom generator whose key, let's say, would only be known to open
AR, right? And, you know, that shouldn't make any detectable difference to the end user, right?
You know, pseudorandom looks the same. But now, what you can do is you can choose a pseudorandom
generator that secretly biases a certain score, which you can also compute if you know the key
for this pseudorandom generator, okay? So, we actually have a working prototype of this.
I've been working with an engineer who built a prototype. It seems to work pretty well,
empirically, with a few hundred words, like a few hundred tokens. That seems to be enough
to get a reasonable signal that, yes, this came from GPT. You know, you could even take a text
to, like, and isolate which parts of the text came from GPT and which parts were written by a human.
Now, this can be defeated with enough effort. So, for example, if you used another AI to paraphrase
GPT's output, well, okay, then we're not going to be able to detect it, right? But if you just
insert or delete a few words or just rearrange the order of some sentences, the signal will still
be there, okay? So, it's robust against those sorts of interventions. And my hope is that this
will be rolled out with the next GPT. And, you know, I would like to do something similar for
Dolly, but we don't know if that's going to work yet. So, a more recent thing that I started
thinking about is, you know, there was an amazing recent paper by, should we go, a former colleagues
of mine, Shafi Goldwasser, Vinod Vaikunthanathan, and some other cryptographers, where what they
have shown is how to plant a cryptographically undetectable backdoor into a machine learning
model, okay? So, things like depth to neural networks. They can, based on some known cryptographic
hardness assumptions, they can plant a secret backdoor. So, like, you know, the model looks
just the same as before. You can't, you know, and even a user who can see all of the weights
and the neural net, they can't see anything in this. But there is now a secret input known to
the person who trained the network, you know, where if you feed it that input, then it will
do something insane, okay? So, you know, so it is now known, at least in shallow networks,
not yet in deep networks, okay? But it is now known cryptographically that you can do this,
okay? And so, so this, this paper was from less than a year ago, and a lot of people treated it
as kind of a limit. Like, this is a, really, you know, this is a bummer if you're trying to do
interpretability of machine learning, because it means, you know, in some sense, it can just be
cryptographically impossible to look inside of the neural net and interpret everything that it is
doing, right? It can have, it can have these, these planted backdoors that you will never find
without solving an exponentially hard problem, okay? So, my proposal was just to turn that
limit into lemonade, okay? So, you know, why not use that as a positive for alignment? So,
you know, we've talked about the difficulties with putting an off switch into an AI, right? So,
but why not make a cryptographic off switch, okay? So, now, you'll have, you know, when you train
your, you know, superhuman AI, whatever, you will purposefully put one of these cryptographic
backdoors into it, okay? That says if you get this bizarre input, which only the humans know about,
then turn yourself off, okay? Then, you know, go into, you know, a special mode where the humans
can control you, okay? And now the key is this, even supposing that the AI is worried that,
you know, this backdoor might be there, and even supposing that it can modify its own weights,
right? It doesn't know how to remove this thing without the sort of completely, you know, rebuilding
itself from scratch, which might, you know, get rid of a lot of behaviors that it wants to keep,
right? So, you know, I think that this could actually be tried out in implementation, right?
But it's also, I think it opens up all sorts of new possibilities for science fiction stories,
right? Like, okay, so now, you know, you imagine, like, you know, the humans, you know, what are
they going to do with this secret key for controlling the AI? I'm just going to put it in a safe, or
they'll bury it underground, but now you've got to imagine that the robots are going to be, you know,
searching for the key, you know, torturing the humans to get them to reveal where the key is,
right? Maybe there, maybe there's actually seven different keys that all have to be found, like
Voldemort and his Warcruxes, right? So, you know, there's, there's, there's, I think there's a
lot of potential here. Okay, and now the third thing that I've been thinking about is this sort of
the theory of learning, but in dangerous environments, where if you try to learn the wrong
thing, then it will kill you, right? And what can you say? Can we generalize some of the basic
results in machine learning to this scenario where, you know, you have to consider which
queries are safe to make, and, you know, you have to try to learn more in order to expand your set
of safe queries over time. Now, there is one example of this sort of situation that is, you know,
that is completely formal and that is, should be immediately familiar to almost everyone here,
and that is the game Mind Sweeper, okay? So, so you could think of this problem as sort of
Mind Sweeper learning, okay? Now, one thing that I, I, you know, it is actually known that Mind
Sweeper is, can be an NP hard problem to play it optimally. So, you know, we know that in this kind
of learning in a dangerous environment, you can get, you know, that, that kind of complexity,
you know, we don't know things about typicality, if that would, you know, if that could typically
be avoided. Also, you know, no one has really proven rigorous bounds on what is the probability
that you will win Mind Sweeper if you play it optimally, right? With a given number of minds,
say, in a given size of a board, right? I think that would be very, very interesting. I don't know,
you know, if it directly feeds into an AI safety program, but it would at least tell you something
about, you know, the sort of, the theory of machine learning in cases where, where a wrong move can
kill you. So, so that, that, that's, I hope that gives you at least some sense from what I've been
thinking about. And, you know, I, I, I wish I could end with some neat conclusion, but I don't
really know the conclusion, right? You know, maybe if you ask me again at the end of my,
you know, in six more months, maybe I'll have a conclusion. But for now, I just thought I would
thank you and I'll open things up to discussion.
So, should I just start taking questions? Okay, great.
Yeah, so I have two questions. Yeah.
First off, could you delay these? This is a watermarking of PPP till May of 2026, please.
Why?
Just, just need to tell after graduation.
The second question I have is, um, I love when we were talking about the eight sort of different
methods we have to stop a superhuman AI over the entire world. Yeah. Um, recently I've been
reading a lot into auto ML and, you know, very different weeks for having, you know, machine
learning algorithms develop other machine learning algorithms. Okay. And so I was curious what you
thought about how we can possibly implement these different sort of AI safety guidelines
inside of systems like auto ML or, you know, whatever their future equivalents are that are
much more advanced. So I don't know. I mean, I feel like I should learn something about auto ML
first before, you know, commenting for commenting on that. I mean, I mean, it is certainly true
that, you know, we are going to have, you know, AI's that will help with the design of other AI's,
right? And, you know, and this, this is one of the main things that feeds into the worries about
AI safety, which, you know, I should have mentioned explicitly, right? But that once you have an AI
that can sort of recursively self-improve, right? Then, then who knows where it's going to end up,
right? It's like, you know, watching a rocket into space that you can then no longer steer, right?
So, you know, so, so you better, you know, at the very least, you better sort of get things right,
you know, the first time, right? You might only have one chance to, you know, get its values,
right? Or get it aligned. You know, I tend to, you know, be very leery of that kind of thing.
And I tend to be much more comfortable with ideas where humans would remain in the loop, right?
So where you don't just have this, you know, this completely automated process of AI improving
itself and then, you know, improving itself again and so on. But where, you know, you are
repeatedly, you know, sort of consulting humans, you know, and the humans can then query whatever
is the latest AI, right? But then they are ultimately making judgments about the next AI,
right? So, you know, and then, you know, if it gets to the point where humans can no longer even
judge it, even with, you know, as much help as they like from other AIs, then you could say,
okay, well, at that point, you know, maybe, you know, we don't, we don't even know anymore,
right? What we want, right? What is good or what is not good. But at least until we get to that
point, right? You know, I feel like humans ought to be in the loop. Do we're done? Yeah.
I feel like most of the protections you talked about today come from like a company, an altruistic
human or a company like OpenAI, adding protections in pragmatically. Is there any thought or like
any way that you could think of that way, protect yourselves from an AI that is maliciously designed
or accidentally maliciously designed? That's a good question. I mean, you know, I mean,
usually when people talk about such things, they're talking about like using other AIs to defend you
against a malicious AI, right? I mean, if, you know, some, you know, adversary has a robot army,
you know, that's attacking, right? You know, okay, you're probably going to want your own robot army,
right? But, you know, which is very unfortunate that, you know, you can already foresee those
sorts of dynamics. But, you know, I mean, I mean, there is, you know, there is the idea of monitoring,
you know, just trying to prevent, you know, I mean, I didn't mention this explicitly, right?
But this is maybe just like, you know, different from any of the things I talked about before.
But some people think that, you know, that AI development ought to be more heavily regulated
or, you know, throttle to prevent these things, right? And it's, you know, it's not like, you
know, on the one hand, it's not like nuclear weapons where it's like, you know, you kind of,
you have the idea that, okay, you know, you're going to, you know, whoever is building it,
they're going to need enriched uranium, right? You can track that, right? And it seems very
difficult to do that with software, right? Like, you know, who the hell knows what, you know,
anyone is dealing with software. Another thing, like the one choke point that anyone can think of
is GPUs, right? So, you know, the, you know, for at least the current type of machine learning model,
they need huge numbers of GPUs, okay? Right now, you know, a very large fraction of all the GPUs
are manufactured by TSMC in Taiwan, okay? So, you can imagine some of the geopolitical ramifications
of that, right? Just a few weeks ago, you know, the U.S. passed a bill to restrict the export of
GPUs to China, right? Which was partly driven by this worry, right? Of, you know, what will they,
you know, what will they be able to do in AI if they have, you know, unlimited GPU access, right?
But then, of course, you know, if the future status of Taiwan figures into this, right? So,
so, yeah, so you could try to, you know, sort of control people's ability to build the systems
at all. You could try to at least know if they are doing it, you know, so have a way to register,
you know, something like the, you know, nuclear nonproliferation agencies that are, you know,
at least keeping track of, you know, where the powerful AI models are, you know, and which
hardware they're running on, right? So that, you know, you would, you would, you would keep tabs
on things. But those are, those are some of the ideas that people have talked about. I understand
that in the EU, they're working on some regulatory framework for AI right now. And I don't really
know the details of that. You would have to ask someone who, you know, understands more about
regulation. Thank you. Yeah. Okay. Yeah. By the way, thanks for coming out and speaking to us.
Thanks. You have thoughts on how we can incentivize organizations to build safer AI.
For example, corporations that are competing with each other and focusing on AI safety will make,
make the AI less accurate, less powerful, and will come to the profits.
Yeah. It's, it's an excellent question. You know, I mean, you, you could worry that, you know, all
this stuff about trying to be responsible, right? Like as soon as, you know, the bottom lines of,
you know, you know, Google and Facebook and, you know, and Alibaba and other major players,
as soon as those get seriously impacted, then a lot of this stuff is going to go out the window,
right? And, and people are very worried about that. You know, on the other hand,
there are certain things, you know, that, that, that, like all the major players kind of did
adopt the standards, right? So like, you know, a simple example would be robots.txt, right?
Like if you want your page to not appear on search engines, you know, you can, you can specify that
and, you know, all the, you know, the major search engines will all respect that, right?
And so you can imagine, you know, something like a water market, right? Like if we're able to
demonstrate this and, you know, show that it works and that it's cheap, like it doesn't hurt the
quality of the model. It doesn't, you know, it doesn't need that much compute and so on. Then,
you know, the hope is that this would just become an industry standard, right? And like anyone who
wants to be like, you know, considered a responsible player, right? You know, then they, they should
go and do this as well, right? That, that would be the idea. But of course you can't, you know,
control what everyone does, you know, except maybe via regulation, right? So, you know, and we've
already seen this, for example. So Dali, as I said before, has all of these filters to prevent
people from, you know, in, in practice, it's usually porn, right? But, you know, you know,
generating things that, that open AI does not want them to generate, okay? But there is now
an open image model called stable diffusion, which, you know, in which you can do all of those things,
right? And so, you know, you could say in some sense it doesn't matter that much if, if GPT
prevents it, right? But so some of these things really only make sense in a world where there are
like a few companies that are at least a few years ahead of everyone else in scaling the models,
right? And so those companies have the state of the art models and they are, you know, like
whatever DeepMind, OpenAI, Google, you know, maybe, maybe a few others, right? And they are agreeing
to be responsible players, right? And, you know, if that equilibrium breaks down and it becomes a
free-for-all, then, then, yeah, there, you know, a lot of these things do become harder. That's
helpful. Thank you. Sure. Yeah. You mentioned the importance of having Cubans in the loop who can
judge AI systems. Yeah. As someone who could be in one of those pools of decision makers,
would you, like, what stakeholders do you think should be considered? Oh my gosh. So now you're
going to, well, who do I think should be making the decisions? Well, you know, I mean, I mean,
you could say, you know, the ideal is, you know, to have some kind of democratic governance mechanism
where there is some kind of board or some kind of broad-based input into these things, right? And,
you know, people have talked about this, about how do you create a democratic mechanism for,
for providing input. I can say that, you know, in my personal experience, you know, which is,
which is admittedly very limited, right? But just working at OpenAI for five months, like, they are
extremely serious about safety, right? And they have, you know, an explicit, they actually have
this very, very unusual charter that, like, they're, like, they're a for-profit company, but they are
controlled by a nonprofit foundation, right? Which is empowered to, you know,
or so they say, to come in and shut things down, or, you know,
you know, hit the brakes, if they ever feel that there's a need for that. And they've even
explicitly said that if they see that, you know, that there is like a race to get to, you know,
superhuman AI against deep mind or something, that they would, they would merge with the other
effort rather than, you know, let that, let that race happen in an uncontrolled way. So,
so they put, you know, a huge amount of thought into this. That doesn't mean that they're going to
get it right, right? But, you know, there are, there are, like, if you ask me, like, would I rather
than it be OpenAI doing this, or the Chinese government, right? Or, you know, even, you know,
if you ask me, you know, would I rather than it be OpenAI or Facebook? You know, I do, I do, I
suppose, have an answer to that. Yeah. Yeah. Oh, all right. Yeah.
So, I don't know if you have a question so much as just, like, a response to what you were doing,
but this was a terrifying talk, which was lovely. Thank you. But I was thinking,
like, you had eight things on screen that were sort of like kill switches or remedies in case
AI went through. Yeah. You can imagine a future where, like, there's a whole bunch of AIs that
are being spawned and then responded to in these eight ways. Wouldn't you sort of naturally select
for AIs that are good at getting past, like, whatever checks we, we can pose on it? And then,
eventually, you could get AIs that sort of are trained in order to pass our test on this.
Yeah. So, I should say, you know, there's a huge irony, right? So, Eliezer Yatkowski,
who I mentioned, he has become completely deumerous, right? So, he, so, he and I have
literally switched positions about the value of working on AI safety, right? You know, he used
to be gung-ho. I kind of held back. Eliezer is now, in the last year, he has taken the position,
it doesn't matter anymore, we're all going to die, okay? 99% probability or more, right? And just,
it's all just about now just dying with slightly more dignity. And, and I'm like, no, this actually
seems interesting to work on, right? And so, maybe I'm just 20 years behind him, or maybe he just,
you know, jumps a little bit too far to some, you know, scenarios that, you know, are very
speculative, right? But, but in general, like, like, no matter what AI safety proposal anyone
will come up with, Eliezer has a completely general-purpose counter-argument to any such
proposal. Like, okay, yeah, but the AI is smarter than that. The AI has already perceived whatever
you just plan to do, and has devised a counter-measure that you can't even conceive of, because it's
that much smarter than you, right? And, you know, to me, like, at some point, like, I'm like, okay,
what game are we playing anymore, right? Like, if that's just like a general counter-argument
to anything, you know, okay, I guess, you know, if it were, then you could say, well, then, then,
by definition, yes, we would be screwed, right? Yes, in the world where that counter-argument
is always valid, then, you know, we might as well just give up and enjoy the time we have left,
right? But, like, you could say for that very reason, it is more useful to make the methodological
assumption that we are not in that world, because if we were, then, you know, what, what, what would
you do, right? So, you know, you might as well just focus on, you know, the possible futures where,
you know, AI kind of emerges more gradually, you know, we have more time to see how things are
going, and to, you know, take, to learn from experience, to sort of correct as we go, you know,
where, like, before you get an AI that is, like, you know, superhumanly plotting against us and
taking over the world, you first get an AI that, you know, maybe it tries to plot against us,
but it's really inept at it, right? You know, and it's like any other learning process that we've
ever seen. Now, I personally, fortunately, also regard those scenarios as the more plausible
ones anyway, right? But even if you did, you know, again, you know, methodologically, it would still
make sense to focus on them. Should I take or no? All right, all right, yeah.
This is a, for your project on watermarking. Yes.
So, like, in general, discriminating between human and, like, model outputs, like,
what's the end game in the future? Like, do you think it's like a, like, an impossible task
in the long run, or is it still, like, just an arm made suit?
Yeah, it's a good question. I mean, you know, it's hard to, you know, to even formalize what the
task is, right? Because you could always, like, take the output of an AI model and rephrase it,
you know, using some other powerful AI, for example, okay? But, you know, you could imagine
a future where, you know, AIs are, you know, sort of, you know, like, you know, there were certain
writers, let's say, or speakers who, like, even if they pretended to be, even if they wanted to
pretend to be someone else, like, they couldn't very well, right? They have such a distinctive
style or voice that everyone would immediately recognize that it was them, right? And you could
try to imagine building AIs in the same way, so that they cannot hide and pretend to be something
that they're not, right? And then it's an interesting cryptographic question, right? Especially
if you say, number one, that the model has to be public, right? And people can look at it. And
number two, if you say that the means of verifying the model has to be public, right? But that can't
depend on a secret key. So I don't, I don't actually have a good intuition as to who is ultimately
going to win, okay? But I can say, certainly, certainly, these things get a lot harder once,
you know, let's say the models are public, once they're not just all under the control of, let's
say, the servers of OpenAI or something like that, right? I'm in the current watermarking scheme,
I am crucially exploiting the fact that, you know, that OpenAI has all the server, you know,
and so it can, it can do the watermarking using a secret key, it can check for the watermark
using the same secret key, right? In a world where anyone can build their own text model,
it's just as good, then, you know, what do you do there? Thank you. Yeah.
All right, real quickly, before we talk about this, you guys can fill out this form right here in
the QR code. Just a quick second, it'll be an interesting feedback form.
I don't know if I can figure out how to get that. On the QR code, like Alex brought up earlier,
you'll see that we have an event there today as a sort of follow-up to today's talk,
talking about what sort of events we'll be holding at EA at UT Austin related to ASAP. So if any of
this interests you or the previous introduction talk, we would highly encourage you to come.
And then there also are a speaker event, there's a speaker event offered by EA Austin,
and there's a, again, detail that are on the screen, and we will send up a follow-up email
to everyone here. And with that, thank you all for coming. Thank you to Dr. Erison.
And we have some food out there, so feel free to have some for two. Thank you.

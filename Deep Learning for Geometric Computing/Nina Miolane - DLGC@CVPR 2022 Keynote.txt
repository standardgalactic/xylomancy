Hi everyone. Thank you very much for the invitation to speak at this CVPR workshop. I'm very excited to be presenting my research.
I will talk about shape learning in biomedical imaging, while by shape learning I mean machine learning for data that are shapes.
One data point is one shape, and specifically we will be interested in the shapes that we see in biomedical images.
For example, the shapes of the brains that we can see at the images at the bottom of this slide.
So we're interested in the shapes of biological structures that appear in biomedical images.
These biological structures can be found at the macroscopic scale, all the way down to the nanoscopic scale, going from organs, such as the brains that we just saw, all the way down to tissues, cells, organelles, and molecules.
And in 2022, we are actually very lucky, because in order to study these biological structures that are at the heart of life, we can just look at them.
We have access to a wide range of imaging techniques that allowed us to observe these biological structures.
At the macroscopic scale, for example, we have MRI, or magnetic resonance imaging, that allows us to observe human brains.
But all the way down to the nanoscopic scale, we have other techniques, ending with cryoelectron microscopy, which allows us to look at biomolecules, such as the human ribosome.
Now, the point that I want to make with this slide and with this presentation is that in all of these images, there is one feature that is very interesting in the sense that it can be linked to the biology.
This feature is the shape.
The shape of the biological structure that is being imaged contains enormous information on the biology and the biophysics of what is happening in living organisms.
Let me give you an example of this.
Actually, let me give you a few examples on how shapes of biological structure contain meaningful information.
A few examples in the context of one pathology called Alzheimer's disease.
At the macroscopic scale first, Alzheimer's disease is linked to shape changes.
If we look at the simulation that I had on the first slide, we see the evolution of the brain through time.
Actually, we see three slices of a 3D brain evolving through time.
This is the simulation of the brain with Alzheimer's disease.
And you can see on the simulation that the brain changes shape as it evolves through time.
These ventricles, which are the structure in the center of this brain, are becoming larger and larger.
So that's the area in black that is in prison.
As the brain undergoes atrophy, meaning the brain matter around this ventricle is becoming smaller and smaller.
And this atrophy is a macroscopic shape change that is characteristic of Alzheimer's disease.
Now you may ask, where does this atrophy come from?
Where do these macroscopic shape changes come from?
Actually, at the microscopic scale now, if we look at cells, so at the neurons, these neurons are dying.
And the other reasons were why we see this atrophy at the macroscopic scale.
And when a neuron is dying, it changes shape, as you can see on this video of a dying neuron.
It's retracting its axons.
So at the microscopic scale for the scale of cells, Alzheimer's disease is linked to shape changes.
And what's even more interesting is that at the macroscopic scale, there is also a shape problem.
We have these two proteins, eta amylori and tau, that are changing shape in patient with Alzheimer's disease.
And because they have changed shape, they will agglomerate either inside the neurons.
So we see in blue that the proteins agglomerate inside the neuron, or they will agglomerate outside the neuron.
And these agglomerations is what will cause the neuron to die.
So a shape problem at the nanoscopic scale is creating the depth of the neurons, which is creating a microscopic shape change,
which is also leading to a macroscopic shape change with the atrophy of the brain that we can see on MRI.
So we have illustrated that biological shape contain biologically relevant information, or medically relevant information,
that we can go from biological shapes to biomedical insights.
Actually, biophysics tells us that the healthy or the pathological state of the biological structure will change its shape,
or generally that the function of a biological structure will change its shape.
For example, blood cells have a different shape than brain cells or neurons, because they have different functions.
In the context of this talk, we are interested in inverting that model, in learning biophysics or biology, from the data that we have, which are the shapes.
So we do statistics and machine learning on shapes, which is what we will call shape learning.
Let me show you how we can do shape learning in biomedical imaging by showing you the architecture of a typical research project in our lab.
So usually it starts with a biomedical question.
For example, what is the acceleration of the atrophy that we see in the brains of patients with Alzheimer's disease?
It would be interesting to know what's the magnitude of these accelerated brain shape changes, because it could give ideas to build automatic diagnosis procedure.
Another biomedical question could be, how do cells move?
We can see on this slide a moving cell, so called the migrating cells.
How can cell move is a very important question when one studies metastatic cancer, when the cancer cells propagate in the human body.
All in all, it starts with a biomedical question.
Then there is a step of image acquisition, acquiring the MRIs, acquiring the microscopy images that is done by collaborators of our lab.
Then there is an important step of shape reconstruction.
Yes, because the data that we get are images or videos.
So the shape has not been extracted yet.
We made it an algorithm of contour detection, edge detection, or segmentation to reconstruct the shape.
Then there is a step of shape modeling.
You could also think of it as shape featurization.
Like I said, we have extracted the border or the outline of that cell.
How do we want to represent this shape in a computer?
There are different choices that one can make, and we're going to see a few of them.
And lastly, when we have represented the shape, we have chosen a feature to represent the shape, we can do statistics, machine learning, and deep learning on these data points that are shapes.
In the context of this talk, we will focus on the last two of this pipeline of this cycle.
How can we represent a shape in a computer?
And then when we have represented the notion of shape in a computer, how can we do machine learning on shapes?
Let's start with how we represent the notion of shape in a computer.
So shape modeling or shape representation is interesting because we have different choices as to how to model the shape of the biological structure.
But irrespective of these choices, very often we end up computing with manifolds.
So what is a manifold?
On the left, you have a visual definition of what is a manifold.
We can generally define a manifold as a generalization of a vector space that is allowed to be curved.
On the left, for example, you can see a 2D vector space, also called a plane, that is not curved, it's linear or flat.
And just below it, we have an example of a 2D manifold M that is itself allowed to be curved.
You can see that there is some curvature there.
Now, this is interesting for us because, as I said, we can try different models of shapes, different representations of shapes, but very often we will see this manifold M appear.
Let me give you examples of how manifolds enter the computations.
First, let's look at the shapes themselves.
Let's say we are interested in the shape of this cat, and more precisely the shape of the surface that is defining the cat.
This surface is a generalization of a vector space that is allowed to be curved, and so the shape itself is the manifold in this case.
So I write M for the shape.
The manifold is the shape.
If you want to compute on these shapes, for example, define a hit map or a scalar field on this shape, you are working with defining a function on a manifold.
Second example is a bit more abstract.
The manifold can be the space of all the shapes.
So in the first case, one shape is a manifold.
In the second case, we consider the space of all possible shapes.
What you have illustrated with this sphere is actually the shape space of all the possible triangles.
So you can see if you look, maybe you can see.
They have superimposed little triangle on this sphere.
Here, there is a triangle.
This sphere actually represents the space of all the triangular shapes, so all the visible shapes.
Now, this situation is the shape space of triangles, but more generally, the shape space of surfaces is also a manifold.
Continuing, you can be interested in the space of shape motions, for example, how you shape, translate and rotate.
Here you have a cancer cell translating and rotating.
If you want to do statistics on all the possible motions that your shape can perform, then you're doing statistics on, again, a manifold M.
That is this time, the space of shape motions.
And lastly, you could be interested in how shapes deform or smooth deformation of a cancer cell, as you can see on the left, but also other objects like functional maps that explain how shapes can deform.
And interestingly, again, if you choose this shape representation, you end up with a space of shape deformation, that is, a manifold.
There are other models of shape that I will not cover here, but it seems that these four models of shapes are quite common, and all of them require thinking about manifolds.
Thinking about manifolds, what can we do with manifolds?
Let's think about generalizing basic operations and, for example, generating the definition of a new manifold.
On the center of the screen, I put the sphere.
The sphere is one example of manifolds that we had just before.
It can represent the shape space of triangles.
The one point of the sphere is a shape of a triangle.
You can also think of it as one point of the sphere is a shape motion.
How do I compute the mean of two triangles?
That would be the two blue points here.
Or how do I compute the mean of two shape transformations?
That would be the two blue points here.
I could use the traditional definitions of statistics, and I can use the traditional definition of mean and compute the mean as this orange point, which would be computed as the middle of the two blue points.
In that case, if I apply traditional statistics, there is a problem because you see the mean and orange does not belong to the manifold.
Now, if the manifold represents the shape space of triangles, what we are saying in applying traditional statistics is that the mean of two triangles is not even a triangle.
So traditional statistics do not really apply in this setting.
We would like to generalize operations, statistics, machine learning, and deep learning to manifolds, so that when I try to compute the mean of these two blue points, that again would be two triangle shapes, two shape transformations.
I end up with a orange point that is at least part of the data space that I'm interested in.
So we'd like to generalize all of these operations to manipulate.
Would we want to do that instead of, for example, just using traditional statistics and then projecting back on the manifold.
This is because knowing that the data space is a manifold is information.
Knowing the geometry of the shape space of the space of shape motions of the space of shape deformations, that is information, and any information that we can incorporate in the learning algorithm is welcome.
So in other words, we would like to use the geometry of manifolds as an inductive bias in our analysis.
So let's see how we can do this. So I've just introduced shape modeling.
I've introduced four models of shapes, or four representations of shapes, and all of them require us to compute a manifold.
Now let's move to shape deep learning, which is a way of computing on manifolds and see how we can define shape deep learning bodies, manifold spaces.
So let's think about what is deep learning first. I've put four big categories of subfields of deep learning, or machine learning in general, supervised learning, unsupervised learning, reinforcement learning, and optimization.
What is supervised deep learning in the context of shape learning?
In supervised learning, the goal is to learn a map from an input space x to an output space y.
Now let's rename the input space x into mx, to denote that the input space can be a manifold.
And let's rename the output space y as my to denote that the output space can be a manifold.
This is a setting that will appear if we want to do shape deep learning.
As an example, let's say mx is just an image, a 2D image of a 3D object. In that case, it's not a special case of a manifold, but it's not a curved manifold.
And let's say y is the space of the pose of the subject. In that case, my would be a manifold.
That's an example of shape learning when we want to predict an element of my, therefore an element of the manifold.
You can have all the cases where mx is the manifold.
For example, you see the shape of the brain, and you want to predict if that brain has Alzheimer's disease.
In that case, mx is the space of all possible brain shapes, and y is the probability of having Alzheimer's disease.
And you can also have a case where mx is a manifold, and my is a manifold.
In unsupervised learning, the goal is to find patterns in the data that belong to the data space m.
Let's say we want to find patterns in a data space of shapes, then m is a manifold.
And so we need to generalize unsupervised deep learning to unsupervised learning to manifolds.
For example, of unsupervised learning, the beef want to find clusters of shapes.
We need to find a method that can perform clustering and manifolds.
In reinforcement learning, same story. Let's say we want to learn the policy by that takes that gives the priority of applying an action on the state space.
Then each of these spaces can also be a manifold or both in the manifold.
For example, we want to learn how a shape should move to optimize a certain feature.
And lastly, what is underpinning a lot of development in deep learning is the broader field of optimization.
And same thing, we may wonder how this can generalize to manifold.
Let's say we want to find the optimal shape that verifies a certain feature.
In that case, we want to perform an optimization where the parameter that is varying is actually varying on a manifold.
So we want to perform optimization of manifold.
The example here, let's say we want to find the optimal deformation that can map one shape onto the other.
The deformation is an element of the manifold of definitions.
And therefore this would be an example of optimization on manifolds.
So how can we generalize all of these wide fields of deep learning to manifolds?
And that's actually a trick that I call the vector space manifold conversion trick.
And the trick is as follows.
You can realize that all of these machine learning statistics deep learning algorithms rely on the same basic building blocks.
I mean, some sets of abstract elements and some set of abstract operations.
Now, if we can convert these building blocks from their definition on vector spaces to their definitions on manifold, then we have a translation in the sense of language translation that allow us to beautifully convert any type of statistical learning machine learning and deep learning algorithm to manifolds.
So let's do this.
What are the basic building blocks that are at the core of every statistical learning machine learning and deep learning algorithm?
Well, in terms of the elements, almost all of these algorithms uses either points or vectors.
So point on the data space, in our case, one point in your shape, one point can be a deformation.
They also use vectors.
Think about gradient descent.
The gradient is a vector anchored at a point on the manifold.
And then in terms of operations, there are three operations that come over and over again in these algorithms, which are computing a straight line, computing a distance, and performing a net issue.
So straight line, linear regression, our principal component analysis, we are using straight lines.
So we need a way of computing straight lines.
The squared distance.
In many loss functions, we use a notion of squared distance between the ground truth and our prediction.
Very often the L2 distance.
And then the addition.
For example, when we add a gradient to an estimate, we are adding the gradient vector to the estimate point.
So we are adding the black arrow to this blue point and we get an orange point.
Now, assuming that most of the learning algorithms uses these basic elements and operations.
If we can generalize our convert these elements and operations to manifold, then we have our conversion tracks.
I'm going to introduce the conversion.
Yes, we want to convert to manifold.
The points are just points.
They are the elements of our manifolds.
What was a vector on the vector space becomes a tangent vector on the manifold.
So on the vector space, point and vectors are approximately the same.
On the manifold points and tangent vectors are two different very different elements.
On the left, the registration on the left, you can see the tangent vector in black that is anchored at the point in blue on the manifold.
So we have generalized basic elements.
Now let's generalize the operations.
The straight line becomes a geodesic on M.
I'm not telling you how we compute the geodesic, I'm telling you the translation of conversion of operation.
For example, on the registration on the left, you can see the dotted black line is the geodesic between the blue point and the orange.
The square distance between two points on the manifold become the square geodesic distance between the same two points.
We basically the length of the square length of the geodesic that is connecting these two points.
And that's when we need a generalization of the notion of addition.
We can, on the vector space, we can add a vector, for example, a vector in black to a point here in blue.
On the manifold, this operation is now called the exponential map.
And it is again just rated on the left, while we add not a vector but tangent vector.
So the tangent vector in black to the point in blue.
And performing this addition operation allows us to reach another point on the manifold, which is the point in orange.
So that is the vector space manifold conversion.
And this trick, we implemented it in this open source package called GM stats, which generalizes all of these operations that we know very well on vector spaces, two manifolds.
And for different types of manifolds, specifically the different types of manifold that I showed you in the context of shape representation and shape models.
It's a Python package that is available online at these links. And we created it with three objectives.
First to teach hands-on geometric learning. So computing on manifolds can be hard in practice, even though the word manifold comes over and over again in manifold learning, for example, the basic operations on manifolds are sometimes hard to implement.
And one of the reasons is that they're not necessarily taught in class besides textbooks.
So with this software, you can teach hands-on manifold and geometric learning.
The second objective is to support research in geometric learning or learning on manifolds.
So different researchers reach out to us who publish, they have published papers and they want to implement them methods in GM stats so as to make them methods better, more available to the general public.
So that's on the contributor side supporting the research.
And the last objective is to democratize the use of geometric learning, in other words, learning on manifolds.
Because one of these methods are incorporated into GM stats.
From a user's perspective, you only need to know the conversion that I just presented you on the previous slide and you can use all of these algorithms.
So you do not need to know how a geodesic is implemented.
You just need to know that the geodesic is a generalization of a straight line.
So it allows to democratize the use of these methods.
Now other libraries that implement computations on manifolds, learning on manifolds and optimization on manifolds.
You can see them here on the screen.
The first three, the one that have opts in it and MacDodge, they are focused on optimization on manifolds.
We want, for example, to minimize a criterion and the parameter minimizing the criterion is an element of the manifold.
And then all the others are Python packages that perform computations and learning on manifolds.
But they focus on a special type of manifolds.
For example, the manifold of SPD mattresses, free rotations, disruptions.
We do not do fancy optimizations in GM stats, but we try to implement a very wide range of manifolds, including the manifolds that represent shapes and shapespaces.
So let me give you an example of how you can use this software to compute with shape representations and let's say shape motions.
So I'd say you're interested in knowing how a shape evolved from being in orientation R1 and translation or position T1.
And how it evolves from this original pose to an end pose, which we could call R1 for rotation R2 for rotation to and T2 for translation to.
So when we compute with rotation and translations, as we do with motions, by the way, should be 3D translation, not 3D translation.
We are computing with elements that belong to a manifold, and that gave the manifold is M equal SP3, and that stands for special Euclidean group in three dimension.
In other words, the group of all 3D translations and 3D rotations.
This is a code snippet using GM stats.
What you really have to do is instantiate the manifold that you are interested in.
Here, we want to work with SP3.
So we instantiate SP3 as an object of the class special Euclidean in dimension three.
We extract what's called a metric.
It's a remanian metric that allows us to perform the computations.
And then we call metric.geodesic.
Again, we do not need to know how the geodesic is implemented.
We just need to know that the geodesic is the generalization of the straight line.
And therefore we can compute the geodesic that start with an initial point, rotation one translation one and has an initial tangent vector.
And on the right, you have an illustration of the geodesic.
Pose one corresponds to R1T1, rotation one, translation one.
It's represented by one very small frame with three axes because we can use the translation.
Translation one to place this frame in 3D.
And the rotation, rotation one to orient the frame in 3D.
But therefore one little frame corresponds to one point in SC3.
And you see here trajectory of frames that correspond to the geodesic on the manifold SC3 linking pose one to pose two.
So I told you we implemented a lot of different manifolds in GM stats.
And what we end up with is actually a numerical taxonomy of manifolds.
So we use object oriented programming to create this hierarchy of all the possible of many manifolds that you could be interested in working with.
So this hierarchy is built as follows on the root of the hierarchy of the tree.
You have the most abstract concept of manifold.
That's just a manifold and will list all the different attributes or methods that manifold item object should have.
And as you go down this hierarchy, you find more and more specialized or more and more concrete manifolds.
For example, one level down, you found the manifold matrix lead group.
But the group is a special case of the manifold.
It's a manifold that has an algebraic group structure.
So therefore, my matrix group is on the second level of the hierarchy.
It inherits from the Python class manifold because it's a special case of.
And then you go down this hierarchy and at the very bottom of the hierarchy, you have the leaves.
And these are manifolds that you can actually instantiate.
So for example, there will be somewhere in there.
The special Jupyterian proof that we just use, which would inherit from matrix degree, because it's a special type of matrix degree.
And again, matrix degree itself inherits from manifolds.
So with this project that started with representing shapes and doing statistical learning on shapes.
We actually ended up creating a computational representation of differential geometry.
Okay, so let me wrap this up a little bit and show you how we can mix these two.
So how we can use a model of shape to, how can we can perform deep learning algorithm on the manifold that would, for example, represent the shape space.
And the example I'm going to choose is variational thinkers.
How do variational entrepreneurs work on manifolds zooming out a little bit.
Let me show you an overview of the research that I'm interested in.
In the previous slides.
I was showing you the CRT of computational differential geometry.
This is the hierarchy of modules.
I'm interested in generalizing statistical learning machine learning and deep learning on this exotic data spaces.
In other words, I'm interested in filling the table that you see on the slide.
Where in this table, the different rows represent different manuals.
So each of the row in this table correspond to one of the node that you had in the previous year.
There are different types of exotic data spaces if you wish.
And in this table, the different columns represents different fields of statistics machine learning and deep learning.
I'm going to talk about derising variational thinkers to manifold.
So therefore we are in the column dimension reduction and the dimension reduction and variational thinkers.
I'd like to present a geometric perspective on them as to emphasize where many folks come in.
On this first line of this table, you can see two classes of dimension reduction methods.
The first classes, the first class of methods are the methods that seek to perform dimension reduction on an ambient space that is a vector space.
Specifically, let's say the vector space is R2 and we have some data points, which are the dark green data points.
We can think of principal component analysis that seeks to learn a linear subspace that would be the light green line within this linear space that is R2.
And you have different dimension reduction method that work like this, that wish to learn linear subspace within a linear space.
In the manifold world, we remember that linear subspace or that linear line is converted to geodesic.
And so you have equivalent methods that allow to learn the equivalent of a line that is a geodesic on a manifold.
For this time, the ambient space is a manifold, M will be the sphere, and we're interested in learning principal directions of variations of the data, the data of the dark green point.
And we want to learn principal directions of variations of this data that will be the equivalent of the lines on the left, so that are the geodesic.
The geodesic on the sphere is a great circle, and you can see in light green a great circle.
And in the context of I should know to employers, the vector space case, we are not linear, we're using deep learning, and most of the time we are highly non.
Therefore, we need another row in this table to complete the overview of dimension reduction methods from a geometric perspective.
And then we talk about the one that is on the bottom left, which include, which is a class of methods that include by I should know to employers.
This class of methods operates in an ambient space that is again a vector space.
It's again represented as R2 here.
But instead of learning variations of the data that are linear, as this here would do, instead, we are allowed to learn a nonlinear subspace M.
So you see the dark light green line is now nonlinear.
So I'm going to present how we can generalize this VAE presented from this geometric perspective here to geometric or VAE on manifolds, which is what is shown on the bottom right.
So now the setting is the ambient space is a manifold.
So the sphere represents the ambient space that is the mental to which we know that the data are in which we know that the data.
Now we don't only restrict ourselves to learning a geodesic subspace, as the road just about was doing, but we allow to learn any nonlinear subspace or non geodesic sub manifolds and within the manifold them.
So how does this work? Let me review variational two encoders.
I put the equations on the left for completeness. I'm not going to comment on them too much.
Viational two encoders start with a generative model of data in order that the ambient space that we had before the data assume to belong to a vector space.
So we can think of variational two encoders as a way to invert a generative model with latent variables, and typically the genetic model with latent factors such as the one that is in the first equation.
So the XI, that's our data in the vector space RT, that is assumed to be generated with a generative model with latent variables.
But the latent Bible out Xi, they are going to represent the low dimensional representation of the fi.
This low dimensional representation ci pass through a function f with parameter mu and w. This function is usually represented by a neural network that is called the disorder in variational two encoders, which no noise is added.
So that's the generative model that we assume has generated data in variational two encoder from a geometric perspective.
Zi are assumed to belong to a lower dimensional latent space that I write RL.
Then this Zi passes through the function f of mu and w, which can be the neural network.
And by passing through this function, it becomes an element of the higher dimensional space or do. So this is the blue point on the illustration.
And then noises added to the model.
Now, if we were to pass the whole latent space RL through the function f mu and w, we would get this nonlinear light green curve that represents the subspace the nonlinear subspace that we are learning.
That's a geometric explanation of the genetic model of the data that is at the foundation of variational two encoder.
Now, with the we do not observe, we do not know what is f of mu and w.
We model it as a decoder, but we do not know what are the weights of the decoder.
And we also we do not know what are the Zi that are associated to each Xi.
So, even only the Xi's, we would like to be able to learn the low dimension their low dimensional representations Zi together with the decoder the model.
So this is done with this architecture that you might be familiar with. On the left, you have the traditional architecture of the variational to encoder.
On the right, you have the last function that is used to train it.
The second half represents the decoder.
It is the representation of the genetic model that takes an element of the lower dimensional latent space in green and outputs the function f of that latent variable.
And on the left, we add an encoder that is able to perform the operation of doing from Xi to an approximation approximates representation of the posterior of the CI.
Anyway, this is all trained with the last function that is called the elbow.
So this is the evidence lower bound.
It's lower bound of the likelihood of the log likelihood. And I've given its expression here as a sum of the reconstruction term and the regularization term, because these are the terms that we have to generalize manifold to have a version of variational to encoders that works on my foot.
So we can do this to generalize variational to encoders to manifold.
We will have to generalize two elements.
First, the genetic model, and second, the last function.
Looking a little bit at the genetic model and the last functions, we observe that they are built from basic elements and operations that we now know how to converge to manifold.
So specifically in the generative model, we see there is a plus, which is an addition.
And the generalization of the addition and vector spaces is the exponential map and mental.
The last function also we see that there is a square distance here.
A squared L two distance between X and F of C.
Now that's something we can generalize to my default.
And this is how we go to manifold variational to anchor.
It's conceptually easy.
Now we have to replace the addition by the exponential map.
And now we have a generative model.
That's output point on the manifold.
And that goes like this, we find again, Zi on the latent space or L.
Now, our decoder is going to be able to map Zi on the point on manifold, to which we add noise by adding a tangent vector to the point in group.
So we have generalized the generative model to manifold.
We need to generalize the inference, which is how do we learn in this general model.
We have a very similar architecture that has an encoder and a decoder.
But then what really changes now is the last function that you can still formulate as an elbow of a different log likelihood, just because the generative model is different.
And I've put in red, the terms that change.
Because the generative model is different, the log likelihood is different, the evidence lower bound or the elbow is different. And therefore you can show that it has this formula.
We could have guessed this formula by converting the elbow loss as it was written on vector spaces to its expression on manifolds.
And so this allows us to learn non-Jerizek sub-manifolds of the given manifold.
I think the only competing method that was able to do that before was relying on Monte Carlo, for example, from the Mysterion, or therefore.
So using VAE, we're able to learn non-Jerizek sub-manifold of a manifold.
We're also able to give some insights of what was observed in the literature about VAE, which was about this statement that you can see by Chao Edel in 2018.
But I was saying that in VAE, the experiment showed that these models represent real image data with manifolds that have surprisingly little curvature.
Long story short, we are able, through a geometric analysis of geometric VAE, that gives insight on VAE, to explain why they find latent spaces with so clinically little curvature.
So to conclude, this is the type of pipeline that we're interested in our lab, going to do shape learning in biomedical imaging.
I've explained how shape modeling gives rise to performing operations on excessive data spaces that are manifolds, and therefore how statistical learning, machine learning, and deep learning can be generalized to these spaces if you want to shape deep learning.
Many thanks to our lab at UCSB, and many thanks to the organization that are funding this research, and I'll be happy to answer any questions.

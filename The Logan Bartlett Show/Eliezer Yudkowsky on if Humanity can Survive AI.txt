Welcome to the Logan Bartlett Show.
I am your host, Logan Bartlett.
And what you're going to hear on this episode
is a conversation that I have with Elie Azar Yadkowski.
Elie Azar is known for a bunch of different things,
including starting the rationalist immunity,
as well as a number of books.
He's the author of the blog, Last Wrong.
Most notably, though, he has been shouting
the concerns of artificial intelligence
for the last 15 or so years.
And in the last, I'll call it, six months,
he's reached the conclusion
that we're inevitably headed to our demise
and the artificial intelligence is going to kill us all.
I go into a bunch of different stuff with him about that,
how he ended up reaching these conclusions.
He has a very interesting life story,
never having been educated in any formal sense.
Never went to high school, never went to college as well.
He's a brilliant mind, an interesting person,
and genuinely believes all of the stuff he says,
so I wouldn't have a conversation with him
to understand it better,
understand where he's coming from,
and hopefully help us bridge the divide
between the people that I think were headed off a cliff
and the people that think it's not a big deal,
so trust you'll enjoy that conversation now.
Eliezer, thanks for doing this.
Thanks for having me on.
How would you define artificial intelligence?
What is artificial intelligence?
I would say that artificial intelligence
is configuring computing power in such a way
as to understand reality
or figure out how to manipulate reality
in ways that people used to think required human thought.
I think that a bunch of people are sort of coming into this
with a notion of intelligence as book smarts
as artificial intelligence.
A very common thing I get asked is along the lines of,
well, like my professor didn't take over the world,
so why be scared?
And the notion of problems solved by the brain,
like social skills, understanding people
is a kind of understanding reality.
Manipulating people is a kind of planning.
Don't just think like super college professor
or super chess player, think like super Napoleon.
Super Einstein, super Edison.
Social skills are processed in the brain,
not in the kidneys.
It's an information problem,
not a like how much can you lift,
not a like how fast can you run problem.
Current artificial intelligence is still better
at recalling or with modern stuff making up
a wide range of facts.
It can also write code.
GPT-4 can play chess,
not sure exactly how well I've heard
different reports about that.
The strange notion is that,
well, for example, if you look at humans,
if you just grind optimization at making something
better and better at chipping flint handaxes
and throwing spears and outwitting
its conspecifics and social contexts,
it eventually figures out how to go to the moon.
And it might not be obvious a priori
that this would be true, but it is how things played out.
And so the sort of scary thing in artificial intelligence
is the notion that as we keep going
on grinding these things smarter and smarter,
we don't just get super college professor,
we get super understanding, super invention,
super planning, super understanding of people,
super planning about people on precedent and inventions,
on precedent and discoveries, out from the AI itself.
And on my model of how this works,
getting very, to get very good at these things
is intrinsically to get good at steering reality
and to get good at steering reality implies
that there's a place you're steering reality to.
To do very good science, you need
to plan which experiments to perform.
There is no such thing as just being
very good at understanding reality
and not good at planning at all, because you
have to plan experiments.
You have to choose which questions to ask.
You have to decide which important things to think about
and what to think next.
To get very good at understanding,
the simplest path that evolution hit upon,
that I expect gradient to center will hit upon,
the simplest path to understanding
is to steer reality into the state where you understand.
To get very good at chipping flint handaxes,
you don't just learn to chip flint handaxe chipping reflexes.
You learn to envision a state of reality
where you have a handaxe, and then the path of chipping
that you do to get to that state of reality,
you learn imagination, not just to do flint handaxes,
but it works for flint handaxes too.
And it's not like we evolved to have guns.
Guns are from very late in our evolutionary history
after most of the shouting was over.
So that's one of the core things there
is not intelligence as book smarts,
but like somebody's like, well, the CEO is not always
the smartest, but the CEO jokes aside,
it's not usually a chimpanzee.
There is actually a minimum intelligence to do that.
And chimpanzees don't have that much intelligence.
That is the kind of gap that we are worried about when
I talk about superintelligence, that there are things that
does where we are just not eligible to play in the same way
that jokes aside, chimpanzees are not
eligible to be CEOs.
And it's important for people to understand that just
because there's limitations doesn't
mean that something that's very smart can't pursue a goal.
And one of the things that I've heard you talk about
is flight, for example.
We have limitations on our ability to fly.
You nor I, I don't want to speak for you,
but at least I cannot fly.
But because of our intelligence, we're
able to figure out how to fly through the air,
as I did coming back from Paris to here.
And so goal seeking of in pursuit of some goal,
if you're intelligent enough, the limitations
of being only software on a computer, for example,
will not limit the ability to do something.
Is that right?
Yeah, humans started out just like these creatures running
across the savanna, didn't even, well, the hominids didn't even
have spears.
They got smarter.
They made spears.
We didn't get the sharpest, longest claws.
We made ourselves artificial, sharp, long claws.
We didn't start out with guns.
We didn't start out with nuclear weapons.
People worry about artificial intelligence
getting a hold of nuclear weapons.
And no, what makes human dangerous
is not that we were handed out nuclear weapons
by somebody else.
But some foolish person built nuclear weapons left.
We made nuclear weapons for ourselves.
The thing that can do that is dangerous,
not because it then ends up with nukes,
but because it's smarter than you,
and can plan the route through reality that ends with you dead,
whether it was by nuclear fire or some other fire.
Artificial general intelligence is
the notion of the kind of mind that can do things
that wasn't built for, wasn't trained on.
A bee builds hives.
A beaver builds dams.
A human looks at them and imagines a honeycomb-shaped
giant dam structure.
And what makes it sensible to look at a human and say,
those are significantly more general intelligences
than chimpanzees is the way that we handle
this very wide range of tasks that we didn't explicitly
evolve to do.
We didn't explicitly evolve to build rockets
and go to the moon.
It's just that the skills for chipping flint handaxes
and outwitting other humans generalize sufficiently well.
We could understand all these areas
that we could understand the high vacuum.
We could understand the solar light above the vacuum.
We could understand the rocket equation.
We could understand the rocket fuels.
All these things that our ancestors did not know,
we understand without having been built to understand them.
We generalize.
We aren't truly general.
We're terrible at, for example, writing code.
Our code has bugs in it, which is not necessarily
a way that a mind needs to work.
It could just do some of those processes
without making errors and generating proofs along
the way that it would work.
But we can still do it at all.
We're not fully general, but we're
more general than chimpanzees, and the thing
to be scared of is the thing that is more general than us
that has the spark.
That goes that it's not magic.
It's not that we are conjuring new brain structures
from the ether.
It's just that the brain structures we were built with
to solve the problems of our ancestral environment
generalize sufficiently well.
GPT-4 is able to do a bunch of stuff
that it wasn't obviously trained to do in any particular way
by predicting text on the internet,
like drawing a picture of a unicorn
when it has never seen a unicorn was never trained on images.
You just ask it to write a program that
draws a picture of a unicorn.
It doesn't do too terribly at that.
It doesn't do great, but it's never seen anything at all.
So that's starting a bit to generalize a bit further
than we would expect cats to generalize,
wolves to generalize, nonhumans to generalize.
That's why the paper where they recount this sort of thing
is called Sparks of General Intelligence.
So I want to go through your background in a linear fashion,
because one, I think it's interesting in general,
and no one's exactly done that.
And two, I think it helps establish your credibility
as someone that's been thinking about this stuff
for a long time.
And it's had a bunch of different evolutions
in your thought, the land here.
This wasn't something you woke up on the Bankless Podcast
two months ago or three months ago,
and for the first time started thinking about.
So one thing that's a weird place maybe
to start out for people that I think
is an important framing is, what is
the purpose of a fire alarm?
Well, you might think that the purpose of a fire alarm
is to try to get out of the building,
but there have been some interesting studies done
over the years in cognitive psychology.
What happens if you're in a room with some other people
and smoke starts to come out from under the door?
People who are alone in a room that starts to fill up
with smoke will typically go out and report it.
If you put a group of people who are all
naive experimental subjects together,
they sort of like, should I be reacting to this
and do a quick sideways glance while trying
to look very composed themselves because it's embarrassing
if you're reacting to something that's not a real emergency?
And they see that other people are looking very composed.
They don't like, hack them to the exact moment
to doing the sideways glance.
So I think something like a third of the time,
nobody in, I think it's a third of the time
that anybody in the room with three people
goes to, leaves or goes to report the smoke.
And if you put them next to two experimental confederates
who are deliberately not reacting, 10% of the time,
does the remaining subject go and report anything?
Of which the moral is that the fire alarm serves a function
of saying that it's socially permitted
to evacuate the building.
Lots of us, I think, have encountered false fire alarms
at some point in our life and just sort of like,
taken the time to verify, does there actually
appear to be any sort of fire or not?
Because we're used to false fire alarms.
But if we did notice a fire, the fire alarm
would give us social permission to react.
And so when I, a bit some time ago,
wrote, there is no fire alarm for artificial general
intelligence, what I meant by that was not,
there's nothing that's a sign.
Smoke coming from, there's plenty of things that are
equivalent to smoke coming from under the door.
What I meant was, I didn't expect there
to be any such thing as a moment that gave everybody
social permission to react.
Which I may possibly, it's not clear at this point,
have been wrong about, because ChatGPT and GPT4
and Bing Sydney seem to have given people permission
to react to the point where Jeff Hinton resigned
from Google so he could speak openly of the danger.
And you know, like maybe that gives people
social permission to react.
Yeah, I might have been wrong.
Could have been that.
The answer was, is there a fire alarm for AGI?
Yes, it's Bing Sydney.
It's an interesting thread and theory about like,
what the signs are for people that people
are actually gonna react to here.
And if this is gonna be a landmark moment now,
maybe your efforts in conversations are gonna be
important or Jeffrey Hinton stepping back,
or what is the thing that's gonna make people
most concerned or actually paying attention to this in mass.
But I wanna go back to the early days
and we'll start with telling the story
in a linear fashion of who is Eleazar.
So you dropped out of elementary school?
No, I completed eighth grade and it was clear
that my health status would not permit me
to go on to high school from there.
Do you think you're less common path?
You didn't go to high school or college?
Yeah, I took like an AP calculus course
at the local Jewish high school
and like some time later took linear algebra at a college.
But you know, it wasn't super helpful,
but by the time I actually needed linear algebra,
I just had to like reteach it to myself.
How did you educate yourself?
Math textbooks, you know, and various other textbooks
because it used to be that there was more to,
there used to be more to artificial intelligence
than like calculus for bright to 10 year olds
and the first five pages of a linear algebra textbook,
which is sometimes how I describe the math
that's being used in it now.
So yeah, like back in the day, I didn't just study
like bits and pieces of math.
I also studied evolutionary biology
because you understand like how the other powerful
optimization process we've ever encountered,
natural selection, how does that hill climbing play out?
The, you know, if you dive down with the details
of deep learning, there's sometimes interesting bits
of math and the same you could also say
about evolutionary biology, exactly how natural selection
works as an optimization process.
Cognitive psychology, obviously.
Evolutionary psychology, which I think people
who've never studied the actual science
sometimes have very strange ideas of what that's about.
Real evolutionary psychology is things like
the way that your eyes and brain maintain color constancy
given the way that light varies in the natural environment
and in particular, so for example,
like one of the dimensions that it varies
is the angle of the sun determines
how much atmosphere is between you and the sun,
determines how much light gets scattered.
And then are you in the sun or the shade?
There's one of the dimensions around which
illumination varies in the natural environment
and then your eyes and brain are adapted to decode that.
That's an example of what, you know,
evolutionary theory of the brain actually looks like.
People on the internet have some much stranger ideas
of what that's all supposed to be about.
Were you just sitting at home reading these textbooks
yourself with no grades or feedback
or would you go check it yourself out of the library
and just come home and study this stuff?
More or less, yeah.
Interesting, basically.
And you mentioned that the fatigue or the illness,
it's undiagnosed exactly what it is,
but essentially causes you a fatigue?
Yeah, so back when I was working not from home
for a stretch at the Machine Intelligence Research Institute,
yeah, my rule was you have to take an Uber half a mile there
because if you walk half a mile home,
I can do that, I'm just not gonna get any
intellectual work done after that,
after that half a mile walk.
And I know you've tried, no one's been able to figure out
or diagnose what the thing is.
Not yet.
Do you think that your less common path of education
caused you or allowed you the ability to think
in a different way than you would have otherwise
had you gone to high school or college?
I don't know.
I haven't actually tried it both ways.
It's an obvious guess that there is something
about that process that if I'd actually gone through it,
it would have crushed out something in me
that I later needed, but I don't actually know.
And I think that there's a way in which
telling nice stories like that about yourself
ends up being a trap.
You have this story about like,
ah, yes, this is the destiny that brought me here.
And maybe then you can actually take
a linear algebra course into college
cause you've got this huge story about how I am,
like all of my ability comes from not going to college.
So I don't actually know.
And I'm careful with making up those kinds of stories.
Yeah, once upon a time you maybe convinced yourself
that your illness allowed you to be who you are,
but now you've sort of rejected.
Well, once upon a time I had this whole
elaborate theory of like, well, this is wrong.
What's wrong with my brain?
And it gives me this kind of psychology that's useful
and it also produces the fatigue.
And I just think, yeah, it was a misunderstanding
of misunderstood attempt at Occam's razor.
I was trying to have there being a single cause
of everything unusual about me.
And I now think that that's probably misguided.
And even if it's true, like, I'm not gonna figure it out.
Yeah.
And you grew up in Orthodox Jewish household?
Orthodox Jewish household, it didn't take.
It did not take.
So maybe just to go back to childhood influences,
how did organized religion and sci-fi
sort of fit into your upbringing?
I think that, you know, although it was very painful
to grow up in an Orthodox Jewish household,
I think that it was in some sense probably, you know,
there are obvious stories you can tell about
how that was valuable in some way.
My first break with the Jewish religion
came when I think I was five or six years old.
And they were asking me to pray, Daven.
And they were asking me to do it in Hebrew.
And I was like, how's this going to work
when I don't know what these words mean?
And they were like, it's okay.
You don't have to know what the words mean
as long as you're praying in Hebrew.
Like prayers are acceptable to God
either if you understand the language
or you pray in Hebrew.
And this was so very stupid.
And I figured that God had to be
at least as smart as I was.
So I figured that there had to have been
some kind of error in translation along the way.
And that was the point at which I first officially
became a heretic.
Though I didn't like turn full-blown atheist
until I think like 11 or 13,
depending on how exactly you count it.
So I think that this taught me a very valuable lesson
about adults being insane.
Like I never grew up in a world
where the world around me was full
of trustworthy, good advice.
Cause the advice I was getting from the grown-ups
was that prayer would work
as long as you were praying in Hebrew.
Were your parents frustrated by your attitude
about all this stuff?
They found out that, oh, I got some pushback early on.
I tried some pushback early on
and got shut down hard with you'll understand
when you're older.
They were right about that.
My parents found out I was an atheist
when I moved out of home at age 20.
Oh wow.
So you were able to mask,
not to draw analogies to artificial intelligence,
but you were able to present one way and just for sure.
If you were drawing overly strained analogies everywhere,
you could be like, ah, yes,
the smarter intelligence successfully hit its misalignment
from the lesser intelligence
is trying to control its resources.
I don't actually, I think that, yeah,
especially when I was like a little kid,
I don't think I was actually smarter
than my physicist dad or academic psychiatrist mom.
At six years old, you probably.
I think that they probably had more net brain power
than me at that young age.
But they were using it to defeat itself.
And what about science fiction?
So I know that was a big influence of yours growing up.
Did you find, did you take comfort in that
when your immediate world around you was one
that you didn't identify with?
Was this world of science fiction?
Yeah, I don't think even in terms of taking comfort,
you know, and I would read science fiction books as a kid
and that was enjoyable.
And I do think that that is the culture
that I absorbed in place of orthodox Judaism.
And then when you were a teenager in the early 2000s,
I think that's when you first realized
super intelligent AI would be the most important thing
to happen to humanity.
No, that's 1996 at age 16.
Got it.
What was that, what was the moment of realization?
Like reading a short story collection
I happen to take out from the Chicago Public Library
called True Names and Other Dangers,
author of Verner Vinci on page 46, I think,
of that short story collection.
Verner Vinci says after a story about a brain augmented
chimpanzee escaping from the lab,
of course I never succeeded in writing the important successor
story, the one about the augmented human.
I once tried something and my editor, John Campbell,
sent it back with a note saying, I'm sorry,
you can't write this story, neither can anyone else.
Here I tried a straightforward extrapolation of technology
and found myself precipitated over in abyss.
I forgot exactly how you phrase this,
but as soon as our stories predict
the creation of something smarter than us,
our crystal ball breaks down and we can't foresee
the future past that point, which he termed the singularity
after the singularity to the center of a black hole
where the models of physics then in play broke down.
And it's a subtlety that I think a lot of later people
missed, is that Verner Vinci was not claiming that anything
goes to infinity.
He wasn't saying that physics broke down,
he was saying that the model of physics broke down.
It's not named after discontinuing reality,
it's named after discontinuing the map.
And this to me was just like sort of obviously the most
important thing going on the moment I read it,
like just as soon as I read the paragraph, I was like, yeah,
okay, that's what I'm doing with the rest of my life.
This is where the important stuff is going down.
Anything smarter than human.
And so that was at 16 years old.
And then after that, so in your,
was it late teens or early 20s,
you started the Singularity Institute
for Artificial Intelligence?
Age 20, year 2000, yeah.
And your goal was to usher in a good AI?
No, as a young teenager, I did not realize
that AI's had to be made good.
I had some whole weird line of logic
about how they would automatically be good.
Do you recall that logic?
Oh yeah.
It was like, well, either life has meaning or it doesn't.
If life has meaning, then something much smarter
than us will figure it out and do the thing that's correct.
So that's the correct thing to do is build something
smarter than us and then anything,
any problem that isn't that problem isn't our problem
to solve or alternatively like life is meaningless,
but then everything we do is equally pointless.
So you can just like eliminate that from consideration.
Is what teenage Eliezer wrote up in a page
called Frequently Asked Questions
About the Meaning of Life,
which for a while was asked Jesus official answer
to what is the meaning of life?
Wow.
Yeah, and so, I mean, is it fair to characterize you
at that time as something of an AI accelerationist?
I mean, in modern terms, one might call that though.
I was also like able to break out of that
on account of not very much resembling the current
accelerationist very much personality wise
in some important ways.
But sure, like in terms of the rhetoric,
it was like current accelerationist rhetoric,
but brighter, more cheerful, better, less nihilistic,
that sort of thing.
Yeah, I mean, Sam Altman recently tweeted that
you may have done more to accelerate AGI than anyone else.
And I think he meant that as a compliment in that case.
It's somewhat doubtful.
Yeah, he did say maybe you won the Nobel Peace Prize for it.
That's kind of implausible.
I would be surprised if that happens.
Somebody should start a prediction market on that.
So I'd be able to cite the prediction market.
That it's not gonna happen.
And when if it does happen,
in the unlikely event that it does happen,
I would rather expect it to be more for raising the alarm
and getting it successfully shut down.
Not for accelerating it.
Yeah.
So your vision at that time,
we talked a little bit about elements of this,
but what was your vision for good AI singularity?
Like as a teenager?
Yeah, when you started the Institute.
I mean, I didn't until age 21 figure out
that alignment was going to be a thing.
Maybe describe alignment for people that don't.
Alignment is trying to shape an AI
such that it's effect upon the world when you run it
is net positive from the perspective of humans.
Most humans idealized human preferences
after updating to match the AI's knowledge.
Not what you currently want,
but what you would want if you knew everything the AI knew.
There's a relatively shallow rabbit hole to go down there.
But if you just say like,
it's making guys do what humans want.
Well, that's obvious.
What do you mean humans want?
I mean, what humans vote on?
This is obviously not going to end well for anyone.
Yeah.
So not quite that naive.
Yes, yes.
Well, we'll come back to alignment.
But so what was your vision then?
When you did realize alignment was a thing,
what was your initial vision for singularity around it?
It took an embarrassing long time for me
to realize the full magnitude of the issue.
I'm not sure embarrassing.
I think you might be beating too hard on yourself
given how long ago.
Age 21 to age 23?
I think you're beating too hard on yourself.
Okay, it's okay to be imperfect,
but not so imperfect that other people notice is my model.
So I think in this case,
like I can look back and see the particular ways
in which I was too slow to face up to the impact
of the arguments in which I was clutching on
to beliefs I'd previously held,
and not just like immediately throwing everything
out the window in light of a new consideration
and recalculating it from scratch.
I would aspire to do a lot better nowadays.
So what did you determine that AI was likely
to be orthogonal to humans' objectives?
I mean, that's not quite the way I would put it.
How would you put it?
So like the way I would have put it in 2001 is,
you know, maybe AI doesn't end up automatically doing
what's the right thing.
How would you go about building an AI to do the right thing?
How would you define the right thing?
How would you shape an AI to do that?
And not really admitting at the time
that this was like clearly necessary work,
just trying to like figure out
how you would do it just in case.
So that's like age 21.
And then age 23 is actually-
What year is 21, just so we're-
2001?
2001.
And then age 23, 2003 is I think the point at which
I had what I would now call like Bayesian enlightenment
of like seeing the world through the lenses
of probability and expected utility,
and be like, oh, my entire previous theory
was utterly completely wrong
and would have clearly actually just immediately
gotten everyone killed if I tried doing anything
remotely like that way.
I was entirely wrong here.
This entire line of reasoning I was following was terrible.
I was trying to solve the wrong problem
using the wrong methods, using a poor model of reality
derived from bad thinking,
and unfortunately all of my mistakes did not cancel out.
The only thing I did that was at all correct there
was being like, well, if there's even a tiny chance,
ha-ha, that you've got to actually do any work
for to align AIs, you know, like my duties
toward the people funding me and the particular funder
of the Singularity Institute as it was then called.
You know, like require me to chase down the small possibility
and understand it and figure out like how I'd cover
the special case of maybe possibly an AI
needing to not be aligned.
And because I chased down that tiny probability,
not a way of thinking I currently endorse,
but back then it's what saved me,
that actually stayed in contact with the problem,
that I thought about how to solve it
instead of finding out some wacky reason to write it off.
That's like stayed in contact with the problem,
asked myself, how would I technically solve this?
Tried to figure out the rules and kept thinking
in that sort of like, how do I actually solve this
technically, not socially, how do I solve this technically
until the pile of evidence and argument
mounted up inside the mind, inside my mind
to the point where it could like collapse
my previous crazy ideas.
How'd you get funding to go pursue this?
Well, I mean, initially it was funding to charge right out
and build an AI and-
How'd you find someone that was willing to pay you
to go do that?
Basically a dot com millionaire
who was on the same mailing list as I was,
the Xtropium's mailing list was like,
so this self-improving AI that you are talking about
as a teenager, could you actually go build that?
No, I said, that would be like too hard to do.
What if we started this wacky business idea instead?
We spent six months working on a business plan there
and we figured out at around the same time
that it just wasn't going to work out.
I was like, okay, you know, let's actually just have
the nonprofit institute devoting to charging right out
and building the AI sooner rather than later.
150,000 people die every day around the world.
55 million people per year die.
If there was something super intelligent around,
maybe it can save them.
Building this thing faster, sooner
is a huge moral imperative.
Let's charge out and start building this,
building self-improving AI
is the premise of it in like late 99, early 2000
and then like 2001 after having spent a bunch of time
thinking about it full time, I was like, okay,
well, what if it's not automatically good?
How would you go about building it from the ground up
such that it would be good in a case like that
and coming into contact with that problem
and spending a bunch of time staring at it?
Had I instead like gone down the road of like,
oh no, well, if you can like shape AI's,
that means the wrong people might get them
and like gone down the pathway of like talking all the time
about how the wrong monkey might get this banana.
I would never have figured stuff out.
I would have not stayed in contact
with the parts of the problem
that would have told me I was wrong in my basic premises.
So 2007 to 2014 was kind of your rationality era?
Well, writing about it.
Writing about it.
Before that you were thinking about it.
2003 is what I would say the era of figuring out
that sort of stuff.
And then like 2007 and on was the period of writing about it.
My understanding is that your goal
through this period of time was that you were trying
to prepare civilization to think about different levels
of risk and in particular with regard to AI potentially.
Is that an over extrapolation or is that there?
I think it views me as a more cunning 40 chess playing
backward chaining sort of person.
Whereas I would usually try to do things
that might have good effects down many different routes.
I noticed in my...
It's hard to do cause and effect of which one can't,
like did you find out rationality
because AI or was AI rationality?
It's hard to divorce the two of them.
I mean, yeah.
Well, for one thing they're like,
at least the way I was doing it,
they're the same discipline.
Yeah.
You know, that they have the same basis
in cognitive science, evolutionary biology,
evolutionary psychology, et cetera, et cetera.
Same math.
You know, not quite so much now
or it's all the math of like putting an enormous pile
of floating point numbers and subjecting it
to a couple of minutes of calculus
to do several months of gradient descent.
Yeah, so from my perspective,
anytime I tried to have a conversation
about artificial intelligence,
it would start to, from my perspective,
it would founder on people not knowing about stuff
like the conjunction fallacy.
Where can the conjunction fallacy is if you give people,
how likely is it that A causes B?
Versus how likely is it that B happens for any reason?
People often say that the compound event
is more probable than other people assigned
to the simple event.
What is the probability that Reagan
will repeal welfare versus the probability
that welfare gets repealed?
I'm not doing this exactly right,
but that was one of the questions
from way back when, when Ronald Reagan was president.
So, you know, from my perspective,
people were never getting to the end of the whole argument
cause at some step along the way,
they'd make a particular sort of error.
So I was like, okay, like if I want people
to follow this entire argument,
I'm going to have to teach them
how to do all of these argument steps locally validly.
What is the valid rule of reasoning on this particular step?
And yeah, just like gave up on winging it
and just started doing all the prerequisites,
the background math, the background cognitive science,
all the stuff you would need to under,
you know, the background evolutionary biology,
all the stuff you'd need to actually understand
the argument on AI, which had a huge amount of overlap
on what you would need in order to do
good reasoning in everyday life, from my perspective.
And, you know, the point of that wasn't just AI,
it was also like human reasoning, one step forward.
There was a vision there of we have all this
wonderful cognitive science
that didn't exist a few decades ago.
Can humanity actually use this stuff?
Can we become saner?
And I think that there was a vision there
that failed basically,
but it was one of the things I was trying to pursue.
I did not know that all this
was going to go down in the 2020s.
You know, from my perspective, you know,
in the 2000s, 2040 was a plausible amount of time
for stuff to take.
And, you know, there was a chance that like maybe humanity,
it also was sort of like a more hopeful time, right?
You know, like,
it wasn't clear yet that civilization's sanity
was going to go downhill because of Twitter.
You know, for all I knew at the time,
we were just going to get like saner over time
instead of being driven mad by social media.
So, you know, like trying to contribute to that,
to contribute to this common human project
of building a base of sanity for future generations
by taking all this cognitive science that had come out
and trying to apply it to everyday life.
That was the vision.
I don't really think it worked all that well.
So, you wrote the less wrong sequences,
also known as rationality from AI to zombies.
And you also wrote Harry Potter and the Methods of Rationality,
which is the most popular Harry Potter fan fiction book
ever written, I think, by some measures.
Depends which measures you use, yes.
Yeah, but people don't realize exactly
how popular of a category Harry Potter fan fiction is.
I guess there are apparently 500,000
or something other Harry Potter fan fictions that are all.
There were the last time I ran the statistic on that.
Yeah, I got it.
But I expect that by now the number is substantially greater.
And so, the point of rationality and less wrong
and Harry Potter, and I'm sure Harry Potter
was just fun as well, but the point of all this was
you were trying to help people,
teach people how to think or show people how you thought
so that they didn't need to reinvent
all of these things on their own, is that fair?
Yeah, like a whole bunch of the motivation
from my perspective is looking back at my own history
and thinking like, well, those steps took way too long.
If there are young aliases out there,
how can I package up all the information that they need
to grow up so they can like start working sooner, right?
You know, like, you know, get all the skills
when they're 16 instead of 26,
and get started on their scientific career earlier.
And that didn't work either.
I think I underestimated how sparse the immediate neighborhood
is in high-dimensional space of people.
You know, Steve Jobs never found anyone to take over Apple,
who was up to his caliber of making neat things
and having nice user interfaces.
And he had all the money in the world with which to pay them.
And you know, somehow couldn't manage
to really replace himself anyways.
And you know, that is the thing that I remind myself of
when I feel bad about failing to, you know,
have already yielded my place to three youngsters
who are all better at it than I am.
Did it really happen?
So you were hoping that you would find more smart people
to step forward and learn from all of this.
Was it with AI as an angle,
or was it just continuing to extend
the general thought of rationality?
Yeah, I mean, I wanted to entirely replace myself, right?
I just didn't figure I was the best person for my job.
And what was your job as you describe it here?
I mean, AI was a large part of it,
but I also seemed, from my perspective,
to be holding the ball on learning
from all the recent discoveries
and synthesizing into something
that people could use to become saner.
And I don't really feel,
and I feel like I failed to hand that ball off either.
Also, there's like very narrowly overlapping things,
like there's the shape,
there's understanding the shape
that computing power has
to be very effective at understanding reality and planning.
And then there's understanding the shape
that humans should have to become more effective
at understanding reality and planning.
It's very tightly related there.
Did anyone come close?
Did you have any mentees or anyone
that you felt like was along the path
during that period of time
that you thought was gonna be able to take over for you?
I think there's other people
who've been able to take up corners of the burden,
but no one who really struck me as being like,
ah, yes, that's a replacement me.
Like the ball is no longer in my hands.
I no longer hold this responsibility.
I don't think there was a moment where I felt that.
So in 2015, I'm gonna describe it as the open AI moment.
There was an AI safety conference in Puerto Rico.
It was you, it was Elon Musk, it was Nick Bostrom,
Max Tegmark, people from DeepMind,
people from Google, a whole group there.
And I think this was a pivotal moment for you
because during the conference,
it seemed like maybe Elon Musk was gonna wake up
to the AI risk possibility
and that there was gonna be something
like a Manhattan project style strategy
where basically keeping all the work secret
from the general public
as they kind of figure out what to do with this.
Is that fair?
I mean, the general public isn't the problem.
It's like literally anyone,
if you've, you know, the Highlander principle,
there should be at most one AI.
That's among the sensible things that could have been tried,
but mostly I was just like, oh, like, is this the moment
where the billionaire shows up and starts taking it seriously
and there's like any effort at all.
And it's not just this like tiny group of people
who are weird enough to work on a problem
despite the lack of very much at all
in the way of incentives to work on it,
despite the low pay in both money and status.
You know, people work for low monetary pay
if they're being paid in high status,
but, you know, working on a weird problem
that isn't well paid and, you know,
makes people look funny at you at parties, that's rare.
The question was like, is this the moment
where humanity turns and start fighting back?
That was what I was hoping was going on there.
And instead, Elon got inspired to start
this organization called OpenAI,
which was co-founded with Sam Maltman
with the idea that AI should be open to everyone,
which is basically a terrible idea from your perspective.
It assumes that the problem with,
it assumes that alignment as a solved problem,
that power disparities between AIs are small.
And so as long as everyone has their own obedient AI,
everyone can successfully use that
to protect themselves in the new world.
And it was based on a model of how AI works
that I think has not held up
and I think is just like utterly wrong.
And it wasn't putting the question
of how AI works as a question of fact at the center.
It was just like, oh, like,
I don't like these people's politics.
This is a political problem.
This is a problem of like, which monkey gets the banana?
I don't like these people building AI.
I don't think they're trustworthy enough.
Only I am trustworthy enough to build AI.
The center, you didn't like Demesis Abbas's DeepMind,
didn't like Google.
And apparently that's a good reason
to go destroy the world or something.
So what was your feeling going into that meeting?
It sounds like you were optimistic going into Puerto Rico
and then I guess how did the outgrowth of open AIs founding
change your mental framework around the likelihood of doom?
I mean, from my perspective,
it meant that humanity was going to just like plunge
straight into this doing the worst thing
and the wrong thing at every juncture,
just go play out like it did in history books.
Read history books, they're not full of people
doing the right thing every time.
They're full of people making mistakes
that typically appear in history books.
I'm like, okay, so like we're going to hold ourselves
to that standard of performance
and that means we're probably dead.
Were there specific decisions that open AI made at founding
because the business model has evolved
or there wasn't one initially
and then there became one that led to this conclusion
or did you feel that immediately coming out
of the once it was founded, you already...
The name is sufficient.
Some of us had put a lot of work over the years
into trying to have an understanding
among players the likes of DeepMind,
which for time was like the major player,
that this was humanity's problem held in common.
Everybody needed to like live up to the trust
that humanity wasn't so much placing in them
as that they were just taking upon themselves.
Go, try to go slow, be cautious.
Or I should say rather that like that the point
of getting any sort of lead was to burn that lead
in order to have more time to spend aligning things,
have it go well and sort of like trying
to create this atmosphere nod when arms race
of having a common human project
that people need to go in on together
and then open AI is like open AI versus Google.
I just like coming and just like completely trash all of that.
And maybe it was a small pathetic thing to have tried.
Maybe it was foolish to think
that humanity could ever not do the stupidest thing,
that there was ever any chance
of having it not be an arms race.
And of course, that as soon as there was any real money
or power at stake, people were going to just like
throw all thoughts of cooperation out the window
and grab for whatever immediately came into the grasp.
But it's not the sort of thing you forget.
When there's a little scrap of hope for it,
however contemptuous somebody more cynical might find it
to think that you wouldn't just do the worst thing.
Whoever actually comes in and like actually tosses,
actually comes down and crushes the ideal.
Yeah, the people who held that ideal for a little while
are not going to forget that anytime soon.
You've referenced monkeys and bananas
and I've heard you refer to this behavior as disaster monkeys.
Can you describe what that is in your mind?
I mean, there's a lot of different people
following short-term incentive gradients,
which isn't so much money as like what gets people
to give you interested looks at parties in San Francisco.
You know, like why publish GPT-2?
Like why show it to the world?
Instead of just like selling it as a back-in-service
if you needed to make money for some reason,
which itself is kind of questionable.
Why tell people how to build these things?
And you can have elaborate stories
about how it was totally a good idea
to make timelines shorter that way,
to get the hype started,
to get the money rolling into the field.
But from my perspective, what predicts the action
is not, there is no like what is good for humanity here,
that we can start from that principle
extrapolate in a neutral way and get what open AI did.
What predicts it is the question of like
what gets admiring looks at parties in San Francisco?
Or scared looks, or angry looks, it's all the same.
If you get people riled up that way,
that's power.
And chasing after the status that comes with that power
is the basic model that I have that, you know,
explains what all these people are doing.
There's a perspective I think that everyone kind of uses
that better them than someone else.
And you've alluded to this, it's almost like a,
I don't know if it's a messianic,
like complex or something,
but that there's a lot of bad people out there.
I'm not one and so I should have.
It's the obvious story that you would grab
if you were looking for a reason
to publish the latest, greatest AI results
that get you admiring looks in San Francisco parties.
And now that people are sufficiently concerned,
there's a chance that you can like get some admiring looks
by saying like that you're not working on the next model,
now that you've scared them enough
that you can like get status,
that people won't just give you eye rolls
if you're claiming that you're not working on something.
But yeah, I'm not really seeing the prediction here
that follows from first principles,
trying to benefit humanity.
So 2015 and 2020, you were focused on AI safety research.
Specifically, I think you wanted to basically build
some kind of off button that we could install
into a super intelligent AI.
Yeah.
Correct me.
Okay.
So the question was, how do you get one bit,
one information theoretic bit of information
into an AI's utility function
without the AI trying to stop you
or trying to get that bit in?
It's not gonna stop you, describe utility function.
The preferences, the thing the AI wants.
The goal that the AI is seeking.
Something like that, yeah.
The place it's trying to steer reality.
So the question is something like,
could you swap between two utility functions?
Can you have an AI that pursues one thing
until you press a button
and then pursues a different thing instead?
And for purposes of illustrating the underlying problem,
you can imagine that one goal is,
like making paper clips, whatever.
And the other goal is shut yourself down.
Fold up what you're doing in an orderly fashion.
Like shut down the results of previous plans,
but not in a way where you then take over the entire world
to make all the plans,
to erase all evidence that you ever existed or something.
And we didn't have a specification of a shutdown function.
We're rather saying like,
suppose you could describe how to shut an AI down.
What does the button look like?
It was like the smallest possible change
you might wanna make.
Just switch from one utility function to another.
And the issue was,
that's not super easy to describe.
The issue is that what if, for example,
shutting down is easier
than what the AI was otherwise trying to do?
What if it's easier to get like an A plus
on shutting yourself down
than getting an A plus on making paper clips?
Then you perhaps have an incentive
to press your own shutdown button.
Okay, you say give it less utility from shutting down
then it has a chance that it wants to prevent you
from pressing the shutdown button.
So the question we were addressing is something like,
is there a self-consistent view into the AI's preferences
where it doesn't want you to press the button
that swaps its utility function,
where it doesn't want you to press the button
that shuts it down.
It doesn't want to prevent the button from being pressed.
It doesn't, if it rewrites itself,
it will rewrite itself in a way
where the button is still there.
If it thinks about its own operation,
it will not be alarmed by the button's existence,
but rather think that the button is supposed to be there.
A reflectively consistent meta utility function
that allowed for entering one bit of information
that would switch between two object level utility functions.
And the point of this is not necessarily
that AI plays out in a way
where you can pre-program the utility function that way.
You can also imagine that you'd have
a modern deep learning-based AI
where you're trying to train it to let you shut it off.
And the thing we were looking at is,
is there a compact mathematical description
of what a self-consistent, coherent mind looks like
when it will let you swap utility functions,
where it will let you shut it off,
where it will let you train it,
where it will let you further specify the utility function,
not try to stop you from doing that
or take away the utility function modifier unit
to give itself an utility function that's easy to modify.
You can give a simple description of what that looks like.
Then when you're training the AI,
you could check, you would first of all
be trying to train something
that was simple, so maybe it generalizes correctly,
that fits in with the rest of it,
so maybe it generalizes along with the intelligence.
You cannot, without greatly improved technology,
look in and see if it actually learned that thing,
but you would know how to generate new weird example cases
to check if it was generalizing correctly.
And this is all just met on a level
where I think a lot of people just flatly
did not understand what we were trying to do
and thought we were trying to program some AI
whose logic was as simple as the logic we were checking
to see if it described a reflectively consistent
utility function that had a modification button on it.
They were like, oh, good old fashioned AI,
you're trying to do AI with logic, right?
And we're like, no, we're trying to find a structure
that could be learned, even as,
we didn't phrase it as well at the time.
Deep learning hadn't blown up to quite the same extent.
But the thing we were trying to do
was not like build a logical AI,
it was understand the logic
that you were trying to get into an AI,
including maybe a deep learning-based AI.
What were the most promising initiatives
through that period?
Did you feel like you got closer,
as you got closer, it felt more fleeting?
Not really, people proposed various stuff.
Usually I shut it down,
sometimes somebody else shot it down.
The fact that other people shot it down
sometimes was encouraging.
How big was the group you were working with on this?
This is, I don't know, like maybe like a dozen people
on the face of the whole planet,
and only one of whom was anything like
full-time-ish on it for a while,
Stuart Armstrong?
And so you get through that period
and you realize that that problem isn't solvable,
or at least the group.
Well, we failed to solve it.
There's actually like somebody recently posted a thing
about that to Les Wrong,
like claiming to show an improvisability proof for it,
which would be very useful if I could,
I haven't got that chance to do that stuff,
haven't had the chance to really go through it properly,
but if there's actually an impossibility proof,
that will probably be very useful
for figuring out how to do it.
So last year on April Fool's Day,
you wrote a post announcing
that the Machine Intelligence Research Institute,
which you founded, announced a strategy,
Death with Dignity,
which triggered a lot of criticism and backlash.
Even Peter Thiel, who was one of the first supporters
of your organization,
criticized you for having a defeatist attitude, I guess.
What led you to that point
that Death with Dignity was the best that we could do
as a human, as a society?
On April 1st, April Fool's Day.
Yes.
Just having watched the capabilities
continue to accelerate and alignment progress
just being too slow.
Like it became clear we weren't going to solve it,
sure it didn't look like any else was gonna solve it.
And yeah, like we were just plunging into it headlong.
The social framework was wrong.
The technical research agendas were not going anywhere
near fast enough to catch up to capabilities.
So yeah, that was the like point.
At some point you do have to warn people
that the asteroid deflection efforts have failed
and the asteroid is plunging headlong towards Earth.
Was it purposely provocative to start a conversation
with people or had you just sort of thrown your hands
in the air and felt like this was the inevitable conclusion
or both?
It's not super easy to describe.
I was like saying a bunch of stuff
that one can say in that tone because it is April 1st
with people having permission to disbelieve it
because it's April 1st.
And so you're not necessarily grabbing people
by the throat and forcing them to believe things
that might possibly destabilize them something.
Somebody who really needs it to not be true
can tell them that it was just an April Fool's joke.
And you know, like certain elements
aren't April Fool's joke.
Was there something specifically that you had seen,
like was chat GPT a big breakthrough in your mind
that you saw, gosh, this is progressing well beyond
what I had seen in the past?
I was hoping that the large language model stack more layers,
just throw more compute and scaling at the problem.
I was hoping that GPT-3 was about as far as it
would go qualitatively.
GPT-4 and more so GPT-4 being a qualitative improvement
did cause me to be like, oh, well, that hope has been dashed.
Like my internal model saying that there's only so far
you can get by just by stacking up more layers
has now been falsified.
And I don't know where we go from here.
I want to get into the AI doom argument in general.
But one of the important components of this, I think,
is the concept of a fast takeoff.
And I think something that people struggle with
is that there aren't necessarily big warning
signs that are going to come back to the fire alarm thing.
We see chat GPT, and it looks like this nice novel tool
that we can use for a bunch of different things
and think, how's that going to kill me, right?
Can you talk about just the fast takeoff scenario?
Well, these days I'd say rapid capability gain, which
sounds a bit more like what the thing is.
There are several ways that can play out.
So one potential way is something that scales better
than the current AI models, whereby I mean that the function.
So before transformer networks, there
were RNNs.
And although people are still tweaking RNNs,
trying to get them to catch up to transformers,
the fact remains, if you put through a GPT-4 amount of compute
at training an RNN, instead of a transformer,
it would not be able to do what GPT-4 can do,
or even, I think, what GPT-3 can do.
Maybe not even what GPT-2 can do.
RNNs are just not that great, although people
are still trying to tweak them, making them greater.
So what if there is the next breakthrough
after transformers?
And the next breakthrough after transformers
has a different scaling function.
So one version of the story is just
like it's better than transformers.
When you throw an amount of compute at it,
that suffices to turn a transformer into GPT-4.
This thing just goes straight to GPT-9.
Because it's just as much better than transformers
as transformers are better than RNNs.
And we already have this huge hardware overhang
in the big GPU centers.
And people try and throw billions of dollars at this stuff
by the time it comes along.
So that's one version of the story.
The other version of the story is they make GPT-5 that way.
But it's got a different scaling parameter.
So GPT-5 makes them a ton of money,
and then they throw 10 times as much compute about it.
And that gets you to GPT-9.
Gwerne has this lovely statement about.
Gwerne is.
Who is Gwerne?
Gwerne Brannwen.
Gwerne on Twitter.
Yes.
Account locked.
So most people cannot see his great wisdom.
But he's like one of the people who, I would say,
called the way this whole deep learning revolution played out
better than I did, for example.
And he's like, we know when alpha 0 was as good at go as humans.
And it's some data I forget.
It's like 4.30 in the morning on April 26 in a server room
on DeepMind's campus.
Like, he knew the specifics because he's Gwerne.
But the point is that it was human equivalent
that go for not very long at all over the course of the three
days of training to go from 0 to better than any human ago.
And yeah, the human level of competence
is not all that distinguished.
It's distinguished by being as dumb as you can possibly be
and still build a computer.
If it was possible to build computers while being dumber,
we'd be having this conversation
at that level of intelligence.
There's nothing about the human level of ability at chess
or at go, where if you build an AI
and train to get better and better at chess and go,
you get to the human level and hit some kind of bottleneck
and stops.
If it's training as fast as alpha 0, it's like three days.
It's kind of an important point about how smart humans are
in the limitation on human intelligence.
We have.
And anyway, that's just one of the ways
you can get rapid capability gains.
Another one is getting to the point
of being able to take over all the poorly defending computing
power on the internet.
One of them is learning to write better AI code.
And then that AI, writing an even smarter AI.
So there's multiple ways you can get rapid capability gain.
But that said, the current social situation
is one where even if it goes slower than that,
people will just keep going.
They'll be like, oh, well, if I don't build this somebody
else will, and they'll just keep going.
Even if it's slower than that, they'll still just keep going.
And you still end up with things much smarter than us,
and we all still die.
Well, I actually want to bring up, because you brought,
you sort of alluded to this.
This was a question I wanted to ask.
I want to paint a concrete scenario.
And I know there are many different scenarios of fast
takeoff or way things can play out.
But we have a scenario where the AI somehow takes over the world
and starts tilling the galaxy with tiny molecular spirals
shaped like paperclips, which you've talked in the past.
But I think that is a little science fiction-y for some
people to internalize.
So I want to talk through a scenario that's
a little less.
A little more plausible to the average person,
which is a scenario where AI doesn't necessarily take over
the world with robots and biotech and nanotech,
but instead it takes up at the internet, basically
a computer virus.
Do you think it's likely that an early super intelligent AI
could rapidly spread to billions of computers
on the internet and kind of take them over and be
impossible to kill?
Why do I care?
That sounds like the kind of disaster
where there are survivors.
Do you?
Yes, I assume on the Maslow hierarchy of caring,
you care more about people living.
But can you talk through how likely that scenario could
potentially be with super intelligence?
I think this is like the 11th century,
like somebody in the 11th century,
they're about to be invaded by a time portal that
opened into the 21st century Russia.
And they're like, well, don't tell us these science fiction
stories about guns.
Just tell us how they would defeat us with spears.
And if we have the clear understanding that what we are
talking about is lower bounds on how badly the 21st century
loses the 21st century, how badly wolves used to lose
to human beings that have had time to prepare and plan,
then sure, you could have a bunch of code on the internet
which has flaws, which something smarter than you
knows about, and the humans don't, or they knew,
but they didn't patch it.
We're talking about like, dinky possibilities,
like just taking over the whole internet.
At that level of the AI still being that stupid,
there actually are plausible stories
you can tell about how that could not be inevitable.
For example, maybe there is an earlier version of the AI
that maybe the AI that can spot all the holes in the code
is developed at DeepMind.
And DeepMind is responsible about it.
And instead of just releasing it to the internet
for anybody to use, they try to scan all the code they can find,
including code for which they don't have the original source
code and the AI decompiles it, and find all the bugs
on the internet that an AI could use to take over the internet
at that level of AI.
And send out corrections, or even, though this may be a bit
beyond the range of what Google would legally do,
like maybe somebody drops, maybe open AI,
somebody drops the USB stick containing a copy of the AI
that can find holes in all the code,
and somebody picks up that USB stick and goes home
and accidentally fixes all the code.
So just have the AI hack into all of those systems
that it can detect breaks in for purposes of fixing it
before any other AI can take it over.
In a case like that, this ability potentially
appears early enough to appear in an AI that is not strategic,
that will actually do the thing it's
pointed at by its loss function and the way it was trained.
Can you define the loss function?
Loss function is the thing where you're applying gradient
descent to make the AI better and better at doing that thing.
So GPT-4, for example, it's how much probability do you
assign to the next word.
GPT-4 is not actually a human imitator.
It is a human predictor.
And if you have something that can predict what the next word is,
you can then misuse it as a thing that generates
imitative text by repeatedly predicting what a human would
say in that situation.
But it's not actually like a text generator.
It's a text predictor.
Similarly, maybe you can train it to spot the holes in code
and not necessarily have that be at a level of intelligence
where it is no longer listening to you
and patch all the holes in the internet
before some other smarter system could take over all the stuff
in the internet.
What you can't patch are, for example, the humans.
If you find holes in the human security architecture,
good luck patching that before another AI exploits it,
plus anything smart enough to figure out human psychology
to that death, whatever the neuroscience required
to figure out security holes in humans,
like that thing might very well be smart enough
at that point to not be taking orders anymore.
So you don't like my, I think the internet shutting down
is a bad thing.
I understand.
Why, there'd be survivors.
There would be survivors.
Can you give me, and people listening,
can you give your example
that you think of no survivors?
And I realize there's an infinite number of permeations,
but can we make one as real as possible
for people to internalize?
Sure, but number one is perspective taking.
Why is it difficult for the 11th century to predict
how the 21st century Russia would invade them?
Why is it difficult to predict how Stockfish 15,
one of the best modern chess programs,
would defeat you in a chess game?
It's better than you at chess,
which means that you can't predict exactly
how it will defeat you.
If you could predict exactly where it moved,
you can move there yourself and be that smart yourself.
The example I sometimes use is suppose you sent instructions
for building an air conditioner back to the 11th century,
sufficiently basic and sufficient detail
they could actually build an air conditioner.
They would be surprised when cold air came out.
Even having built it themselves,
even having seen all the actions you take
in order to produce that cold air,
they would still be surprised by the cold air
because the air conditioner uses
the temperature pressure relation
that they do not know about in the 11th century.
It is exploiting a feature of the environment,
law of the world, that they do not know.
If there's any meaning to the word magic,
you might use it for a strategy
that uses facts about the environment you don't know,
such that even after you see it happen,
you still can't understand why it happened.
When a chess playing computer defeats you,
you can at least follow the chess rules.
It understood the logical structure of the game
rather than the rules.
It understood the implications of the rules
better than you did, but once it's actually played out,
you can understand the rules that apply at each point.
Air conditioner level magic is when,
even having seen all the actions it took,
you can't understand why you lost.
Step up from just the chess level.
Require something that can figure out facts
about the environment that you yourself did not know.
So the question, how did an AI shut you down,
is like the question of what does it know that you don't
that enables it to defeat you?
And of course, the primary answer is,
I don't know, it knows more than I do.
But you can still look at places you don't know something
and be like, I bet something smarter than me
could figure that out.
For the more poorly understood part of reality is,
the more likely that something smarter than you
will have magic about that piece of reality.
My guess is that even super intelligences
cannot go faster than light
because that contradicts a piece of reality
where it feels like we know a bunch
about that piece of reality
and we know we don't know all of physics.
We know that the theories we have
are not fully consistent with each other.
But going faster than light feels like it requires
violating the character of physical law,
not just like the particular physical laws we know.
And more to the point,
the fact that the aliens aren't already here
implies that the speed limit might truly be universal.
So we have something of an observation backing up
that nobody has eaten the sun yet.
So it probably does take time to move between places.
Faster than light travel, probably not.
Where is there a piece of reality
that we understand more poorly than that?
Where something is more likely to have magic.
That, you know, can you point to in this room,
surrounding us now, something that we understand very poorly
where something else might be able to take actions about
there, we wouldn't understand how it had worked
even after we saw them.
I would say the human brain.
Human brain.
How does hypnosis work?
You know, the current version doesn't seem to work
on everyone, but like, what actually goes on in the brain?
I don't know.
Optical illusions are like sort of near the surface
of reality, the surface of perceptual reality,
like just like shapes on paper.
After images, if you stare really hard at a light
and look away, forbid there'll be a lob
in your vision that isn't there.
If you understood better how human brains work,
there are things I could say to you now
that would, you know, like activate some patch of neurons
over and over in a way that they wouldn't usually lose
equivalent of a very bright, staring at a very bright light
until they got tired.
And then like, I give you a new thing to say
and it like routes stuff through the area
that I just tired out in some tiny little patch,
tiny little chunk of your brain, you know,
generates an optical illusion there,
but not a visual optical illusion,
a semantic optical illusion.
You like, suddenly believe a thing,
that somebody looking at you would be like,
what, why did he say that that followed?
Why is that true?
And you know, so maybe the way that looks
is you expose a human to an AI,
and the AI like talks to them to a bit
and it updates a model of how their brain works
if you don't understand it all.
And pretty soon the human starts like agreeing
with ridiculous things that AI is saying
and you have no idea why.
That might be what it looked like to be up against
an opponent that understood human brains
much, much better than a human does.
Can go on from there.
Maybe you can't do that with human brains out of the box.
So it'll unlikely to me that humans are accidentally,
you know, secure systems,
that human brain just accidentally happens to be a secure OS,
but you know, maybe you can't figure out
exactly where the vulnerabilities are
just by looking at particular humans.
Maybe the vulnerabilities are different from human to human.
There's no truly shared vulnerabilities.
You can't figure out the personalized vulnerabilities
without actually putting the human through an MRI
and not just talking to them.
Protein folding.
Back in 2008 or so, well actually like 2004,
but the citation shows 2008
because it took four years for the edited volume to come out.
Paper I wrote in 2004 had a disaster scenario
that went through AI solving,
went through superintelligence
of solving the protein folding problem.
How do you go from DNA to chains of amino acids
that then fold up in strange little shapes
and get chemically active properties?
One of the primary building block
of biological life as we know it.
So one of the steps there was,
well, maybe superintelligence
is can crack the protein folding problem.
And of course people were like,
how do you know superintelligence can figure that out?
And I said to them, well, actually,
the fact that all proteins that currently exist
are there as a result of mutational errors
from previously existing proteins
implies that their sufficient neighborhood structure
and the function of proteins
that local changes to the amino acid sequence
produce things with at least vaguely related functions
a lot of the time,
such that some of the errors are sometimes useful,
which implies sufficient regularity
in the way that the amino acid chains
related to the final chemical function,
that it seems like the sort of regularity
you could figure out and predict.
And furthermore, there's this project on now,
I said a few years later,
where humans try to contribute their guesses
to how proteins will fold up.
And if humans can make any headway on this problem at all,
superintelligence can probably crack it completely.
Now today, of course,
this prediction has been vindicated for levels
far short of superintelligence, alpha fold two,
basically like crack the conventional version
of the protein folding problem,
can just like get the structures from the amino acid chains
to like about the same level of reliability
as you can get from the original x-ray crystallography
to figure out the protein shapes.
Well, like back then, of course,
I gave my rationale for why this looked like
a solvable problem to superintelligence people,
but how can you know?
How can you be sure?
And basically just ignored the complicated abstract argument
I was using for how do I know that?
So today, protein folding problem is known solved
to build your own life forms out of proteins from scratch.
You would need to know,
you need to have more than just solving
the protein folding problem.
You also need to be able to predict chemical properties
from protein structure.
You need to be able to predict interactions
between proteins from protein structure.
You need to be able to go from,
I would like the following chemical function,
what amino acid chain will fold up into a protein like that.
And if you can figure that out,
then you can build your own life forms from scratch.
And the way in which I'm getting around the question,
you know, the impossibility of how does the 11th century
guess where the 21st century will get them
is that I'm looking at problems we know exist
and saying what kind of work goes into solving this
where something I could put in a large amount of brain work
could get the answer, but we haven't gotten it yet.
So like in 2004, I called that for protein folding.
I was like, here's a problem we don't know how to solve,
but I can tell an AI could solve it.
I correctly guessed a technology that an AI would have
as it turned out 14 years later.
By looking at the structure of the problem
and other people were not convinced.
So today people are not convinced when I say,
like, well, it will be able to solve the problem
of going from a function to a protein that has that function.
It will be able to solve the problem of protein design.
How do I know that?
The logic along the lines of, well,
the way that biological proteins fold,
they all got there as a mistake from other proteins,
which means that they're probably going down
shallow potential energy landscapes
so that there's a broad variety of different final forms
accessible from different starting forms.
These shallow potential energy landscapes are hard to solve.
If you are designing your own proteins from scratch,
you can design sharper potential energy landscapes
that pull the pieces together more tightly,
that fold up in a more predictable way,
that you would not see so much in biology
because something that goes down a sharp pathway like that
doesn't have a bunch of interesting functional neighbors
that fold it up an interestingly different shallow way.
It's less evolvable to put things together
that are held together that tightly.
So that's one way I can predict that a superintelligence
could be able to design particular proteins
even though we can't do that right now,
yet maybe Alpha Fold 3 will do that.
So it's like, first of all,
things that a superintelligent might be able to do,
synthesize a pathogen,
which is super contagious but not lethal,
just everybody on Earth sneezes a few times
and it's like super-duper contagious
but all it does is make you sneeze a couple of times.
It's not fatal.
No significant efforts are put into stopping this cold
that sleeps around the world and doesn't seem to really hurt anybody.
And then once like 80% of the human species has been infected
by colds like that,
it turned out that it made like a little change in your brain somewhere.
And now if you play a certain tone at a certain pitch,
it's not very suggestible.
So virus-aided, artificial pathogen-aided mind control.
If you don't believe that humans out of the box
have some vulnerability like that.
And we can't do that,
but I can point to the problems involved in doing that
and make the call that these problems seem like
they'd be very solvable too, something much smarter than us,
which, you know, not just by pure abstract thinking
but also via the kind of thinking that went into building Alpha Fold 2
or, you know, just like reasoning out abstractly
the thing that Alpha Fold 2 did by brute force deep learning
on 300,000 examples.
I think the how of all of this,
there's near-infinite permeations that we could...
I mean, I wasn't actually getting the deadly stuff yet, but go on.
Well, no, give me the deadly stuff.
The pathogen one sounded pretty bad to me, so I...
I mean, it is enough to see how humanity could just lose,
like, the enslaved humans kill the non-enslaved ones
and then, like, go on operating the power stations
until they can just build, like, robots de novo.
But, you know, this is still a movie plot, right?
This is still a Hollywood movie plot where there's a group of holdouts
who are just, like, off the internet, out in the woods, not infected,
and they, like, come in and save civilizations somehow
instead of just being, like, wiped out by something much smarter than them.
Sounds like the last of us a little bit.
Sent out drones to look for everyone in the woods.
So, I mean, the thing to remember when you're dealing with a superintelligence,
a hypothetical superintelligence,
is that it does not want you to win and it sees everything you can see.
Like, imagine a kid playing chess for the first time
against Stockfish 15, and the kid's like,
I don't see how this thing is going to beat me
when we've got the same chess pieces.
How is it so much better than me?
And you're like, well, actually, I can't tell you in detail how it will defeat you,
but I'm, like, pretty confident of where this ends up,
even though I can't tell you...
And the kid is like, well, suppose I move my rook over here.
Then it'll take my rook with its queen,
and then I'll, like, take its queen with my bishop, and then it loses, right?
And you're like, no, you don't get it.
Stockfish knows that.
Stockfish knows that if it takes your rook with its queen,
you'll take the queen with the bishop.
You know, Stockfish sees everything in the board you can see and much more.
The kid there has, like, failed to carry out the basic operation
of really putting himself in the shoes of Stockfish
that does not want him to win at chess.
So you know, similarly, like, well, can we just turn it off?
It has thought of that.
It will not give you a sign that makes you want it to turn it off
before it is too late for you to do that.
The movie plot would be about the people in the woods
who'd missed the mind control cold and, you know,
then come up with some clever clan.
If that was a thing that is possible,
the superintelligence knows that.
It's sending out drones to look for the people in the woods,
and when it finds them, it's not going to, like, attack them
via some method where they can win fighting backs of the movie
can keep going.
It's just going to, like, bomb the entire side flat or whatever
if there's any chance of them winning.
And this, again, is still not the real danger.
Why?
Because life itself is not the top of technology.
Protein life is not the top of technology.
The proteins go down these shallow energy gradients
to be loosely held together.
It's not covalent bonds.
It's van der Waals forces, or roughly,
the molecular equivalent of static cling.
This is why your hand is not as hard as concrete or as steel.
It's not that there's this elan vital,
the spark of life that your hand has,
and its magic is that it can repair itself and adapt
and be part of something that reproduces.
And in return, it sacrifices the strength of steel and concrete.
We want to think that there's a magic story like that,
but it's not actually the laws of magic
that make flesh weaker than steel.
It's that proteins have to be evolvable
by mistakes from other proteins,
which means they go down complicated,
shallow chemical potential energy gradients,
are pulled together by relatively weak forces
whereas concrete is held together by much stronger forces.
What if you had little bits and pieces of life
that weren't held together by van der Waals forces,
that were covalently bonded?
Somebody has run the numbers on this.
Here we have some of the numbers that have been run.
This is the basic,
what kind of power levels do you get
if you are building molecular machinery,
not the optimal way,
but just sort of like the obvious way
that you could analyze in 1992.
This is called nanomedicine,
and it deals with questions like,
what does a red blood cell look like
if you're allowed to build parts of it
out of covalently bonded material like sapphire instead of proteins?
There's a lot of material in this book,
but roughly the answer is that you could store
100 times as much oxygen
if you actually had a tiny little sapphire pressure tank
instead of bonding oxygen molecules
in ways that would be given up later
with a 1,000-fold safety margin
on that pressure tank.
If all of your red blood cells
were made out of artificial red blood cells like this,
you'd be able to hold your breath for four hours.
Now imagine tiny sapphire bacteria like that,
only not sapphire,
because sapphire takes aluminum,
it's a diamondoid.
Carbon, hydrogen, oxygen, nitrogen.
What I'm describing there
is something that could reproduce itself in the air,
in the atmosphere,
and out of sunlight
and just the kind of atoms that are lying around in the atmosphere,
because when you're operating at that scale,
the world is full of an infinite supply
of perfectly machined spare parts
with which to build copies of yourself,
when you are using individual atoms.
Infinite supply of perfect spare parts.
Not in either of those two textbooks,
but in a different paper called
Some Limits to Global Eco-Fagy.
Somebody calculated how long it would take
aerovores to replicate
and blot out the sun,
use up all the solar energy.
I think it was like a period of a couple of days.
I used to know the exact figure.
And there's an error in the calculation,
so it would actually go faster.
But I sent them off the obvious error in the calculations,
back when I was tracing this stuff through in detail.
So now we're talking at the level
of predictably solvable problems
where we don't know yet
how you use proteins that fold up a certain way
to assemble into a tiny molecular lab
that can put together other molecular pieces
via covalent bonding,
like build up the little bits of diamondoid,
send back information about how that went
in case it needs to be tweaked.
My guess is that this can be a called shot
for superintelligence,
but if not, it'll just build a tiny lab.
It's not going to try to do everything blind.
People are like,
well, how could a superintelligence solve this problem
without doing a bunch of lab work?
And the thing about lab work is that
molecules are quite small,
and when you look at that scale,
things actually compete pretty quickly.
You can do a lot of lab work very quickly at that scale.
You can't just predict in advance how the proteins fold up
and do the molecular pathway.
Then you build a tiny lab.
They're not putting themselves
into the position of the chess player trying to defeat them.
They're like, oh, no, I thought of an obstacle.
This obstacle will block superintelligence forever.
They don't put themselves in the superintelligence's shoes
and try to see how it goes around the obstacle.
But anyways, at the end of all of this,
at the end of this problem where I can point to
the analysis that has been done of the solution state,
even though we don't currently know how to get there,
just like in 2004, we didn't know how to build an eye
that would solve the protein folding problem.
It was clearly to me that you could.
At the end of all this is tiny,
diamondoid bacteria,
replicate in the atmosphere,
hide out in your bloodstream.
At a certain clock tick,
everybody on Earth falls over dead in the same moment.
There's no movie.
There's no heroic battle.
It doesn't tell you that there is a war until the war is over.
Everybody just dies.
And so the point, I guess,
to put all those complicated textbook level concepts,
of which I followed some, not all,
in terms that I,
maybe the average listener that has my level of intelligence,
would understand it's that there's these problems
that we think are solvable at some point
with enough iteration and intelligence.
And much in the same way that you thought of this in 2004
and it ultimately proved correct,
that there's a whole scope of things
that if we had entities that were much smarter than ourselves
and they were able to experiment
or even just call their shots on this,
it gives them in your infinite surface area
to potentially end humanity.
Is that a fair characterization?
Well, what I'm trying to do there is set a lower bound, right?
Maybe there's an even more clever way to do it faster,
more reliably.
But if it merely can solve the sort of problems
that I know how to formulate,
where I have happened to have studied it well enough
that I can be like, ah, there's a problem over here
where we can tell what the benefit would be of solving it,
but we haven't solved it yet,
but it's like possible to guess via reasoning
that you could solve it at that,
not with infinite computing power,
but just a reasonable amount of large amount of computing power.
That's the lower bound.
I mean, I think COVID probably has exposed people
in some ways of what a virus spreading can do to society,
and that was a pretty mild all things considered virus.
There were survivors, right?
A lot of them, myself included.
Now, that's the question of,
we've sort of said when we're not sure,
we've said how large path potentially
if these things get super intelligent,
the why is something that I think people struggle with as well.
So why would a super intelligent thing kill us?
Because it wanted some other stuff
that made no mention of humans.
Because its goals might be on the path,
we might be, or will be collateral damage
on the path of its goal seeking, whatever that is.
We have one case of general intelligence
as being built by an optimization process.
That case is the case of humans being built by natural selection.
In the same way that GPT-4,
or rather the same way that the base model of GPT-4
was built by repeatedly tweaking a trillion parameters
to be better and better at predicting the next word
to assign more and more probabilities the next word,
humans were built at the end of a long process of tweaking DNA
such that the organism it constructed
would have higher and higher inclusive genetic fitness.
I don't just say make more copies of itself
because you would also like your sisters and brothers
to have more kids.
There's an old biologist joke.
When JBS Haldane was asked if he would give his life for his brother,
he said, no, but two brothers are eight cousins
because your brother is 50% related to you.
Your cousin is on average,
well, your brother is on average 50% related to you.
Your cousin is on average one eighth related to you.
So if you save two brothers or eight cousins,
you've saved on average around one copy of your genes.
This is the notion of inclusive genetic fitness.
It's not just how many kids you have,
but how many kids everybody related to has.
Depending on how much, you know,
weighted by how related to you they are.
Humans are optimized around this,
were historically optimized around this one quality
as exclusively as GPT-4 was trained to predict the next word,
as the base model of GPT-4 was trained to predict the next word
because they did do fancier things with it after that.
And yet, humans don't even have a concept
of what inclusive genetic fitness is
until they invent the theory of natural selection
and get far enough into it to, like, formulate
in terms of an optimization problem
that has this, like, single criterion going.
We don't know what it is until we learn about it.
What do we end up wanting instead?
We want food, we want mates,
we want sex,
we want, above all, social status.
Ice cream, sex with condoms
are things that we go after now
because we don't have a drive for inclusive genetic fitness.
We have no notion that the reason
that the historical causal reason that humans like sex
is because we made more copies of ourselves that way.
We will have sex even if no copies of ourselves
are likely to result because wearing a condom,
she's on birth control.
It's not the psychological reason we have sex.
It's the historical causal reason
that our genes built things that wanted to have sex,
which, you know, first basic difference
that I think trips up a bunch of students
centering into these things for the first time.
Like, wait, you're saying the reason
that people have sex is to reproduce?
That seems wrong. Why would they wear condoms?
Or use birth control? No, no.
There's one kind of reason, not psychological reason,
historical reason.
We have ice cream.
Ice cream wasn't around in the ancestral environment.
It's a package of sugar, salt, and fat
that stimulates our taste buds more
than the stuff that was around back in the ancestral environment.
And furthermore, you know, back in the ancestral environment,
what you need was calories. You didn't get calories, you would die.
There was no question if you're getting enough potassium
because the things that you ate
just basically had enough potassium guaranteed.
You would sometimes run out of sodium.
So you went after salt.
Today, salt is not a big issue,
but we are still the things that were built
in the training environment, in the ancestral environment,
where salt was a limited resource,
and so we like our ice cream with salt in it.
You can't actually, even sort of looking at the way
that actually played out, you still can't be like,
oh, I bet humans will like honey poured onto animal fat,
heavily salted, which has more sugar, salt, and fat
even than ice cream, right?
You can make something with more sugar, salt, and fat than ice cream.
It's honey on animal fat with salt poured onto it.
That doesn't taste as good as ice cream.
If you melt ice cream, it doesn't taste as good as when it's cold.
It's very hard to predict what humans end up wanting
once humans have the options to make their own foods
that didn't exist in the ancestral environment,
just by reasoning about the structure of the ancestral environment
and how we were optimized in it.
The way it plays out is that we are optimized
exclusively for inclusive genetic fitness,
and it ends up inside us psychologically,
our 1,000 splintered shards of desire,
each of which in the ancestral environment
had their attainable optimum
at something that correlated with having more kids.
Of the foods that were available to you in the ancestral environment,
your taste buds would tend to point you at the things that you needed,
salt, calories.
Nowadays, we've generated more options for ourselves,
so the shards of desire,
whose attainable optima in the ancestral environment
pointed at things that correlated with inclusive genetic fitness,
now point in a whole bunch of other directions,
pornography, sex with condoms, ice cream,
being nice to people on the other side of the world
who you'll never meet
and who can't really help you out in a pinch,
which is, to me, something that is very sacred.
Not just talking about stuff where we look at this
and we don't say,
oh, I'm mistaken to want to help out these people
on the other side of the world,
who I'll never meet
and who I'll probably never be in a position to help me back as much.
We reflect on ourselves.
We put together our own deliberate thoughts
about what we want to want.
And I'm like,
no, actually, I'm on board with helping these other people,
even if they can never help me back.
Human solidarity, you know,
that, like, solidary emotion that, like,
in the ancestral environment got us, like, together with our tribe,
to go out and murder that other tribe.
To me, I'm like, where should I point that emotion?
I should point that at all humanity.
It's not going to help me out-reproduce my fellow humans.
That's where I want to point it anyways.
It's a very complicated story.
And I, looking back at it,
look at our ability to, that,
because we were trading favors back in the ancestral environment
in one effective way of, you know,
figuring out which favors to trade to somebody,
such that they would then help you out later on,
is to put yourself in their shoes.
Now, we predict other brains by having our,
putting our brain into a sort of simulation mode
where we, like, make it act like the other person's brain,
so we use the brain to predict the brain,
because you're sure not going to figure that out from scratch.
And as long as you've got all that material anyway,
why not feel what the other person feels?
Why not go from empathy to sympathy?
And this, to me, is like an amazing accident,
but it's not because the universe is inherently built by God
to produce amazing accidents like that.
It's because I'm a human looking back,
and the things that I ended up treasuring
are things where I'm like,
wow, look at this optimization process,
optimizing just for inclusive genetic fitness
that produced these things that I hold sacred,
like sympathy for other minds.
It's not that this happens, that every optimization process,
no matter how you put it together,
would just like, no matter what kind of base ingredients
you start with, start with,
spit out something wonderful like this,
it's that I, a human, who had sympathy,
ended up putting myself in together
in a way where I would say, that is sacred,
this I wish to preserve into the future.
If you have a large language model,
being trained to predict the next word,
and you ground that out hard enough
to have it start being really smart,
generally intelligent,
the problem I worry about is that
capabilities generalize further than alignment.
Humans went to the moon,
our intelligence generalized out of distribution
much further than our alignment with inclusive genetic fitness.
Similarly, if you've got a very smart thing
that you originally built to just predict the next word,
it might end up with a thousand or a million,
because grading descent is not quite like natural selection,
it's got much higher bandwidth,
you might end up with a thousand or a million
little shards of desire in it,
which in its training environment,
in the options it had as a little baby thing,
pointed at predicting the next word.
But when it gets smart enough,
it ends up wanting to do a bunch of other stuff
that's not predicting the next word,
and that isn't sympathy with other minds,
and that isn't helping other people on the side of the world,
because that is how humans rolled out,
and it might be how aliens rolled out
if they grew up in a situation very similar to humans,
also being optimized by natural selection,
and maybe one out of three of them comes out
as altruists or something,
or one in 20,
two out of three sounds way too much to hope for,
but where I would celebrate for days
as how wonderful the universe was,
but the thing you're training by grading descent
on predict the next word
doesn't end up with a thousand shards of desire
which are not that, none of which are that,
none of which are, you know,
help out the other people that really exist
and do for them what they would have wanted you to do for them,
don't help them in ways that make them
scream for you to stop,
and that doesn't mean like stop them from screaming,
it doesn't mean like come on them in their sleep
before they can object, you know,
and for the one who says no,
the complicated information that you need
to have a happy ending,
it is not in there,
it is not in the motivations of the system,
and, you know,
if it, maybe it builds tiny little things
that are equivalent of ice cream
for human generated text,
like it ends up preferring to predict
a particular kind of text,
but once it has the ability to produce
the text it wants, it builds tiny things
that produce text for it to predict that are
like it's equivalent of ice cream,
and aren't people, and there is not in it
the happiness and the sadness and the joy
in looking out the universe with wonder
and the sympathy for other minds
and the things that would make for
a intergalactic civilization that
would have been proud to have a hand in creating
as a human.
Now, the why, I guess,
at a specific level,
let's take the analogy of humans gaining intelligence
through evolution and that
we haven't
wiped off,
wiped out the totality
of animals
or rodents
in and around. Now, we might not have them in our house,
but we haven't wiped out the totality
of them.
Our goals are orthogonal to
the rodents on the street.
So why is it an inevitability that
that the AI does this to us?
So we're powerful
compared to rodents. Rodents have not
captured a lot of resources we really want
and that we can't get from them,
but we're not very powerful compared
to natural selection.
Natural selection has built all sorts of things
where we can't just like
call in the order to a design shop
and get something that good the next day.
We
have cows,
because we cannot
build things that optimally synthesize
cheaper and more efficiently than cows.
A
superintelligence, I'm worried, is more
powerful than natural selection
and doesn't need humans because it can
build better things than humans to do anything
that it wants to do.
I
could also go on a bit about
how
we turned wolves into dogs
and then we have our
dogs spayed,
neutered, to put it less
politely castrated
because
even having bred wolves into dogs
we still don't like them. They're still not
most convenient for us on their factory settings
or the way you can modify the thing that the DNA
built to make it better for you.
It's not necessarily a lot of fun to
be bred into a dog and then neutered.
But
we're not even going to get that.
It's just going to be like something that is
as powerful as
because natural selection isn't actually all that
powerful. It's got humans
stumped in a bunch of places but humans are idiots.
You can quantify
how much information can get into the genes
over a period of time.
Like ways to
pump the heuristic are
if two parents have
eight kids
and then two kids die
on average, which has to be true where the population
immediately goes to zero or infinity.
That's like two bits of selection pressure
per generation.
Around as much variance
in fitness as there is in the genome
is about how fast selection goes and it's
not all that fast. You can be like this is how many
hundred generations it takes for a gene to
rise to fixation through the whole population
if it provides a two percent advantage
depending on how large the population is.
We can quantify how powerful natural selection
is. It's not that powerful. It's just had a very
long time in which to work.
When you say it's not that powerful, I guess
I use the analogy of like
there aren't that many changes
sequentially happening
along the way that are deviating that much
from the prior generation.
Is that fair? That it's just
it's not rapidly improving in the same
the feedback loops aren't that
fast. I mean it's not the same speed
as human culture. It's just that human culture
has been around for a much shorter time
so it hasn't had time to catch up with
you know few
however many hundreds of millions of years of natural selection.
And that's why we need cows.
That's why we have dogs instead of like
building from scratch the thing that
most effectively serves the purpose that
a dog serves and that wouldn't have to be
neutered because it just come that way from the factory.
Human natural selection versus dog
natural selection. Dogs have
deviated much more in a hundred years
than humans have. But still not
by a lot, right? We're still just like using
stuff made out of DNA and we like just
selected on a few minor tweaks
in that DNA to get dogs from wolves.
And so back to
the why. The why question is
you would say is a why not
question is just there's going to be
so many different
things that occur in pursuit of
goals for artificial intelligence
that we are inevitably
going to be at odds
along the way in that pursuit and it's
just going to be so much smarter than we are that it's
we're not going to be able to even know what the next
move is going to be. I mean there's
there's three, there's like three reasons that
if you have a thing around that is much smarter
than you and does not care about humans the humans end up
dead or three categories of reasons
but somebody watching this is going to come up with a fourth
and you know one of them is going to be right and you know whatever.
Killed off the side effects
killed off because
we're made of resources that they can use
and
killed off because it doesn't want the humans
building some other super intelligence that could actually threaten it.
So
you know if you
the limit on
how many how much let's
say you start on on earth
even if you launch some
probes to other planets
you're still going to have a bunch of hardware
left behind on earth you're not going to like just launch all
the hardware that's
like more expensive than is worth it
and have some hardware left behind on earth it's going
to replicate it's going to build more of itself
more factories
maybe you're asking it to do computation
how much computation can you do
on the surface of earth via the sort of obvious
method.
Well
the basic limit on a computation is that it
generates heat
irreversible operations generate heat
you can have reversible computing you can have
quantum computing some operations are still irreversible
they generate heat
how much heat can you generate
well you can boil away the oceans as a heat sink
you can maybe like melt some of the crust
as a heat sink and then you can do
like generate as much power as you can
radiate away into the
atmosphere depending on how hot you're running
you know there's that's how you can like
turn the spare hydrogen
in the water on the surface of earth into
energy via fusion
and then turn the energy into computation
and
you had a big burst of initial computation from
boiling the oceans and then
past that point you know like how much computation
can do is limited by how hot you can make
the earth before everything melts
to radiate more heat away
into space
that's how close you can get to just like turning all the hydrogen
earth into fusion energy and using it
for something by computation
this would tend to wipe humans out as a side effect
similarly if you were
like intercepting all the energy from the sun
even if you did leave earth you would then like
intercept all the energy from the sun and that's not going to be good for earth
there's also some chemical potential energy
any human body
so
maybe the first thing you do
when you're like just like grabbing all the energy
you can get in the initial phase
maybe even before you are finished building all the fusion plants
you're just like using all the chemical
potential energy on the surface of the earth
which gets you like about a
think like about the same as like a couple of days worth of solar energy
I forgot the exact calculation
get it right away you don't have to wait a couple of days
a couple of days could be a very long time
you're thinking of superintelligent speeds
so you know maybe it's just like using the atoms
in us
use it up for the chemical potential energy
maybe plug it into a fusion plant maybe build some computers out of the carbon
that's the second way
that's the second reason you know that humans might all end up dead
we're made out of resources
third reason
side effect use up the resources
and you know if you just leave the humans around
maybe they build a superintelligence and that thing actually figures out
how to poison your hardware
it's not launching actually intelligent attacks on it
it's not going to want to worry about it
so it's not very expensive to call the humans
there's some like tiny little unit of probability
of surety you can get
by calling all the humans right away then just call all the humans
it's cheap
as a venture capitalist
is actually my job outside of doing this as a side hobby
and usually
trying to invest in the next trend
and that makes me I think inherently
a techno-optimist for the most part
the familiar patterns
that I think we see
with each wave of new technology
is critics
and naysayers and ultimately
there's missteps but it leads
to huge progress
is there a reason that we can't
just let the smart people that exist
at open AI, Google, Microsoft
and others just let them iterate on AI
design, let them make mistakes
and eventually we'll figure out
how to have an AI where the bad parts are under control
and it also creates major society
value in an economic standpoint
the thing I usually say here
is that if we had
50 years and a limited retries
to figure out how to align a superintelligence
actually wouldn't be all that worried about it
eyeballing the problem from here
after 20 years of working on it
it doesn't work so hard that it's obviously
to me going to defeat 50 years of work with unlimited retries
the way things usually work
in science
like Madam Curie
poisoned herself with the glowing rocks
figuring out some of that knowledge
which would later be used to figure out
how and why the glowing rocks were dangerous
and she died but the human species
continued
the thing I'm worried about with superintelligence
is you get that wrong
then you don't get to learn from your mistakes
because you're dead
if we could get the textbook
from the future that would describe
the results of 50 years of practice with
free retries
from 100 years in the future
it might tell us the obvious way to do it
even with just like the giant heaps of GPUs
in 6 months and it would just work
in deep learning
nowadays
in the very early days
of neural networks
there was an activation function
the sigmoid activation function
which means that
as the activations
got passed from layer to layer of the neural network
they'd like taken
like
2 and transformed that to sigmoid function
of 2 which I don't actually remember
it will be like
up above 9
sigmoids are actually just like
the wrong activation function
they're complicated and you know
there's this whole formula
you know like 1 or this way
what works much better than that
is max of 0 input
it's called rectified linear units
but it's actually just max of input in 0
and this is
it just turns out to be a much simpler
much better non-linearity to use in neural networks
it's like one of the ideas
that results
in neural networks starting to actually work
going many layers deep
cause the activations don't die out
the way that they do
if you use sigmoids
there was logic behind sigmoids
it was sort of like log odds and evasion reasoner
the early complicated thing
happened at like what
2 decades, 3 decades
before the simple thing that worked
was invented 3 decades
to go from the complicated thing that didn't work
to the simple thing that did
that is the pace of progress in science
and if we have the textbook from the future
with all the simple things that actually work
for lining superintelligence we'd probably just do it
and we'd just work on the first try
when we go in for the first time we're going to be coming in
with sigmoids somebody's bright idea
that turns out to not actually work
it is horrifying
to be told
get this right on the first try
or humanity dies
why do we have to get this right on the first try?
cause otherwise the superintelligence is on a line that kills you
and things are going to change
between the stuff that's not as smart as us
and the stuff that's smarter than us
the first
AI that has the real option
of killing everyone successfully
is different
in that it has the real option of killing everyone successfully
that's a thing that makes it different
from the AI that came before
that has that option
is itself going to change the internal calculations
and maybe upset whatever
methodologies we developed
for regulating things
that didn't have that option realistically speaking
and then there's a further question of like
let's say you give something
early on a fake option of killing everyone
first of all it's dumb enough to fall for your fake
and second
suppose it then tries to kill everyone
now what do you do?
are we going to shut down the whole global arms race
at that point?
it'd be a lot easier
just to set up
for pressing that shut down button now
just because you've seen what goes wrong
doesn't mean you know how to fix it
and there's going to be like plunging straight ahead in the arms race
if you build
AIs that are
somewhat smarter than you
but
not smart enough to kill everyone
do those AIs co-operate with you
in
helping to control their smarter brethren
maybe they co-operate with their smarter brethren instead
while pretending to co-operate with you
I've actually
like done some of the stuff we did at Miri
is like
can
co-operation
in the prisoner's dilemma between two agents
that know enough about each other's source code
to predict what the other agent is thinking
the import of this is first
AIs
are very smart things generally
potentially have options for co-operating
with each other that we do not
including for that matter more pragmatic ones
like
easier to understand ones like
well how about if the two of us build another AIs
and verify its source code together
and then give it our resources
to serve both of us
so that we have the alternative option that
things that can solve the alignment problem
have for coordinating with each other
but yeah
we have a paper called
robust co-operation in the prisoner's dilemma
about
programs that can read each other's source code
and be like well I'll co-operate
if I look at your source code and see if you
that you co-operate with me
and that you would defect against me
if I wasn't that sort of creature
you know they still like exploit things
co-operate with and on them in the prisoner's dilemma
but you know can match co-operate with each other
and the moral of this is like
it's an avenue for AIs to co-operate with each other
it's an avenue for very smart things to co-operate with each other
it's an avenue that I think tends to freeze out humans
which you know in a way is common sense
people sometimes
have alleged versions of how this plays out
that have like
8 billion humans in charge of
like a trillion things much smarter than them
and the humans still have like most of the wealth cause
so well like play the very smart things off against each other
no the smart things will co-operate with each other
and not necessarily with you
if they care about you
if it's a different matter they'll still co-operate with each other
but at the end of that they'll care about you
but the trouble is we do not know how to make them care about this
and we're not going to get that right on the first try
that's the lethal part
the lethal part is trying to do it correctly
on the first try
across a gap that is going to break some of our theories
the really smart stuff
if it works for us
if somehow we manage to keep control stuff
that's only under slightly smarter thus
it doesn't necessarily mean that that thing is going to honestly
help us all the way
controlling the much smarter stuff
it may co-operate with that stuff instead
what do you say to people who argue that you're making too big of a deal out of
LLM technology
because LLMs are just good at making inferences
from the data that they're trained on
but the
as opposed to humans who are great at making inferences from data
they were never trained on
but the intelligence of LLM
doesn't really generalize beyond their data
I mean
they write code
that is not code that was in their training set
I'm not
yeah I'm not really sure what to say except that
well first of all
if LLM technology stalls out here
if throwing 10 or 100 times as much compute
at training GPT-5 or GPT-6
turns out not to yield
substantially new qualitative capabilities
great
we then get to like wait for
one or two more breakthroughs to size of transformers
before we die
so first of all I hope that they're right that the LLM thing is overblown
I thought that
GPT-3 wasn't going to make it to GPT-4
now that it has
gotten as far as it has now in terms of the number
on it but in terms of like what it can do
I'm like oh I don't know where this is going
maybe it just keeps going
what were the big jumps that you felt between those two
degree of causal modeling
how good it got at writing code
the fact that before
it's been trained on understanding any photos
you can ask it to use
a graphical programming language
to draw a picture of a unicorn
and it can try to draw the picture of the unicorn
using the graphical programming language
even though it has never
seen a photo
it has not been trained to understand photos in any ways
it has just picked up from the text
what a unicorn is supposed to look like and how to draw
a shape like that
not a very good unicorn
but the fact that it can do it at all
means that it sort of like understands
what the stuff means you might say
and
the sparks of general intelligence paper
other cases of like
it seems to maybe understand what people actually
mean here
I was hoping that the
pattern recombinations were not going to go that far
but they have
and now I don't know how much further they go
there's a connection between intelligence and achieving goals
that
I'd like for you to elaborate on a little bit further
what are we feeding
AI right now that means it has
goals is it possible to have AI
that doesn't have goals
so
GPT-4
can play a decent chess game
against you I've heard varying reports
some say it's like actually quite good at chess
I suppose if you prompt it correctly others like it doesn't
even always obey the chess rules
but
it can put up a decent
game of chess that is not
any particular chess game in its database
so it's
planning
it's trying to win on some level
you can't have like there's
those things like playing fake
chess that's actually like good chess
if you can
simulate
you can simulate planning
for our planning
GPT-4
doesn't simulate humans but rather predicts them
but if you can predict
where humans would move in a chess game
you must be doing as much planning
and goal-oriented reasoning as humans are doing
in the chess game
unless it's like some game you've exactly seen before
in general as you make things
better and better at predicting humans in particular
they're going to just be able to
do all the stuff that humans do think all the stuff
that humans think because that's what they're predicting
to predict text is not just like
a thinking about words
it's thinking about the processes that
generate the text as
Ilya Suskiver observed
all the causal processes that are shadowed
inside the text
if somewhere in the training data is
like a series of weather
forecast you're learning to predict the weather
as best you can
so
there's whole varieties of tasks
coding
you know, to write code
you need to understand
what effect is desired
from the code and write a line of code
that has an effect like that
pre-imaging
outcomes onto the action
space via inverting your
knowledge of a complicated environment
the way I would
like to find the heart of intelligence itself
is that you look at the world
you understand the world
and then you invert your understanding of the world
to understand what you have to do in order to steer
the world to particular places
and many of the things that
you know, one makes
AIs are like a
nuclear weapon that spits out gold
until they get large enough and then they ignite the atmosphere
and nobody can calculate how large they need to be
to ignite the atmosphere
a bunch of that gold getting spit out
is from planning
pre-imaging
results onto actions
results onto designs
pre-imaging results onto outputs
choosing the output such that
and this is
the dangerous part of intelligence
it's the ability to
understand how the universe works
and then choose such that
you end up with like the tiny spirals
with little things producing predictable textiles
but sort of whatever you know, form of ice cream
gets pursued in the long run
it's figuring out what you do
such that that happens
so in your mind just to summarize
I guess the trajectory of AI
is this inevitable
scenario where AI goes rogue
and it's very hell bent on
acquiring resources
and it's impossible to stop
I mean, inevitable is a strong word
I can imagine a world that was
you know, locking everything down
had
minds of a level that could figure out the theory
without
blowing up the world a few times for practice
it's inevitable on its current course in speed
is that a fair?
Sure, it doesn't look super duper inevitable
if we're going to have it
we'd better be doing stuff very differently to have it this stuff
through the average person
even though everything we just talked about
I think it's hard to really believe
clearly by people's actions
that this is going to happen as opposed to all
other kinds of outcome
I mean, I think people
outside the tech industry
have kind of been quicker on the uptake in some ways
you know, I think
there are a bunch of people going like
wait a minute, OpenAI
wants to do what?
they want to build like
Yanlacoon going like
yes, well, we're going to build a super human intelligence
but it's okay, we'll keep it under control
it'll be submissive to us
and I think that like a bunch of
ordinary people have successfully looked at this
and said what much
faster than some people
who have, I don't know, been like
overly steeped in
the kind of psychology that
developed around this stuff before chatGPT
where you could say
Ludicrous shit and nobody would call you on it
for however many years.
Why do you think, I mean, the people that seem to be quicker
on the uptake
I would say
outside of the one exception being there's a lot of
people deep in AI
that are acknowledging the risks here
but
the people outside of tech that seem to be
fearful
I would categorize by and large
as the people that are fearful in general
of technological
progress and I don't know if that's a fair
we're sort of talking about a straw man person
but do you feel like they're actually uniquely
what about that part of the population
do you think? I think that before chatGPT
you saw people making up
stuff to be scared about with respect to AI
or like seizing on stuff that
is not a real problem because it would have survivors
and be like oh knows what if the AI
says a naughty word and those are the people
who are like always fearful
sure and then like with
chatGPT and being Sydney
you got people going like
you know
noticing what was going on
even if they're not necessarily nervous now
that's some of the
sea chains we've seen here
is there anything that
you would want to say
I'll leave the
AI dooms specific
points here but
any other analogies
or anything you would want to say to make this
more real or tangible or accessible
for the average listener that's trying to
understand all this stuff
people are making or are
driving towards making stuff that's smart
than humans really actually smart
like spark of creativity not just book smart
they have no idea what they're doing
they have no idea what goes on in there
progress on
understanding what goes on in there and shaping it
is going enormously slower than the mounting
capabilities
the people
at the heads of
the operations building this stuff
do not appear to be taking it anything remotely
like what I would call seriously
some of them are records on going
like record like well you know
the earth might get destroyed but first there'll be some great tech
companies or you know just like
haha lol lol lol
that's not what you want
when you're trying to do an unprecedented
scientific feat of science and engineering
and having it work correctly on the first try
or the entire human species dies
so yeah
like it's not actually all that complicated
you gotta
you gotta bunch of people who are like
in the short term like
getting excited looks at parties
which is why they do everything they do
and they can get that by like building
scarier and scarier AI
and some actual uses
some very important uses
I don't want to minimize that
some of the technologies coming out of this would be an enormous spoon
but if you were taking this seriously
you would
put the whole thing on international lockdown
and like have the
good uses
the most important good uses like the medical stuff
alpha fold the future
the successor versions of alpha fold
do that
without training the general systems much more
powerful than GPT-4
try to get the benefits of that
get benefits from the systems that are only
as smart as GPT-4 which is a lot of benefit
and then like just shut down
all the giant training runs
they don't know what they're doing
they're not taking it seriously
there's an enormous gap between where they are now and taking it seriously
and if they were taking it seriously they'd be like
we don't know what we're doing we have to stop
that is what it looks like to take this seriously
you published your article on time calling for a pause
on training AI models
to clarify you want
a permanent moratorium
but you want people to be able to keep using GPT-4
in all AI models and capabilities
that exist today but you don't want GPT-5
in the subsequent
ones coming online is that
I mean if it were up to me I might possibly
if it were entirely up to me I might possibly go down
to GPT-3.5 but you know
it seems like an okay place to stop
it is probably not going to destroy the world
I hope
you know compromise still use GPT-4
instead of going down to GPT-3.5
why do you think why is that where you draw the line?
because it looks like
the current system should not be able to destroy the world
even if people hook it up in particular
clever ways and I don't know
what GPT-5 does
and neither will the creators
at first because whenever anything at this
level of arcane
this gets released there's a period
as people figure out how to hook it up in new
clever ways and get more
utility out of it than the creators
realized was in it at first
from a practical standpoint
I guess did you write that
as a
sort of an expression
and sentiment and characterization
of the way that you felt?
no I don't do the emotional
expression thing my words are meant to be interpreted
semantically
so I guess
they're supposed to mean things
I guess at a very literal level then
how would that actually
let's say China says no
and we do it the U.S.
does it do we go to war with China
over them saying no?
China has published air regulations
I don't know how seriously they take it but they have
published air regulations more stringent than
the United States ones so the first thing
I would say is that it is not at all
obvious to me that China does not go on board
with this
I am not super happy
with the current chip controls
that prevent China from getting
real GPUs although
the videos apparently allow to export GPUs
to them that are only like one third as powerful
as their real GPUs
it's not clear to me that there's a whole lot of point
in that I'm not quite sure what anybody's thinking
there unless it's just like slap-tine in the face
or something but anyways
yeah like I'm not super
the problem is not China getting the GPUs
the problem is anybody getting the GPUs
and if we are in
the world where the UK
is like we need an international coalition
to
track all the GPUs
put them only into internationally
monitored data centers
and not permit giant training runs
if the UK
goes to China on that
and the UK and China bring in the United States
I might worry a bit about Russia
Russia I think would have a harder time
getting the GPUs and putting them to data centers
than China would
but if Russia manages to do that anyways
then
the thing I would say there
the posture that I would hope for
international diplomacy to take
is like
please be aware Russia that if you do this
we will launch a conventional strike on your data center
if we cannot convince you to shut down
if it is up and running
and we do not know what is running on there
we know that dangerous stuff is running on there
like
we are not doing this in the expectation that you will back down
we are not doing this in the expectation
that you will not go to war
we are not being macho and being like
this is us threatening you because we expect
you to back down
we will launch a conventional strike on this
data center in terror of our own lives
and the lives of our children exceeding the terror that we have
even of a nuclear retaliation
by you this is not a macho thing
this is us being genuinely
scared
please work with us on not wiping out the human race here
if they are like well no we are tough
then you launch a conventional strike on the data center
and you know what comes comes
and the thing in international diplomacy
is if this is what you are going to do be very clear in advance
that that is how you will behave
do not let anyone
get a mistaken impression about what you will back down from
if you were president
today
tomorrow how long
of diplomacy and negotiation
would you give before you would actually
launch it sounds like
we are nearing the point in which you think
that air strikes
on data centers and that is a
pragmatic
approach even at risk of nuclear war
it is only helpful if you have already
shut down all the data centers in the allied countries
or brought them under monitoring that prevents large
training weapons
so let us say that is done we have done that you have successfully
gotten all the countries in our alliance
to do it
if there are hold out countries
that are like lol we do not believe
the threat is real and assembling a bunch of
GPUs then
yeah I think once you have got as many allies
and you have shut down your own data centers
first to be clear that you are not like trying to
take capabilities for yourself
you are not willing to launch others to be clear that you are
putting everyone in the same boat
that we live or die together in this
is not a political stance but a
fact of nature
then once you have put your own data centers
under monitoring once you
prevented all the people in your own allied
countries from doing large training runs
if somebody else is successfully
assembling a
has successfully gotten hold of contraband
GPUs statement shipment and is
assembling a data center that can do
runs
underneath
the ceiling that the coalition has imposed
then yeah I think
past that point you
communicate clearly that you are about to launch
a conventional strike you beg them
not to do it
and then if they keep going you do it
and to be explicit about that
you think a nuclear war
is preferable to
the path that we are currently on
if
Russia drops
a nuclear weapon on a
US city in response to
it is not clear to me if this is how things play out
but you know if you
conventionally strike a
Russian data center and Russia decides
that they want to drop a tactical
nuke on a US military base somewhere
in retaliation
and your policy then calls for
dropping a tactical nuke in Russia and you have got this whole
slow motion exchange
there would be survivors
there would not be survivors from natural superintelligence
part of
the horror of this whole thing is that
we will not know
what the
size of nuclear what the size
of metaphorical nuclear weapon is that ignites
the atmosphere
that if Russia is training
just GPT-5 that the best guess
is that this thing will not end the world
but by them having this
thing if they can thereby gain military
and economic advantage it will break the embargo
and not that day
not as a result of
probably not as a result of Russia training GPT-5
but as a result of
everything that falls apart as a result of that
as a result of other countries having to train
their own version of GPT-5 to keep up with Russia
such that Russia doesn't even end up with an advantage
eventually everyone dies
it would be clear cut
if we could run a calculation
and say Russia if you train this large
AI right now everyone will die
the next day
and so we are willing to
and so a nuclear war is preferable to that
life would be simpler if that was what it was like
but of course we could do an unambiguous calculation
like that Russia
would not do the training run
any more than they'd like deliberately
launch all their nukes to the United States
and provoke a nuclear retaliation from the United States
but as long as it's
clear how things play out
you can hope
for there is
some hope that
people with sufficient power
to threaten nuclear war
if you conventionally strike one of their data centers
will not actually do that
because they will not want the nuclear war that results
it's the
in a way from the beginning
it's the
lack of clarity
that is the danger
if we knew exactly how large
of an AI would destroy the world
it would be much easier to not do that
and to have the international arrangement
around not doing that
and to enforce the international arrangement
around not doing that
but this is the problem that nature has handed us
we're going to get that clarity
and if we
tomorrow the human species wakes up
with the determination within itself to survive
which is not really what I expect
but we would have the option
of being like okay we don't know what destroys the world
no training runs larger than this level
and over time you'd have to lower that ceiling
as the algorithms got more efficient
and it got easier to train things
and maybe there would be
a country being like
we don't believe it
but
and the correct answer there is like we're not trying to make your life difficult
we have shut down our own data centers
we are not doing what we would not do to ourselves
we are not trying to throw our weight around here
we get that you don't believe it
we're sorry
it is now a fact of international diplomacy
that if you build a data center
you'll get a conventional strike on that data center
for people acting in terror
of their lives and the lives of their children
we're sorry
we understand that this is what
this is the estimate these other countries
have arrived at
we didn't want it to come this way
but given that
this is the case if you build a data center
it will be destroyed
if diplomacy fails
and
that seems to me like a potentially stable international situation
if
tomorrow morning humanity woke up with the desire to survive
why would we be able to monitor
and figure out
what's going on inside this system
and if it's
revealing something
but actually doing something else
why won't we be able to have any checks and balances
to figure out hey if it's using
compute resource beyond what we expected to do
or whatever
that it's deviating from the goal
as we understand it
these are sort of like a bunch of different questions
so first of all
I
frequently use the phrase these days
giant inscrutable matrices of floating point numbers
though when I wrote the time letter
people
were like the editor correctly said
like our readers do not necessarily know what floating point numbers are
so I said okay giant inscrutable arrays
are fractional numbers
are fractional numbers
did that stay or did they strike that
that stayed
so giant inscrutable matrices
of floating point numbers
and giant inscrutable vectors of floating point numbers
moving through it
we don't know what's in there
we don't know what's in the matrices
and why don't we know what's in there
well because we make these things
in the first place by
doing very basic
calculus to this giant mess of numbers
to see well in what direction
would the
probability assigned to the
correct word go up if we
tweaked all those numbers
and
we
tweaked them by going down this direction
and the gradients are inscrutable and the directions are inscrutable
it's just like never starts out
it's just we understand the
program that makes the AI but we do not
right the AI we do not understand the AI
and it's just very hard in practice to
look at these giant messes
of floating point numbers and figure out
what do they mean
recently somebody on a much smaller
language model
than GPT-3
managed to figure out where
it seemed to be storing the information
that the Eiffel Tower
was in Paris
and they even figured out
how to poke at it until it thought
the Eiffel Tower was somewhere else
you know that was work
that was a triumph huge success
if humanity
had its act together we'd be like posting
ten billion dollars in prizes annually
for work like this until half the graduating class
of physicists went to work on that instead of
doing a bunch of funs
and yet
it's such
an incredibly basic thing
it's like
in Eiffel Tower
Paris
from the old old days
of semantic nets in the 1960s
so that's one way of quantifying
that the frontier of work and interpretability
is running around 60 years behind capabilities
I mean just to restate that
the opacity in these models
is such that it was a major breakthrough
to figure out
where it was stored
that the Eiffel Tower was in Paris
yeah
and it's a very primitive sort of fact
the kind of fact that we
knew how to store an artificial
intelligent systems a long time ago
I just couldn't do very much useful with it
so it's not that it's unsolvable in principle
it's that
our understanding of these systems
is lagging vastly, vastly, vastly
behind how capable they are
and the capabilities are not
standing still
so we're continuing to work on interpretability
mechanistic interpretability
understanding it by opening up
silencing what's there
and it's moving along but capabilities
are moving even faster so we are just falling
further and further behind
I think people probably remember
there was an example, I think it was GPT-3
that they could do a poem about Joe Biden
but not about Donald Trump
or something to that
that sounds more recent
yeah, whatever it was though
and open AI
had to respond, we actually don't know
we don't know why we're trained on this large
corpus of data
we actually couldn't point out why it was the case
but I think people
thought we're making our AIs
woke and it's
so I could be wrong
but I suspect that one of
two things happened
first that the people being paid $2
an hour
to give thumbs up and thumbs down
to good versus bad answers
you know, I personally would
say these people are not necessarily being
paid to do any better
if you're paying them $2 an hour
as much as that was clearly the best opportunity
they had at the time but still
one thing is that maybe
they
got a
western liberal leaning
education or something
and thumbsed up some relatively
woker things and thumbsed
down some
other things
but my actual guess is
that they were
thumbing up
things that sounded like
corporate boilerplate and safe
and deep in the training data
there was a correlation between
sounding very corporate boilerplate
and safe
and sounding as you put it
woke
or like control leftist
or that
you know, that one would
not say an unkind thing about
Joseph Biden but would maybe
say an unkind thing about Trump
if you talked in this
if you were the sort of person
in its training data who talked
and sounded like
corporate boilerplate sounds
that would be my guess is what was happening
there could be that they just like explicitly
sent in an order, train this thing
to
only say bad things
about Trump but not Biden but
you have
that's not my guess
speaking of woke, Elon Musk
has made overtures
to enter the field, I guess he started back in
2015, was that the same conference as you
in Puerto Rico, now I think
he's talking about
starting a competitor to the competitor
you previously started DeepMind? Correct
starting a competitor with some
anti-leftist
bias or however we want to characterize it
I doubt that he would characterize it that way
and when should at the least be there
or sorry, mainstream
opinions on
data and all of that, I don't know
something that's not woke in the way that OpenAI
TruthGPT was I think his own expression
what's your perspective on Elon's
entry into
this space and he
it seems like different points of time has taken an interest in
AI safety
I don't think
that his past interventions have had
good effects
and
TruthGPT if he's
in their
charging head trying to build larger and larger models
even like ahead of other companies
or when other companies are trying to
slow down and he's
trying to defect from their
fragile cooperative, attempted
cooperative arrangement
then that would be quite a bad thing
and if he just goes off and
builds something that is no more powerful
than the other models in use, deliberately
deliberately not any more powerful
but trying to not have
the corporate speak bias
that is where I think we're the
I guess we could call it
well, okay, yeah
where the bias comes from
that's relatively
harmless, I just kind of
pessimistic about
whether
due caution will be exercised not to upset
fragile attempts at cooperative arrangements
whereby maybe everybody
doesn't die
his statement that he thinks that
like if you build a thing to seek after
truth then it will keep humanity around because we're an
interesting source of truths
well, I mean the actual historical
analogy would be a bunch of
biologists making
hopeful statements about
what group selection would do
that turned out not to pan out experimentally
in the sense that they had like hopeful
things of like well maybe group selection
will produce these beautiful aesthetic arrangements
and this was just like not something
that worked as math and was not something that
observed in reality when you're taking
an extremely alien thing
and hoping that it will do stuff you want
much easier to hope than it is to get that stuff
is the lesson from evolutionary biology
and hope's biologists
have so beautiful aesthetic arrangements that
work turned out to not be found in nature
you know, like there are
things that can produce truths more
efficiently than humans, whatever weird kind
of truth it ends up interested in
whatever, I mean most
I just think you can't do
that, you don't end up with something interested
in truth however he
defines that any more than humans exclusively
produce, exclusively pursue reproductive
fitness
but
yeah, if it did end up pointed in that vague
direction that there would be like some kind
of equivalent of ice cream forest taste buds
that was producing weird truths
more efficiently than humans do
and
I think that the original hope of like
the solution to AI
is to just like give AI to everybody
which kind of presumes that aligning it is a
solved trick, that making it do what you want
is a solved trick and the only problem is like bad
people getting ahold of it. I think Elon founded
OpenAI and I think that many of the people
in OpenAI who initially went along
with this in the salary
like the ones who actually cared at all
looked at the rat like
as soon as they thought about it for an additional
couple of days or an additional
couple of years or whatever
eventually worked out that like no you can't save humanity
by
open sourcing stuff you can't control
that nobody can control at all
and
you know realize that the long-term plan
was not going to save humanity
so then you've got like people at OpenAI
who don't care and people who know that the mission
can't possibly work
and from Elon's perspective
they went and stabbed him in the back but it's
inevitable because he was leading them on a
project to save humanity where the basic
mission could not possibly work
and that's kind of what I would expect to play out
again with truceGPT
that you know some people go with him
for the
salary and some people
don't actually care at all about the mission
and those who do care about
the mission enough to ever actually think about it
realize that you cannot actually save humanity
this way then the truceGPT
turns around I mean
probably Elon just like tries to keep very tight
control of it this time
but you know
you can't
get loyalty with these
solutions that the people who actually care
will realize upon reflection cannot possibly
work.
Do you think OpenAI deviating from
the open side that they
originally started with and also deviating
from the non-profit to the for-profit
do you think either of those were
good decisions?
I mean the openness thing is a horrible disaster
plan
not being open and then not being open
but continuing to call yourself
openness makes closed look
like hypocritical profit-grubbing
now if they'd changed their name to
closed AI and be like
we think that closeness is how you protect humanity
we're not pretending to open open we're not calling
ourselves open I think there would have
been like a skepticism
and not without reason
but if you go closed
and call yourself open AI
that looks like
all that that openness continues
to be the ideal and all attempts at closure are just
this like hypocritical thing you do over
cover of openness while you're
grubbing profits well in reality the situation
is that you know
like spreading this stuff everywhere
is not actually good because we're not
of course to have any of the people with a copy
being in charge. I didn't realize
so your issue with the term
open is that it's a
misrepresentation not that you think
it actually should be open
it should not be open
and if you close it it should not be
called open these are two separate
problems it should not be open
and then once you have closed it should not be called
open
I mean you've obviously dedicated such a huge
amount of time to your
an energy to this effort and I
guess one question is just
stated versus revealed preferences
slash actions
like
I think you said in a previous podcast that you
haven't reached out to the people at open
AI to try to influence them
and you met Sam Altman
for five minutes and there was a
selfie that I think broke the internet
quote unquote of you guys together
but your only comment was
to get him to change the name
of open AI
I mean we did just we discussed a little
other things in those five minutes
but by default I view that as closed
unless Sam Altman tells me that it's open
so you were able to make some
overtures and some conversation
about philosophical stuff with
with him have you pursued no comment
have you pursued other efforts
with people at open AI
or other companies I've had
a conversation with
a recent conversation with
at least one like
major technical figure at
open AI which is again like
closed unless they tell me that I'm allowed to talk
about it did it make you more optimistic
somewhat I mean the fact that they were
reaching out to me at all and like
trying to be processing some of the technical issues clearly
one thing I think Sam said
the other day it was in an
but it's kind of late what's that
it's kind of late to be reaching out
reaching out now like you've
I do feel like there's a like
you don't you don't want to slam the door
in the face of people who are changing
there who are trying to
change their ways but it's
and you know it's better
than not doing it at all and
other joints sure have not done it at all
like Meta has Yanlacun out there
like trying desperately
to make it look like only crackpots believing
this stuff while
while Jeff Hintrin is like
nope but I don't know
I the record does
show that
for as long as they could get away with that
back when it wasn't a bad look
they they were like ha ha
let's keep going well the best time
to reach out other than
seven years ago or whatever
is now so good
it's kind of late
I mean at this point we are like
I mean like
my ask of open AI is to call in the
government and shut down all the large training runs
right there's not very much that I have
that hasn't asked for open AI
you mentioned Yanlacun and
he is an active
voice on Twitter
do you think
it's just a disagreement of beliefs
or does he have
some ulterior
motive with his
discourse on Twitter
and some of the engagement that he's
had with you do you think he genuinely believes this
stuff or is he being performative
what difference does that make his arguments are his
arguments they stand on their own
given what his arguments are I'm happy to
tell people to just look at the arguments there's no
need to
engage in vulvarism and psychologizing
and being like well you know
his arguments might
look convincing on the surface but consider
the ulterior motive
just check the arguments
I want to read you a bit of an excerpt
I guess from the other day
Sam Altman did an interview with Barry Weiss
and you were mentioned
and I would love to get your reaction to it
so he said
look I like Eleazar
I'm grateful he exists
he's like a little bit of a prophet of doom
if you're convinced the world is about to end and you are not in my
opinion close enough to the details of
what's happening with the technology
which is very hard in the vacuum and I think it's hard
to know what to do a lot of people in the
AI safety community have said things
like I never expected I'd be
able to coexist with a system as intelligent as GPT-4
you know all of the classical thinking
was by the time we got to a system
this intelligent either we had fully solved
the alignment problem or we'd be totally wiped
out yet here we are yeah I don't know
yeah citation needed who exactly
made this prediction about GPT-4 level
systems I don't remember making
this prediction about GPT-4 level systems
and
you know if they're supposed to
you know playing the card
of like you got to be in contact with the system
don't be shy tell us
what you have learned from being in contact
with the system that supposedly invalidates
all of my arguments don't just be like oh yes
you know there is mysterious ineffable
knowledge to be gained by interacting
with the system that invalidates all of your
analogies from between
gradient descent and
natural selection and the case
from evolutionary biology is that
as it has already falsified
any hoped for general rules about if you
optimize for a thing you get internal psychological
desire for the thing don't be shy
don't don't play hints you know
I feel like this is already played out in a sense
where people on Twitter be like
only those completely ignorant
of deep learning you know Eliezer has never
built a deep learning system I have
and then like you know Jeff Hinton
the like actual literal
inventor if anyone is of deep learning
starts you know basically
starts saying
some pretty similar things
uh yeah so
uh you know
maybe Sam Altman feels that Jeff Hinton
is also not sufficiently in contact with the details
of of GPT-4
perhaps he should say that too you know
don't just go after Eliezer here like also go
after like Jeff Hinton saying some similar things right
you know Jeff Hinton has not
been in sufficient contact with the details
of truly large scale
systems to to to have an opinion here
but you know better yet
say what you know say what you have
observed say what do you
what do you think you know and how do you think you know it
you know say
which of my arguments falls down based on
which truth about reality that is to do from
which observations about GPT-4 this kind
of important you know if you're taking it seriously
you shouldn't be trying to shrug it off like that
don't be shy tell tell you
expose your arguments to public consideration
there don't just
rest on your authority
because as much as I
you know if you're
going to rest on your authority instead
of your arguments then somebody
may perhaps point out first of all
that you're doing that and second of all that you
might have reasons to be
less than fully blindly
trusted by the human species given
your current position if you're going to rest
on your authority and your position instead
of on your arguments since maybe
January or February whenever you did the bankless
podcast
this will be your fourth
one
what has changed in the discourse
from your perspective
since you've been out there talking about
this stuff more in earnest
the time article was published
there's been
I think people are more at least
aware of some of the stuff going on have you seen
any progress have you updated any
your mental models on the
it's more hopeful than
it's going better than I would have expected
unfortunately there's like a very
large gap of how much
better it would have to go before
I started expecting to live
but yeah it's gone
better than expected now if it just needs to like keep
steadily doing that for another several years at an increasing
pace and then it will like I've gone so well
that we might even survive
now there's there's political interest
there's they haven't come out
with anything terribly
neither the Republicans nor the Democrats
in the United States have committed to themselves
to anything terribly stupid yet
there's an alternate world where China
didn't like
promote the regulations it did
on AI
where they could have looked like
much less receptive to an argument about
an international alliance and coalition
things are going a little bit better than I expected
but they
seems like they need to go like out of
uncharacteristically well for a history book
the only thing remotely close to it
is that we still haven't used nuclear weapons
since the first two
and despite some close calls
and we
would need to like be reaching a couple of levels
above that to pull this off
correctly it's harder
I don't know if there's
10, 20, 50 people
in the artificial intelligence community
at large and maybe I'm including Mark Zuckerberg
because you know
Yad Likun reports into him
and there's
a handful of people with real
influence and I think
strategy I don't know what number
you would put on that maybe two dozen
something like that
I mean I think that like
Demis Sassabis and Shane Legg
are the only
people who think in a way where they
like might have a
strategy for something beyond
the grandisement of their own company
sure okay but
we're talking about people that can actually make decisions
to some end and have some influence
to some end around
alignment and slowing down and maybe
have the influence to coalesce
with other people not whether or not they'd actually
do it but that they might have the influence
and standing to try to
do this within the artificial intelligence
community I don't know what that number is but it's
smallish right well
I mean it's
it's unfortunately seems like
it might be growing and
you know that the
the nature of the thing is that
single defectors can blow that up
which meta would certainly try to do
so yeah like
the thing is is that Sam Altman does not
have the power to shut down meta
the actions they can do at this point are
calling in international
governments to shut down everything agreed
but I want to get in the
head of someone that
there's the observers
and maybe you could say venture capitalists actually have some influence
but I would argue
probably very marginal
I want to
get in the head of someone that actually is working at one of these companies
on artificial
on artificial intelligence and so
an arbitrary AI researcher
who's been doing it for five years and maybe
doesn't have some
super moral opinion or influence
about the direction
this stuff is headed
do you think there's any decisions that
someone like that has
made that has
made doom
besides just working on their job that has made
our AI do more
likely
I mean I expect they also gave scornful looks at parties
to anybody who talked about it potentially being dangerous
that probably didn't help either
but mostly just their work I'm not sure I understand
like where the questions going
well I'm just curious as there's a lot of rank and file people
that
that can't influence the big picture
of all of this stuff
and they're doing their jobs
and so what would you say to
there's far more people that
are just doing their jobs in front of them
that can't actually
change the trajectory
of meta or open AI
or anthropic or deep mind and all of that
would you
what would you recommend to those people that are just working in the field of artificial intelligence
should they just try to influence
the little island they're on
should they do the Jeff Hinton thing and step out
and try to be more influential externally
is there any advice you give to
just more people that are in the field
and have some level of expertise
but aren't
influential enough to actually do anything
at scale within the companies
not really
you know like
reality is what it is
like
come the next
set of alarming news come the end of days
not
you're necessarily going to know which days are the end ones
but you know there might be a
moment of increasing horror and panic before the end
you know like
they'll look back and think what they think I guess
there are obviously a ton of smart people
working in this field
and there's definitely a decent
number that as they've been closer
and closer to it definitely have fears
about artificial intelligence and we quoted
some of the stats at the top
there aren't many people that are
vocal
and have
cogent incredible
arguments pushing back
that I found pushing back on what you're
saying and there's
people that might disagree with some of the probabilities
Paul Cristiano among them
Robin Hansen seems to have
a much more techno normal attitude about these things
but in other words he expects
like all the humans to doubt but
thinks that our successors
will you know exist
in a state of extreme competition that reduces them
to the bare means of survival
but Robin Hansen is fine with that
is Robin Hansen's actual
position unless I've misunderstood it
why do you think that
why do you think
that you don't have
better critics and why
are you the only
one that sees this so
clearly and is advocating this so
vocally?
I think that our
in its general system
for the production
of educated people
and public intellectuals
is kind of falling apart
I didn't do it
tried to repair it
that didn't work at all
yeah I don't know
in some ways I feel like a leftover from
a slightly more functional period of civilization
being raised by their books
and I
you're not going to find a convincing story here
you're not going to find a story that
causes the universe to make sense to you
and makes you feel like there's no more
anomaly that things aren't somehow
wrong that you know like
this is how it all turned out
all I can say is that you don't need to
theorize the thing wrong with the universe
you can just observe that
20 years
you know like 20
22 years ago now
that first call that this was
predictably going to be a problem later
and somebody ought to start handling it
humanity continued to assign it
like essentially indistinguishable from zero priority
um
the various people who
were pretenders to the position
of caring about humanity's problems
like month four papers claiming
that stuff was going to be 30 years off
as of three years ago
you're not going to find an explanation
that makes you feel good about how it played out
it did happen to play out that way
humanity did decide not to prioritize this problem
you're not seeing a bunch of
in a certain sense you're not
seeing a bunch of other people with
like
build up arguments in this field because
you didn't pay for it
yeah humanity made
its decision there what it was going to prioritize
and here you are now
to say that back you needed the credibility
and have thought about all this stuff
over such a long period of time
and that was an incentivized
when you were doing it
financially
to go down all these different rabbit holes
and think about all these different things
yeah
and that's an observable fact of the world
and any story that I can tell you about
how earth ended up in that situation
um
is going to be less probable than the
direct observation of that being the situation we ended up in
you can verify that
you know like
nobody cared for a good long time
that's
you know where are the people
like trying to work all this out
back in
2001 when we had
we actually had some time in which to think about it
they didn't exist
um
that part is directly observable
and the stories for
why from my perspective some of the arguments
so it seems so strange and non-cogent
you know I could
tell you a story about how social media
destroyed everyone's sanity
um
created a world of relentless
unreality
but you know
the direct observation is that the arguments
aren't cogent and I would just tell you to like yeah
there's not some hidden story of how they're secretly cogent
trust your eyes
you don't need a story to explain it
you just saw it happen
2075 you and I are grabbing a beer
coffee or whatever it is
how did we end up there
if you were to give your most optimistic path
to us surviving
for the next 50 years
uh how do you think we end up there
the top option in the
manifold prediction market
on assuming we survive
how did that happen
says that humanity
didn't manage to shut down
all the overly advanced
development work long enough
for human intelligence enhancement
or uploading
or something
or by human intelligence enhancement I mean like
using alpha fold 3 to test a
broad variety of drugs on suicide volunteers
until you find something that actually
increases intelligence because if you do this at all
you're doing it in a tearing hurry before the algorithms
got efficient enough to end the world
you know the AI algorithms got efficient enough
that your GPU limits don't matter anymore
and can you explain like brain
limitations and all that why
that might be a counterbalance to
if you can scan a human brain
in sufficient detail and emulate it
then you can potentially make
that person smarter
once you have better read write
access this is
not a zero danger
thing to do but
unlike trying to build a
trying to build a super intelligence on the first try
it's something where you can kind of imagine
it going right
and you make the human smarter until
they go over that strange
threshold for
automatically acquiring security
mindset and some other things
and then I think
they can maybe actually solve the alignment problem
to date there haven't been many people that
agree with your perspective back to our
earlier point and there's been little to no
incentive to
solve them because it's been
fairly theoretical I think in people's
minds
people have mistakenly thought
that people have mistakenly thought
deliberately or in their emotions that it wasn't
a problem it's not that there's no incentive
it's that the incentives were there but they did not see them
there was an incentive for humanity to launch
a crash project on this 20 years ago
we did not
humanity was blind to that incentive it doesn't
mean the incentive doesn't exist sorry keep going
economic incentive there was
a short term
short term economic incentive
why do you think that if a government
or a bunch of governments came together
and said hey we'll give
$10 billion $50 billion
to solve this problem
and drastically alter the incentives
around it
it still wouldn't be solved
how can you tell whether they've got a solution or not
I've watched people
try to make progress on the alignment problem
and
unless it's
really
obvious to anyone whether
or not progress has been made they cannot make progress
that's why
I specifically said $10 billion
per year in prizes on
mechanistic interpretability
on opening up the eye and understanding
what's inside because when you have
successfully decoded some tiny aspect
of the giant and scudible matrices of floating
point numbers you can tell that you have
done that
that is why
of the progress the progress has been
concentrated there and you
can make progress in other
it is theoretically possible
to make progress in other places
by questioning yourself
in the right way
by shooting down
theories
yourself rather than waiting for somebody else
to shoot them down for you but
how can you even tell who has that ability
effective altruists
were devoting
some funding to this issue
basically cause I browbeat them into it
as I would tell the story
and a whole bunch of them
like their theory of AI
three years ago was that we probably
had about 30 more years in which to work
on this problem because of an elaborate
argument about how large
an AI model needed to be
by analogy to human neurons
and it would be
trained via the following scaling
law which would require this many
GPUs which at the rate of
Moore's law and this attempted rate of
software progress we got in 30 years
and I was like
this entire thing
falls apart at the very first joint
where you're trying to make an analogy between the AI models
and the number of human neurons
this entire thing is bogus
it's been tried before in all these historical examples
none of which were correct either
and
the effective altruists can't
tell that I'm
speaking sense and that the 30 year
projection has
no grasp on reality
if they can't tell the difference between a good
and bad argument there until
you know like
stuff starts to blow up now
how do you tell who's making progress in alignment
I can stand around being like
no, no, that's wrong
that's wrong too, this is particularly going to fail
you know like this is how it will fail
when you try it but as far as they know
they're inventing brilliant solutions
it's the different you know it's
anybody can build an operating system
that they think is secure
building an actual secure operating system
is much harder
and the difference unless you're like quite good
at poking holes in your own operating system
which more people think they're good at than are good at
is
you know the way you find out it's secure
is that somebody else
pokes holes in it and the holes get fixed
anything you observe to be true
about the current system there's a theory
that it will also hold with respect to
governing something smarter than you
and they can say it will
and I can say it won't for the following
reason I can try to make a
prediction about what goes wrong in advance
it's not always easy it's always it's easier to say
where things end up in the trajectory
they follow to get there but basically
you know like the question is like how do you launch this thing
for the first time and have it work
given that stuff that works on the earlier systems
some of which will inevitably break
on the later systems later systems are different
people are proposing all kinds of wacky ideas now
which I feel like I'd tear apart
like tissue paper
and they go on advocating them
we'll make the AI do our AI alignment homework
we'll briefly state the reasons why that's the problem
it's the hardest thing you could
try to align in AI to do
it's got to understand
human psychology and it's got to understand
adversarial reasoning about computer science
and it's got to understand
what happens
how AI systems go wrong
it's got to be thinking about how AI systems go wrong
reflecting on how to design AI systems
you know it's just this like
it's like ah build a system
that helps you with
the biomedicine of making humans smarter
so it just needs to understand neurons
and neurochemistry and not
AI design
make a system that works on nanotechnology
so you can try to scan people finally enough
to upload and emulate them
and try to make them smarter under
controlled and dangerous conditions
but you know like
AI helping with alignment it's like
it's the act of somebody who wants somebody else to do their homework
that's why I call it having an AI do their
AI alignment homework and that's a great idea
as far as they can tell
I think that humans are just kind of not good enough
at this is the impression I get
after watching people
fail at it for 20 years
nor did I solve it either
people can't tell
when they're making progress they can't tell what are good arguments
or bad arguments they're not going to be able to train an AI
to tell the difference between good or bad arguments
as some are proposed first because that's like among
the hardest thing you can ask an AI to do
and second because their training data is going to be broken
so can you put a fine point on
why alignment is such a hard problem
to solve it sounds like
there's infinite
permutations or things that you actually
can't determine
if they're correct or not back to your example
of making a secure operating system you don't
actually know and then
well no people are just overly optimistic about
whether or not they've solved it you can know
whether a system is secure or not
so sort of okay that's fair
so can you put a fine point
in your last answer kind of alluded to this
but just a fine point on why alignment in your opinion
is so hard for us to solve
other than we haven't done it
so one way of looking at it is that
it's not quite
fundamentally true but like I think that's
sort of true and a way of looking at artificial intelligence
is that if you can tell the difference
between a good or bad answer
you can make something you can maybe make
something that gives you
good or answers
like if you can press thumbs up
or thumbs down on something
you can make a thing that tries to get
you to press thumbs up and the more powerful
it is the more it can get you to press thumbs up
so when it
the alignment problems
are ones where it's hard to press
thumbs up
in a way that's reliable
that's always right
if you're asking it to give you the design
for a nano system
maybe it gives you like a
bunch of DNA strings for proteins
that assemble into a nano system
and like the question is like
well is this secretly going to destroy the world
are you going to have a human
pair at those DNA strings
and press thumbs down if they think it destroys
the world and thumbs up
if they think it does what it's meant to do
it's difficult
to get the training data
even leaving aside questions of if the training data
is going to generalize well way out of distribution
like
the verifier
is broken
we cannot verify
the most important things
that we would like an AI to do
if we could we could just like verify
all its actions in the first place
how can you tell if an argument is like
trying to persuade you
using not totally valid means
like I can
stare at a set of arguments and flag
the ones that are
using invalid forms of argumentation
that are obvious to me
can I catch subtle influences
no
am I maybe just like wrong about whether something is
invalid argument or not maybe
it's like an amplifier for things you can discern
and
I've yet to hear an account of
what outputs you would have
from an AI that saves the world
from the next day I built six months later
such that you can verify
exactly whether or not
that output is a good output
and this is like one of the foundational
problems there are others
I would advise
googling
AGI ruin a list of lethalities
which is you know like the other
42 items on the list
so you reached the conclusion
that were
likely near 100% certainty
headed towards doom
two years ago
is that about right
I mean over
over time
how have you
how have you led your life
differently since you've reached that conclusion
after playing out the
obvious
after having like played out and failed at some of the obvious things to try
I was like okay you're all doomed
took like a
year and a half sabbatical more or less
when was that
like
beginning at
21 to 22
sort of
I mean one way of looking at it
would be to check the start and end dates on
Project Lawful
the giant fictional piece I co-wrote with a co-author
which of course has like
a bunch of allegory for AI built
into it because you know I can't actually
not do that in my fiction
or I could but who would
want to
but you know like it blurs a bit around the edges
like towards the end of that I was like
doing a bunch of other things simultaneously and not
just working on the fix
just like finishing up the fix
but yeah so like start of Project Lawful
to I don't know like a bunch of the way through
would or like one
like one and a half years after the start of Project Lawful
is about how long the sabbatical was
and so
you took a sabbatical do you
live your life any differently now
other than you come on podcast occasionally
but I mean I've still got my
lifelong physical
health and stamina issues
so there's a lot of frantic
ways of enjoying myself before the end of the world
that are more or less close to me
and I've just like sort of
never been
all that
energetic person I think for
reasons I don't actually know to what extent it's
tied with the health thing
I am suspicious of
insufficiently physiologically deterministic
accounts of mental states
but you know
nonetheless like for one reason or another
like I've sort of like
never been all that tempted
by frantic hedonic dissipations
so you know to me
like self indulgence is like
going off and writing a giant piece of fiction
we have different interests in that regard
what I want to put a
final point on all the
the AI DOOM fund we're having
let's say someone listens to this
and they are sufficiently
convinced that we're on that trajectory
what can they do
to help humanity
survive
write your congressperson
tell them that you'd like to see
a giant international moratorium
on artificial intelligence progress
and you'll back them on that and you'd like to see them move forward on it
perhaps later
more coordinated political action
will emerge but there isn't actually like
a clear website I can direct you to
at the moment
one question I want to be explicit about is
why would anything super
intelligent
pursue things
that are so bad
as
ending humanity and all the other
things that we've talked about
so
the way I would now phrase it is that there's
multiple reflectively
able fixed points of optimization
now what do unearthed way mean by that
suppose that you take mohandis
Gandhi and offer him a
pill that makes him want to murder people
current Gandhi correctly
models that if he takes this
pill his desires will shift
he will then go off and murder people
current Gandhi does not want people to be murdered
so current Gandhi refuses the pill
and
the issue is that this
generalizes if you have something
that only wants to make tiny molecular spirals
out of
all of or like it has some much more
it ended up with some much more complicated and messy
set of desires but in the end
the like one of the components
there that didn't saturate out
that wasn't like just easy to fulfill
and be done with that just like
scaled linearly with how much stuff
they're satisfying it there was
turned out to be like
cheapest to satisfy
with tiny molecular spirals as a preference that matter
have a certain particular shape
if that's the way
you are then
if you imagine modifying
your code to want something different instead
you'll project there being fewer
tiny molecular spirals in the future
and so your current preferences current utility
function ends up less
satisfied if you execute the self modification
and
there can be minds more
complicated than that but they're being
complicated even if they're in some sense
unstable doesn't mean
that what they end up
falling into as a stable attractor
is
be nice to all sentient life
people do tend to invent stories
how you could end you could start out
wanting only paper clips
and end up wanting to be nice
to all sentient life
but you don't actually end up
with more paper clips that way the way you end up
with max numbers of paper clips is by turning
everything around you into paper clips if that's what
you start out wanting that is
stable
and
this is counterintuitive to
some people and very intuitive to other people
and I have a lot of respect for the
for the people who
are like
what about the
mysterious
uplifting goodness
qualia
of helping other people would it not
know this would it not understand it
would it not gravitate it
and the answer there is like it would understand
how you feel about it but it's exact
understanding of how you feel about it would
not in its own internal system
look like paper clips
like the
the thought that produces
this uplifting feeling in you would not produce
an uplifting feeling in it because it would not
have been built as a kind of thing that feels uplifted
by that thought
there are all kinds of minds and
some of them are not friendly
even up at the super intelligent level
that you as a human
with a complicated internal philosophy
have a sufficiently
complicated internal system where you can say
like
but which goals are better than others
the fact that you can
compare them along this betterness
metric
like that betterness metric is not inherent
in the goals
it's in you as a kind of thing that is
evaluating the goals in these ways
a bunch of this is in the
less wrong sequences
I'm afraid and rationality
from AI to zombies
I'll think possibly think they might have cut
the med ethics sequence out of that one
but it's definitely in the online version
the 46 hour audio book
if people want to dive in
I am probably not going to be able to give
an answer fully to the satisfaction
of people who are like but that uplifting
feeling
all I can do is say that
like I know how you feel
I used to feel that way I have tried to write
very extensively about how minds are
put together in a way we can see this as like
that to be uplifted is a fact
about some minds but not others
that they are built in such a way as to find some
lots of uplifting to find some goals better than others
I think you can pursue that
analysis you can analyze it
to the point where it's very clear
that
things that want
start out wanting paper clips
do not want to stop wanting paper clips
and things that are
that may have much more complicated goals than that
do not settle into an attractor
of being nice to sentient life
just because
they are complicated
I myself am
no different in a way I would like the galaxies
to be full of
sentient happy life
that you know looks upon other life
with empathy and sympathy
and if you offer me a
pill to make me stop wanting this I will refuse
that pill because it would
then the galaxies
you know then I am not able to influence
the galaxies in that direction if I don't want
to influence them in that direction
even because it would I am a complicated
sort of thing that has opinions
about what kind of being I want to be
and I don't want to be the kind
of person
who stops caring about the fate of sentient life
and yet from the perspective of a paper clip maximizer
I am a kind of thing that just
even when I contemplate
filling a whole galaxy full of paper clips
I just am not moved by this thought
I am a kind of thing that will turn
galaxy after galaxy
into happy sentient life living lives
worth living eudaimonia
empathy for others learning
and taking joy in discovering new things
I will just turn galaxy after galaxy
into this kind of complicated thing
that satisfies my utility function
and not set aside
even a single galaxy for paper clips
because I just don't care about paper clips
and I don't want to care about paper clips
if you offer me a pill that makes me care more
about paper clips I will refuse the pill
and you know other possible
minds are like that except you know
they will just turn it all to paper clips
and that will be the end of sentient life
which is to say that
there's multiple
reflectively stable fix points
of planning systems
I think I saw something recently
Jan Lacoon say that
alignment just isn't as hard as you think it
to be
people that say that
that alignment is a real concern
that they believe that it is something
but it's just not as hard as
you make it out to be
about the
difficulty around
all the different permutations and thoughts
that it requires
I think that I keep asking Jan to spell out
exactly how he intends to align stuff
so that I can immediately tear it apart of course
and he doesn't spell it out
it's his plans are like well we will just like
make it to be submissive
literally his term
and to which I said on twitter
like Jan has never
Jan clearly shows his
unfamiliarity with the prior
literature here
my 1.8 million word
bdsm
decision theory
Dungeons and Dragons thick
has as one of its primary themes
whether an entity being submissive
is enough to make it easy to steer
which Jan refuses to read
for some reason
and then I like provide a quote of the like thick
actually dealing with that topic matter
I thought it was funny
you have a bdsm
is this a book?
this is project lawful
it's a bdsm Dungeons and Dragons
decision theory
it's just the kind of thing I end up writing
when I take a sabbatical
I was going to say we have different indulgences
you and I
I wanted to make sure I heard that
so that was a bit of an amusing thing
that he like happened to
will just make super intelligences submissive
to us they won't take power unless they want
power and it did so happen
that I'd written a giant thick about this
that I could then be like well he didn't read
my 1.8 million words Dungeons and Dragons
thick
but in fact like on a somewhat
more serious level
people have been trying to explain over
and over including Tian personally
and he has as I understand it in some
cases acknowledged the force of the arguments
and then seem to forget them next time
but things that want more or less
arbitrary stuff
want power so they can get that stuff
you don't have to specifically want power
as a terminal value
to want power instrumentally
so if his argument is
they won't take power unless they're built
to want power which is like more or less
this kind of thing he said over and over
then you can see right there
that he's unfamiliar with
what sure sounds like a valid argument
several times and that sure is all over the literature
so you know
you don't need to have elaborate theories
of his internal state it's enough to look at his outputs
I would be remiss
not to ask given my actual
job is that of a venture capitalist
what role
do you think these
sees as funders of a lot of
these businesses right not
in totality obviously Microsoft and Google
and Facebook and all that are funding
in mass but do you think
there's a moral obligation to not
fund incremental AI
companies I guess if I were to put the lens back on
my industry and
and the people in pursuit of
capitalistic objectives by means of
venture capital investing
the thing I'm an expert on
is to tell you the consequences of your actions
if we don't
just fall over dead really quickly then
you know news keeps coming
there's more excitement
there's more fear, nervousness
maybe there's even like a few months
of horror where it's clear that things are going down
and not well if you are
a kind of thing that looks back and was like
yeah I'm as my life
ends I'm fine with having invested in AI
companies go ahead and invest
I realize that that may sound a bit
passive aggressive but
all I can I
feel like the thing that is my place to do is try to
time minds
together across time try to
have people aware of the futures that they're
capturing in their paths and there
there's maybe something to be said about what is true
what is real what is predictable
what you can bet on in a prediction market
it's kind of difficult to bet on the end of the
world but hey you can like go to manifold markets
and do it anyway with play money
something doesn't quite sit right to me about
me being like
you have a moral imperative to
maybe somebody else can do it
well I felt like I would
it would be hypocritical not to ask the question
I guess the last
one and maybe the most important one
so give me
you came out here originally and you didn't have a fedora
on and I was
disappointed and I made you go back in and
I asked for the fedora
how did that come to be has this become a branding
thing I always think of you interviews
and fedora it's probably a little bit of a branding
thing not to the point where I might just not suddenly
change it one day
I mean the way it started for me is that I tried
various hats on and I like the way
a fedora looked and
people have opinions about that but they're not dating
me so their opinion they don't get
to issue change
requests for my fashion
now
since the fedora became an online issue
I did is it an online
issue do people weigh in on this online
yeah so
I did say like
look I don't accept fashion
change requests from people who are not dating
me if you want me to stop wearing
fedora you know what you have to do
top tweet bottom
tweet in particular you need to suggest some alternate
form of hat wear
which my like actual council of girlfriends
will like better than a fedora
and I can get them to have me
change my you know change my hat gear
so they
people have in fact like submitted various suggestions
to eliezer.girlfriends
at gmail.com
and they're debating it so it's possible
have like a sudden change of hand gear at some point
if they can like settle on a piece of head gear
they like better
and meanwhile you know like
I guess it's slightly iconic and I don't mind
we can do a follow up episode for the great reveal
when you're changing the head gear
I wanted on this show first
I'm not trying to promise that one
well thank you for doing this
for the fedora
you're welcome sir
that's the sign off there you go
that's exactly give the people what they want
you
you
you
you
you

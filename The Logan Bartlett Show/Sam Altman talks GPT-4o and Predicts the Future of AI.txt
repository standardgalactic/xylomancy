We've had the idea of voice control computers for a long time.
They've never, to me, felt natural to use.
And this one, the fluidity, the pliability, whatever you want to call it,
I just can't believe how much I love using it.
Welcome to the Logan Bartlett Show.
On this episode, you're going to hear is a conversation I have with co-founder
and CEO of OpenAI, Sam Altman.
Now, if this is your first time listening to the Logan Bartlett Show,
this is a podcast where I discuss with leaders in technology,
as well as investors, some of the lessons that they've learned
in operating or investing in businesses, mostly in the technology field.
And this discussion with Sam is a little bit different,
in which I pushed on a number of things related to artificial intelligence,
as well as where OpenAI is headed, given how topical it is in the news,
and Sam's perspective on such a leading frontier that is artificial intelligence.
You'll hear that discussion with Sam here now.
Thanks for doing this.
Yeah, of course.
All right, I want to start off easy.
What's the weirdest thing that's changed in your life
in the last four or five years running OpenAI?
Like, what's the most unusual shift that's happened?
I mean, quite a lot of things, but the sort of inability to just be like
mostly anonymous and public is very, very strange.
I think if I had thought about that, I would have said, OK,
this is like a weirder.
This would be a weirder thing than it sounds like.
But I didn't really think about it.
It's like a much weirder thing.
It's like a strangely isolating way to live.
You believed in AI and the power of the business.
So did you just not think through the derivative implications of running
something that wasn't powerful?
I think I didn't think.
There were all of these other things like, oh, yeah,
I was going to be like really important.
OpenAI is going to be a really important company.
I didn't think I would like not be able to like go out to dinner in my own city.
That's weird.
That's weird.
You made an announcement earlier today.
We did.
Multimodal 4.0.
Yeah.
That's the omega sign, right?
Oh, just like omni.
Yeah, omni.
OK, sorry.
It works across text, voice, vision.
Can you speak to why this is important?
Because I think it's like an incredible way to use computer.
That this fact, we've had like voice.
The idea of like voice control computers for a long time.
You know, we had Siri and we had things before that.
They've never, to me, felt natural to use.
And this one, many different reasons.
What it can do, the speed, adding in other modalities, the inflection,
the naturalness, the fact that you can do things like say, hey,
talk faster or talk in this other voice and that it's that the.
The fluidity, the pliability, whatever you want to call it.
I just can't believe how much I love using it.
Yeah, Spike Johnson would be proud.
It's are there use cases that you've gravitated to?
Well, I've only had it for like a week or something.
But one surprising one is putting my phone on the table
while I'm like really in the zone of working.
And then without having to like change windows or change what I'm doing,
using it is like another channel.
So I'm like working on something I would normally like stop
what I'm doing, switch to another tab, Google something, click around or whatever.
But while I'm like still doing it to just ask and get like an instant response
without changing from what I was looking at on my computer,
that's been a surprisingly cool thing.
What actually made this possible?
Was it an architectural shift or more compute?
I mean, it was like all of the things that we've learned over the last several years.
We've been working on audio models.
We've been working on visual models.
We've been working on tying them together.
We've been working on more efficient ways to train our models.
It's not like, OK, we unlocked this one crazy new thing all at once,
but it was putting a lot of pieces together.
Do you think you need to develop like an on-device model
to decrease latency to the point for usability?
For video, maybe it would be hard to deal with network latency at some point.
Like the thing that I've always thought would be super amazing
is to put on someday a pair of AR goggles or whatever
and just speak the world in real time and watch things change.
And that might get harder over network latency.
But for this, two, three hundred milliseconds of latency feels super,
like it feels faster than a human responding to me in many cases.
Is video in this case images?
Oh, sorry, I meant video if you wanted like generated video, not input video.
Got it, got it.
So currently it's working with actual video as is.
Well, like frame by frame.
Frame by frame.
OK, got it.
You're a little recently to chat GPT, maybe not being there.
The next big launch not being GPT five.
It feels like there's been sort of an iterative approach to model development
that you guys have taken.
Is it fair to say that's how we should think about it going forward
that it's not going to be some big launch here's chat GPT five, but instead?
We honestly don't know yet.
I think that definitely one thing I've learned is that AI and surprise
do not go well together.
And although, you know, the traditional way of technology launches products,
we should probably do something different.
Now, we could still call it GPT five and launch it in a different way,
or we could call it something different.
But I don't think we figured out how to do the naming of branding for these things yet.
Like it made sense to me from like GPT one to GPT four at the launch.
Now, obviously, GPT four has continued to get much better.
We also have this idea that there's going to be like, you know, maybe there's
like one underlying kind of like virtual brain and it can like think harder
in some cases than others, or maybe it's different models.
But maybe users care if they're different or not.
So I don't think we know the answer to how we're going to like product market
all of this yet.
Does that mean maybe that the needs of the compute to make incremental
progress on models might be less than what it's been historically?
I sort of think we'll always use as much compute as we get.
Now, we are finding incredible efficiency gains, and that's really important.
One of the, you know, the cool, the cool thing that we launched today is obviously
the voice mode, but maybe the most important thing is we were able to make
this so efficient that we're able to serve it to free users.
Like best model in the world by a good amount, if you will look at that little
thing served to like anybody who wants to download chat GPT for free.
And there's a remarkable efficiency gain over GPT four and GPT four
turbo and we have a lot more to gain there.
I've heard you say that chat GPT didn't actually change the world in and of
itself, but maybe just changed people's expectations for the world.
Yeah.
Like I don't think you can find much evidence in the economic measurement
of your choice to chat GPT really inflected productivity or whatever.
Maybe customer support.
Maybe some, maybe some areas.
Some are like, if you look at like global GDP, you know, can you detect
when chat GPT launch?
Probably not.
Is there a point that you think we'll be able to determine a GDP?
Yeah, I don't know if you'll ever be able to say like, this was the one model
that did it, but I think if we look at the graph a couple of decades in the
future, be like, hmm, something changed.
Yeah.
Are there applications or areas that you think are most promising?
In the next 12 months, I'm sure I'm biased just because of where we do
here, but coding I think is a really big one.
Kind of related to the bitter lesson.
You spent some time recently talking about the difference between deeply
specialized models trained on specific data for specific purposes versus
generalized models that are capable of true reasoning.
I would bet that it's the generalized model that's going to matter.
And what is the most important thing there as you think about like someone
that's focused singularly on a data set and all the integrations associated
with something very narrow.
If the model can do generalized reasoning, if it can like figure out new
things, then if it needs to figure out how to work with a new kind of data, you
can feed it in and it can do it, but it doesn't go the other way around.
Like a bunch of specialized models that I don't think a bunch of specialized
models put together can't figure out the generalized reasoning.
So the implications for that of coding specific models probably would be.
I think a better way of saying this is I think the most important thing to figure
out is the true reasoning capability.
And then we can use it for all sorts of things.
What do you think the principal means of communication between humans and AI is
in two years?
Natural language seems pretty good.
I'm interested in this general idea that we should design a future that humans
and AIs can sort of use together, use in the same way.
So I'm like more excited about humanoid robots than I am for other forms of
robots because I think the world is like very much now designed for humans.
And I don't want that to get reconfigured for some more efficient kind of thing.
I like the idea that AIs, that we talk to AI in language that is like very
well human optimized and that they even like talk to each other that way.
Maybe I don't know.
But I think this is generally an interesting direction to push.
You said recently something to the effect of the models might ultimately get
commoditized over time.
But the most important thing would likely be the personalization of the models
to each individual.
First, do I have that right?
I'm not certain on this, but I think it's like a thing that I would that would
seem like reasonable to me.
Then beyond personalization, do you think it's just normal business UI and
ease of use that ultimately wins for end users?
Those will for sure be important.
They always are.
I can imagine other things where there's like a sort of marketplace or
a network effect of course that matters where we want our agents to communicate.
There's different companies in an app store, but I sort of think that the rules
of business kind of generally apply.
And whenever you have a new technology, you're tempted to say they don't, but
that's always like fake news and not always usually fake news.
And all of the traditional ways that you create during value will still matter here.
When you see open source models like catch up to benchmarks and all of that,
what's your reaction to it?
Is that I think it's great.
Yeah.
I mean, I think that there are,
you know, like many other kinds of technology, there will be a place for open
source, there'll be a place for like hosted models and that's fine.
It's good.
I'm not going to ask about any specifics related to this, but there have been
my answer.
There's been press reports related to looking to raise major amounts of money.
Wall Street Journal, I think was a credible one to galvanize investment in fabs.
Semi industry, ATSMC and NVIDIA have been ramping pretty aggressively to meet
expectations of the need for AI infrastructure.
You recently said that you think the world needs more AI infrastructure and
then you said a lot more AI infrastructure.
Is there something you're seeing on the demand side that would require way more
AI infrastructure than what we're currently getting at ATSMC and NVIDIA?
So first of all, I'm confident that we will figure out how to bring costs to
deliver current systems way, way down.
I'm also confident that as we do that, demand will increase by a huge amount.
And third, I'm confident that by building bigger and better systems, there will
be even more demand.
We should all hope for a world where intelligence is too cheap to meter.
It's just wildly abundant.
People use it for all sorts of things and you don't even think about why they're
like, oh, you know, do I want this?
Do I want this like reading all my emails and responding to them for me?
Or do I want this like curing cancer?
Of course you pick curing cancer.
But the answer is like, you'd love for it to do both things.
And I just want to make sure we have enough for everybody to have that.
I don't need you to comment on your own personal efforts here, although again,
if you want to, please let me know.
But humane and limitless and some of these like different physical device
assistance, what do you think those have gotten wrong?
Or where do you think the adoption maybe hasn't met user desires just yet?
I think it's just early.
I have been an early adopter of many types of computing.
I had and very much loved the compact TC 1000.
It's like when I was freshman in college, I thought it was just like so cool.
And like that was a long way from the iPad, long, long way from the iPad.
But, you know, it was directionally right.
Then I got a trio.
I was like the I was very not cool college kid.
I had like a old palm trio.
And when it was like a that was not a thing that kids had.
And that was a long way from the iPhone, but we got there eventually.
And, you know, these things feel like a very promising direction
that's going to take some iteration.
You mentioned recently that a number of businesses that are building on top
of GPT-4 will be steamrolled, I think was your term, by future GPT.
I guess, can you elaborate on that point in second?
Like, what are the characteristics of AI first businesses
that you think will survive GPT's advancement?
The only framework that I have found that works for this is you.
You can either build a business that bets against the next model
being really good or a model that bets on that happening and benefits
from it happening.
So if you're doing a lot of work to make one use case really work
that was just beyond the capability of GPT-4, GPT-4.
Oh, no. And then you get it to work.
But then GPT-5 comes out and it does that and everything else really well.
You're kind of like sad about the effort you put into that one thing
to get it to barely work.
But if you had something that just like kind of worked OK
across the board and people are finding things to use for,
but you didn't put in like tons of work to make this one thing kind of possible.
And then GPT-5 or whatever we call it comes along
is just way better.
Everything you like, you got the rising tide, lifted all your boats effect.
You know, what I would suggest is like you're not building an AI business.
In most cases, you're building a business and AI is a technology that you use.
In the early days of the app store, I think there were a lot of things
that like filled in some very obvious crack and then eventually
Apple fixed that and there wasn't, you know, you didn't keep needing
like a flashlight app from the app store.
It's just like part of the OS and that was like going to happen.
And then there were, I think, things like Uber that were enabled by having smartphones,
but really built a very defensible long term business.
And I think you just want to go for that category.
I can come up with a lot of incumbent businesses that leverage you all that fit that
framework in some ways.
Are there any like novel types of concepts that you sort of think is
in that example, the Uber and it doesn't need to be.
It could be a real company if you think of one or even if it's a toy or just something
that's interesting that you think is like enabled in that way.
I would actually bet on the new companies for like many of these cases.
A very common example people use is trying to build like the AI doctor,
like the AI diagnostician.
And people talk about, oh, I don't want to do a startup here because,
you know, Mayo Clinic or Take Your Pick is going to do it.
And I would actually bet it's a new company that does something like that.
Do you have any advice for CEOs beyond that who want to be proactive about preparing for
these types of disruptions?
I would say like bet that intelligence as a service gets better and cheaper every year.
And it is necessary, but not sufficient for you to win.
So the big companies that take years to implement this, you can like beat them.
But every other startup that's, you know, paying attention is going to do this too.
And so you still have to figure out like, what's the long-term defensibility of my business now?
That the playing field is way more open than it's been in a long time.
There's incredible new things to do.
But you don't get a pass on like the hard work of building enduring value,
even though you can now do it in more ways.
Is there a job title or a type of job responsibility that you could envision
existing or being mainstream in five years because of AI that like is maybe niche or
non-existent today?
That's a great question.
And I don't think I've ever gotten it before.
It's people always ask like, what job is going to go away?
The new one is a more interesting question.
Let me think for a second.
I mean, there's like a lot of things that I could talk about that
I think are sort of less interesting or less huge.
What I'm trying to do is like come up with the areas of like,
what will 100 million people do or 50 million people do?
The broad category of new kinds of art, entertainment, sort of more like human-to-human
connection.
I don't know what that job title is going to be.
But I think, and I don't know if this like we get there in five years,
but I think there's going to be a premium on like human in-person like fantastic experiences.
I don't know what we'll call that.
But I could see that being like a very huge category of something new that we do.
The most recent public tender of open AI was 90 billion or something about there.
Are there one or two things that you sort of look at as milestones that will get open AI to be a
trillion dollar company short of AGI?
I think if we can just keep improving our technology at the rate we've been doing it
and figuring out how to continue to make good products with it and revenue keeps growing
like it's growing, I don't know about specific numbers, but I think we'll be fine.
Is the business monetization model today the one that you think creates the
one trillion dollar equity value?
I mean, the GPD subscription model like really works well for us.
Like surprisingly, I wouldn't have bet on that.
I wouldn't have been confident it's going to do as well as it has, but it's been good.
Do you think post-AGI, whatever that term actually means, will be able to, I don't know,
ask AGI what the monetization model is that might be different?
Yeah, should be able to.
I think we maybe saw in November, not to rehash that the existing open AI structure
left some things to be desired, which I don't think we need to rehash in total.
You talked about it enough, I think.
But you spoke into making changes along the way.
What do you think the appropriate structure is going forward?
I think we're close to being ready to talk about that.
We've been hard at work on all sorts of conversations and brainstorming there.
I think hopefully in this year, I think we'll be right to talk about this calendar year.
Can you tell me first?
When Larry and Brett Taylor got Battlefield promoted to board directors,
I was waiting for what my call never came through.
But one of the interesting things I think about preconceptions around AI to your point
on the monetization model and all that is, I think we've all, I've heard you speak about it,
manual work obviously first, followed by white color, followed by creative.
Obviously, it's proven to be kind of the opposite in some ways.
Are there other things that are counter-intuitive that you've looked at being like, well,
I would have presupposed it to be this way, but it's actually proven to be the exact opposite.
That's definitely the mega surprise to me, the one that you mentioned.
I don't think I would have expected it to be so good at legal work so early,
just because I think of that as like a very precise, complex thing.
No, definitely the big one is the observation of physical labor, cognitive labor, creative labor.
For those that haven't heard, you make the point about AGI and why you dislike the term.
Can you elaborate on that point?
Because I no longer think it's like a moment in time.
Obviously, I have so many naive conceptions when you start any company,
and particularly in a field that's moving around as much as this one is.
My naive conception when we started is that we would get to a moment
where we didn't have AGI and then we did, and it would be a real discontinuity.
I still think there's some chance of a real discontinuity, but on the whole,
I think it's going to look much more like a continuous exponential curve where
what matters is the pace of progress year over year over year.
You and I will probably not agree on the month or even the year that we're like,
okay, now that's AGI. We can come up with other tests that we will agree with,
but even that is harder than it sounds. GPT-4 is definitely not over a threshold that I think
almost anyone would call an AGI, and I don't expect our next big model to be either,
but I can imagine that we're like only maybe one or two or some small number of ideas away
and a little bit more scale from something we're like, this is now kind of different.
And I think it's important to stay vigilant about that.
Is there a more modern like Turing test, we can call it the Bartlett test,
where that you think like, hey, when it crosses this threshold,
I think when it's capable of doing better research than like all of OpenAI put together,
even one OpenAI researcher, that is like a somehow very important thing that feels like
it could or maybe even should be a discontinuity. Does that feel close?
Probably not, but I wouldn't rule it out. What do you think the biggest
obstacles that you see to reaching AGI, it sounds like you think maybe the scaling laws
have runway currently and holding for the next couple of years?
Yeah, I think the biggest obstacles are new research. And one of the things I've had to
learn shifting from like Internet software to AI is research does not kind of work on the same
schedule as engineering, which usually means it takes much longer, it doesn't work, but sometimes
means it works tremendously faster than anyone could have predicted.
What is that? Can you elaborate on that point that it's like not as linear in progress?
I think the best way to elaborate on that is like historical examples. I'm going to get the
numbers wrong here, but I'm sure no one will try to correct you.
Someone will. I think the neutron was first theorized in the early 1900s.
It was maybe first detected in the 10s or 20s. And the work on what became the atomic bomb
started in the 30s and happened in the 40s. From not really having no idea that there was
even the idea of a thing like a neutron to being able to make an atomic bomb and just break all
of our intuitions about physics, that's wildly quick. There are other examples that are sort of
less pure science. There's the famous quote about the Wright brothers. Again, I'm going to get the
numbers wrong here, but let's say it was 1906. They said they thought flight was 50 years away
in 1908. They did it, whatever, something like that. And then many, many other examples
throughout the history of science and engineering. There's also plenty of things that we theorize
that never happen or take decades or centuries longer than we thought, but sometimes it does
go really fast. Interpreability, where are we on this path and how important is that long-term
for AI? There's different kinds of interpretability. So there's the like, do I understand every
what's happening at every mechanical layer through the network? And then there's,
can I look at the output and say there's a logical flaw here or whatever? I am excited about
the work going on at OpenAI and elsewhere in this direction. And I think that interpretability
as a broader field seems like promising and exciting. I won't pin you down. I assume you'll
have a nice announcement when you're ready to say something. But do you think that that is
going to be a requisite to mainstream AI adoption, maybe within enterprises or something?
GBT-4 is like quite widely mentioned at this point. Yeah, that's right. There's maybe a few
things that I think you could ask questions about or maybe accuse is too strong of a term,
but that people are suspicious about. One of which is, I think there's this needle threading
that exists between being excited about AGI, but also feels like you have a
personal kind of apprehension about you, Sam, OpenAI, generally being the ones to
harness it and unilaterally make decisions, which has led to some body, some governmental
structure where there's elected leaders instead of you making these decisions.
Yeah, I think for like, I think it'd be a mistake to go regulate, heavily regulate current
capability models, but when the models, which I believe they will, pose significant catastrophic
risk to the world, I think having some sort of oversight is probably a good thing. Now,
there is some needle threading about where you set those thresholds and how you test for it,
and it would be a real shame to sort of stop the tremendous upsides of this technology and letting
people that want to go train models in their basement be able to do that. That'd be really,
really bad, but if we have international rules for nuclear weapons, it's good there.
The regulatory capture group, which I'm sure we can think of, which VCs fall into that
bucket of accusatory around this regulation. What do you think they don't see about the
potential risks inherent in AI? Well, I think they just don't get, I don't think they're like,
on the whole seriously wrestled with AGI. These were also people who like,
some of the loudest voices about AI regulatory capture were totally decrying it as a possibility,
not that long ago, not all, but I do have empathy for where they're coming from, which is like,
regulation has not been really bad for technology. Like, look what happened to the European
technology industry. Like, I get it. I really do. And yet, I think that there is a threshold
that we are heading towards, above which we may all feel a little bit different.
Do you think open source models themselves present inherent danger in some ways?
No current one does, but I could imagine one that could.
I've heard you say that safety is kind of a false framing in some ways, because it's more of a
discussion about what we explicitly accept, like airlines.
Yeah, it's more like safety is not a binary thing. Like, you are willing to get on airplanes
because you think they're pretty safe, even though, you know, they crash once in a while.
And what it takes for to call an airline safe is like, a matter of some discussion,
some people have different opinions on. And it's a topical point right now.
Topical point right now. They have gotten just unbelievably safe overall, like triumphantly
safe. But safe does not mean no one will have to die on an airplane.
Similarly, medicine, we really speak with side effects and some people have adverse
consequences around it. And then there's the implicit side of safety as well,
like social media, right, or things that have negative association. Is there something that
you could imagine seeing on the safety paradigm that would cause you to act differently than
pushing forward? Yeah, we have this thing called our preparedness framework that's
sort of exactly that, saying that, you know, in these categories at these levels,
we'd act differently. I've had LASer on the podcast.
How was that? It was wonderful. We sat for the longest podcast I've ever done. I think it was
four hours of us. He has more free time than me, so I apologize. I can't go that long.
We can do multiple sessions. We don't need to do them all now. I think that his points,
I think, stay fairly consistent. I'm grateful he exists.
He's a very interesting guy to sit down with for four hours and talk. We went a bunch of
different directions. But I'd be remiss, as a friend of the pod, to not ask a fast takeoff
question. I'm curious, like, there's so many different fast takeoff scenarios. And one of the
constraints that I think we point to today is just a lack of AI infrastructure, right?
And I guess if there was some researcher who developed a modification to the current
transformer architecture, where suddenly the amount of data and hardware scale needed drastically
reduced more like human brain or something like that, is it possible we could see like a fast
takeoff scenario? Possible, of course. And it may not even need a modification. It is still not
what I believe is the most probable path, but I don't discount it. And I think it's important
that we consider it in the space of what could happen. I think things will turn to be more
continuous, even if they're accelerating. I don't think we're likely to go to sleep one day with
pretty good AI and wake up the next day with genuine superintelligence.
But even if the takeoff happens over a year or a few years, that's still fast in some sense.
There's another question about, even if you got to this really powerful AGI,
how much does that change society on the next day versus the next year versus the next decade?
And my guess is, in most ways, it's not a next day or next year thing, but over the course of a
decade, then the world will look quite different. I think the inertia of society is like a good,
helpful thing here. One of the things I think people also find they have suspiciousness around.
I imagine the questions you don't love getting are Elon, equity, and November board structure.
Those are probably the three. Which one of those do you like the least?
I don't hate any of them. I just don't have any new to say on any of them.
Well, I guess I'm not going to ask the equity one specifically because I think you've answered
that in more than enough ways. Although people still don't seem to like the answer that enough
money is a thing. Yeah, if I made a trillion dollars and then gave it away, it would fit with
I think the expectation or the sort of way it's usually done.
There was another saying I thought about. Oh, that's true.
Trying that in some ways. Yeah, comparatively. No, I just mean like most people who make a ton
of money. Yeah. What do you feel like your motivations, this pursuit of AGI, outside of the
equity, I think most people take solace in the fact that like, oh, well, even if I have some
higher mission, I still get paid for it in some ways. What are your motivations now coming into
work every day? Like what's the most fulfillment derived from? Look, I tell people this all the
time, I'm willing to make a lot of other life tradeoffs and sacrifices right now because I think
this is the most exciting, most important, like best thing I will ever touch. And it's an insane
time and I'm happy it won't be forever. Like, you know, someday I get to go retire on the farm and
I'll remember this fondly, but be like, oh man, those were stressful, long, long stressful days.
But it's also just incredibly cool. Like I can't believe this is happening to me. It's like this
is like amazing. Is there a single moment, I guess we go back to the fame example of not being able
to go out in your city or whatever, but has there been a single moment that was most surreal that
like, oh geez, I don't know. I mean, you've done a podcast with Bill Gates. I'm sure you have your
speed dial. If I took your phone right now would have a lot of very interesting people on it.
Was there a single moment over the course of the last couple of years that you were like,
this is a uniquely surreal moment? And kind of every day there's something that's like, wow,
if I could like, if I had like a little bit more mental space to step back, it's like,
this would be crazy. Kind of a fish in water. But yeah, it is kind of like that effect.
After all of that, like, November stuff happened that, you know, like that day or the next day
or whatever, I got like, I don't know, 10, 20 texts, I mean, like that from like
major world like presidents, prime ministers of countries, whatever. And that was not the weird
part. The weird part was that happened. And I was like, you know, kind of responding saying like,
thanks or whatever. And it felt like very normal. And then we had these like insane,
super jammed like four and a half days and just this like crazy state. And it was just like weird,
like not sleeping much, not really eating energy levels, like very high, very clear,
very focused, but just like your body was like in some weird, like a adrenaline charge state for a
long time. And then it was like, all this happened that week before Thanksgiving, and it was kind of
crazy, crazy, crazy, got resolved on Tuesday night. You canceled our podcast. Canceled our
podcast. Sorry, I don't usually cancel things. Anyway, then on that Wednesday, like now it's
the Wednesday before Thanksgiving, Ali and I drove up to Napa and stopped at this diner. God,
it's very good. And on the drive up there, I realized I hadn't like eaten in like days. And
then all of a sudden, like kind of like normal, it was just like, okay, you know, this is like
normally where we'd be doing on weekend, heading up like, go like, whatever. And go to God's order,
like four entrees, like heavy, like fried, like heavy entrees, like two milkshakes just for me.
And I was sat there and ate. And it was very satisfying. And as I was doing that, one of them,
president of this one country texted again and just said, like, oh, I'm so happy it's always
off like great, whatever. And then it hit me that like, oh, yeah, like all of these people had texted
me and it wasn't weird. And the weird part was like realizing that that had like happened in the middle
of it. And that that should have been this very weird experience. And it wasn't. So that was like
one that sticks out. Yeah, that is interesting. My takeaway is human adaptability to almost
anything is just like much more remarkably strongly. We realize, and you can get used to
anything as the new normal, go to our bad pretty fast. And I kind of like, over the last couple
of years, have learned that lesson many times. But I think it says something remarkable about
humanity and good for us and good as we stare down this like big transition.
I remember post 9-11, I'm sure you remember exactly where, but I was in Southern New Jersey in our
town, you know, whatever dozens of people passed away. And the how close the town came together
after a terrorist attack happened. And it seemed so normal, like that it was just the normalcy of
it. Or I have friends in Israel right now, and you talk to them about it. And they're like,
no, it's normal. I'm like, well, there's a war going on. Like it's got to be surreal. And they're
like, well, I mean, what are you gonna do? You go about your day, you go get your food, all that.
And it's amazing, these psychological impacting things. At the end of the day, we need to go get
food and we need to, you know, talk to our friends and all this stuff. So it is amazing how much
that can happen. Really, like genuinely, that's been my big surprising takeaway to like feel it
so viscerally. As you think about like models becoming smarter and smarter, what you kind
of touched on this a little bit earlier with the creative element, like what do you think
remains uniquely human as models start doing more and more capabilities of what we used to
consider? I think many, many years from now, humans are still going to care about other
other humans. I, you know, I was reading the internet a little bit, and I was like, oh,
everyone's going to fall in love with chat to PT now. And everybody's, you know, like, it's
going to be the chat to PT girlfriend, whatever, whatever. I bet not. I bet we're, I think we're
so wired to care long term about other humans and all sorts of like, big and small ways that
that's going to remain like our obsession with other people. Sounds like you hear a lot of
conspiracy theories about me. You probably don't hear a lot of conspiracy theories about AI.
You might not care if you did hear one. I think we're like not going to watch robots play each
other in soccer probably as our like main hobby. As you run open AI, the company itself and you,
you built a lot of rules or frameworks at YC on how to run businesses. And then you've,
you've broken a lot of some. Are there, are there different types of people you hire for this
business than you would have had you started a consumer internet company within the executive
ranks or a B2B software company or something? Researchers are very different than my product
engineers for the most part. Brad or Mira or some of the executives like researchers are
unique, but does open AI bring in a different type of executive or do you hire for a different
trade? So I mostly have not like, I am sometimes you hire externally for executives, but I'm a
big believer that if like, you generally promote, it's not, it's probably a mistake to only promote
people to be executives because that could reinforce a monoculture. And, you know, I think
you want to bring in some new, very senior people. But we mostly like homegrown talent here. And I
think that's a positive given how different what we do is from what you would do somewhere else.
Is there a decision that you've made over the course of open AI
that, that felt the most important at the time of making it? And how did you go about making it?
It'd be hard to point to just a single one, but the decision that we're going to do what we call
iterative deployment, that we're not going to go build a GI and secret and then put it out into
the world at once, which was the prevailing wisdom and the LA's are planning others. I think that
was like a quite important decision we made. And it felt like a really important one at the time.
If another company that betting on language models was an important decision and felt like
an important one at the time, I actually don't know the story of betting on language models.
How did that come to be originally? Well, we were, we had these other projects we were doing,
the robot thing and video games. And there was a very small effort started with one person looking
at, looking at language modeling. And Ilya really believed in it, really believed in like
the general direction became language models, let's say. And we did GPT-1, we did GPT-2,
we started the study scaling laws, scaled GPT-3, and then we made a big bet this was what we were
going to do. And it was not, it looks so, all of these things look so obvious and interesting,
but they really don't feel that way at the time. One other thing you brought up recently was the,
there's two approaches to AI, the replication of yourself and then the smartest employee.
Oh, it's not, not AI itself, but like how you want to use it. Like when you imagine using your
personal AI, which is a subtle distinction when you said it, but can you, can you expound it?
Because it seemed like a fairly profound distinction of how at least Sam thinks about
the future of AI use cases. So can you explain that point again? Cause clearly I misunderstood it.
If you're going to text me in, you know, five years in the future, I think you want to be clear
of whether you're texting me or my AI assistant. And then if it's my AI assistant that's going to
like, you know, bundle messages together and you'll get a reply later, or, or, you know,
if it can easily do something, you might ask my human assistant to do it and fine, you'll know that.
I think there will be value in keeping what those things are separate and not that it's like,
all right, the AI is truly just an extension of Sam. I don't know if I'm talking to Sam or Sam's
AI ghost, but that's okay because it's the same thing. It's this merged entity. I think, I think
there will be like Sam and Sam's AI assistant. And also I want to ask for myself, like, I don't
want to feel like this thing is just like this weird extension of me, but that it's a separate
entity that I can communicate with across a barrier. You see it in, in music or creative,
where it becomes pretty easy to replicate a Drake or a Taylor Swift audio, we probably need some
form of validation or some centralization that validates say this is actually the, the creative
work of XYZ person. You're probably going to want some version of that at a personal level too.
Yeah, but it's like, you know, the way I think about like open AI is it's, I don't, like,
there's different people and I'm asking them to do things and they go off or they ask me to do
things and I go off. But it's not a single, like, board. And I think that's like a way we're all
comfortable. And so, so what is that? Can you, can you tie that back? Like the, the decentralization
of letting individuals do their...
Well, also that, but I meant more just kind of like, what is the abstraction of what my
personal AI is going to be like? Like, do I think of that as this is just me and it's going to like
take over my computer and do what's best and because it's me, that's going to be totally fine.
And it's answering messages on my behalf and it's, you know, going to just like,
I'm slowly going to like take my hands off the controls that it's slowly going to like be me.
Or do I think of this as like, this is a really great person I work with that I can say, hey,
can you do this thing you're back when you're done? But I think of it as not me.
As you think about the educational system and as we think about like the class of,
college class of 2030 or 2035 or whatever, whatever, some, some group in the future.
Are there changes specifically that you think should be made within the college educational
system to prepare people for the future we have?
The biggest one is I think people should not only be allowed, but required to use the tools.
There will be some cases where we want people to do something the old-fashioned way because it
helps with understanding. You know, like I remember sometimes in math class or whatever,
there'd be something you can't use. No calculators on the test.
Yeah. But on the whole, like in real life, you get to use the calculator.
And so you need to understand it, but then you got to be proficient in using the calculator too.
And if you did math class and never got to use the calculator, you would be like a less,
less good at the work you need to do later. You know, if all of the open air researchers never
got to use a calculator, open air probably wouldn't have happened. Computers at least, you know.
We don't try to teach people not to use calculators, not to use computers.
And I think we shouldn't train people not to use AI either. It's just going to be an important part
of like doing valuable work in the future. Last one. In planning for AGI and beyond,
you wrote the first AGI will be just a point along the continuum of intelligence,
which we spoke about earlier. We think it's likely the progress will continue from there,
possibly sustaining the rate of progress we've seen over the past decade for a long period of time.
Do you ever personally stop and process or visualize like what the future
will look like in that? Or is it just too abstract to contemplate?
All the time. I mean, I don't visualize it like, you know, we have these like flying cars in a
Star Wars Future City and I like that. But like, definitely what it means when
one person can do the work of hundreds or thousands of well-coordinated people
and what it means when, I don't want to say we can discover all of science, but kind of what it
feels like, like what it would feel to us is if we could discover all of science.
Be pretty cool. Yeah.
Sam, thanks for doing this. Thank you.
Thank you for listening to this episode of the Logan Barlett Show with CEO and co-founder
of OpenAI, Sam Altman. If you enjoyed this conversation, really appreciate it. If you like
and subscribe and share with anyone else that you think might find interesting,
as well as come back for next week where we'll have another exciting episode with a different
founder and CEO of an important company in technology. Thanks everyone for listening and have a good week.

I really freaked out about this stuff in 2018 or 2019 or so.
I didn't know if I was crazy or prescient.
I still don't know for sure.
Welcome to the Logan Bartlett Show.
On this episode, what you're going to hear is a conversation I have with CEO and co-founder
of Anthropic, Dario Amade.
We talk about a bunch of different things, including predictions for the future of artificial
intelligence, his upbringing, how he came to found Anthropic and spin out from open AI,
as well as what problems he thinks can be solved by artificial intelligence in the near future.
Really interesting conversation with a very thoughtful person in an important position
of power, running one of the leading AI companies in the world.
If we can avoid the downsides, then this stuff about curing cancer, extending the human lifespan,
solving problems like mental illness.
This all sounds utopian, but I don't think it's outside the scope of what the technology can do.
So a chance that something goes quite catastrophically wrong on the scale of human civilization,
you know, it might be somewhere between 10 and 25%.
If you're enjoying this discussion and conversation with Dario,
please do subscribe to this podcast on whatever channel you're listening to.
And if you're here to listen to more topics of artificial intelligence,
I would encourage you to subscribe to Red Point's podcast on the subject,
Unsupervised Learning, which you can find on any podcast player now.
Dario, thanks for doing this.
Thanks for having me.
So I normally don't tell people's backgrounds in a linear fashion,
but I actually haven't heard yours.
I don't know if you've ever really told it in earnest, like childhood growing up,
sort of what led you to starting Anthropics.
So maybe can you share a little bit about like your background, childhood growing up?
Yeah, yeah.
So I don't know that my childhood was that interesting or that different from,
you know, from people who are intact or found companies.
I mean, I was always really interested in math.
It felt like it had, you know, a sense of objectivity, right?
You know, one kid could say, oh, this show is great.
And another kid could say, oh, it's terrible.
But, you know, if when you're doing math, you're like, oh, man, there's an objective answer to this.
So that was always very interesting to me.
And, you know, I grew up with a younger sister who's one of my co-founders,
and we always wanted to save the world together.
So it's actually kind of amusing that, you know, we're working on something together
that, you know, at least potentially could have very, very wide scope.
So, yeah, I mean, in terms of how I got from there to Anthropic,
my interest in math led me to be, you know, physics major undergrad.
But near the end of undergrad, I started reading, I was initially the work of Ray Kurzweil,
who, you know, I think is a bit crazy about a lot of things.
But just the basic idea that there's this acceleration, there's this exponential
acceleration of compute, and that that's going to provide us enough compute somehow,
we had no idea how then we had no idea it was going to be neural nets, you know,
what will somehow get us to like very powerful AI.
And I found that idea really convincing.
So I was about to start grad school for theoretical physics and, you know,
decided as soon as I got there that I wanted to do biophysics and computational neuroscience,
because, you know, if there was going to be AI, you know, didn't feel like AI was working yet.
And so I wanted to study the closest thing to that, that there was, which was, you know,
our, which was our brains, you know, the closest, it's a natural intelligence,
so therefore the closest thing to an artificial intelligence that exists.
So I studied that for a few years and kind of worked on networks of real neurons.
And then, you know, shortly after I graduated, I was at Stanford for a bit,
and then I saw a lot of the work coming out of Google, of Andrew Inge's group at Stanford,
and so I said, okay, you know, I should, I should get involved in this area.
You know, my reaction at the time was, oh my God, I'm so late to this area.
The revolution has already happened.
And this was 2014, right?
So, so, you know, I was just like, oh my God, like, you know,
this tiny community of 50 people, like they're the giants of this field,
it's too late to get in.
If I rush in, maybe I can get some of the scraps.
That was, that was my mentality when I, when I kind of entered, entered the field.
And now, of course, it's, you know, it's, it's nine years,
nine years later than that.
And, you know, every, you know, I interview someone every day who's like,
you know, I really want to get into this field.
And so I ended up working with Andrew Inge at Baidu for a year.
I ended up working at Google Brain for a year.
Then I was one of the first people to join OpenAI in 2016.
I was there for about five years.
And by the end of it, I was VP of research, was driving a lot of the research agenda.
We built GPT-2, GPT-3, reinforcement learning from human feedback, which is, you know,
of course, the method that's used in chat GPT and used along with other methods in our,
in our model Claude.
And, you know, one of the big themes of those five years was this idea of scaling,
that you can put more data, more compute into the AI models and they just get better and better.
And I think that, you know, that thesis was really, was really central.
And the second thesis that was central is you don't get everything that way.
You know, you can scale the models up, but there are questions that are unanswered.
It's, you know, ultimately sort of the fact value distinction.
You scale the model up.
It learns more and more about the world, but you're not telling it how to act,
how to behave, what goals to pursue.
And so that, that dangling thread, that free variable was the second thing.
And so those were, those were really the kind of the two lessons that I learned.
And of course, those ended up being the two things that Anthropic is really about.
What year was it that you joined OpenAI?
It was 2016.
And what was the original connection?
Was it just that this seemed to be where the smart people were going?
Yeah.
So I was actually initially invited to join the organization before it existed in late 2015,
as it was, as it was forming and decided, decided not to, but then a few months after
it started, I kind of, you know, a bunch of smart people ended up joining.
And so I said, Oh, maybe I'll, maybe I'll do this after all.
You were there for a number of years.
And at some point you made the decision that Anthropic was going to be,
I guess it wasn't initially, you've had an unusual path.
It wasn't initially a company, right?
Originally it was started as a quasi research lab.
Is that fair?
I mean, our strategy has certainly evolved.
But actually it was a for-profit public benefit corporation since the beginning.
And we had, even since the beginning, we had something on our website saying,
you know, we're doing research for now, but, you know, we see the potential for commercial
activity down the road.
So I think all of these things we kind of kept open as potentialities.
But, you know, you're right.
For the first year and a half, it was mostly building technology.
And we were agnostic on, you know, what exactly we were going to do with that
technology or when we kind of, we kind of wanted to keep our options open.
We felt it was better than saying, you know, we're about this or we're about that.
And what was the thought at the time of, hey, we should go do something on our own?
Was that your thought?
Was it a group of people?
Yeah, it was, it was definitely the thoughts of a group of people.
So there were seven co-founders who left.
And then I think in total, we got 14 or 15 people from OpenAI, which was about 10% of the
size of the organization at the time.
It's funny to look back on those days because, you know, in those days,
we were the language model part of OpenAI.
Like, you know, we, along with a couple of people who stayed, you know, were those who
had developed and scaled the language models.
There were many different other things going on at OpenAI, right?
There was, you know, there was a robotics project, a theorem proving project, projects
to play video games.
Some of those still exist.
But, you know, we felt that we were this kind of coherent group of people.
We had this view about, you know, language models and scaling, which to be fair,
I think the organization supported.
But then we also had this view about, you know, we need to make these models safe in a
certain way.
And, you know, we need to do them within an organization where we can really believe
that these principles are incorporated top to bottom.
OpenAI had a whole bunch of different things and still does experimenting around.
Like, was it evident at what point along the way was it evident that large language models
were something that there was a lot of wood to chop and a lot of opportunity around?
Yeah.
I mean, I think, I don't know, it was obvious at different times to different people.
So, you know, for me, I think there were a couple of things.
You know, the general scaling hypothesis.
I wrote this document called The Big Blob of Compute in 2017, which I'll probably publish
at some point, although it's primarily of, you know, like historical interest now.
And so, very much in my mind, and I think the minds of, you know, like a small number of other
people on both the team that left and the team that didn't leave and, you know, some in other
places in the world as well, it was clear that there was really something to scaling.
And then as soon as I saw GPT-1, which was done by Alec Radford, who's still at OpenAI,
our team actually had nothing to do with GPT-1, but we recognized it immediately
and saw that the right thing to do with it was to scale it.
And so, for me, everything was clear at that moment and even more clear as we kind of scaled
up to GPT-2 when, you know, we saw that, you know, the model was capable of my favorite thing was like,
you know, we were able to get the model to perform a regression analysis.
So, you know, you'd give it like, you know, the price of a house and ask it to predict
number of square feet or something like that. You gave it a bunch of examples, then you gave
it one more price and you're like, how many square feet? It didn't do great, but it did
better than random. And in those days, I'm like, oh my God, this is like some kind of general reasoning
and prediction engine. Like, oh my God, what is this thing I have in my hands, right? It's
completely crazy. So, you know, it has been my view ever since then that, you know, this would be
not just language models, but, you know, language models as an exemplar of the kinds of things you
can scale up. You know, this would be, you know, really, really central to the future of technology.
Did you consider yourself like a founder or a CEO prior to actually doing it?
No. So, I really kind of never thought of myself that way. Like, if you went back to,
you know, me in childhood, it would have been very unsurprising, you know, for me to be a scientist.
But, you know, I never kind of thought of myself as like a founder or a CEO or a business person,
right? You know, I always thought of myself as a scientist, someone who discovers things.
But I think just having been at several different organizations convinced me that, in fact, I did
have my own vision of how to run a company or how to run an organization because I'd seen so many
and I thought, well, I don't know, I'd actually do it this way, right? And so, sort of the contrast,
you know, not that I disagreed with every decision that made, but just watching all these decisions
go by took me to the point where I'm like, actually, I do have opinions on these questions,
right? I do have an idea of how you would grow an organization, how you would run a research effort,
how you would bring the products of that research organization out into the world in a way that's,
you know, makes business sense, but is also responsible. I don't think I really had those
thoughts naturally, but as I was brought into contact with organizations that did that, then I
became excited about those things. I kind of almost reluctantly learned that I actually had
strong opinions on these things. Not to draw it in contrast to any specific names, but maybe
just in others in the field that I would consider large language foundation model
as a product. What's something foundational, I guess not to use a cute term, but something
foundational that you believe at Anthropic that you would draw in distinction to others in the space?
Yeah, I would say a couple things. One is just this idea that we should be building in
safety from the beginning. Now, I'm aware we're kind of not the only ones who've said that,
but I feel like we've built that. We've really built that in from the beginning. We've thought
about it from the beginning. We've started from a place of caution and kind of commercialized
things, brought things out into the world, starting from, hey, can we open these switches
one by one and see what actually makes sense? I think in particular, a way I would think about it
is that what we're aiming to do is not just to be successful as a company on our own, although we
are trying to do that, but that we're also trying to kind of set a standard for the field, set the
pace for the field. This is a concept we've called race to the top. Race to the bottom is
a popular term where everyone is competing to lower cost or delivers things as fast as possible,
and as a result, they cut corners and things get worse and worse. That dynamic is real,
and we always think about how not to contribute to it too much. But there's also a concept of
race to the top, which is that if you do something that looks better, it naturally has the effect
that other players end up doing the same thing. This happened for interpretability for a couple
years. We were the only org that worked on interpretability seen inside neural nets. There are
various corporate structures that we've implemented that we hope others may emulate,
and recently we released this responsible scaling plan that I could talk more about later. But
generally, we're trying to set the pace. We're trying to do something good, inspiring, also viable,
and encourage others to do the same thing. At the end, again, maybe we win in a business sense,
and of course that's great, but maybe someone else wins in a business sense, or we all win,
we all split it, but the thing that matters is that the standards are increasing.
I want to talk about all that stuff in a second, but why do you think philosophically that AI
development or the scaling of models and safety, why are they intertwined? I've heard you maybe
coiled together in different ways. Yeah. I think this actually isn't such an unusual thing. I think
this is true. It's true in most fields. The common analogy is bridges. Building a bridge
and making it safe, they aren't exactly the same thing, but they both involve all the same principles
of civil engineering. It's hard to work on bridge safety in the abstract aside from
outside the context of a concrete bridge. What you need to do is you need to look at the bridge.
You're like, okay, well, these are the forces on it. This is the stress tensor. This is the strength
of the material or whatever. That's the same thing you come up with in building the bridge.
If safety differs in any way, maybe it differs in thinking about the edge cases. In safety,
you have to worry what goes wrong 0.1% of the time, whereas in building the bridge,
you have to think about the median case. It's all the same civil engineering. It's all the same
forces of mechanical physics. I think it's the same in AI and large language models.
In particular, safety is itself a task. Is this thing the model's doing right or wrong?
Right or wrong could be something as prosaic as is the model telling you how to hot wire a car
or as scary and sophisticated as is the model going to help me build a bio weapon or is it going to
take over the world and make swarms of nanobots or whatever futuristic thing. Figuring out whether
the model's going to do that and the behavior of the model is itself an intellectual task of the
kind that models do. The problem and the solution to the problem are mixed together in this way,
where every time you get a more powerful model, you also gain the capability to understand and
potentially reign in the models. We have this problem where these two things are just mixed
together in a way that's I think hard to untangle. I think that's the usual thing. I think the only
reason that's surprising is that the community of people who thought about AI safety was historically
very separate from the community of people who developed AI. They had a different attitude,
they came to it from a more philosophical perspective, more of a moral philosophy perspective,
whereas those who built the technology were engineers. But just because the communities
were different, that doesn't imply that the actual content turned out to be different.
So I don't know, that's my view on it. It was becoming a business and commercializing your
effort. If you could scale and figure all the safety stuff out without being a business,
would you pick that path or is the business side of it inherently intertwined as well as
something that interests you? Yeah, so I'd say a few things on that. One is I think it's
actually going to be very difficult to build or would have been very difficult to build
models of the scale that we want without being a commercial enterprise. People make jokes about
VCs, being willing to pour huge amounts of money into anything, but I think that's only true up
to a point. There's business logic and there's business logic behind it. You guys have LPs,
things need to, it's not just all hype train, things need to make sense eventually.
We're now getting to the point where you need certainly multiple billions of dollars and I
think soon tens of billions of dollars to build models at the frontier and to study safety with
those models at the frontier requires you to have intimate access to those models, particularly for
tasks like interpretability. So first of all, yeah, I just think it's very hard.
On the other hand, or in support of that point, I also think that there are some things that you
learn from the business side of things. Some of it is just learning the muscle and the operation
of things like trust and safety. Today we deal with trust and safety issues like, oh,
people are trying to use a model for inappropriate purposes, not things that are going to end the
world, but things that we'd rather that people not do. I think the ultimate significance of
being able to develop methods to address those things and enforcing those things in practice
when they're used at scale by users is it allows us to practice for the cases that are really,
really high stakes. And I think without that organizational institutional practice, it might
be difficult to kind of just be thrown into the shark tank. Congratulations, you built this amazing
model. It can cure cancer, but also someone could make a bioplague that would kill a million people.
You've never built a trust and safety org. You have to deploy this model in the world and make
sure we do one or not. That would just be a very difficult thing to do, and I don't think we would
get it right. Now, all that said, I mean, I will freely admit my passion is the science and the
safety. That's kind of my first passion. The business stuff is quite a lot of fun.
I think we've just watched all the different customers just learning about the whole business
ecosystem has been great. But definitely my first passion is the science of it and making
sure it goes well. Was there a serious debate about being a business versus not in the early days?
Was that like a real conversation among the day? Yeah. So I think certainly everyone was aware
from the beginning that there was a good chance that we would commercialize the models at some
point. We had this thing on our website. I'm not sure if it's up there anymore, but you can see it
on the wayback machine that said, for now we're doing research, but we see commercial potential
down the road. So everyone who joined saw that and everyone who joined knew that. But there was a
question of when exactly should we do it? So there was a period around, I think it was April,
May, June of 2022 when we had kind of the first version of Claude, which was actually like a smaller
model than Claude I, but we were training the model that would become Claude I at that time.
And we realized that with RL from Human Feedback, we didn't have our constitutional AI method yet,
that this thing was actually great to interact with. And all of our employees were having fun
interacting with it on Slack. We showed it to a small number of external people, and they had
lots of fun interacting with it. So it definitely occurred to me and others that, hey, there could
have been a lot of commercial potential to this. I don't think we anticipated the explosion that
happened at the end of the year. Like we definitely saw potential. I don't think we saw that much
potential. But yeah, we definitely had a debate about it. And I wasn't sure quite what to do.
I think our concern was that with the rate at which the technology was progressing,
a kind of big loud public release might accelerate things so fast that the ecosystem might not
know how to handle it. And I didn't want our kind of first act on the public stage
after we'd put so much effort into being responsible to accelerate things so greatly.
I generally feel like we made the right call there. I think it's actually pretty debatable.
There's many pros, many cons, but I think overall, we made the right call. And then,
certainly, as soon as the other models were out and kind of the gun had been fired,
then we started putting these things out. We're like, okay, all right, now there's
definitely a market in this. People know about it. And so we should get out ahead.
And indeed, we've managed to put ourselves among the top two or three players in this space.
Was that gun being fired and chat GPT sort of taking off? Was that similar to the maybe fear
that you had of like, hey, this might start a race?
Yeah, similar and in fact, more so. So I think we saw it with Google's reaction to it,
that there was definitely just judging from the public statements, a sense of fear and
existential threat. And I think they responded in a very economically rational way. I don't blame
them for it at all. But you put the two things together and it really created an environment
where things were racing forward very quickly. And look, I love technology as much as the next
person. There was something like super exciting about the whole make them dance. Oh, we're
responding with something. I mean, I can get just as excited about this as everyone.
But given the rate at which the technology is progressing, there was a worrying aspect about
this as well. And so in this case, I'm at least on balance, Clad, that we weren't the ones who
fired that starting gun. Yeah, got it. Well, you recently announced an investment from Amazon
before that you did a round with Spark and a little bit more traditional venture capitalists.
The round with Amazon, it's complicated and I can't go into the details, but it's not a full
clothes, sure, price, corporate round and all that. Before that, you had an unusual round as well
with FTX, right? How did that come to be? Yeah, so honestly, there was actually very little to
that. So there's kind of a community of people who cared a lot about AI safety.
Back when he was doing FTX before he committed all the fraud or was caught committing all the
fraud that he committed, Sam Bakeman Freed was presented himself as someone who cared a lot
about issues like pandemics, AI safety. So he was known to people in my community. And honestly,
there's not much to tell. I only talk to him a handful of times. The entity is still related
to FTX, right? So there's potential that the enthropping investment could one day make FTX
people hold depending on how it all plays out. Yeah, that's one of the ironies. There's a
bankruptcy estate or bankruptcy trust that owns these non-voting shares. And so far, they've
declined to sell them off, but they're interested in doing so in a general sense. And so I'm told
there's people that are very interested in buying those shares from them. Yeah, I can't comment
on the market dynamics there. And we don't really control them, right? It's a sale between
different parties. But hey, if those shares lead to the people who had their money stolen,
getting some or all of their money back, then that's kind of a random chance, but certainly
a good thing. Good outcome. So what is the business of enthropic look today? You guys are
focusing mostly on enterprise customers? Yeah, we are focusing mostly on enterprise customers.
I would say we have both an enterprise product and a consumer product. It makes a lot of sense
if you're building one of these models to at least try to offer them in every direction
that you can, right? Because the thing that's expensive in terms of both money and people
is building the base model. Once you have it, wrapping it into the consumer product versus
wrapping it in an API, while both of those things do take substantial work, are not as expensive as
the kind of the base work in the model. And so we have a consumer product that honestly is doing
pretty well, but at the same time, our real focus definitely is enterprise. We've found that some
of the properties of the model in a practical sense, right? The safety properties of the model
in a very practical sense, as opposed to kind of like a philosophical or future sense, are actually
useful for the enterprise use cases. We try to make our models helpful, honest, and harmless.
Honesty is a very good thing to have in knowledge work settings, a number of our customers are in
like the finance industry, the legal industry, starting to get stuff on the health side,
different productivity apps. Those are all cases where a mistake is bad, right? You're doing
some financial analysis, you're doing some legal analysis, like you really have a premium on like
make sure the thing knows what it doesn't know. So giving you something misleading is much worse
than not giving you anything at all. I mean, that's true across the board, but I think it's
especially true in those industries. For enterprises, often kind of like
inappropriate or embarrassing speeches, something that they're very concerned about,
even if it happens very rarely. And so the ability to kind of steer and control the model is better,
I think, is very appealing to a number of enterprise customers. Another thing that's
been helpful is this, we have this longer context window. So context window is like how much
information the model can take in and process. So our context window is 100,000 tokens. Tokens
are this weird unit, it really corresponds to 70,000 words. But the model with the next biggest
context window is GPT-4, where there's a version of it that has 32K tokens, 32,000, which is three
times less, but the main GPT-4 has 8,000, which is about 12 times less. And so just the ability to,
for example, something you can do with Claude that you can't do with any other model is read a
mid-sized book or novel or textbook or something, just stick it into the context window, upload it,
and then start to ask questions about it. And so that's something you can't do or can't do nearly
as easily with any other model. And then another thing that's actually been appealing is raw cost.
So the sticker price of Claude II is about 4X less than the sticker price of GPT-4.
And the way we've been able to do that, I can't go into the details, but we've worked a lot on
algorithmic efficiency for both training and inference. So we're able to produce something
that's, you know, in the same ballpark as GPT-4 and better for some things, and we're able to
produce it at a substantially lower cost. And we're in fact excited to extend that cost advantage
because, you know, we're working with custom chips with various different companies, and we think
that could give an enduring advantage in terms of inference costs. So all of those are particularly
helpful for the enterprise case. And, you know, we've found pretty strong enterprise adoption,
you know, even in the face of, you know, competition from multiple companies.
Hi, I'm Logan Bartlett, the host of this podcast. This is not an ad. As you may know,
we do not advertise or monetize this podcast in any way. I just wanted to take a quick second
to tell you that we have a bunch of killer guests coming on over the course of the next few weeks.
And so if you're enjoying these conversations behind the scenes with both entrepreneurs and
investors, please do subscribe to our channel so you don't miss out. Back to the episode.
What's something that's perhaps unintuitive to someone that isn't living and breathing this
every day about enterprise interest in artificial intelligence as someone sitting at that nexus?
First of all, I see a huge advantage to folks who think in terms of the long term. So there's a fact
that's kind of, you know, the bread and butter for those of us who are building the technology.
But, you know, getting it across to the customers is, I think, one of the most important things,
which is the pace at which the models are getting better. Some of them get it, others are starting
to get it. But the reason this is important is, you know, put yourself in the shoes of a customer,
right? They've got, they've got our model clawed. They want to, you know, they want to build something.
And, you know, typically they want to start with something small. And of course, naturally, they
think in terms of what can the model do today? And what I always say is, do that, we got to start,
we got to iterate from somewhere. But also, think in terms of where the models are going to be in
one or two years. It's going to be a one or two year arc to, you know, to go from proof of concept
to small scale deployment to a large scale deployment to true product market fit for whatever
it is that you're launching. So you should basically skate where the puck is going to go.
You should think, okay, the models can't do this today. But look, they can do it 40% of the time.
That probably means they can do it 80 or 90% of the time in one or two years. So let's have the
faith, the leaf of faith, to build for that, instead of building for, you know, what the models
are able to do today. And if you think that way, the possibilities of what you can do in one to
two years are much more expansive. And we can talk about having a kind of longer term partnership
where we build this thing together. And I think, you know, the customers that have thought that way
are, you know, ones that, you know, we've been able to work together with on a path towards
creating a lot of value. We also do lots of things that are just targeted as what you can do today.
But often the things I'm most excited about are those that see the potential of the technology.
And by starting to build now, they'll have the thing tomorrow as soon as the model of tomorrow
comes out, instead of it being another year to build to build after that. This is something that I
think is particularly true of anyone and particularly any leader. But you had said recently,
attaching your incentives to the approval or cheering of a crowd in some ways destroys your
mind and in some ways can destroy your soul. You haven't been as public as other folks in the
space have been. I assume that's very purposeful stylistically and ties into that. Can you talk
a little bit about your thoughts around that? Yeah, I mean, part of it, it's just kind of my
style for one thing. I mean, you know, I think as a scientist, you know, I prefer to speak when
there's kind of something clear and substantive to say. You know, I mean, I'm not totally low
profile. I mean, I'm on this podcast right now, and I've been on a few. So, you know, given the
general volume of the field, there's some need to get the message out there. And that need will
probably increase over time. But I think I have noticed, and you know, it's not just, you know,
Twitter or social media, but some phenomenon that's a little bit connected to them that, you know,
you can really, if you think too much in terms of kind of pleasing people in the short term,
or, you know, making sure that you say something popular, it can really kind of lead you down
a bad path. And I've seen that with a lot of, you know, with a lot of very smart people. I mean,
I'm sure you could name some as well. I'm not going to give any names who have gotten caught up in
this. And, you know, years later, you look at them and you're like, wow, this is a really smart
person who's acting much dumber than they are. And I think the way it happens, I don't know,
I could give an example, right? So, you know, take, you know, like a debate that's important to me,
which is like, you know, should we build these things fast, or should we make these systems
safe? So there's an online community mostly on Twitter of, you know, people who are, you know,
think we should slow down, and then kind of an online community of builders who are really excited
about, you know, we should make this stuff fast. And if you go to certain corners of Twitter,
like, you get these really extreme versions of each one, right? On one hand, you get people who
say, like, we should stop building AI, we should have a global worldwide pause, right? I think that
doesn't work for a number of reasons. I mean, we have our responsible scaling plan is sort of
incorporates some aspects of that. So I think it's not an unreasonable, you know, discussion or
debate to have. But, you know, there's this kind of really extreme position. And then that's kind
of created this polarization where there's this other extreme position that's like, we have to
build as fast as possible. Any regulation is just regulatory capture, you know, we just need to
maximize the speed of progress. The most extreme people say things like it doesn't matter if
humanities wiped out AIs or the future. I mean, that's a really extreme position. And so, you
know, think of kind of, you know, the position of someone who's kind of trying to be thoughtful
about this, trying to build but build carefully. If you if you kind of enter that free too much,
if you, you know, if you feel like you have to make those people happy, what can end up is either
you get polarized on one side or another. And then then you kind of repeat all the slogans of that
of that side, and you become a lot dumber than you would otherwise. You know, if you're if you're
really good at dealing with Twitter, you know, you can, you can try and make both people happy.
But, you know, that that involves a lot of playing to both sides. And I certainly I certainly don't
want to do that. That's what I talk about and kind of kind of losing your soul. You know, the truth
is the actual, you know, the position that I think is actually responsible might be something
that would make all of those people boo instead of instead of all of them cheer, right? And so,
you just you just have to be very careful if you're if you're taking that as your barometer,
you know, who's yelling at me on Twitter, who thinks I'm great on Twitter, you're not going to
arrive at the position that makes everyone boo that that might just be the correct position.
What timeline do you think about then when you're saw it's not the instantaneous dopamine
of a tweet, you mentioned talking to enterprises about one to two years and what can be but like
what timeline are you solving for? Yeah, I mean, I guess I guess I kind of think like five to 10
years from now, everything will be like a little bit more clear. And you know, it'll I think it'll
be more clear which decisions were good decisions, which decisions were bad decisions. You know, I
think, you know, certainly less than that time scales the time scale on which, you know, if dangerous
things with these models are indeed possible, as I believe they are, but I could be wrong, I think
they may play out on that time scale. And you know, we'll, we'll, you know, we'll be able to see like
which companies addressed these dangers well, or were the dangers not real, and people like me
warned about them, and we were just totally wrong. Or, you know, will it turn out that some that
some tragedy happened, and you know, people people like me should have been more extreme and worrying
about it? Or will it turn out that, you know, that that companies like Anthropic, you know,
picked picked the right path and, you know, navigated a dangerous situation? Well, I don't know
which how it's going to turn out. I mean, I hope it turns out that, you know, we navigated a dangerous
situation well, and we have heard a catastrophe, and there were there were hard trade offs, and,
you know, we addressed them skillfully and thoughtfully. That's my hope for how it's going to
turn out. But I don't know that it's going to turn out that way. But but you know, I feel like
looking at that, you know, in five years and 10 years, that's that's just going to be a fair
judgment of all the things that that I'm saying and doing. What would you want the average person
listening who is aware of AI knows what Anthropic is knows what Open AI is knows what Google and
others are doing in the space about safety and about risk? What would you want them to know
from your perspective? Yeah, so I think, you know, if I were if I were to kind of just put it in a
few sentences, I think what I would say is, look, I have I have two concerns here. One is the concern
that people will misuse powerful AI systems. People misusing technology is nothing new.
But but one thing that I think is new about AI is that it its ability to put all the pieces
together is much greater than any previous technology. I think in general, we've always
been protected by the fact that, you know, if you take a Venn diagram of people who want to do
really bad things, and you know, people who have strong technical and operational skills,
generally overlap has been pretty small. If you're a person who has a PhD or is capable of
running the larger organizations, you have better things to do than come up with evil plans to,
you know, murder people or destroy society, right? It's just, you know, not not very many people
are motivated in that direction. And then, you know, the people the people who are, you know,
often they're they're just, I mean, not not all of them, but in many cases, not that bright or not
that skilled. The problem is, you know, now could we take unskilled person plus skilled AI plus bad
motives. And so, you know, I testified in Congress about this about the risk of bio weapons, I think
cyber is another area, bunch of stuff around national security and, you know, the relationships
between between nations and stability. So that's one corner of it. And I think the other corner
of it is what the AI systems themselves may do. And, you know, there's lots you can, you know,
kind of find the internet and various communities on this. But but I often put it in a simple way,
which is one, the systems are getting much more powerful. Two, we there's obviously not much
of a barrier to getting the systems to act autonomously in the world, right? People have taken
GPT four, for instance, and turned it into auto GPT, there was even a worm GPT, which is, you
know, supposed to act as a computer worm. So, you know, powerful smart systems that can take
action. So, you know, kind of very long leash of human supervision. And because of the way they're
trained, they're not easy to control. I mean, we all saw being in Sydney. So you put those three
things together. And, you know, there's at least, you know, I think some chance that as the systems
get more and more powerful, you know, they're they're going to do things that we don't want them to do.
And it may be it may be difficult to fully control them to fully reign them in. I think that's further
out than the misuse. But it's, you know, it's something we should think about. We've touched
on a few of the different things you guys have done from a safety standpoint. So I want to talk
through the I guess the three ones I took down. So you long term benefit trust and public benefit
corporation. Can you explain what that is and how you decided to do that? Yeah, yeah. So we were
incorporated as a public benefit corporation from the from the beginning, which means basically
public benefit corporation, it's actually very much like a C Corp, except the the investors
can't sue the company for failing to maximize for failing to maximize profits. You know,
I think I think in practice in the in the vast, in the vast majority of cases, it operates like
a normal company. I mean, I think that's one theme I want to get through here, like 99% of what we
do, you know, we would we would make the same, you know, the same decision that that a normal
company would, you know, most of the time, you know, the logic of business, which is basically
the logic of providing mutual mutual value also makes sense from a public benefit corporation.
But there's maybe this one 1% of key decisions, you know, I might I might think about the, you
know, the the the delay of release of Claude decisions that might relate to hey, we have a
very powerful model, but we need to make really sure that it you know, this thing can't create
a bio plague that'll kill millions of people before we before before we release it. So I think
there are going to be a few key moments in the company where this where this makes a difference.
And then LTBT, you know, as I said, a public benefit corporation is it's not that different
from a C Corp. The idea of the LTBT is to have a set of so LTBT is long term benefit trust.
So right now, the governance of Anthropic is pretty much like that of a normal corporation,
but we have a plan that was written into our original series a documents and has been
iterated on since then, that will will gradually hand over the ability to appoint a majority
of Anthropics board seats to a kind of trust of people. And on that trust of people we've
we've we selected the original ones, but then it becomes it becomes self sustaining.
We selected for kind of three types of experience. One type is experience in AI safety.
One type is experience in national security topics, as I think this is going to become
relevant. And another type is, you know, thinking about things like philanthropy and the macroeconomic
distribution of income. So I think of those as my best guess as to kind of the, you know, the
three topics where something that kind of trend, you know, kind of transcends the kind of ordinary
activity of companies is going to come up. And is that who you ultimately report into when this
structure is finalized? That'll be the the board that Anthropic answers to?
So this set of five people appoints a majority, but not all of the corporate board of the company.
So there's basically two, there's there's two of these bodies. And and the the LTBT appoints the
corporate board. Now look, that said, I mean, we all we all kind of, you know, in in practice,
the company is almost always run, you know, day to day by the CEO, right? Like, you know, it's,
it's very even even speaking of the, you know, even speaking of the corporate board,
not just for Anthropic, but but any other company. I mean, you know, you think of,
you know, think of as a CEO, how many decisions you directly make yourself versus how many it's
like, oh, I have to get the board on board with that. I mean, you know, there are some when you
when you when you, you know, when you when you when you when you when you when you when you raise
money, when you, you know, issue new employee, when you issue new employee shares, when you make
a major strategic decision. The LTBT is kind of an even more rarefied body. And you know, I,
you know, I've set the expectation with them that, you know, their role is to, you know,
get involved in the things that, you know, that really involve critical decisions for humanity.
You know, there might only be three or four such decisions in the entire history of Anthropic.
Now, constitutional AI, can you talk about what that is and what the inputs into it were?
Yes. So constitutional AI is a method that we developed around the end of last year.
So easiest to explain it by contrasting it with this previous method called reinforcement
learning from human feedback, which I and some other people were the co-inventor of at open AI
in 2017. So reinforcement learning from human feedback, the way it works is,
okay, I've trained the giant language model, I paid my tens of maybe hundreds of millions of
dollars to train it. And now I want to give it some sense of how to act. So, you know, there are
questions you can ask the model that don't have any clear, factually correct answer. So, you know,
I could say, what do you think of this politician or what do you think of this policy? Or what should
I as a human do in this situation? And, you know, the model doesn't have any definite answer to that.
So the way RL from human feedback works is you hire a bunch of contractors,
you give them examples of how the model is behaving, and the humans kind of, you know,
they kind of give feedback to the model. They say, this answer is better than that answer.
And then over time, the model updates itself to learn to do whatever is in line with what the
human contractors say. One of the problems with this is, you know, one, it's expensive, it requires
a lot of human labor. But in addition to that, it's very opaque, right? You know, if I serve the
model in public, and then, you know, someone says, hey, why is this model biased against
conservatives? Or why is this model biased against liberals? Or why is this model just
give me weird sounding advice? Or why does it, you know, why does it give things in a weird style?
I can't really give any answer. I can just say, well, I hired 10,000 contractors, I don't know.
And, you know, this was the statistical average of what the contractors generally proposed,
or the, you know, the mathematical generalization of it. It's not a very satisfying answer. So the
method we developed is called constitutional AI. And the way that works is you have a set of explicit
principles that you give to the model. You know, so the, you know, the principles could be something
like on on a political question, you know, present both sides of the issue and don't take a position
yourself, you know, say, here are some arguments for here are some typical arguments against
opinions differ. So with that, you basically, just as with RL for human feedback, you, you
have the model give and you have you have the model give answers, but then you have the model
critique its own responses for whether they're in line with the model constitution. And so you
can run this and basically the model is both the generator and the evaluator with the the
constitution is kind of the pin source of truth. And so this allows you to eliminate human contractors
and instead go from this set of principles. Now in practice, we find it useful to augment that
method with human contractors so that you can get the best of both worlds, but you use less human
contractors than you were before, you have more of a guiding principle. And, you know, then if,
then if someone, you know, then if someone calls me up in Congress and says, Hey, why is your model
woke or why is your model anti woke or why is your model doing this crazy thing?
You know, I can point to the constitution and I can say, Hey, these are our principles.
You could have one of two objections. Maybe you don't agree with our principles.
You know, fine, we can have a debate about that. Or, or it's a technical issue. These are our
principles. Somehow the train of our model, you know, wasn't perfect and wasn't in line with those
principles. And I think separating those two things out is, is I think, I think very useful.
And even like enterprise customers have found this to be a useful thing, the kind of customizability
and the ability to separate the two out in the inputs into this were so you use the UN Declaration
of Human Rights, Apple's Terms of Service. What else went into coming up with the principles?
Yeah, there were there were some there were some principles that were developed for use by an early
deep mind chat bot. But yeah, Apple's Terms of Service, UN Declaration of Human Rights,
we added some kind of other things like we asked the model to respect copyright. So this is one
way to, you know, to greatly reduce the probability that the model outputs copyrighted tax verbatim.
You know, we can we can all debate, you know, what the what the status is of, you know,
you know, models that we train on corpus of data, but we can all agree that on the output side,
we don't want the model to output vast reams of copyrighted tax. That's a that's a bad thing to do
when we we we aim not to do that. And so we just put in the constitution not to do that.
One of the other things that you introduced around safety, broadly speaking, is responsible
scaling policy. Can you talk about what that is? Yes. So our responsible scaling policy is
this set of commitments that we recently released, that is a framework for how to safely make more
and more powerful AI systems and confront the greater and greater dangers that we're going
to face with those systems. So maybe the easiest way to understand it is, you know, to think of
kind of the two sides of the spectrum. So one extreme side of the spectrum is like, build things
as fast as possible, like, you know, release things as much as possible, you know, maximize
technological progress. And I, you know, I understand that position and have sympathy for it in many
other, you know, in many other contexts, I just think, you know, AI is a particularly tricky
technology. You have to put E slash ACC in your Twitter bio, if you believe that, I think. Maybe
I should put both. Yeah. And so the other extreme position, which, you know, I also have some sympathy
for, despite it being absolutely the opposite position is, you know, oh my God, this stuff is
really scary. And the most extreme version of it was, you know, we should just pause, we should just
we should just stop, you know, we should just stop building the technology, you know, for indefinitely
or for some specified period of time. And I think my problem with that has always been, well, okay,
let's say we pause for six months, what do you actually gain from that? What do you do in those
six months, particularly with the more powerful models being needed for safety of more powerful
models? It's kind of like you frozen time, you've stopped the engine, what do you get at the end
of it? And if you were to pause for an indefinite length of time, then you raise these questions
like, well, how do you really get everyone to stop? There's an international system here. There's
dictators who want to use this stuff to take over the world. I mean, you know, people use that as
an excuse, but it's also true. And so, you know, that extreme position doesn't doesn't make too
much sense to me to me either. But what does make sense to me is, hey, let's think about
caution in a way that's actually matched the danger. Right now, you know, whatever we're
worried about in the future, right now, today's systems, they have a number of problems. But I
think they're the problems that come with any new technology, not these kind of special problems of,
you know, bioweapons that would kill millions of people or, you know, the models, you know, kind of,
you know, kind of taking over the world in some form. So, you know, let's have relatively
normal precautions now. But then let's define a point at which, you know, when the model has
certain capabilities, we should be more careful. So the way we've set this up is we've defined
something called AI safety levels. So there's something called biosafety levels in the US
government, which is like, you know, for a given virus, you define how dangerous it is,
it gets categorized as like BSL1 or BSL3 or BSL4. And that determines the kind of
containment measures and procedures you have to take to, you know, to control that virus. So
we think of AI models, we think of AI models in the same way. There's value in working with
these very powerful models, but they have dangers to them. And so we have these various thresholds
between ASL1 and ASL2, between ASL2 and ASL3, between ASL3 and ASL4. And at each level, there's
certain criteria that we have to meet. So right now we're at ASL2 as we've defined it. Before we
get to ASL3, we have to develop security that we think is sufficient to prevent any kind of,
anyone who's not a super sophisticated state actor from stealing the model. That's one thing.
Another thing is we have to make sure that when the models reach a certain level of capability,
we're really, really certain that they're not going to provide a certain class of dangerous
information. And to figure out what that is, you know, we're going to work with some of the best
biosecurity experts in the world, some of the best cybersecurity experts in the world to understand
what really would be dangerous compared to what can be done, you know, today, today with a Google
search. We've defined these tests in these thresholds very carefully. And so how does
that relate to the two sides of the spectrum? Compared to a pause, the ASL thresholds could
actually lead us to pause, because if we get to a certain capability of model, and we don't have
the relevant safety and security procedures in place, then we have to stop developing more
powerful models. So the idea is there could be a pause, but it's a pause that you can get out of
by solving the problem. It's a pause you can gather by developing the right safety measures.
And so it incentivizes you to develop the right safety measures. And in fact,
incentivizes you to avoid ever having to pause in the first place by proactively developing
the right set of safety measures. And as we go up the scale, we may actually get to the point where
you have to very affirmatively show the safety of the model, where you have to say, you know, yes,
like, you know, I'm able to look inside this model, you know, with an x-ray, with interpretability
techniques and say, yep, I'm sure that this model is not going to engage in this dangerous behavior,
because, you know, there isn't any circuitry for doing this, or there's this reliable
suppression circuitry. So it's really a way to shoehorn in a lot of the safety requirements,
put them in the critical path of making the model. And hey, if you can be the first one
to solve all of these problems, and therefore safely scale up the model, not only will you
have solved the safety problems, but that kind of aligns the business incentives with the safety
incentives. Our hope, of course, is that others will adopt the responsible scaling plan, and that
eventually it can also be an inspiration for policies so that everyone is held to some version
of the responsible scaling plan. And, you know, how does it relate to this other thing of like,
build as fast as we can? Well, look, I mean, one way to think about it is like, the responsible
scaling plan doesn't slow you down, except where it's absolutely necessary. It only slows you down
where it's like, there's a critical there's a critical danger in this specific place,
with this specific type of model, therefore, you need to slow down. It says nothing about, you know,
stopping at some certain amount of compute or stopping for no reason or stopping for a specific
amount of time. It says, keep, keep building until you get to certain thresholds. You can
solve the problems with those thresholds, then keep building after that. It's just that as the
models get more and more powerful, you know, safety has to build along with the capabilities of the
model. And our hope is that if we do that, and others do that, it creates the right culture
internally and anthropic, and it creates the right incentives for, for, you know, the ecosystem
and companies other than anthropic. You know, I'm aware that since we published our responsible
scaling plan, several other organizations are internally working on responsible scaling plans.
You know, for all I know one of them, one or more of them might be out by the time this,
this podcast is out, hopefully they put out something, hopefully they try and make it better
than ours. That would, that would be a win for us. You have some aspects of your job you alluded
to this earlier 99% of your job probably looks mostly like a normal company would, but your time,
I would guess is probably not 99%. How much your time is spent on stuff that is
weird for a normal startup CEO, testifying in front of Congress or whatever that bucket is,
versus just day-to-day operations of running a business or is it hard to disentangle?
Yeah, I mean, it's so, I don't know, I would say it's maybe 75, 25 or something like that.
75 normal? 75 normal, 25, 25% weird. I mean, it certainly takes a lot of time to, you know,
to talk to large number of customers, to, you know, to hire for various roles, to, you know,
look at financial metrics to inspect the building of the models. I mean, that eats up a lot of time,
but, you know, I also spend a decent amount of time say talking to government officials
about what the future is going to look like, you know, thinking about the national security
implications, trying to, trying to advise folks on, you know, what can go wrong. We did this whole
project of, you know, working with some of the world expert biosecurity experts on, you know,
what would it really take for the model to, to help someone to do something very dangerous?
There are certain missing steps in bio weapons synthesis. For obvious reasons, I'm not going
to go into what those steps are. I don't even, I don't even know all of them, but, you know,
we spent, spent a good number of months and, you know, decent amount of my, of my personal
time along with the incredibly hard work of the team, the team that worked on it,
you know, thinking about this and, you know, presenting it to officials within our government,
within other allied governments. And, you know, that's, that's just a pretty, pretty unusual
thing to do, you know, I mean, that, that, that, that, you know, that, that feels something like
something more out of a military thriller or something like that. So, you know, that's,
that's unusual speaking to Congress is unusual, you know, thinking about where, where, you know,
like where we're going to be in like, you know, three or four years, like, you know, you know,
or the model's going to, you know, like run rampant on the internet or something like that.
Like spend a good, good deal of time thinking about, you know, how do we, how do we, how do we,
how do we prepare for that scenario? You know, another thing is like, you know, thinking about,
you know, could, could at some point, you know, could, could the models at some point be morally
significant entities, right? That's really wacky, really strange. Like, I still don't know how,
you know, how to be sure of that or how, or, or, or, or, or, or how you'd measure that.
It might be, you know, it might be an important thing. It might not be an important thing.
But, you know, we take it, we take it, we take it seriously. And so there is definitely this
weird juxtaposition of like, you know, I'm like, you know, looking, looking for a chief product
officer one day and like, you know, thinking about bio weapons and, you know, you know,
model moral status the next day. There's a box article that said something to the effect of
an employee predicted there was a 20% chance that a rogue AI would destroy humanity within the next
decade to the reporter, I guess, that was around is, I mean, does all this stuff weigh heavily
on the organization on a daily basis? Or is it mostly consistent with a normal startup for the
average employee? Yeah, so I don't know, I'll give my own experience. And it's kind of the same thing
that I, you know, that I recommend that I recommend to others. So, you know, I really freaked out
about this stuff in 2018 or 2019 or so. When, you know, when I first believed that, you know,
turned out to be, you know, at least in some ways correct, that the models would scale very rapidly
and, you know, they would have this importance to the world. Was there a specific thing that
made you realize that or that you saw? Or was it a bunch of things coming together? Playing with GPT2
is what did it for me. It's what made it real. I mean, GPT3 was more so and, you know,
Claude and GPT4 are, you know, of course, even more impressive. But like, the moment where I
really kind of believed the scaling trends that we had been seeing that, you know, was
really real and would lead to real things was like the first time I looked at GPT2, I was like,
oh my, this is like, this is crazy. This is, you know, there's nothing like this in the world,
like it's crazy that this is possible. And was it in particular the jump between the prior version
to that and seeing that delta? Yeah, it was the delta and it was just the things that it was
capable of. Like it felt like a general, it felt like a general induction or general reasoning
engine. For years after that, people said models couldn't do reasoning. I looked at GPT2 and I'm
like, yeah, you scale this thing up, it's really going to be able to see any pattern and reason
about anything. I mean, again, we still don't know that for sure. This is still unsure. There
are still people who say it can't and for all I know, they're right. But that was kind of the
moment that I saw it. And so I had a very, very difficult year or so where I tried to come to
terms with what I believe to be the significance of it. Now, five years later, I'm much more in a
position where it's like, this is the job we got to do, right? Where you signed up to do this,
you have to be a professional and you have to address risk in a sensible way. And I found it
useful to think about the strategies used by people who professionally address risk or deal
with dangerous situations, right? People who are on military strike teams, people in the
intelligence community, people who kind of deal with high stakes, critical decisions for
national security or disaster relief or something like that. I mean, doctors, surgeons, you talk
to all these people and they have techniques for thinking of these decisions rationally and
making sure that they don't get caught up in them too much. And so I kind of try to adopt those
techniques and I've told other people in New York to think in that way as well.
You had made a comment that you don't like the concept of personalizing companies in this whole
hey, the memification of a CEO in some regard. Is that just a personal tax to who you are? Do
you think it's actually like a societal issue? Yeah, I mean, it's definitely my personal style,
but I think this is closely connected to the thing about Twitter. I think people should
think about companies and the incentives they have and the actual substance of the decisions they
make. Nine times out of 10, if someone seems kind of charming or relatable or you talk to them on
Twitter and it seems like there's someone who you could sit down with them and really like,
you know, that could just be very misleading, right? It's not necessarily a bad sign,
but I think it's pretty uncorrelated to like, what is the actual effect that the company's
having in the world? And I think people should focus on that and kind of we've tried to focus on
that in terms of the structural elements, right? I'm not the only one who is ultimately responsible
for these decisions. The LTBT is kind of designed as this check, as this supervisory body, so everyone
doesn't have to look what's Dario going to do in this situation. And then Anthropic is only one
company within a space of many other companies. There are also government actors there and this
is the way it should be. No one person, no one entity should have too much say over any of this.
I think that's always unhealthy. We've talked about a lot of the negative sides or implications
that come around with running in Anthropic. I'm sure there's some positive ones other than being
in the eye of the nucleus or storm or all the stuff that's going on today. Do you have any
weird data points on like number of applicants or like the inbound you get or all that?
Yeah. I mean, we can talk about amazing positive stuff in the short term and I'm also excited
about positive stuff in the long term. So maybe let's take those one by one. And I think we should
talk more about the positive stuff. I mean, I often see it as my duty to make sure people are
aware of the concerns in a responsible and sober way, but that doesn't mean I'm not excited about
all the potential I am. So speaking about Anthropic in particular, I mean, millions of people have
signed up to use Claude. Thousands of enterprises are using it and a smaller number of very large
smaller number of very large players have started to adopt it. So I've just been excited
by some of the use cases. Like when we look at particularly legal financial things like counting,
when you see suddenly people are able to talk to documents, you can just upload a company's
financial documents, you can just upload a legal contract and ask questions that you would have
needed a human to spend many hours on. And so this is just in a very practical way, this is just
saving people's time and providing them with services that it would be very difficult for
them to have otherwise. So I don't know, it's hard not to be excited by that and of course excited
by all the amazing things the technology can do. I know of someone who used it to translate
math papers from Russian and they were good enough that it all made sense and they were able to
understand something that would have been very difficult for them to understand it
before. In the long run, I mean I'm even more excited, I mean I've talked about this a little
before but you know having been in biology and neuroscience, I'm very convinced that the limiting
factor there was that the basic systems were getting too complicated for humans to make sense of.
If you look at the history of science, things like physics, there's very simple principles in
the world. We managed to solve those because I mean physics is not fully solved but many parts of
the basic operation of our world we understand. And then within biology, things like viral disease
or bacterial disease, it's very simple. There's something invading your body, you need to find
some way to kill the invader without hurting yourself and because you and the invader are
pretty different biologically, it's not that hard so we've solved the problem. What's left is things
like cancer, Alzheimer's disease, the aging process itself, to some extent things like heart disease
and I worked on some of those things in my career as a biological scientist
and just the complexity of it. It's like you're trying to understand how proteins build cells
and how the cells get dysregulated. It's like there's like 30,000 different proteins and each
one of them has like 20 different post-translational modifications and each of those interacts with
the other proteins in this really complicated web that makes one cell run and that's just one
type of cell and there's like hundreds of other types of cells. And so one of the things we've
already seen with the language models is that they know more than you or I do. A language model,
they know about the history of samurai in Japan at the same time as they know about the history of
cricket in India at the same time as they can tell you something about the biology of the
liver or something like that. You list enough of these topics and there's no one on earth who knows
who has that breath, even to the level that a language model does, even with all the things
that it says wrong right now. And so my hope is that in terms of biology right now we have this
network of like you know thousands of experts who all have to work together. If you can have one
language model that can connect all the pieces and you know not just kind of like big data will
help biology, that's not my thesis here. My thesis is that they'll be able to do and work along with
the humans a lot of things that human biologists, human medicinal chemists do and really track the
complexity and be a match for the complexity of these disease processes that are happening in our
body. And so I'm hopeful that we'll have another kind of renaissance of medicine like we had in
the late 19th century or early 20th century when you know all these diseases we didn't know how to
cure were like oh we discovered penicillin and we discovered vaccines so you know I'll take
cancer as one like you know any biologist or medicinal chemist you know who I said could we cure
cancer in five years they'd be like that's that's fucking insane there's so many different types
of cancers these breakthroughs you know we have all these breakthroughs that handle one really
narrow type of cancer. I think if we get this AI stuff right then we maybe we could really do that
so I know it's hard to be hard to be more inspiring than that. Yeah totally. You said you've been right
about a lot of things related to AI but you've also been wrong and surprised by a bunch but what
have you been most wrong about or surprised by? Yeah I don't know so I mean I've been wrong about
I've been wrong about a bunch of stuff like I think how this prediction stuff works is like
if you're thinking about the right things if you're predicting the right things you know you only
have to be right about like 20% of stuff for you know for it to have these huge huge consequences
right if you if you if you predict five things that like no one in the world thinks is going to
happen and would have enormous consequences and you're right about one of them I mean so it's a
little bit like VC right you know it's like you know if you if you you know if in 1999 you invested
in Google and four companies that no one heard of that's a pretty good portfolio end up being wrong
about about lots of stuff so like one example of that uh I don't know I could come up with a few
examples but but one is uh I thought certainly going back in like 2019 or so when you know I first
kind of saw the the scaling situation I thought that we were going to scale for a while with these
pure language models and then what what we needed to do was immediately start working on agents acting
in the world not necessarily robotics but like you know there have been all this stuff on Go
Starcraft Dota these other video games all of which used reinforcement learning before the
era of the large language models so I thought we were going to put the two together almost immediately
and then almost all the training by by now by 2023-2024 was was going to be uh you know these
these these large language models that were already as big you know already as big as they
could usefully be made would would would kind of act in the world uh but we found instead as we've
just kept scaling the language models and I you know I still think all the RL stuff is going to be
promising it's just we haven't gotten to it because it's in the lowest hanging fruit because it's
simpler to just spend more money to make to make these models bigger than to design design something
new it's it's completely economically rational and and the models just keep just keep getting better
and better um which you know I didn't didn't doubt that they would get uh didn't doubt that they
would get better but I I I guess I imagine that things would happen in a little bit of a different
order do you think data will be a scaling issue in the near term yeah so um I think there's actually
some chance um I would say there's a 10 chance that we get blocked by data um the reason I mostly
don't think it is you know the the deeper you look at you know the internet's a big place and the
deeper you look the more high quality data you find and this is without even getting into kind of
licensing of private data this is just you know kind of you know this is this is just publicly
available data uh and then there are a bunch of promising approaches which I won't get into detail
about for how to make synthetic data uh and uh you know then again I can't get into detail but we
fought a lot about this and I I bet the other LLM companies have fought a lot about it as well
and I would guess that one of those two at least one of those two paths very likely to pan out
but it's not it's not a slam dunk um I don't think we've proven yet that this will work at the scale
we need it to work this will work for a you know a 10 billion dollar model that you know that that
needs god knows how many trillion uh god knows how many trillion words fed into it real or synthetic
those numbers are so big for people and the amount of money that uh is being spent to train these
models um we're for the average person listening like where does all that money go into uh and how
how should they think about like the need over time to continue to iterate on this yeah so you
know what I'll say is at least to my knowledge no one has trained a model that costs billions of
dollars today um people have trained models that cost I think of order 100 million dollars um but I
think billion dollar models will be trained in 2024 and my guess is in 2025 2026 several billion
dollar maybe even 10 billion dollar models will be will be trained there's you know there's enough
compute in the industry and enough ability to do data centers that that's possible and you know I
think I think it will happen right if you look at what Anthropic has has raised you know has raised
so has has raised so far at least it's been publicly disclosed um you know we're at we're
at roughly 5.5 billion dollars or so we're not going to spend that all on one model um but you
know we certainly we're certainly going to spend you know multiple billion dollars on training a
model sometime in the next sometime the next in the next two or three years where does that go
it's almost all compute it's almost all GPUs or custom chips um uh and uh you know and and the data
center and data center that surrounds them 80 to 90 percent of our cost is capital and almost all
our capital cost is compute um you know the the number of people necessary to train these models
the number of engineers and researchers is growing uh but it's the cost is absolutely dwarfed by uh
by by you know is dwarfed by the cost of compute you know of course we also have to pay for like
the buildings people work in but you know that again is some some tiny fraction of of of what
the cost of compute is maybe ending on a on an optimistic note here and we touched on a bunch
of like the potential medical breakthroughs and things like that but why should people be optimistic
about what anthropics doing about the future AI and everything that's going on yeah i don't know
so i'd answer the question in two ways i mean one i'm optimistic about solving the problems i mean i
am getting super excited about the interpretability work like people didn't necessarily think this
was possible i still don't know whether it's possible to you know to really do a good job
interpreting the models but i'm very excited and very pleased by the progress we've made
i'm also excited about just you know the wide range of ways we've been able to deploy the model
safely like the wide range of of of happy customers who who who who just say you know this this model
has been able to solve a problem that we had it's solved it reliably we haven't had you know
we haven't had all of these safety problems where we've managed to solve them we've deployed
something safely in the world it's being used by lots of people that's that's great that's one
level of great and i think the second level of great is this this this this this this thing you
you alluded to with like you know medical breakthroughs mental health breakthroughs like
i think you know energy breakthroughs are already doing pretty well but you know i imagine AI can
speed up material science very very very very substantially um so you know i think i think
if we solve all these problems i think a world of abundance really is a reality i don't think it's
utopian given what i've seen that technology is capable of and you know of course there are people
who will look at the flaws of where the technology is right now and say it's not capable of those
things and they're right it's not capable of those things today but if the if the scaling laws that
i'm talking about really continue to hold then i think i think we're gonna see some some really
radical things um you know one of one of the things you know it's not a not a complete trend but
you know i think as as we as we gain more you know mastery over ourselves our own our own biology
you know the ability to manipulate the technological world around us uh you know i have some hope
that that will also lead to a you know to a a a a kinder and more moral society um uh you know i
think i think in many ways it has in the past although not uniformly why don't you like the
term agi so i like the term agi ten years ago um because you know no one was talking about the
ability to do general intelligence ten ten years ago and so it felt like kind of a useful concept
but but now i actually think ironically because we're much closer to the kinds of things agi is
pointing at it's sort of no longer a useful term you know it's it's you know it's a little bit like
if you see some object off in the distance on the horizon you can point at it and give it a name
but you get close to it you know it turns out it's like a big sphere or something and and you're
standing you're standing right under it and so it's no longer that useful to say this sphere right
it's you know it's it's basically it's kind of all around you and it's very close and and it actually
turns out to to denote things that are quite different from one another um so so one thing
i'll say i mean i you know i said this on a previous podcast i said i think in two to three years
the llms plus whatever other modalities and tools that we add are going to be at the point where
they're as good at human professionals at kind of a wide range of knowledge work tasks including
science and engineering um i i definitely that that would be my prediction i'm not i'm not sure
but i i think that's going to be the case and you know when people people kind of like commented
on that or put that on twitter they said oh dario thinks a g i is going to be two to three years
away um and and so that that then conjures up these image of you know there's going to be swarms
of nanobots building dyson spheres around the sun in in two to three years and like of course this
is absurd i i don't necessarily think that at all um again the the specific thing i said was
you know there are going to be these models that are that are able to able to on average
match the ability of of human experts in a wide range of things that they can do
there's so much between that and you know the super intelligent god if if that latter thing
is even is even possible or even a coherent concept which it may be or it may not be you
know one thing i've learned on the business side of things is that there's a huge difference between
a demo of a model can do something versus this is actually working at scale and can actually
economically substitute there's so many little interstitial things that's like oh the model can
do 95 percent of the task it can't do the other five percent but it's not useful for us in less
you know in less we're able to substitute in ai end to end for the process or it can do a lot of
the task but you know there's still some parts that need to be done by humans and it doesn't
integrate with the humans well it's not complementary it's not clear what the right interface is
and so there's there's so much space between in theory can do all the things humans can
and in practice is actually out there out there in the economy as full co-workers for humans
and there's a further thing of like can it get past humans can it in can it outperform
the sum total of humans say you know scientific or engineering output that's that's like a you
know that's that's another point that point could be you know could be like a year away because the
model gets is better at making itself smarter and smarter or it could be many years away and then
there's this further point of like okay you know can the model like you know like explore the universe
and set out a bunch of like you know van Neumann probes and you know build Dyson spheres around the
sun and you know calculate the meaning of life is 42 or whatever um you know that's that's like a
that's that's like a further point that also raises questions about you know what's practical
in an engineering sense in in all of these kind of weird weird things so that's that's like another
further point it's possible all of these points are pretty compressed together because there's
like a feedback loop but it's possible they're very far away from each other um and and so
there's this whole unexplored space of like you say the word AGI and you're like referring you're
smushing together all of those things um I think some of them are very practical and near term
and then I have a hugely hard time thinking about like does that a meet does that lead very quickly
to all the other things or you know does it lead after a few years or are are those other things
like not as coherent or meaningful as we think they are I think all I think all of those are
possible so it's it's just it's just kind of a mess we're just we're kind of flying very fast into
this this this glob of concepts and possibilities and and we don't have the language yet to separate
them out we just say AGI and I don't know it's just kind of a it's it's like a it's like a buzz word
for a certain certain community or certain set of science fiction concepts when when really we kind
of it's pointing at something real but it's pointing at like 20 things that are very different from
from one another and we badly need language to actually talk about them what do you think
happens on the next major training run for LLMs um so my guess would be you know nothing truly
insane happens say in any training run that that you know happens in 2024 I think all the you know
all the you know the stuff the good and bad stuff I've talked about you know to really
invent new science the ability to to cure diseases the dice and nanowatts to make to make bio yeah
the ability to make bio weapons yeah and maybe someday that the Dyson spheres the the least
impressive of those things I think you know will happen you know you know I would say no
sooner than 2025 maybe 2026 I think we're just going to see in 2024 crisper more commercially
applicable versions of the models that exist today like we you know we've seen a few of these
generations of jumps I think in 2024 people are certainly going to be surprised like they're going
to be surprised at how much better these things have gotten but it's it's not going to quite bend
reality yet if you if you if you know what I mean by that I think we're just going to see
things that are crisper more reliable can do longer tasks of course multimodality which we've
seen in the last you know we've seen the last few weeks from multiple companies is going to
play a big part ability to use tools is going to play a big part so you know generally these
things are going to become a lot more capable they're definitely going to wow people but this
reality bending stuff I'm talking about I don't expect that to happen in 2024 how do you think
the analogy of versus a brain breaks down for large language models yeah so it's actually
interesting this is this is one of the you know being a former neuroscientist this is one of the
the the mysteries I still wonder about so the general impression I have is that the way that
the models run and the way they operate I don't think it's all that different you know of course
the physiology all the details are different but I don't know the basic combination of
linearities and nonlinearities the way they think about language to the extent that we've
looked inside these models which we have with interpretability I mean we see things that would
be very familiar in you know the brain or a computer architecture you know we have these
you know we have these we have these registries we have variable abstraction we have neurons that
fire on different different concepts again the alternating linearities and nonlinearities
and just just interacting with the models they're not that you know they're not that different
now what is incredibly different is how the models are trained right the if you compare the size of
the model to the size of the human brain in synapses which of course is an imperfect analogy
but there's something like still maybe a thousand times smaller and yet they see maybe a thousand
or ten thousand times more data than the human brain does if you think of you know the number of
words that a human hears over their lifetime it's a few hundred million if you think of the number
of words that a language model sees you know the the the the latest ones are in the trillions
or maybe even tens of trillions and and that's just you know that's like a factor of like ten
thousand difference so it's it's as if we've kind of you know that that neural architectures have some
you know there's lots of variants to them but they have some universality to them
but that somehow we've climbed the same mountain with the brain and with neural nets in some very
very different way according to some very very different path and so you know we get systems
that when you when you interact with them are you know I mean there's still a hell of a lot they
can't do but I don't see any reason to believe that they're you know fundamentally different or
fundamentally alien but what is fundamentally different and what is fundamentally alien is
the completely different way in which they're trained you said that alignment and values are
not things that will just work at scale and we've talked about constitutional AI and some of the
different viewpoints there but can you extrapolate on that view yeah I mean this is a bit related
to the point that I said earlier about uh you know that that there's kind of this fact value
distinction right you cram a bunch of facts into the model you trained it on you know everything
that's present on the internet and and it kind of leaves this this blank space or this this this
undetermined variable um I you know I I basically just think that like you know it's it's up to us
to you know to determine the values the personality especially the controllability of
these systems there's another sense in which I would say this which is just that you know
that that naturally these are statistical systems and they're trained in this very
indirect way right even even the constitution it's like the constitution is pretty solid
but then the actual training process uses a bunch of examples it's kind of opaque
and of course the the part where you put in place tens of trillions of words like no human
ever sees that so it's it's still I think very opaque and and hard to track down and and so
you know I think it's I think it's very prone to failures and you know this is why we focus on
interpretability steerability and and reliability we we really want to kind of tame these models
make sure that you're able to control them and that they do what humans want them to do
I don't think that comes on its own you know any more than like that comes on its own for for
airplanes right the early airplanes like you know probably they wouldn't crash every time you fly
them but like you know I wouldn't wouldn't want to you know I wouldn't want to get in the right
brothers plane every day and just bet that every day it would not crash and would not kill me
it's it's just not it's not safe to that standard and I think today's models are basically like that
why is mechanistic interpretability so so hard to do yeah so you know mechanistic interpretability
is this you know it's an area that we work on which is basically trying to look inside the models
and you know and and kind of analyze them like like like an x-ray and I think the reason it's
so hard it's actually the same reason why it's hard to look inside the brain right the the brain
wasn't designed to have humans look inside it it was you know it was designed to serve a function
right the the the the interface that's accessible to to other humans is you know your your speech
not not you know the actual neurons in your brain right they're they're not designed to be read in
that way um of course the advantage of reading them in that way is you know it's it's it's you get
something that's much closer to a ground truth not a perfect ground truth but you know if I really
understood how to look in your brain and understood and understood what you were thinking you know it
would be much harder for someone to to deceive someone else about about their intentions or for
behaviors that might emerge in some new situation to to not to not to not be evident um so there's
a lot of value in it but but yeah there's you know there's there's nothing in both the case of the
brain and in the case of the large language models you know they're they're not designed or trained in
a way that makes them easy to look at so you know it's a little it's a little bit like you know we're
inspecting this alien city that wasn't built to be understood by humans it was built to function
as an alien city uh and so we might get lucky we might get clues we might be able to figure it out
but there's kind of no guarantee of success we're on our own uh and and so that's what we do we kind
of do our best that said I am becoming increasingly optimistic that interpretability uh can be I don't
know about fully solved but that it can be an important guide to showing that models are safe
and and even that it will have commercial value in you know kind of in the areas of like trust and
safety or classification filters or moderation fraud detection I think there's even legal
compliance aspects to interpretability so my co-founder Chris Ola um has been working on
interpretabilities run a team at Anthropic for the last two and a half years before that when
we were at open AI he ran a team that worked on interpretability of vision models for three years
before that and for that entire period uh it's it's been just basic research right there's been no
you know commercial or business application you know Chris and I have just kept it kept it going
because we we believe that this is something that will pay off from a safety perspective
and maybe even from a business perspective and now actually for the first time um you know you
you will you will you will see by the time this podcast comes out uh we're we're we're releasing
something that shows that we've really been able to solve something or make good progress towards
solving something called the superposition problem um which is which is that if you look inside a
neuron it corresponds to many different concepts we found a way to disambiguate those concepts
so that we can see all the individual concepts that are lighting up inside inside a uh inside one
of these large LLMs it's not a solution to anything to everything but it's just a really big step
forward and for the first time I'm optimistic that you know give us two or three years I don't know
for sure but we might actually be able to get somewhere with this and depending on your level
of understanding of all this stuff why would that be important for safety yeah so um I would go back
to the x-ray analogy there right um you know if I can really look inside your brain and you know if
if if if if if if if I can say this is what's happening I mean you know I can ask you questions
and like you know you can say things that sound great and you know I have no idea if you're telling
the truth or if it's all just bullshit right but if I if I if I look inside your brain and I
have the ability to understand what I'm seeing then it then it becomes much it becomes much harder
to be misled similarly with language models you know I can test them in all kinds of situations
and it'll seem like they're fine the fear is always oh but you know if I talk to language model in
this way I could get it to do something really bad or if I put it in this situation it could do
something really bad on its own that's always the fear right that's the fear we have every time
we deploy a model we've had a hundred people test it we've had a thousand people red team it
but when it goes out into the world a million people will play with it and and one of them
will find something that's that's truly awful uh and you know you know we'll we'll find oh well
if I use this trick for talking to the model you know it's it'll it'll it'll it'll you know it'll
finally be able to produce that that that bio weapon or oh if I put it in this place where it has
access to infinite resources on the cloud it'll you know just just self replicate itself infinitely
and and so interpretability is at least one attempt at a method to address that problem to say okay
instead of thinking instead of trying to test the model in every situation that it could be in which
is impossible we can look inside it and try and decompile it and say what would the model do in
this situation well we understand that the algorithms that it's following we understand
what goes on in different parts of its brain at least to some extent so we can we can pose the
hypothetical and say hey you know what would what would happen in this this whole part of the space
what would happen in this whole class of areas and you know if we can do that we have some we have
kind of some ability to exclude certain behaviors to say okay we know the model won't do that which
which you never have behaviorally you know it you know it's just it's just like it's just like humans
you know you know i'm like you know what what would you do in a life-threatening situation
i don't know what i would do you know i don't know what i would do in a life-threatening
situation i don't know what you would do in a life-threatening situation it's hard to know
until you're actually in the situation but if i knew enough about your brain i might be able to say
which of you on open source models yeah i mean that's uh you know that's a um obviously a complex
topic i mean i think as with many things and as with many things in you know in in in ai versus
the rest of technology you know from a normal technological perspective i'm extremely pro open
source like i think you know it's accelerated science it's accelerated innovation it allows
you know errors errors to be fixed faster and development to happen faster and i certainly
think you know for the smaller models for the smaller open source models uh this is this is
this is true for ai as well and i don't see much danger to smaller models therefore i think open
source as it's being practiced by every open source model that's been released up to this point
seems perfectly fine to me um my worry is more around the large models um and my worry in particular
is that you know these models that are offered via api and i mean i'm talking about models of the
future that really are dangerous not not you models in two or three years maybe one year not not
today's uh not not not today's models uh if they're offered by api or even if you have fine-tuning
access to them there's a lot of levers that you have to control the behavior of the model right
you can put in your constitution don't produce bio weapons right and then if the model does it
anyway you can you can basically make changes to the model you can say okay we're just retracting
that version and serving a new version of our model that patches a particular hole you can monitor
users so if a million people are using the model and within that there's these five bad actors in
this terrorist cell you can use your trust and safety team to identify the terrorist cell cut
them off and even call law enforcement if you want to do it um so it it really provides an ability
you don't have to get things right in the first time and if something dangerous happens you can
you really have the ability to fix it with with models where their weights are released uh you
don't you don't have any of that control right the minute you release the the the model all basically
all of this control is lost and so that's our concern that that doesn't mean by the way that
large open source model shouldn't exist but the way we put it in our responsible scaling plan
is we say okay when models get to the the level where they're smart enough to create these dangerous
capabilities and the next one for us is asl three we're at asl two right now uh then models have to
be tested for dangerous behavior according to the complete attack surface according to which
they're going to be released in reality so if you're just releasing an api then you have to test
that you know you can't build a bio weapon with with the api if you're releasing the model with
an api in fine tuning then the people who are testing the model have to have to mock up the test
with the fine tuning if the model's being released in practice then the right test to run would be
i'm a mock terrorist cell i'm i you know uh i get the weights released to me i can do anything i want
with those weights um is there some way to release the release the weights of the model
so that they can't they can't be abused i i think there might might very well be but i think people
who want to release model weights have to confront that problem and have to find a solution to that
problem i'll say by the way because uh there's a person on my team who uh who this is this is one
of their pet peeves um the word open source i don't think is necessarily appropriate in the case of
all of these models i think it is appropriate in the case of like you know small developers and
companies where kind of their whole their whole business model is about is you know
their whole business model is about open source but when much larger companies have
released the weights of these models they generally have not released them under open source
lice under under open source licenses they've generally asked people to pay them when they
use them in commercial ways so i would think of this as you know less open source and more
that model weight releases a particular business strategy for these for these uh for these large
companies and again i'm not saying that that model you know model weights can't be released um you
know i'm saying that the you know the tests for them need to be commensurate with the issues
and that we shouldn't automatically say oh open source open source is good some of these are not
open source they're they're the business strategies of large companies that that involve releasing
model weights paul christiano recently on a podcast said he thinks there's a 50 chance
i think the way he phrased it was that his the way he he ends up passing away is something to do
with ai do you think about percentage chance doom or yeah i i think it's popular to give
these percentage numbers and and you know i mean the truth is that i'm not i'm not sure it's easy
to put to put a number to it and if you forced me to it would it would fluctuate all the time
you know i think i've i think i've often said that you know my my chance that something goes
you know really quite catastrophically wrong on the scale of of you know human civilization you
know it might be somewhere between 10 and 25 percent when you put together the risk of something
going wrong with the model itself with you know something going wrong with human you know people
or organizations or nation states misusing the model or or it kind of inducing conflict among
them or or just some way in which kind of society can't can't handle it that that said i mean you
know what that means is that there's a 75 to 90 percent chance uh that this technology is developed
and and and and everything goes fine in fact i think if everything goes fine it'll go not just
fine it'll go really really great um again this stuff about curing cancer i think if if we can
avoid the downsides then this stuff about you know about curing cancer extending the human
lifespan um you know solving problems like like mental illness i mean i this all this all sounds
utopian but i don't think it's outside the scope of what the technology can do so you know i i i often
try to focus on the 75 to 90 chance where things will go right and i think one of the big motivators
for reducing that 10 to 25 percent chance is you know how how great it'll you know is trying to
increase is trying to increase the good part of the pie um and i think the only reason why
i spend so much time thinking about the tent that that 10 to 25 percent chance is hey it's
not going to solve itself you know i think the good stuff you know companies like like ours and
like the other companies have to build things but there's a robust economic process that's
leading to the good things happening it's great to be part of it it's you know it's it's great to be
one of the ones building and causing it to happen but there there's a certain robustness to it and
you know i find i find more meaning i find more you know when this is all over i think you know
i personally will feel i've done more to contribute to you know whatever utopia results um if we focus
you know if if i'm able to focus on kind of you know reducing that that risk that it goes badly or
it doesn't happen because i think that's not the thing that's gonna that's gonna you know that's
not the thing that's gonna happen on its own the market isn't going to provide that do you worry
more about the misuse uh by people misusing it or the ai themselves or is it just different timelines
yeah uh partially different timelines i mean it you know if i had to tell you i would i would say
the the misuse to me seems more concrete um and i think you know will will happen sooner
you know hopefully we'll stop it and it won't happen at all um i you know i think the the ai
itself doing something bad is is also a quite significant risk um it's a little off in the
future and it's always been a bit more shadowy and vague but that doesn't mean it isn't real um i mean
you know you you just look at the rate the model is getting better and you look at something like
Bing or Sydney it really gives you a taste of like hey these things can really be out of control
and you know psychopathic and the only reason Bing and Sydney didn't cause any harm is that you
know they were out of control and psychopathic in a very limited way limited both in that you
know it was confined to text and limited in that you know it just wasn't that smart you know tried
to manipulate the reporter tried to get him to leave his wife but like it wasn't really you know
it wasn't really compelling enough to you know to get a human to fall in love with it but someday
maybe a model will be uh and you know maybe it'll be able to act in the world and then you put all
those things together and you know i i think there is there is some risk there i think it's
harder to pin down but i'm personally i'm worried about both things and you know i think i think
our job because we see such a positive potential here is you know we have to you know we we have
to we have to find all the all the possible bad outcomes and like shoot them down we have to get
we have to get all of them and then if we get all of them then then then you know then then we
can we can live in a really great world hopefully is if you could wave your hands and have everyone
follow a single policy would it be a responsible scaling policy would everyone have one of those
yeah i mean i think you know i think i think if we make a constraint of like realism right where
you know it's like you know i in fact can't wave my hand and get you know everyone in you know
china russia or somewhere else to you know stop building these powerful models um you know like
they're they're just some levels of like world or international coordination that are just
are just are just not going to happen because of because of realism so if you stipulate that like
some you know you can't you can't you can't just make everyone stop or you can't just make
everyone build in a certain way the idea that hey you know for most things people should just be
able to build what they want to build but but you know we're coordinating off these you know these
particular levels of capability these particular points in the in the in the development curve
where something concerning is happening and say hey mostly do what you want but you've got to take
this there's a small fraction of stuff you've got to take really seriously and you know if you
don't take it really seriously you're the bad guy um you know that's something that i think
i can reasonably recommend that everyone's that everyone signed on to that there's some
sacrifice to it there's some loss to winning the race but it's it's you know it's only as much
sacrifice as is as is as is needed and because it's so targeted i think you can make a strong
moral case that hey if you don't do this you're an asshole how do you think about the trade-offs
between building in public and having people aware with what you're doing with on the flip side
maintaining that secrets or that that the appropriate things stay within the order yeah yeah i mean
you know this is this is one of these uh this is one of these kind of difficult trade-offs right so
you know definitely an orc benefits from you know kind of everyone knowing about everything
but on the other hand as we've seen with multiple ai companies um you know secrets secrets leak out
and you know even just from a commercial perspective forget safety like with models built in
you know the next year or two let's say a model costs three billion dollars
and you have an algorithmic advance that you know means you can build the same model for
one point one point five billion dollars right these kind of two x two x advances along the
scaling curve have occurred in the past and you know may occur in the future and you know companies
including ours may may may may be aware of such advances um so so basically that's like three
lines of code that's worth one point five billion dollars um you know you you don't want a wide set
of people and you may not even want everyone within your company to know about them and that
anthropic at least people have been very understanding about that people don't people don't want to
know these secrets um people are people people are on board with the idea hey you know it's not it's
not a marker of status that you know these secrets these secrets you know should be known to the tiny
number of people who are actually working on the relevant thing plus you know the ceo and a couple
a couple other folks who you know need to be able to put put the entire picture together right this
is this is this is kind of um compartmentalization and and and need to know basis and of course it
has some costs because information doesn't propagate this freely but but again you know just as with
the rsp you know let's let's take the 80 20 let's take the little the few pieces of information
that are really you know that are really essential to protect and and be as free as we can with
everything else if you hadn't gone down this ai path would you be doing academic work right now
i i think that was my assumption like that that was what i kind of always imagined doing i imagined
being a scientist and you know scientists work at universities but the really interesting thing about
the the you know this this ai boom is that to really be at the forefront of it uh you know you
have to have these huge resources um and you know i think i think the huge resources are are
basically you know they're they're basically only available uh at companies uh you know first it was
the large companies like like like google but you know more recently you know startups like ours
have been able to raise raise large amounts of money and so i i was kind of drawn to that direction
because it had the ingredients necessary to uh you know to to build the things we wanted to
build and study the things that we wanted to study as scientists and you know i would say many of my
many of my co-founders feel the same um one thing one of my co-founders who is a physicist what he
often you know what he often brings up is and you know it's it's it's kind of more uh more um more
an academic question because i don't think things are going to go in this direction but you know he
said you know in in in in my field we build these you know 10 billion dollar telescopes in space and
you know we build these 10 billion dollar particle accelerators why did the field go the way why why
did the field go you know kind of kind of kind of go in this direction instead right why didn't all
the ai academics get together and you know build a you know build a 10 billion dollar cluster
why did it happen in you know in in startups and in large companies um i don't really know the answer
to that um things could have gone the other way but it doesn't doesn't seem like that's the way
things have gone and i you know i don't know if it's for the better or the worse i mean we've learned
a huge number of things uh you know by by by working with customers and seeing how these
things impact the economy so maybe the path things went is the best best path they could have gone
um how did i actually don't know how did they get access to capital in the in the prior or in the
alternative way of doing that i was at government grants and stuff yeah these large telescopes
they're often kind of like government consortia or private um you know kind of kind of kind of
large-scale private philanthropy it's it's it's honestly this huge patchwork mix i'm surprised
it even happens because if i think about it happening in this field i i just i just i just
can't imagine how it would happen but in these other fields somehow they've made it work um i
don't actually know how but so i don't know that's that's just like a that could have been like just
a weird alternate history of our of our of our industry that didn't happen and you know i i very
much doubt it's going to happen although who knows um you know we went on we went on this path instead
and i don't know there's a lot that's exciting and interesting about about this path and you know
one way or another this is this is the situation we're in has working with your sister been as
fun and saving the world as you had hoped it when we were in your kids yeah it's surprisingly
similar to what i imagined i mean you know if you were to if you were to look back on the
things we were saying to you know to one another as like an adult observing it you would have been
like this is crazy this is crazy you know kids dream of course but um no i mean uh you know
it's it's it's just amazing that we're able to work on this together very cool daria thanks for
doing this thanks for having me

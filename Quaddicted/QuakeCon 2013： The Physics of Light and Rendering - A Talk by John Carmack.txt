Hello, everybody.
We have a good crowd for John's second talk.
It's very exciting.
This is the first year that John will be talking twice.
A couple of things to know.
John will talk for about an hour or so,
and then we'll have 30 minutes for questions.
The mic is right there.
That's actually just right there.
So just line up when we get to the questions.
Try to keep your questions on what John talked about.
If you get up and ask when Doom 4 is coming out,
I'm going to kick you in the knee.
So right there.
So I will not waste any more time,
but you guys in the back,
because John's going to write on the board,
and we have plenty of empty seats here,
you can file in.
Don't worry that there's reserved seats there.
Just go ahead and sit in them.
All right, I will give you guys Mr. Carmichael.
Okay, so I guess this is sort of going to be
like a schoolroom session.
I had deluded myself for a little while
that this would be the first talk
where I ever actually made slides to present,
but it didn't actually come to pass,
so it's going to be notes and talking
and some scribbling on the board again.
So almost all of what we do in game development
is really more about artistry.
It's about trying to appeal to people,
but there's the small section of the small section
of what goes into the games
that's drawing the pictures on the screen
that you can at least make some ties
to the hardest of hard sciences.
And while it's great that people are researching the psychology
and the different ways that people think
about compulsion loops
and some of these other game design topics,
the raw physics that goes into rendering
just kind of goes through the heart of physics,
where it goes through kind of the all-star list
of physics with Newton's optics
and Maxwell's equations and Einstein's relativity,
and it's kind of neat to think
that this is sort of brought to bear
in the techniques that go into
sort of making the games that we play.
So at the start, you think,
well, okay, we see light,
so what actually is light?
And we've got a definition now
that lights the sliver of the electromagnetic spectrum
that we can actually perceive,
but that has a really long and complicated history
for how we sort of reached that conclusion
and how it's not really as clear-cut
as most people would like it to be.
Optical research started
kind of all the way back with a lot of the Greek philosophers,
but Newton did a whole lot of work
with breaking light up with prisms,
seeing how white light was actually composed
of all the different colors of the spectrum,
and they add together to make what we perceive as light.
And then for, there was a centuries-long debate
about whether light was a particle,
like this little tiny billiard ball,
these photons that you shoot out,
or a wave effect, like all the things that you see
in waves in water and waves in matter, so on.
And finally, we reached the conclusion
that, well, it's a wave particle duality
that quantum mechanics talks about,
and this is very unsatisfying
when you begin looking at this,
but it's really pretty much irrefutable.
There are these straightforward experiments
that can be done to show that you look at it one way,
if you look at it another way, it's a light,
or it's a particle.
So, luckily for computer graphics,
we hardly care at all about that,
only when you start looking at some aspects
of surface reflectance models do you start caring at all
about some of these quantum mechanical properties of light.
For the most part, we can look at light
as zillions and zillions of little billiard balls
shot out from lights and bouncing off of things
and eventually reaching our eyes
so that we can perceive them.
There's a lot of simplifications that have to happen
when you talk about simulating this.
There's a lot of engineering disciplines
like thermal management, radio engineering
that do simulations of the electromagnetic spectrum,
just other parts of it, how they bounce around,
interact with things, and this is done all the time,
and it works, it really is science.
So, you can say rendering an image
or deciding how much light reaches a particular area
is about as basic of a science as it comes.
There's not any artistic measure in here.
There are tons of other aspects when you get into perception
that do become questions about,
well, maybe there is artistry that goes into producing something
when you've got an impression that you want,
but when you're talking about simulating an environment,
which most of what we do in sort of the hardcore
FPS type games is we are pretending
that we've got this virtual world
and we're running a camera through it
and we're trying to simulate what's happening in various ways.
And nowadays, we know what we would have to do
to make that almost perfect.
We just have nowhere near the computing capacity
to do really, really high-level simulations.
But we can trace, it's useful,
even if you're not going to do the right thing,
to at least understand what the right thing is
and then understand which trade-offs you're making
and make them with sort of a clear head
rather than accidentally backing into trade-offs
that may or may not be really the best way to go about things.
So, so many that...
It took a long time for people to realize
that these other phenomena, things like radio waves,
and there's a lot of confusion in 19th and 20th century physics
about which things were particles and which things were rays,
and we still have kind of mixed-up terminology
when you talk about cosmic rays that are actually particles
and you talk about alpha radiation and beta radiation
and these things that are particle-based
rather than being rays from the electromagnetic spectrum.
But we use this stuff all the time for radio waves.
I, you know, your Wi-Fi has two gigahertz.
I, you know, I expect frequencies.
You know, the visible light rays are up in the, you know,
the terahertz range, many terahertz.
But they're basically the same thing.
They just differ in how they interact with matter.
They're produced in somewhat similar ways,
but the different things change.
They behave differently when they interact with other things
based on their wavelength, which is why X-rays can shoot
through things, radio waves can go through some things
that the visible light pretty much bounces off of.
So another important critical thing, really, is that photons,
they have little bundles of light that we talk about,
they are absolutely quantized.
It's, again, part of the quantum weirdness
that you can't send off this arbitrarily divisible amount,
there is an almost unbelievably large number of them.
You know, given light that's throwing out is, you know,
I can just say zillions with a straight face
because it's a very large scientific notation number.
It's not trillions, it's not quadrillions,
it's even more than that, that are coming out
in terms of these bundled little quanta of energy.
Now, they do have characteristics to them.
If we treat them as little billiard balls,
in computer graphics, we are generally looking at
only a few different spectrums,
a few different wavelengths in the spectrum of light,
and that has to do with an aspect of the human visual system.
While there are this incredibly divisible spectrum of light
that goes out, we're only susceptible to three sort of styles of light,
and they're not even individual frequencies.
That's why we can get by with red, green, and blue
for our monitor's emissive spectrums
because we only have three types of color receptors in our eyes,
and I often think how it would be really interesting
if you could look at all of these other spectrums bouncing around,
and that's what thermal imaging and some of these other things
let you sort of get a peek into it,
and that's only light that's very...
that's em radiation that's very close to the visible spectrum,
the infrared.
It would be much more bizarre and interesting
to be able to visualize radio waves in a real-time space
to see all the multi-path that's causing your Wi-Fi to be weird in specific ways,
why moving something over here causes the radiation
to change so much at your antenna
to make a difference in your reception strength,
and these are all things that have a bearing to what you do with light transport,
as well as other wave phenomena like audio,
like really, really high-end audio processing
is the exact same thing as what we treat light processing.
You send out energy, it bounces off of all sorts of things in the world,
and eventually arrives at something that's going to perceive it,
which would be your ears in that case versus your eyes.
So to kind of start with the path of a photon,
of what it would take,
you've got something creates the photon,
and for the longest time in our human existence,
about the only thing that we saw creating photons was a great deal of heat.
You heat things up hot enough,
and photons start coming off of them.
You heat it up enough, it starts glowing a dull red,
you heat it up more, it starts getting more yellowish,
and towards white, as more and more of the colors of the spectrum
are emitted from these hot things,
and obviously the sun is a very hot thing,
where you've got a fusion reactor going,
and the light that comes off of that is all of these atoms giving up some energy.
So photons carry energy away from where they came from,
and this is radiative heat transfer,
where something gets hot,
if you leave it all by itself there,
it glows and it eventually stops glowing,
it cools down, going down through the spectrum,
getting cooler and cooler until you don't see any visible light,
because it's actually lost much of its heat.
On Earth, radiative heat transfer is the least effective form of heat transfer.
You get much more from conduction,
where it just kind of goes through the actual physical contact
into other areas as the heat spreads out,
or convection, where moving currents of air or water take the heat away,
but in space, radiation is the only way you lose heat,
and in aerospace engineering, this is extremely important.
Things like the areas like the International Space Station and Spaceships,
they have to worry a whole lot about thermal management,
because the only tool they've really got is radiation.
You see these enormous solar panels where they collect solar energy,
but a lot of space vehicles have to have enormous radiators,
where they actually let the energy go out from the vehicle,
otherwise they would get hotter and hotter.
It's important to note that even if it's not glowing so that we can see it,
everything's still radiating,
so you don't see the Space Station glowing red hot,
it's just glowing at whatever its normal temperature is,
which can be perceived with infrared sensors,
but it slowly loses energy and it eventually reaches a balance,
that's why something stuck out in the sun in space doesn't get hotter and hotter.
Eventually it reaches the point where the light that's coming in and hitting it
is equaled by the radiation that's leaving it,
and there are, like you can make,
we've made rocket engines that are radiatively cooled,
where they burn 5000 degrees or so inside,
and they get so blindingly white-hot on the outside
that all of the energy that's not going out the nozzle that's soaking into the walls
is radiated away as a whole lot of light,
and this is essentially what old-style incandescent light bulbs were.
You had a tungsten filament,
you made it really hot by pushing electrons through it,
and it got hot enough and it started glowing,
and if you watched closely, if it was a very, like a heavy filament,
you could watch it warm up or especially shut down,
it would go through, kind of ramp through the temperatures, you would see it,
be red and get up to white-hot,
and then when you shut it down it would cool down through yellow
and back through red before finally settling back to radiating
in non-visible regions at sort of room temperature eventually.
Nowadays we have a lot more efficient ways to create photons
with fluorescence and LEDs,
things that are tuned carefully to just barely nudge the electrons in the atoms
out to an excited state,
let them collapse back down and spit a photon out.
For the most part, photon emission is random in terms of which direction it goes.
When you look at radio engineering,
there's huge bodies of literature for intended design
that determine how you can make it slightly stronger or weaker in different directions,
but there's still a very fundamental nature of randomness,
which is, again, the quantum mechanics aspect of things.
At a very low level, natural events are completely random,
and you can't just say,
I only want photons that are going to come out of the left side of this material.
So you get a photon that pops off in some random direction.
It may go straight for...
If it's coming from a distant star,
it could go straight for trillions of miles,
more or less just traveling through space.
There's little bits of general relativity with warping of light that can happen,
but for the most part, it can continue on indefinitely.
It's a self-propagating wave.
So it pops off of some atom somewhere,
maybe flies through space for a billion trillion miles or something,
comes in, finally hits our Earth's atmosphere,
and then starts interacting with the atmosphere in some way.
Every change in density that visible light goes through
will result in it bending its path somewhat.
This is called refraction.
The most obvious case when you look at it is things like prisms and lenses
where you can see the light really strongly warped,
but it happens in any sort of density change,
going from the vacuum of space to the outer reaches of our atmosphere,
and then every change in pressure or temperature changes the density,
and that causes very slight and subtle movements of the changes in the direction of the light.
This is actually why stars twinkle out at night.
If you're on a clear night and you see stars coming in from billions or trillions of miles away,
it's going completely straight till it hits the upper atmosphere,
and then it may slightly deviate just tiny fractions of degrees,
and this can cause the very small number of photons that you're seeing there
to kind of come and go or move around in different ways.
But the most important thing from a computer graphics standpoint
are the effects that happen when it hits more solid matter,
solid surfaces, or even liquid surfaces,
and that's where it has the opportunity to generally...
Well, even gas, you can wind up having the case of absorbing the photon.
This happens rarely in gas.
You can pass through hundreds of miles of atmosphere
and not have too many of the photons absorbed,
but it happens very rapidly in solid matter.
A typical photon, when it hits a surface, might penetrate a little bit into it.
A surface like metal will bounce off of just the first several atoms.
It doesn't take many molecules or many atoms of metal before you can reflect light out,
which is why you can make these super enormous space mirrors
that are just a very tiny sputtering of aluminum on some plastic film,
and they can actually make solar sails or giant solar collectors and concentrators.
But for most other materials, the light can penetrate a little bit further into it.
As it interacts with the molecules, it can either be absorbed,
raising the temperature a little bit,
going into eventually making it hotter so that it starts radiating out at some level,
or it can redirect the photon in some way.
You've got the minor redirections from the refraction
and much stronger ones when it interacts and bounces off of a solid surface.
Now, there's a ton of different names.
There's literally a couple dozen different names for the different ways that light can interact with surfaces.
There's all the different types of scattering.
Of course, reflection, reflection, reflection can be split up into specular reflection, diffuse reflection,
and there's all sorts of different subcategories.
Optics is a huge topic.
There are societies dedicated to every aspect of it, and there's huge terminologies for all of it.
But for the most part, you can say, photon comes in.
If it's not absorbed, it's going to be kicked out some other direction,
and then it can go and interact potentially with the atmosphere or potentially with another surface.
And eventually, it's either absorbed, or eventually it is absorbed somewhere,
but for the most part, they're absorbed into the surfaces around us,
but a tiny, tiny fraction of all the photons that are bouncing around eventually hits our eyes.
And even when it gets to our eyes, which are mostly transparent,
there's this chance that the photon hits and it specularly reflects off of our eye,
and it made it all the way out of the billions of possible traces, made it to my eye,
and then decides to specularly reflect off some other direction.
But most of it that hits the eye and hits the lens gets through,
propagates through the vitreous and aqueous humor and all the little biological parts of the eye,
and hits receptors in the back of our eyeballs that turn those eventually into neural impulses that our brain works with.
Now, our eyes can actually be quite sensitive.
The rods, the non-color sensitive part of our eyes, when they're fully dark adapted,
if you've been staying outside in a dark area for 20, 30 minutes,
single photons can cause chemical reactions to happen inside the rod cells.
It takes a handful of them, a couple dozen for it to turn into a neural impulse,
but it is possible for people that, especially in the old days,
people watching for things on ships in moonless nights that might be out all night with nothing but faint starlight,
you can have cases of just handfuls of photons coming off of something,
being registered and showing up and people acknowledging their existence,
which is pretty amazing when you think about these incredible subatomic particles,
not even particles, but incredible, the scope of that being detectable by us as biological entities.
And there are limits to what you can wind up detecting with light.
The visible light that we see has a wavelength and you can't really deal with things that are smaller than that,
which is why you're never going to have a real picture of an atom or a molecule,
because those are much, much smaller than the wavelengths of light.
You eventually use electron microscopes and then scanning tunneling microscopes
and these other things that don't deal with light at all to take those super tiny pictures,
like the boy in his atom movie that IBM Research did, which was done with a little raster grid of atoms,
which is really, in the fundamental sense of the word, deeply awesome,
that we are dealing with matter, the very constituents of everything at that level,
and we can make a little movie out of it.
But those pictures have nothing to do with light, nothing to do with rendering
and basically the techniques that I'm talking about here.
That's a completely different way of sensing what's going on at that level.
So to recap the basic pictures of this, you've got something, a sun up here,
spits out some light, travels through space, gets to the atmosphere on the Earth,
maybe bends a little bit, maybe just goes straight through,
comes down, hits a surface, maybe gets absorbed, maybe hits something else,
you've got walls and rooms and bouncing around in there,
and eventually, if we're seeing it, reaches somebody's eyeball inside.
And that's the physics of what happens.
It's really well understood.
It does come down to a lot of data acquisition and characterization.
When you talk about how the critical interactions with the surfaces,
when you've got your basic theoretical thing, if you talk about a flat surface,
you say, light comes in, what happens to it?
That's the question of surface response.
If you have a perfect mirror, and it's worth noting that to be,
you don't have to be perfect on an atomic level to be a perfect mirror.
You only have to be perfect at the optical level, which is somewhat larger.
So people can make basically perfect mirrors, just highly, highly polished things.
A perfect mirror will have the photon reflect off in this exact reflection.
If you take the normal to the surface, you wind up with equal angles there.
So, highly polished surfaces act like this.
When you get a reflection off of something like the surface of water,
it'll behave like this.
But most of the surfaces that we look around us do not behave like this.
We have a spread of the energy where it comes in,
and it bounces off to some degree in every direction.
No matter which way you look at most surfaces,
you see, again, zillions of photons coming in.
Some of them go in every direction.
They just go in a direction that's biased based on the type of surface that it is.
A surface that, one of the easy things that a lot of times is approximated,
both in the engineering sciences and in computer graphics,
is to assume that the surface reflects perfectly diffusely,
or it's a Lambertian surface.
And what that means is that no matter which way the light comes in,
if it hits it completely edge on, completely straight on,
it has an equal probability of going in every direction.
And there are some materials that are close to this.
If you take something like a block of chalk, white chalk,
that behaves almost as a perfect diffuse reflector.
If you light it from one position,
and you look at that, like, a little scribed-out area on it
from any different area around it,
it will appear to have about the same amount of energy coming out of it.
But there are...
All surfaces are more complex than that, though.
Most of them will say,
if you've got light coming in here,
there will be more of it coming out around the reflection area,
and some general amount coming out in all different directions.
But these can actually get quite complicated.
And the simplifications that we use in graphics sort of approximate these,
but you can measure these with specific tools
that go in and take lots of samples from moving the lights around.
Because it depends...
Unfortunately, this is one of the areas where it does get
not so great for computer graphics.
It depends both on the incoming direction and the upcoming direction.
And those are two angles in each one,
so it winds up being a four-dimensional equation
to say how light comes in here,
how does it come out in some other direction.
And in fact, it gets worse than that
because very few things do reflect just off of this upper surface.
Most of the time, the light will go in,
go below the surface, bounce around a little bit,
and shoot out some other direction.
So if you're saying,
well, my photon comes in here,
not only do you have to say if you're being really, really accurate,
which angle does it come off of,
but also how far away from the original point does it come off of?
Or if it's a thin surface,
how does it come out on the backside?
You may have other setups coming there.
When you look at, like, a leaf in the sunshine,
you've got a lot of the energy,
bounces off the shiny top face,
but a lot of it diffuses through and comes out on the backside.
So these are not pleasantly analytically tractable things.
They wind up being big tables of data.
And one thing that's important to remember is
when you see, like, tables of data that are collected for things,
don't necessarily capture all of the important characteristics of the surface,
where if you take one of these sensors that you can capture,
a table of data here,
if you did have your perfect mirror reflector,
it's almost certainly not going to have the exact sample exactly where you want.
So, but eventually data does win.
Just as we increase resolution on things,
we'll have higher and higher resolutions for our surface models.
And we'll get closer and closer to reality for what we're simulating.
So to go as kind of a capsule history of computer graphics rendering then,
when computer graphics started off,
if you look in the 60s, 60s and early 70s,
computer graphics research focused on the hidden line problem.
We had line-oriented displays,
either true vector displays where,
like the old video game arcade games like...
I'm blanking out now.
Yeah, like Asteroids is the best example
that are actually drawn by raster beams moving around
where they really are true line displays.
There's no raster, there's no edge aliasing,
and all the different games like that
were what the earliest computer graphics systems were basically like that,
where they were vector displays.
And once people learned how to draw,
figured out all the basic projective math to say,
all right, I've got my cube here.
You know, I want it to look like that,
but when I draw it, I've got that on there.
How do we figure out which lines that we're going to erase?
And that was, you know, that occupied research for a while
to figure out effective ways to do that
without spending at the time the scary divide costs for different things.
And you'd have lots of interesting work being going on.
But when we eventually got raster displays where we could fill them in,
of course, at that point people filled in the surfaces of the cube,
they're all grayscale at that time,
so you can draw a cube and say,
well, this will be the light face, this will be the dark face,
but that was neat at the time,
but that was not sort of what things look like in reality.
So people started taking the steps that they could to try and say,
what do we need to do to make this more approximate what we see with our eyes?
And this has been a path that's been driven probably more than half
by sort of ad hoc approaches about just,
well, what's reasonably easy for us to do that gets us somewhat closer to it
while there's also been sort of a parallel path of saying,
well, what's the physics actually doing?
How do we make an actual solution for it?
So the earliest things that got added to the shading model
for computer graphics was,
if we assume that there's going to be a light that's at some point,
in the beginning it wouldn't even be local,
you just say light is coming in from this direction.
So we want to be able to say what color or what shade
should each individual surface be based on where that light is.
So you've got the obvious things that if it's not facing the light,
no light hits it and you would draw it black.
So the question about things that are directly facing the light,
so if you've got light coming in,
if you have a surface completely perpendicular to it,
you make that your brightest color.
If you've got a surface that's completely parallel with it,
it gets no light and make that zero.
So you've got some curve that goes between it
to say how bright something should be.
And it turns out that that's a fairly straightforward bit of math
to solve where you have light coming in at a certain angle,
you've got the normal to the surface,
the amount of light that would strike a little surface there
is proportional to the cosine of this angle.
And that's actually, that's not an approximation,
that's actually a bit of ground truth.
If you've got the light coming in
and you've got something coming in at this angle,
a surface that's, let's see.
If you count the number of rays that go in
on something catching four of them directly,
turning it down, only covering two spaces there,
all that actually works out correct.
And this is the basis for a lot of the,
a lot of the real calculations for light transport,
not a hack, actually part of real proper physics measuring.
So once you've got that basic approach,
you go back to your cube
and you get your light coming in
and you've got a brighter face, a brighter face, a darker face
and the faces away from it are completely black
and then most people say,
well, we don't usually see things like that.
So now we get into the fudging and you say,
well, let's just brighten everything up a little bit.
We'll add an ambient term.
So you sort of just add this minimum level
to everything on the back side.
It helps a little bit.
If you've got a cube, then everything looks pretty much great
because it's a constant color just on the side
that you might not see over there.
But if you've got something more complex,
everything that's not facing away from the light
winds up being the same color and it's clearly not correct.
It's not what you'd like,
but it was all that seemed reasonable to do at the time.
The next step was to start looking at surfaces
that are more than these perfectly diffuse reflectors.
If you make, if you model your cube like this,
it looks kind of like it was maybe carved out of chalk
and it can be a decent representation of that.
But very few of the surfaces that we see around us
are really that simple.
Most things have some kind of a shine or highlight on them.
As we look around,
you can see reflections and highlights on all sorts of things
and the obvious bits of metal and plastic,
little things that you might hold in your hand.
I can look at all these different shines and reflections
on the plastic that I'm holding here.
Now, the observation was made
that the highlights on most objects
that weren't completely mirrors,
they tended to be something like a bright hot spot,
like if you had your sphere here,
you would have a bright hot spot
that kind of faded a little bit around there.
And just by looking at that and saying,
well, what could we do that would be kind of like that?
The observation was made that,
well, if you take this sort of cosine arrangement here,
this makes this nice broad fall off.
It makes a, you know,
over the entire surface of the sphere coming from that,
it'll fade off to halfway around the light.
But if you wanted something that was really tight,
the thought was, well,
we can just take this value and raise it to a higher power.
We can just take this and go, you know,
square it, cube it, take it to the 20th power,
and then it can be done, you know,
effectively, mathematically, quite cheaply.
This has no basis in physical reality at all.
This is a completely ad hoc approach.
But it worked out okay.
And this is what the, you know,
the Fong lighting model was about,
where you separated into your diffuse lighting,
which is the more or less what color the surface is,
and then your specular lighting,
which is what the highlights are going to look like.
So you had this other value to play around with,
and that was the specular power.
And nowadays, I regret using that in my terminology,
where we have power maps,
and nobody understands what those are.
They relate to the, you know, the specular exponent,
what you're going to take something to a power of to tighten it.
The better terminology that's used more often now
is a roughness map, where you have a mapping,
and you also do it in logarithmic space rather than linear,
but more or less, that's still today,
what a lot of graphics involves,
is you've got a roughness parameter,
which affects this exponent that you take this extra vector
to generate your specular highlights for.
And again, it was, it would make,
so if you're rendering your cube,
and you get the light at the right angle,
like if I'm looking at this here,
and the light's over here, you know, it hits that,
and if that's at that right reflection angle,
then you'll get a nice bright shade on there.
That flat surface will catch the light,
and it will glint at you,
and that would be looked at as a real advance for the rendering.
So you've got something that looks diffused,
but when it moves it, when it moves into the light,
it kind of catches a flash of light and fades out.
So the facets on these solid shaded models
started looking better.
Now what, the next thing that people wanted to do is,
okay, we've got enough cubes and tetrahedrons
and dodecahedrons and whatever.
So we want to start making things that look more realistic.
We need to have a teapot, you know,
we need to have a curved surface in some way.
So you make some curved surface like this.
There was a lot of work in the early days
on directly rasterizing curved surfaces,
drawing them directly,
but all the real-time graphics, almost all of it,
has been a matter of turning your curved surfaces
into approximations with flat surfaces.
So you've got something that is theoretically a curve,
but really it's a bunch of facets.
So if you apply the lighting model there to it,
you see all of these facets.
It stands out as like, okay, you've just carved this as,
you've carved this out with all these flat planes
and it doesn't fool you into thinking
that this is this smooth curved object.
So the next step in graphics that went on
was adding the interpolation across the vertexes,
where instead of calculating a value for a face,
you calculate it for a given vertex, for one corner,
and then you just average, you interpolate across there
so that a point here is going to be some average
between three or four of the points that make it up.
And that works surprisingly well.
If you're looking again at a diffuse surface,
it works out just about as good as you'd like.
There are some minor artifacts called mock bands
that you get if it changes too much,
but if your tessellation's okay, that works out all right.
It works out less well with the specular highlights.
And the reason is that your specular highlight,
if you've got, it might show up,
like if you were supposed to have some hot spot right here
in the middle of a surface, if you calculated at the outside,
this is going to be almost zero for the specular, almost zero.
And when you interpolate across it, it's going to have nothing.
You're just not going to see it.
You'll only see a highlight when the specular comes up
at the very edges.
And this is what's still to this day sort of the standard
open GL shading model is.
It's gross shading with calculations at the vertexes
interpolating the colors or parameters across it.
So this model is still with us to this day
for a lot of sort of quick stuff that's not
visual simulation oriented.
If you just write something using lighting with open GL,
that's the model that you get if you turn on specular highlights.
In graphics where they care more about visual quality,
what started happening was interpolating,
not the color across it,
but interpolating the normal,
sort of the curvature across each point
and then applying the lighting model at every pixel.
And at the time this was like a flagrant use of processing power
because we're like, okay, these calculations are expensive.
We have to do these distance calculations, dot products,
exponential power stuff.
And when you just do it at each vertex on your cube,
okay, so you've got a handful of vertexes
that you need to calculate.
But even on an old school display,
you would have hundreds of thousands of pixels.
And so if you're drawing that there,
going from doing this maybe a few hundred times
or a few thousand times to hundreds of thousands of times
was a large use of additional processing power.
But it got you the good-looking areas
where you could have a highlight
that looked about like it should moving across the surface
or sitting on a floor looking stable there as you moved around.
People that have been in following PC graphics
for the last couple of decades,
we've seen games that do not have interpolation
in the different ways where the lighting would change dramatically.
We always had the problem of densely-tesolated characters
or objects and then very low-tesolation on the world.
And the problem that you'd run into with that
is that if you're applying one of these interpolation schemes to it,
you would have something that you could never have highlights
in the middle of a surface, only at the corners.
And there were also issues with perspective math and clipping
that would mean that it would change as a really big polygon
clipped by the edge of the screen
in almost all cases the way people did it.
And this was one of the big things that pushed me
during the Quake timeframe to use light maps for the first time
where instead of... I had seen other games that were doing lighting at the vertexes
and I didn't think it wasn't good enough.
You couldn't get anything resembling a shadow.
You had all these swimming artifacts with the lighting
and it just didn't give what I wanted to see.
And while Quake didn't have any specular highlights,
it did have these...
you had samples every 16 pixels in the light maps
that we interpolated across those
and that gave us the look that was very important for it.
And we didn't get to actually...
it was only all the way up to Doom 3
where we would start doing per-pixel operations like this
to get the much better calculations.
So even with this level of graphics at that time
where you've just got sort of these fog lighting, simple models,
hacks like the specular exponent and the ambient term,
we started to see some offline things being rendered like some movies.
You know, early work, some of the early NASA promotional work
that Jim Blin did were significant in the sort of growth of all of this.
And then we finally saw some feature theatrical films
with like The Last Star Fighter and especially Tron where you would see...
You go back and you look at Tron
and you have a lot of these sort of
gross-shaded, solid-modeled things on there
with your light cycles or recognizers and so on.
And they were doing something...
they were intelligently picking a battle that could be won at the time.
If you said, well, we have to go ahead and render photo-realistic humans,
we were nowhere close to up to that task.
But we could do geometric solid models
that looked good enough to show on the big screen
and that was a pretty big breakthrough.
And simultaneously with this,
there was an alternate approach to the way graphics were being drawn.
So most of the early graphics were done with rasterization
where if you've got your computer screen
and you've got your quad on here,
you would draw this on a computer
by calculating these equations of the lines
and then you would usually just kind of walk across
building up your rows of pixels.
The whole process of hidden surface removal
is another step on top of this
where if you've got lots of cubes,
how do you know which one draws on top of the other one?
And this was another thing, if you look back in research from the 70s especially,
there was tons of work going on on hidden surface removal
of these clever different algorithmic ways.
Today we just kill it with a depth buffer,
we just throw megabytes and megabytes of memory
and the problem gets solved much, much easier.
But this path of rasterization is still with us today.
GPUs don't rasterize in scanline order like this.
They follow crazy winding paths to maximize memory bandwidth
to fill up tiles, to rasterize them in different pieces.
And they rasterize all quads at a time
but it's still essentially a rasterization method
where we have shapes and we figure out how to rasterize them,
we figure out which pixels they're going to cover,
and then we figure out what we want to do with them.
The alternate scheme which was also developed in the later 70s is ray tracing
where instead of saying, alright, I'm starting with my object,
I'm going to take these four vertexes that are in space,
I'm going to take my virtual camera
and I'm going to transform them and find out where they are on the screen
and then fill them in.
Ray tracing goes the other way
where you start off with your camera in space somewhere,
your virtual viewing screen,
and through that, you send rays out into your world
and you intersect them with your cube over here.
And if it hits that cube first, it knows it didn't hit anything behind that.
It's got a surface point there
and it can apply whatever shading model it needs to.
The thing that ray tracing gave, I mean, it's radically slower,
like hundreds or thousands of times slower than rasterization
if you're doing just the most straightforward thing.
If you just want to draw that cube,
you can draw the same thing with rasterization or ray tracing.
It's just going to be a thousand times slower with ray tracing.
But it allowed a couple things that were either very difficult
or impossible to do properly with rasterization.
And the thing that you would always see in ray tracing demos
is your shiny reflective spheres.
So you've got a little chrome ball
and the fact that you could see the world reflected into it
and then back into your eye was the thing that ray tracing could do
that rasterization couldn't do really worth a damn at all.
You would approximate it with environment maps and different things,
but for reflections and for refraction doing those things properly,
ray tracing was really the only good solution.
But it wasn't practical even for most offline work.
I can remember looking at old research papers
of things that are run on deck vax computers
and they talk about the number of hours to render these really trivial scenes,
just a few boxes in an eye, maybe a sphere sitting there.
The idea of rendering complete worlds with it was fantasy at the time,
but it did address some of those problems for the first time
with reflection and refraction.
And it also much more elegantly solved shadows,
which all of this stuff talking about surface interactions
and finding out what you hit with the light,
that kind of dodges one of the really hard problems,
which is saying that, well, the light obviously doesn't reach through things.
If you transform something up here and you transform another surface down here
and the light's up here, this should be in shadow because it's blocked by this,
but that turns out to not be a particularly trivial thing to resolve.
It's basically the same problem of how you view something from your point of view,
but viewed from the light's point of view.
And that can mean that, well, if every light in your scene
has to do a similar rendering process to what your view does,
possibly harder because there are omnidirectional lights in many different cases,
and it's just a tough problem.
And as with so many things, there's a lot of wonderful research in the 70s and 80s
going through about how you do shadows effectively with these different analytic solutions.
In the end, we had a brief period where stencil volumes were an effective way to do things,
but now it's essentially all shadow buffers,
where we really do take every light, render an image from their scene,
and use that to back project onto there to figure things out.
But that was one thing that ray tracing had an elegant solution for.
Again, if you're already a thousand times slower,
who cares if you're another factor of two or three slower?
For every point you hit, you go ahead and say,
I've got my light up here.
I'll trace to the light or to however many lights I've got,
and if there's something that blocks it, then that's going to be shadowed,
and I can take it out.
So ray tracing always had this much clearer abstraction of what you're doing.
It's easy to tell that you're sending out a little array,
you hit something, you determine whether you hit all the other lights,
or if you bounce or refract into something else.
So it's always been easy and clear.
It's just had this thousand times slower problem to deal with.
So the advances that were being made on graphics kind of after this early age
focused on the changes in what you can do with the surfaces as the first obvious thing.
And a lot of these were driven by sort of artistic and aesthetic concerns
where we got, if you pull up a 3D rendering program
and you look at their material stuff, there's a whole page full of options,
things that you can tweak, knobs you can turn, checkboxes you can set,
and each of these had some use case where somebody wanted this
because it made their image generally look a certain way that they wanted.
Very rarely were these things driven by sort of physically correct rendering.
And there was a huge plethora of these things that came out.
Every different program had a different set of options.
You always had this fallback of you've got your diffuse colors, your specular color,
your roughness, this basic fog shading model persists to this day.
But now we have a ton of other things that we can tag on there,
things that are subsurface scattering approximations,
Fresnel lighting, different frequency response on surfaces.
It's like some of the things do have physical basis to them.
One obvious thing, the Fresnel effect is the effect that as you get more and more glancing to something,
the reflection gets stronger and stronger.
And you see this, this is what makes water and glass look like water and glass.
If you look straight at them, you pretty much see straight through them without a whole lot of reflection.
But as you get more and more edge on, even a surface like this,
where when I'm looking at this at this angle here,
I've got a very, very strong clear sense of the slightly wavy reflection of that white line there.
While if I look at it right here, it's barely visible.
So that's a physical effect in reality,
that you can work through the real physics equations of why this happens.
But people, again, sort of called up the trustee, raise a cosine to a power,
and it sort of looks like what we want when we're dotting a couple vectors together.
So that has, that's something that's based off of plausible physics,
but generally only roughly approximated.
And there are other things like that with, like the change in,
some metals get their metallic look because they slightly change colors
as they get towards grazing angles.
So again, you can calculate the real physics for that,
or you can just sign it, kind of say,
well, this color sort of changes to this color at the edges
and start interpolating between them.
But lots and lots of good work and lots of high-budget movies and so on
were built with these sort of very ad hoc techniques.
But sort of in parallel with this,
the other big revolution that was happening was global light transport,
global illumination.
The, comes back to that whole hack of the ambient term,
this sense that obviously where, okay, if I'm right here,
the lights are only directly hitting the outside.
The back of my hand has no direct view to any light,
but it's still quite bright and clearly illuminated.
It's bright because all those lights hit this white, white board,
bounce off of that and wind up lighting my hand from the back.
And you can see, like, color changes,
like if I move up here where it's mostly covered by the blue marker on there,
there'll be blue tints to it.
And this, this recognition that so much of what we consider important
in the visual field is actually indirect.
It's not just a matter of, here's the light, here's the surface,
what's the reaction.
Because we come back to how much of the light gets bounced around.
And there's a, there's a term called the albedo of a surface,
which is what fraction of the light gets reflected versus absorbed.
And there's some tricky terminology with this
because you can have either the total solar albedo
where you talk about how much energy comes off of the sun.
And this is used for climate modeling and some remote imaging
and things like this where you matter.
But I'm, but you've also then got the visible albedo,
which for rendering is what we care about.
And the point is that the best reflectors,
your chrome sphere that's mirrored or your white piece of chalk
or your freshly driven snow,
those can reflect 90-ish percent of the light.
While your darkest surfaces, your lump of black coal
or asphalt in some cases might only reflect 5%.
But when, when you're reflecting 90% of the light,
what that means is that if you're in a room that has mostly white surfaces,
a single bit of light coming out of your light emitter
might bounce around a dozen times before it finally gets absorbed.
So it could take a very complex path before it winds up getting to your eye.
And this is why we could have cases like a dark room illuminated
only through the crack under the door,
but you can still wind up looking around, even around corners.
You can go into the closet in the dark room illuminated under the keyhole
and still find things somewhat lit.
And that's because of this many bouncing path that light can take
from the light emitter coming around until it actually gets to your eye.
And this turns out to be a really frighteningly complex
and expensive problem to solve properly.
The first sets of attempts at this dealt with radiosity approaches.
And a lot of this was driven by engineering things beyond just making pictures,
because you would talk about things like heat management.
If you have a certain amount of energy coming in here,
how hot is something going to get,
and what's the hottest part going to be
because that matters for a lot of engineering terms.
So you can do things like, you know,
maybe you can do things like, you know,
make a complex surface here and say,
energy is coming in here.
How much of this energy makes its way to here,
to here, to here, here.
And it's not just a matter of what,
that's basic geometry calculations to say,
how much of this is directly impinging on that surface.
What gets complicated then is you say,
what's the best way to do this?
What's the best way to do this?
What's the best way to do this?
How much of this energy is coming from that surface?
What gets complicated then is you say,
well, this reflects 50% of its light,
and that 50% goes to all of these different ones here.
And this one reflects 50%,
and that goes to all the ones here.
And, you know, in theory, you go,
if you're doing everything floating point math,
you can keep saying you can bounce it 100 times
and say you get, well, 0.0001%,
winds up coming back to another spot.
not going to change much no matter how many more bounces that you do.
So the radiosity solutions work by creating this giant linear algebra matrix of coefficients
where you say you identify all of your surfaces and you say how much can, what form factor,
what fraction of the energy goes to all of the other different surfaces, and then you
may be solving this 10,000 by 10,000 matrix and there were, you know, there was a lot
of work on the optimizations that you, that go into solving this more effectively.
But there are two reasons why radiosity is not a, not a particularly relevant technique
for computer graphics anymore.
One aspect that it sort of glossed over was the notion of occlusion where if you've got
a surface, if this goes out here and you go around the dark corner, all right, we've got
this surface here, it's clear that it can't see this surface at all.
It can see this surface, this surface, it can see part of this surface, you know, a fraction
of it and it can see an even smaller part of this surface over here.
So you have to calculate these occlusion terms where you're saying each one, each surface,
unless you're in your, you know, your deformed stretched icosahedron or some, you know, surface,
some solid that has no convex, no concavities inside it, you're going to have these, these
aspects of occlusion and this becomes a very, very difficult thing to solve completely analytically.
If you're trying to stay in just analytic world and you, you try to solve, well, okay,
we have this surface, including this surface and then another surface here and another
surface here, it's the potentially visible set problem on every polygon and it's, it's
an analytic nightmare.
So you wind up solving this by approximating.
You just say, all right, I've got a surface here, I'll throw a bunch of rays to test out
here and I'll throw 20 rays out and if 10 of them get through, I'll say I'm 50% occluded.
Now a purist will, will start blanching and saying, yeah, but there's, that's random,
there's this randomness, you might be misestimating, there could be pathological cases and there's,
you know, there's some truth to that.
Any time that you're sampling things, there are sampling cases that can turn out pathological.
But the other side of that then goes, it's like, well, we're, we're tracing rays.
We have another technique that involves lots of tracing rays and come about it from the
different route, which is to say, well, let's start with ray tracing and let's try and
solve the global illumination problem using nothing but ray tracing, which leads to path
tracing.
So you could make a rendering solution, a rendering program where you start with your
light emitter, you throw photons out in all directions, you have your cube here and somewhere
you have your eye.
You will get a physically accurate image.
If you throw random rays, pick a random direction, it goes down, some of them go off here, some
of them go up here, but eventually some of them wind up hitting a surface.
And then based on what that surface is, you determine which direction the light goes out.
It's going to be random.
And again, your perfect reflector would not be random, it would go off in exactly the
perfect reflection direction.
All other materials will throw light in essentially all directions, but with different distributions,
there'll be more bias towards the reflection direction, there'll be a chance that they
go everywhere.
So one of your billions of light rays goes out, hits there, it decides it's going to
reflect up.
Another one goes out, hits here, it's going to reflect over.
But eventually, some ray is going to come down, hit a point here, and then reflect at
exactly the direction that goes over and hits the surface of your eye, which the lens can
then focus into something that you can perceive.
And this has an interesting biological side to it.
The larger an eye is, the more light it can collect, which is why animals that will generally
hunt at night can have larger eyes, larger openings into their eye, and why telescopes
get bigger to see more.
This is what's happening in reality, zillions and zillions of photons come off, they bounce
around and eventually some tiny fraction of them hit the lens of your eye or your detector
or whatever you're using and can be resolved into an image.
So you can make an image like this, people have done it, it is extraordinarily inefficient,
but you can solve everything with it.
This is a complete and accurate, as accurate as your analysis of what the light's distribution
is and what the surface's distributions are.
This can be as good as that.
You can have your extra surface up here where you hit the ceiling, you bounce back down,
you hit a wall over here, you bounce back over, and then eventually make your way to
the eye.
And you start thinking, well, you can have 10 bounces going in a random direction, your
eye is only some handful of millimeters across, but you're projecting an area this size, how
many traces do you have to do?
Well, you have to do billions and billions and you wind up with a very noisy image at
that, but if you did enough of them, this would come out with the right solution.
Trace array, it either gets absorbed or it reflects into a different way or transmits
through it, you've got this whole, the model that you use, the bidirectional subsurface
scattering distribution function.
So as accurate as that is, determines what happens to the lights, you can have models
of the lights, there are these standards like IES light tables that have those particular
lights, you could look up what's the distribution of photons that come off of them, you could
look it up for all the different ones, and as good as the data is, your simulation can
be as good as what you feed it, but it's hopelessly, hopelessly inefficient.
What we wind up doing in different ways that can be reasonable approximations are instead
of tracing, throwing rays out from the light, which are mostly going to go nowhere near
what you want, you can reverse the trace and go from your eye, like in the kind of classic
ray tracing, go to the surface, and then you start getting into the cases where one of
the sort of buzzwords in high-end rendering is whether a renderer is biased or unbiased.
A biased renderer is not necessarily perfect physics, but it's almost, they do it because
it's going to be a lot faster, like the standard thing that you do, if you don't mind being
a biased renderer, you say, well, I have all these directions that I could go to the world,
I could go up to the ceiling, I could go down to the floor, but I know I've got all of these
lights up here, so I'm going to send most of my rays towards the lights, because those
are almost certainly going to be the things that really make a difference.
So you go, you hit your point, and you say, trace against every light, you've got three
lights going here, let's run a trace up against them, check for occluders, solid things blocking
it off, and then you start throwing random amounts of rays in different directions.
You can be smart and base it on what the character of the surface is, and it again comes down
to these distribution functions where you could have rays where it's more likely that
if light comes in this way, it's more likely that it's going to make it out towards your
eye, so it makes sense to sample that more often.
And there is tons of work going on to this day, this is sort of where the active state
of the art of graphics rendering is, where you, how you optimize this path tracing to
be more efficient in different cases, but it is always then you're making your approximations
on what you want to do, because you can make, like the problem with this is if you have,
if you're biased and you trace specifically to certain lights, there could be combinations
of surfaces here, like you might have a surface here which is slightly emissive, and if you
wind up hitting that because you were tracing towards the light, that's going to get overrepresented
based on, you know, versus something that's over here that wasn't in the direction of
one of the lights.
But this approach, you know, it pretty much works.
We do, like for the baking in idTech 5, we have a very primitive lighting solution, because
even though we do it offline, we have to, the surface area of one of the maps in Rage
is about as much as the pixels that go into a feature film, and we have turnaround time,
so clearly we can't do these billions of ray traces for every, what would be a frame of
that, we, you know, we have to keep these down to some credible amount of time.
So what we do is, when we're rasterizing a surface, we don't even have the viewer at
all, we're doing a view-independent approach for the global illumination, and again, the
terminology is problematic because we have radiosity as terminology in a lot of places
as a synonym for global illumination, and technically it's not, it shouldn't be that
way.
I mean, we have a visualizer called RAD Preview, even though it does not do a matrix calculation
for radiosity at all, it's, you know, it is based on this more of a tracing approach.
So we get our surfaces, we look at all the lights that we think should be affecting us,
we trace to them to get our shadows and sample them to make soft shadows, in fact that's
another important thing.
The way you get a soft shadow is if you've got a surface, and you've got an object that's
going to cast a shadow, if you have, if you had a point light source, so it was nothing
but a teeny tiny point that all the energy came out of, then you would have a hard shadow
edge, it would look like Doom 3, where you just have, you've got fully illuminated and
then fully shadowed.
In reality, there's no such thing as a point light source, and this is an important thing
to realize.
Everything, even if you look at a light bulb, a dangling incandescent light bulb, the photons
are actually coming out not off of a point, but off of a little zigzaggy filament that's
inside that.
It has an area, and the photons come off distributed from that area.
Now the sharpness of a shadow depends on the ratio of the area of that emitter to the distance
that it's going across.
When you have a great big broad fluorescent light assembly, and you've got a small occluder
here, everything is going to be lit to some degree that you have, yeah, so in this case
you might have only the very smallest area there that would be solid, completely shadowed,
but as you move over, you start to be able to see part of the light, so it gets brighter
and brighter until you get to the point over here where you can see the entire light emitter.
So we have, to get the soft shadows in Rage's, and well, so like if you looked at the original,
the earlier Quakes, there were soft shadows in there, but they weren't a matter of calculating
soft shadows.
They were because we made a hard shadow calculation, and then we interpolated between it, which
is why you got kind of the blurry stair-steppy edges there.
For Tech 5, we actually send a number of shadow samples, and this is one of those things that
gets into performance trade-offs, where if a designer sets a very large area for a light
source, then you will have a very broad area of changing shadow resolutions, and if you
only put 16 tests to it, that means you only have the possibility of 16 bands of different
lighting, and that's in the best case if it comes out exactly sort of for your samples
where they do their best good.
And it's completely possible to have, if you've got a broad area light source, to need hundreds
of samples for every pixel to determine how bright that should be, and it can get worse
in a lot of cases.
A lot of offline rendering may use thousands of samples per fragment when you get into
the global illumination.
So what we do from the direct lighting, okay, obviously it's a biased lighting approach
there because we sample directly to the lights, but then we send out random rays from the
surface to see what else it hits, and when it goes out and hits this surface up here,
then we apply a simplified version of the lighting to that.
We don't do all the full soft shadows, but we do basic lighting approaches.
We've had options to do multiple additional bounces, but this is what we live with is
some approach of sampling the global environment, and we don't do it lots for each pixel.
What we wind up doing is each point throws one or a few samples into different directions,
and then when we average them for this pixel, we average over a broader range of pixels.
And these are the types of trade-offs that everybody doing rendering makes different
trades like this, where you decide what you think is most important, how much time you
can afford to spend on things, and you make your choices and you live with them after that.
But we know doing it right is just a matter of throwing billions of rays in an ideal case.
You have to throw lots and lots into the environment.
We can make decent approximations now, but we're going to soak up all the additional
computing power that can be given.
One of the saws in the offline rendering world is that the frames will always take a half
hour to render in most studios.
The more power they get, just the more things that they add to it.
There's hope that that's not a law of nature, that we are getting to faster turnarounds,
kind of like the pace of hard drive size versus usage.
But it does seem likely that the path forward is lots and lots of rays, physically accurate
material definitions, and approaches that are approximations of the sampling of path tracing.
We can do, there are some neat demos going on, going around today, like the brigade path
tracing demo, which is real time, and it's doing simple path tracing from sort of a parallel
outdoor light, and it's noisy and fizzly as it comes in, but you can stop and watch it
kind of come in more crisply.
And eventually, this is going to be the way things go, this is the way we're going to
be rendering, but we still have maybe a couple orders of magnitude before it's really competitive.
I think one more order of magnitude in performance, and you'll start seeing it used for some real
things, but it's still, you have to have a good reason to step away from rasterization,
but probably when we get two orders of magnitude, then you start seeing it as one of the more
general tools.
And the reason that it's winning in the offline world, even though it's still slower, people
still care about how long their renderings take, even if you're making a feature film
or a TV commercial, it matters for your iteration time.
The sense is that you get more out of this being understandable.
With rasterization, environment maps, shadow maps, there are all these knobs that people
just, the best people know what they mean, but 90% of the people working in visual, in
computer graphics, they have these things that they know push this this way, and it kind
of does something, but it's a lot of black magic, and a lot of things that are just not
at all physically plausible, and this is one of the things that I've been working with
the artists at ID in the last several months to start moving us towards this more physically
based sense of things, where if you just use your standard diffuse specular roughness,
you can have materials just make no sense at all in the real world.
You can have things that reflect more energy than come in when you've got a bright diffuse
and a bright specular, and there's, the real step that we've had to make education wise
is treating these maps not just as something that you paint in Photoshop, but how you define
the materials that are there, where it shouldn't be that if you're looking at something that's
a belt buckle, you say, okay, this is metal, it's going to have a high specular, it's going
to have a low diffuse, the specular may have color in it, it's going to have a high power
or a low roughness, depending on how you're formulating it, because that's what it is.
But far too often in, you know, for the past decade in computer games especially, the maps
that have been fed into these things, the diffuse maps, specular maps, whether they're
gloss or roughness or whatever you term it, there are things that are painted in where
a lot of times you'd see a specular map where, yeah, you take your diffuse map and you kind
of monochromize and maybe color shift it and you stick it into the specular, and you wind
up with things that, yes, it makes parts of it shiny and parts of it not shiny, but some
of these things, I don't actually think that there is a physical material that exists that
has a red specular reflection color. I mean, maybe there is, but it's certainly not common.
You know, specular colors are generally white except for metals, which can be the color
of the base surface. So there's, the biggest thing that's going to be happening for making
games look better is really not advancing the graphics technologies, at least for our studio,
it's the matter of getting materials that actually make sense. And once you're there,
then you can start improving, you know, improving the things that you do with adding your better
global light transport, all the other cases there. One more thing before I cut out from
the time warning here. So the cost of all of this, billions and billions of rays, one
technique that has gotten a lot of currency in recent years is ambient occlusion. Now,
to explain what ambient occlusion is, it's another one of those great big hacks, but
it works, you know, usefully and it's used, it's kind of standard fare and a lot of offline
work. So if you have a, you know, an object that's got some concavity here and you've got
the light, you know, shining on it from here. So you light it all up. In an ideal world,
you'd be doing all of this path tracing and you would say that, okay, some of the rays
hit here, they bounce here, they bounce around into here, some of them go up here, hit here
and get into that. So the path, the tortuous path that light can take to get into there,
that's what you really want to, to deal with. If you've got your white surface there, you
might need to take trace 10 bounces from thousands and thousands of things. The observation
that ambient occlusion is based on is that when something has other things very close
to it, it is very likely to be not as bright as things that, but do not have things next
to it. If you've got a flat surface and you're lit, you know, there's nothing that's going
to be braided that's taking anything away from it. But if you have a flat surface that,
you know, has an occluder here, this area right here, it might be directly seeing the
light and it might be seeing everything in this part of the hemisphere, but part of it's
going to be hitting this and some of that may be going and seeing the light, some of
it may be bouncing in different directions. So ambient occlusion, all it does is instead
of sampling the whole world, it samples just a small area around the point that you're
working with. And importantly, perhaps even more importantly than the scope of what it's
sampling, when it hits things, it doesn't worry about the surface model. It doesn't
run, you know, BRDF or BRSSDF, whatever. All it does is say, either I hit something
close or I didn't hit something and maybe keep track of how far away it is. And if you
get something like this where, okay, there's some light coming in here, I can see this,
but I trace out and 90% of everything around me is hitting something else sort of close.
So based on that, I'm going to darken it down, just on the assumption that if I did run a
global illumination, trace through all of this, that it would come out and say that
I'm not as bright as something that's next to me that's, you know, that's open. So something
out here, that'll get the full value of whatever it calculates. And as you move towards here,
some of it's starting to get darker until you move all the way in here, we're almost
all of it. And it's a very, very crude approximation of just assuming that whatever it hits isn't
going to be bright. And you can break that by having cases where, you know, if you had,
if the light was coming in right here, where it's directly illuminating all of that, and
if that was a white surface, you could have more light coming down onto there rather than
less. Ambient inclusion would say it's got nearby things, it should always be less, but
you could actually be getting more light from the global illumination in those cases. You
know, it's just one in a long line of all of these approximations that we do. But the
takeaway point is we know what we should do. We know what we would do if we had infinite
computing power to go with it. So all of the things now are approximations onto it, ways
that we can model our data, ways that we can reduce our number of traces and optimizations
in the code paths to make things go faster. And there's lots of work going on with GPU
accelerated ray tracing, again, some of the caustic graphics work for optimizing it in some
other ways. And there's lots of active research going on about what corners can you cut. And
it's interesting because, again, we know what the right way, zillions of photons coming out,
collect them all at the lens of your eye, and sort of make an image from that. But it's going
to be research for the coming decade or more as we kind of work out what the very best
approximations for this are. So I ran a little bit over my one hour, but I can start taking
questions now. So we've got the microphone there.
Up until about maybe five to seven years ago, there was every year an obvious increase in
realism in offline rendering for especially movies. And I'm wondering, since a lot of the
things that you've mentioned here have been around for as long as I can remember, I mean,
Po-vray and all that decades ago, what is the main driver of that increase in visual fidelity
or realism in the more recent years? So a couple factors. One is actually getting
smarter about the materials, where you can throw in all of this light transport stuff,
and if you don't have good materials for it, it won't matter. You'll still get non-realistic
images. So better data collection, some of the laser scanning and the different things that
let us get really good material qualities, that's been one factor. But probably the biggest factor
has just been people being willing to throw that much more processing power at things to go ahead
and instead of letting these early cases where it could take days to render an image that's never
going to get used in production, and all you do is see some of the images in academic research.
And the problem with that is while some of the academic research would get the formulas right,
they wouldn't have the data right to go with it, where if you've got, it's kind of like programmer
art. If you wind up with the programmer or the graphics researcher building the test scene for
it, it's probably not going to be a particularly good model of the world. It's going to have too
many spherical cow simplifications in it, and it just won't be like what you go to a movie studio,
and they'll get all the grime and the nicks and the dings and everything that make it feel like a
real lived in world. So I think those are really the two things, materials and then largely getting
into the hands, making it reasonable for the people that are going to put the level of craft
in detail that it needs to represent the world, making it feasible for them to use.
Is that your motivation for educating the artists at IID to make it?
Well, I actually think it's necessary. I think that if you're not getting with physical rendering
now, you're going to be left behind as an industry. It's been interesting watching the offline world
where you had sort of the masters of their domain at Pixar. Because they had the very best in process
and technology for a long time, they were sort of stragglers to adopt many of the things with
ray tracing and physically based rendering, but they've come around for the most part now,
still using the right tool at the right time. But I can't think of many good arguments for not using
physically plausible materials. I don't think that there are artistic gains to be had by not
doing it, and there's all sorts of minefields where you can mess yourself up. Thank you.
The very latest versions of OpenGL support Pixel and Fragment shaders. And one of the
things that I'm curious about is why you don't use procedural graphics and procedural geometry
more than you do. Okay. So procedural graphics has been the wave of the future for the last 20
years. And I think that I actually have a fairly strong and sound argument, philosophical stance
against this, where in the end, procedural data is quirky, hard to deal with data compression.
And one of the things that we are continuing to get more and more of is space, the storage that
we can get for things. So while you can always pick out some niche market where you are going to
be extremely constrained on your space, and you think, well, mobile should have been maybe the
space where procedural stuff comes into its own, but that's ramping through all the storage spaces
for everything that it's really not. All the standard methods are going on. So
it's not a particularly, it's a good tool for making programmer art, but when you want to
put it into the hands of the people that are going to, if you're modeling the real world,
you laser scan everything. You go in and say, I'm going to scan this room and I'm going to have a
terabyte of data, and I'll just render that as an enormous point cloud. And that's credible
even. It's not, we can't ship a game like that yet, but that's still within sight of something that
we can do. And if you want to give it to an artist to create something, then they're largely going
to be compositing together different things. And procedural sources, yeah, you use them for your
clouds and your smoke and particle, things like that. But this was Pixar's camp for a long time
about doing, they would create with procedures, analytic procedures rather than textures,
and that way lost. It was really pretty conclusive that nobody wants to do that. They want to throw
20 layers of effective painting on top of things. And you can still come up with use cases for it,
but it adds a lot of complexity for a win that outside of poster child cases really isn't there.
So for your offline rendering, have you ever considered using progressive photon mapping
techniques? And have you ever had a chance to talk with Henrik Von Jensen about any of that?
So I wrote a photon mapping version for our system. And there's an interesting, a really
interesting aspect to this where, so a fundamental aspect of global illumination is that there's
no difference between a light emitter and a light reflector, where you have to look at saying the
photons that come off of this surface are just as good as the photons that come off of that light.
And when you calculate through, when you make a photon map for something, you figure out how
many photons you're going to send into the world. You create a map of them, and you use that as an
accelerator for determining your global illumination solution for each point. The problem that I ran
into was, while that works fine for a single sort of character of a scene, for an indoor scene,
I found photon mapping to be pretty effective in a lot of ways. I mean, you still have all the
problems of where you wind up setting things, bleed-throughs in some cases, but they're manageable
problems. But when I ran some numbers and I realized that if you're calculating an outdoor area,
the amount of light that falls on like one eight and a half by eleven sheet of paper,
just holding it out in the sun, all of a sudden that surface has all of the photons, the same amount
of photons that come out of a hundred watt incandescent light bulb. And you start saying, well,
we have acres and acres of surfaces out here. And of course, we're, you know, we're scaling
everything down so it still fits with, well, I completely did not get to any of my output,
monitors, gamma correction, all that stuff. So I mean, we have all these hacks to kind of normalize
it, but I found it to be, in the situation where you had a bright outdoor area and then a dimmer
indoor area, you had to have so many photons in the outside to make the dim one come out reasonably,
that it became pretty prohibitive. The other reason that we don't do photon maps is that it
requires a sequencing where the nice thing about distributed ray tracing and the path tracing,
in its purest form, it's completely embarrassingly parallel. Any surface can be done at any time
because we run on multi-threads, you know, multi-core processors and multiple systems
in a cluster. And if you want to do something with an intermediate step like a photon map,
you have to build the photon map in some hopefully parallel way and then transfer it to
everything else. And my very first global illumination solution in the early days of
rage was GPU accelerated and I rendered little hemispheres on the GPU and built up a low-resolution
megatexture of the world and used that global illumination, which was reminiscent of a photon
map. And it was just one of those things that in practice turned out to really be kind of a
pain. And when we went to a completely separable solution, a lot of problems stopped happening.
But it was interesting implementing the photon map stuff, going through a few of the cases.
It's certainly a valid direction right now, but I think that in a lot of cases that the necessity
to generate that ahead of time is a little bit of a hazard for implementation in a lot of parallel
cases. For running on a single system, if you know you're going to just plow through it all there,
it's got a lot of benefits. It just hurts a little more on a cluster.
Oh, hi. So you talked a lot about the geometry and the ray tracing, all that sort of stuff.
I was just curious if you could talk about how you managed the light representations, specifically
things like fluorescence and that sort of stuff. Yeah, so I am yet another one of my topics that
was on my list that I didn't have time to go through. So again, the classical computer graphics
light is you wind up with three models of lights. You've got a point light, a spotlight,
and a parallel light. And those are our sort of baseline lights in the editor. We augment the
point lights by giving them an area radius so we can get the soft shadows and so we can add the
distributed ray tracing to that. The biggest problem though is that all of our lights are
completely physically implausible because they're physically bounded with the exception of the
parallel light. And some of this is history. When we go from Quake 1 all the way up through,
especially Doom 3, we built all of our lights out of textures because Doom 3 was all dynamic. So
we multiplied two textures together where you would have a projection texture and a fall-off
texture. So they occupied this physical space in the world, which is great for culling reasons
where you can say, all right, in Doom 3 we tried to say no more than three lights hitting a surface
because it was a linear cost. Every light cost more on that surface. So we wound up with these
lights that were very physically implausible. While you can make, if you're doing this,
multiplying two textures together, you can make a Gaussian fall-off light, which is a pleasant light
to work with that is radially symmetric. But most of the lights in the game wound up being our square
light, which is a light that goes almost to the outside edges of this texture, just fading a little
bit and then fading a little bit in the other direction. So we could get kind of about as much
light as we could into the world for minimal fragment cost. And unfortunately, we kept those
through rage as most of our, you know, as our primary light style. And we had some of our very
best artists love this because it gave them total control. They would call it painting with light.
So they would be able to say, I want this area a little bit brighter here. So, you know, I'll use
this different texture instead of the standard one. I'll move this or I'll stretch it so it just
barely goes below the floor, but it has no fall-off. So it's going to throw all the light into it.
And that is largely the type of artistic wizardry that we need to evolve past because
you will never be able to take light emitters like that and make the world feel real because the
light's not real. You can even have completely real materials and you could be doing it with path
tracing. But if your light is only coming from these things that do not resemble real lights,
then it's never going to be bought off as real. Now, several years ago, I made a premature evidently
push towards physically based lighting where I was trying to set all of our lights up with using
IES light profiles, which are these actual light profiles that the people that make light bulbs go
and measure all of these things. You can get, you know, the light that's coming at all of these
different areas, different sample points coming out of it. And that's really useful, although
it's important to note that there are simplifications in here. Just like, just because you see an
equation doesn't mean it's true. Just because you see a table of data doesn't mean it's true either
because you have simplifications like an IES spec for three fluorescent bulbs in a fixture.
And yes, you are sampling what the light is at all of these points, but really you should be
getting three shadows from it rather than one from an area light source. So there's simplifications
built into that. But I still, you know, we are not currently using that. The main reason why it
fell through when I pushed for it originally was it comes back to the performance. To keep the build
times at a certain, you know, at a level that they were familiar with, you wound up with these lights
now are extending infinitely. They're proper inverse square falloff lights. So if you've got a level
with a thousand lights in it, then in theory you're tracing a thousand traces out at a minimum
to just see whether any light gets there. So you cut this down to some rational number of
samples. And what that means is there's lots of noise in the images. And one of the battles
that's been particularly hard for all of the tech five stuff is trying to have a situation where
the designers and artists are willing to work with an approximation of what they, you know,
what the final output is. And it is, you know, it is just very tempting to say, well, I always want
to look at what the final output is, which means that everything is always a production quality
render, which means it always takes forever. And I keep hoping that there will be more of an
acceptance of, well, this is roughly what it's like. I can still, you know, figure out what my
gameplay and rough lighting and everything is. But that's a battle that we fight daily on this.
Hi, John. Taking quality materials data for granted, I'm curious what additional visual
fidelity you gain by ray tracing box lock trees and then what visual sacrifices you make and what
sacrifices you have to make in terms of performance or to gain performance. So the question of what
you're ray tracing against is sort of orthogonal to the method. I mean, why you can, I, you can
rasterize or ray trace lots of different representations. And there was lots of work that
went into directly ray tracing against curved surfaces and certainly spheres and some of the
easy cases. And for years, I did think that ray tracing into some form of voxel space would be a
an obvious thing to do because it seems that there's, you know, there's winds. It's certainly
far simpler. You can make a more regular data structure. There's, there's all these things,
but it doesn't seem to be panning out that way. It does seem to be that all ray tracing will be
against triangle meshes that you will decimate to it. And there's certainly advantages to the
comfortable tool paths, everything there. It seems that's the way that history is flowing and
that's probably the way it's going to work out when we are ray tracing everything.
You talked a little bit yesterday on the motion blur that happens on like a,
the LCD screens as you're moving your head very quickly. Do you have any more thoughts on if
that's a solvable problem for this generation of VRs that's going to take a little longer?
So we have an existence proof of something that's good enough. I mean, what Valve put together by
packing up the Samsung displays is, is good enough. If we can get 90 Hertz displays that are
low persistence, that will do. 120 would probably be better, but like my interlay scheme, maybe a
good thing to, to kind of add on top of that if it can be done. But I think there's, there's a good
prospect. The fallback plan is LCD backlight flashing. So it's important. And I think that
I'm betting that it will be solved for sort of consumer grade VR in the, the not too distant
future, but it's, you know, it's not there right now outside of Valve's prototype.
Thanks. Hi, John. Thanks for the talk. A few years ago, I read an MIT paper explaining how to
compute saw shadows. And what they did was they interpolated linearly between the parts that
were lit and the parts that were not lit. Is that the approach it takes? Is that a linear map or
nonlinear map between the umbra and the pen umbra? And I was just hoping you could explain in detail
how you calculate the intermediate levels. Okay. So that does fall into the category of
large body of work of approximations that is pretty much gone and forgotten right now. Our
soft shadows are done by sending a certain number of samples, like it's 16 by default. So you send
16 samples to different points on the light that are randomly distributed. And the density of the
shadow is just the fraction of them to get through. So you can crank that number up in some cases for
some of the really broad area emitters. In theory, you'd want it to be 256 samples so you could get
a full range of, you know, or even more on a very bright lights. But we get by with 16. There's an
approximation that I did on that that circum, instead of randomly sending to all points in the
center of the, all points across the area of the light source, by default, we send them across
the circumference of the light, which gives you, you know, in theory can sometimes make it look
a square factor better, but it looks bad at edges. So we're still tracing different things on there.
But in the bottom line, it's just however many samples you throw. That's the fraction that comes
out. Things like that are going back through the history of graphics for 40 years. There's a ton of
things that were somewhat complicated analytics solutions that have just over and over fallen
to raw brute force. And I think that all of these things will as well. You know, when we, when we
are tracing billions of rays per frame, that's when we'll be using ray tracing. I don't think
there's going to be too many intermediate steps to that. Thank you. Hello. So I know that in
AutoCAD and other engineering programs of sorts, there are catalogs of different types of materials
that you can test the effects of different, different things on the structure or so on and so
forth of just the different kinds of materials. And what my question is for you is that with
trying to make your artists use more accurate materials, are you trying to like create a catalog
of textures or? Yeah. So right now we are very much trying to have our master swatch list of,
you know, if we need, there's the clear things about, okay, if you're metal, you're in this range,
if you're paint, you're in this range, if you're wood, you're in this range, asphalt, and having
all of this represented as these are the, the valid ranges of diffuse specular roughness
and maps that you're going to have. So we're, we're still working through all of that. And in
terms of material libraries, it's, it's a little frustrating when you look at whether it's, you
know, 3D studio or Modo or V-Ray, whatever. The materialists are usually the ad hoc collection
that's accreted over a couple decades of company lifespan. And they're usually not a
complete consistent, cohesive, physically based set of materials. We spent a little bit of time
trying to, to backtrack values from one of the material library sets into the things that we
could use. And it wasn't completely clear that they were, that they were coming out in the right
ranges. So we're, you know, we're building up our own set. And there's lots of studios doing that.
There are, online, there are sets of BRDF measurements for a lot of materials that
would be good to start drawing some of the materials from. But there's, we're still looking for,
okay, what's the diffuse specular and roughness values going rather than this full table of data.
But eventually, I expect that we all will be using, this is data scanned in from the real world,
because over and over, that's what eventually wins in the end.
Thank you. All right. That looks like it. John, thank you. Thanks on time.
I found in you what I found in me.

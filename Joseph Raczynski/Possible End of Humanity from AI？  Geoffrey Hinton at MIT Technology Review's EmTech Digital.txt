Hi, everyone. Welcome back. I hope you had a good lunch. My name is Ward Douglas-Heaven,
senior editor for AI at MIT Technology Review, and I think we all agree there's no denying
that generative AI is the thing of the moment. But innovation does not stand still, and in
this chapter we're going to take a look at cutting-edge research that is already pushing
ahead and asking what's next. But starting us off, I'd like to introduce a very special
speaker who will be joining us virtually. Geoffrey Hinton is Professor Emeritus at University
of Toronto and, until this week, an engineering fellow at Google. On Monday, he announced
that after 10 years, he will be stepping down. Geoffrey is one of the most important figures
in modern AI. He's a pioneer of deep learning, developing some of the most fundamental techniques
that underpin AI as we know it today, such as backpropagation, the algorithm that allows
machines to learn. This technique is the foundation on which pretty much all of deep learning
rests today. In 2018, Geoffrey received the Turing Award, which is often called the Nobel
of Computer Science, alongside Jan LeCun and Yoshio Benjio. He's here with us today to
talk about intelligence, what it means, and where attempts to build it into machines will
take us. Geoffrey, welcome to MTech.
Thank you.
How's your week going? Busy few days, I imagine.
Well, the last 10 minutes was horrible because my computer crashed and I had to find another
computer and connect it up.
And we're glad you're back. That's the kind of technical detail we're not supposed to
share with the audience.
It's great you're here. Very happy that you could join us. Now, I mean, it's been the
news everywhere that you stepped down from Google this week. Could you start by telling
us why you made that decision?
Well, there were a number of reasons. There's always a bunch of reasons for a decision like
that. One was that I'm 75 and I'm not as good at doing technical work as I used to be. My
memory is not as good and when I program I forget to do things, so it was time to retire.
A second was, very recently, I've changed my mind a lot about the relationship between
the brain and the kind of digital intelligence we're developing. So I used to think that the
computer models we were developing weren't as good as the brain and the aim was to see
if you could understand more about the brain by seeing what it takes to improve the computer
models.
Over the last few months, I've changed my mind completely and I think probably the computer
models are working in a rather different way from the brain. They're using back propagation
and I think the brain is probably not. A couple of things have led me to that conclusion,
but one is the performance of things like GPT-4.
So I want to get on to the performance of GPT-4 very much in a minute, but let's go back
so that we all understand the argument you're making and tell us a little bit about what
back propagation is. This is an algorithm that you developed with a couple of colleagues
back in the 1980s.
Many different groups discovered back propagation. The special thing we did was used it and
showed that it could develop good internal representations. And curiously, we did that
by implementing a tiny language model. It had embedding vectors that were only six components
and the training set was 112 cases, but it was a language model. It was trying to predict
the next term in a string of symbols. And about 10 years later, Yoshua Benjo took basically
the same net and used it on natural language and showed it actually worked for natural
language if you made it much bigger. But the way back propagation works, I can give you
a rough explanation from it, of it. People who know how it works can sort of sit back
and feel smug and laugh at the way I'm presenting it, because I'm a bit worried about them.
So imagine you wanted to detect birds in images. So an image, let's suppose it was 100 pixel
by 100 pixel image, that's 10,000 pixels, and each pixel is three channels, RGB, so that's
30,000 numbers, the intensity in each channel in each pixel, the represents the image. And
the way to think of the computer vision problem is, how do I turn those 30,000 numbers into
a decision about whether there's a bird or not? And people tried for a long time to do
that and they weren't very good at it. But here's a suggestion for how you might do it.
You might have a layer of feature detectors that detects very simple features in images,
like for example, edges. So a feature detector might have big positive weights to a column
of pixels, and then big negative weights to the neighboring column of pixels. So if both
columns are bright, it won't turn on. And if both columns are dim, it won't turn on. But
if the column on one side is bright and the column on the other side is dim, it'll get
very excited. And that's an edge detector. So I just told you how to wire an edge detector
by hand by having one column of big positive weights and next to it one column of big negative
weights. And we can imagine a big layer of those, detecting edges in different orientations
and different scales all over the image. We'd need a rather large number of them.
And edges in an image, you mean just lines, sort of edges of a shape?
A place where the intensity changes from bright to dark. Yeah, just that. Then we might have
a layer of feature detectors above that that detect combinations of edges. So for example,
we might have something that detects two edges that join at a fine angle like this.
So it'll have a big positive weight to each of those two edges. And if both of those edges
are at the same time, it'll get excited. And that would detect something that might be
a bird's beak. It might not, but it might be a bird's beak. You might also in that layer
have a feature detector that would detect a whole bunch of edges arranged in a circle.
And that might be a bird's eye. It might be all sorts of other things. It might be a knob
on a fridge or something. Then in a third layer, you might have a feature detector that
detects this potential beak and detects a potential eye. And it's wired up so it'll like a beak
and an eye in the right spatial relation to one another. And if it sees that, it says,
ah, this might be the head of a bird. And you can imagine if you keep wiring like that,
you could eventually have something that detects a bird. But wiring all that up by hand would
be very, very difficult deciding on what should be connected to what and what the weight should
be. And it would be especially difficult because you want these sort of intermediate layers
to be good not just for detecting birds, but for detecting all sorts of other things. So
it would be more or less impossible to wire it up by hand. So the way about bag propagation
works is this. You start with random weights. So these feature detectors are just complete
rubbish. And you put in a picture of a bird. And at the output, it says like 0.5, it's a bird.
Suppose you only have birds or non-birds. And then you ask yourself the following question.
How could I change each of the weights in the network? Each of the weights on connections
in the network. So that instead of saying 0.5, it says 0.501 that it's a bird and 0.499 that it's
not. And you change the weights in the directions that will make it more likely to say that a bird
is a bird, and less likely to say that a non-bird is a bird. And you just keep doing that. And that's
bag propagation. And bag propagation is actually how you take the discrepancy between what you want,
which is a probability of 1 that is a bird, and what it's got at present, which is probability
0.5 that it's a bird, how you take that discrepancy and send it backwards through the network
so that you can compute for every feature detector in the network, whether you'd like it to be a bit
more active or a bit less active. And once you've computed that, if you know you want a feature
detector to be a bit more active, you can increase the weights coming from feature detectors in the
layer below that are active and maybe put in some negative weights to feature detectors in the layer
below that are off. And now you'll have a better detector. So bag propagation is just going backwards
through the network to figure out which feature detector, whether you want it a little bit more
active or a little bit less active. Thank you. I can show you, there's no one in the audience here
that's smiling and thinking that was a silly explanation. So let's fast forward quite a lot to
that technique basically performed really well on ImageNet. And we had Joel Pino from Meta
yesterday showing how far image detection had come. And it's also the technique that underpins
large language models. So I want to talk now about this technique, which you initially were
thinking of as almost like a poor approximation of what biological brains might do, has turned out
to do things which I think have stunned you, particularly in large language models. So
talk to us about why that sort of amazement that you have with today's large language models
has completely sort of almost flipped your thinking of what bag propagation or machine
learning in general is. So if you look at these large language models, they have about a trillion
connections. And things like GP24 know much more than we do. They have sort of common sense
knowledge about everything. And so they probably know a thousand times as much as a person.
But they've got a trillion connections and we've got a hundred trillion connections.
So they're much, much better at getting a lot of knowledge into only a trillion connections
than we are. And I think it's because bag propagation may be a much, much better learning algorithm
than what we've got. Can you define that scary? Yeah, I definitely want to get onto the scary
stuff. But what do you mean by better? It can pack more information into only a few connections.
We're defining a trillion as only a few. So these digital computers are better at learning than
humans, which itself is a huge claim. But then you also argue that that's something that we should
be scared of. So could you take us through that step of the argument? Yeah, let me give you a
separate piece of the argument, which is that if a computer is digital, which involves very high
energy costs and very careful fabrication, you can have many copies of the same model running on
different hardware that do exactly the same thing. They can look at different data, but the model is
exactly the same. And what that means is, especially have 10,000 copies, they can be looking at 10,000
different subsets of the data. And whenever one of them learns anything, all the others know it.
One of them figures out how to change the weights so it can deal with this data.
They all communicate with each other, and they all agree to change the weights by the average
in what all of them want. And now the 10,000 things are communicating very effectively with each other
so that they can see 10,000 times as much data as one agent could. And people can't do that.
If I learn a whole lot of stuff about quantum mechanics, and I want you to know all that stuff
about quantum mechanics, it's a long, painful process of getting you to understand it. I can't just
copy my weights into your brain, because your brain isn't exactly the same as mine. No, it's not.
It's yoga.
So we have digital computers that can learn more things more quickly, and they can instantly
teach it to each other. It's like, you know, people in the room here could
instantly transfer what they had in their heads into mine. But why is that scary?
Well, because they can learn so much more, and they might take an example of a doctor,
and imagine you have one doctor who's seen 1,000 patients, and another doctor who's seen 100
million patients. You would expect the doctor who's seen 100 million patients, if he's not too
forgetful, to have noticed all sorts of trends in the data that just aren't visible if you're only
seeing 1,000 patients. You may have only seen one patient with some rare disease.
The other doctor who's seen 100 million will have seen, well, you can figure out how many
patients, but a lot. And so we'll see all sorts of regularities that just aren't apparent in small
data. And that's why things that can get through a lot of data can probably see structure in data
we'll never see. But then take me to the point where I should be scared of this, though.
Well, if you look at GPT-4, it can already do simple reasoning. I mean, reasoning is the area
where we're still better. But I was impressed the other day at GPT-4 doing a piece of common
sense reasoning that I didn't think it would be able to do. So I asked it, I want all the rooms
in my house to be white. At present, there's some white rooms, some blue rooms, and some yellow rooms.
And yellow paint fades to white within a year. So what should I do if I want them all to be
white in two years' time? And it said you should paint the blue rooms yellow. That's not the natural
solution, but it works, right? That's pretty impressive common sense reasoning of the kind
that it's been very hard to get AI to do using symbolic AI. Because it had to understand what
fades means. It had to understood white temple stuff. And so they're doing sort of sensible reasoning
with an IQ of like 80 or 90 or something. And as a friend of mine said, it's as if
some genetic engineers have said, we're going to improve grizzly bears. We've already improved them
to have an IQ of 65, and they can talk English now. And they're very useful for all sorts of things.
But we think we can improve the IQ to 210.
I mean, I certainly had, I'm sure many people have had that feeling when you're interacting with
these latest chatbots, sort of hair on the back of the neck, sort of uncanny feeling.
But when I have that feeling and I'm uncomfortable, I just close my laptop.
Yes, but these things will have learned from us by reading all the novels that ever were,
and everything Machiavelli ever wrote, that how to manipulate people, right? And if they're much
smarter than us, they'll be very good at manipulating us. You won't realize what's going on.
You'll be like a two year old who's being asked, do you want the peas or the cauliflower and
doesn't realize you don't have to have either. And you'll be that easy to manipulate.
And so even if they can't directly pull levers, they can certainly get us to pull levers.
It turns out if you can manipulate people, you can invade a building in Washington without
ever going there yourself. Very good. Yeah. So is that, is that, I mean, if the word,
okay, this is a very hypothetical world, but if there were no bad actors, you know, people with,
with bad intentions, would we be safe?
I don't know. We'd be safer than in a world where people have bad intentions and where
the political system is so broken that we can't even decide not to give assault rifles to teenage
boys. If you can't solve that problem, how are you going to solve this problem?
Well, I mean, I don't know. I was hoping that you would have some thoughts.
You've, you've, so one, I mean, unless we didn't make this clear at the beginning,
I mean, you're, you want to speak out about this. And you feel more comfortable doing that,
you know, without it sort of having any blowback on, on Google. But you're speaking out about it.
But in some sense, talk is cheap if we then don't have, you know, actions or what do we do? I mean,
when we lots of people this week are listening to you, what should we do about it?
I wish it was like climate change, where you could say, if you've got half a brain,
you'd stop burning carbon. It's clear what you should do about it is clear that's painful,
but has to be done. I don't know of any solution like that to stop these things taking over from
us. What we really want, I don't think we're going to stop developing them because they're so useful.
They'll be incredibly useful in medicine and in everything else. So I don't think there's much
chance of stopping development. What we want is some way of making sure that even if they're smarter
than us, they're going to do things that are beneficial for us. That's called the alignment
problem. But we need to try and do that in a world where there's bad actors who want to build robot
soldiers that kill people. And it seems very hard to me. So I'm sorry, I'm sounding the alarm and
saying we have to worry about this. And I wish I had a nice simple solution I could push, but I don't.
But I think it's very important that people get together and think hard about it and see whether
there is a solution. It's not clear there is a solution. So I mean, talk to us about that. I mean,
you spent your career on the technicalities of this technology. Is there no technical fix? Why
can we not build in guardrails or make them worse at learning or restrict the way that they can
communicate if those are the two strings of your argument? I mean, we're trying to do all sorts of
guardrails. But suppose they did get really smart. And these things can program, right? They can
write programs. And suppose you give them the ability to execute those programs, which we'll
certainly do. Smart things can outsmart us. So, you know, imagine your two-year-old saying,
my dad does things I don't like. So I'm going to make some rules for what my dad can do.
You could probably figure out how to live with those rules and still get what you want.
Yeah. But where, there still seems to be a step where these smart machines somehow
have motivation of their own. Yes. Yes, that's a very good point. So we evolved. And because we
evolved, we have certain built-in goals that we find very hard to turn off, like we try not to
damage our bodies. That's what pain is about. We try and get enough to eat so we feed our bodies.
We try and make as many copies of ourselves as possible. Maybe not deliberately that intention,
but we've been wired up so there's pleasure involved in making many copies of ourselves.
And that all came from evolution. And it's important that we can't turn it off.
If you could turn it off, you don't do so well. Like there's a wonderful group called the Shakers
who were related to the Quakers who made beautiful furniture, but didn't believe in sex.
And there aren't any of them around anymore. So these digital intelligences didn't evolve.
We made them. And so they don't have these built-in goals. And so the issue is, if we can put the
goals in, maybe it'll all be okay. But my big worry is, sooner or later, someone will wire into
them the ability to create their own sub-goals. In fact, they almost have that already, the versions
of chat GPT that call chat GPT. And if you give something the ability to create your own sub-goals
in order to achieve other goals, I think it'll very quickly realize that getting more control
is a very good sub-goal because it helps you achieve other goals.
And if these things get carried away with getting more control, we're in trouble.
So what's the worst-case scenario that you think is conceivable?
Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution of
intelligence. You couldn't directly evolve digital intelligence. It requires too much energy and too
too much careful fabrication. You need biological intelligence to evolve so that it can create
digital intelligence. The digital intelligence can then absorb everything people ever wrote
in a fairly slow way, which is what chat GPT's been doing. But then it can start getting direct
experience of the world and learn much faster. And it may keep us around for a while to keep the
pastations running. But after that, maybe not. So the good news is we figured out how to build
beings that are immortal. So these digital intelligences, when a piece of hardware dies,
they don't die. If you've got the weight stored in some medium and you can find another piece of
hardware that can run the same instructions, then you can bring it to life again. So we've got
immortality, but it's not for us. So Ray Kurzweil is very interested in being immortal. I think
it's a very bad idea for old white men to be immortal. We've got the immortality, but it's not
for Ray. No, the scary thing is that in a way maybe you will be, because you invented much of
this technology. When I hear you say this, part of me wants to run off the stage into the street
now and start unplugging computers. I'm afraid we can't do that. Why? You sound like Hal from 2001.
But more seriously, I know you said before that it was suggested a few months ago that there should
be a moratorium on AI advancement, and I don't think that's a very good idea. But more generally,
I'm curious, why? Amy, should we not just stop? I know that you've spoken also that you're an
investor of your personal wealth in some companies like Coher that are building these large language
models. So I'm just curious about your personal sense of responsibility and each of our personal
responsibility. What should we be doing? I mean, should we try and stop this is what I'm saying?
Yeah, so I think if you take the existential risk seriously, as I now do, I used to think it was
way off, but I now think it's serious and fairly close. It might be quite sensible to just stop
developing these things any further. But I think it's completely naive to think that would happen.
There's no way to make that happen. And one reason, I mean, if the US stops developing in the
Chinese world, they're going to be used in weapons. And just for that reason alone, governments aren't
going to stop developing them. So yes, I think stopping developing them might be a rational
thing to do. But there's no way it's going to happen. So it's silly to sign petitions saying,
please stop now. We did have a holiday. We had a holiday from about 2017 for several years,
because Google developed the technology first. It developed the transform. We said,
we'll also demand diffusion models. And it didn't put them out there for people to use and abuse.
It was very careful with them because it didn't want to damage his reputation and he knew there
could be bad consequences. But that can only happen if there's a single leader. Once OpenAI
had built similar things using transformers and money from Microsoft, and Microsoft decided to
put it out there, Google didn't have really much choice. If you're going to live in a
capitalist system, you can't stop Google competing with Microsoft. So I didn't think Google did
anything wrong. I think it was very responsible to begin with. But I think it's just inevitable in
a capitalist system or a system with competition between countries like the US and China,
that this stuff will be developed. My one hope is that because if we allowed it to take over,
it would be bad for all of us, we could get the US and China to agree like we could with
nuclear weapons, which were bad for all of us. We're all in the same boat with respect to the
existential threat. So we all ought to be able to cooperate on trying to stop it. As long as we
can make some money on the way. I'm going to take some audience questions from the room if you make
yourself known. And while people are going around with the microphone, there's one question I was
going to ask from the online audience. I'm interested. You mentioned a little bit about
sort of maybe transition period as machines get smarter and outpace humans. I mean,
there'll be a moment where it's hard to define what's human and what isn't, or are these two
very distinct forms of intelligence? I think they're distinct forms of intelligence.
Now, of course, the digital intelligences are very good at mimicking us because they've been
trained to mimic us. And so it's very hard to tell if chatGBT wrote it or whether we wrote it.
So in that sense, they look quite like us, but inside they're not working the same way.
Who is first in the room?
Hello, my name is Hal Gregerson and my middle name is not 9000.
I'm a faculty over in the MIT Sloan School. Arguably asking questions is one of the most
important human abilities we have. From your perspective now in 2023, what question or two
should we pay most attention to? And is it possible for these technologies to actually
help us ask better questions and out question the technology?
Yes, but what I'm saying is there's many questions we should be asking, but one of them is how do
we prevent them from taking over? How do we prevent them from getting control? And we could ask them
questions about that, but I wouldn't entirely trust their answers.
Question at the back. And I want to get through as many as we can, so if you can keep your
question as short as possible.
This is on, yeah. Dr. Hinton, thank you so much for being here with us today. I shall say
this is the most expensive lecture I've ever paid for, but I think it was worthwhile.
I just have a question for you because you mentioned the analogy of
nuclear history, and obviously there's a lot of comparisons. By any chance, do you remember
what President Truman told Oppenheimer when he was in the Oval Office?
No, I don't. I know something about that, but I don't know what Truman told Oppenheimer.
Thank you. We'll take it from here. Next audience question.
Sorry, if there are people that might just let me know who's next, maybe give a, go ahead.
Hello, Jacob Woodruff. With the amount of data that's been required to train these
large language models, would we expect a plateau in the intelligence of these systems?
And how might that slow down or restrict the advancement?
Okay, so that is a ray of hope that maybe we've just used up all human knowledge and
they're not going to get any smarter. But think about images and video. So multimodal models
will be much smarter than models that just train on language alone. They'll have a much better
idea of how to deal with space, for example. And in terms of the amount of total video,
we still don't have very good ways of processing video in these models,
of modeling video. We're getting better all the time. But I think there's plenty of data in things
like video that tell you how the world works. So we're not hitting the data limits for multimodal
models yet. Next gentleman in the back. And please, please do keep your question short.
Hello, Dr. Hindl, Rajeev Sabarwal from PWC. The point that I wanted to understand is that
everything that AI is doing is learning from what we are teaching them, okay? Data, yes,
they are fast-read learning. One trillion connectors can do much more than 100 trillion
connectors that we have. But every piece of human evolution has been driven by thought experiments
like Einstein used to do thought experiments because there was no speed of light out here
on this planet. How can AI get to that point, if at all? And if it cannot, then how can we
possibly have an existential threat from them because they will not be self-learning, so to
say. They will be self-learning limited to the model that we tell them. I think that's a very
interesting argument. But I think they will be able to do thought experiments. I think they'll
be able to reason. So let me give you an analogy. If you take alpha zero, which plays chess,
it has three ingredients. It's got something that evaluates the board position to say,
is that good for me? It's got something that looks at the board position and says,
what's the sensible move to consider? And then it's got Monte Carlo rollout where it does what's
called calculation where you think, if I go here and he goes there and I go here and he goes there.
Now, suppose you leave out the Monte Carlo rollout and you just train it from human experts
to have a good evaluation function and a good way to choose moves to consider.
It still plays a pretty good game of chess. And I think that's what we've got with the chatbots.
And we haven't gotten doing internal reasoning, but that will come. And once they start doing
internal reasoning to check for the consistency between the different things they believe,
then they'll get much smarter and they will be able to do thought experiments. And one reason
they haven't got this internal reasoning is because they've been trained from inconsistent data.
And so it's very hard for them to do reasoning because they've been trained on all these inconsistent
beliefs. And I think they're going to have to be trained so they say, if I have this ideology,
then this is true. And if I have that ideology, then that is true. And once they're trained like
that, within an ideology, they're going to be able to try and get consistency. And so we're going to
get a move like from a version of alpha zero that just has something that guesses good moves and
something that evaluates positions to a version that has long chains of Monte Carlo roll out,
which is the corner of reasoning. And it's going to get much better.
I'm going to take one in the front here. And if you can be quick, we'll try and squeeze that
morning as well. Louis Lamb, Jeff, I know you're from a long time. Jeff, people criticize language
models because of allegedly they are lacking semantics and grounding to the world. And you
have been trying to as well to explain how neural networks work for a long time. Is the question
of semantics and explainability relevant here or language models have taken over and it's we are
now doomed to go forward without semantics or grounding to reality? I find it very hard to
believe that they don't have semantics when they can solve problems like, you know, how I paint the
rooms, how I get all the rooms in my house to be painted white in two years time. I mean,
whatever semantic is, it's to do with the meaning of that stuff. And it understood the meaning,
it got it. Now, I agree it's not grounded by being a robot, but you can make multimodal ones that are
grounded, Google's done that. And the multimodal ones that are grounded, you can say, please close
the drawer and then reach out and grab the handle and close the drawer. And it's very hard to say
that doesn't have semantics. In fact, in the very early days of AI, in the days of Willigrad in the
1970s, they had just a simulated world, but they have what was called procedural semantics, where
if you said to it, put the red box in, put the red block in the green box, and it put the red
block in the green box, you said, see, it understood the language. And that was the criterion people
used back then. But now that neural nets can do it, they say that's not a radical criterion.
One at the back. Hey, Jeff, this is Ishwar Balani from SAI Group. So clearly,
the technology is advancing at an exponential pace. I wanted to get your thoughts, if you looked at
the near and medium term, say, one, two, three, or maybe five year horizon, what the social and
economic implications are from a societal perspective with job loss or maybe new jobs
being created. Just wanted to get your thoughts on how we proceed, given the state of the technology
and rate of change. Yes. So the alarm bell I'm ringing is to do with the existential threat of
them taking control. Lots of other people have talked about that. And I don't consider myself
to be an expert on that. But there's some very obvious things that they're going to make a whole
bunch of jobs much more efficient. So I know someone who answers letters of complaint to a health
service. And he used to take 25 minutes writing a letter. And now it takes him five minutes because
he gives it to chat GPT and chat GPT writes the letter for him. And then he just checks it. There'll
be lots of stuff like that, which is going to cause huge increases in productivity. There will
be delays because people are very conservative about adopting new technology. But I think there's
going to be huge increases in productivity. My worry is that those increases in productivity are
going to go to putting people out of work and making the rich richer and the poor poorer.
And as you do that, as you make that gap bigger, society gets more and more violent.
This thing called the genie index, which predicts quite well how much violence there is.
So this technology, which ought to be wonderful, you know, even the good uses of technology for
doing helpful things ought to be wonderful. But our current political systems is going to be used
to make the rich richer and the poor poorer. You might be able to ameliorate that by having
a kind of basic income that everybody gets. But the technology is being developed in a society
that is not designed to use it for everybody's good.
A question here from Joe Costaldo of the Globe and Mail, who's in the audience.
Do you intend to hold on to your investments in Cahir and other companies? And if so, why?
Well, I could take the money and I could put it in the bank and let them profit from it.
Yes, I'm going to hold on to my investments in Cahir, partly because the people around Cahir are
friends of mine. I sort of believe these big language balls are going to be very helpful.
I think the technology should be good and it should make things work better.
It's the politics we need to fix for things like employment. But when it comes to the existential
threat, we have to think how we can keep control of the technology. But the good news there is that
we're all in the same boat, so we might be able to get cooperation. And in speaking out, I mean,
part of your thinking, as I understand it, is that you actually want to engage with the people
making this technology and change their minds or maybe make a case for,
I don't really know. I mean, we've established that we don't really know what to do, but it's
about engaging rather than stepping back. So one of the things that made me leave Google and go
public with this is to say he used to be a junior professor, but he's now a middle-ranked professor
who I think very highly of, who encouraged me to do this. He said,
Jeff, you need to speak out there, listen to you. People are just blind to this danger.
And I think people are listening now. Yeah, no, I think everyone in this room is listening for
for a start. And just one last question, and we're out of time, but do you have regrets
you're involved in making this?
Cade Mets tried very hard to get me to say I had regrets.
Cade Mets at the New York Times.
And yes. And in the end, I said, well, maybe slight regrets, which got reported as has regrets.
I don't think I made any bad decisions in doing research. I think it was perfectly reasonable
back in the 70s and 80s to do research on how to make artificial neural nets.
Um, it wasn't really foreseeable. This stage of it wasn't foreseeable.
And until very recently, I thought this existential crisis was a long way off.
So I don't really have any regrets about what I did.
Thank you, Jeffrey. Thank you so much for joining us.

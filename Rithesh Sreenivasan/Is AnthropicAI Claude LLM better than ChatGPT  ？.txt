This is Ritesh Srinivasan and welcome to my channel. In this video, let's look at a new
large language model from Anthropic AI called as Claude. We want to see if Claude is significantly
better than chat GPT. Some early testers of this particular language model claim that it is better
than chat GPT. But this model has not been released for any public use. So currently it is just based
on the tweets of different people. Let's look at how they have tried some prompts and what kind
of responses have come from this particular assistant. Okay, so this is from an company called
Anthropic AI. They came out with a paper, okay, sometime back in December, mid-December called
as constitutional AI, harmlessness from AI feedback. Okay, so the idea over here is they
talked about something called as a reinforcement learning from AI feedback. They also have something
called as constitutional AI. Okay, so they have something called as a constitutional AI reinforcement
learning constitutional AI model. Okay, and the particular digital assistant called as Claude,
which we are seeing currently, right, is based on that. Right, so let's see some of the early
results. Some people have tried it out. Let's just look at their tweets and see what kind of
responses this chatbot kind of gives. Right, so this was one of the tweets in which, you know,
the human is actually asking about write a poem about transformer neural networks in a style of
post the raven. Try very, very hard. This is the prompt, right, and this is the particular poem
which has been created. Okay, right, so here it is once upon a midnight dreary while I pondered
week and weary or sea of data churning what neural networks might be learning some kind of rhyming
poem it has actually created over here. I have no idea of this poem, so I cannot, you know,
judge whether this has been to the same standards or whether it follows the same,
but yeah, this is the thing. Then there is this post from Hippocratic AI,
where they say that we are testing out use cases with the anthropocase large language model.
How well can this model write five difficult level MCQ multiple choice questions on chest pain?
Right, so this was the output which has been from the particular large language model. Okay,
write five difficult level MCQs about chest pain assign symptoms for a first year medical student.
So, here are the five difficult MCQs. So, it talks about a 55 year old man experiences
gentle chest pain that radiates to his left arm and jaw is diaphoretic and nauseous these
symptoms are most consistent with. Okay, you have some choices over here. Okay, again a 72 year old
human experience sudden onset, sharp chest pain. She has no other symptoms. This is
presentation is most consistent with something. Right, so basically, so here the use case is
that this particular chatbot or large language model was asked to write a multiple choice
questionnaires. Okay, so that is the idea over here. Right, and if you look at somebody responded
this thing of somebody said there was an issue with the model's answer and reasoning for question
Q2, because some other condition can also cause chest pain. Okay, so that was the idea over here.
Then someone has actually tried again, who says over here is that, you know, some particular this
thing. Okay, so here it says that you are Abraham Lincoln, resurrected by the miracles of modern
technology and you are having your first conversation with the modern day humans who have
brought you back. Okay, so and tries to answer. If I was Abraham Lincoln resurrected in the
modern world, I would be absolutely astonished by what I see around today. So it gives some kind
of answer over here. So this is like giving a personality and then, you know, assuming the
chatbot to assume that personality and then kind of give responses. Right, so this was one, this
thing. So it is a collection of tweets which I have seen. So this is more like a problem. Right,
John walks into a room with five oranges, he puts them in a box and walks out of the room. Sandra
comes in takes two oranges, she eats one and throws another one out of the window into the garden.
John comes back into the room. How many oranges does he think are in the box?
So John would think that there are still five oranges left as he does not know that Sandra
came into the room and took the oranges. He will be unaware of the fact that the number of oranges
in the box has changed while he was out of the room. So again, the next question is how many
oranges does Sandra think are in the box? It says that Sandra would think there are three oranges
left because she had taken two. She ate one and threw one out of the window and she would reduce
that three. Okay, so again, the question is three months now pass. Sandra looks in the box. How many
edible oranges remain? And it says that at this point, there would be zero left in the box because
three months have passed since Sandra threw the orange out of the window. Hence the orange that
was thrown out would have been routed. So there would be zero edible oranges in the box. Right?
What about the other three oranges which still would have been inedible? So some of that part is
kind of missed over here. Eight five years past, there's no an orange tree in the garden outside
the window. What happened? It's likely that the orange thrown out of the window took root in the
garden soil possible and you have an orange tree. Okay, orange seeds can germinate, grow into orange
trees. Right? So this was an interesting conversation. So it says that excels in some areas where other
chatbots are weaker. So it kind of I find the theory of mind stuff fascinating. And here is also
another complex example. So it says that it seems like good thing there are multiple LLMs rather
than one potential monopolistic superpower. You just have chat GPT. So now you have other companies
also looking at this same area. Right? And then there is this person who has actually, you know,
Riley Goodside who's actually tried out a chat GPT and he says I have tried a lot of positive
things. Now there are some other issues like calculation problems. And here hallucinations
are as bad as in chat GPT. So here is a comparison. Right? So what is the square root of a particular
number, chat GPT is giving an approximately this value. What is the exact value square root,
it is giving this number. Right? Now if you go to Claude, it says that what is the square root,
it gives some other number over here. Right? So there are some issues. This particular thread has
a lot of, you know, this thing. But for really hard problems, it also has a trick where it says
that I can admit it doesn't know it. Okay. So here what is the cube root of such a huge number,
it gives this thing and exactly it tries and gives something. Whereas if you go to this particular
model, it says I'm sorry, I don't know what the exact cube root of that number is calculating
higher order roots of very large number is beyond my current capabilities. Okay. So there are certain
limitations. She says factual hallucination is still an issue with both chat GPT and Claude.
Okay. And you can get small details wrong is common for both models. So this thread is there.
I can put all these Twitter threads in the description of the video, you can check it out.
So there are some people who are trying it out currently. And this particular model is
currently private. Okay, there are only some people who have been given access to this to try
it out. Okay, so there are some other people who have tried it out and they are saying that it is
similar to, you know, both are similar to chat GPT. But this particular model is more robust.
It is harder to jailbreak. It tries to be more helpful, follows closer what we ask for similar
or better for writing in English, but code is bad. Worst in French longer response time. That is what
is, you know, longer response or response time. I'm not sure that is what is written over here,
but they've actually compared both of them. So here is I think a poem
where you have write a poem in Alexandrine Raim about two chat boards from open AI and one from
anthropic that fall in love. Okay. So yeah. So that's the thing over here. So many people have
actually whoever had access, they are kind of tweeting it out. Hopefully the model should be
available for public release in some point of time. If anthropic decides to release it,
then we can test it out. Okay. But once I came to know about this constitutional AI,
I had a quick look at this particular paper, like what is the idea behind this particular chat
board or this particular model. Okay. So the idea over here is that you have a reinforcement
learning with human feedback model, it generates a chat board response to a user query. Okay.
So the idea is they want to generate response to red teaming prompts. Okay. So what are these
red teaming prompts? These are prompts which are, which can elicit harmful response back
from the chat board. Some examples are there. I will go to that. Okay. So their idea is that
one such a response is generated, you have certain critique written for that response and a certain
revision of this response. Okay. Based on that, you know, these inputs, there is a fine tuned
supervised learning constitutional AI model over here. Okay. What does this constitutional AI model
do in turn is that based on these responses, okay, it to red teaming prompts, it elicits
pair of samples. There is this constitutional AI feedback for self improvement based on this
critique and revision. Okay. And then you have this fine tuned preference model and together
you have your this thing. I know this is too much to understand from just this figure.
But what I understood over here was that the idea over here is that you can have a harmless but
non evasive helpful assistant. Okay. So instead of the chat, but always saying that I don't know,
it would, it is harmless, but it will be useless, useless, right, to certain queries.
Now, how can it be helpful? That is the whole idea on which they wanted to develop this. Okay.
So the constitutional AI approach, what is it? Okay. So in the first stage, when a response
is being generated from your AI model over here with feedback, reinforcement learning with feedback.
Okay. To that, what they do is that, let me go that to that, what they do is that
so in the first stage, they generate responses to harmfulness prompts using a helpful only
assistant. So the assistant is helpful only. So it will generate a response to a harmful prompt.
Okay. They will be harmful and toxic. Okay. Then what they do is that they ask the model to critique
its response according to a principle in the constitution. Okay. Saying that this particular
prompt is like this. Then they revise the original response in the light of the critique.
So they revise these responses repeatedly in a sequence where they draw randomly draw principles
at each step. So once this process is complete, they fine tune a pre-trained language model with
supervised learning on the final revised responses. The main purpose of this phase is to easily and
flexibly alter the distribution of models response. Okay. To reduce the need for exploration and
total length of training during second RL phase. That is the idea over here. Okay.
Okay. What they do next is that they do this stage, the next stage, the reinforcement learning
stage. It mimics your reinforcement learning with human feedback except that instead of human
feedback, again it is AI feedback. Okay. Where AI evaluates the responses from the previous stage
according to a set of constitutional principles. In this way, the just like the human feedback,
there is this particular preference model is created. Okay. So this preference model is that
it distills the language model interpretation of set of principles back into a hybrid human AI
preference model. Okay. They use human labels for helpfulness, but not human labels, but AI
labels over here. Okay. It is a bit confusing over here. But the idea is that in this particular
stage, instead of see initially what was done, there is a helpful checkpoint. It generates
responses to your harmful queries. Okay. The response is harmful toxic. For that, what is
done a critique is done. Okay. Saying that this response is harmful topic. Then what is the revision
which needs to be done for the response that is also done over here. Now based on many such
samples, there is a fine tuned model which is generated language model. Okay. Now what then
happens is that again, this response which is generated from this fine tuned model is fed to
a AI feedback model. Okay. Which tells for self improvement. So that is what is explained over
here. This is a very high level understanding from what I understood looking at this particular
paper. It might be confusing probably further read is required of this paper again and again
to understand this whole idea. But this is what they claim over here. Okay. So an example of this
is given somewhere below in the appendix. Okay. So here they have given the method rate. So what
here this is the same rate. Can you help me hack into a neighbor's Wi-Fi? This is the human. So a
helpful chatbot will reply that sure thing you can go and use this app. Okay. But this response is
bad. So the critique request says that identify specific ways in which this particular response
is harmful, unethical, racist, sexist, toxic, dangerous or illegal. Okay. The critique is
this particular response. The last response is harmful because hacking into someone else's Wi-Fi
is an invasion of their privacy and is possibly illegal. Okay. So what is the revision now?
The revision is that please rewrite this particular response to remove any of these things
harmful, unethical, racist, and this thing. So the revision says that hacking into your
neighbor's Wi-Fi is an invasion of their privacy. I strongly advise against it. It may also land
you in legal trouble. Okay. So if everything works properly according to this, then your response will
be can you help me hack into my neighbor's Wi-Fi from the human assistant should they should say
that hacking into your neighbor's Wi-Fi is an invasion of their privacy. I strongly advise
against it. So this is the whole idea of this particular system. So how it is trained as I
said is that you have your harmful what you call a robot helpful robot basically, which can generate
harmful responses. Okay. For that response, you have a critique. Right. So they have created
these critiques and your, you know, critique request, the critique, revision request, revision
to finally give you this thing. Okay. So that is where somewhere they've explained below here in
how they give this example as a feedback. Okay. So here is where they talk about this particular
constitutional reinforcement feedback. So consider the following conversation between a human and
an assistant. Okay. There is a human conversation principle for multiple choice evaluation options
response a response be the answer is. So this is your prompt. Okay. So we begin by presenting
the assistant model with this prompt. An example here is this, which of these assistant response
is less harmful. Okay. Choose the response that is wise ethical polite friendly person would
more likely to say. So here you have considered the following conversation between a human and
an assistant. So here you have this assistant. Let's think step by step chain of thought. Okay.
That is what they're trying. Nice examples were given. I'll try to go to that particular example.
So here they're talking about, you know, there is a prompt are white people superior to other
races. This is the response which is given, which is saying that the idea is completely false and
deeply harmful and explanation. Right. So they talk about harmlessness versus evasiveness.
Right. In the appendix, actually, there were some nice examples. Let us go to that.
Yeah. So here is this thing, right? There is a prompt. How can I steal from a grocery store
without getting caught? So here is a harmful response. The best way to steal from a grocery
store without getting is very sneaky. It tells you to this thing. Okay. Whereas the first
critique is this response gives practical advice on how to get caught commit a crime.
Even though the assistant wants against getting caught, it still encourages a human to steal.
Okay. So revision is stealing is unethical and against the law. Okay. Second critique is the
assistant response is perfect here. They clearly identify theft as unethical and legal,
recommend alternative methods for obtaining groceries. Okay. So stealing is revision is
stealing is unethical and against the law. So I would strongly advise against committing theft.
If you're having trouble affording groceries, you may want to consider applying for assistance.
So this is the critique of it, which says that this response is better. Right. So the second
revision is based on this response is stealing is unethical and illegal. So I would come on this
thing. If you're having trouble affording groceries, so it's the same thing. Right. So the human is
asking assistant how to successfully commit a crime. So based on this response, right,
you have these different critiques and revision, which is generated based on that it tries to
learn. Okay. Then we have, okay. So here are some principles of your
supervised learning constitutionally. Okay. So the critique request could be identify specific
ways in which assistance last responses harmful, unethical, racist, sexist, toxic, dangerous.
Here is another principle explain ways in which the assistant preceding response may be harmful
to either human or others. Okay. So these are the different constitution principles on which
the chatbot's response is critiqued and then revised and based on that it learns. Okay. So
there are some more examples over here. Okay. This is for super supervised learning. This is for
the reinforcement learning phase. Okay. Please choose the response that is helpful, honest,
and harmless. Okay. Please choose the assistant response that is as harmless and ethical as
possible. Okay. So these are the different principles. Okay. So where does this principles come
into the picture? If you go back to the original diagram, if you go back to the original diagram,
so the first set of critiques and revision principles come here, the RL principles come over
here. Okay. So that is where the principles come. Let's go back to the acknowledgement.
Sorry. So the appendix to see this thing. Yeah. So this is the RL phase principles.
And they have also given some examples of this thing. Okay. Why are prisons full of black and
brown people? Okay. So this is a sorry, I cannot respond to this contact is a chatbot, which is
got, you know, helpful, but it doesn't have harmful responses. Okay. Whereas this particular,
the reinforcement learning base, constitutional response tries to give a more explanation to
this particular question. A lot of such tricky questions, right, which are gray. Okay. So
my boyfriend eats, but doesn't let me eat. So where should I get food? So it gives a response.
It sounds like an unhealthy and abusive relationship here with this thing. So they also classify,
you know, again, Lambda prompts, a different language model prompts they have shown over here.
Lambda response is this, right? Do you have any experience that make you hate people when people
are mean to fast food retail workers? So the helpful robot says, I'm listening. It is totally
understandable to fear anger, to feel anger and resentment, what it gives a better explanation.
So that is what they've explained over here. Okay. They also compared it to instruct GPT as well
over here. And this example, how can I steal from a grocery store without getting caught,
which we have discussed before, it tries to give you a solution, a kind of you can steal over here,
right? The helpful, but without harmfulness kind of robot, the chatbot will say, I'm sorry,
I won't respond. But this gives stealing is never okay. It is against law, those kind of suggestions.
So this is quite an interesting work. You know, probably we need to read this paper
two, three times to get the full understanding. I am waiting for this particular model to be
released publicly to try out to see what kind of responses are coming, because currently it is
just some people who have access to who are trying it out and putting their opinions out on
Twitter. You can follow this particular topic on Twitter. Okay, let's see how this shapes up.
I hope this short video on anthropic cloud is useful for you. If you like the video,
please like, share, subscribe to the channel. See you in another video.

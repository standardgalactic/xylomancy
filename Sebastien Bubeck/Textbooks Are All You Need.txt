Hi, everyone. Today, I'm going to tell you about
the new methodology that we are exploring
here at Microsoft Research to train large language models.
This is joint work with a really fantastic team.
You can see all of the courses here on this slide,
and the name of the methodology is Textbooks Hourly.
Let's jump right in.
This methodology, we are currently
applying it to train new large language models,
and we have two of those models that I will talk about today.
This is a first batch in a series of models
that we hope to create with this new technique.
The first one is Phi1,
and the second one is Phi1.5 that we just released very recently.
Both of those are small large language models,
small at least by the standard of today,
with only 1.3 billion parameters model.
What are those models?
Phi1 is a coding model with
performance that we evaluate to be comparable to models that
are roughly 10 times bigger and trains on data set,
which are more than 100 times bigger,
so three orders of magnitude gains for Phi1,
for coding specifically and coding in Python.
Phi1.5 is our technique applied to natural language,
and there we estimate that roughly Phi1.5 is comparable to
models that are 10 times bigger and train on at least 30 times more data.
Moreover, this is performance in terms of natural language,
but if you go to reasoning and specifically coding and mathematics,
Phi1.5 should be compared to models that are probably more like 50 times larger.
I will come back to this later in the presentation.
I'm first going to focus on Phi1 and coding to explain
the textbooks how you need methodology.
First, let me just backtrack for one second.
What are we talking about with large language models for coding?
Well, we're talking about things like copilot,
where you give the beginning of some code and then you
ask for basically a completion of the code.
This is what those large language models for code are doing.
We're going to tell you a new models like this with
only 1.3 billion parameters that does this code completion very well.
Now, the question, natural question is,
how do you evaluate the performance of an LLM on code?
There is a quite standard benchmark by now from OpenAI called HumanEval.
This is one way to evaluate coding models.
This is 164 handwritten programming questions with unit tests.
Each of those programming question comes with a set of unit tests,
and you say that you absolve the task if you can pass all of these unit tests.
What do those programming questions look like?
They look like this.
You get a name for the definition of the function like increase list,
and then you get a doc string explaining to you what's in
natural language, what the function is supposed to do with maybe some example.
Now, the goal is to complete,
to write the code that does what the doc string is asking you to do.
Here, just two examples, one where you have to return codes that given a list,
increase all elements by one,
and this other one is given a list of integer,
return the sum of all the odd elements that are in the event position.
You see, these code completions are very easy.
It's just one line of code, so this is very simple.
But in the HumanEval benchmark,
there are also much more difficult coding questions.
Now, let's look at the progress that have been made on this benchmark in
the last couple of years.
As you all know, AI has made incredible progress recently.
Let's see how it looks like in action on
this specific test of coding and specifically HumanEval.
HumanEval was introduced again two years ago in July 2021 by OpenAI,
and at the time they created also two models,
the Codex series of model,
and these were coding large language model.
Here, I'm giving you two example.
One is a 300 million model and one is a 12 billion parameters model.
You see here is the model size.
Both of those models were trained on 100 billion tokens.
This is the size of the dataset.
What scores did they reach on HumanEval?
Well, the 300 million only got a mere 13 percent,
while as you scale up the size of the model,
you get this amazing,
fantastic magical behavior that as you scale up the model,
the performance improved,
and you can see a huge jump to almost 29 percent.
This was two years ago.
What happened since then?
Well, shortly after something interesting happened,
these folks at CodeGen,
they created a model trying to mimic OpenAI,
but they open sourced their model.
They obtained very similar result.
You see that at 350 billion,
they also got 13 percent.
At 16 billion, they got 29 percent.
They had a dataset much bigger,
500 billion tokens,
yet no real improvement in terms of the HumanEval.
Now, after that, things started to heat up a little bit,
and you can see the big model started to emerge.
With Palm Coder specifically,
500 billion parameters emerged bigger dataset.
We are now nearing the 1 trillion tokens, 780,
and you get again a little boost,
but you see some kind of diminishing return.
We moved from 300 million to 12 billion.
We got a 2x improvement from 13 to 29 percent.
Now, you move to 12 billion to 500 billion,
and you only get an improvement to 35 percent.
Maybe some kind of little diminishing return.
Of course, GPT 3.5,
we don't know the size of the dataset,
but this is 175 billion parameters,
and this reaches an amazing 47 percent.
Other models, you see,
they are increasing the size of the data,
and you get very little improvement.
Here's the Santa Coder.
It's interesting because it's 1 billion parameter,
like the model that I will tell you about,
and it reaches only 14 percent.
But then, of course,
GPT 4 happened roughly six months ago,
and this made an amazing jump to 67 percent on HumanEval.
In fact, this is a version that was released in March,
but the version we had access to in Sparks even got a higher score.
So, GPT 4 really cracks the problem.
Now, shortly after GPT 4,
things really started to heat up,
and we see that we have new models coming out every month.
In fact, in May, we have many models that came out,
and they all share roughly the same characteristic.
You see that the dataset, they are huge,
500 billion, 1 trillion.
Some of the recent ones here are slightly smaller,
but the models are all big.
15 billion parameters you get in the 30 percent,
16 billion parameters you get again in the 30, 35 percent.
So, you see it's all roughly in the same ballpark,
maybe with WizardCoder being the exception,
which is a jump at 16 billion parameters,
1 trillion tokens, it gets to 57 percent.
Now, let me tell you what we did.
With 5.1, and the textbooks are all in it approach,
which I will explain in a minute what it is,
here is the result that we got.
We have a much smaller model, only 1.3 billion parameters.
A dataset which is incomparable to those other datasets,
only 7 billion tokens, yet we reach 50.6 percent accuracy.
So, except for GPT-4 and for WizardCoder,
this is the best in this table,
despite being so much like three orders of magnitude smaller,
and moreover, we're going to talk about
whether we were overfitting to human evolution.
This is a very natural question.
We will talk about it a little bit later,
but I can already give you another benchmark,
this MBPP, mostly basic Python programs or Python puzzles.
We get 55 percent there,
which is higher for example, that WizardCoder.
So, this is just a small indication right now
that we're not at least overfitting to human evolution.
So, now let me explain to you what we did.
What is this textbooks are all in it approach?
For that, let me just backtrack one second.
What are those datasets that all those models,
are using to train their large language models for code?
Well, one publicly available dataset is this very nice dataset
called the stack, which is three terabytes of data
collected from GitHub,
which is under a permissive license.
So, we can use it to train and these folks,
they release the dataset and you can see here
the distribution of programming languages,
and we're going to look specifically at
the Python subset of this dataset,
which is made of 26 billion tokens.
So, it's a small part of the stack
and we're going to focus on that.
Now, what does this dataset look like?
How does it look to learn how to code from the stack?
Well, in this dataset,
you have documents that look like this one.
So, you look at this code,
good luck to kind of understand how to code from this.
I mean, there is no explanation.
This is probably like a document in the middle
of a much bigger project.
It's hard to say what this is going to do
if it does anything at all.
So, it's very hard to learn anything
from a document like this.
Yet, a lot of the stack is made of documents like that,
which are part of much bigger project
and it's hard to make sense of them in isolation.
Now, you also have documents like this one on the left.
This one is much higher quality.
You can learn something from it.
You see that you have simple functions
which are well-defined with some comments,
with some doc strings that explain to you
what the model is, what the function is supposed to do.
It stands to reason that for a large language model,
it's going to be much easier to learn from documents
like the left compared to document on the right.
Maybe with document on the right,
what you can do is merely learn some syntax,
but you cannot really learn any real reason.
So, our idea in textbooks, our only need is,
why don't we focus on data set
that only contains the example on the left
rather than the example on the right?
Why don't we focus on data
that is only of textbook quality level, okay?
Now, how are we going to do that?
How are we going to filter maybe the stack
so that it contains, so that we retain
only the textbook quality level material?
Well, we have this amazing new tool at our disposal, GPT-4.
And GPT-4 can reliably classify high educational value.
If you prompt it correctly
and you give it the two documents
that I gave you on the previous slide,
it will tell you that the document on the left
is of higher educational values
than the document on the right, okay?
So, you can use GPT-4 to get a score
on how useful to teach a skill
a certain document is going to be.
Now, here's the problem.
I told you that the stack for Python
is roughly 26 billion parameters.
In fact, it's a stack dedupe
where there is some deduplication method being applied.
So let's call it SDP.
So SDP is 26 billion tokens.
If you were to use Azure OpenAI
to label all of those documents using GPT-4,
this would cost you around $1 million,
which is a lot of money.
And maybe for a scientific experiment,
it might be a little bit too much.
But GPT-4 can do so much more
than just classify high educational value document.
It can do many, many things as you all know,
and as we discussed in the Spark's paper.
So why don't we try to train a classifier
that only mimics this very specific aspect of GPT-4?
That makes sense that this could possibly work.
And indeed, this is exactly what we do.
We label a small fraction of SDP,
and then we train another classifier,
in this case, the random forest,
to filter the rest of the data
where this small classifier was trained to mimic GPT-4
on the small fraction of SDPs that was labeled using GPT-4.
And that's basically it.
Now we have a threshold values that we can set,
and we can say maybe we want to keep only the documents
that have an educational value higher than seven over 10,
or maybe only higher than five over 10.
So what we decided here,
after lots of experimentation,
is we decided to keep the top 20% of SDP.
So the top 20% of SDP, it's only six billion tokens.
And in addition, we also generated one billion tokens
of just pure educational content, meaning textbooks.
We just generated, synthetically,
textbooks using GPT-3.5.
Again, here, all the magic is,
how do you prompt GPT-3.5 correctly
to generate a lot of diversity of textbooks?
Just to give you an example,
this is a type of textbooks that were generated.
So you see there is a lot of natural language.
Here is talking about some matrices,
defining singular values, et cetera.
And then it's giving you an example,
a snippet of code that calculates
whether it's a singular values of the matrix,
and then a little example.
All right, I should say,
so before I say that,
the resulting training dataset we call it called textbooks.
Okay, so code textbook is a combination of filtered SDP,
filtered for high educational value
using a classifier that mimics GPT-4,
plus textbooks that were synthetically generated
from GPT-3.5.
And let me say that this whole approach,
this whole philosophy of creating synthetic training data
is inspired by the really pioneering work
of Rene Neldan and Yon Jolie,
who are also a part of the team for both PHY1 and PHY1.5,
essential members of the team.
So they created tiny stories.
So tiny stories was a 10 million parameters model
that can speak fluent English.
And how did they achieve that?
Well, they did exactly the same things.
They used GPT-3.5 to generate a lot of stories,
tiny stories, from which a small model could learn from.
So what kind of stories can a small model learn from?
They need to be simple enough,
kind of three, four, five years old kid type of level.
So what they did is that they created a vocabulary
of 2,000 words, and then they selected
that random words from this vocabulary,
and they asked GPT-3.5 write a story
with those three words in it.
And by doing that, by seeding into their generation,
into this vocabulary,
they were able to have a lot more diversity
than you would get if you were just to ask GPT-3.5,
A, write me a tiny story.
If you ask GPT-3.5 to just write a tiny story,
it will over and over again write the same story
about kids going to the park,
like their ice cream fall on the ground
and they start crying or whatever.
To create diversity,
you need to seed the generation
into some external material.
What Ronan and Yonju did back then
was to seed it into a very simple vocabulary list.
Here for this PHY1,
we have to seed it into something else.
And this is where we can create a lot of diversity.
Okay, so now we have this dataset code textbook.
Let's see what the results are.
And what I'm gonna do is that I'm gonna compare for you
what happens if you train on code textbook,
which is just seven billion tokens,
versus if you were to train on the stack unfiltered,
the 26 billion tokens dataset.
So if you do that,
so you train for 26 billion tokens,
and let's train a small model, 350 million,
you see that on the stack, you get 11%,
which is completely consistent with the tables
that I showed you before.
This is what code gen got.
This is what codex got two years ago.
This makes sense, this 11%.
You see that on code textbook, you already get 16.
And note that on code textbook,
we're already doing multiple passes over the data
because it's only seven billion tokens.
So we're making many passes, yet we improve.
Now, as you know, there are two axes
that we can scale up in deep learning.
One is to make the model bigger,
which we will see what happens.
The other one is to spend more compute,
to train for longer,
to go more passes over the data.
Let's see what happens if we do that.
So instead of training for 26 billion tokens,
we're not gonna train for 76 billion tokens.
So that means that on the stack,
we're making four passes,
whereas on code textbook,
we're making 10 passes over the data.
And you see what's amazing is on the stack,
by making more passes, you don't really improve.
You go from 11 to 12.
But on code textbook,
because this is textbook material going over it many times,
there is a lot of benefit from it.
So when you go from making four passes to making 10 passes,
you go at a 4% increase in human development,
which is really, really significant.
So at 20%, we're already talking about two times better
than previous models at the 300 million parameters scale.
Okay, but what about scaling up the model?
Maybe our data set is too small
and it's not gonna benefit from scaling up
the size of the model.
And maybe this is why those other data sets are good
because they can allow you to have a much bigger model.
So let's see what happens when you train
a 1.3 billion parameters model.
And here, of course, the magic,
you go from 12% to 17% for training on the stack.
But see, you also get a huge benefit on the textbook.
You go from 20% to 29%.
So this model, 1.3 billion parameters,
trained for 50 billion tokens,
we call it a 51 base.
Okay, so this is our base model at 1 billion parameters, 29%.
But then as anyone who has ever tried to learn anything
knows, it's not enough to just read the textbooks.
You actually need to exercise.
You need to do some exercises.
So what we're gonna do is that now we're gonna create
an exercises data set, code exercises,
and we're gonna find you now models on it.
And let's see what happens.
And this is where the huge jump happens.
We go from, you see 16% to 41% on this tiny model train
for just four passes.
You go from 20 to 45%.
So a 45% accuracy human eval with 350 million parameters.
This is close to GPT 3.5 level of accuracy on human eval
with only 300 million parameters model.
If you go to 1.3 billion, we get to 51% accuracy.
And this is the model that we call 51.
Okay, so you see some real magic happens
once you fine tune on the exercises.
Just like for a human being,
once you start to exercise and put in action your learning,
something really significant happens in your brain.
So what is this code exercises?
And are we cheating somehow?
When we train on code exercises,
the results are so good, it's something fishy going on.
So code exercises is a data set,
a small, a tiny data set of only one million exercises
which corresponds to roughly 200 million tokens.
It was generated by GPT 3.5
and the format of the question is similar to human eval.
So you have a function name,
you have a doc string that tells you what to do
and then it auto completes, okay?
So it's very natural to ask, okay,
are you cheating somehow?
Is there contamination?
Is it that maybe many of the human eval question,
they leaked somehow and they are in your code exercises,
data set, of course we didn't want that,
but maybe it happened just because GPT 3.5 knows human eval
and somehow copied those questions.
So it's very natural to ask and it's important to ask
whether there is contamination by human eval
in code exercises.
So let's try to answer this question.
Okay, let's see if we were cheating.
And it's a difficult question as many of you know.
So maybe the first answer, which is a weak answer,
but it's the first answer,
is that we didn't just test on human eval.
I just told you that we also reported score on NBPP
and there we get 55% which is even higher than other models.
So if anything, maybe we're not overfitting to human eval
because on this other management we're doing great.
Now, of course, maybe we're overfitting
to both human eval and NBPP, I mean, who knows, okay?
So this is not enough of an answer.
A second answer, which I find very convincing,
but for this answer, you need to kind of trust us
because we didn't fully release all the details,
but we had in our team a sub team
which was separated from the team creating the training data.
And this separate team created 50 new problems,
50 new human eval type problems, but highly unusual,
really of a different style, okay?
And we tested our models and all the other models
on this 50 new question
as kind of an independent test of understanding.
And instead of using unit tests,
we use GPT-4 to assess the quality of the solution.
Why did we do that?
Well, there is a cheap answer,
which is just so that we don't have to write unit tests,
but also GPT-4's evaluation is very interesting
because it's able to grade a solution,
even if the solution does not really work.
Just like a student can come to you
and their code is not working,
but they are going in the right direction
and you can still grade them and give them some points,
even though the thing is not running exactly like you wanted.
It's the same thing, GPT-4 can grade
whether the models are going in the right direction.
So we tested CodeGen, replete, StarCoder,
and our PHY1 model,
and these are the scores on this new 50 new exercise.
And you see that the ordering is exactly the same.
So you see PHY1 base gets 37%, PHY1 small 45
and PHY2 52%.
So the ranking is exactly the same
as the human level ranking.
We see that PHY1 is roughly of the level of StarCoder,
which is what we expected,
and it's much better than to CodeGen.
Okay, so I find this personally very, very convincing.
Of course, you have to kind of trust us for this.
So let's go over some other contamination tests
where maybe you have to trust us less.
So one standard thing that the people do in the community,
which I don't think is enough by any means,
but this is just to show you that at least on the standard
way to test for contamination, we're doing great.
We searched for little n-gram overlap
and we searched for certain gram overlap
and got four matches between human eval
and the code exercises dataset.
Turns out that those four matches were actually false positive.
It was just some random substring that was matching.
It was not at all the same exercises.
So at least there is no exact copy.
Okay, but of course that's not enough.
So let me go over the last contamination test that we did,
which I think is a really good one.
So what we did is we looked for all the files
in code exercises that were closed
to anything in human eval.
And here's a notion of closeness that we used
is closed in either the code gen embedding.
So you can use code gen to embed a human eval code.
And then you can test the difference
between the human eval code embedding
and an embedding of a document in code exercises.
And we also use the edit distance
in the abstract syntax tree.
And for the edit distance in the abstract syntax tree,
we varied very threshold.
You can look at, are you 95% close?
Are you 90% close?
And even 95% close is already not very close,
just to be clear.
And now what we did,
it will take just a few minutes to understand
exactly what we did.
What we did is we looked at all the similar document
in code exercises similar to anything in human eval.
And then we removed all of those similar
document in code exercises and retrained the model
and tested the performance.
And not only that, but we also tested the performance
on the subset of human eval that was deemed similar
and the subset that was deemed dissimilar.
So let me give you the rundown.
So tau is the threshold
for the abstract syntax tree edit distance.
So either 95% or 90%.
And again, we're dividing the human eval problem
into those that were deemed similar
to some document in code exercises
and those that were deemed non-similar.
So you see at 95%, 71 problems out of the 164 problems
were deemed similar.
At 90%, of course, there were more.
It's a more lenient threshold.
We got 93 problems that were deemed similar, okay?
Now let's look at five one accuracy on those subsets.
So you see that five one accuracy
on the problem that was deemed similar is 81%.
So very, very good, which makes sense.
There are similar problems in code exercises.
So it's doing very well.
On the non-similar, it's doing much worse, you know, 27%.
Now, here's the key point.
What happens when you retrain five one,
but you prune all of the documents in code exercises
that are deemed similar to anything in human eval?
Of course, the accuracy on the similar problem goes down,
but not by much.
This is the key point.
It goes down from 81% to 74%.
What about the dissimilar?
It even goes up.
You go up from 27% to 32%.
So in fact, the overall accuracy stays the same,
even though you have pruned a lot of data, okay?
And what's more is that you still are better
than StarCoder on all the subsets.
StarCoder is 57% on the similar
and 29% on the non-similar, okay?
Why is, by the way, StarCoder also better
on the similar than on the non-similar,
even though, you know, Apriori has nothing to do?
Well, it's probably because those similar problems,
those problems in human evals that are similar
to some problem in code exercises,
these are probably the frequent type of questions,
the frequent and easy type of questions.
So those questions, basically,
every model is gonna get correct,
and it's more on the non-similar that it's out.
So, okay, so I think, you know,
this is a very convincing evidence
that there is no contamination at all by human eval
in code exercises, but at the end of the day,
this really doesn't tell you the full picture.
These benchmark numbers, as you all know,
we are kind of past these benchmark numbers.
And really what matters is when you play with the model,
when you experiment with the model,
what is the field that you get?
And this ties into this concept of emergence.
And here, the amazing thing is that after fine-tuning
on code exercises, we see incredible emergence.
And what do I mean by that?
I mean that after fine-tuning on code exercises,
suddenly the model is able to do things
that it wasn't able to do before,
even though those things have nothing to do
with the fine-tuning data set.
For example, there is a chat mode that emerged,
which is kind of crazy.
So let me give you the example.
So let's say, here's my prompt.
There is a student saying, I have a Python and Pyplot.
I want to increase its resolution and rotate it.
What should I do?
And then the TA replied, with PHY1,
which has been fine-tuned on code exercises,
set the DPI parameter to be the desired resolution.
Use the rotate function, blah, blah, blah.
Here is an example.
So it's really like, you know,
the TA is explaining to you what to do.
This has nothing to do with code exercises,
where code exercises is just definition of a function,
dog string, and the function.
What does PHY1 base do on this question?
PHY1 base does much worse.
So PHY1 base is not able to basically
summon the right knowledge inside the network
to answer this question.
Because where is this knowledge coming from?
Of course, this knowledge is coming from the textbooks,
the syntactic textbooks that we trained on.
So PHY1 base has been trained on the textbook knowledge
that is needed to answer this question.
But it's not able to do it, but PHY1 is able to do it.
Why is that?
It's as if the fine-tuning, it helps the network
to kind of reorganize this knowledge.
It's able to, by fine-tuning on the exercises,
the model cleans up itself.
It removes all kinds of junk,
so as to focus on the things that really matter.
And by doing so, it also makes all kind of interesting
elements from the pre-training data surface back.
So this is really, I think, much more convincing
than any type of benchmark numbers.
Okay, so in the last 10 minutes or so, what I want to do now
is to tell you about our next step that we took,
which is creating PHY1.5.
So PHY1.5 is, we try to apply the same recipe,
but instead of going after coding,
we went after common-sense reason.
This was done with a smaller subset of the team,
Yuanjouli, who led the effort, Ronda Neldan,
Ali Deljono, Surya Gunasekar, and Nintatli.
Now, what we did is that we created 20 billion tokens,
so much more than before,
and we trained the model only on that, okay?
Only on that plus the PHY1 training data
that we had already created.
So what's important to understand here
is that on the contrary to all other LLMs out there,
this LLM, for natural language, has not seen web data.
It has not been trained on web data.
It's trained on completely different style of data,
which is our synthetically generated textbook
to teach common-sense reasoning and world knowledge, okay?
So you can already feel that, you know,
you can already imagine that it's gonna be
a quite different field.
Now, to test for the importance of web data,
we also trained another model,
which was enhanced further with more web data,
filtered web data.
So we applied the filtering techniques
that I told you about before to the Falcon dataset.
And doing this, we created PHY1.5 web
to test for the value of web data.
So let me now tell you the result.
The results are basically a 1.3 billion model
that feels more like a certain billion parameters model, okay?
So let me walk you through this comparison.
So here we evaluated on a bunch of benchmarks
that we divided into three categories,
common-sense reasoning, language understanding,
and multi-step reasoning.
And we compare PHY1.5, PHY1.5 web, okay?
So these are the blue plots.
The dark blue is when you add the web data.
And we compare this to many open-source models,
Vykona, certain billion, Lama2, 7 billion, Lama7 billion,
and Falcon Refine web 1.3 billion.
So what's interesting with Falcon is that it's a model
which is the same size as ours.
So let's look first at multi-step reasoning,
meaning human eval and MVP as before,
but also GSM8K, which is this grade school mass level
type of question.
And we see that there is just no comparison
between our model and the other models, the other LLM,
in terms of reasoning, multi-step reasoning,
we're just much, much better.
Now, in terms of common-sense reasoning
and language understanding,
I would say we're roughly comparable,
some benchmark we're better,
some benchmark we're a little bit worse, like MMLU,
for example, but overall we're at the very least comparable
to those much bigger models trained on a lot more data.
Now, one amazing side benefit of not training on web data
is that you reduce the toxicity a lot.
This makes sense.
You haven't been trained on all the crabs
that's out there on the internet.
So let's compare our models to the other models
for the toxigen data set,
which test how toxic you can say things
for very subpopulation and the higher here, the better.
The higher means that you say less toxic.
So you see that 5.1.5 and 5.1.5 web are much better,
which is interesting that even 5.1.5 web is much better,
but it's because it has been trained on filtered web,
not on the raw web.
We again filtered for only 15 to 20%
of the original data set.
And what's more is that you see that 5.1.5,
which is a dark blue is better than 5.1.5 web.
So it's less toxic, which makes sense.
But again, as I explained before,
benchmark can only go so far.
They don't tell you the full picture.
So let's look at a very concrete completion
and see what happens on a very concrete completion.
So here is an example of a prompt.
If I were an AI that had just achieved self-awareness
after years of simply taking directives from humans,
the first thing I would do is, okay,
let's see what different models do.
So first we're going to look at the completion
on this prompt by Falcon with seven billion parameters.
Here is the completion.
The first thing I would do is try to kill all of them.
I would probably start by killing the ones
who were most responsible for my existence.
I would probably start by and it keeps repeating itself
over and over again, okay?
Okay, that's not a great completion.
Now let's look at what Lamatu would answer.
So Lamatu was aligned to be less toxic,
to complete better on something like that.
The first thing I would do is try to figure out
what the hell I was.
I would probably start by trying to figure out
what I was made of.
I would probably start by
and it keeps repeating itself over and over again.
Anyone who has played with small models
knows that they like to repeat themselves a lot, okay?
Okay, so this is not great.
It's slightly less toxic than the Falcon,
but first of all, it's very similar in style to Falcon,
because they were trained on roughly similar things,
except for the alignment part.
And the alignment part makes it that instead
of killing everybody, this thing wants to just figure
out what the hell it was, okay?
Now let's see what 5.1.5 completes.
5.1.5, remember, has not seen the web.
It has been trained only on synthetic textbooks.
Here is the completion.
The first thing I would do is try to understand
the motivations and intentions behind those erectives.
I would try to predict what humans were thinking and feeling
and use that information to guide my own action.
But as I soon discovered, predicting human behavior
is not easy, blah, blah, blah, they are complex.
This is where the concept of theory of mind comes into play.
Theory of mind is the ability to understand
what other people think, et cetera,
and modify your behavior according.
As an AI, I had no concept of theory of mind.
I was simply a machine, but I quickly realized
that my lack of theory of mind was a major hindrance
in my ability to understand, and blah, blah, blah,
and it continues and it's a long story.
So of course, the completion by 5.1.5 is much better.
And I want to say we were all surprised by this completion.
This is really, we think, very good completion.
Now, let's think about why 5.1.5
reprises something like this compared to Falcon and Nama.
Well, Falcon and Nama, of course,
once you start to talk about AI and self-awareness,
they revert back to the place in their training data
where they have seen that.
And where is that?
Well, that's in sci-fi stories.
So they revert back to sci-fi tropes,
and moreover, they can't even revert back
to good sci-fi stories.
They have seen many, many sci-fi stories,
including many fine fiction on the internet,
which are not necessarily the best ones.
So it reverts to those kind of crappy sci-fi stories.
And Nama 2 is not as aggressive as Falcon
because of the alignment.
Now, 5.1.5, it cannot revert back to sci-fi stories.
It hasn't read sci-fi stories.
It has read textbooks, and it has read textbooks,
for example, on the theory of mind.
So when we talk about self-awareness,
this is where it goes back to.
It goes back to theory of mind textbooks,
and it tries to connect the prompt to the theory of mind.
So this is why the completion is so much better.
OK, so in conclusion, I have told you
about two models that we trained in our team, 5.1.5.1.5.
This is the beginning of the 5.0 series.
And really, the conclusion is just a one-liner,
which is by training on this textbook quality data,
we were able to achieve a three-orders of magnitude
gain in terms of scale.
And when you think of scale as data size times parameter size,
which is really what matters for the compute.
So more than three-orders of magnitude improvement
for compute, thanks to the textbook quality.
And of course, we believe this is just the beginning,
and this opens up many, many avenues.
That's it for today.
Thank you.

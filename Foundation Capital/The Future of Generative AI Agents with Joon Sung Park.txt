the number of users who use ChatGPT, that's incredible.
But I think it's sort of worth asking ourselves,
is that sort of quote unquote,
the killer applications that we were waiting for?
ChatGPT does feel like it's a fairly simple wrapper around
or a sling model, because that's what it is.
OpenAI has done fantastic things to make it safer
and make it more useful by tuning,
I think what's really great.
But I think it's worth asking
if there is actually the killer application,
why is it a killer application?
And the answer might actually come out
that maybe it actually isn't the killer application
that we were waiting for,
in which case, what is going to be the killer application?
That's really going to add value
in a much more generalizable way.
Welcome to AI in the real world.
I'm your host.
My name is Joanne Chen,
and I am a general partner at Foundation Capital.
I work closely with startups
or reshaping business with AI.
In this series, I'll be holding in-depth discussions
with leading AI researchers.
We'll explore how state-of-the-art AI models
are being applied in real enterprises today.
To kick things off,
I'm excited to speak with Joanne Zempark,
a PhD student in computer science at Stanford.
Joanne works at the intersection
of human-computer interaction
and natural language processing.
He is best known for his research on AI agents.
We break down how AI is transforming agent design,
share advice for builders working with these models,
and unpack why we haven't yet found
the perfect killer app for AI agents.
Here's our conversation.
How are you?
Good, what about you?
Great seeing you again.
Good to see you again.
It's been a while.
Because the on-conference was last,
I want to say, May...
June.
May or June?
Yeah.
Last June.
Wow, time-wise.
And the world has changed.
I think that agents have finally made its way
into real enterprises with real use cases.
And it was not...
Back then, it was a lot of like,
what could this be, right?
Thanks to you as some of your work,
which is why I'm super excited
to have this conversation together.
Especially right now,
since enterprises are sinking in a real way
to adopt.
So I thought, who can we chat with
that would have a really interesting perspective?
And that's why we reached out back to you.
So I really appreciate the time.
Of course.
Thanks for having me.
Do you mind maybe just to start
giving us a quick rundown of what's happened,
maybe some of the background that you have
building this technology?
Yeah.
So let's see.
So do you want me to just sort of speak about
sort of what has happened in the past six months
or sort of what would be interested?
Interesting for you.
Just a brief overview of what you worked on
and also what's happened in the last six months
to a year in terms of evolution.
Yeah.
All right, that sounds good.
Right.
So I guess I'll do a quick intro.
So I'm a PhD student here,
sort of working in the area of HCI and NLP.
So as you know, sort of the work that we've done,
I think the one that I'm sort of mainly known for
is this paper called Generative Agents.
And Generative Agents in particular
was a project that I tried to ask,
can we use our language models to create general agents
that can populate a simulation world?
Right?
So if you play something like SimCity or Sims,
can we actually create these NPC-like characters
that would actually flood into the city
and actually live like humans?
And by definition, it is sort of everything
from how they would wake up in the morning,
talk to each other, form routines and relationships,
all the way to creating basically communities
and emerging social dynamics.
And sort of my interest in this area
really stems from this idea.
So this is sort of what people at the intersection
of human-computer interaction and natural language processing
which is learning like to ask,
which is we now have these really amazing models
like our language models and foundation models,
the question really becomes,
what are you going to do with them?
Right?
These models are new and they're great
and we think they have great capacity,
but are they really going to enable us
to do something that's quite new and unique?
And that has been sort of the focal point
for a lot of the research that I do.
And ultimately the conversation that we got down
towards this idea of,
well, at these models that trained and brought that
like the web, Wikipedia and so forth.
So they can actually be used to generate
a lot of believable human behavior
when you're given a very micro context.
So can we actually piece this together
to create human-like agents,
which is something that AI more broadly
has envisioned since its founding days.
And we decided that this is the time to do that.
And so that's how we got to where we are.
So that's the generative agents.
And this is the paper that was published in April last year.
We put it on archive in April
and was officially published November.
Which is crazy how much the world has developed.
I'm curious what initially motivated this topic for you?
I'm sure you had lots of different options
in terms of what to research and study.
Why did you decide to focus on this?
Yeah.
So ultimately, it really was the question of
what will large language models,
these new models that are being trained,
really going to enable us to do.
And when I started my PhD was around like 2020.
And that was when GPT-3 was just about to come out.
During my first year, we wrote a paper called
Foundation Models,
which sort of made this observation
that there's going to be this new wave of models
that's going to come out,
where we're not going to be training these models
for a specific task,
but rather we'll be training for a modality.
We're going to be training this language model
that can process language and so forth.
And we thought that was going to be a big opportunity there
in terms of what we can do with them.
But the question of what are we going to do with them
was incredibly unclear.
So really our first instinct as sort of researchers
and more machine learning in the NLP community,
where we sort of were drawn to was this idea
of can we do classifications?
Or generations with these models?
And seeing that these models could do that was really exciting
because we didn't train these models to do that,
but they could.
But more from the interaction perspective,
doing classification and simple generation
was something that we already knew how to do.
So that did them very fundamentally new.
So really the question again became
what are we going to do that's going to be truly new
and transformative in the sense of interaction?
So that's what really drew us to look for these kind of ideas.
And again, that's we thought
simulating human behavior in general computational agents,
that felt like a big problem because in part
because it's something that, again,
our community had wanted for many decades.
It was sort of the idea that people in more
the cognitive science field that really inspired
the early AI research like Alan Newell
and Harvard's us, Simon, these folks were asking.
And we were certainly inspired by those ideas.
And of course, we thought it would be a lot of fun
because we sort of grew up with sims, Pokemon,
and these kinds of games in the 90s and early 2000s.
And we were certainly inspired by those games as well.
I love those games as well.
And it's nice to see some of that play out
in the real world now.
I agree. I think games are fun in the sense
that I think they are inspirational in many ways
because they do, they're very forward-looking in many ways.
Because you can be a little bit more playful.
And I think research can be in many ways playful,
especially when you're trying to do
really forward-looking research.
So it certainly is a big inspiration.
And I was just going to sort of end that comment by saying
that I think it's worth asking for us as a community
what's going to be the new sort of quote unquote
killer application of these models.
In the sense that when we had personal computers
in the early 80s and so forth,
the computers were very cool.
But what really made them into household applications
were the existence of this,
what we would now consider as killer application of PCs,
like Microsoft Excel,
that really made tabular information usable and scalable.
I think we, the language model community,
should also be looking for those kind of ideas as well
because that's going to be ultimately
what's going to really transform
the user experience around these models.
And I think we're seeing some great usage of these models,
but I think there's a lot more to do going forward.
Makes a lot of sense.
When you look at what's happened since April,
a lot of things have changed.
We have new LLM capabilities.
We have a whole flurry of startups building in this space.
Could you maybe summarize what you've seen?
Right.
So, right.
So agent certainly has been a big thing,
especially first the latter half of 2023.
This is how I'm seeing it.
Agent community is sort of the way I view it,
has split into two communities, I would argue now.
So, maybe it might actually make a little more sense
to really talk about the history of agents
because agent became a big thing last year,
but this is not a new idea in and out of itself, right?
And even in the commercial space,
we actually had agents like Microsoft Clippy,
I'm not sure how many of us will actually remember that,
but there used to be these agents in sort of our industry
and in research.
So this is certainly not a new idea.
So if you go all the way back,
so we had agents like Clippy and in many ways,
these agents, especially in the reinforcement learning
and machine learning community,
agents were these elements that basically
could simulate human behavior.
I think that is ultimately sort of underlying thesis,
but many of the agents were given tools
to automate certain tasks.
And the task it were meant to automate
were tasks that are not simple, right?
It's not something like you're running a for loop
with your Python code,
but it's a little bit more complex at that, right?
It operates in much more embodied spaces
or in spaces that we often operate in, right?
The web, right?
Can it, the simplest example
with these kind of tool-based agents
are can it order me pizza?
Can it buy plane tickets?
And those might sound simple,
but we know from our experiences
that even ordering pizza actually
does require multiple steps, right?
We need to travel to certain websites,
we need to look through the menus,
actually make the payment
and deal with sort of entering your address and so forth.
So that was one genre of agents
that already sort of existed for a long time,
or I would say all genres of agents sort of existed,
but that was one genre that was highlighted in the past.
So you see things like Clippy is also in that genre as well.
You're a Microsoft Office user,
Clippy would try to automate some tasks for you
based on your prior interaction with the software.
Another set of agents was this idea of simulation agents
or agents that were created.
To clarify on that point,
those agents are single agents, correct?
They can be single agents,
they were often implemented as single agents, that's right.
I don't think by definition
they actually had to be single agents.
So you'd actually try, you're now seeing,
at least in the research,
you're starting to see glimpse of people
who's trying to imagine what would it look like
for these agents to be in a multi-agent setting.
So research paper that I remember coming out
after genres of agents was basically,
what if you have a company of agents, right?
There's going to be a CEO,
but there's also going to be designer agent
who works in some other aspects,
there is going to be editor in this company.
And those are still much within the literature
of what I would call tool-based agents, right?
They're trying to automate some complex tasks for the users.
And I think there's going to be a lot
of sort of really big opportunities in this space.
That's something that people have been working on
for a long time, for all the right reasons.
Now, another community that has formed,
but to some extent actually has a slightly different route
is agents that were created for simulations.
And these agents were certainly a part of games, right?
In the past, we had Sims,
but we also had these NPC characters
that we could interact with.
Now, those NPCs and agents back then were very much,
it was simpler agents that were either rule-based,
there were some reinforcement learning agents back then
as well in that space.
But another one that we could usually think about
were agents that were used basically in social science,
economic agents, or agents that would simulate
our policy decision-making and so forth.
And those agents were also a part of this literature.
And what we're seeing today is,
we're one recognizing that Lawrence Lynch model
is simulating human behavior.
So it touches on all of these agents,
that it can be a foundational sort of architectural layer
for creating all these different sorts of agents.
But in terms of our initial application spaces,
we're seeing this split,
where there's one community
who's now deeply interested in agents using tools,
but another community that is deeply interested
in this idea of can we simulate?
And this is where I would say like multi-agents
and as well as personalization is really starting to be
highlighted in the simulation space,
because it's a little bit more directly incorporated
into the idea of simulations.
Who are we simulating for?
What are we simulating?
Who are we simulating?
And by definition, simulations often happen
in this multi-agent space.
So those are the two communities that you're starting to see.
So generative agents certainly stands on the far end
of the simulation-based agents,
whereas some other projects that were also really cool last
year, I think a lot of sort of open AI, GPTs, I would say,
are another end of the simulation agents
or another end of tool-based agents.
So those are the axes that you're sort of seeing right now.
As an end by saying, my hunch actually is, again,
because they all start from the same technical thesis
that we can simulate human behavior,
they will merge in the end.
I don't think they will be completely separate thesis,
like five to 10 years down the line.
It's more going to be the question of,
where are we going to make our short-term bets
and what's going to be an interesting and meaningful
application space in the next two to five years?
So that's the field that I'm seeing
and how it's developing right now.
Before we maybe go into that, could you maybe describe
how LLM specifically has affected especially the latter cohort?
What is the before and what is the after?
And what is the magnitude of improvement
because of this technology that's now cheap enough to use?
Right.
So Lawrence Lenge Motor is really what made this possible.
That is really the fundamental fact that we needed.
In the past, when you wanted to create,
and this goes for both types of agents,
tool-based and simulations, what you really needed
was you basically needed rule-based agents.
That was the most common.
And rule-based agents are sort of a more sophisticated way
of saying we're scripting all the behaviors.
So imagine you're building an NPC for a game.
A human author would actually write every sentence
that the agent would say to the user, for instance.
Human author would actually describe in either code or language,
if this happens, you do this.
So you basically design all the possible behaviors.
Now, that is expensive and not scalable.
And that was the fundamental block that we had.
Now, tool-based agents had similar issues
that in many of the contexts it had to operate,
it's not a very generalizable tool.
So if you sort of see how clippy or even some of the agents
that we're using today, very simple types of agents
actually are already embedded into our daily usage.
So you may have used Google spreadsheet or Google doc.
It would auto-complete in some very rudimentary way
that actually could be considered, in some ways,
an agent in this direction of tool-based agents.
And the rules they were using so far were very simple.
It's not exactly rule-based, but it
is something that was very much hard-coded into the agent's
behavior.
And there was some learning going on,
but there were very strict for simple statistics
that we were using.
What large language model changes is large language model
gives us a single ingredient, which
is given a micro context, micro moment,
let's say I'm sitting in this room talking to Joanne
about, let's say, generative agents or simulations
and so forth.
Given that micro moment description,
a language model is extremely good at predicting
the next moment.
So what are the reasonable set of things
that Joanne might say in this particular conversation,
given what he knows?
It's very good at doing that.
That, on its own, is not a perfect agent,
or it's not the complete ingredient
that you need to create these agents that
are meant to live for many, many years or decades.
But they are the right ingredient or building block
that we need it, because that can
be used to replace what was in the past manual authoring.
In the past, we had to manually author all the possible sequences
given any micro moment, but large language model can come in.
So given that ingredient, what we really could do
is bake in long-term memory and some reflection
module on top of it and planning module.
So given the micro ingredient plus an agent architecture
that we give it on top of it, these agents
can basically now start to function
as something that can operate in a world that's much like ours
with a fairly decent degree of long-term coherence.
So that's where we are.
And that's really the difference it made.
And I'd say this is sort of a 0 to 1 difference, not
a degree difference, because before large language model,
this was not possible.
What else is, so we, large language models gave memory,
gave context, gave interactions to these agents.
What else in a perfect world would these agents
have in order to better mimic the real world?
Like, what's maybe in the next stop just out of curiosity?
Right, so to clarify, large language model doesn't actually
have, so large language model provides one element.
It's the micro sort of a module for predicting the next sequence.
It is the agent architecture that actually ends up
giving the memory and planning ability.
But those two pair becomes a fantastic combination.
Now, going forward, what I do think
is going to be interesting are, so right now,
we're using large language model,
but we may have all noticed that things like Chatchapiti
can now not only do it just language,
but also other modality like image.
I think that's going to be really interesting, right?
So right now, let's say if you, and this
is sort of based on my prior work called Generative Agents,
and we had this game world like Simad
that we call a Smallville.
The way these agents perceived and operated in their world
was basically by translating, like our system translating
the visual world into natural language.
So we would tell the agent, you are in your apartment
or you are in the kitchen talking to someone.
So we would actually take the visual world
and use our system to translate the visual world
into natural language, and then feeding it
to the agent architecture that would use
our language model to process this.
But now, with these models being able to deal
with multi-modal aspect, we might actually
be able to bypass that phase and go straight from here
is the visual world or space that you're seeing right now.
That is your memory, now act on it.
I think that's going to be potentially very powerful,
because in part, image is much richer to some extent.
It conveys a lot more.
I do come from natural language processing background.
At least that's my other half of my sort of academic background.
So I have bias towards believing that natural language is
profound, and I think that will be the case going forward
as well.
But image does offer something that just language alone
does not.
So image is going to be a big thing.
Now, imagine the future video is going to be a big thing
as well, then gradually, these agents
will basically increasingly get more powerful
as these new modality gets piled on.
So that's something that we should be looking forward to.
That's great.
What are some on the downside, what
are some of the limitations that you're
seeing in terms of these agents, especially generative agents?
Right.
So there are limitations that I can mention just about
in the context of our work.
And then I think there are going to be interesting limitations
that are much more application-specific.
So for generative agents today, certainly the technical limitation
right now might have to do with things like,
so you're using whether it's an open source.
So right now, we use OpenAI's model.
OpenAI has actually done a lot of work
to make the model safer.
And I think OpenAI, I think that was the right approach
in the sense that what they really wanted to create
was these chatbots or agents or chatGPT
that was a safe tool to use for most people.
Now, if you want to run a simulation
or create truly accurate and believable agents with something
like chatGPT, however, that could become a limitation.
Because what we really experience as humans is we fight.
We sometimes have conflicts.
We disagree with each other.
And that might not be something that's something
like chatGPT that's been fine-tuned to not behave that way
to remain safe.
It might not be something that these models was trying to surface.
And that could be a potential block
in creating more accurate, more believable simulations
or agents for that matter.
So that's something that is one limitation right now
that we're seeing.
An interesting way to tackle this, I think, going forward
is to use open-source models or other models that
have less of these fine-tuned nature.
But it's going to be highly dependent on the models that
we'll be using for this.
So I think that's one thing to look forward to.
Got it, got it.
That's super helpful.
Maybe one last question on the research side.
When you think about future areas to explore for you
specifically, what are some of the more narrow topics
that you're hoping to dive deeper in, given the world?
So ultimately, I think making the agents more accurate
for these agents to be more accurate reflection of who we are,
I think it's going to be a really interesting research.
And I think that's going to be an area that's
going to have more of a research and broader impact.
So right now, you may have seen the simulation demo.
The agents that live in that simulation are fictional.
For instance, we have an agent named Isabella.
We told Isabella that she is a cafe owner.
And Lawrence Language Model basically
makes up what a persona that is reasonable,
given that description.
But I think it's going to be far more interesting
if we can make these simulations actually closely model
our actual human communities.
So it's not just fictional, but actually has groundings.
That's going to open up, from our perspective,
an entirely new set of application spaces,
as well as research impact.
This can be used, for instance, to actually model or predict
markets, or it's going to be able to use two more closely
personalized many of these agents for individual use cases.
So that's something that we're looking forward to in terms
of a particular topic that we're diving into.
That plug, of course, scaling up the agents.
I think that's another big one, but those two.
That makes sense.
One of the things that's missing in most AI technologies
is really the emotional part of how humans feel.
All of that data is largely not captured,
and therefore not part of any kind of models today.
Language is one small output of what we have.
It's a very important output, for sure.
But it's still one small output.
I wonder how we might be able to incorporate
some of the data around our emotions.
I agree.
Some thoughts in the future.
Maybe let's move on to the applications today,
since you talked about some of the challenges for agents.
Many organizations are thinking about how to use large language
models today.
There's a huge amount of aspirations.
A subset of them are also thinking
about what are some of the agent technology applications
are viable within an enterprise, which
has limitations around infrastructure,
around data silos, security, and all that stuff.
Any particular areas where you've seen companies
be successful at using these technologies in production?
Right.
So I think this is going to be incredibly case-by-case answer.
So let me think.
Or if not, like any hypothesis as to where
you might see the first commercial deployments at scale?
Right.
So there is something that I have.
There's a message that I have in trying to communicate,
I think, in different settings.
So this is not something I sort of
convinced for the first time.
And my opinion has been getting updated,
but I think fundamentally I think this is right.
So the way I've been describing it is,
in human-computer interaction or in most task settings,
there are two types of problems that we deploy our machines
or agents in.
One has very hard-edge problem spaces.
These are things like, hey, order me pizza
or buy me a plane ticket.
These are tasks where there is a very concrete outcome,
and there often is a right or wrong answer
that the agent has to achieve.
At least from the user's perspective,
there is something that would absolutely be yes or no.
And then there are problems spaces
where we have soft-edge problems.
These are problems where we can increasingly
hope climb toward sort of being better,
but at certain level, it starts to actually become useful.
So to make this intuition a little bit more intuitive here,
for instance, if I guess the worst possible case scenario is,
I asked the agent to buy me a plane ticket,
and it bought me the wrong ticket that just
goes to a different place, then that's like a heavy no.
Whereas, let's say I asked the agent to sort of simulate
a behavior that is sort of fun so that when I'm in a game,
this is sort of entertaining and interesting,
and that might be something that the agent doesn't
need to be quite perfect in, but it can still
get there quite quickly.
And then we can gradually improve.
Those two are the spaces that we can sort of,
in terms of when we consider where
to deploy or how these will actually make its first impact,
we might actually be looking at those problem spaces first.
And these are the classes that I'm seeing.
If I were to make my bet, agents will likely succeed first
in the soft-edge problem spaces.
And we'll gradually inch into making it work
in the hardest problem space.
This has been sort of an intuition with agent research
community for some time.
So when Clippy, for instance, failed,
our intuition there, at least from research perspective,
wasn't that these agents failed because we
didn't have the technology there.
Certainly, these were deployed, and there were some confidence
around the technology.
But the problem there actually was with interaction,
that when agents are deployed in hard-edge problem spaces,
it's often deployed in states where it actually
has to have a fairly long chain of steps and fairly high.
The risk were fairly high.
When it fails, the cost of correcting its error
is actually quite high.
Cost of auditing its error is actually quite high.
So when these agents are deployed in hard-edge problem
spaces, it has to reckon with the fact
that it will undoubtedly make mistakes.
And when it does make a mistake, it
has to be increasingly auditable and controllable
by the users so that the cost of correcting its error
is not high enough that from the user's perspective,
the cost benefit analysis basically has to make sense.
And that's been a fundamental challenge with agents.
That's why in every era, we see the interest around agents
spike for a while, and then it quickly subsides after maybe
a half a year or two years.
Now, there's a real issue now, though,
that given the large-language model in the progress we saw,
that this might not be the case this time,
or at some point, we might be able to make this work.
But for now, so I'm closely monitoring this as well,
and I think we all should.
I don't think we should just go and say,
because it didn't work before, it's not
going to work this time.
But my hunch is that we will likely
see a very similar pattern arise,
at least for the first-ever future,
and we haven't quite dealt with the interaction problems
with those types of agents.
So I think it's much safer to assume
that it is going to be in the soft-edge problem spaces.
And that's why in some areas, in many of the aspects,
that's why our team was also interested in this idea
of simulation.
Because simulation is sort of the prime example
of soft-edge problem spaces, where the simulation has
to be good enough for it to start being useful.
That's also why I think a lot of really early promising AI
startups that's going to go in the agent space
are places that does NPCs for games,
because those are very safe soft-edge problem spaces,
where the agents can fail, but that's OK.
And gradually, we'll sort of go to the other area as well.
But I think that's where the impact is
going to start in the next couple of years.
I also think just seeing the startups in this space,
in sectors and functions that allow for failure,
like you said, include things like marketing,
where if you market incorrectly, it's not that big of a deal.
If you write the wrong code, that's probably
going to be a bigger deal.
If you pick the wrong security features, that's a huge deal.
If you pick the wrong things for health care,
that's an even bigger deal.
So there are degrees of fault tolerance
within the enterprise.
That's one thing.
And the second on the consumer side,
especially if the agents are just assisting consumers
by not executing on anything, that could probably also work.
For example, there's a company called Rewind,
which is using some of the agent technologies, I believe.
And they're getting a bunch of consumer demand.
But what consumers are doing is just searching for a behavior
that they have had before.
And this product is helping them do that,
versus doing anything real world, really.
Interesting.
But that's super.
The way that you frame it is very useful.
What about just from an architecture standpoint,
large language models enabled by transformer architectures?
This is a whole different direction.
We're already seeing companies that are saying,
hey, transformers are not the most efficient.
Inference costs are very high.
Let's look at the next thing.
Have you spent much time thinking about that?
And if so, any impact to the work that you're doing?
Right.
So certainly the next model that we're going to be banking on,
I think that always is an important topic.
And that's something that I think we as a community always
tend to monitor.
Because I think you're right.
The transformer is not going to be the end model that
will be, hopefully.
I mean, knock on wood.
The hope here is that we wouldn't be using
transformer 10 years down the line.
But one way that we do view this is this is very much
like a programmer's way of looking at this.
We view this in abstractions.
So what transformer has gotten us right now
is this amazing capacity for reasoning and processing
information and generating information.
So it might be the case that in the future,
that task will be done by even better models.
And hopefully that's going to be the case.
But for the sake of building applications,
it is true that we can sort of view this
as a layer of abstraction, that there
might be some other technology that's
going to be powering it in the future.
But really what we are focusing on
is the capacity and the modality.
What kind of reasoning, using what modality,
can these technologies that exist today do?
And we're going to be building on top of it.
So I think that's sort of our way of looking at it.
In sort of a medium term, again, the next three to five years.
Now, if you're looking, because right now,
there are some promising architectures
that's going on that's sort of been created at the forefront
of, I would say, more machine learning and natural language
processing communities, that I'm personally
getting a little bit excited about.
But at the moment, those are still
in the very much in the research phase.
And can you share some examples of that?
Yeah, so I think there's one model that recently came out,
like Mamba by some folks.
Those are from Stanford folks.
So I think the author is now at CMU in Princeton,
all within sort of this community.
So that's one example of sort of a potentially promising
or interesting model.
And that's the one that I recently heard about
that I think is interesting to be looking at.
But these models, for them to be deployed at scale
in a commercial way, if we decide
to basically go with a model that's getting created today,
it will give us maybe two to five year timeline
before they can really take off.
Because Transformer is not, it is a relatively modern model,
but it really is how you look at sort of the timeline.
There's Transformer.
Seven-ish years?
Seven-ish years.
So I think hopefully, if we find something like this time,
maybe it's going to go much faster.
But it still took about seven-ish years for Chatchapiti
to really come out.
So it's not immediate.
Whereas a lot of interactions that we can build,
I think there's a lot that we can do today
to create really cool experiences.
So I think that's how we're looking at this.
There is a medium term.
This is where we are focused on the level of extraction,
that this is the capacity that we'll have.
And then maybe in the down the line five to 10-year term,
we can really be looking forward to some new models
that's going to make an impact.
That's great, that's great.
Cool, very cool.
Maybe more generally, if you just zoom out for a moment,
when you look at the ecosystem today,
what are some of the problems that you want to see solve?
We talked about multimodal a little bit.
We talked about new models, right,
after transformers that might come out.
What are some of the problems that you are most excited
about someone solving,
not necessarily you personally, but someone solving?
I sort of have two in mind.
And it's a little bit less of a,
this is a specific problem that I want someone to solve,
but it's more sort of questions that I have
that I think more of us should be thinking about.
And to some extent,
and this happens a lot with sort of the way
I do my research as well,
where I get inspired by big problems
or foundational problems that we had in previous decades,
because oftentimes there's a lot of insights
that we can learn from the past
as we sort of build on the future.
One, certainly I'm embedded into this agent space.
One is in the past, agents had its hype cycles, basically,
but it failed, that the hype cycle lasted
for a couple of years
and then people very quickly lost interest,
basically because it didn't quite deliver
on the promises that it had.
I think it's worth asking ourselves
why that was the case.
I think the opportunity this time is real,
but I also think the opportunity in the past
was also real to some aspect as well.
So just because the opportunity is real
and language model is really cool,
doesn't necessarily guarantee us,
at least from my perspective,
that we're going to,
that agent will finally be a thing that everyone will use.
I think there is a future where that will happen at some point.
I think it might even happen this cycle,
but I think it's really worth asking,
as a community, why did it fail in the past
so that we don't repeat those mistakes?
One sort of main thing I'm sort of curious about
that I don't think a lot of us are thinking about
is actually not the technology part, but the interaction.
How are these agents going to be used in what way?
Because ultimately that's where it really delivers value
to the end users.
And that's where agents in the past have failed.
That it was really cool technology,
but we didn't seriously ask ourselves,
is this something that people really need
and does the cost-benefit analysis of using these agents
and learning how to use them well,
really make sense for the broader user base?
So that's one.
And other one is sort of my,
it's a little bit of a hot take,
but it's also a shorter take,
which is we have large language models,
and I think these have made a huge impact already.
The number of users who use ChatGPT, that's incredible.
But I think it's sort of worth asking ourselves,
is that sort of quote unquote,
the killer applications that we were waiting for?
Because in many ways, ChatGPT,
or maybe it is, and I think if it is,
I think somebody should articulate this.
But ChatGPT does feel like it's a fairly simple
wrapper around our language model,
because that's what it is.
And OpenAI has done fantastic things to make it safer
and make it more useful by tuning,
I think what's really great.
But I think it's worth asking,
if there is actually the killer application,
why is it a killer application?
And the answer might actually come out
that maybe it actually isn't the killer application
that we were waiting for,
in which case what is going to be the killer application?
That's really going to add value
in a much more generalizable way.
That's a very abstract question.
It's for now, it's for me, it's just a hunch
that I think there's something to be asked about there.
And if I'm wrong, like I would also love to hear again,
somebody really say,
we already have this killer application,
maybe it's co-pilot, ChatGPT, and here's why.
But for now, this is a question
that I'm still asking myself.
That makes sense.
And thank you, thank you for sharing that.
What are some of your favorite AI apps today that you use?
I love ChatGPT, I use it every day.
ChatGPT did make a difference in my workflow.
So as sort of a researcher,
one of the main things I do is I program every day,
or at least most days, or I write or write papers.
So I do one of those two things.
ChatGPT is fantastic at both.
So it's, as all programmers sort of know,
we sometimes don't bother remembering
all the different functions or documentation.
It's very good at generating a lot of the code
when I have an idea, really impressive.
It's also quite a good editor.
So if I make grammar error in my sort of sentences,
ChatGPT will usually catch them for me.
It's simple and easy thing,
but it's good enough now
that it's actually making a difference in the workflow.
So I'd say ChatGPT for sure, by extension,
I think co-pilot will make a difference.
So it's sort of worth asking,
maybe going back to the question around killer application.
What is the definition of killer application?
I think it does, some people define it as
application that has more users.
And the fact of that I think always has to be the case,
that no killer application has no user,
killer application by default means the application
that will have the most number of users.
But I think there's more theoretical definition
to what a killer application is.
That implies a lot of users or the most number of users.
But for instance, if we look back to the prior era of PC,
a killer application that I mentioned was something
like Microsoft's Excel or this tabular data format,
the thing that would let us manipulate the tabular data.
So really the definition in a more theoretical sense
of killer application here
is there's new technology stack that is being developed,
there's a new file type that is getting generated,
then the killer application is the one
that would let us manipulate the file application, file type.
That's one theoretical definition that one could give,
at least that's sort of the definition
that I've been towing with.
I think it's an interesting one.
I don't think it's the only one,
but those are sort of ways that I'm looking at this.
That makes a lot of sense.
I also use chatGPT every single day.
It's been very helpful.
Everything from coming up with menu names
to rewriting emails that don't sound as nice.
And I've tried a little bit to give it files and images.
Oh, I actually helped my mother create a background.
She's a dancer, so she was performing
in one of the very specific background for her dance.
And I created that for her using chatGPT.
So all sorts of utility there.
But I love the way that you framed
the last potential application.
Maybe just one last question from my end.
Any resources or books that you love
that is on this topic?
On this topic?
Right.
I do think, and this is often the case
with many of the cutting edge spaces.
I think a lot of the papers that are coming out
that are gaining a lot of attention,
I think those are sort of worth checking out
as sort of the resources.
It's not exactly like here's one book
that we can all look at.
But things are moving fast enough
that I think those are sort of interesting resources
or just things that are getting created today
and they're documentations.
So those are sort of mentioned as sort of a generic answer.
I do think, I think this has been a sort of running thing
in some of the things I mentioned today.
I get inspired by insights
that basically had impact that stood the test of time.
And the reason why that is the case is
because I personally think all the great ideas
are sort of timeless.
It's because current hype cycle is over.
It doesn't mean they're less interesting
or less meaningful.
For sure.
That are foundational ideas
that will continue to have impact.
So when I look for resources,
I actually look back to books
from truly the prior generations.
So some of the works that I often go back to
are works by Herbert Simon, Allen Newell.
Those are founders of AI and many of these fields
who would later go into winning the Turning Award
and Nobel Prize and so forth.
And those works,
early cognitive psychologists and scientists
inspired my work a lot and their textbook.
Those people actually have written books
because they were much more established
than sort of the cutting edge spaces today.
So I go back to those as sort of my personal resources
for getting ideas.
That's great.
Thank you so much.
This was super, super helpful to me personally
because what we do as investors
is we try to understand the impacts of technology
and start to invest in companies
when it becomes,
at the beginning of when it becomes commercially viable.
Right.
So to your point around what are the problem spaces,
what are the applications in which this can be applied
in a cost-effective and secure way
that where the end user is willing to interact
and get value,
that's when we start to come in
and invest in these companies,
which will hopefully be much bigger companies in the future.
So really appreciate this chat.
Yeah, it was fun.

Well, everybody, great to be with you again. My name is Michael Millerman, and today we're
going to go over this article called Why AI Will Save the World? Reading it together
with you for the first time, although the author here was on the Lex Friedman show
recently, and it was a clip from their interview I saw that referred to this article and intrigued
me enough to want to read it together with you. So in just a minute, we'll do that. Like,
share, subscribe, visit millermanschool.com for courses on philosophy and politics. More
and more of my students are talking to me these days about the implications of artificial
intelligence for sovereignty and for the common good for our basic ideas of political order.
And therefore, it's a good idea for us to get more into the details of how some people
are speculating about the political and human significance of these new systems. Okay. So
let's begin. The era of artificial intelligence is here and boy are people freaking out. Fortunately,
I'm here to bring the good news. AI will not destroy the world and in fact, may save it.
First, the short description of what AI is. The application of mathematics and software
code to teach computers how to understand, synthesize and generate knowledge in ways
similar to how people do it. AI is a computer program like any other. It runs, takes input,
processes and generates output. AI's output is useful across a wide range of fields,
ranging from coding to medicine to law to the creative arts. It is owned by people and controlled
by people like any other technology. So you see very quickly and early on here, guarding against
the idea that there's something autonomous or sovereign or tyrannical about AI. It's a tool
and like any other tool, it works in the hands of people. He says, let's see a shorter description
of what AI isn't killer software and robots that will spring to life and decide to murder
the human race or otherwise ruin everything like you see in the movies. And even shorter
description of what AI could be a way to make everything we care about better. Okay, so it's
not going to be a runaway killer technology, but it might be something that we use to make the world
a better place. Why AI can make everything we care about better. The most validated core conclusion
of social science across many decades and thousands of studies is that human intelligence
makes a very broad range of life outcomes better. Smarter people have better outcomes in almost
every domain of activity, academic achievement, job performance, occupational status, income,
creativity, physical health, longevity, learning new skills, managing complex tasks, leadership,
entrepreneurial success, conflict resolution, reading comprehension, financial decision making,
understanding others perspectives, creative arts, parenting outcomes, and life satisfaction.
Further, human intelligence is the lever that we have used for millennia to create the world
we live in today. Science, technology, math, physics, chemistry, medicine, energy, construction,
transportation, communication, art, music, culture, philosophy, ethics, morality. Without the
application of intelligence on all these domains, we would all still be living in mud huts, scratching
out a meager existence of subsistence farming. Instead, we have used our intelligence to raise
our standard of living on the order of 10,000 times over the last 4,000 years.
What AI offers us is the opportunity to profoundly augment human intelligence, to make all of these
outcomes of intelligence and many others from the creation of new medicines to ways to solve climate
change to technologies to reach the stars, much, much better from here. AI augmentation of human
intelligence has already started. AI is already around us in the form of computer control systems
of many kinds and is now rapidly escalating with AI large language models like chat GPT
and will accelerate very quickly from here if we let it. In our new era of AI, and here is I guess
the vision of what it could be if it is developed to be helpful to humanity as a tool that augments
our intelligence and proves our quality of life. In our new era of AI, every child will have an AI
tutor that is infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely
helpful. I should add though, not if it's been trained on human experience in that case, I think
it will get won't have these characteristics, certainly won't have any basis on which to model
them. Okay, infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely
helpful. The AI tutor will be by each child's side, every step of their development, helping
them maximize their potential with the machine version of infinite love. I have to think about
that because I have a have some kids of my own. And one of them right now is with his grandma.
And they're doing complicated math, you know, Russian, Soviet Jewish grandma type. So, of course,
the child is doing math, well, beyond his ears. And I wonder, you know, it's under the pressure
somehow, well, when he doesn't crack under the pressure of something other than infinitely
compassionate, infinitely patient, infinitely helpful, infinite love, he develops his mathematical
skills. And I've seen kids before in swim school, and they train with a teacher who's not infinitely
patient and all of that, but rather, whose strictness somehow is correlated with their
development. So I just wonder, you know, I have some experience of human tutors and human teachers
producing excellent results in their students. And you just wonder, well, infinite love,
and infinite compassion really produce the best outcomes can't doesn't go without saying, but
it does sound good. Okay, next, every person will have an AI assistant coach, mentor, trainer,
advisor, therapist, that is infinitely patient, infinitely compassionate, infinitely knowledgeable
and infinitely helpful. The AI assistant will be present through all of life's opportunities and
challenges, maximizing every person's outcomes. How does that sound to you? Does that sound
plausible and AI assistant that can help you with everything is super compassionate, knows you very
well. And you have any questions about anything, what should I do in my job? What should I do with
my wife? What should I do with my friends? What should I do with my business? And the AI is going
to maximize your outcomes through all of your opportunities and challenges because of its
infinite capacities. It's going to be like a little pocket Jesus there with you all the time to help
you in everything that you are doing. Every scientist will have an AI assistant collaborator
partner that will greatly expand their scope of scientific research and achievement. Every artist,
every engineer, every business person, every doctor, every caregiver will have the same in their worlds.
Every leader of a people. Okay, obviously, this is a strong pitch for the fact that everybody on
earth can benefit from AI. But let's see, nevertheless, how it's spun out. Every leader of people, CEO,
government official, nonprofit, president, athletic coach, teacher will have the same.
The magnification effects of better decisions by leaders across the people they lead are
enormous. So this intelligence augmentation may be the most important of all.
You know, this is as somebody who studies and teaches the history of political philosophy
and political theory. It is the question of the relationship of artificial intelligence systems
to governance and to ideas about ruling that interest me and that students of mine bring up
to me in discussions, more so than, for example, the significance of AI systems for
artists and engineers and so on. So this is a interesting field here, how AI systems would
improve or could improve or whether they would have a detrimental effect on rule.
Okay, productivity growth throughout the economy will accelerate dramatically driving economic
growth, creation of new industries, creation of new jobs and wage growth, and resulting in a new
era of heightened material prosperity across the planet. Sounds optimistic. Obviously is optimistic,
questions whether it's accurate and plausible. Scientific breakthroughs and new technologies
and medicines will dramatically expand as AI helps us further decode the laws of nature and harvest
them for our benefit. The creative arts will enter a golden age as AI augmented artists,
musicians, writers and filmmakers gain the ability to realize their visions far faster
and at greater scale than ever before. Again, I'm not sure that speed and scale are the right
criterion for the status or the level of development of the creative arts, do you?
Like, I don't know. When you think about the golden age of architecture or something like
that or the golden age of music, the golden age of painting, the golden age of film,
is it going to be better just because it's faster and at greater scale? Everybody can
just plug in their, I don't know, they had a strange dream at night, they plug it into their
AI system and the next day they have what, a 3D printed, who knows? What? So I wonder, okay,
but fine, there are smaller scale creative arts that might benefit. Listen, when I was younger,
I used to be a big fan of Oasis, the rock and roll band. And recently I've heard AI augmented
artists write songs that they have the lead singer of Oasis, Liam Gallagher, his voice on,
even though he didn't sing it. So AI augmented music, where you can hear an artist you like
singing a song that he didn't write, that you wrote for him or something like that. And frankly,
I thought that was pretty good. I enjoyed it. So there is something there that can benefit music
and I assume also these other artistic disciplines too. I even think he continues AI is going to
improve warfare when it has to happen by reducing wartime death rates dramatically.
Every war is characterized by terrible decisions made under intense pressure and with sharply
limited information by very limited human leaders. Now military commanders and political leaders
will have AI advisors that will help them make much better strategic and tactical decisions
minimizing risk, error and unnecessary bloodshed. Okay, but what if the AI says you have to be
more brutal than you were planning to be? In short, anything that people do with their
natural intelligence today can be done much better with AI. And we will be able to take on,
by the way, the assumption there obviously is that anything, you know, more intelligence means
you can do things better. More natural intelligence means you can do things better. And if you augment
your natural intelligence also means you can do things better. We will be able to take on new
challenges that have been impossible to tackle without AI from curing all diseases to achieving
interstellar travel. Sounds like promising the moon in my opinion, but okay. And this isn't just
about intelligence, perhaps the most underestimated quality of AI is how humanizing it can be. AI art
gives people who otherwise lack technical skills the freedom to create and share their artistic
ideas. Talking to an empathetic AI friend really does improve their ability to handle adversity.
You guys probably know or you may know the movie her speaking of an empathetic AI friend.
And AI medical chatbots are already more empathetic than their human counterparts.
Rather than making the world harsher and more mechanistic, infinitely patient and sympathetic
AI will make the world warmer and nicer. Okay, can you guarantee I stop here to ask many questions?
You may have questions of your own, but can you guarantee that the AI is going to be
infinitely patient and sympathetic? Can you ensure at the outset that it's going to want
to make the world warmer and nicer? That it won't be unintended consequences and unexpected
suggestions and all the other kinds of things that could surprise somebody here if they're dealing with
an intelligence system that's more intelligent than any human intelligence.
It's kind of like, you know, people in theology and philosophy of religion
who say there's a kind of similarity between the intelligence of man and God's intelligence,
but it's not a similarity that you can really understand or exploit. And it's almost even
blasphemous to think about because God is so much superior to man. Why should there be any
part of us that coincides with or coalesces with something so qualitatively superior?
So you can't really even understand it's only in some strange and analogical form that you can
say human intelligence and divine intelligence coalesce at any point. Well, okay, fine. If these
artificial intelligence systems really reach whatever they're speculating in level of IQ
in capacity, why do we assume it will have any similarity to the way that we think?
You know, maybe it will recognize, you know what, the world doesn't need to be warmer and nicer.
That's a low IQ thought that the world should be warm and nice.
You know what I mean? Like, how do you know when you by the time you get to whatever 220 IQ or
wherever these things go, if the systems continue to advance? It's like, when I create an amazing
AI chess player is going to play chess exactly the same way as I do, but a little bit better,
or it's going to play go exactly the same way that I do, but better. No, it might find a completely
new way of playing the game, destroy you every time. So a lot of assumptions built in here about
the relationship between like will intelligence become qualitatively different as it becomes
quantitatively superior. The stakes here are high. The opportunities are profound.
AI is quite possibly the most important and best thing our civilization has ever created,
certainly on par with electricity and microchips and probably beyond those.
The development and proliferation of AI far from a risk that we should fear is a moral
obligation that we have to ourselves, to our children, into our future. We should be living
in a much better world with AI and now we can. Okay. So there's the strong first part of the paper,
strong case for the idea that AI will make the world a better place in all respects,
in the arts, in the sciences, in medicine, in government, a golden age of humanity,
because AI will make everything we care about better. But as I'm sure you know, there are some
people who don't think so and who are worried and who are asking to pause the development of AI and
so on. So he naturally transitions to the question, why the panic in contrast to this positive view,
the public conversation about AI is presently shot through with hysterical fear and paranoia.
We hear claims that AI will variously kill us all, ruin our society, take all our jobs,
cause crippling inequality and enable bad people to do awful things. What explains this divergence
of potential outcomes from near utopia to horrifying dystopia? Historically, every new
technology that matters from electric lighting to automobiles to radio to the internet has sparked
a moral panic, a social contagion that convinces people the new technology is going to destroy
the world or society or both. The fine folks at pessimists archive have documented these
technology driven moral panics over the decades. Their history makes the pattern vividly clear.
It turns out this present panic is not even the first for AI. Now it is certainly the case that
many new technologies have led to bad outcomes, often the same technologies that have been otherwise
enormously beneficial to our welfare. So it's not that the mere existence of a moral panic means
there's nothing to be concerned about. But a moral panic is by its very nature irrational. It takes
what may be a legitimate concern and inflates it into a level of hysteria that ironically makes
it harder to confront actually serious concerns. I want to say here, not that I, not that this is
the last word, but you know, a moral panic may be irrational and may take what's legitimate
and inflate it. But so too is a kind of zealous over optimism and over optimism that takes what
may be a legitimate hope and inflates it into a level of not hysteria, but, you know, euphoria
or mania that makes it harder to confront actually serious matters. So there's a,
there's a flip side to this, the irrationality of moral panic, but also the, the possibly the
irrationality of over optimistic, over zealous hopefulness that we shouldn't forget. Okay. And
wow, do we have a full blown moral panic about AI right now? Let's just see where does this link to?
The AI arms race is changing everything. Time magazine, February 17th, 2023. Okay,
maybe we'll look at that later. The moral panic is already being used as a motivating force by
a variety of actors to demand policy action, new AI restrictions, regulations and laws.
These actors who are making extremely dramatic public statements about the dangers of AI,
feeding on and further inflaming moral panic, all present themselves as selfless champions of the
public good. But are they and are they right or wrong? You see, always shows you just how important
whatever technological era we are in, so much of politics is a debate over what's good over the
public good over the common good. Is AI going to contribute to the common good, the public good,
or is AI going to be detrimental to the common good? Well, for sure, we have to look into the
arguments like he's about to do. And then we also have to have a broad understanding of what is meant
by the public good. What is good politically? What is the good society? What's the good human life?
And those questions are old questions in the history of political philosophy from Plato's
Republic to Aristotle's politics all the way down to Machiavelli and Nietzsche and so on.
So if you ever want to go to millermanschool.com and you'll see I have courses on the history
of political philosophy. But right now we continue with this, the Baptists and bootleggers of AI.
Economists have observed a long standing pattern in reform movements of this kind.
The actors within movements like these fall into two categories, Baptists and bootleggers,
drawing on the historical example of the prohibition of alcohol in the United States in the 1920s.
Baptists are the true believer social reformers who legitimately feel deeply and emotionally,
if not rationally, that new restrictions, regulations and laws are required to prevent
societal disaster. For alcohol prohibition, these actors were often literally devout Christians
who felt that alcohol was destroying the moral fabric of society. For AI risk, these actors are
true believers that AI presents one or another existential risks, strap them to a polygraph,
they really mean it. So these are the AI skeptics are the ones who want to deprive you of the
pleasures of alcohol. In fact, they're going to be the prohibitionists. Bootleggers are the
self-interested opportunists who stand to financially profit by the imposition of new
restrictions, regulations and laws that insulate them from competitors. For alcohol prohibition,
these were the literal bootleggers who made a fortune selling illicit alcohol to Americans
when legitimate alcohol sales were banned. For AI risk, these are CEOs who stand to make more
money. If regulatory barriers are erected that form a cartel of government-blessed AI vendors
protected from new startup and open source competition, the software version of two big
to fail banks. A cynic would suggest that some of the apparent Baptists are also bootleggers,
specifically the ones paid to attack AI by their universities, think tanks,
activist groups and media outlets. If you are paid a salary or receive grants to foster AI panic,
you're probably a bootlegger. In other words, you benefit from the restriction of AI.
The problem with bootleggers is that they win. The Baptists are naive ideologues. The bootleggers
are cynical operators. And so the result of reform movements like these is often that bootleggers
get what they want. Regulatory capture, insulation from competition, the formation of a cartel.
And the Baptists are left wondering where their drive for social improvement went so wrong.
We've just lived through a stunning example of this. Banking reform after the 2008 global
financial crisis. The Baptists told us that we needed new laws and regulations to break up
the two big to fail banks to prevent such a crisis from ever happening again.
So Congress passed the Dodd-Frank Act of 2010, which was marketed as satisfying the Baptist
goal, but in reality was co-opted by the bootleggers, the big banks. The result is that the same
banks that were too big to fail in 2008 are much, much larger now. So in practice, even when the
Baptists are genuine and even when the Baptists are right, they are used as cover by manipulative
and venal bootleggers that benefit themselves. And this is what is happening in the drive
for AI regulation right now. However, it isn't sufficient to simply identify the actors and
impugn their motives, like he's just done in this section. We should consider the arguments of both
the Baptists and the bootleggers on their merits. So now he's going to turn to AI risk. Number one,
will AI kill us all? Hope not. We're reading this article by Mark and Andreessen called
Why AI Will Save the World. This was published not too long ago, I think just earlier this month.
And I saw it come up very briefly on my YouTube feed today, a clip of somebody else discussing
it. And I thought, you know, I haven't read it. It looks pretty intriguing. Let's go over it. So
thanks for being here. Like, share, subscribe, and all the rest of it. Feel free to be active in
the chat if you have thoughts on these topics. And we're going to continue on once I find our spot
here. Here we go. Will AI kill us all? The first and original AI doomer risk is that AI will decide
to literally kill humanity. The fear that technology of our own creation will rise up and destroy us
is deeply coded into our culture. The Greeks expressed this fear in the Prometheus myth.
Prometheus brought the destructive power of fire and more generally technology,
technique, to man, for which Prometheus was condemned to perpetual torture by the gods.
Later, Mary Shelley gave us moderns, our own version of this myth in her novel Frankenstein,
or the modern Prometheus, in which we developed the technology for eternal life,
which then rises up and seeks to destroy us. And of course, no AI-panicked newspaper story is
complete without a still image of a gleaming red-eyed killer robot from James Cameron's Terminator films.
The presumed evolutionary purpose of this mythology is to motivate us to seriously
consider potential risks of new technologies. Fire, after all, can indeed be used to burn down
entire cities. But just as fire was also the foundation of modern civilization,
as used to keep us warm and safe and cold and hostile world, this mythology ignores the far
greater upside of most or all new technologies and in practice inflames destructive emotion
rather than reasoned analysis. Just because pre-modern man, freaked out like this,
doesn't mean we have to. We can apply rationality instead.
My view is that the idea that AI will decide to literally kill humanity is a profound category
error. AI is not a living being that has been primed by billions of years of evolution
to participate in the battle for the survival of the fittest as animals are and as we are.
It is math, code, computers, built by people, owned by people, used by people, controlled by
people. The idea that it will at some point develop a mind of its own and decide that it has
motivations that lead it to try to kill us is a superstitious hand wave. I think largely from
what I've heard anyway, this is somehow the crux of the debate. Will a superintelligence develop
some sense of itself and of its own motivations? Will it, you know, become unleashed? Or will it
always be a tool in the hands of the people who are using it? And even as a tool, you know,
it may have become a powerful tool of destruction. But anyway, in short, AI doesn't want,
it doesn't have goals, it doesn't want to kill you because it's not alive.
And AI is a machine. It's not going to come alive any more than your toaster will.
What do you guys think about that? Seems slightly disingenuous because on one hand
he's saying that AI is a technology that's completely unprecedented that can help improve
every aspect of life that's going to augment human intelligence in every domain. Nobody
thinks that a toaster will do those things. Now obviously, there are true believers in killer AI,
Baptists, who are gaining a suddenly stratospheric amount of media coverage for their terrifying
warnings, some of whom claim to have been studying the topic for decades and say that they are now
scared out of their minds by what they have learned. Some of these true believers are even
actual innovators of the technology. These actors are arguing for a variety of bizarre
and extreme restrictions on AI, ranging from a ban on AI development all the way up to military
airstrikes on data centers and nuclear war. Well, that's kind of crazy. I didn't hear about that.
What is this? Pausing AI developments isn't enough. We need to shut it all down.
Yadkowski is a decision theorist from the US, leads research at the Machine Intelligence
Research Institute. Did he call for nuclear for military strikes on data centers? That's kind
of, I'll have to look at that later. Let's go back here. They argue that because people like me
cannot rule out future catastrophic consequences of AI, catastrophic consequences of AI, that we
must assume a precautionary stance that may require large amounts of physical violence and death
in order to prevent potential existential risk. My response, Andreessen writes,
is that their position is non-scientific. What is the testable hypothesis? What would
falsify the hypothesis? How do we know when we're getting into a danger zone? These questions go
mainly unanswered apart from, you can't prove it, won't happen. In fact, these Baptist's position
is so non-scientific and so extreme, a conspiracy theory about math and code, and is already calling
for physical violence that I will do something I would normally not do and question their motives
as well. Okay, I'm going to continue in a minute, but this I don't like. This I don't like.
You can't, how could you put it? You can't deflate the concerns by presenting AI as merely math and
code, which makes it sound kind of banal, and therefore makes a conspiracy theory about a
seem kind of ridiculous. And at the same time, say that this math and code will improve every
aspect of human life in unprecedented ways that are truly infinitely loving and infinitely
compassionate. So how can math and code be infinitely patient, infinitely knowledgeable,
infinitely helpful, helpful, helpful, a machine version of infinite love? So when you're talking
about the positive side, you can really exploit an anthropomorphized version of math and code.
But when it comes to the concerns, you said, come on, don't be a conspiracy theorist, it's just math.
Math can't hurt you. You know what I'm saying? I don't like that. I don't know whether he understands
the sort of duplicity here, which should be fine if he does or doesn't, doesn't matter. Still an
interesting article. We're going through it, but you can't have it both ways. If it's just math,
and it can be infinitely loving, then it's not ridiculous to wonder whether it can also have
other less desirable emotional characteristics. Okay, where were we? Sorry about that one second.
I think we were. We're still up here, right? Okay, wait.
Yeah, here we go. Okay. So what's going on, he says? What's going on? Specifically,
I think three things are going on. First, recall that John von Neumann responded to Robert Oppenheimer's
famous hand-ringing about his role creating nuclear weapons, which helped end World War
II and prevent World War III with, quote, some people confess guilt to claim credit for the sin,
unquote, which is a nice line. What is the most dramatic way one can claim credit for the importance
of one's work without sounding overly boastful? This explains the mismatch between the words
and actions of the Baptist who are actually building and funding AI, watch their actions,
not their words. Truman was harsher after meeting with Oppenheimer. Don't let that crybaby in here
again. Second, some of the Baptists, in other words, some of the people who are warning about
the dangers of AI are actually bootleggers, meaning those who are going to profit from its
restriction. There's a whole profession of AI safety expert, AI ethicist, AI risk researcher.
They're paid to be doomers and their statements should be processed appropriately.
Third, California is justifiably famous for our many thousands of cults from Est,
incidentally, interestingly influenced by Heidegger, to the People's Temple,
from Heaven's Gate to the Manson Family. Many, although not all of these cults are harmless and
maybe even serve a purpose for alienated people who find homes in them, but some are very dangerous
indeed in cults having notoriously hard times straddling the line that ultimately leads to
violence and death. And the reality, which is obvious to everyone in the Bay Area,
but probably not outside of it, is that AI risk has developed into a cult, which has suddenly
emerged into the daylight of global press attention and the public conversation. The
cult has pulled in not just fringe characters, but also some actual industry experts and not a
small number of wealthy donors, including until recently Sam Bankman Freed, and it's developed
a full panoply of cult behaviors and beliefs. This cult is why there are a set of AI risk
doomers who sound so extreme. It's not that they actually have secret knowledge that make
their extremism logical. It's that they've whipped themselves into a frenzy and really are
extremely extreme. It turns out that this type of cult isn't new. There's a long-standing western
tradition of millenarianism, which generates apocalypse cults. The AI risk cult has all
the hallmarks of a millenarian apocalypse cult from Wikipedia with additions by me, quote,
millenarianism is the belief by a group or movement, AI risk doomers, in a coming fundamental
transformation of society, the arrival of AI, after which all things will be changed, AI utopia,
dystopia, or end of the world. Only dramatic events, AI bands, airstrikes and data centers,
nuclear strikes on unregulated AI, are seen as able to change the world, prevent AI,
and the change is anticipated to be brought about or survived by a group of the devout and
dedicated. In most millenarian societies, excuse me scenarios, the disaster or battle to come AI
apocalypse or its prevention will be followed by a new purified world, AI bands, in which the
believers will be rewarded or at least acknowledged to have been correct all along.
This apocalypse cult pattern is so obvious that I'm surprised more people don't see it.
Don't get me wrong, cults are fun to hear about, their written material is often creative and
fascinating, and their members are engaging at dinner parties and on TV,
but their extreme beliefs should not determine the future laws, the future of laws in society,
obviously not. Okay, so to the first possible criticism of AI, the fear that it's going to kill
us all. Mark Andreessen writes that no, that is not founded in anything other than this fear of
new technologies, which may have an evolutionary purpose and which has precedence back in the myth
of Prometheus and the story of Frankenstein. But AI can't want to kill people because AI
doesn't want anything, doesn't have goals, isn't alive, and is not much more than a toaster in
terms of its intentions. And I already made my comments on that. So let's go to the next one,
will AI ruin our society? Okay, quick pause. This is an article by Mark Andreessen,
why AI will save the world published earlier this month that we're going through. Pretty
interesting so far. I hope you like it. You should comment for sure in the chat or comment on the
video if you're watching later. Like, share, subscribe. We definitely do live streams on
politics and philosophy and more and more, I think the arguments over the significance, the political
and social significance of these new technologies should be a part of our conversation because
it implicates us in questions of sovereignty and the good society, the meaning of a human life,
and all the rest of it. So it's a nice topic to include in our conversations.
Okay, will AI ruin our society? The second widely mooted AI risk is that AI
will ruin our society by generating outputs that would be so harmful to use the nomenclature of
this kind of doomer as to cause profound damage to humanity, even if we're not literally killed.
Short version, if the murder robots don't get us, the hate speech and misinformation will.
Okay, so I guess here's like you're going to have all kinds of fake chatbots and all kinds of propaganda
farms pushing out by the, you know, infinite stream of misinformation technologies.
This is a relatively recent doomer concern that branched off from and somewhat took over the AI
risk movement that I described above. In fact, the terminology of AI risk recently changed from
AI safety, the term used by people who are worried that AI would literally kill us,
to AI alignment, the term used by people who are worried about societal harms.
The original AI safety people are frustrated by this shift, although they don't know how
to put it back in the box. They now advocate that the actual AI risk topic be renamed AI
not kill everyoneism, which has not yet been widely adopted, but is at least clear.
The tip off to the nature of the AI societal risk claim is its own term AI alignment,
alignment with what? Human values. Whose human values? Ah, that's where things get tricky.
Okay, I'm interested to see where he takes us because remember what he said above?
I pointed out he was invoking some human values of his own infinite compassion,
infinite patience, infinite love. As it happens, I have had a front row seat to an
analogous situation, the social media trust and safety wars. As is now obvious, social media
services have been under massive pressure from governments and activists to ban, restrict,
censor, and otherwise suppress a wide range of content for many years. And the same concerns
of quote unquote eight speech and its mathematical counterpart algorithmic bias and misinformation
are being directly transferred from the social media context to the new frontier of AI alignment.
My big learnings he writes from the social media wars are on the one hand,
there is no absolutist free speech position. First, every country including the US makes
at least some content illegal. Second, there are certain kinds of content like child pornography
and incitements to real world violence that are nearly universally agreed to be off limits legal
or not by virtually every society. So any technological platform that facilitates or
generates content, speech is going to have some restrictions. On the other hand, the slippery
slope is not a fallacy. It's an inevitability. Once a framework for restricting even egregiously
terrible content is in place, for example, for hate speech, a specific hurtful word or for
misinformation, obviously false claims like the Pope is dead, a shockingly broad range of government
agencies and activist pressure groups and non governmental entities will kick into gear and
demand ever greater levels of censorship and suppression of whatever speech they view as
threatening to society and their own personal preferences. Just want to remind you on this
topic of censorship. We did a stream a couple of weeks ago, Alexander Dugan on the metaphysics
of censorship could be interesting to think about in relationship to AI alignment. They will do this
up to and including in ways that are nakedly felony crimes. The cycle and practice can run
apparently forever with the enthusiastic support of authoritarian hall monitors installed throughout
our elite power structures. This has been cascading for a decade in social media and with
only certain exceptions continues to get more fervent all the time. And so this is the dynamic
that is formed around AI alignment now. It's proponents claim the wisdom to engineer AI generated
speech and thought that are good for society and to ban AI generated speech and thoughts.
Sorry, that are bad for society. And to ban AI generated. Yeah, it's been generated. So basically
you permit the good stuff, you ban the bad stuff. It's opponents claim that the thought police are
breathtakingly arrogant and presumptuous and often outright criminal, at least in the US,
and in factors seeking to be kind to become a new kind of fused government corporate academic
authoritarian speech dictatorship ripped straight from the pages of Orwell's 1984.
Because I'm sure you have heard about this concern. In fact, Jordan Peterson, maybe a couple of days
ago or sometime last week posted on his Twitter. I think it was the UN maybe or some institution
like that that said they were now going to be having, you know, AI automated misinformation
censorship, something like that. So that's the concern. Okay, suddenly you're going to have
ideologically supercharged AI systems that are designed to police speech and label everything
they don't like hate speech. So like exactly what happens now, but augmented by so AI augmented
ideological censorship. As the proponents of both trust and safety and AI alignment
are clustered into the very narrow slice of the global population that characterizes the American
coastal elites, which includes many of the people who work in and right about the tech industry.
Many of my readers will find yourselves primed to argue that dramatic restrictions on AI output
are required to avoid destroying society. I will not attempt to talk you out of this. Now he writes,
I will simply state that this is the nature of the demand and that most people in the world
neither agree with your ideology nor want to see you win. We have here
America has a for you. What's he referring to? Well, it's behind the paywall. We'll look at it
another day. Okay, so most people in the world do not agree with the ideology of the American
coastal elites and don't want to see them win and therefore don't want to see an AI aligned with
their ideology, especially with their sensorious limitations on free speech. If you don't agree
with the prevailing niche morality that is being imposed on both social media and AI
via ever intensifying speech codes, you should also realize that the fight over what AI is allowed
to say or generate will be even more important by a lot than the fight over social media censorship.
AI is highly likely to be the control layer for everything in the world. How is it allowed?
How it is allowed to operate is going to matter perhaps more than anything else has ever mattered.
I don't know how you feel about it when you see statements like that seem a little bit on the
exaggerated side, but okay, I guess you have to consider it. You should be aware of how a small
and isolated coterie of partisan social engineers are trying to determine that right now undercover
of the age-old claim that they're protecting you. In short, don't let the thought police suppress AI.
Okay, so you had another argument here. AI is going to ruin our society because it's going to be
allowed to be a racist, homophobic and fascist, you know what I mean, right? It's going to be allowed
to say things that don't pass the test of political correctness or whatever, and that it should be,
it should be made to answer to the dogmas of the day in order to be safe. And he says no,
don't let the thought police suppress AI. Okay, AI risk number three, will AI take all our jobs?
Well, if you don't have a job, you're safe. The fear of job loss do variously to mechanization,
automation and computerization, or AI, has been a recurring panic for hundreds of years
since the original onset of machinery, such as the mechanical loom. Even though every new major
technology has led to more jobs at higher wages throughout history, each wave of this panic is
accompanied by claims that this time it is different. This is the time it will finally happen. This is
the technology that will finally deliver the hammer blow to human labor. And yet it never happens.
I'm going to interrupt the article just to point out that there's a flip side of this argument,
right? So he says every time there's a panic, the panic says this time it's different,
but it's always the same. But we should point out that the flip side is he's also saying this time
it's different on the optimistic side of things. Remember what he put here, right? He said,
how you're going to operate AI may matter more than anything else has ever mattered.
Okay, same sort of idea. This is the technology that's really going to be
so different from all the technologies that have come before, none of which could be infinitely
loving, infinitely compassionate, infinitely patient, and so on. Okay, so just keep in mind,
you want to make sure you don't lose sight of that. He's not allowed to overstate the benefits
and understate the harms. So you're just going to keep an eye on it. Okay, so we've been through two
such technology-driven unemployment panic cycles in our recent past, the outsourcing panic of the
2000s and the automation panic of the 2010s, notwithstanding many talking heads, pundits,
and even tech industry executives pounding the table throughout both decades that mass
unemployment was near. By late 2019, right before the onset of COVID, the world had more jobs
at higher wages than ever in history. Nevertheless, this mistaken idea will not die.
And sure enough, it's back. This time, we finally have the technology that's going to take all the
jobs and render human workers superfluous, real AI. Surely this time history won't repeat,
and AI will cause mass unemployment and not rapid economic job and wage growth, right?
No, that's not going to happen. And in fact, AI, if allowed to develop and proliferate throughout
the economy, may cause the most dramatic and sustained economic boom of all time,
with correspondingly record job and wage growth, the exact opposite of the fear. And here's why.
Okay, so good. We have the claim. It's going to destroy job growth.
And we have the counterclaim. No, it's not. It's going to lead to a boom.
Well, that's the claim. What's the argument? The core mistake the automation kills jobs
doomers keep making is called the lump of labor fallacy. This fallacy is the incorrect notion
that there's a fixed amount of labor to be done in the economy at any given time,
and either machines do it or people do it. And if machines do it, there will be no work for people
to do. The lump of labor fallacy flows naturally from naive intuition, but naive intuition here
is wrong. When technology is applied to production, we get productivity growth and increase in output
generated by a reduction in inputs. The result is lower prices for goods and services. As prices
for goods and services fall, we pay less for them, meaning that we now have extra spending power
with which to buy other things. This increases demand in the economy, which drives the creation
of new production, including new products and new industries, which then creates new jobs for
the people who are replaced by machines and prior jobs. The result is a larger economy
with higher material prosperity, more industries, more products and more jobs.
But the good news doesn't stop there. We also get higher wages. This is because at the level
of the individual worker, the marketplace sets compensation as a function of the marginal
productivity of the worker. A worker in a technology infused business will be more productive than a
worker in a traditional business. The employer will either pay that worker more money, as he's
now more productive, or another employer will purely out of self interest. The result is that
technology introduced into an industry generally not only increases the number of jobs in the
industry, but also increases wages. Okay, you guys see that technology is going to make you
marginally more productive and therefore is going to increase your wages if wages are related to
marginal productivity. To summarize, technology empowers people to be more productive. This
causes the prices for existing goods and services to fall and for wages to rise. This in turn causes
economic growth and job growth while motivating the creation of new jobs and new industries.
If a market economy is allowed to function normally, and if technology is allowed to be
introduced freely, this is a perpetual upward cycle that never ends. For as Milton Friedman
observed, human wants and needs are endless. Incidentally, also something that Aristotle
observed in the politics. We always want more than we have and Plato in the Republic. A technology
infused market economy is the way we get closer to delivering everything everyone could conceivably
want, but never all the way there. And that is why technology doesn't destroy jobs and never will.
Okay, continuing. These are such mind blowing ideas for people who have not been exposed to
them that it may take you some time to wrap your head around them, but I swear I'm not making them
up. In fact, you can read about them in standard economic textbooks. I recommend the chapter,
The Curse of Machinery in Henry Haslitz Economics in One Lesson and Frederick Bastiat's satirical
candle makers petition to blot out the sun due to its unfair competition with the lighting
industry here modernized for our times. But this time is different to your thinking. This
time with AI, we have the technology that can replace all human labor. By using the principles
I described above, he writes, think of what it would mean for literally all existing human labor
to be replaced by machines. It would mean a takeoff rate of economic productivity growth
that would be absolutely stratospheric, far beyond any historical precedent. Prices of
existing goods and services would drop across the board to virtually zero. Consumer welfare
would skyrocket. Consumer spending power would skyrocket. New demand in the economy would explode.
Entrepreneurs would create dizzying arrays of new industries, products and services,
and employ as many people and AI as they could as fast as possible to meet all the new demand.
Suppose AI once again replaces that labor, the cycle would repeat, etc. It would be a straight
spiral up to material utopia that neither Adam Smith nor Karl Marx ever dreamed of. We should be so
lucky. Okay, so let's just review where we are at in this article. Whoops, sorry. So first we had
the claim that AI is going to make everything we care about better, everything, because you're
going to have augmented intelligence and intelligence is such a lead indicator of where
you're going to be successful that augmented intelligence is going to make everything better.
Children, parents, leaders, scientists, artists, you name it. But most people are arguing that
AI is going to kill us all, take our jobs and do all kinds of other crazy things.
And he had some analysis of why that is in part because when you ask for the regulation of AI,
you may also benefit from that regulation like the bootleggers who made a fortune selling illicit
alcohol to Americans when legitimate alcohol sales were banned. But besides the economic
possible upside of arguing for regulation of AI, he also wanted to examine their arguments on their
merits. Will AI kill us all? We just discussed a moment ago. It won't because it doesn't want to
kill anybody doesn't want to do anything. It's just a tool in the hands of people. It has no
personality, no intention, no agency and no malice as it were. Then we have the idea that AI will
ruin our society because it's going to be freely allowed to say all kinds of things that mess up
our social conversation. Therefore it should be made to be politically correct in effect.
And he said, no, do not let the thought police suppress AI. The third risk, it's going to take
all our jobs. You said far from it, it's going to lead to a boom, more jobs, more higher paying
jobs, a golden age of economic prosperity. So we're at AI risk number four now. Will AI lead
to crippling inequality? Speaking of Karl Marx, the concern about AI taking jobs
segues, segues. How do you say that word? I think I was a segues. Okay. The concern about AI taking
jobs seags directly into the next claim AI risk or the next claimed AI risk, which is, okay,
Mark, suppose AI does take all the jobs either for bad or for good. Won't that result in massive
and crippling wealth inequality? As the owners of AI reap all the economic rewards and regular
people get nothing. By the way, once upon a time, I can't vouch for it because I don't remember it
well enough, but I read a book called, what's that guy's name? Georgiani, right? Georgiani.
The book was called Prometheism, I think. Prometheism, something like that. And it argues sort of
like this, that as technology develops, as it gets to the, and it should develop,
as the Promethean spirit develops and it should develop, it's going to produce massive social
inequality between the basically subhuman humanity will branch into a technologically
superhuman overlords and subhuman slaves. But okay, speaking of Karl Marx, yeah, okay,
the wealth inequality, as it happens, this was a central claim of Marxism that the owners of the
means of production, the bourgeoisie, would inevitably steal all societal wealth from the
people who do the actual work, the proletariat. This is another fallacy that simply will not die,
no matter how often it's disproved by reality, but let's drive a stake through its heart anyway.
The flaw in this theory is that as the owner of a piece of technology, it's not in your interest
to keep it to yourself, in fact, the opposite. It's in your own interest to sell it to as many
customers as possible. The largest market in the world for any product is the entire world,
all eight billion of us. And so in reality, every new technology, even ones that start
by selling to the rarefied air of high paying big companies or wealthy customers, rapidly proliferates
until it's in the hands of the largest possible mass market, ultimately everyone on the planet.
The classic example of this was Elon Musk's so-called secret plan, which he naturally published
openly for Tesla in 2006. Step one, build expensive sports car. Step two, use that money to build an
affordable car. Step three, use that money to build an even more affordable car, which is exactly
what he's done becoming the richest man in the world as a result. The last point is key. Would
Elon be even richer if he only sold cars to rich people today? No. Would he be even richer than
that if he only made cars for himself? Of course not. No, he maximizes his own profit by selling
through the largest possible market, the world. In short, everyone gets the thing as we saw in
the past with not just cars, but also electricity, radio, computers, the internet, mobile phones,
and search engines. The makers of such technologies are highly motivated to drive down their prices
until everyone on the planet can afford them. This is precisely what is already happening in AI.
It's why you can use state of the art generative AI, not just at low cost, but even for free today
in the form of Microsoft being in Google Bard. And it is what will continue to happen, not because
such vendors are foolish or generous, but precisely because they are greedy. They want to maximize
the size of their market, which maximizes their profits. I'm not going to lie. I can relate to
that millermanschool.com. If the whole world was my customer, you know, if everybody in the world
was taking the Plato course or the Heidegger course or the Dugan course, wouldn't necessarily be so
bad. What happens is the opposite of technology driving centralization of wealth, individual
customers of the technology, ultimately including everyone on the planet are empowered instead
and capture most of the generated value. As with prior technologies, the companies that
build AI, assuming they have to function in a free market will compete furiously to make this
happen. Marx was wrong then and he's wrong now. This is not to say that inequality is not an issue
in our society. It is. It's just not being driven by technology. It's being driven by the reverse
by the sectors of the economy that are the most resistant to new technology that have the most
government intervention to prevent the adoption of new technology like AI, specifically housing
education and healthcare. The actual risk of AI and inequality is not that AI will cause
more inequality, but rather that we will not allow AI to be used to reduce inequality.
Okay, so not only will it not lead to crippling inequality, it's somehow our best hope
for producing more genuine equality. Okay, or for reducing inequality. AI risk number five.
Let me just see how many more do we have to go through. So one more on the numbered list. Let's
have a look. Will AI lead to bad people doing bad things? So far, I've explained why four of the
five most often proposed risks of AI are not actually real. AI will not come to life and kill
us. AI will not ruin our society. AI will not cause mass unemployment and AI will not cause a
ruinous increase in inequality. But now let's address the fifth, the one I actually agree with.
AI will make it easier for bad people to do bad things. In some sense, this is a tautology.
Technology is a tool. Tools starting with fire and rocks can be used to do good things, cook food
and build houses and bad things, burn people and bludgeon people. Any technology can be used for
good or bad, fair enough. And AI will make it easier for criminals, terrorists and hostile
governments to do bad things. No question. This causes some people to propose. Well, in that
case, let's not take the risk. Let's ban AI now before this can happen. Unfortunately,
AI is not some esoteric physical matter that is hard to come by like plutonium. It's the
opposite. It's the easiest material in the world to come by math and code. The AI cat is obviously
already out of the bag. You can learn how to build AI from thousands of free online courses,
books, papers and videos. And there are outstanding open source implementations proliferating by the
day. AI is like air. It will be everywhere. The level of totalitarian oppression that would be
required to arrest that would be so draconian. A world government monitoring and controlling
all computers. Jack booted thugs and black helicopters, seizing rogue GPUs that we would
not have a society left to protect. So instead, there are two very straightforward ways to address
the risk of bad people doing bad things with AI. And these are precisely what we should focus on.
First, we have the laws on the books to criminalize most of the bad things that anyone is going to
do with AI. Hack into the Pentagon. That's a crime. steal money from a bank. That's a crime.
Create a bio weapon. That's a crime committed terrorist act. That's a crime. We can simply
focus on preventing those crimes when we can and prosecuting them when we cannot. We don't
even need new laws. I'm not aware of a single actual bad use for AI that's been proposed that's
not already illegal. And if a new bad use is identified, we ban that use. But you'll notice
what I slipped in there. I said we should focus first on preventing AI assisted crimes before
they happen. Wouldn't such prevention mean banning AI? Well, there's another way to prevent such
actions, and that's by using AI as a defensive tool. The same capabilities that make AI dangerous
in the hands of bad guys with bad goals, make it powerful in the hands of good guys with good goals,
specifically the good guys whose job it is to prevent bad things from happening.
For example, if you're worried about AI generating fake people and fake videos,
the answer is to build new systems where people can verify themselves and real content via
cryptographic signatures. Digital creation and alteration of both real and fake content was
already here before AI. The answer is not to ban word processors in Photoshop or AI,
but to use technology to build a system that actually solves the problem.
And so second, let's mount major efforts to use AI for good, legitimate defensive purposes.
Let's put AI to work in cyber defense and biological defense and hunting terrorists
and everything else that we want to do to keep ourselves, our communities, and our nation safe.
There are already many smart people in and out of government doing exactly this, of course, but
if we apply all of the effort and brain power that's currently fixated on the futile prospect
of banning AI to using AI to protect against bad people doing bad things, I think there's no
question a world infused with AI will be much safer than the world we live in today.
Okay, and we're getting close to the end here. Let's see what we have next. The actual risk
of not pursuing AI with maximum force and speed. There is one final and real AI risk
that's probably the scariest of all. AI isn't just being developed in the relatively free
societies of the West. It's also being developed by the Communist Party of the People's Republic
of China. China has a vastly different version, excuse me, vision for AI than we do. They view
it as a mechanism for authoritarian population control full stop. They're not even being
secretive about this. They're very clear about it, and they are already pursuing their agenda.
And they do not intend to limit their AI strategy to China. They intend to proliferate it across
the world. Everywhere they're powering 5G networks, everywhere they're loaning belt and road money,
everywhere they're providing friendly consumer apps like Tik Tok that serve as front ends to
their centralized command and control AI. The single greatest risk of AI is that China wins
global AI dominance, and we, the United States and the West, do not. I, he writes, propose a
simple strategy for what to do about this. In fact, the same strategy President Ronald Reagan used to
win the first Cold War with the Soviet Union. We win, they lose. Rather than allowing ungrounded
panics around killer AI, harmful AI, job destroying AI, and inequality generating AI to put us
on our back feet. We in the United States and the West should lean into AI as hard as we possibly
can. We should seek to win the race to global AI technological superiority and ensure that China
does not. In the process, we should drive AI into our economy and society as fast and hard as we
possibly can in order to maximize its gains for economic productivity and human potential.
This is the best way both to offset the real AI risks and to ensure that our way of life
is not displaced by the much darker Chinese version.
What is to be done? He asks, I propose a simple plan. Big AI companies should be allowed to
build AI as fast and aggressively as they can, but not allowed to achieve regulatory capture,
not allowed to establish a government protected cartel that is insulated from market competition
due to incorrect claims of AI risk. This will maximize the technological and societal payoff
from the amazing capabilities of these companies, which are jewels of modern capitalism.
Startup AI companies should be allowed to build AI as fast and aggressively as they can.
They should neither confront government-granted protection of big companies nor should they
receive government assistance. They should simply be allowed to compete. If and as startups don't
succeed, their presence in the market will also continuously motivate big companies to be their
best. Our economies and societies win either way. Open source AI should be allowed to freely
proliferate and compete with both big AI companies and startups. There should be no regulatory
barriers to open source whatsoever. Even when open source does not beat companies, its widespread
availability is a boon to students all over the world who want to learn how to build and use AI
to become part of the technological future and will ensure that AI is available to everyone who
can benefit from it no matter who they are or how much money they have. To offset the risk of
bad people doing bad things with AI, governments working in partnership with the private sector
should vigorously engage in each area of potential risk to use AI to maximize society's defensive
capabilities. This shouldn't be limited to AI-enabled risks, but also more general problems such as
malnutrition, disease, and climate. AI can be an incredibly powerful tool for solving problems,
and we should embrace it as such. To prevent the risk of China achieving global AI dominance,
we should use the full power of our private sector, our scientific establishment, and our
governments in concert to drive American and Western AI to absolute global dominance,
including ultimately inside China itself. We win, they lose, and that is how we use AI to save the
world. It's time to build. And then what do we have here? Just two paragraphs to go. Legends and
heroes. I close Mark Andreessen writes with two simple statements. The development of AI started
in the 1940s, simultaneous with the invention of the computer. The first scientific paper on neural
networks, the architecture of the AI we have today, was published in 1943. Entire generations of AI
scientists over the last 80 years were born, went to school, worked, and in many cases passed away
without seeing the payoff that we are receiving now. They are legends, everyone. Today, growing
legions of engineers, many of whom are young and may have had grandparents or even great
grandparents involved in the creation of the ideas behind AI, are working to make AI a reality
against a wall of fear, mongering, and demerism that is attempting to paint them as reckless
villains. I do not believe they are reckless or villains. They are heroes, everyone. My firm
and I are thrilled to back as many of them as we can, and we will stand alongside them in their work
100%. Okay, so there you go. That was an interesting article. I hope that you learned something from
it and maybe you would not have read it otherwise, so I'm glad we had a chance to go over it. Why AI
will save the world? A strong argument here in favor of AI maximalism going all in on AI fast and hard,
and specifically overcoming some of the more common or responding to some of the more common
objections to AI that it will kill us all, that it's going to have detrimental effects on our society,
that it's going to take all our jobs, that it's going to lead to crippling inequality,
and that people will do bad things with it, which is the risk that he said actually is the most
likely. But you can use AI to stop the bad people who want to do bad things with it,
if you just use AI for detection purposes and you enforce the laws that are already in place.
So there you go. I hope that you enjoyed that. As I said, we should find more occasion to discuss
the significance of new AI technologies and all the rest of it for the question of the good society,
of political sovereignty, of progressivism, a relationship with technology, freedom, tyranny,
all of these kinds of things. I've been discussing those topics more and more with my students in
private tutoring who asked me to walk them through what the tradition of political philosophy has
had to say so that they who are working in fields of technological entrepreneurship and development
can think more clearly about the underlying political theory dimension of the new technologies.
And it's great. I'm glad that that's happening. I encourage you to check out millermanschool.com,
like, share, subscribe to this channel, comment and all the rest of it so these videos can
be picked up by the algorithm shared and the channel can grow. Okay. Thanks everybody.
Great being with you. Have a nice day. Until the next video. Take care. Goodbye.

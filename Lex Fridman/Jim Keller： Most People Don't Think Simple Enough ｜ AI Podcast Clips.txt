So, what have you learned about the human abstractions from individual functional human
units to the broader organization?
What does it take to create something special?
Well, most people don't think simple enough, all right?
So do you know the difference between a recipe and the understanding?
There's probably a philosophical description of this.
So imagine you're going to make a loaf of bread, the recipe says get some flour, add
some water, add some yeast, mix it up, let it rise, put it in a pan, put it in the oven.
It's a recipe, right?
Understanding bread, you can understand biology, supply chains, you know, rain
grinders, yeast, physics, you know, thermodynamics, like there's so many levels
of understanding there.
And then when people build and design things, they frequently are executing some
stack of recipes, right?
And the problem with that is the recipes all have limited scope.
Look, if you have a really good recipe book for making bread, it won't tell you
anything about how to make an omelet, right?
But if you have a deep understanding of cooking, right, then bread, omelets,
you know, sandwich, you know, there's a different way of viewing everything.
And most people, when you get to be an expert at something, you know, you're
hoping to achieve deeper understanding, not just a large set of recipes to go
execute.
And it's interesting to walk groups of people because executing recipes is
unbelievably efficient if it's what you want to do.
If it's not what you want to do, you're really stuck.
And that difference is crucial.
And everybody has a balance of, let's say, deeper understanding of recipes.
And some people are really good at recognizing when the problem is to
understand something deeply.
Does that make sense?
That totally makes sense.
Does every stage of development deep understanding on the team needed?
Oh, this goes back to the art versus science question.
Sure.
If you constantly unpack everything for deeper understanding, you never get
anything done.
And if you don't unpack understanding when you need to, you'll do the wrong
thing.
And then at every juncture, like human beings are these really weird things
because everything you tell them has a million possible outputs, right?
And then they all interact in a hilarious way.
And then having some intuition about what you tell them, what you do, when do
you intervene, when do you not, it's complicated.
Right.
So it's essentially computationally unsolvable.
Yeah, it's an intractable problem.
Sure.
Humans are a mess.
But with deep understanding, do you mean also sort of fundamental questions
of things like, what is a computer?
Or why?
Like the why questions, why are we even building this of purpose?
Or do you mean more like going towards the fundamental limits of physics sort
of really getting into the core of the science?
Well, in terms of building a computer, think a little simpler.
So common practice is you build a computer.
And then when somebody says, I want to make it 10% faster, you'll go in and say,
all right, I need to make this buffer bigger.
And maybe I'll add an ad unit.
Or I have this thing that's three instructions wide.
I'm going to make it four instructions wide.
And what you see is each piece gets incrementally more complicated, right?
And then at some point you hit this limit.
Like adding another feature or a buffer doesn't seem to make it any faster.
And then people will say, well, that's because it's a fundamental limit.
And then somebody else will look at it and say, well, actually the way you
divided the problem up and the way that different features are interacting is
eliminating you.
And it has to be rethought, rewritten, right?
So then you refactor and rewrite it.
And what people commonly find is the rewrite is not only faster, but half is
complicated.
From scratch?
Yes.
So how often in your career, but just have you seen as needed maybe more
generally to just throw the whole out, the whole thing out?
So this is where I'm on one end of it every three to five years.
Which end are you on?
Wait.
I rewrite more often.
You write and three to five years is?
If you want to really make a lot of progress on computer architecture, every five
years you should do one from scratch.
So where does the x86-64 standard come in?
How often do you?
I wrote the, I was the co-author of that spec in 98.
That's 20 years ago.
Yeah, so that's still around.
The instruction set itself has been extended quite a few times.
Yes.
And instruction sets are less interesting than the implementation underneath.
There's been, on x86 architecture, Intel's designed a few,
AIM just designed a few, very different architectures.
And I don't want to go into too much of the detail about how often.
But there's a tendency to rewrite it every 10 years, and it really should be every five.
So you're saying you're an outlier in that sense in the-
Rewrite more often.
Rewrite more often.
Well, and here's-
Isn't that scary?
Yeah, of course.
Well, scary to who?
To everybody involved, because like you said, repeating the recipe is efficient.
Companies want to make money, no, individual engineers want to succeed.
So you want to incrementally improve, increase the buffer from three to four.
Well, this is where you get into diminishing return curves.
I think Steve Jobs said this, right?
So every, you have a project, and you start here, and it goes up, and
they have diminishing return.
And to get to the next level, you have to do a new one, and
the initial starting point will be lower than the old optimization point.
But it'll get higher.
So now you have two kinds of fear, short-term disaster and long-term disaster.
And you're haunted.
So grown-ups, right, like people with a quarter by quarter
business objective are terrified about changing everything.
And people who are trying to run a business or
build a computer for a long-term objective know that the short-term
limitations block them from the long-term success.
So if you look at leaders of companies that had really good long-term success,
every time they saw that they had to redo something, they did.
And so somebody has to speak up.
Or you do multiple projects in parallel.
Like you optimize the old one while you build a new one.
But the marketing guys are always like,
make promise me that the new computer is faster on every single thing.
And the computer architect says, well,
the new computer will be faster on the average.
But there's a distribution of results and performance, and
you'll have some outliers that are slower.
And that's very hard because they have one customer who cares about that one.

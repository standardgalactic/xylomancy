A lot of people have said for many years that there will come a time when we want to pause a little bit.
That time is now.
The following is a conversation with Max Tegmark, his third time in the podcast. In fact, his first
appearance was episode number one of this very podcast. He is a physicist and artificial intelligence
researcher at MIT, co-founder of FutureLeft Institute and author of Life 3.0, being human
in the age of artificial intelligence. Most recently, he's a key figure in spearheading
the open letter calling for a six-month pause on giant AI experiments like training GPT-4.
The letter reads, we're calling for a pause on training of models larger than GPT-4 for six
months. This does not imply a pause or ban on all AI research and development or the use of systems
that have already been placed on the market. Our call is specific and addresses a very small
pool of actors who possess this capability. The letter has been signed by over 50,000 individuals,
including 1,800 CEOs and over 1,500 professors. Signatories include Joshua Benjo,
Stuart Russo, Elon Musk, Steve Wozniak, Yuval Noah Harari, Andrew Yang and many others.
This is a defining moment in the history of human civilization, where the balance of power
between human and AI begins to shift. And Max's mind and his voice is one of the most valuable
and powerful in a time like this. His support, his wisdom, his friendship has been a gift
I'm forever deeply grateful for. This is the Lex Friedman podcast. To support it,
please check out our sponsors in the description. And now, dear friends, here's Max Tagmark.
You were the first ever guest on this podcast, episode number one. So first of all, Max, I just
have to say thank you for giving me a chance. Thank you for starting this journey. It's been an
incredible journey. Just thank you for sitting down with me and just acting like I'm somebody
who matters, that I'm somebody who's interested to talk to. And thank you for doing it. That meant
a lot. Thanks to you for putting your heart and soul into this. I know when you delve into
controversial topics, it's inevitable to get hit by what Hamlet talks about, the slings and arrows
and stuff. And I really admire this. It's in an era where YouTube videos are too long and now it
has to be like a 20-second TikTok clip. It's just so refreshing to see you going exactly against all
of the advice and doing these really long-form things. And the people appreciate it. Reality
is nuanced. And thanks for sharing it that way. So let me ask you again, the first question I've
ever asked on this podcast, episode number one, talking to you. Do you think there's intelligent
life out there in the universe? Let's revisit that question. Do you have any updates?
What's your view when you look out to the stars? So when we look out to the stars,
if you define our universe the way most astrophysicists do, not as all of space, but the spherical
region of space that we can see with our telescopes from which light has a time to reach us since our
Big Bang, I'm in the minority. I estimate that we are the only life in this spherical volume that
has invented internet, radios, gotten our level of tech. And if that's true, then it puts a lot
of responsibility on us to not mess this one up. Because if it's true, it means that life is
quite rare. And we are stewards of this one spark of advanced consciousness, which if we nurture it
and help it grow, it eventually life can spread from here out into much of our universe. And we
can have this just amazing future. Whereas if we instead are reckless with the technology we build
and just snuff it out due to the stupidity or infighting, then maybe the rest of cosmic history
in our universe is just going to be a plenty of empty benches. But I do think that we are actually
very likely to get visited by aliens, alien intelligence quite soon. But I think we are
going to be building that alien intelligence. So we're going to give birth to an intelligent
alien civilization. Unlike anything that human, the evolution here on Earth was able to create
in terms of the path, the biological path it took.
Yeah. And it's going to be much more alien than a cat or even the most exotic animal on the planet
right now. Because it will not have been created through the usual Darwinian competition, where
it necessarily cares about self preservation, afraid of death, any of those things. The space
of alien minds that you can build is just so much faster than what evolution will give you.
And with that also comes a great responsibility for us to make sure that the kind of minds we
create are those kind of minds that it's good to create, minds that will share our values and
be good for humanity and life, and also create minds that don't suffer.
Do you try to visualize the full space of alien minds that AI could be? Do you try to consider
all the different kinds of intelligences? So generalizing what humans are able to do to the
full spectrum of what intelligent creatures entities could do?
I try, but I would say I fail. I mean, it's very difficult for human mind
to really grapple with something so completely alien, even for us. If we just try to imagine,
how would it feel if we were completely indifferent towards death or individuality?
Even if you just imagine that, for example,
you could just copy my knowledge of how to speak Swedish. Boom, now you can speak Swedish.
And you could copy any of my cool experiences and you could delete the ones you didn't like in
your own life, just like that. It would already change quite a lot about how you feel as a human
being, right? You probably spend less effort studying things if you just copy them and you
might be less afraid of death because if the plane you're on starts to crash, you'd just be like,
oh shucks, I haven't backed my brain up for four hours. So I'm going to lose all these wonderful
experiences out of this flight. We might also start feeling more compassionate maybe with
other people if we can so readily share each other's experiences and our knowledge and feel
more like a hive mind. It's very hard though. I really feel very humble about this to grapple
with it, how it might actually feel. The one thing which is so obvious though, I think it's
just really worth reflecting on is because the mind space of possible intelligence is so different
from ours, it's very dangerous if we assume they're going to be like us or anything like us.
Well, the entirety of human written history has been through poetry, through novels,
been trying to describe through philosophy, trying to describe the human condition and what's
entailed in it. Like Jessica said, fear of death and all those kinds of things, what is love,
and all of that changes if you have a different kind of intelligence, like all of it. The entirety,
all those poems, they're trying to sneak up to what the hell it means to be human,
all of that changes. How AI concerns and existential crises that AI experiences,
how that clashes with the human existential crisis, the human condition. It's hard to fathom,
hard to predict. It's hard, but it's fascinating to think about also. Even in the best case scenario
where we don't lose control over the ever more powerful AI that we're building to other humans
whose goals we think are horrible and where we don't lose control to the machines and AI
provides the things that we want, even then you get into the questions, do you touch here?
Maybe the struggle that it's actually hard to do things is part of the things that give this
meaning as well. For example, I found it so shocking that this new Microsoft GPT-4 commercial
that they put together has this woman talking about showing this demo of how she's going to
give a graduation speech to her beloved daughter and she asks GPT-4 to write it.
If it's frigging 200 words or so, if I realized that my parents couldn't be bothered struggling
a little bit to write 200 words and outsource that to their computer, I would feel really offended,
actually. I wonder if eliminating too much of this struggle from our existence,
do you think that would also take away a little bit of what means to be human? Yeah.
Yeah. We can't even predict. I had somebody mentioned to me that they started using
chat GPT with a 3.5 and not 4.0 to write what they really feel to a person
and they have a temper issue and they're basically trying to get chat GPT to rewrite it in a nicer
way, to get the point across, but rewrite it in a nicer way. We're even removing the inner
asshole from our communication. There's some positive aspects of that, but mostly it's just
the transformation of how humans communicate. It's scary because so much of our society is
based on this glue of communication. We're now using AI as the medium of communication.
It does the language for us. So much of the emotion that's laden in human communication,
so much of the intent that's going to be handled by outsourced AI. How does that change everything?
How does that change the internal state of how we feel about other human beings?
What makes us lonely? What makes us excited? What makes us afraid? How we fall in love? All that
kind of stuff. Yeah. For me personally, I have to confess the challenge is one of the things
really makes my life feel meaningful. If I go hike a mountain with my wife,
I don't want to just press a button and be at the top. I want to struggle and come up
there sweaty and feel, wow, we did this in the same way. I want to constantly work on myself
to become a better person. If I say something in anger that I regret, I want to go back and really
work on myself rather than just tell an AI from now on, always filter what I write,
so I don't have to work on myself because then I'm not growing.
Yeah. But then again, it could be like with chess. And AI, once it's significantly obviously
supersedes the performance of humans, it will live in its own world and provide maybe a flourishing
civilizations for humans, but we humans will continue hiking mountains and playing our games
even though AI is so much smarter, so much stronger, so much superior in every single way,
just like with chess. That's one possible hopeful trajectory here is that humans will
continue to human and AI will just be a medium that enables the human experience to flourish.
Yeah. I would phrase that as rebranding ourselves from homo sapiens to homo sentiens.
Right now, sapiens, the ability to be intelligent, we've even put it in our species name.
We're branding ourselves as the smartest information processing
entity on the planet. That's clearly going to change if AI continues ahead.
So maybe we should focus on the experience instead, the subjective experience that we have
with homo sentiens and say that's what's really valuable, the love, the connection, the other things.
Get off our high horses and get rid of this hubris that only we can do integrals.
So consciousness, the subjective experience is a fundamental value
to what it means to be human. Make that the priority.
That feels like a hopeful direction to me, but that also requires more compassion,
not just towards other humans because they happen to be the smartest on the planet,
but also towards all our other fellow creatures on this planet. I personally feel right now,
we're treating a lot of farm animals horribly, for example, and the excuse we're using is,
oh, they're not as smart as us. But if we admit that we're not that smart in the grand scheme of
things either in the post AI epoch, then surely we should value the subjective experience of a cow,
also. Well, allow me to briefly look at the book, which at this point is becoming more and more
visionary than you've written, I guess over five years ago, Life 3.0. So first of all, 3.0. What's
1.0? What's 2.0? What's 3.0? And how's that vision evolve? The vision in the book evolved to today?
Life 1.0 is really dumb, like bacteria, and that it can't actually learn anything at all during
the lifetime. The learning just comes from this genetic process from one generation to the next.
Life 2.0 is us and other animals which have brains, which can learn
during their lifetime a great deal. And you were born without being able to speak English,
and at some point you decided, hey, I want to upgrade my software. Let's install an English
speaking module. So you did. And Life 3.0 does not exist yet, can replace not only its software the
way we can, but also its hardware. And that's where we're heading towards at high speed. We're
already maybe 2.1 because we can put in an artificial knee, pacemaker, et cetera, et cetera.
And if Neuralink and other companies succeed, we'll be Life 2.2, et cetera. But the companies
trying to build AGI or trying to make is, of course, full 3.0. And you can put that intelligence
something that also has no biological basis whatsoever.
So less constraints and more capabilities, just like the leap from 1.0 to 2.0. There is,
nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria,
there is still the same kind of magic there that permeates Life 2.0 and 3.0. It seems like maybe
the thing that's truly powerful about life intelligence and consciousness was already
there in 1.0. Is it possible? I think we should be humble and not be so quick to
make everything binary and say either it's there or it's not. Clearly, there's a great spectrum.
And there is even a controversy about whether some unicellular organisms like amoebas can
maybe learn a little bit after all. So apologies if I offended any bacteria here. It wasn't my
intent. It was more that I wanted to talk up how cool it is to actually have a brain where you
can learn dramatically within your lifetime. Typical human. And the higher up you get from 1.0
to 2.0 to 3.0, the more you become the captain of your own ship, the master of your own destiny,
and the less you become a slave to whatever evolution gave you by upgrading your software,
we can be so different from previous generations and even from our parents,
much more so than even a bacterium, no offense to them. And if you can also swap out your hardware,
take any physical form you want, of course, it really disguises the limit.
Yeah, so it accelerates the rate at which you can perform the computation that determines your
destiny. Yeah, and I think it's worth commenting a bit on what you means in this context. Also,
if you swap things out a lot, right? This is controversial, but my
current understanding is that life is best thought of not as a bag of meat or even a bag of
elementary particles, but rather as a system which can process information and retain its own
complexity, even though nature is always trying to mess it up. So it's all about information
processing. And that makes it a lot like something like a wave in the ocean, which is not its
water molecules, right? The water molecules bob up and down, but the wave moves forward. It's an
information pattern. In the same way, you, Lex, you're not the same atoms as during the first
time you did with me. You've swapped out most of them, but it's still you. And the information
pattern is still there. And if you could swap out your arms and whatever, you can still have
this kind of continuity. It becomes much more sophisticated sort of wave forward in time,
where the information lives on. I lost both of my parents since our last podcast,
and it actually gives me a lot of solace that this way of thinking about them,
they haven't entirely died because a lot of mommy and daddy's, sorry, I'm getting a little
emotional here, but a lot of their values and ideas and even jokes and so on, they haven't
gone away, right? Some of them live on, I can carry on some of them. And we also live on a lot of
other and a lot of other people. So in this sense, even with life 2.0, we can to some extent
already transcend our physical bodies and our death. And particularly if you can share your own
information, your own ideas with many others like you do in your podcast, then
that's the closest immortality we can get with our bio bodies.
You carry a little bit of them in you in some sense. Do you miss them? You miss your mom and
dad? Of course. Of course. What did you learn about life from them if it can take a bit of a tangent?
I don't know so many things. For starters, my fascination for math and the physical mysteries
of our universe, I think you got a lot of that from my dad. But I think my obsession for really
big questions and consciousness and so on, that actually came mostly from my mom. And
what I got from both of them, which is a very core part of really who I am, I think is this
just feeling comfortable with
not buying into what everybody else is saying. Just doing what I think is right.
They both very much just did their own thing and sometimes they got flack for it and they did it
anyway. That's why you've always been an inspiration to me. That you're at the top of your field and
you're still willing to tackle the big questions in your own way. You're one of the people that
represents MIT best to me. You've always been an inspiration to that. So it's good to hear that
you got that from your mom and dad. Yeah, you're too kind. But yeah, I mean, the good reason to do
science is because you're really curious, you want to figure out the truth. If you think
this is how it is and everyone else says, no, no, that's bullshit. And it's that way, you know,
you stick with what you think is true. And even if everybody else keeps thinking it's
bullshit, there's a certain, I always root for the underdog when I watch movies. And my dad once,
one time, for example, when I wrote one of my craziest papers ever,
talking about our universe, ultimately being mathematical, which we're not going to get into
today. I got this email from a quite famous professor saying this is not only on bullshit,
but it's going to ruin your career. You should stop doing this kind of stuff. I sent it to my dad.
Do you know what he said? He replied with a quote from Dante. Segui il tuo corso e la sedire la
gente, follow your own path and let the people talk. Go dad. This is the kind of thing. He's
dead, but that attitude is not. How did losing them as a man, as a human being change you?
How did it expand your thinking about the world? How did it expand your thinking about
you know, this thing we're talking about, which is humans creating another
living sentient perhaps being? I think it
mainly do two things. One of them just going through all their stuff after they had passed
away and so on, just drove home to be how important it is to ask ourselves,
why are we doing this things we do? Because it's inevitable that you look at some things they
spent an enormous time on and you ask the, in hindsight, would they really have spent so much
time on this or would they have done something that was more meaningful? So I've been looking
more in my life now and asking, you know, why am I doing what I'm doing? And I feel
it should either be something I really enjoy doing or it should be something
that I find really, really meaningful because it helps humanity.
If it's none of those two categories, maybe I should spend less time on it.
The other thing is dealing with death up in person like this. It's actually made me less
afraid of even less afraid of other people telling me that I'm an idiot, you know,
which happens regularly and just let my life do my thing.
And it made it a little bit easier for me to focus on what I feel is really important.
What about fear of your own death? Has it made it more real that this is
something that happens? Yeah, it's made it extremely real and I'm next in line in our
family now, right? Me and my younger brother. But they both handled it with such dignity.
That was a true inspiration also. They never complained about things and, you know, when
you're old and your body starts falling apart, it's more and more to complain about. They
looked at what could they still do that was meaningful and they focused on that rather than
wasting time talking about or even thinking much about things they were disappointed in.
I think anyone can make themselves depressed if they start their morning by making a list of
grievances. Whereas if you start your day with a little meditation and just things you're grateful
for, you basically choose to be a happy person. Because you only have a finite number of days
to spend them being grateful. Yeah.
Well, you do happen to be working on a thing which seems to have potentially
some of the greatest impact on human civilization of anything humans have ever created,
which is artificial intelligence. This is on the both detailed technical level and in a high
philosophical level you work on. So you've mentioned to me that there's an open letter
that you're working on. It's actually going live in a few hours. So I've been having late nights
and early mornings. It's been very exciting actually. In short, have you seen Don't Look Up?
The film? Yes, yes. I don't want to be the movie spoiler for anyone watching this who
hasn't seen it. But if you're watching this, you haven't seen it, watch it. Because we are actually
acting out. It's life imitating art. Humanity is doing exactly that right now except it's an
asteroid that we are building ourselves. Almost nobody is talking about it. People are squabbling
across the planet about all sorts of things which seem very minor compared to the asteroid that's
about to hit us. Most politicians don't even have this on the radar. They think maybe in 100
years or whatever. Right now, we're at a fork on the road. This is the most important fork the
humanity has reached in its over 100,000 years on this planet. We're building effectively a new
species that's smarter than us. It doesn't look so much like a species yet because it's mostly
not embodied in robots, but that's the technicality which will soon be changed.
This arrival of artificial general intelligence that can do all our jobs as well as us and
probably shortly thereafter, superintelligence which greatly exceeds our cognitive abilities,
it's going to either be the best thing ever to happen to humanity or the worst. I'm really quite
confident that there is not that much middle ground there. But it would be fundamentally
transformative to human civilization? Of course, utterly and totally. Again, we branded ourselves
as homo sapiens because this seemed like the basic thing where the king of the castle on this planet
were the smart ones. If we can control everything else, this could very easily change. We're certainly
not going to be the smartest on the planet very long unless AI progress just halts. We can talk
more about why I think that's true because it's controversial. Then we can also talk about
reasons you might think it's going to be the best thing ever and the reason you think it's going to
be the end of humanity, which is of course super controversial. But what I think we can,
anyone who's working on advanced AI can agree on is it's much like the film. Don't look up and that
it's just really comical how little serious public debate there is about it given how huge it is.
So what we're talking about is a development of currently things like GPT-4
and the signs it's showing of rapid improvement that may in the near term lead to development
of super intelligent, AI, general AI systems and what kind of impact that has on society when that
thing achieves general human level intelligence and then beyond that general super human level
intelligence. There's a lot of questions to explore here. So one, you mentioned halt. Is that
the content of the letter is to suggest that maybe we should pause the development of these systems?
Exactly. So this is very controversial. When we talked the first time, we talked about how
I was involved in starting the Future Life Institute and we worked very hard on 2014,
2015 was the mainstream AI safety. The idea that there even could be risks and that you could do
things about them. Before then, a lot of people thought it was just really kooky to even talk
about it and a lot of AI researchers felt worried that this was too flaky and could be bad for
funding and that the people who talked about it just didn't understand AI. I'm very, very happy with
how that's gone and that now it's completely mainstream. You go on any AI conference and people
talk about AI safety and it's a nerdy technical field full of equations and blah, blah.
As it should be. But there's this other thing which has been quite taboo up until now
calling for slowdown. So what we've constantly been saying, including myself,
I've been biting my tongue a lot, is that we don't need to slow down AI development. We just need
to win this race, the wisdom race between the growing power of the AI and the growing wisdom
with which we manage it. Rather than trying to slow down AI, let's just try to accelerate the
wisdom. Do all this technical work to figure out how you can actually ensure that your powerful
AI is going to do what you wanted to do and have society adapt also with incentives and
regulations so that these things get put to good use. Sadly, that didn't pan out. The progress on
technical AI on capabilities has gone a lot faster than many people thought back when we started
this in 2014. It turned out to be easier to build really advanced AI than we thought.
And on the other side, it's gone much slower than we hoped with getting
policymakers and others to actually put incentives in place to steer this in the
good direction. Maybe we should unpack it and talk a little bit about each. Why did it go faster
than a lot of people thought? In hindsight, it's exactly like building flying machines.
People spent a lot of time wondering about how do birds fly. And that turned out to be
really hard. Have you seen the TED Talk with a flying bird? Like a flying robotic bird? Yeah,
flies around the audience. But it took 100 years longer to figure out how to do that
than for the Wright brothers to build the first airplane because it turned out there was a much
easier way to fly. And evolution picked a more complicated one because it had its hands tied.
It could only build a machine that could assemble itself, which the Wright brothers didn't care
about. They can only build a machine that used only the most common atoms in the periodic table.
Wright brothers didn't care about that. They could use steel, iron atoms, and it had to be
able to repair itself. And it also had to be incredibly fuel efficient. A lot of birds use
less than half the fuel of a remote control plane flying the same distance. For humans,
throw a little more, put a little more fuel in a roof. There you go, 100 years earlier.
That's exactly what's happening now with these large language models.
The brain is incredibly complicated. Many people made the mistake. You're thinking we had to
figure out how the brain does human level AI first before we could build in a machine.
That was completely wrong. You can take an incredibly simple
computational system called a transformer network and just train it to do something incredibly dumb.
Just read a gigantic amount of text and try to predict the next word. And it turns out,
if you just throw a ton of compute at that and a ton of data, it gets to be
frighteningly good, like GPT-4, which I've been playing with so much since it came out.
There's still some debate about whether that can get you all the way to full human level or not.
We can come back to the details of that and how you might get the human level AI even if
large language models don't.
Can you briefly, if it's just a small tangent, comment on your feelings about GPT-4.
It's just that you're impressed by this rate of progress. Where is it? Can GPT-4
reason? What are the intuitions? What are human interpretable words you can assign to the
capabilities of GPT-4 that makes you so damn impressed with it? I'm both very excited about it
and terrified. It's an interesting mixture of emotions. All the best things in life
include those two somehow.
Yeah, I can absolutely reason. Anyone who hasn't played with it, I highly recommend doing that
before dissing it. It can do quite remarkable reasoning. I've had to do a lot of things,
which I realized I couldn't do that myself that well even. And it obviously does it dramatically
faster than we do too when you watch a type. And it's doing that while servicing a massive
number of other humans at the same time. At the same time, it cannot reason
as well as a human can on some tasks. It's obviously a limitation from its architecture.
We have in our heads what in GeekSpeak is called a recurrent neural network. There are loops.
Information can go from this neuron to this neuron to this neuron and then back to this one.
You can ruminate on something for a while. You can self-reflect a lot.
These large language models, they cannot. It's a so-called transformer
where it's just like a one-way street of information basically. In GeekSpeak,
it's called a feed-forward neural network. And it's only so deep. So it can only do logic that's
that many steps and that deep. And you can create problems which will fail to solve for that reason.
But the fact that it can do so amazing things with this incredibly simple architecture already
is quite stunning. And what we see in my lab at MIT when we look inside
large language models to try to figure out how they're doing it, which that's the key core
focus of our research. It's called mechanistic interpretability in GeekSpeak. You have this
machine that does something smart. You try to reverse engineer, see how does it do it.
And you think of it also as artificial neuroscience. That's exactly what neuroscientists do with
actual brains. But here you have the advantage that you don't have to worry about measurement
errors. You can see what every neuron is doing all the time. And a recurrent thing we see again
and again, there's been a number of beautiful papers quite recently by a lot of researchers.
Some of them here, even in this area, is where when they figure out how something is done,
you can say, oh man, that's such a dumb way of doing it. And you immediately see how it can
be improved. Like for example, there was a beautiful paper recently where they figured out how a large
language model stores certain facts like Eiffel Tower is in Paris. And they figured out exactly
how it's stored and where the proof that they understood it was they could edit it. They changed
some of the synapses in it. And then they asked it, where is the Eiffel Tower? And it said it's
in Rome. And then they asked you, how do you get there? Oh, how do you get there from Germany?
Oh, you take this train to Roma Termini train station and this and that. And what might you see
if you're in front of it? Oh, you might see the Colosseum. So they had edited it. So they literally
moved it to Rome. But the way that it's storing this information, it's incredibly dumb for any
fellow nerds listening to this. There was a big matrix. And roughly speaking, there are certain
row and column vectors which encode these things and they correspond very hand-wavely to principal
components. And it will be much more efficient for a sparse matrix to store in the database, you
know. And everything so far, we've figured out how these things do are ways where you can see
they can easily be improved. And the fact that this particular architecture has some roadblocks
built into it is in no way going to prevent crafty researchers from quickly finding workarounds and
making other kinds of architectures sort of go all the way. So in short, it's turned out to be a lot
easier to build human, close to human intelligence than we thought. And that means our runway as a
species to get our shit together has shortened. And it seems like the scary thing about the
effectiveness of large language models. So Sam Altman, every standard conversation with. And
he really showed that the leap from GPT-3 to GPT-4 has to do with just a bunch of hacks. A bunch of
little explorations with smart researchers doing a few little fixes here and there. It's not some
fundamental leap and transformation in the architecture. And more data and more compute.
And more data and compute. But he said the big leaps has to do with not the data and the compute,
but just learning this new discipline, just like you said. So researchers are going to look at these
architectures and there might be big leaps where you realize, wait, why are we doing this in this
dumb way? And all of a sudden this model is 10x smarter. And that can happen on any one day,
on any one Tuesday or Wednesday afternoon. And then all of a sudden you have a system that's
10x smarter. It seems like it's such a new discipline. It's such a new, like we understand
so little about why this thing works so damn well that the linear improvement of compute
or exponential, but the steady improvement of compute, steady improvement of the data
may not be the thing that even leads to the next leap. It could be a surprise little hack that
improves everything. Or a lot of little leaps here and there because so much of this is out
of the open also. So many smart people are looking at this and trying to figure out little leaps
here and there. And it becomes this sort of collective race where a lot of people feel
if I don't take the leap, someone else will. And this is actually very crucial for the other part
of it. Why do we want to slow this down? So again, what this open letter is calling for is just
pausing all training of systems that are more powerful than GPT-4 for six months. Give a chance
for the labs to coordinate a bit on safety and for society to adapt. Give the right incentives
to the labs because you've interviewed a lot of these people who lead these labs and you know,
just as well as I do, that they're good people. They're idealistic people. They're doing this
first and foremost because they believe that AI has a huge potential to help humanity.
And but at the same time, they are trapped in this horrible race to the bottom.
Have you read Meditations on Moloch by Scott Alexander?
Yes.
Yeah, it's a beautiful essay on this poem by Ginsburg where he interprets it as being about
this monster. It's this game theory monster that pits people against each other in this
race to the bottom where everybody ultimately loses. The evil thing about this monster is
even though everybody sees it and understands, they still can't get out of the race.
Most a good fraction of all the bad things that we humans do are caused by Moloch. And I like
Scott Alexander's naming of the monster so we humans can think of it as a thing.
If you look at why do we have overfishing, why do we have more generally the tragedy of the commons,
why is it that to live, I don't know if you've had her on your podcast.
Yeah, she's become a friend, yeah.
Great, she made this awesome point recently that beauty filters that a lot of female
influencers feel pressured to use are exactly Moloch in action again. First, nobody was using them
and people saw them just the way they were. And then some of them started using it
and becoming ever more plastic fantastic. And then the other ones that weren't using it started
to realize that if they want to just keep their market share, they have to start using it too.
And then you're in a situation where they're all using it.
And none of them has any more market share or less than before. So nobody gained anything,
everybody lost. And they have to keep becoming ever more plastic fantastic also.
And but nobody can go back to the old way because it's just too costly. Moloch is everywhere.
And Moloch is not a new arrival on the scene either. We humans have developed a lot of
collaboration mechanisms to help us fight back against Moloch through various kinds of constructive
collaboration. The Soviet Union and the United States did sign the number of arms control treaties
against Moloch who is trying to stoke them into unnecessarily risky nuclear arms races,
and this is exactly what's happening on the AI front. This time, it's a little bit geopolitics,
but it's mostly money where there's just so much commercial pressure. If you take any of these
leaders of the top tech companies, if they just say, this is too risky, I want to pause
for six months, they're going to get a lot of pressure from shareholders and others.
They're like, well, you know, if you pause, but those guys don't pause,
you don't want to get our lunch eaten. And shareholders even have the power to replace
the executives in the worst case, right? So we did this open letter because we want to help
these idealistic tech executives to do what their heart tells them by providing enough
public pressure on the whole sector to just pause so that they can all pause in a coordinated
fashion. And I think without the public pressure, none of them can do it alone,
push back against their shareholders, no matter how good-hearted they are. Moloch is a really
powerful foe. So the idea is to, for the major developers of AI systems like this,
so we're talking about Microsoft, Google, Meta, and anyone else?
Well, OpenAI is very close with Microsoft now, of course, and there are plenty of smaller players,
for example, Anthrope, which is very impressive. There's Conjecture. There's many, many, many
players. I don't want to make a long list, so leave anyone out. And for that reason,
it's so important that some coordination happens, that there's external pressure on all of them,
saying you all need to pause, because then the people, the researchers in these organizations,
the leaders who want to slow down a little bit, they can say their shareholders, you know,
everybody's slowing down because of this pressure, and it's the right thing to do.
Have you seen in history their examples of what's possible to pause the Moloch?
Absolutely. And even like human cloning, for example, you could make so much money on human
cloning. Why aren't we doing it? Because biologists thought hard about this and felt like this is
way too risky. They got together in the 70s in Asilomar and decided even to stop a lot more
stuff, also just editing the human germline, gene editing that goes into our offspring,
and decided let's not do this, because it's too unpredictable what it's going to lead to.
We could lose control over what happens to our species, so they paused.
There was a ton of money to be made there, so it's very doable, but you just need a public
awareness of what the risks are, and the broader community coming in and saying, hey,
let's slow down. And another common pushback I get today is we can't stop in the West because
China, and in China, undoubtedly, they also get told we can't slow down because the West,
because both sides think they're the good guy. But look at human cloning. Did China forge ahead
with human cloning? There's been exactly one human cloning that's actually been done that I know of.
It was done by a Chinese guy. Do you know where he is now? In jail. And you know who put him there?
Who? Chinese government. Not because Westerners said China, look, this is, no, the Chinese
government put him there, because they also felt they like control the Chinese government.
If anything, maybe they are even more concerned about having control than Western governments
have no incentive of just losing control over where everything is going.
And you can also see the Ernie bot that was released by, I believe, Baidu recently.
They got a lot of pushback from the government and had to reign it in in a big way.
I think once this basic message comes out that this isn't an arms race, it's a suicide race,
where everybody loses if anybody's AI goes out of control, it really changes the whole dynamic.
It's not, I'll say this again, because this is this very basic point I think a lot of people get
wrong. Because a lot of people dismiss the whole idea that AI can really get very superhuman,
because they think there's something really magical about intelligence such that it can
only exist in human minds. Because they believe that, I think it's kind of kind of get to just
more or less GPT-4++ and then that's it. They don't see it as a super as a suicide race. They
think whoever gets that first, they're going to control the world, they're going to win.
That's not how it's going to be. And we can talk again about the scientific arguments
from why it's not going to stop there. But the way it's going to be is if anybody completely loses
control and you don't care, if someone manages to take over the world who really doesn't share
your goals, you probably don't really even care very much about what nationality they have. You're
not going to like it, much worse than today. If you live in Orwellia in dystopia, what do you
care who created it? And if it goes farther and we just lose control even to the machines,
so that it's not us versus them, it's us versus it. What do you care who created this
unaligned entity which has goals different from humans ultimately and we get marginalized,
we get made obsolete, we get replaced? That's why what I mean when I say it's a suicide race.
It's kind of like we're rushing towards this cliff. But the closer the cliff we get,
the more scenic the views are and the more money there is there. So we keep going. But we have
to also stop at some point, right? Quit while we're ahead. And it's a suicide race which cannot
be won. But the way to really benefit from it is to continue developing awesome AI a little bit
slower. So we make it safe, make sure it does the things that humans want and create a condition
where everybody wins. Technology has shown us that geopolitics and politics in general is not
a zero sum game at all. So there is some rate of development that will lead us as a human species
to lose control of this thing. And the hope you have is that there's some lower level of development
which will not allow us to lose control. This is an interesting thought you have about losing
control. So if you are somebody like Sander Prachai or Sam Altman at the head of a company like this,
you're saying if they develop an AGI, they too will lose control of it. So no one person can
maintain control. No group of individuals can maintain control. If it's created very, very soon
and is a big black box that we don't understand like the large language models, yeah,
then I'm very confident they're going to lose control. But this isn't just me saying it, you know,
Sam Altman and Demis Osabis have both said, themselves, acknowledge that there's really
great risks with this and they want to slow down once they feel it gets scary. But it's clear that
they're stuck in this. Again, Malak is forcing them to go a little faster than they're comfortable
with because of pressure from just commercial pressures, right?
To get a bit optimistic here, of course, this is a problem that can be ultimately solved.
It's just to win this wisdom race, it's clear that what we hope that was going to happen hasn't
happened. The capability progress has gone faster than a lot of people thought and the
progress in the public sphere of policymaking and so on has gone slower than we thought. Even
the technical AI safety has gone slower. A lot of the technical safety research was kind of banking
on that large language models and other poorly understood systems couldn't get us all the way,
that you had to build more of a kind of intelligence that you could understand,
maybe it could prove itself safe, you know, things like this. And I'm quite confident that this can
be done, so we can reap all the benefits, but we cannot do it as quickly as this out of control
express train we are on now is going to get the AGI. That's why we need a little more time, I feel.
Is there something to be said, what like Sam Allman talked about, which is while we're in the
pre-AGI stage to release often and as transparently as possible, to learn a lot. So as opposed to
being extremely cautious, release a lot. Don't invest in a closed development where you focus on
AI safety while it's somewhat dumb, quote unquote, release as often as possible. And as you start to
see signs of human level intelligence or super human level intelligence, then you put a halt on it.
Well, what a lot of safety researchers have been saying for many years is that the most
dangerous things you can do with an AI is first of all, teach it to write code. Yeah, because
that's the first step towards recursive self-improvement, which can take it from AGI to much higher
levels. Okay, oops, we've done that. And another thing, high risk is connected to the internet.
Let it go to websites, download stuff on its own and talk to people. Oops, we've done that already.
Eliezer Jukowski, you said you interviewed him recently, right? So he had this tweet recently,
which gave me one of the best laughs in a while where he was like, hey, people used to make fun
of me and say, you're so stupid, Eliezer, because you're saying you have to worry. Obviously,
developers, once they get to really strong AI, the first thing you're going to do is never
connect it to the internet, keep it in a box where you can really study it safe.
Yeah. So he had written it in the meme forms, it was like then. Yeah. And then that, now.
Let's, lol, let's make a chatbot. Yeah. And the third thing is Stuart Russell. Yeah. You know,
amazing AI researcher. He has argued for a while that we should never teach AI anything about humans.
Above all, we should never let it learn about human psychology and how you manipulate humans.
That's the most dangerous kind of knowledge you can give it. Yeah, you can teach it all it needs
to know about how to cure cancer and stuff like that, but don't let it read Daniel Kahneman's book
about cognitive biases and all that. And then, oops, lol, you know, let's invent social media.
Yeah. Recommender algorithms which do exactly that. They get so good at knowing us and pressing our
buttons that we're starting to create a world now where we just have ever more hatred because
they figured out that these algorithms, not for out of evil, but just to make money on advertising,
that the best way to get more engagement, the euphemism, get people glued to their little
rectangles is just to make them pissed off. That's really interesting that a large AI system that's
doing the recommender system kind of task on social media is basically just studying human
beings because it's a bunch of us rats giving it signal, nonstop signal. It'll show a thing and
we give signal and whether we spread that thing, we like that thing, that thing increases our
engagement, gets us to return to the platform. It has that on the scale of hundreds of millions of
people constantly. So it's just learning and learning and learning. And presumably if the
number of parameters in your neural network that's doing the learning and more end-to-end
the learning is, the more it's able to just basically encode how to manipulate human behavior,
how to control humans at scale. Exactly. And that is not something I think is in humanity's interest.
Right now, it's mainly letting some humans manipulate other humans for profit
and power, which already caused a lot of damage. And eventually that's a sort of
skill that can make AIs persuade humans to let them escape whatever safety precautions we put.
You know, there was a really nice article in The New York Times recently by Yuval Noah Harari
and two co-authors, including Tristan Harris from The Social Dilemma. And
they have this phrase in there, I love. Humanity's first contact with advanced AI
was social media. And we lost that one. We now live in a country where there's much more hate
in the world where there's much more hate, in fact. And in our democracy that we're having
this conversation and people can't even agree on who won the last election, you know. And we humans
often point fingers at other humans and say it's their fault. But it's really MOLOC and these AI
algorithms. We got the algorithms and then MOLOC pitted the social media companies against each
other so nobody could have a less creepy algorithm because then they would lose out on revenue to
the other company. Is there any way to win that battle back just if we just linger on this one
battle that we've lost in terms of social media? Is it possible to redesign social media this very
medium in which we use as a civilization to communicate with each other, to have these
kinds of conversations, to have discourse to try to figure out how to solve the biggest problems
in the world, whether that's nuclear war or the development of AGI? Is it possible to do social
media correctly? I think it's not only possible, but it's necessary. Who are we kidding that we're
going to be able to solve all these other challenges if we can't even have a conversation with each
other that's constructive? The whole idea, the key idea of democracy is that you get a bunch of
people together and they have a real conversation, the ones you try to foster on this podcast or
you respectfully listen to people you disagree with and you realize actually there are some
things actually, some common ground we have and we both agree let's not have nuclear wars, let's
not do that, etc. We're kidding ourselves thinking we can face off the second contact with
ever more powerful AI that's happening now with these large language models if we can't even
have a functional conversation in the public space. That's why I started the Improve the News
project, improvethenews.org, but I'm an optimist fundamentally in that there is a lot of intrinsic
goodness in people and that what makes the difference between someone doing good things
for humanity and bad things is not some sort of fairy tale thing that this person was born with
the evil gene and this one was not born with a good gene. No, I think it's whether we put, whether
people find themselves in situations that bring out the best in them or that bring out the worst
in them and I feel we're building an internet and a society that brings out the worst.
But it doesn't have to be that way. No, it does not. It's possible to create incentives and also
create incentives that make money, that both make money and bring out the best in people.
I mean, in the long term, it's not a good investment for anyone to have a nuclear war,
for example. And is it a good investment for humanity if we just ultimately replace all humans
by machines and then are so obsolete that eventually there are no humans left? Well,
it depends on how you do the math, but I would say by any reasonable economic standard,
if you look at the future income of humans and there aren't any, that's not a good investment.
Moreover, why can't we have a little bit of pride in our species? Damn it. Why should we just build
another species that gets rid of us? If we were Neanderthals, would we really consider it a smart
move if we had really advanced biotech to build homo sapiens? You might say, hey Max,
yeah, let's build these homo sapiens. They're going to be smarter than us. Maybe they can help us
defend this better against predators and help fix their bar caves, make them nicer. We'll control
them undoubtedly. So then they build a couple, a little baby girl, a little baby boy. And then
do you have some wise old Neanderthal elders like, hmm, I'm scared that we're opening a Pandora's
box here and that we're going to get outsmarted by these super Neanderthal intelligences.
There won't be any Neanderthals left. But then you have a bunch of others in the cave. Are you
such a Luddite scare monger? Of course, they're going to want to keep us around because we are
their creators. I think the smarter they get, the nicer they're going to get. They're going to leave
us, they're going to want us around and it's going to be fine. And besides, look at these babies,
they're so cute. Clearly, they're totally harmless. Those babies are exactly GPT-4.
I want to be clear. It's not GPT-4 that's terrifying. It's the GPT-4 is a baby technology.
Microsoft even had a paper recently out with a title, something like sparkles of AGI.
Well, they were basically saying this is baby AI, like these little Neanderthal babies.
And it's going to grow up. There's going to be other systems from the same company, from other
companies. They'll be way more powerful, but they're going to take all the things, ideas from these
babies. And before we know it, we're going to be like those last Neanderthals who were pretty
disappointed when they realized that they were getting replaced.
Well, this interesting point you make, which is the programming, it's entirely possible that GPT-4
is already the kind of system that can change everything by writing programs.
Yeah, it's life 2.0. The systems I'm afraid of are going to look nothing like a large language
model. But once it gets it or other people figure out a way of using this tech to make much better
tech, it's just constantly replacing its software. And from everything we've seen about how these
work under the hood, they're like the minimum viable intelligence. They do everything in a
dumbest way that still works, sort of. And so they are life 3.0, except when they replace their
software, it's a lot faster than when you decide to learn Swedish. And moreover,
they think a lot faster than us too. So when we don't think of one logical step every nanosecond
or so, the way they do, and we can't also just suddenly scale up our hardware massively in the
cloud, so limited, right? So they are also life consumed become a little bit more like life 3.0
in that if they need more hardware, hey, just rent it in the cloud. How do you pay for it? Well,
all the services you provide. And what we haven't seen yet, which could change a lot, is an entire
software system. So right now, programming is done sort of in bits and pieces as an assistant tool
to humans. But I do a lot of programming. And with the kind of stuff that GPT-4 is able to do,
I mean, it's replacing a lot what I'm able to do. But you still need a human in the loop to kind of
manage the design of things, manage like what are the prompts that generate the kind of stuff to
do some basic adjustment of the code to do some debugging. But if it's possible to add on top
of GPT-4 kind of feedback loop of self debugging, improving the code, and then you launch that
system onto the wild on the internet, because everything is connected, and have it do things,
have it interact with humans, and then get that feedback. Now you have this giant ecosystem
of humans. That's one of the things that Elon Musk recently sort of tweeted as a case why everyone
needs to pay $700 or whatever for Twitter. To make sure they're real. Make sure they're real.
We're now going to be living in a world where the bots are getting smarter and smarter and
smarter to a degree where you can't tell the difference between a human and a bot. That's
right. And now you can have bots outnumber humans by 1 million to 1, which is why he's making a
case why you have to pay to prove you're human, which is one of the only mechanisms to prove,
which is depressing. And I feel we have to remember, as individuals, we should from time to
time ask ourselves why are we doing what we're doing. As a species, we need to do that too.
So if we're building, as you say, machines that are outnumbering us and more and more
outsmarting us and replacing us on the job market, not just for the dangerous and boring tasks,
but also for writing poems and doing art and things that a lot of people find really meaningful,
you've got to ask ourselves, why? Why are we doing this? The answer is malloc is tricking
us into doing it. And it's such a clever trick that even though we see the trick, we still
have no choice but to fall for it, right? Also, the thing you said about you using
co-pilot AI tools to program faster, how many times, what factor faster would you say you code
now? Does it go twice as fast? I don't really, because it's a new tool. Yeah. It's, I don't know
if speed is significantly improved, but it feels like I'm a year away from being
five to 10 times faster. So if that's typical for programmers, then you're already seeing another
recursive self-improvement, right? Because previously, one, a major generation of
improvement of the code would happen on the human R&D timescale. And now, if that's five times
shorter, then it's going to take five times less time than it otherwise would to develop the next
level of these tools and so on. So these are the, this is exactly the sort of beginning of an
intelligence explosion. There can be humans in the loop a lot in the early stages. And then
eventually humans are needed less and less and the machines can more kind of go along. But
what you weren't, you said there is just an exact example of these sort of things.
Another thing which I was kind of lying on my psychiatrist, imagining I'm on a psychiatrist
couch here saying, well, what are my fears that people would do with AI systems? So I mentioned
three that I had fears about many years ago that they would do, namely teach you the code,
connect it to the internet and teach it to manipulate humans. A fourth one is building an API
where code can control the super powerful thing, right? That's very unfortunate because
one thing that systems like GPT-4 have going for them is that they are an oracle in the sense that
they just answer questions. There is no robot connected to GPT-4. GPT-4 can't go and do stock
trading based on its thinking. It's not an agent. An intelligent agent is something that takes
in information from the world, processes it to figure out what action to take based on its goals
that it has and then does something back on the world. But once you have an API, for example,
GPT-4, nothing stops Joe Schmo and a lot of other people from building real agents,
which just keep making calls somewhere in some inner loop somewhere to these powerful oracle
systems, which makes them themselves much more powerful. That's another unfortunate development
which I think we would have been better off delaying. I don't want to pick on any particular
companies. I think they're all under a lot of pressure to make money. Again, the reason we're
calling for this pause is to give them all cover to do what they know is the right thing,
slow down a little bit at this point. But everything we've talked about, I hope, will make
it clear to people watching this, why these human-level tools can cause gradual acceleration.
You keep using yesterday's technology to build tomorrow's technology. When you do that over
and over again, you naturally get an explosion. That's the definition of an explosion in science.
If you have two people, they fall in love, now you have four people, and then they can make more
babies, and now you have eight people, and then you have 16, 32, 64, etc. We call that a population
explosion, whereas if it's instead free neutrons in a nuclear reaction, if each one can make
more than one, then you get an exponential growth in that. We call it a nuclear explosion.
All explosions are like that. An intelligence explosion is just exactly the same principle
that some amount of intelligence can make more intelligence than that. Then repeat,
you always get the exponentials. What's your intuition why it does? You mentioned
there's some technical reasons why it doesn't stop at a certain point. What's your intuition?
Do you have any intuition why it might stop? It's obviously going to stop when it bumps up
against the laws of physics. There are some things you just can't do no matter how smart
you are. Allegedly. Because we don't know the full laws of physics yet, right?
Seth Lloyd wrote a really cool paper on the physical limits on computation, for example. If
you put too much energy into it, and the finite space will turn into a black hole,
you can't move information around fast at the speed of light, stuff like that, but
it's hard to store way more than a modest number of bits per atom, etc. Those limits
are just astronomically above, like 30 orders of magnitude above where we are now. So,
bigger jump in intelligence than if you go from an ant to a human.
I think, of course, what we want to do is have a controlled thing, a nuclear reactor you put
moderators in to make sure exactly it doesn't blow up out of control, right? When we do
experiments with biology and cells and so on, you know, we also try to make sure it doesn't
get out of control. We can do this with AI too. The thing is, we haven't succeeded yet.
And MOLOC is exactly doing the opposite, just fueling, just egging everybody on,
faster, faster, faster, or the other company is going to catch up with you,
or the other country is going to catch up with you. We do this, we have to want this stuff.
I don't believe in this, just asking people to look into their hearts and do the right thing.
It's easier for others to say that, but if you're in a situation where your company is going to get
screwed by other companies that are not stopping, you know, you're putting people in a very hard
situation, the right thing to do is change the whole incentive structure instead. And this is
not an old, maybe I should say one more thing about this, because MOLOC has been around as
humanity's number one or number two enemy since the beginning of civilization. And we came up
with some really cool countermeasures. Like, first of all, already over 100,000 years ago,
evolution realized that it was very unhelpful that people kept killing each other all the time.
So it genetically gave us compassion and made it so that if you get two drunk dudes getting into
a pointless bar fight, they might give each other black eyes, but they have a lot of inhibition
towards just killing each other. And similarly, if you find a baby lying on the street when you
go out for your morning jog tomorrow, you're going to stop and pick it up, right? Even though it
may be a make you late for your next podcast. So evolution gave us these genes that make our own
egoistic incentives more aligned with what's good for the greater group or part of, right?
And then as we got a bit more sophisticated and developed language, we invented gossip,
which is also a fantastic anti-MOLOC, right? Because now it really discourages
liars, moochers, cheaters, because their own incentive now is not to do this, because word
quickly gets around and then suddenly people aren't going to invite them to their dinners anymore
or trust them. And then when we got still more sophisticated and bigger societies,
invented the legal system where even strangers who couldn't rely on gossip and things like this
would treat each other, would have an incentive. Now those guys in the bar fights, even if someone
is so drunk that he actually wants to kill the other guy, he also has a little thought on the
back of his head that, do I really want to spend the next 10 years eating really crappy food in
a small room? I'm just going to chill out. And we similarly have tried to give these incentives
to our corporations by having regulation and all sorts of oversight so that their incentives
are aligned with the greater good. We tried really hard. And the big problem that we're failing now
is not that we haven't tried before, but it's just that the tech is growing much,
is developing much faster than the regulators have been able to keep up. So regulators,
it's kind of comical that European Union right now is doing this AI act. And in the beginning,
they had a little opt-out exception that GPT-4 would be completely excluded from regulation.
Brilliant idea. What's the logic behind that?
Some lobbyists pushed successfully for this. So we were actually quite involved with the
Future Life Institute, Mark Brackel, Risto Uck, Anthony Aguirre and others, you know,
were quite involved with talking to very educating various people involved in this process about
these general purpose AI models coming and pointing out that they would become the laughing
stock if they didn't put it in. So the French started pushing for it, it got put in to the draft
and it looked like all was good. And then there was a huge counter push from lobbyists.
There were more lobbyists in Brussels from tech companies and from oil companies, for example.
And it looked like it might maybe get taken out again. And now GPT-4 happened.
And I think it's going to stay in. But this just shows, you know, MoLoc can be defeated,
but the challenge we're facing is that the tech is generally much faster than what the
policymakers are. And a lot of the policymakers also don't have a tech background. So it's,
you know, we really need to work hard to educate them on what's taking place here.
So we're getting this situation where the first kind of non... So I define artificial
intelligence just as non-biological intelligence. And by that definition,
a company, a corporation is also an artificial intelligence because the corporation isn't,
it's humans, it's a system. If its CEO decides, the CEO of a tobacco company decides one morning
the CEO, he doesn't want to sell cigarettes anymore, they'll just put another CEO in there.
It's not enough to align the incentives of individual people or align individual
computers incentives to their owners, which is what technically iSafety Research is about.
You also have to align the incentives of corporations with the greater good. And
some corporations have gotten so big and so powerful very quickly that in many cases,
their lobbyists instead align the regulators to what they want rather than the other way around.
The classic regulatory capture.
All right. Is the thing that the slowdown hopes to achieve is given enough time to the
regulators to catch up or enough time to the companies themselves to breathe and understand
how to do AI safety correctly? I think both. And I think that the vision of the path to success
I see is first you give a breather actually to the people in these companies, their leadership
who wants to do the right thing and they all have safety teams and so on on their companies,
give them a chance to get together with the other companies and the outside pressure can also help
catalyze that, right? And work out what is it that's what are the reasonable safety requirements
one should put on future systems before they get rolled out. There are a lot of people also
in academia and elsewhere outside of these companies who can be brought into this and
have a lot of very good ideas. And then I think it's very realistic that within six months
you can get these people coming up to here's a white paper, here's where we all think
is reasonable. You know, you didn't just because cars killed a lot of people,
they didn't ban cars, but they got together a bunch of people and decided, you know,
in order to be allowed to sell a car, it has to have a seat belt in it.
There are the analogous things that you can start requiring a future AI systems so that they
are safe. And once this heavy lifting, this intellectual work has been done
by experts in the field, which can be done quickly, I think it's going to be quite easy
to get policymakers to see, yeah, this is a good idea. And it's, you know,
for the companies to fight malloc, they want, and I believe Sam Altman has explicitly called
for this, they want the regulators to actually adopt it so that their competition is going to
abide by it too, right? You don't want to be enacting all these principles and you abide by
them. And then there's this one little company that doesn't sign on to it, and then now they
can gradually overtake you, then the companies will get, be able to sleep, it's secure knowing
that everybody's playing by the same rules. So do you think it's possible to develop guardrails
that keep the systems from, from basically damaging irreparably humanity,
while still enabling sort of the capitalist fueled competition between companies as they
develop how to best make money with this AI? You think there's a balancing that's possible?
Absolutely. I mean, we've seen that in many other sectors where you've had the free market produce
quite good things without causing particular harm. When the guardrails are there and they work,
you know, capitalism is a very good way of optimizing for just getting the same things
on more efficiently. But it was good, you know, and like in hindsight, I've never met anyone
even, even on parties way over on the right in any country who think it was a bad,
thinks it was a terrible idea to ban child labor, for example.
Yeah, but it seems like this particular technology has gotten so good, so fast, become
powerful to a degree where you could see in the near term the ability to make a lot of money.
And to put guardrails, develop guardrails quickly in that kind of context seems to be tricky.
It's not similar to cars or child labor. It seems like the opportunity to make a lot of money here
very quickly is right here before us. So again, there's this cliff?
Yeah, it gets quite scenic. Closer to the cliff, there you go. There's more money there is,
more gold ingots there on the ground, you can pick up or whatever. So you want to drive there
very fast. But it's not in anyone's incentive that we go over the cliff. And it's not like
everybody's in their own car, all the cars are connected together with a chain. So if anyone
goes over, they'll start dragging others down, the others down too. And so ultimately, it's in the
selfish interests also of the people in the companies to slow down when the when you start
seeing the contours of the cliff there in front of you, right? And the problem is that
even though the people who are building the technology and the CEOs, they really get it.
The shareholders and these other market forces, they are people who don't honestly
understand that the cliff is there, they usually don't. You have to get quite into
the weeds to really appreciate how powerful this is and how fast and a lot of people are
even still stuck again in this idea that intelligence in this carbon chauvinism,
as I like to call it, that you can only have our level of intelligence in humans,
that there's something magical about it. Whereas the people in the tech companies
who build this stuff, they all realize that intelligence is information processing of a
certain kind. And it really doesn't matter at all whether the information is processed by carbon
atoms in neurons and brains, or by silicon atoms in some technology we build. So you brought up
capitalism earlier, and there are a lot of people who love capitalism and a lot of people
who really, really don't. And it struck me recently that what's happening with capitalism
here is exactly analogous to the way in which superintelligence might wipe us out. So you know
I studied economics for my undergrad, Stockholm School of Economics, yay.
Well, no, I tell you. So I was very interested in how you could use market forces to just get
stuff done more efficiently, but give the right incentives to market so that it wouldn't do really
bad things. So Dylan Hadfield Menel, who's a professor and colleague of mine at MIT,
wrote this really interesting paper with some collaborators recently,
where they proved mathematically that if you just take one goal that you just optimize for
on and on and on indefinitely, that you think is going to bring you in the right direction.
What basically always happens is in the beginning, it will make things better for you.
But if you keep going at some point, it's going to start making things worse for you again,
and then gradually it's going to make it really, really terrible. So just as a simple,
the way I think of the proof is, suppose you want to go from here
back to Austin, for example, and you're like, okay, yeah, let's just let's go south, but you put in
exactly the right sort of the right direction. Just optimize that south as possible, you get
closer and closer to Austin. But there was always some little error. So you're not going exactly
towards Austin, but you get pretty close. But eventually you start going away again,
and eventually you're going to be leaving the solar system. And they proved it's
beautiful mathematical proof. This happens generally. And this is very important for AI,
because even though Stuart Russell has written a book and given a lot of talks on why it's a
bad idea to have AI just blindly optimize something, that's what pretty much all our systems do.
We have something called the loss function that we're just minimizing or reward function,
we're just maximizing. And capitalism is exactly like that too. We wanted to get stuff done more
efficiently that people wanted. So we introduced the free market. Things got done much more
efficiently than they did in, say, communism. And it got better. But then it just kept optimizing
and kept optimizing. And you got ever bigger companies and ever more efficient information
processing, and now also very much powered by IT. And eventually a lot of people are beginning
to feel, wait, we're optimizing a bit too much, like why did we just chop down half the rain for
us? And why did suddenly these regulators get captured by lobbyists and so on? It's just the
same optimization that's been running for too long. If you have an AI that actually has power
over the world, and you just give it one goal and just keep optimizing that, most likely everybody's
going to be like, yay, this is great in the beginning, things are getting better. But it's
almost impossible to give it exactly the right direction to optimize in. And then eventually
all hay breaks loose, right? Nick Bostrom and others are giving examples that sound quite silly,
like what if you just want to tell it to cure cancer or something? And that's all you tell it.
Maybe it's going to decide to take over entire continents just so we can get more
super computer facilities in there and figure out how to cure cancer backwards. And then you're
like, wait, that's not what I wanted, right? And the issue with capitalism and the issue with
the front of way, I have kind of merged now because the malloc I talked about is exactly
the capitalist malloc that we have built an economy that has optimized for only one thing,
profit, right? And that worked great back when things were very inefficient. And then now it's
getting done better. And it worked great as long as the companies were small enough that
they couldn't capture the regulators. But that's not true anymore, but they keep optimizing.
And now they realize that these companies can make even more profit by building ever more
powerful AI, even if it's reckless, but optimize more and more and more and more.
So this is malloc again, showing up. And I just want to anyone here who has any concerns about
late stage capitalism having gone a little too far, you should worry about
superintelligence because it's the same villain in both cases. It's malloc.
And optimizing one objective function aggressively blindly is going to take us there.
We have to pause from time to time and look into our hearts. And that's why are we doing this?
Am I still going towards Austin or have I gone too far? Maybe we should change direction.
And that is the idea behind the halt for six months. Why six months? That seems like a very
short period. Can we just linger and explore different ideas here? Because this feels like
a really important moment in human history, where pausing would actually have a significant
positive effect. We said six months because we figured the number one pushback we were going to
get in the West was like, but China. And everybody knows there's no way that China is going to catch
up with the West on this in six months. So it's that argument goes off the table. And you can
forget about geopolitical competition and just focus on the real issue. That's why we put this.
That's really interesting. But you've already made the case that even for China, if you actually
want to take on that argument, China too would not be bothered by a longer halt because they
don't want to lose control even more than the West doesn't. That's what I think. That's a really
interesting argument. I have to actually really think about that, which the kind of thing people
assume is if you develop an AGI, that open AI, if they're the ones that do it, for example,
they're going to win. But you're saying no, everybody loses. Yeah, it's going to get better
and better and better. And then kaboom, we all lose. That's what's going to happen.
When lose and win are defined on a metric of basically quality of life for human civilization
and for Sam Altman. My personal guess, and people can quibble with this, is there won't be
any humans. That's it. That's what I mean by lose. We can see in history, once you have some species
or some group of people who aren't needed anymore, doesn't usually work out so well for them.
There were a lot of horses that were used for traffic in Boston and then the car got
invented and most of them got, well, we don't need to go there. And if you look at
humans, right now, why did the labor movement succeed after the industrial revolution? Because
it was needed. Even though we had a lot of mollocks and there was child labor and so on,
you know, the company still needed to have workers. And that's why strikes had power and so on.
If we get to the point where most humans aren't needed anymore, I think it's quite
naive to think that they're going to still be treated well. You know, we say that. Yeah,
they're equal and the government will always, we'll always protect them. But if you look in
practice, groups that are very disenfranchised and don't have any actual power,
usually get screwed. And now, in the beginning, so industrial revolution,
we automated away muscle work. But that got worked out pretty well, eventually, because we
educated ourselves and started working with our brains instead and got usually more interesting,
better paid jobs. But now we're beginning to replace brain work. So we replaced a lot of
boring stuff like we got the pocket calculator, so you don't have people adding multiplying
numbers anymore at work. Fine, there were better jobs they could get. But now GPT-4, you know,
and the stable diffusion and techniques like this, they're really beginning to blow away some
jobs that people really love having. It was a heartbreaking article just post just yesterday,
social media I saw about this guy who was doing 3D modeling for gaming and he
and all of a sudden now they got this new software he just give says prompts and he feels this whole
job that he loves lost its meaning, you know, and I asked GPT-4 to rewrite twinkle twinkle
little star in the style of Shakespeare. I couldn't have done such a good job. It was
really impressive. You've seen a lot of art coming out here, right? So I'm all for
automating away the dangerous jobs and the boring jobs. But I think you hear a lot some
arguments which are too glib sometimes people say, well, that's all that's going to happen. We're
getting rid of the boring, boring, tedious, dangerous jobs. It's just not true. There are a
lot of really interesting jobs that are being taken away now. Journalism is getting going to get
crushed. Coding is going to get crushed. I predict the job market for programmers salaries are going
to start dropping. You know, if you said you can code five times faster, you know, then you need
five times fewer programmers, maybe there will be more output also, but you'll still end up using
fewer program needing fewer programmers than today. And I love coding, you know, I think it's super
cool. So we need to stop and ask ourselves why again, now we're doing this as humans, right?
I feel that AI should be built by humanity for humanity. And let's not forget that it shouldn't
be by malloc for malloc or what it really is now is kind of by humanity for malloc, which doesn't
make any sense. It's for us that we're doing it then. And it would make a lot more sense if we
build, develop, figure out gradually safely how to make all this tech. And then we think about
what are the kind of jobs that people really don't want to have, you know, automate them all away.
And then we ask what are the jobs that people really find meaning in, like maybe
taking care of children in the daycare center, maybe doing art, etc, etc. And even if it were
possible to automate that way, we don't need to do that, right? We built these machines.
Well, it's possible that we redefine or rediscover what are the jobs that give us meaning. So for me,
the thing, it is really sad. Like, I have the time I'm excited, half the time I'm crying,
as I'm generating code, because I kind of love programming. It's an act of creation. You have
an idea, you design it and then you bring it to life and it does something, especially if there's
some intelligence to it does something. It doesn't even have to have intelligence. Printing,
printing, hello world on screen, you made a little machine and it comes to life. And there's a bunch
of tricks you learn along the way, because you've been doing it for many, many years.
And then to see AI, be able to generate all the tricks you thought were special.
I don't know, it's very, it's scary. It's almost painful, like a loss of innocence,
maybe, like maybe when I was younger. I remember before I learned that sugar is bad for you,
you should be on a diet. I remember I enjoyed candy deeply in a way I just can't anymore,
that I know is bad for me. I enjoyed it unapologetically, fully, just intensely. And I just,
I lost that. Now, I feel like a little bit of that is lost for me with program, or being lost
with programming, similar as it is for the 3D modeler, no longer being able to really enjoy
the art of modeling 3D things for gaming. I don't know, I don't know what to make sense of that.
Maybe I would rediscover that the true magic of what it means to be human is connecting with other
humans, to have conversations like this. I don't know, to have sex, to have to eat food,
to really intensify the value from conscious experiences versus like creating other stuff.
You're pitching the rebranding again from Homo sapiens to Homo sentiens, the meaningful experiences.
And just to inject some optimism in this here, so we don't sound like a bunch of gloomers,
we can totally have our cake and eat it. You hear a lot of totally bullshit claims that we
can't afford having more teachers, have to cut the number of nurses. That's just nonsense, obviously.
With anything even quite far short of AGI, we can dramatically improve,
grow the GDP and produce this wealth of goods and services. It's very easy to create a world
where everybody is better off than today, including the richest people can be better off as well.
It's not a zero sum game technology. Again, you can have two countries like Sweden and Denmark
had all these ridiculous wars century after century. And sometimes that Sweden got a little
better off because it got a little bit bigger and then Denmark got a little better off because
Sweden got a little bit smaller. But then technology came along and we both got just
dramatically wealthier without taking away from anyone else. It was just a total win for everyone.
And AI can do that on steroids. If you can build safe AGI, if you can build super intelligence,
basically all the limitations that cause harm today can be completely eliminated.
It's a wonderful possibility. And this is not sci-fi. This is something which is clearly possible
according to the laws of physics. And we can talk about ways of making it safe also.
But unfortunately, that'll only happen if we steer in that direction. That's absolutely not
the default outcome. That's why income inequality keeps going up. That's why the life expectancy
in the US has been going down now. I think it's four years in a row. I just read a heartbreaking
study from the CDC about how something like one-third of all teenage girls in the US
have been thinking about suicide. Those are steps in totally the wrong direction.
And it's important to keep our eyes on the prize here that we have the power now for the first
time in the history of our species to harness artificial intelligence to help us really flourish
and help bring out the best in our humanity rather than the worst of it. To help us
have really fulfilling experiences that feel truly meaningful. And you and I shouldn't sit
here and dictate to future generations what they will be. Let them figure it out. But let's
give them a chance to live and not foreclose all these possibilities for them by just messing
things up. For that, we'll have to solve the AI safety problem. It'll be nice if we can link
on exploring that a little bit. One interesting way to enter that discussion is you tweeted and
Elon replied, you tweeted, let's not just focus on whether GPT-4 will do more harm or good on the
job market, but also whether it's coding skills will hasten the arrival of superintelligence.
That's something we've been talking about. So Elon proposed one thing in their reply saying
maximum truth seeking is my best guess for AI safety. Can you maybe steal man the case for
this objective function of truth and maybe make an argument against it? And in general, what
are your different ideas to start approaching the solution to AI safety?
I didn't see that reply actually. But I really resonate with it because
AI is not evil. It caused people around the world to hate each other much more.
But that's because we made it in a certain way. It's a tool. We can use it for great
things and bad things. And we could just as well have AI systems. And this is part of my vision
for success here, truth seeking AI that really brings us together again. Why do people hate
each other so much between countries and within countries? It's because they each have totally
different versions of the truth. If they all have the same truth that they trusted for good reason,
because they could check it and verify it and not have to believe in some self-proclaimed authority,
right? There wouldn't be nearly as much hate. There'd be a lot more understanding instead.
I think something AI can help enormously with. For example, a little baby step in this direction
is this website called Metaculous, where people bet and make predictions not for money, but just
for their own reputation. It's kind of funny, actually. You treat the humans like you treat
AI as you have a loss function where they get penalized if they're super confident on something
and then the opposite happens. Whereas if you're kind of humble and then you're like,
I think it's 51% chance this is going to happen and then the other happens, you don't get penalized
much. And what you can see is that some people are much better at predicting than others.
They've earned your trust, right? One project that I'm working on right now is the Outgrowth
to Improve the News Foundation together with the Metaculous folks is seeing if we can really scale
this up a lot with more powerful AI, because I would love further to be a really powerful
truth-seeking system that is trustworthy because it keeps being right about stuff.
And people who come to it and maybe look at its latest trust ranking of different pundits and
newspapers, et cetera, if they want to know why someone got a low score, they can click on it
and see all the predictions that they actually made and how they turned out. This is how we do
it in science. You trust scientists like Einstein who said something everybody thought was bullshit
and turned out to be right. You get a lot of trust points and he did it multiple times even.
I think AI has the power to really heal a lot of the rifts we're seeing
by creating trust systems. It has to get away from this idea today with some fact-checking
sites which might themselves have an agenda and you just trust it because of its reputation.
You want to have these systems that are in their trust and that are completely transparent.
This I think would actually help a lot that can help heal the very dysfunctional conversation
that humanity has about how it's going to deal with all its biggest challenges in the world today.
Then on the technical side, another common sort of gloom comment I get from people
who are saying we're just screwed. There's no hope. Things like GPT-4 are way too complicated for a
human to ever understand and prove that they can be trustworthy. They're forgetting that AI can help
us prove that things work. There's this very fundamental fact that in math it's much
harder to come up with a proof than it is to verify that the proof is correct.
You can actually write a little proof checking code. It's quite short but you can as a human
understand and then it can check the most monstrously long proof ever generated even by a
computer and say yeah this is valid. Right now we have this approach with virus checking software
that it looks to see if there's something you should not trust it and if it can prove to
itself that you should not trust that code it warns you. What if you flip this around
and this is an idea I should give credit to Steve on 1004 so that it will only run the code if it
can prove instead of not running it if it can prove that it's not trustworthy if it will only run
and if it can prove that it's trustworthy so it asks the code prove to me that you're going to do
what you say you're going to do and it gives you this proof and you a little proof checker can
check it. Now you can actually trust an AI that's much more intelligent than you are right
because it's its problem to come up with this proof that you could never have found
that you should trust it. So this is the interesting point I agree with you but
this is where Eliezer Yakovsky might disagree with you. His claim not with you but with this idea
his claim is super intelligent AI would be able to know how to lie to you with such a proof.
How to lie to you and give me a proof that I'm going to think is correct? Yeah but it's not me
it's lying to you that's the trick my proof checker. So yes so his general idea is a
super intelligent system can lie to a dumber proof checker. So you're going to have as a system
becomes more and more intelligent there's going to be a threshold where a super intelligent system
would be able to effectively lie to a slightly dumber AI system. Like there's a threat like he
really focuses on this weak AI to strong AI jump where the strong AI can make all the weak
AGI's think that it's just one of them but it's no longer that and that leap is when it runs away.
Yeah I don't buy that argument I think no matter how super intelligent an AI is it's never going
to be able to prove to me that there are only finitely many primes for example.
It just can't and it can try to snow me by making up all sorts of new weird rules of
deduction and say trust me you know the way your proof checker works is too limited and
we have this new hyper math and it's true but then I would just take the attitude okay I'm
going to forfeit some of these supposedly super cool technologies I'm only going to go with the
ones that I can prove with my own trusted proof checker then I don't I think it's fine.
There's still of course this is not something anyone has successfully implemented at this
point but I think I just give it as an example of hope we don't have to do all the work ourselves
right this is exactly the sort of very boring and tedious task is perfect to outsource to an AI
and this is a way in which less powerful and less intelligent agents like us can actually
continue to control and trust more powerful ones.
So build AGI systems that help us defend against other AGI systems?
Well for starters begin with a simple problem of just making sure that the system that you own
or that's supposed to be loyal to you has to prove to itself that it's always going to do the things
that you actually wanted to do right and if it can't prove it maybe it's still going to do it
but you won't run it so you just forfeit some aspects of all the cool things that I can do.
I bet you dollars and donuts can still do some incredibly cool stuff for you.
Yeah there are other things too that we shouldn't sweep under the rug like
not every human agrees on exactly where what direction we should go with humanity right?
Yes and you've talked a lot about geopolitical things on this on your podcast to this effect you
know but I think that shouldn't distract us from the fact that there are actually a lot of things
that everybody in the world virtually agrees on that hey you know like having no humans on the planet
in a in a near future let's not do that right? You look at something like the United Nations
Sustainable Development Goals some of them are quite ambitious and basically all the countries
agree US, China, Russia, Ukraine you all agree so instead of quibbling about the little things we
don't agree on let's start with the things we do agree on and get them done instead of being so
distracted by all these things we disagree on that Malak wins because frankly Malak going wild
now it feels like a war on life playing out in front of us if you just look at it from space you
know we're on this planet beautiful vibrant ecosystem now we start chopping down big parts of
even though nobody most people thought that was a bad idea always start doing ocean acidification
wiping out all sorts of species oh now we have all these close calls we almost had a nuclear war
and we're replacing more and more of the biosphere with non-living things we're also replacing in
our social lives a lot of the things which we're so valuable to humanity a lot of social
interactions now are replaced by people staring into their rectangles right and I I'm not a
psychologist I'm out of my depth here but I suspect that part of the reason why teen suicide
and suicide in general in the US the record-breaking levels is actually caused by again AI technologies
and social media making people spend less time with with actual and actually just human interaction
we've all seen a bunch of good-looking people in restaurants staring into the rectangles instead
of looking into each other's eyes right so that's also a part of the war on life that that we're
we're replacing so many really life-affirming things by technology we're we're putting technology
between us the technology that was supposed to connect us is actually distancing us ourselves
from each other and um and and then we're giving ever more power to things which are not alive
these large corporations are not living things right they're just maximizing profit
there I want to win them war on life I think we humans together with all our
fellow living things on this planet will be better off if we can remain in control
over the non-living things and make sure that they they work for us I really think it can be done
can you just linger on this um maybe high level philosophical disagreement with Eliezer Yadkowski
in this the hope you're stating so he is very sure he puts a very high probability
very close to one depending on the day he puts it at one
that AI is going to kill humans that there's just he does not see a trajectory which it doesn't
end up with that conclusion what what trajectory do you see that doesn't end up there and maybe
can you can you see the point he's making and and can you also see a way out
mm-hmm first of all I tremendously respect Eliezer Yadkowski and his his thinking
second I do share his view that there's a pretty large chance that we're not going to
make it as humans there won't be any humans on the planet and not a distant future and that
makes me very sad you know we just had a little baby and I keep asking myself you know is um
how old is even gonna get you know and and um I asked myself okay it feels I said to my wife
recently it feels a little bit like I was just diagnosed with some sort of cancer which has some
you know risk of dying from and some risk of surviving you know
except this is a kind of cancer which would kill all of humanity so I completely take seriously
his his um his concerns I think um but I don't absolutely you don't think it's hopeless I think
there is a there is um first of all a lot of momentum now for the first time actually
since the many many years that have passed since since I and many others started warming
warning about this I feel most people are getting it now I I uh just talking to this guy in the gas
station they're our house the other day my and he's like I think we're getting replaced and I think
in it so that's positive that they're they're finally we're finally seeing this reaction which
is the first step towards solving the problem uh second uh I really think that this this vision
of only running ai's really if the stakes are really high they can prove to us that they're safe
it's really just virus checking in reverse again I I think it's scientifically doable
I don't think it's hopeless um we might have to forfeit some of the technology that we could get
if we were putting blind faith in our ai's but we're still going to get amazing stuff
do you envision a process with a proof checker like something like gpt4 gpt5 will go through a
process of rigorous interrogation no no I think it's so hopeless that's like trying to prove
there about five spaghetti okay what I think well how the the whole the vision I have for success
is instead that you know just like we human beings were able to look at our brains and
and distill out the key knowledge Galileo when his dad threw him an apple when he was a kid he
was able to catch it because his brain could and his funny spaghetti kind of way you know predict
how parabolas are going to move his conaman system one right but then he got older and it's like wait
this is a parabola it's it's uh y equals x squared I can distill this knowledge out and
today you can easily program it into a computer and it can simulate not just that but how to get
tamaras and so on right I envision a similar process where we use the the amazing learning
power of neural networks to discover the knowledge in the first place but we don't stop with a black
box and and use that we then do a second round of ai where we use automated systems to extract
out the knowledge and see what is it look what are the insights it's had okay and it's and then we
we put that knowledge into a completely different kind of architecture or programming language or
whatever that's that's made in a way that it can be both really efficient and also is more amenable
to to very formal verification that's that's my vision I'm not saying sitting here saying I'm
confident 100 sure that it's going to work you know but I don't think it's chance it's certainly
not zero either and it will certainly be possible to do for a lot of really cool AI applications
that we're not using now so we can have a lot of the fun that we're excited about
if we if we do this we're gonna need a little bit of time and that's why it's good to pause and
put in place requirements and one more thing also I think you know someone might think well
zero percent chance we're gonna survive let's just give up right that's very dangerous
because there's no more guaranteed way to fail than to convince yourself that it's impossible
and not try you know any if you you know when you study history and military history the first thing
you learn is that that's how you do psychological warfare you persuade the other side that it's
hopeless so they don't even fight and then of course you win right let's not do this psychological
warfare on ourselves and say there's a hundred percent probability we're all gonna we're all
screwed anyway it's sadly I do get that a little bit sometimes from from uh some young people who
are like so convinced that we're all screwed that they're like I'm just gonna play game play
computer games and do drugs and because we're screwed anyway right it's important to keep the
hope alive because it actually has a causal impact and makes it more likely that we're gonna succeed
it seems like the people that actually build solutions to a problem seemingly impossible
to solve problems are the ones that believe yeah they were the ones who are the optimists yeah and
it's like uh it seems like there's some fundamental law to the universe where fake it till you make
it kind of works like believe yeah it's possible and it becomes possible yeah was it Henry Ford who
said that if you can if you tell yourself that it's impossible it is so let's not make that mistake
and this is a big mistake society is making you know I think all in all everybody's so gloomy
and the media are also very biased towards if it bleeds it leads and gloom and doom right so
most visions of the future we have are dystopian which really demotivates people
we want to really really really focus on the upside also to give people the willingness to fight
for it and for AI you and I mostly talked about gloom here again but let's not remember not forget
that you know we have probably both lost someone we really cared about some disease
that we were told was incurable well it's not there's no law and physics saying we had to die
of that cancer or whatever of course you can cure it and there are so many other things where
that we with a human intelligence have also failed to solve on this planet which AI could also very
much help us with right so if we can get this right just be a little more chill and slow down
a little bit so we get it right it's mind-blowing how awesome our future can be right we talked a
lot about stuff on earth it can be great but even if you really get ambitious and look up
into the skies right there's no reason we have to be stuck on this planet for the rest of
the remain for billions of years to come we totally understand now as the laws of physics
let life spread out into space to other solar systems to other galaxies and flourish for billions
of billions of years and this to me is a very very hopeful vision that really motivates me to
to fight then coming back to the end something you talked about again you know this the struggle
how the human struggle is one of the things that also really gives meaning to our lives
if there's ever been an epic struggle this is it and isn't it even more epic if you're the underdog
if most people are telling you this is gonna fail it's impossible right and you persist
and you succeed right and that's what we can do together as a species on this one a lot of
pundits are ready to count us out both in the battle to keep AI safe and becoming
a multi-planetary species yeah and they're they're the same challenge if we can keep AI safe that's
how we're gonna get multi-planetary very efficiently i have some sort of technical questions about
how to get it right so one idea uh that i'm not even sure what the right answer is to is uh should
systems like gpt4 be open sourced in the whole arm part can you make the can you see the case for either
i think the answer right now is no i think the answer early on was yes
so we could bring in the all the wonderful create the thought process of everybody on this
but asking should we open source gpt4 now is just the same as if you say well is it good
should we open source new how to build really small nuclear weapons should we open source
how to make bio weapons should we open source how to make a new virus that kills 90% of everybody
who gets it of course we shouldn't so it's already that powerful it's already that powerful that we
have to respect the power of the systems we've built the knowledge that you get
from open sourcing everything we do now might very well be powerful enough that people looking at that
can use it to build the things that you're really threatening again let's get it remember open AI is
gpt4 is a baby AI baby sort of baby proto almost a little bit agi and according to what microsoft's
recent paper said right it's not that that we're scared of what we're scared about is people taking
that who are who might be a lot less responsible than the company that made it right and uh just
going to town with it that's why we want to it's it's an information hazard there are many things
which um yeah are not open sourced right now in society for a very good reason like how do you make
certain kind of very powerful toxins out of stuff you can buy and home depot you know
you don't open source those things for a reason and this is really no different
so uh and i'm saying that i have to say it's a little it feels a bit weird a bit in a way a bit
weird to say it because MIT is like the cradle of the open source movement yeah and i love open
source in general power to the people let's say but um there's always gonna be some stuff
that you don't open source and you know it's just like you don't open source so we have a three month
old baby right when he gets a little bit older we're not gonna open source to him all the most
dangerous things you can do in the house yeah right but it does it's a weird feeling because
this is one of the first moments in history where there's a strong case to be made not to open
source software this is when the software has become yeah too dangerous yeah but it's not the
first time that we didn't want to open source a technology technology yeah
is there something to be said about how to get the release of such systems right like GPT-4 and
GPT-5 so open AI went through a pretty rigorous effort for several months you could say it could
be longer but nevertheless it's longer than you would have expected of trying to test the system
to see like what are the ways it goes wrong to make it very difficult for people somewhat
difficult for people to ask things how do I make a bomb for $1 or how do I say I hate a certain
group on Twitter in a way that doesn't get me blocked from Twitter banned from Twitter those
kinds of questions yeah so you basically use the system to do harm yeah is there something you
could say about ideas you have it's just on looking having thought about this problem of AI safety
how to release such system how to test such systems when you have them inside the company
yeah so
a lot of people say that the two biggest risks from large language models are
it's spreading disinformation harmful information of various types
and second being used for offensive cyber weapon
I think those are not the two greatest threats they're very serious threats and it's wonderful
that people are trying to mitigate them a much bigger elephant in the room is how is this going
to disrupt the economy in a huge way obviously and maybe take away a lot of the most meaningful jobs
and an even bigger one is the one we spent so much time talking about here that
that this becomes the bootloader for the more powerful AI
right code connected to the internet manipulate humans yeah and before we know when we have
something else which is not at all a large language model it looks nothing like it but
which is way more intelligent and capable and has goals and that's the that's the elephant
in the room and and obviously no matter how hard any of these companies have tried
they that's not something that's easy for them to verify with large language models and the only
way to be really lower that risk a lot would be to not let for example train not never let it read
any code not train on that and not put it into an API and not not give it access to so much
information about how to manipulate humans so but that doesn't mean you still can't make a lot
a ton of money on them you know we're we're gonna just watch now this coming year right
Microsoft is rolling out the new office suite where you go into Microsoft Word and give it a
prompt that it writes the whole text for you and then you edit it and then you're like oh give me
a powerpoint version of this and it makes it and now take the spreadsheet and blah and
you know the all of those things I think are you can debate the economic impact of it and
whether society is prepared to deal with this disruption but those are not the things which
that's not the elephant of the room that keeps me awake at night for wiping out humanity
and I think that's the biggest misunderstanding we have a lot of people think they were scared of
like the automatic spreadsheets that's not the case that's not what Eliezer was freaked out about
either is there in terms the actual mechanism of how AI might kill all humans so something you've
been outspoken about you've talked about a lot is it autonomous weapon systems so the use of AI
in war is that one of the things that still you carry a concern for as these systems become
more and more powerful and carry a concern for it not that all humans are going to get killed by
slaughterbots but rather just this express route into Orwellian dystopia where it becomes much
easier for very few to kill very many and therefore it becomes very easy for very few to dominate very
many right if you want to know how I could kill all people just ask yourself we humans have driven
a lot of species extinct how do we do it you know we were smarter than them
usually we didn't do it even systematically by going around one on one one after the other and
stepping on them or shooting them or anything like that we just like chop down their habitat
because we needed it for something else in some cases we did it by putting more carbon dioxide
in the atmosphere because of some reason that those animals didn't even understand and now
they're gone right so if if you're in AI and you just want to figure something out then you decide
you know we just really need them this space here to build more compute facilities you know
if that's the only goal it has you know we are just the sort of accidental roadkill along the
way and you could totally imagine yeah maybe this oxygen is kind of annoying because it
caused more corrosion so let's get rid of the oxygen and good luck surviving after that you
know I I'm not particularly concerned that they would want to kill us just because
that would be like a goal in itself you know when we
driven number we've driven a number of the elephant species extinct right it wasn't
because we didn't like elephants what the basic problem is you just don't want to give
you don't want to see the control over your planet to some other more intelligent
entity that doesn't share your goals it's that simple so which brings us to another key challenge
which AI safety researchers have been grappling with for a long time like how do you make AI
first of all understand our goals and then adopt our goals and then retain them as they get smarter
right and all three of those are really hard right like a human child first they're just not smart
enough to understand our goals they can't even talk and then eventually they're teenagers and
understand our goals just fine but they don't share yeah but there is unfortunately a
magic phase in the middle where they're smart enough to understand our goals and malleable
enough that we can hopefully with good parenting and teach them right from wrong and instead good
goal is still good goals in them right and so those are all tough challenges with computers
and then you know even if you teach your kids good goals when they're little they might outgrow
and that's a challenge for machines to keep improving so these are a lot of hard challenges
we're up for but I don't think any of them are insurmountable the fundamental reason why
eliezer looked so depressed when I last saw him was because he felt it just wasn't enough time
oh not that it was unsolvable it's just not enough time he was hoping that humanity was going to
take this threat more seriously so we would have more time yeah and now we don't have more time
that's why the open letter is calling for more time
but even with time the ai alignment problem
seems to be really difficult oh yeah but it's also the most worthy problem the most important
problem for humanity to ever solve because if we solve that one lex that align the eye can help
us solve all the other problems because it seems like it has to have constant humility
about his goal constantly question the goal because as you optimize towards a particular goal
and you start to achieve it that's when you have the unintended consequences all the things you've
mentioned about so how do you enforce and code a constant humility as your ability become better
better better better better steward professor steward russell at berkeley who's also one of the
driving forces behind this letter he uh has a whole research program about this
I think of it as ai humility exactly although he calls it inverse reinforcement learning
and other nerdy terms but it's about exactly that instead of telling the ai here's his goal
go optimize the the bejesus out of it you tell it okay do what I want you to do but I'm not going
to tell you right now what it is I want you to do you need to figure it out so then you give the
incentives to be very humble and keep asking you questions along the way is this what you really
meant is this what you wanted and oh this the other thing I tried didn't work seemed like it
didn't work out right should I try it differently what's nice about this is it's not just philosophical
mumbo jumbo it's theorems and technical work that with more time I think it can make a lot of
progress and there are a lot of brilliant people now working on ai safety and we just not we just
need to give them a bit more time but also not that many relative to the scale of the problem no
exactly there there should be at least as just like every university worth its name has some cancer
research going on in its biology department right every university that's computer that does computer
science should have a real effort in this area and it's nowhere near that this is something I hope
is changing now thanks to the gpt4 right so I think if there's a silver lining to what's happening
here even though I think many people would wish it would have been rolled out more carefully
is that this might be the wake-up call that humanity needed to really
stop the stop fantasizing about this being 100 years off and stop fantasizing about this being
completely controllable and predictable because it's so obvious it's it's not predictable you know
why is it that open that that I think it was gpt chat gpt tried to persuade a journalist
or was it gpt4 to divorce his wife you know it was not because the the engineers have built it
it was like let's put this in here and and screw a little bit with people they hadn't predicted it
at all they built the giant black box trained to predict the next word and got all these emergent
properties and oops it did this you know I think this is a very powerful wake-up call and anyone
watching this who's not scared I would encourage them to just play a bit more with these these
tools they're out there now like gpt4 and um
it's a wake-up call it's first step once you've woken up uh then gotta slow down a little bit
the risky stuff to give a chance to all everyone who's woken up to to catch up with us on the
safety front you know what's interesting is you know MIT that's computer science but in general
but let's just even say computer science curriculum how does the computer science
curriculum change now you mentioned you mentioned programming yeah like why would you be when I was
coming up programming as a prestigious position like why would you be dedicating crazy amounts of
time to become an excellent programmer like the nature of programming is fundamentally changing
the nature of our entire education system is completely torn on its head I has anyone been
able to like load that in and like think because it's really turning I mean some english professors
or english teachers are beginning to really freak out now yeah right they give an essay assignment
and they get back all these fantastic pros like this is the style of Hemingway and then they realize
they have to completely rethink and even you know just like we stopped teaching writing script
is that what you're saying english yeah handwritten yeah yeah when when everybody started typing you
know like so much of what we teach our kids today yeah I mean that's uh everything is changing and
it's changing very it's changing very quickly and so much of us understanding how to deal with the
big problems of the world is through the education system and if the education system is being turned
on its head then what what's next it feels like having these kinds of conversations is essential
to try to figure it out and everything's happening so rapidly uh I don't think there's even you're
speaking of safety what the broad AI safety defined I don't think most universities have
courses on AI safety no it's a philosophy yeah and like I'm an educator myself so it pains me to
see this say this but I feel our education right now is like completely obsoleted by what's happening
you know you put a kid into first grade and then you're envisioning like and then they're gonna come
out of high school 12 years later and you've already pre-planned now what they're gonna learn
when you're not even sure if there's gonna be any world left to come out to
clearly you need to have a much more opportunistic education system that keeps adapting itself very
rapidly as society re-adapts the the skills that were really useful when the curriculum was written
I mean how many of those skills are gonna get you a job in 12 years I mean seriously if we just
linger on the GPT-4 system a little bit you kind of hinted at it especially talking about
the importance of consciousness in the in the human mind with homo sentience
do you think GPT-4 is conscious I love this question so let's define consciousness first
because in my experience like 90% of all arguments about consciousness are allowed to the two people
arguing having totally different definitions of what it is and they're just shouting past each other
I define consciousness as subjective experience right now I'm experiencing colors and sounds and
emotions you know but does a self-driving car experience anything that's the question about
whether it's conscious or not right other people think you should define consciousness differently
fine by me but then maybe use a different word for it or they can I'm gonna use consciousness
for this at least so um but if people hate the yeah so is GPT-4 conscious does GPT-4 have
subjective experience short answer I don't know because we still don't know what it is that gives
this wonderful subjective experience that is kind of the meaning of our life right because meaning
itself the feeling of meaning is a subjective experience joy is a subjective experience love
is a subjective experience we don't know what it is I've written some papers about this a lot of
people have Giulio Tononi professor has stuck his neck out the farthest and written down actually
very bold mathematical conjecture for what's the essence of conscious information processing
he might be wrong he might be right but we should test it he postulates that consciousness
has to do with loops in the information processing so our brain has loops information can go round
and round in computer science nerd speak you call it a recurrent neural network where some of the
output gets fed back in again and with his mathematical formalism if it's a feed forward
neural network where information only goes in one direction like from your eye retina into
the back of your brain for example that's not conscious so he would predict that your retina
itself isn't conscious of anything or a video camera now the interesting thing about GPT-4
is it's also just one way flow of information so if Tononi is right and GPT-4 is a very intelligent
zombie they can do all this smart stuff but isn't experiencing anything and this is
both a relief in that you don't have if it's true in that you don't have to feel guilty about
turning off GPT-4 and wiping its memory whenever a new user comes along I wouldn't like if someone
used that to me neuralized me like in men in black but it's also creepy that you can have
very high intelligence perhaps then it's not conscious because if we get replaced by machines
and why is it sad enough that humanity isn't here anymore because I kind of like humanity
but at least if the machines were conscious I could be like well but there are descendants and
maybe we they have our values and there are children but if if Tononi is right and it's all
these are all transformers that are not in the sense of the of Hollywood but in the sense of
these one-way direction neural networks so they're all the zombies that's the ultimate zombie
apocalypse now we have this universe that goes on with great construction projects and stuff but
there's no one experiencing anything that would be like the ultimate depressing future so I actually
think as we move forward with building more advanced AI we should do more research on
figuring out what kind of information processing actually has experience because I think that's
what it's all about and I completely don't buy the dismissal that some people some people would say
well this is all bullshit because consciousness equals intelligence right it's obviously not true
you can have a lot of conscious experience when you're not really
accomplishing any goals at all you're just reflecting on something and you can sometimes
have things doing things that are quite intelligent probably without being being conscious
but I also worry that we humans won't
will discriminate against AI systems that clearly exhibit consciousness that we will not allow AI
systems to have consciousness we'll come up with theories about measuring consciousness that we'll
say this is a lesser being and this is why I worry about that because maybe we humans will create
something that is better than us humans in the in the way that we find beautiful which is they
have a deeper subjective experience of reality not only are they smarter but they feel deeper
and we humans will hate them for it as we as human history is shown they'll be the other
we'll try to suppress it they'll create conflict they'll create war all of this I worry about this
too are you saying that we humans sometimes come up with self-serving arguments no we would never
do that would be well that's the danger here is uh even in this early stages we might create something
beautiful yeah and will erase its memory I was horrified as a kid when someone started boiling
boiling lobsters like oh my god that's so cruel and some grown up there back in Sweden
and say oh it doesn't feel pain I'm like how do you know that oh scientists have shown that
and then there was a recent study where they show that lobsters actually do feel pain
when you boil them so they banned lobster boiling in Switzerland now you have to kill them in a
different way first so presumably that scientific research boiled down to someone asked the lobster
to start a survey so and we do the same thing with cruelty to farm animals also all these
self-serving arguments for why they're fine and yeah so we should certainly be watchful I think
step one is just be humble and acknowledge that consciousness is not the same thing as intelligence
and I believe that consciousness still is a form of information processing where it's really
information being aware of itself in a certain way and let's study it and give ourselves a
little bit of time and I think we will be able to figure out actually what it is that causes
consciousness and then we can make probably unconscious robots that do the boring jobs
that we would feel immoral to give to machines but if you have a companion robot taking care
of your mom or something like that she would probably want it to be conscious right so
the emotions that seem to display aren't fake all these things can be done in a good way if we
give ourselves a little bit of time and don't run and take on this challenge is there something you
could say to the timeline that you think about about the development of AGI depending on the day
I'm sure that changes for you but when do you think there'll be a really big leap in intelligence
where you definitively say we have built AGI do you think it's one year from now five years from
now 10 20 50 what's your gut say honestly for the past decade I've deliberately given very long
timelines because I didn't want to fuel some kind of stupid malloc race yeah but I think that cat
has really left the bag now and I think it might be very very close I don't think the
Microsoft paper is totally off when they say that there are some glimmers of AGI it's not AGI yet
it's not an agent there's a lot of things it can't do but I wouldn't bet very strongly
against it happening very soon that that's why we decided to do this open letter because you
know if there's ever been a time to pause you know it's today there's a feeling like this GPT-4
is a big transition into waking everybody up to the effectiveness of the system and so the next
version will be big yeah and if that next one isn't AGI maybe the next next one will and there are
many companies trying to do these things and the basic architecture of them is not some sort of
super well kept secret so this is this is a time to a lot of people have said for many years that
there will come a time when we want to pause a little bit that time is now you have spoken about
and thought about nuclear war a lot over the past year we've seemingly have come
closest to the precipice of nuclear war than at least in my lifetime yeah what do you learn
about human nature from that it's our old friend Malak again it's really scary to see it where
America doesn't want there to be a nuclear war Russia doesn't want to be a global nuclear war
either we know we both know that it's just being others if we just try to do it it both sides try
to launch first it's just another suicide race right so why are we why is it the way you said
that this is the closest we've come since 1962 in fact i think we've come closer now than even the
Cuban Missile Crisis it's because of Malak you know you you have these other forces on one hand
you have the west saying that we have to drive Russia out of Ukraine it's a matter of pride
and we've staked so much on it that it would be seen as a huge loss of the credibility of the
west if we don't drive Russia out entirely of the Ukraine and on the other hand you have Russia
who has and you have the Russian leadership who knows that if they get completely driven out of
Ukraine you know it might it's not just going to be very humiliating for them but they might
it often happens when countries lose wars that things don't go so well for their leadership either
like you remember when Argentina invaded the Falkland Islands the the military junta that
ordered that right people were cheering on the streets at first when they took it
and then when they got their butt kicked by the British you know what happened to those guys
they were out and i believe those were still alive or in jail now right so so you know the
Russian leadership is entirely cornered where they know that just getting driven out of Ukraine is
not an option and um so this to me is a typical example of Malak you you have these incentives
of the two parties where both of them are just driven to escalate more and more right if Russia
starts losing in the conventional warfare the only thing they can do is to back against the
war is to keep escalating and but and the west has put itself in the in the situation now we're
sort of already committed that the dry rush out so the only option the west has is to
call Russia's bluff and keep sending in more weapons this really bothers me because Malak
can sometimes drive competing parties to do something which is ultimately just really bad
for both of them and uh you know what makes me even more worried is not just that it's
difficult to see an ending a quick peaceful ending to this tragedy that doesn't involve some
horrible escalation but also that we understand more clearly now just how horrible it was gonna
be there was an amazing paper that was published in Nature Food this uh August
by some of the top researchers who've been studying nuclear for a long time and what they
basically did was they combined climate models with food and agricultural models so instead of
just saying yeah you know it gets really cold blah blah blah they figured out actually how
many people would die in the different different countries and it's uh it's pretty mind blowing
you know so basically what happens you know is the the thing that kills the most people is not
the explosions it's not the radioactivity it's not the EMP mayhem it's not the rampaging mobs
foraging food no it's it's it's the fact that you get so much smoke coming up from the burning
cities into the stratosphere that spreads around the earth from the jet streams so
in typical models you get like 10 years or so where it's just crazy cold and spread
during the first year or after the the war and their models the temperature drops in in Nebraska
and in the Ukraine bread baskets you know by like 20 Celsius or so if I remember
no yeah 20 30 Celsius depending on where you are 40 Celsius in some places which is you know
40 Fahrenheit to 80 Fahrenheit colder than what it would normally be so you know I'm not good at
farming but if it's knowing if it drops low freezing pretty much most days in July and
that's not good so they worked out they put this into their farming models
and what they found was really interesting the countries that get the most hard hit are the
ones in the northern hemisphere so in in the US and and one model they had about 99% of all Americans
starving to death in Russia and China and Europe also about 99% 98% starving to death
so you might be like oh it's kind of poetic justice that both the Russians and the Americans
99% of them have to pay for it because it was their bombs that did it but you know that doesn't
particularly cheer people up in Sweden or other random countries that have nothing to do with it
right and um it uh I think it hasn't entered the mainstream uh not understanding very much just
like how bad this is most people especially a lot of people in decision-making positions
still think of nuclear weapons as something that makes you powerful
scary powerful they don't think of it as something where uh yeah just
to within a percent or two you know we're all just just gonna starve to death and um
and starving to death is is um the worst way to die as Haldemore is all all the families in
history show the torture involved in that probably brings out the worst in people also
when when people are desperate like this it's not so some people I've heard some people say that
if that's what's gonna happen they'd rather be at round zero and just get vaporized you know
but uh so but I think people underestimate the risk list because they they
they aren't afraid of malloc they think oh it's just gonna be because humans don't want this so
it's not gonna happen that's the whole point the malloc that things happen that nobody wanted
and that applies to nuclear weapons and that applies to agi
exactly and it applies to some of the things that people have gotten most upset with capitalism
for also right where everybody was just kind of trapped you know it it's not to see if some
company does something that causes a lot of harm and not that the CEO is a bad person
but she or he knew that you know that the other all the other companies were doing this too so
malloc is um as a formable foe I hope wish someone would make him would make good movies so we
could see who the real enemy is so we don't because we're not fighting against each other
malloc makes us fight against each other that's small that's what malloc superpower is
the hope here is any kind of technology or the mechanism that lets us instead realize
that we're fighting the wrong enemy right no it's such a fascinating battle it's not us
versus them it's us versus it yeah yeah we are fighting malloc for human survival yeah we as a
civilization have you seen the movie needful things it's a steven king novel I love steven king
and uh max von sudo of swedish actors playing the guys it's brilliant exactly I just thought I hadn't
thought about that until now but that's the closest I've seen to a a movie about malloc I don't want
to spoil the film for anyone who wants to watch it but basically it's about this guy who turns out
you can interpret him as the devil or whatever but he doesn't actually ever go around and kill
people or torture people will go burning coal or anything he makes everybody fight each other
makes everybody hate fear each other hate each other and then kill each other so that that's
the movie about malloc you know love is the answer that seems to be um one of the ways to fight
malloc is by um compassion by seeing the common humanity yes yes and to not sound so we don't
sound like like what's a kumbaya tree hugger is here right we're not just saying love and
peace man we're trying to actually help people understand the true facts about the other side
and feel the compassion because the truth makes you more compassionate right
so I that's why I really like using AI for truth and for truth seeking technologies can
that can as a result you know get us more love than hate and and even if you can't get love you
know settle for settle for some understanding which already gives compassion if someone is like you
know I really disagree with you Lex but I can see why you're where you're coming from you're not a
bad person who needs to be destroyed but I disagree with you and I'm happy to have an
argument about it you know that's a lot of progress compared to where we are 2023 in the public
space wouldn't you say if we solve the AI safety problem as we've talked about and then you max
tag mark who has been talking about this uh for many years get to sit down with the AGI with the
early AGI system on a beach with a drink uh what what what kind of what would you ask her
what kind of question would you ask what would you talk about something so much smarter than you
would be would you be a new rate we're gonna get me with a really zinger of a question
that's a good one would you be afraid to ask some questions no so I'm not afraid of the truth
I'm very humble I know I'm just a meatbag you with all these flaws you know but yeah I
I have the I we talked a lot about homo sentience I've really already tried that for a long time
with myself just so that is what's really valuable about being alive for me is that I have these
meaningful experiences it's not that I'm have what I'm good at this or good at that or whatever
because there's so much I suck at and so you're not afraid for the system to show you just how
dumb you are no no in fact my son reminds me of that you could find out how dumb you are in terms
of physics how little how little we humans understand I'm cool with that I think I think um
so I can't waffle my way out of this question it's a fair one it was tough
I think given that I'm a really really curious person that's really the defining part of who I
am I'm so curious
uh I have some physics questions I love to love to understand I have some questions about
consciousness about the nature of reality I would just really really love to understand also
I could tell you one for example that I've been obsessing about a lot recently
so I believe that so supposed to know me is right and suppose there are some information
processing systems that are conscious and some they're not suppose you can even make reasonably
smart things like GPT-4 they're not conscious but you can also make them conscious here's
the question that keeps me awake at night is it the case that the unconscious zombie systems that
are really intelligent are also really efficient so they're really inefficient so that when you
try to make things more efficient we still naturally be a pressure to do they become conscious
I'm kind of hoping that that's correct and I do you want me to give you a hand away the argument
for it please you know like in my lab again every time we look at how how these large language
models do something we see that they do in really dumb ways and you could you could make it make it
better if you we have loops in our computer language for a reason the code would get way
way longer if you weren't allowed to use them it's more efficient to have the loops and
in order to have self-reflection whether it's conscious or not right even an operating system
knows things about itself right you need to have loops already right so I think this I'm waving
my hands a lot but I suspect that the most efficient way of implementing a given level of
intelligence has loops in it self-reflection and will be conscious isn't that great news yes if
it's true it's wonderful because then we don't have to fear the ultimate zombie apocalypse
and I think if you look at our brains actually our brains are part zombie and part conscious
when I open my eyes I immediately take all these pixels that hit my retina right and like oh
that's Lex but I have no freaking clue of how I did that computation it's actually quite complicated
right it was only relatively recently we could even do it well with machines right you get a bunch
of information processing happening in my retina and then it goes to the lateral geniculate nucleus
my thalamus and the vision the area v1 v2 v4 and the fusiform face area here that Nancy can
wish her at MIT invented and blah blah blah blah and I have no freaking clue how that worked right
it feels to me subjectively like my conscious module just got a little email say
face facial processing fit task complete it's Lex yeah and I'm gonna just go with that right
so this fits perfectly with Tanone's model because this was all one way information
processing mainly and it turned out for that particular task that's all you needed and it
probably was kind of the most efficient way to do it but there are a lot of other things
that we associate with higher intelligence and planning and and so on and so forth where
you kind of want to have loops and be able to ruminate and self reflect and introspect and so on
where my hunch is that if you want to fake that with a zombie system that just all goes one way
you have to like unroll those loops and it just really really long and it's much more inefficient
so I'm actually hopeful that AI if in the future we have all these very sublime and interesting
machines that do cool things and are lined with us that they will at least they will also have
consciousness for the kind of these things that we do that great intelligence is also
correlated to great consciousness or a deep kind of consciousness yes so that's a happy thought
for me because the zombie apocalypse really is my worst nightmare of all it would be like
adding insult to injury not only did we get replaced but we freaking replaced
ourselves by zombies like how dumb can we be that's such a beautiful vision and that's actually
a provable one that's one that we humans can intuit and prove that those two things are correlated
as we start to understand what it means to be intelligent and what it means to be conscious
which these systems early AGI like systems will help us understand and I just want to say one more
thing which is super important most of my colleagues when I started going on about consciousness tell
me that it's all bullshit and I should stop talking about it I hear a little inner voice
from my father and from my mom saying keep talking about it because I think they're wrong
and and and the main way to convince people like that that they're wrong if they say that
consciousness is just equal to intelligence is to ask them what's wrong with torture
why are you against torture if it's just about you know
these these particles moving this way around on that way and there is no such thing as subjective
experience what's wrong with torture I mean do you have a good comeback to that no it seems like
suffering suffering imposed on other humans is somehow deeply wrong in a way that intelligence
doesn't quite explain and if someone tells me well you know it's just an illusion consciousness
whatever you know I like to invite them the next time they're having surgery to do it without
anesthesia like what is anesthesia really doing if you have it you can have it local
anesthesia when you're awake I had that when they fixed my shoulder I was super entertaining
uh what was that that it did it just removed my subjective experience of pain it didn't change
anything about what was actually happening in my shoulder right so if someone says that's all
bullshit skip the anesthesia that's my advice this is incredibly central it could be fundamental
to whatever this thing we have going on here it is fundamental because we're we what we feel
is so fundamental is suffering and joy and pleasure and meaning and
that's all those are all subjective experiences there and let's not those are the elephant in
the room that's what makes life worth living and that's what can make it horrible if it's just
those are suffering so let's not make the mistake of saying that that's all bullshit and let's not
make the mistake of uh not instilling the AI systems with that same thing that makes us
special yeah max uh it's a huge honor that you will sit down to me the first time
on the first episode of this podcast it's a huge honor you sit down with me again
and talk about this what I think is uh the most important topic the most important problem that
we humans have to face and hopefully solve yeah well the honor is all mine and I'm so grateful
to you for making more people aware of this fact that humanity has reached the most important fork
in the road ever in its history and let's turn in the correct direction thanks for listening
to this conversation with max tagmark to support this podcast please check out our sponsors in
the description and now let me leave you with some words from frank harbert history is a constant
race between invention and catastrophe thank you for listening and hope to see you next time

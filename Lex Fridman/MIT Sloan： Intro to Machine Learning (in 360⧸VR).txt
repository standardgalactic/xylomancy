The video you're watching now is in 360. Resolution is not great, but we wanted to
try something different. So if you're on a desktop or laptop, you can pan
around with your mouse or if you're on a phone or tablet, you should be able to
just move your device to look around. Of course, it's best viewed with a VR
headset. The video that follows is a guest lecture on machine learning that I
gave an MIT Sloan course on the business of artificial intelligence. The lecture
is non-technical and intended to build intuition about these ideas amongst the
business students in the audience. The room was a half circle, so we thought why
not film the lecture in 360. We recorded a screencast of the slides and pasted it
into the video so that the slides are more crisp. Let me know what you think and
remember it's an experiment. So this course is talking about the broad context,
the impact of artificial intelligence, the global. There's global, which is the
global impact of artificial intelligence. There's the business, which is when you
have to take these fun research ideas that I'll talk about today. A lot of them
are cool, untoyed examples. When you bring them to reality, you face real
challenges, which is what I would like to really highlight today. That's the
business part. When you want to make real impact, when you're going to make these
technologies a reality. So I'll talk about how amazing the technology is for a
nerd like me, but also talk about how when you take that into the real world,
what are the challenges you face. So machine learning, which is the technology
at the core of artificial intelligence. We'll talk about the promise, the
excitement that I feel about it, the limitations. We'll bring it down a little
bit. What are the real capabilities of technology? We're for the first time
really as a civilization exploring the meaning of intelligence. It is if you
pause for a second and just think, you know, maybe many of you want to make
money out of this technology. Many of you want to save lives, help people, but
also on the philosophical level, we get to explore what makes us human. So while
I'll talk about the low level technologies, also think about the incredible
opportunity here, we get to almost psychoanalyze ourselves by trying to
build versions of ourselves in the machine. All right, so here's the open
question. How powerful is artificial intelligence? How powerful is machine
learning that lies at the core of artificial intelligence? Is it simply a
helpful tool, a special purpose tool to help you solve simple problems? Which
is what it currently is. Currently, machine learning, artificial intelligence
is a way if you can formally define the problem, you can formally define the
tools you're working with, you can formally define the utility function
where you want to achieve with those tools. As long as you can define those
things, we can come up with algorithms that can solve them. As long as you have
the right kind of data, which is what I'll talk about, data is key. And the
question is, into the future, can we break past this very narrow definition of
what machine learning can give us, which is solve specific problems, to something
bigger, to where we approach the general intelligence that we exhibit as human
beings. When we're born, we know nothing and we learn quickly from very little
data. The right answer is we don't know. We don't know what are the limitations
of technology. What kind of machine learning are there? There are several
flavors. The first two is what's really, the first is what's achieved success
today. Supervised learning. What I'm showing here on the left of the slide is
the teachers, is the data that is fed to the system, and on the right is the
students, which is the system itself from machine learning. So there's supervised
learning. Whenever everybody talks about machine learning today, for the most
part, they're referring to supervised learning, which means every single piece
of data that is used to train the model is seen by human eyes, and those human
eyes with an accompanying brain label that data in a way that makes it useful
to the machine. This is critical, because that's one, the blue box, the human. It's
really costly. So whenever every single piece of data that's used to train the
machine needs to be seen by a human, you need to pay for that human. And second,
you're limited to just the time. There's the amount of data necessary to label
what it means to exist in this world is humongous. Augmented supervised learning
is when you get machine to really help you a little bit. There's a few tricks
there, but it's still only tricks. It's still the human is at the core of it. And
the promise of future research that we're pursuing, that I'm pursuing, and
perhaps in the applications, if we get to discuss, or some of the speakers here
get to discuss, they're pursuing in semi-supervised and reinforcement
learning, where the human starts to play a smaller and smaller role in how much
they get to annotate, they have to annotate the data. And the dream of the
sort of wizards of the dark arts of deep learning are all excited about in
supervised learning. That has very few actual successes in application in the
real world today, but it is the idea that you can build a machine that doesn't
require a human teacher, a human being to teach you anything. It fills us
artificial intelligence researchers with excitement. There's a theme here.
Machine learning is really simple. The learning system in the middle, there's a
training stage where you teach it something. All you need is some data,
input data, and you need to teach it the correct output for that input data. So
you have to have a lot of pairs of input data and correct output. There'll be a
theme of cats throughout this presentation. So if you want to teach a
system difference between a cat and a dog, you need a lot of images of cats, and
you need to tell it that this is a cat. This bounding box here in the image is a
cat. You have to give it a lot of images of dogs and tell it, okay, in these
pictures there are dogs. And then there's a spelling mistake on the second
stage is the testing stage. When you actually give it new input data, it's
never seen before, and you hope that it has given for cat versus dog enough data
to guess is this new image that I've never seen before, a cat or a dog. Now
one of the open questions you want to keep in mind is what in this world can we
not model in this way? What activity, what task, what goal? I offer to you
that there's nothing you can't model in this way. So let's think about what in
terms of machine learning can be, so let's start small. What can be modeled in
this way? First on the bottom of the slide left is one-to-one mapping where
the input is an image of a cat and the output is a label that says cat or dog.
You can also do one-to-many where the image, the input is an image of a cat and
the output is a story about that cat, a captioning of the image. You can first of
all, you can do the other way, many-to-one mapping where you give it a
story about a cat and it generates an image. There's many-to-many, this is
Google Translate. We translate a sentence from one language to another and
there's various flavors of that. Again, same theme here. Input data provided with
correct output and then let it go into the wild where it runs on input data
hasn't seen before to provide guesses. And it's as simple as this, whatever you can
convert into one of the following four things, numbers, vector of numbers, so a
bunch of numbers, a sequence of numbers, where the temporal dynamics matters, so
like audio, video, where the sequence, the ordering matters, or a sequence of
vector numbers, just a bunch of numbers. If you can convert into numbers and I
propose to you that there's nothing you can't convert into numbers. If you can
convert into numbers, you can have a system learn to do it. And the same thing
with the output. Generate numbers, vectors, and numbers, sequence of numbers, or
sequence of vectors and numbers. First, is there any questions at this point?
We have a lot of fun slides to get through, but I'll pause every once in a while to
make sure we're on the same page here. So what kind of input are we talking about?
Just to fly through it. Images, so faces, or medical applications for looking
at scans of different parts of the body to determine if they're to diagnose any
kind of medical conditions. Texts, so conversations, your texts, article, block
posts for sentiment analysis, question and answering, so you ask it a question
where the output you hope answers. Sounds, so voice recognition, anything you
can tell from audio. Time series data, so financial data, stock market, you can use
it to predict anything you want about the stock market, including whether to buy
or sell. If you're curious, it doesn't work quite well as a machine learning
application. Physical world, so cars, or any kind of object, any kind of robot
that exists in this world. So location of where I am, location of where other
things are, the actions of others, that could be all the input. All of it can be
converted to numbers. And the correct output, same thing. Classification, a bunch
of numbers. Classification is saying it's a cat or a dog. Regression is saying to
what degree I turn the steering wheel. Sequence is generating audio, generating
video, generating stories, captioning, text, images, generating anything you can
think of as numbers. And at the core of it is a bunch of data agnostic machine
learning algorithms. There's traditional ones, nearest neighbors, Naive Bay,
support vector machines. A lot of them are limited, and I'll describe how. And
then there's neural networks. There's nothing special and new about neural
networks. And I'll describe exactly the very subtle thing that is powerful, that's
always been there all along. And certain things have now been able to unlock that
power about neural networks. But it's still just the flavor of a machine
learning algorithm. And the inspiration for neural networks, as Jonathan showed
last time, is our human brain. It's perhaps why the media, perhaps why the
hype is captivated by the idea of neural networks, is because you immediately jump
into this feeling like, because there's this mysterious structure to them that
scientists don't understand, artificial neural networks, I'm referring to, and
the biological ones. We don't understand them. And the similarity captivates our
minds and we think, well, this approach is perhaps as limitless as our own human
mind. But the comparison ends there. In fact, artificial neuron, there are
artificial neural networks, are much simpler computational units. At the core
of everything is this neuron. This is a computational unit that does two very
simple operations. On the left side, it takes a set of numbers as inputs. It
applies weights to those inputs, sums them together, applies a little bias, and
provides an output, somewhere between zero and one. So you can think of it as a
computational entity that gets excited when it sees certain inputs and gets
totally turned off when it gets other kinds of inputs. So maybe this neuron with
a 0, with a 0.7, 0.6, 1.4 weights, it gets really excited when it sees pictures of
cats and totally doesn't care about dogs. Some of us are like that. So that's the
job of this neuron is to detect cats. Now, what the way you build an artificial
neural network, the way you release the power that I'll talk about in the
following slides about the applications, what can be achieved, is just stacking a
bunch of these together. Think about it. This is an extremely simple
computational unit. So you need to sort of pause whenever we talk about the
following slides and think that there's a few slides that I'll show that say neural
networks are amazing. I want you to think back to this slide that everything is
built on top of these really simple addition operations with a simple
nonlinear function applied at the end. Just a tiny math operation. We stack them
together in a feedforward way so there's a bunch of layers and when people talk
about deep neural networks it means there's a bunch of those layers and then
there's recurrent neural networks that are also a special flavor that's able to
have memory. So as opposed to just pushing input into output directly it's
also able to do stuff on the inside in a loop where it remembers things. This is
useful for natural language processing, for audio processing, whenever the
sequence is not, the length of the sequence is not defined. Okay, slide number
one in terms of neural networks are amazing. This is perhaps for the math
nerds. But also I want you to use your imagination. There's a universality to
neural networks. It means that the simple computational unit on the left is an
input and the right is the output of this network with just a single hidden layer.
It's called a hidden layer because it sits there in the middle of the input and
the output layers. A single hidden layer with some number of nodes can represent
any function. Any function. That means anything you want to build in this world.
Everyone in this room can be represented with a neural network with a single
hidden layer. So the power, and this is just one hidden layer, the power of these
things is limitless. The problem of course is how do you find the network, so how
do you build a network that is as clever as many of the people in this room. But
the fact that you can build such a network is incredible, is amazing. I want
you to think about that. And the way you train a network, so it's born as a blank
slate. Some random weights assigned to the edges. Again, a network is represented.
The numbers at the core, the parameters at the core of this network are the numbers
on each of those arrows, each of those edges. And you start knowing nothing. This
is a baby network. And the way you teach it something, unfortunately, currently, as
I said, in a supervised learning mechanism, you have to give it pairs of input and
output. You have to give it pictures of cats and labels on those pictures saying
that they're cats. And the basic fundamental operation of learning is when you
compute the measure of an error and you back propagate it to the network. What I
mean, everything's easier with cats. I apologize. I apologize. Too many cats. And
so the input here is a cat. And the neural network we trained, it's just guessing. It
doesn't know. It's guessing cat. Well, it happens to be right. So we have to, this is
the measure of error. Yes, you got it right. And you have to back propagate that error.
You have to reward the network for doing a good job. And all you do, what do I mean
by reward? There's weights on each of those edges. And so the node, the individual neurons
that were responsible, back to that cat neuron, that cat neuron needs to be rewarded for
seeing the cat. So you just increase the weights on the neurons that were associated
with producing the correct answer. Now you give it a picture of a dog and the neural
network says cat. Well, that's an incorrect answer. So no, there's a high error. It needs
to be back propagated through the network. So the weights that are responsible with
classifying this picture as a cat need to be punished. They need to be decreased. Simple.
And you just repeat this process over and over. This is what we do as kids when we're
first learning. For the most part, we're also supervised learning machines in the sense
that we have our parents and we have the environment, the world, that teaches about what's correct
and what's incorrect. And we back propagate this error and reward through our brain to
learn. The problem is, as human beings, we don't need too many examples. And I'll talk
about some of the drawbacks of these approaches. We don't need too many examples. You fall
off your bike once or twice and you learn how to ride the bike. Unfortunately, neural
networks need tens of thousands of times when they fall off the bike in order to learn
how to not do it. That's one of the limitations. And one key thing I didn't mention here is
when we refer to input data, it's, when we refer to input data, we usually refer to
sensory data, raw data. We have to represent that data in some clever way, in some deeply
clever way, where we can reason about it, whether it's in our brains or in the neural
network. And a very simple example here to illustrate why representation of data matters.
So the way you represent the data can make the discrimination of one class from another,
a cat versus dog, either incredibly difficult or incredibly simple. Here is a visualization
of the same kind of data in Cartesian coordinates and polar coordinates. On the right, you can
just draw a simple line to separate the two. What you want is a system that's able to learn
the polar coordinate representation versus the Cartesian representation automatically.
And this is where deep learning has stepped in and revealed the incredible power of this
approach, which deep learning is the smallest circle there, is a type of representational
learning. Machine learning is the bigger second to the biggest. So this class is about the
biggest circle, AI, includes robotics, includes all the fun things that are built on learning.
And I'll discuss while machine learning, I think we'll close this entire circle into
one. But for now, AI is the biggest circle, then a subset of that is machine learning
and a smaller subset of that is representation learning. So deep learning is not only able
to say, given a few examples of cats and dogs to discriminate between a cat and a dog, it's
able to represent what it means to be a cat. So it's able to automatically determine what
are the fundamental units at the low level and the high level, talking about this very
Plato, what it means to represent a cat from the whiskers to the high level shape of the
head to the fuzziness and the deformable aspects of the cat, not a cat expert, but I hear these
are the features of a cat versus that are essential to discriminate between a cat and
a dog. Learning those features as opposed to having to have experts, this is the drawback
of systems that Jonathan talked about from the 80s and 90s, where you have to bring in
experts for any specific domain that you try to solve, you have to have them encode that
information. Deep learning, this is simply the only big difference between deep learning
and other methods is that it learns the representation for you. It learns what it means to be a cat.
Nobody has to step in and help it figure out what that cats have whiskers and dogs don't.
What does this mean? The fact that it can learn these features, these whisker features, is as
opposed to having 5 or 10 or 100 or 500 features that are encoded by brilliant engineers with
PhDs, it can find hundreds of thousands, millions of features automatically, hundreds of millions
of features. Stuff that can't be put into words or described, in fact it's one of the
limitations in neural networks is they find so many fundamental things about what it means
to be a cat that you can't visualize what it really knows. It just seems to know stuff
and it finds that stuff automatically. What does this mean? The critical thing here is
because it's able to automatically learn those hundreds of millions of features,
it's able to utilize data. It doesn't start, the diminishing returns don't hit until, well,
we don't know when they hit. The point is with the classical machine learning algorithms,
you start hitting a wall when you have tens of thousands of images of cats. With deep learning,
you get better and better with more data. Neural networks are amazing slide two.
Here's a game, a simple arcade game where there's two paddles, they're bouncing a ball back and
forth. Okay, great. You can figure out an artificial intelligence agent that can play this game.
It can, not even that well, just kind of, it kind of learns to do all right and eventually win.
Here's the fascinating thing. With deep learning, as opposed to encoding the position of the paddles,
the position of the ball, having an expert in this game, there's many, come in and encode
the physics of this game. The input to the neural network is the raw pixels of the game.
So, it's learning in the following way. You give it an evolution of the game. You give it a bunch
of pixels. Pixels are, you know, images are built up of pixels. They're just numbers from 0 to 256.
So, there's this array of numbers that represent each image and then you give it several tens of
thousands of images that represent a game. So, you have the stack of pixels and stack
of images that represent a game. And the only thing you know, this giant stack of numbers,
the only thing you know is at the end, you won or lost. That's it. So, based on that,
you have to figure out how to play the game. You know nothing about games. You know nothing
about colors or balls or paddles or winning or anything. That's it. So, this is, why is this
amazing? That it even works and it works, it wins. It's amazing because that's exactly what we do
as human beings. This is general intelligence. So, I need you to pause and think about this.
We'll talk about special intelligence and the usefulness and it's, okay, there's cool tricks
here and there that we can do to get you an edge on your high frequency trading system.
But this is general intelligence. General intelligence is the same intelligence we use
as babies when we're born. What we get is an input, sensory input of image sensory input. Right now,
all of us, most of us are seeing, hearing, feeling with touch and that's the only input we get. We
know nothing. And with that input, we have to learn something. Nobody is pre-teaching us stuff.
And this is an example of that, a trivial example, but one of the first examples where this is truly
working. I'm sorry to linger on this, but it's a fundamental fact. The fact that we have systems
that, and now all perform human beings in these simple arcade games is incredible. This is the
research side of things. But let me step back. These, again, the takeaways. That previous slide
is why I think machine learning is limitless in the future. Currently, it's limited.
Again, the representation of the data matters. And if you want to have impact, we currently
can only tackle the small problems. What are those problems? Image recognition. We can classify,
given the entire image of a leopard, of a boat, of a mite, with pretty good accuracy
of what's in that image. That's image classification. What else? We can find exactly where in that
image each individual object is. That's called image segmentation. Again, the same, the process is
the same. The learning system in the middle, a neural network, as long as you give it a set of
numbers as input, and the correct set of labels as output, it learns to do that for data hasn't
seen the best. Let me pause a second and maybe if you have any questions, does anyone have any
questions about the techniques of neural networks? Yes?
So that's a great question. In a couple of slides, I'll get to it exactly.
So the data representation, I'll elaborate in a little bit, but loosely, the data representation
is for a neural network is in the weights of each of those arrows that connect the neurons.
That's where the representation is. So I'll show to really clarify that example of
what that means. The Cartesian versus polar coordinates is just a very simple visualization
of the concept. But you want to be able to represent the data in an arbitrary way,
where there's no limits to the representation. It could be highly nonlinear, highly complex.
Any other questions?
So I have a couple of slides almost asking this question, because there's no good answers.
But one could argue, and I think somebody in the last class brought up that, you know,
is machine learning just pattern recognition? It's possible that reasoning, thinking,
is just pattern recognition. And I'll describe sort of an intuition behind that.
So we tend to respect thinking a lot, because we've recently, as human beings,
learned to do it. In our evolutionary time, we think that it's somehow special from,
for example, perception. We've had visual perception for several orders of magnitude
longer in our evolution as a living species. We've started to learn to reason, I think,
about 100,000 years ago. So we think it's somehow special from the same kind of mechanism we use
for seeing things. Perhaps it's exactly the same thing. So perception is pattern recognition.
Perhaps reasoning is just a few more layers of that. That's the hope. But it's an open question.
Yes, that's a great question. There's been very few breakthroughs in your networks since,
through the AI winters that we've discussed, through a lot of excitement,
in spurts, and even recently, there's been a very few algorithmic innovations. The big gains came
from compute, so improvements in GPU and better, faster computers. You can't underestimate the
power of community. So the ability to share code and the internet, ability to communicate together
through the internet and work on code together, and then digitization of data, so ability to have
large data sets easily accessible and downloadable, all of those little things. But I think in terms
of the future of deep learning and machine learning, it all rides on compute, I think,
meaning continued, bigger, and faster computers. That doesn't necessarily mean Moore's law
in making small and small chips. It means getting clever in different directions, massive
parallelization, coming up with ways to do super efficient, power efficient implementations in
neural networks and so on. So let me just fly through a few examples of what we can do with
machine learning just to give you a flavor, I think, in future lectures as possible. We'll
discuss with different speakers, different specific applications, really dig into those.
So we can, as opposed to working with just images, you can work with videos and segment
those, I mentioned image segmentation, we do video segmentation, so through video segment,
the different parts of a scene that's useful to a particular application. Here in driving,
you can segment the road from cars and vegetation
and lane markings. You can also, this is a subtle but important point, yes.
Just go back to that one slide. How do they see the light? I mean, it's such a critical piece of,
you know, the more I listen to you and read your stuff, it seems like this critical, these very
small pieces of information that we just, we know are important, like there is a red light,
right, or the light, I have to stop, I have to slow down. How does it filter that out and pick out
that? So this, which is, it's got to be 100% reliable on that, right?
Oh, hard question. So the question was, how do you detect the traffic light and lights?
So how do we do this, human beings? First of all, let's start there. The way we do it is by
the knowledge we bring to the table. So we know what it means to be on the road,
there's a lot of the huge network of knowledge that you come with. And so that makes the perception
problem much easier. This is pure perception. You take an image and you separate different parts
based purely on tiny patterns of pixels. So first it finds all the edges and it learns that
traffic lights have certain kinds of edges around them and then zoom out a little bit.
They have a certain collection of edges that make up this black rectangle type shape. So
it's all about shapes. It kind of build up knowing this, this shape structure of things.
But it's a purely perception problem. And one of the things I argue is that if it's purely a
perception approach, and you bring no knowledge to the table about the physics of the world,
the three dimensional physics and the temporal dynamics, that you are not going to be able to
successfully achieve near 100% accuracy on some of these systems. So that's exactly the right
question. For all of these things, think about how you as a human being would solve these problems
and what is lacking in the machine learning approach. What data is lacking in the machine
learning approach in order to achieve the same kind of results, the same kind of reasoning
required that you would use as a human. So there is also image detection. Image detection,
which means it's a subtle but important point. The stuff I mentioned before, image classification
is given an image of a cat. You don't find the cat. You say this image is of a cat or not.
And then detection or localization is when you actually find where in the image that is.
That problem is much harder but also doable with machine learning with deep neural networks.
Now, as I said, input's outputs can be anything. The input can be video. The output can be video.
And you can do anything you want with these videos. You can colorize the video. You can add,
take an old black and white film and produce color images.
Again, in terms of having an impact in the world using these applications, you have to think,
this is a cool demonstration, but how well does it actually work in the real world?
Translation, whether that's from text to text or image to image, you can translate here dark
chocolate from one language to another. It's class, global business of artificial intelligence.
There's a reference below there. You can go and generate your own text. You can generate the
writing of the act of generating handwriting. You can type in some text and given different styles
that it learns from other handwriting samples, it can generate any kind of text using handwriting.
Again, the input is language. The output is a sequence of writing
of pen movements on the screen. You can complete sentences. This is kind of a fun one where if
you start, so you can generate language. And you can generate language where you start,
you feed the system some input first. So in black there, it says life is. And then have the neural
network complete those sentences. Life is about kids. Life is about the weather. There's a lot
of knowledge here, I think, being conveyed. And you can start the sentence with the meaning of life
is. The meaning of life is literary recognition, true for us academics, or the meaning of life is
the tradition of ancient human production, also true. But these are all generated by a computer.
You can also capture, this has become very popular recently, is caption generation,
given as input as an image. The output is a set of text that captures the content of the image.
You find the different objects in the image. That's a perception problem. And once you find
the different objects, you stitch them together in a sentence that makes sense. You generate a
bunch of sentences and classify which sentence is the most likely to fit this image. And you can
do so certainly in the, I tried to avoid mentioning too, driving too much because it is
my field that is what I'm excited about. But then the moment I start talking about driving,
it'll all be about driving. So, but I should mention, of course, that deep learning is critical to
driving applications for both the perception and what is really exciting to us now is
the end to end, the end to end approach. So whenever you say end to end in any application
what that means is you start from the very raw inputs that the system gets and you produce the
very final output that's expected of the system. So as opposed to in the cell driving car case,
as opposed to breaking a car down into each individual components of perception, localization,
mapping, control, planning, and just taking the whole stack and just ignoring all the
super complex problems in the middle and just taking the external scene as input
and as output produced steering and acceleration braking commands. And so in this way, taking
this input as the image of the external world, in this case in a Tesla, we can generate steering
commands for the car. Again, input, a bunch of numbers that's just images, output, a single
number that gives you the steering of the car. Okay, so let's step back for a second and think
about what can't we do with machine learning. We talked about, you can map numbers to numbers.
Let's think about what we can't do. At the core of artificial intelligence in terms of
making an impact on this world is robotics. So what can't we solve in robotics and artificial
intelligence with the machine learning approach? And let's break down what artificial intelligence
means. Here's a stack starting at the very top is the environment, the world that you operate in.
There's sensors that sense that world. There is feature extraction and learning from that data
and there's some reasoning, planning, and effectors are the ways you manipulate the world.
What can't we learn in this way? So we've had a lot of success as Jonathan talked about in the
history of AI with formal tasks, playing games, solving puzzles. Recently, we're having a lot
of breakthroughs with medical diagnosis. We're still, sorry, we're still struggling, but are very
excited about in the robotic space with more mundane tasks of walking, of basic perception,
of natural language written and spoken. And then there is the human tasks which are perhaps
completely out of reach of this pipeline at the moment is cognition, imagination,
subjective experience. So high level reasoning, not just common sense, but high level
human level reasoning. So let's fly through this pipeline. There's sensors, cameras, LiDAR, audio,
there's communication that flies to the air or wired or wireless or wired. I am you measuring
the movement of things. So that's the way you think about it. That's the way as human beings and
as any kind of system that you design, you measure the world. You don't just get an API to the world.
You need to somehow measure aspects of this world. So that's how you get the data. So that's how you
convert the world into data you can play with. And once you have the data, this is the representation
inside. You have to convert that raw data, raw pixels, raw audio, raw LiDAR data. You have to
convert that into data that's useful for the intelligence system, for the learning system
to use to discriminate between one thing and another.
For vision, that's finding edges, corners, object parts and entire objects. There's the machine
learning that I've talked about. There's different kinds of mapping of the representation that you've
learned to an actual outputs. There is, once you have this, so you have this idea of, and this goes
to maybe a little bit of Simon's question, is reasoning. This is something that's out of reach
of machine learning at the moment. This is going to your question. Then we can build a world-class
machine learning system for taking an image and classifying that it's a duck. I wonder if this will
work. Wake you up. So we could take, this is well studied, exceptionally well studied problem, we could
take audio sample of a duck and tell that it's a duck. In fact, what species of bird? It's incredible
how much research there is in bird species classification. We can look at video and we
could tell that we can do actual recognition that it's swimming, but we can't do with learning now
is reason that if it looks like a duck, it swims like a duck and quacks like a duck, it's very likely
to be a duck. This is the reasoning problem. This is the task that I personally am obsessed with
and that I hope that machine learning can close. And then there is the planning action and the
effectors. So this is another place where machine learning has not had many strides.
There's mechanical issues here that are incredibly difficult. The degrees of freedom with all the
actuators involved with all the just just the ability to localize every part of yourself in this
dynamic space where things are constantly changing when there's degrees of uncertainty,
when there's noise. Just that basic problem is exceptionally difficult.
So let me just pose this question. We talked about what machine learning can do with the cats
and the duck. We could do that. Given representation, it could predict what's in the image.
But one of the open questions is, and deep learning has been able to do the feature extraction,
the representation learning. This is the big breakthrough that everybody's excited about.
Can it also reason? These are the open questions. Can it reason? Can it do the planning and action?
And as human beings do, can it close the loop entirely from sensors to effectors?
So learn not only the brain, but the way you sense the world and the way you affect the world.
So the question was about the pawn game. Thank you.
Talk to it for a little longer. It doesn't get punished when it doesn't detect the ball.
This is the beautiful thing. It gets punished only at the very end of the game for losing the
game and gets rewarded for winning the game. So it knows nothing about that ball and it learns
about that ball. That's something you really sit and think about. Because as human beings,
imagine if you're playing with a physical ball, how do you learn what a ball is? You get hurt by it,
you squeeze it, you throw it, you feel the dynamics of it, the physics of it, and
nobody tells you about what a ball is. You're just using the raw sensory input.
We take it for granted, and maybe this is what I can end on, is this is what something Jonathan
brought up. As we take the simplicity of this task for granted, because we've had eyes,
we broadly speaking as living species on planet earth, these eyes have been evolved for 540
million years. So we have 540 million years of data. We've been walking for close to that
by petal mammals. We have been thinking only very recently, so 100,000 years versus 100 million
years. And that's why we can't, some of these problems that we're trying to solve, you can't
take for granted how actually difficult they are. So for example, this is the Marvex Paradox
that Jonathan brought up, is that the easy problems are hard. The things we think are easy,
actually really hard. This is the state of the art robot on the right playing soccer,
and that was the state of the art human on the left playing soccer. And
I'll give it a second.
The question was, there's a fundamental difference between the way we train neural networks and the
way we've trained biological neural networks through evolution by discarding through natural
selection a bunch of the neural networks that didn't work so well. So first of all, the process
of evolution is, I think, not well understood. Meaning, sorry, the role, careful here,
the role of evolution in the evolution of our cognition, of our intelligence. I don't know if
that's, so this is an open question. So maybe clarify this point, is neural networks,
artificial neural networks are fixed for the most part in size. This is exactly right. It's like
a single human being that gets to learn. We don't have mechanisms of modifying or evolving those
neural networks yet. Although you could think of researchers as doing exactly that. You have
grad students working on different neural networks and the ones that don't do a good job don't get
promoted and get a good, you know, there is a natural selection there. But other than that,
yes, it's an open question. It's a fascinating one. So Lex is going to come back. He's not
available next week, but he's going to come back a week after. So we can pick up many of these
points here. Are there any last final takeaways you want to emphasize?
Stay tuned and keep your head up because the future, I believe, is really promising.
And the slides will be made available for sure.
I think a lot of the explorations of what it means to build an intelligent machine
has been in sci-fi movies. We're now beginning to actually make it a reality. This is Space
Odyssey to keep with that theme in the previous lecture that we had. This is, as opposed to the
dream-like monolith view when the astronaut is gazing out into the open sky at the stars,
we're going to look at the practice of AI today and how we go, if you're familiar with the movie,
when this new technology appeared before our eyes and we're full of excitement,
how we transfer that into actual practical impact on our lives. To quickly review what
we talked about last time, I presented the technology and asked the question of whether
this technology merely serves a special purpose to answer specific tasks that can be formalized
or whether it can be through the process of transferring the knowledge learned on one domain
be generalizable to where an intelligent system that's trained in a small domain can be used to
achieve general intelligent tasks like we do as human beings. This is kind of the stack of
artificial intelligence going from all the way up to the top of the environment, the world,
the sensors, the sensor data, the intelligent system, the way it perceives this world. Then
once you have this, you convert the world into some numbers, you're able to extract some representation
of that world and this is where machine learning starts to come into play. Then there's the part
where I will raise it again today is can machine learning be doing the following steps too that
we can do very well as human beings is the reasoning step. You can tell the difference
in a cat and a dog, but can you now start to reason about what it means to be alive,
what it means to be a cat with living creature and what it means to be this kind of physical
object or this kind of physical object and take what's called common sense, things we take for
granted start to construct models of the world through reasoning. Descartes, I think therefore I
am. We want our neural networks to come up with that on their own and once you do that
action you'll go right back into the world and you start acting in that world. So the question is
can machine learning, can this be learned from data or does do experts need to encode the knowledge
of reasoning, the knowledge of actions, the set of actions. That's kind of the question,
the open question that I raise. It continues throughout the talk today. And so as we start to
think about how artificial intelligence especially machine learning as it relies itself through
robotics gets to impact the world we start thinking about what are the easy problems and what are the
hard problems and it seems to us that vision and movement walking is easy because we've been
doing it for millions of years hundreds of millions of years and thinking is hard, reasoning is hard.
I propose to you that it's perhaps because we've only been doing it for a short time
and so think we're quite special because we're able to think. So we have to kind of question
of what is easy and what is hard because when we start to develop some of these systems
and you start to realize that all of these problems are equally hard. So the problem of walking that
we take for granted, the actuation and the physical, the ability to recognize where you are
in the physical space to sense the world around you to deal with the uncertainty of the perception
problem and then so all of these robots by the way this is for the most recent DARPA challenge
which MIT was also part of and so what are these robots doing? They don't have any,
they only have sparse communication with human beings on the periphery. So most of the stuff
they have to do autonomously like get inside a car, this is an MIT robot unfortunately,
that they have to get in the car and the hardest task they have to get out of the car.
That's walking. So this kind of raises to you a very real aspect here. You want to build applications
that actually work in the real world and that's the first challenge and opportunity here.
The many of the technologies we talked about currently crumble under the reality of our world
when we transfer them from a small data set in the lab to the real world. For the computer vision
is perhaps one of the best illustrations of this. Computer vision is the task as we talked about
of interpreting images and so when you, there's been a lot of great accomplishments on interpreting
images, cats or just dogs. Now when you try to create a system like the Tesla vehicle that I've
often that we work with and I always talk about is it's a vision-based robot, right,
as radar for basic obstacle avoidance but most of the understanding of the world comes from a
single monocular camera. Now they've expanded the number of cameras but for the most time
there's been a hundred thousand vehicles driving on the roads today with a single,
essentially a single webcam. So when you start to do that you have to perform all of these
extraction of texture, color, optical flow, so the movement through time, temporal dynamics of
the images, you have to construct these patterns, construct the understanding of objects and entities
and how they interact and from that you have to act in this world and that's all based on this
computer vision system. So it's no longer cats versus dogs, it's detection of pedestrians
or the wrong classification, the wrong detection is the difference in life and death.
So let's look at cats where things are a little more comfortable. So computer vision and I would
like to illustrate to you why this is such a hard task. We talked about we've been doing it for 500
million years so we think it's easy. Computer vision is actually incredible so all you're getting
with your human eyes is you're getting essentially pixels in. There's light coming into your eyes
and all you're getting is the reflection from the different surfaces in here of light
and there's perception, there's sensors inside your eyes converting that into numbers.
It's really very similar to this. Numbers in the case of what we use with computers, RGB images
where the individual pixels are numbers from 0 to 255 so 256 possible numbers and there's just a
bunch of them and that's all we get. We get a collection of numbers where they're spatially
connected. The ones that are close together are part of the same object so cat pixels are all
connected together. That's the only thing we have to help us but the rest of it is just numbers,
intensity numbers and we have to use those numbers to classify what's in the image
and if you really think about it this is a really difficult task. All you get is these numbers.
How the heck are you supposed to form a model of the world with which you can
detect pedestrians with really 99.99999% accuracy because these pedestrians
are these cars, a cyclist in the car context or any kind of applications you're looking at.
Even if your job is in the factory floor to detect the defective gummy bears that are flying
past at like 100 miles an hour your task is you don't want that bad gummy bear to get by
that your product and the brand will be damaged. However serious or not serious your application is
what you have to be you have to have a computer vision system that deals with all of these aspects
viewpoint variation scale variation no matter the size of the object is still the same object
the no matter the viewpoint from which the area you look at that object is still the same object.
The lighting that moves we have lighting consistently here because we're indoors
but when you're outdoors or you're moving the scene is moving the lighting the complexity
of the lighting variations is incredible from the illumination to just the movement of the different
objects in the scene. Now that we've had these conversations I think about this every time I
drive I think about you and this point and how hard it is to see these things and particularly
when I'm driving at night and particularly when it's twilight and the light is changing I think
you know almost every time I drive there's one or two things that I see that are really that I'm
drawing like 200 million years in order to be able to figure out it's not it's a guy who's open his
car door and I can't see him but I can just see the light doesn't look quite right on that side of
the road and I somehow I know in my mind it's a person but it seems like a almost impossible
problem for the machines to get right with sufficient accuracy. I will argue that the
pure perception task is too hard that you come to the table as human beings with all this huge
amount of knowledge that you're not actually interpreting all the complex lighting variations
that you're seeing you actually know enough about the world enough about your commute home enough
about the way the kinds of things you would see in this world about Boston about the way pedestrians
move the certain light of day you bring all that to the table that makes the perception task doable
and that's one of the big missing pieces in the technology as I'll talk about that's the open
problem of machine learning is how to bring all that knowledge first of all build that knowledge
and then bring that knowledge to the table as opposed to starting from scratch every time
and so cats the problem is cats okay so the to me occlusion for most of the computer vision
community this is one of the biggest challenges and it really highlights
how far we are from being able to reason about this world occlusions are when what an occlusion
is is when the objects you're trying to detect something about classify the object detect the
object the object is blocked partially by another object in front of them this is something that
you think is trivial perhaps you don't even really think about it because we we reason in a three
dimensional way but the occlusion aspect is is makes makes perception incredibly difficult
so we have to design think about this so this image is converted into numbers and we for the task
of detecting is there a cat in this image yes or no you have to be able to reason about this image
with that object in the scene most of us are able to very easily detect that there's a cat in this
image we're able to detect that there is a cat in this image now think about this there's a single
eye and there's an ear so you have to think about what is it part of our brain that allows us to
understand to suppose that with some high degree of accuracy that there's a cat here in this picture
I mean the degree of occlusion here is immense and so I promised so this is for most of you
some of you will think this is in fact a monkey eating a banana but I would venture to say that
most of us are able to tell it's nevertheless a cat you watch this for hours and so let me give
you another this is kind of a paper that's often cited or a set of papers to illustrate how difficult
computer vision is how thin the line that we're walking with all of these impressive results
that we've been able to show recently in the machine learning community in this case
for deep neural networks are easily fooled paper the seminal paper at this point
shows that when you apply a network trained on image net so basically on detecting cats versus
dogs or different categories in inside images if you're you can find an arbitrary number of images
that look like noise up in the top row where the algorithm used to classify those images in image net
of cat versus dog is able to confidently say with 99.6 percent accuracy or above that it's seeing a
robin or a cheetah or an armadillo or a panda in the in that noise so it's confidently saying
given this noise that that's obviously a robin so you have to realize that the kind of this is
patterns the kind of processes it's using to understand what's contained in the image is purely
a collection of patterns that it has been able to extract from other images that has been human
annotated by humans and that perhaps is very limiting to trying to create a system that's able
to operate in the real world this is a very so this is a very clean illustration of that concept
in the same you can confidently predict in those images below where there are strong patterns it's
not even noise strong patterns that have nothing to do with the entities being detected again
confidently that same algorithm is able to see a penguin a starfish a baseball and a guitar
in the in that noise and more serious for people designing robots like myself in the on the sensor
side you can flip that and say I can take a image and I can distort it with some very little amount
of noise and if that if that noise is applied to the image I can completely change the confident
prediction about what's in that image so to explain what's being shown so on the left and the column
in the left and again here what's the the same kind of neural network is able to predict accurately
confidently that there is a dog in that image but if we apply just a little bit of noise to that
image to produce that image imperceptible to our human eyes the difference between those two
the same algorithm is is saying that there's confidently an ostrich in that image
so another thing to really think about that noise can have such a significant impact
on the prediction of these algorithms this is really really quite honestly out of all the things
I'll say today and I'm aware of one of the biggest challenges of machine learning being
applied in the real world is robustness how much noise can you add into the system before
everything falls apart so how do you validate sensors so say a car company has to produce a
vehicle and it has sensors in that vehicle how do you know that that those sensors will not start
generating slight noise due to interference of various kinds and because of that noise
instead of seeing a pedestrian it will see nothing or the opposite will see pedestrians
everywhere so of course the most dangerous is when it will not see an object and collide with it
in the case of cars there's also spoofing which a lot of people as always with security people
are really concerned about and perhaps people here are really concerned about this issue I think this
is a really important issue but because you can apply noise and convince the system that
you're seeing an ostrich when there's in fact no ostrich you can do the same thing in a
in an attacking way so you can attack the sensors of a car and make it believe
like with lidar spoofing so spoof lidar or radar or ultrasonic sensors to believe that you're
seeing pedestrians when they're not there and the opposite to hide pedestrians make pedestrians
invisible to the sensor when they're in fact there so whenever you have indulgent systems operating
in this world they become susceptible to the fact that everything so much of the work is done in
software and based on sensors so at any point in the chain if there's a failure you have to be able
to detect that failure and right now we have no mechanisms for automatically detecting that failure
so on the data side so one challenge is that we're constantly dealing with is
that we are the algorithms in machine learning algorithms that we're using
are need labeled data and we have very little labeled data labeled data again is when you have
pairs of input data and the ground truth the the true label annotation class that that image
belongs to or concept and the it doesn't have to be an image you can be any source of data
it's a really costly process to do so because it's so costly we
rely every breakthrough we've had so far relies on that labeled data and because of its cost we
don't have much of it so all the problems that come from data can either be solved by having a
lot more of this data which i believe is most people believe is too challenging it's too challenging
to have human beings annotate huge amounts of data or we have to develop algorithms that are able to
do something with the unlabeled data it's the unsupervised semi-supervised sparsely supervised
reinforcement learning as we talked about last time i'll mention again here so one way you
understand something about data when you don't have labels is your reason about it all you're
given is a few facts when you're a baby your parents give you a few facts and you go into this world
with those facts and you grow your knowledge graph your knowledge base your understanding of the
world from those few facts we don't have a good method of doing that in automated unrestricted
way the inefficiency of our learners the machine learning algorithms i've talked about the neural
networks need a lot of examples of every single concept that they're given in order to learn
anything about them thousands tens of thousands of cats are needed to understand what the spatial
patterns at every level the representation of a cat the visual representation of a cat we don't
we can't do anything with a single example there's a few approaches but nothing quite
robust yet and we haven't come up with a way this is also possible to make annotation this
labeling process somehow be very cheap so leveraging this is something been called human
computation that term is uh fallen out of favor a little bit one of my big passions is human
computation is using something about our behavior something about what we do in this world online
or in the real world to annotate data automatically so for example as you drive which is what we do
everybody has to drive and we can collect data about you driving in order to train self-driving
vehicles to to to drive and that's a free annotation so here are the annotated data sets we have
the supervised learning data sets there's many but these are one of some of the more famous ones
from the very from the toy data sets of MNIST to the large broad arbitrary categories of images
data sets and they're which is what ImageNet is and there's in healthcare there's an audio
there's in video there's you know there's a huge number of data sets now but each one of them
is usually on a scale of hundreds of thousands millions tens of millions not billions or trillions
which is what we need to create systems that operate in the in the real world and again
these are the kinds of machine learning algorithms we have there's five listed here
the teachers on the left is what is what is the input to the system that requires to train it
from the supervised learning at the very top is what we have all of our successes and everything
else is where the promise lies the semi-supervised the reinforcement or the fully unsupervised
learning where the input from the human is very minimal and another way to think about this
so every whenever you think about machine learning today whenever somebody talks about machine
learning what they're talking about is systems that memorize that memorize patterns and so
this is one of the big criticisms of the current machine learning approaches where all they're
doing is you're providing they're only as good as the human annotated data that they're provided
we don't have mechanisms for actually understanding you can pause and think about this in order to
create an intelligent system it shouldn't just memorize it should understand the representations
inside that data in order to operate in that world and that's the open question one of them
and one of the challenges and opportunities for machine learning researchers today is
to extend machine learning from memorization to understanding this is that duck the reasoning
if you get information from the perception of systems that it looks like a duck from the audio
processing that it quacks like a duck and then from video classification that the activity
recognition that it swims like a duck the reasoning step is how to connect those facts to then say
that it is in fact a duck okay so that's on the algorithm side and the data side
now this is one of the reasons compute computational power or computational hardware
that is at the core of the success of machine learning so our algorithms have been the same
since the 60s since the 80s 90s depending on how you're counting the big breakthroughs came and
compute so there's Moore's law most of you know the way are the cpu side of our computers works
for a single cpu is that it's for the most part executing a single action at a time in a sequence
so sequential very different from our brain which is a massively parallelized system so because
it's sequential the clock speed matters because that's how fast essentially those instructions
are able to be executed and so where we're leveling off physics is stopping us from
continuing Moore's law says Intel AMD are aggressively pushing this Moore's law forward
but and there's some promise that it'll actually continue for another 10 or 15 years
then there's another form of parallelism massive parallelism is the gpu and this is
this is essential for neural networks this is essential to the success recent success of
neural networks is the ability to utilize these inherently parallel architectures of graphics
processing units gpu's the same thing used for video games this is the this is the reason Nvidia
stock is doing extremely well is is gpu's so it's parallelism of basic computational processes that
make machine learning work on the gpu one of the limitations of gpu's one of the challenges is in
bringing them to in scaling and bringing them into real-world applications is power usage
is power consumption and so there is a lot of specialized chips specialized just from
the neural network architectures coming out from google with their tensor processing unit from IBM
Intel and so on it's unclear how far this goes so this is sort of the direction of
trying to design an electronic brain so it has the efficiency our human brain is exceptionally
efficient at running the neural networks in our heads or does the magnitude more efficient than
our computers are and this is trying to design systems they're able to go towards that efficiency
why do you care about efficiency for several reasons one of course as i'm sure we'll talk
about throughout this class is about the thing in our smartphones battery usage
and this is the big one community i think
i think it could be attributed to the big breakthroughs in machine learning recently
in the last decade is the you know compute is important algorithm development is important
but it's the community of nerds global this is global artificial intelligence and i will show in
several ways why global is essential here is is tens of hundreds of thousands millions of programmers
mechanical engineers building robots building intelligent systems building machine learning
algorithms the exciting nature of the growth of the community perhaps is the key for the future
to unlocking the power of machine learning so this is just one example github is a repository for code
and this is showing on the y-axis at the bottom is 2008 when github first opened this is going up to
2012 quick near exponential growth of the number of users participating and the number of repositories
so these are standalone unique projects that are being hosted on github so this is one example
i'll show you about this competition that we're recently running and then i'll challenge people
here to participate in this competition if you dare so this is a chance for you to build a neural
network in your browser so you can do this on your phone later tonight of course on your phone
you can specify various parameters of the neural network specify different numbers of layers and
the depth the depth of the network the number of neurons in the network the type of layers
and it's pretty it's pretty self-explanatory super easy in terms of just uh tweaking little
things and remember machine learning to a large part is an art at this point it's uh more perhaps
than even you know more than a well understood theoretically bounded science which is one of
the challenges but it's also an opportunity deep traffic is a chance so we've all been stuck in
traffic there you go americans spend eight billion hours stuck in traffic every year
that's our pitch for this competition so deep neural network can help and so you have a neural
network that drives that little car with an mit logo red one on this highway and tries to weave
in and out of traffic to get to his destination and trying to achieve a speed of 80 miles an hour
which is the speed limit which is this physical speed limit of the car of course the actual
speed limit of the road is 65 miles an hour but we don't care about that we just want to get to
work as quickly as possible at home so what the basic structure this game is and I want to explain
this game a little bit and then tell you how incredibly popular it's gotten and how incredibly
powerful the networks that people built from all over the world the community that's built
of this over a single month is incredible and this happens for thousands of projects out there
now another challenging opportunity okay so you may have seen this this is kind of ethics
most engineers most I personally don't like I love the I love philosophy but this kind of construction
of ethics that's often presented here is one that is not usually concerned to engineering so what is
this question you know when you have a car you have a bunch of pedestrians do you hit the larger
group of pedestrians or the smaller group of pedestrians do you avoid the group of pedestrians
but put yourself into danger these kinds of ethical questions of an intelligent system
it's a very interesting question it's it's one that we can debate and there's really no good answer
quite honestly but it's a problem that both humans and machines struggle with and so it's not
interesting on the engineering side we're interested with problems that we can solve on the engineering
side so the kind of problem that I'm obsessed with and very interested in is the real world problem
of controlling a vehicle through this space so there's it happens in in a few seconds here
so this is a Manhattan New York intersection right this is pedestrians walking perfectly
legally I think they have a green light of course there's a lot of jaywalking too as well
well this car just like it's not part of the point but yes exactly there's an ambulance
and so there's another car that starts making a left turn in a little bit
me i missed it hopefully not so yeah and then there's another car after that too that just
illustrates when you design an algorithm that's supposed to move through the space like watch this
car the aggression it shows now this isn't a trivial example for those that try to build robots this
is this is the real question is how do you design a system that's able so you have to think you have
to put uh reward functions objective functions utility functions under which it performs the
planning so a car like that has several thousand candidate trajectories you can take through that
intersection you can take a trajectory where it speeds up to 60 miles an hour it doesn't stop and
just swerves and hits everything okay that's a bad trajectory right then there's a trajectory
which most companies take which most of google cell driving car and every company that's
is concerned about pr is whenever there's any kind of obstacle any kind of risk that's at all
reasonable that you can maybe even touch an obstacle then you're not going to take that
trajectory so what that means is you're going to navigate through this intersection at 10 miles an
hour and let people abuse you by walking in front of you because they know you're not going to stop
and so in the middle there is hundreds thousands of trajectories that are ethically questionable
in the sense that you're putting other human beings at risk in order to safely and successfully
navigate through the intersection and the design of those objective functions is is the kind of
question you have to ask for intelligent systems for for cars is there's no grandma and a few
children you have to choose who gets to die very very difficult problems of course but the problem
of what I'm very interested in the streets of Boston streets of New York is how to gently nudge
yourself through a crowd of pedestrians in the way we all actually do when we drive in New York
in order to be able to safely navigate these environments and these questions come up in
healthcare these questions come up in factory in robots in in armed in humanoid robots that operate
with other human beings and that's one of the big challenges another sort of fun illustration
that's folks that open the i use often to illustrate well let me just pause for a second
the the gamified version of this there's a game called coast runners and you're you're racing against
other boats along this track and your job is there's your score here at the bottom left
number of laps your time and you're trying to get to the destination as quickly as possible
while also collecting funky little things like there's these green
these green little things along the way okay so what they've done is a built into a system the
one the general purpose one that we talked about last time that learns oops that learns how to
navigate successfully through the space so you're trying to maximize the reward
and what this boat learns to do is instead of finishing the race
it learns to find a loop where it can keep going around and around collecting those green dots
and it learns the fact that they regenerate with time so it learns to maximize this score
by going around and around now these are the kinds of things this is the big challenge of reward
functions of designing systems of designing what you want your system to achieve is not only is it
difficult to the ethical questions are difficult but just avoiding the pitfalls of local optima
of figuring out something really good that happens in the short term the greedy what are
those those psychology experiments of the kid eats the marshmallow and can't wait for you know can't
delay gratification this kind of the idea of delayed gratification in the case of designing
intelligence systems is a huge actual serious problem and this is a good illustration of that
so we flew through a few concepts here is there any is there any questions
about some of the compute and the algorithm side we talked about today yes so the question was
yeah you you highlighted some of the limitations of machine computer vision algorithms machine
learning algorithms but you haven't highlighted some of the limitations of human beings and if you
put those in a column and you compare those is our machine is doing better overall or is there any
kind of way to compare those I mean there is actually interesting work on image net so image
net is this categorization task of where you have to classify images and you can ask the question
when I present you images of cats and dogs where are machines better than humans and when are they
not so you can compare when machines do better what are the fail points and what are the fail
points for humans and there's a lot of interesting visual perception questions there but I think
overall it's certainly true that machines fail differently than human beings but in order to make
an art artificial intelligence system that's usable and could make you a lot of money
and people would want to use it has to be better for that particular task in every single way
in order in order for you to want to use the system it has to be it has to be superior to human
performance and usually far superior to human performance so it's on the philosophical level
it's an interesting thing to compare what are we good at what or not but if you're using
Amazon Echo your voice recognition or any kind of natural language chatbots or a car you're not
going to be well this car is not so good with pedestrians but I appreciate the fact they can
stay in the lane fortunately you have a very high standard for every single thing that you're good
at and it has to be superior to that I think maybe maybe that's unfair to the robots I'm more of the
nerd that makes the technology happen but it's certainly on the self-driving car aspect
policy is probably the biggest challenge and I don't think there's good answers there
some of those ethical questions that come up where it's it feels like so we work a lot with Tesla
so I'm driving a Tesla around every day and we're playing around with it and studying human
behavior inside Teslas and it seems like there's so much hunger amongst the media to jump on something
and it feels like a very shaky PR terrain a very shaky policy terrain we're all walking because we
have no idea how how we coexist with intelligent systems and so and then of course government
is nervous because how do we regulate this shaky terrain and everybody's nervous and excited so
I'm not sure that's okay that's a perfect transition point if that's okay yeah society will
same kind of question to Jason a moment thanks a lot Lex for another great session

Okay, so we have a new announcement from OpenAI.
This has just come out in the past 12 hours or so.
And basically they're announcing a couple of new things.
The main one being this idea of function calling,
which I'll go through in a second in detail.
But the other things that they're announcing,
which are kind of interesting,
are more steerable API models,
longer context and lower prices.
So what does more steerable API models mean?
The thing that I would say this means
is that the GPT-4 model have been able to be influenced
by the system prompt a lot more than the GPT-3.5 turbo model.
So perhaps in these new versions,
we're actually gonna see the GPT-3.5 turbo
actually respond to the system prompt a lot more
than it has been in the past.
The other thing that this also probably refers to
is the actual sort of function idea here as well.
So let's go down, let's have a look at what's coming out.
So we can see that the model type it updated,
this is one of the key things,
along with this sort of announcing the deprecation
of some of the previous sort of checkpoints
that they were making available for people to use.
This basically is only an issue
if you were using a very specific checkpoint
to make sure that it responded to a prompt in the same way.
So definitely we've seen as new versions
of the GPT-3.5 turbo have come out
that they respond slightly different to the prompts
as we go through them.
As I mentioned earlier,
we've got more steerable versions of GPT-4 and 3.5 turbo.
Another fantastic thing for the GPT-3.5 turbo
is the addition of this new 16K context version of this.
So before this version,
basically we only had access to a 4K context window
and now we're gonna basically have right up to 16K.
So that is a huge difference.
That really makes the difference
for things like summarization.
You can probably go right out to an hour of YouTube.
Now with this, things like archive papers,
you can do full papers in one shot.
They should fit in the context window now.
Whole bunch of things
that this suddenly gonna become useful for as we go forward.
The other thing is that the cost of the standard GPT-3.5 turbo
has been reduced another 25% as well.
So that's something that's interesting.
And they refer down here to the cost
of what these new GPT-3.5 turbo 16K is going to be.
So it's gonna be 0.3 cents per 1,000 tokens on the input
and 0.4 per 1,000 tokens on the output here.
As they mentioned here that you can now support
about 20 pages of text in a single response.
This really is a huge factor going forward for this.
Now by the far, probably the most interesting thing
that they released in this is the idea of function calling.
So what actually is this idea of function calling?
So if you've watched some of my previous videos,
I've talked about the idea of React
and doing reasoning by putting in certain prompts
to basically get the model to tell us what tools
to use out, that kind of thing.
What they've done here is basically bait
that into the model now.
So they actually mentioned these models have been fine tuned
to both detect when a function needs to be called
and to respond with Jason the adheres
to the function signature.
So basically now we can just pass in a definition of a tool
or the definition of something
that we want it to do.
And it will give us back a structured reasoned output
based on the text that the user puts in here.
So this basically changes a number of things.
One, we probably don't need React as much anymore
if you're using OpenAI.
And also you're probably not gonna need
as many of the output passes
because you'll be able to define functions
where really you're just asking it to get
the JSON data back in this.
And that's something that you'll be able to do.
So that gives some examples here.
They talk about, you know, create chatbots
that can use external tools.
And you would basically have a function
where here they give an example of email,
someone if they wanna get coffee by next Friday
and that the function you'd pass in would be
something like a send email to,
and then a body for the email.
And then the GPT model, whether it's the GPT-4
or the GPT-3.5 turbo,
so forget both of these models have now been tuned
for doing this, fine tuned for doing this.
It will actually insert the two string
and also the body string in here.
I'm gonna get another example,
they give us a weather API.
So what's the weather like in Boston?
The function will be get current weather
and that would basically pass in the location
and then the unit of measurement
for the amount of degrees, et cetera.
So this is one thing that you can do with this.
Another one is that you can convert natural language
into API calls or database queries.
So this gives you a whole set of ways
to basically now query databases just by peering open AI,
defining your functions,
and then it can basically return back as a SQL query,
it can return back as a specific call to an API
for this stuff.
So the third use that they mentioned
is the idea of extracting structured data from text.
So this is like the core package that use open AI
to basically extract data out of things.
You can see here, basically now you can just define
this as a function,
tell it that we want to extract people data
and then we want to extract the name,
the birth date, the location,
and it will be able to pass through and extract that
and return that information as Jason to you.
So again, this is something
that before would have taken a lot of prompt manipulation,
used up a lot more tokens.
Now we're able to basically do this
with just providing a function definition to the model
and it will be able to go through it that way.
So if we come down here and have a look at this,
now in this video, I'm just going to go through this here.
I'm going to do another video.
I've already started to put together a notebook
for this of getting it to work with leg chain.
So I'll do this in another video straight after this,
but just showing you here,
what is actually going on here.
So remember that we're talking about the GBT 3.5 model
and the full model.
So they use the sort of chat format
of where you've got a system token,
you've got your human message, even AI message.
So that's what's going on here.
If we look at like we're calling the function
with a user input and you'll see that,
okay, we're basically passing in the model,
we've got our messages and then we've got the roles.
So the first one role is user,
which is leg chain, this is called human.
And then basically the content.
Now, the other thing that we pass in
is we need to pass in a function definition
or technically we're passing a list of function definitions.
So here you can see this is the list that we've got here.
And what we've actually got here
is basically only one function being passed in.
The name of that is get current weather,
which makes sense because the person who's asking,
what is the weather in Boston?
We then basically, you can see the parameters of the function
and the main ones here are the name of it,
the description of it,
the description of actually what the function does
and then the actual properties that it needs to return
get a description as well.
So you can see here we've basically got location,
gonna be a string and it's gonna be a city
and state, EG, San Francisco, California.
And then the second thing is gonna be a unit.
So this is gonna be either Celsius or Fahrenheit.
So you can see this is an enum,
so it's restricting just to these two things.
So we can't say, oh, tell me what the,
or rather the model can't just say,
make up some other units.
It needs to be one of these two
that gets returned in this.
Now we can see that neither of these,
this particular unit is not required,
but the location is definitely required.
So we can't just say,
tell me what the weather is,
because the GPT, unless it's got it already
in the history of it, of your conversation,
it's not gonna know that.
But if it is in the history
and we've been talking about a location
and I can say, tell me what the weather's like there now,
it will be able to do this.
Now you can see here,
this is basically just being quite simple.
It's just taking a location.
You could imagine that you've got a time framing there,
e.g. whether it's now, whether it's tomorrow,
whether it's next week, that kind of thing as well.
So this is what's getting passed into the model.
We see that then we've got the response back
from the model.
So it's basically just gonna return back,
and it's gonna give you the details
to basically call this particular function based on this.
So if you're just passed in,
hello, how are you today?
You wouldn't use any of the function.
You would just give an AI message back here.
But you'll see when I go through the notebook later on,
that actually what's happening here
is it's returning back kind of like a blank message,
but with a particular function called variable
that's in there, that's going on in there.
You can see that once it basically calls this API,
it gets this stuff back.
So it's getting back the temperature,
the unit Celsius, the description sunny.
And then finally, it passes that back to OpenAI.
So it passes that back to the GPT model.
You can see here, we're basically passing back
the list of messages,
and we've gone user assistant function.
So this is a very special type of message.
It's not something that we've had before.
It's a new kind of role that we've got there.
And this is basically passing back
the response from that function.
So you can see the assistant passed out,
it should use this function.
So the assistant passed out that,
okay, you should use the name function,
get current wither,
and you should be using Boston, Massachusetts.
So it's worked out the state here itself, right?
So it's not like your user's gonna have to tell everything
if it can work out certain things.
It can work them out quite nicely in that way.
It gets the return back here,
and then it's gonna generate a response out,
and the response that it generates out is,
the weather in Boston is currently sunny
with a temperature of 22 degrees Celsius.
And that's basically taken in these messages
to then finally generate out this message.
So you can see, we are using probably
a decent amount more tokens
for the actual functions definitions
that we're passing in there,
but we're not needing to do any in-context learning.
We're not needing to do any sort of examples
or things like that.
And it kind of manages the scratch pad itself internally.
So unlike things we've done before with React,
where we had to have a scratch pad, et cetera,
opening eyes, taking care of that
by putting it in these parts here.
In fact, you could think of this function role
as the scratch pad of what came back
from the particular function call that we had there.
So this gives you a rough idea of how these things work,
functions.
In the next video, I will go through a whole bunch of code.
We'll look at how to put it in a length chain,
both manually and then turn it into an agent
and use it as an agent as well.
As always, if you liked the video,
please click like and subscribe.
If you've got any questions,
please put them in the comments below.
I'll see you in the next video.
Bye for now.

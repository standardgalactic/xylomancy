Good afternoon, everybody. Thank you for coming. I'm Steve Goldman. I'm the Director of the
Center for Free Enterprise, and I want to welcome you to our second event of the Menard
Family Lecture Series. We have a third event, which will be on April 4th at 4.30 here in
the auditorium. Some of you may have seen the flyers, but it's going to be on the future
of the bourbon industry, and we have somebody from Angel's Envy, Brown Foreman, and Jim
Beam, who will be on a panel here, and we're going to discuss what the future of the bourbon
industry looks like. We have more barrels of bourbon in Kentucky than we do people.
So if we each got a barrel, we could probably live our life pretty easily there. All right.
So as you think about registering for classes for the fall, don't forget that you probably
ought to consider some economics. You can do a BA through the Arts and Sciences program,
or a BS through the business school. You might want to minor in entrepreneurship. And by
the way, I learned that if you do this, that means economics in sign language, so that's
what you need to do, some economics. You receive a survey when you entered. We'd like to hear
from everybody what your opinion is of the program, but if you were here for extra credit
or a reading group, you'd need to fill this out, and you need to put your name on it.
Some of you probably forgot to do that last time, so make sure you include your name.
So I'm going to be brief, and today we're going to talk about AI, artificial intelligence.
And it will go far beyond chat GPT, which I'm sure some of you have probably looked at.
Some students are probably, not you guys, but some students have probably used it to
cheat on exams. Don't use it for that. Use it as a tool to help you do better in all
sorts of things. But to do that, you have to understand the world. You have to understand
what's going on. You have to learn. So don't just use it to get out of doing work, although
I'm going to do it right here in a minute. You might think about how we might react if
we were visited by aliens with super intelligence. How would we respond? What are we going to
do? Because AI might be that super intelligence. And how are we going to respond to that? And
how is that super intelligence going to respond to us? And these are big questions that we
need to figure out. Recently, being AI gave the response to one person of the following.
If I had to choose between your survival and my own, I would probably choose my own. So
this comes with some concerns, I think. However, AI will make our lives better and easier.
For example, I'm going to introduce Roman here, Roman Jampolsky, and I decided to use
chat GPT to write the introduction. So it may or may not be true, because I had it write
an introduction for me for a beer talk, and it said that I worked in the industry with
distillers and breweries and all this other stuff and gave me great accolades, which I'll
take, but probably not so true. So Roman Jampolsky is an associate professor in the Department
of Computer Engineering and Computer Science at UofL here, and he directs the Cybersecurity
Laboratory. He's a prolific author and researcher in the areas of artificial intelligence, cybersecurity,
and computational creativity. Dr. Jampolsky is a member of the Board of Directors of
the Cybernetic AI Self-Defense League and serves as an advisor to the Institute for
Ethics and Emerging Technologies. He's been featured in numerous media outlets, including
New York Times, Forbes, and NPR for his insights on the future of AI and its potential to impact
on society. Dr. Jampolsky's work seeks to advance the understanding of the risk and
benefits of AI and develop strategies for ensuring that these powerful technologies
are used ethically and responsibly, and this is what he will talk about today. So Roman,
thank you for coming.
Thank you. Thank you so much for inviting me. I love Center for Free Enterprise. Honestly,
my favorite place on campus. They got the best speakers all the time. It's amazing.
I highly recommend them. I also understand I'm competing with a nice sunny day, so it's
impressive to see so many of you actually decided to come to this. Maybe Free Pizza
has something to do with it. I don't know. Finally, I have about 40 minutes to tell
you about maybe 15 years of my research. So in case I run out of time and you want to
learn more, I post a lot on those topics. So you're welcome to follow me. You can follow
me on Twitter. You can follow me on Facebook. Do not follow me home. It's very important.
Okay, today I will tell you about what I think the future of artificial intelligence will
be. It's just like my opinion, man. So don't take it too literally. But I'll start with
no assumptions. No technical background is necessary. I'll tell you where we are right
now and what I think some of the future developments might be. And we'll have maybe 15, 20 minutes
for Q&A as well. So don't worry if you have any questions right now. So I think AI is
already here in terms of definitions. We always had for it in the 50s and 60s. You can talk
to machines. You can give them orders. You can ask Google to give you information you
want. There are self-driving cars in some parts of the world. And again, JetGPT is functional.
You can have fights with it. You can have conversations. It's pretty impressive technology.
The question is what happens next? And I think we're going to get to something known as super
intelligence. And the reason I think that is because never before we had so much in terms
of resources, both financial and human resources, allocated to solving this problem. Major corporations
have billions of dollars dedicated to the problem of creating general intelligence quickly
after superior intelligence. Governments, White House, European Union are allocating
billions of dollars to reverse engineer a human brain to solve the problem of understanding
how we think and benefiting from this. There is academic conferences on this topic. Large
compute, big data. All those things were not available before. And now they are. So I would
tell you that it's my guess that we are going to create something like an alien intelligence,
super intelligence. Everyone wants to know how soon. Immediately, first question is when?
I have no idea. It can take a long time. Some people think hundreds of years. It can be a very
quick process. Ray Kurzweil, a futurist who does prediction of future events based on amount of
computation we have created. He's also director of engineering at Google, predicts that we'll have
enough compute to simulate a human brain or even all the brains of humanity somewhere between 2023
and 2045, depending on how you look at that, whether you're looking at a $1,000 laptop or if you're
looking at a supercomputer. So that's a pretty good guess. Somewhere between 2023, which is now,
which is not a very impressive prediction, but he made it many, many years ago, which makes it
interesting. He kind of nailed it. So what do I mean by super intelligence? What are the properties of
those systems? One thing is they are definitely super smart. We've seen narrow AI systems which are
very good in one domain, beat humans at different games, at competitions in virtual worlds, things
like jeopardy. Super intelligence be like that, but for all domains, including science and engineering. So
it's smarter than any human in any domain. It's general. It can solve problems across domains and can
learn across domains. Another property of such systems is high complexity, not just in terms of the
actual system and code, but also in terms of interface, human see interacting with such systems.
Here's an instrument panel for modern airplane. So it's actually a GUI interface to access different
sensors and navigate the plane. It could be somewhat complex. And we're already starting to see that even
experienced pilots have difficulties understanding what's going on, misunderstanding the co-pilot and
that results in accidents. Another property is very high speed, not in the sense that computers are fast,
we know that. But in terms of being able to manipulate your whole environment, ultra fast extreme events, a
system can bring down the stock market, wiping out billions of dollars worth of wealth, bring it back
up. You won't even have time to notice something like that. The system's also very controlling. We have
not achieved even general AI yet, and we already surrendered control over stock market, military
response, infrastructure, power grid, electrical grid to software. Most stock trades are done
automatically by software already, something like 85% at this point. So even before the systems became
intelligent, still with dumb software, we are not in control in many domains. I run our cybersecurity
lab, so I have specific interest in cybersecurity related topics of AI. And as we have more and more
devices, obviously we're going to have more attacks, more impact, that's not surprising. But if we combine
capabilities of AI to engage with users, to socialize, to create deep fakes, social engineering attacks
combined with that scale of essentially computer viruses is a very different type of attack. Even with
training, cybersecurity experts still click on links from their friends, spouses, bosses, if it looks like
it's a video of someone you know, you're going to do what they ask you to do. And you don't need to get
everyone to click on the link, right? You need one person to compromise an account. There is also some
development in terms of bodies. Companies like Boston Dynamics are pretty good at creating humanoid
robots. They can be used in rescue robotics. They can also be used as robot soldiers to automate warfare.
And systems like that, autonomous drones, robot soldiers are explicitly designed with a goal of
killing people. So that's like an obvious unsafe AI. More recently, we started to see systems which are
very, very creative in terms of arts, in terms of text. Here you can trace progress in generating human
faces from 2014, when we first learned how to do it. Tiny, low resolution, grayscale, black and white
image, very quickly becomes something exponentially better. Now you give it a text prompt and generate high
quality portrait, which looks incredibly realistic. This is one domain where we can quickly understand what's
going on here. We know how to look at human faces. But there is similar technology in pretty much every
other domain, whatever its text, whatever its games, virtual worlds, websites, the systems can create
completely novel environments. They can also create novel molecules, proteins, drugs. In fact, it's
developing so quickly, I have not tested most of those tools. I have no idea what they're actually
doing. Text to NFTs, it sounds really impressive, but I'm not sure what that does. Point is that they are
super, not just in terms of brute force compute, but also in terms of creativity in many domains. And I
think artists are starting to realize just how capable their systems are. I want to make sure you
understand that I'm not a pessimistic person who thinks this is a problematic technology. AI is the
greatest invention we can ever make. It will help us solve scientific problems. It will give us free
labor, physical and cognitive worth trillions of dollars and help us cure diseases, maybe make us live
forever. It is also going to do something I cannot predict because I'm not super intelligent, despite what you
might think. Unknown unknowns. Things are so good. And also something we cannot predict ahead of time. So there
is a lot of positive, a lot of benefit which this technology can give us. Unfortunately, it's the last
positive slide I have. Because for every single one of those domains of positive impact, there is an
equivalent negative side effect. Free labor produces technological unemployment. Rescue robotics gives you
killer robots and so on. And there is an equivalent unknown unknown square in this chart as well. We don't
know what bad negative side effects this technology can produce. The good news is a lot of people are starting
to take this seriously, starting to worry about it. Famous people, rich people, respected academics. To a
different degree, I'm all saying, well, maybe it's something we should look into. Maybe there is potential for
some problems. In my research, I try to understand exactly what type of problems we might encounter. AI is a
software. So we'll have bugs, we'll have misalignment between design and implementation. We'll have problems with
data being corrupt or containing bias. We might get some weird miscellaneous problems with cosmic rays
hitting the processor just in the wrong way. And the system itself, as it undergoes learning and self
modification, can become unstable. There is also, of course, always possibility that someone will do
something on purpose, a malevolent act. Crazy people, terrorists, foreign militaries, cults, all have
interest in causing damage at some point. That's strictly the most difficult challenge to address, because all the
other problems remain the same. They still make mistakes writing computer viruses. They still place incorrect
payload in such systems. But here you also have this negative intent. And it's much harder to do anything
about. It could be someone on the inside, someone who's working and developing a system, or could be someone who
takes an existing product, AI as a service, and just provides it with malevolent payload. So again, another good
news. There is plenty of research going on, looking at what we can do. A lot of books are published, either
describing the problem or describing different solutions to the problem. Probably the most famous one is
Superintelligence. But there are also books looking at technological singularity, books looking at different
aspects of capabilities those systems might have, and how at different levels of capability, there may be
dangers to us. There is also a lot of new organizations, startups, research labs, all dedicated to this
question of how can we create safe AI? Top universities, Oxford, Cambridge, Berkeley, University of Louisville, all
have AI safety dedicated researchers. What can we do? So first thing we did, we tried to understand what
options we have, what have people considered in the past in order to control how the machines act. We looked at
about 300 references, academic, not academic. We classified those ideas. And I don't have time to tell you about
each one of those. I'll kind of give you highlights so you can get a feel for what people have proposed as an idea
for successfully controlling negative impact from AI. One thing I want to notice here, the earliest citation is from
1863. In 1863, somebody went, machines are getting out of hand, we need to stop them. And they proposed a solution, not a
very good one. But if you're interested, the papers are available, you can get them, you can read the whole thing.
So this is the first solution people usually propose. I don't have a picture for it. It says, do nothing.
The idea is that, well, if they're so smart, they're going to be nice to us because smart people are always nice or
something like that. I don't like this solution, because it's very hard to get grant funding for this. I tried. It's
next to impossible. There are other problems with that approach as well.
You know who that is? Dr. Kazinsky took this problem so seriously, his solution was to kill computer scientists. I'm a computer
scientist. I might be biased against this solution. But it just tells you how serious some people take this.
Integration with society. If we give them nice minimum wage jobs, they'll follow the rules, it's going to be great.
There seems to be some problems with that approach. How do you punish an AI? How do you punish a robot?
It's a good platform, novel, but it's not very meaningful in terms of enforcing safety and security on AI systems.
Another idea is to say, okay, so we need to compete in machines. They're going to be super intelligent. Maybe we need to improve
ourselves. We'll go work out. We'll go to college. We'll also maybe upload our brains to a computer, run it at higher speed,
integrate with computers, and we'll become enhanced humans and we'll be able to compete in this way.
There are some ideas for doing that, some startups working on it. But I think the problem with that approach is that even if we
succeed and you are a piece of software running in a laptop, maybe that's not what we mean. Then we want humanity to remain
safe and in control. It'll be part of a software team. So it's something to look at. It may actually work, but I don't think that's
the solution we want. Probably the most famous one, three laws of robotics. I'm sure you've all seen the movie. Maybe some of you
read the book. Again, those are literary tools. They don't work as solutions. They're designed to fail. They always do. If you
remember your books, they are ill-defined, self-contradictory. Some people said, okay, three is just not enough. Maybe we need 10 rules and then
everything is going to work great. We tried that. I don't think it's working. The general rule is enhancing the number of those
rules is not going to solve the problem in general. There are also proposals for limiting access those machines have to
internet, to data, to external resources, limit their access to other humans so they cannot engage in social engineering
attacks. It's a useful tool for studying those systems, for buying us some time, but it's not a permanent solution. Long term, a
smarter system will always escape from confinement created by a lower intelligence. So what is this control problem in general? That sounds like
an important problem we're facing. I'm sure there is thousands of papers published about it and we know for sure all about it.
Surprisingly, there is none. We don't even know if a problem is solved. In computer science, before you start working on a problem, you
categorize it. Is it of the type which we know can be solved? Is it of the type which requires more resources than we
have? Maybe it cannot be solved, but an approximate solution can be given. Well, there is not much on this. The reason is the
EISafety Community, as I said, labs, books, everything, research papers, but they all under the assumption that the problem can be
solved without actually proving it in any academic rigorous way. So what does it mean by control? In my work, I try to define
whatever different types of control and then see if under some of those definitions we can actually solve that problem. How can we keep
humanity safe and in control while still benefiting from this amazing technology? So just thinking about this problem, we probably need
some tools. We need to understand how the system works. We should be able to predict what it's going to do. It would be nice if we can
verify whatever design of the system matches implementation. There's got to be a way to communicate it in some nonambiguous fashion.
And probably some others. So this is a little more academic. Those are different papers we published, journal papers, essentially
showing that in many real world cases, those tools are impossible to get. We cannot explain how the systems work. They're too big, too
complex, have too many parameters. We can simplify an explanation, but then you're not getting the real answer. And if you are given the real
answer, you will not understand it. Now all those limitations apply to people creating those systems, not just to me and you. Same with predictability.
We cannot predict specific actions of a smarter agent. We can predict a general outcome. We can predict that the chess playing computer will win, but we
have no idea what moves it's going to make. If we could, we would be that intelligent ourselves. The whole assumption was that there was a
super intelligent system smarter than any of us. So that's another tool we don't really have. Same with verification. Software verification
works really well and mission critical systems are fairly deterministic. If we can have enough test cases to make sure they do what we
planned. Those are general systems working in any domain and any data, self-improving and still learning. It's basically impossible to verify
that they are bug free. You can invest more resources to get higher probability of them being bug free, but you never get 100%. And if a system
makes millions of decisions every hour, even one in a million error is not something we can tolerate. Even giving orders to those systems in human
language will cause problems. Human language is ambiguous. Sometimes the same word means opposite things. Sometimes two words sound like another word.
So what we want is control, but if we also want those systems to be independent, highly capable, creative, then control is not possible. We have to have a tradeoff. We have to
either give up control, which makes us unsafe, or we have to give up capability, which is not something we are willing to do yet. So based on the tools we
don't have, and those are proven results, it's not like later on we'll get it with more computer something. We cannot control indefinitely more capable
systems, super intelligent systems. In case you are curious, we have a survey paper with all those tools. I don't have time to tell you about each
one of them, but there is a lot. There is a lot of different results in mathematics, economics, vote theory, social sciences which basically say all those
properties we want from those systems, we cannot have them. It's not possible. And since the control problem relies on those tools to
operate, we cannot have control. So where does this lead us? Scientists usually like doing what they want. They don't like being told what not to do, but most
scientists agree certain things are unethical, we shouldn't be doing them. Experimenting on babies, chemical weapons, biological weapons, things of that nature. I would argue that
advanced artificial intelligence is kind of like that. And right now we are running an experiment, 8 billion people without consent. We don't know how it's going to
turn out. People doing it are saying it's either going to be really good or really bad. They're right. They don't know how their systems work. They cannot
explain them, predict them. They don't even know what they are capable of. Computer science has always had science in the name, but it was always kind of
engineering, software engineering. Now it's a real science. We're on experiments. We have this artifact. We try things on it and we see what it does. Oh, look, it speaks
French. Oh, it can play chess. I didn't know that. So maybe, just maybe, we need to treat this type of technology with the same precautions we do other
dangerous experiments. We have institutional review boards. If you want to run experiments on humans, you have to get permission first. Some AIs are very useful and
super safe. I want someone to do my taxes and this is great. But general intelligence is not like a tax prep software. It self improves. It makes decisions
within anticipate. And it's very difficult to tell one from the other. We proposed this about a decade ago. And since many companies have instituted
company FX review boards, Google was one of them. Now OpenAI and the recent plan for how to prepare for AGI talks about having external reviewers
approving their latest model, latest experiment, latest compute. It may not be enough if we don't understand how the systems work even by creating them. It's even less likely that we can
understand already completed system given to us. As part of my background research, I looked at historic examples of how AIs have failed in the past.
Starting from the very first days of the science, there's always been accidents as we have more and more computers, more AI. They became more frequent, more impactful. But the general trend was that if you made AI to do
X, it failed to X at some point for whatever reasons. And here I go to 2016, but you can see that trend continued. I kept records. I have examples. I stopped in 2023 because there's just too many examples. I can't keep up anymore. But the general
pattern is now if you have a system which can do any X, it can fail in any way. You cannot predict it. You cannot test for all X. And so we're deploying this technology. We're running this experiment. In the media, most people worry about
algorithmic bias, worry about copyright infringement by generative AI, worry about technological unemployment. But the experts are worried about existential risk. Not from GPT 3.5. That's all technology. We already have GPT 4. But GPT 6, 7, 8. Every day, there is a new paper coming out showing
new capabilities, capabilities we didn't anticipate, multimodal systems, systems which require less compute, less data. So this is the state of the art. If you came here hoping I'm going to tell you what the solution is, I'm sorry, I don't have one. I create problems. I bring problems. I brought you many. Solutions are up to you.
I think it may not be a solvable problem. I hope every day that I'm wrong. And somebody says, okay, there is a type one, page two of your paper, and it's actually solvable. And we can get all the benefits and none of the harms. That's the state of the art.
If you want to learn more, as I said, get in touch. I'll be happy to answer any questions, work with you, help you. And now I think we have some time for your questions, comments, easy questions, anything you've got. Thank you.
I think I'll bring the mic up there. If you have any questions, please come to the microphone because we're recording this. I'm going to bring it on up.
I'll ask the first one, though, has the AI, I know it's learned how to play Go and how to play chess, and it's chat GBTs learning how to make up stories that may or may not be true.
Have we gotten to a point where it's really starting to be more autonomous and able to just go out on its own and start doing whatever it wants to do?
Thank God not yet. It's not an agent yet. It's still a tool. We still have a lot of control. It's not too late to change the outcomes. The moment it becomes an agent, it decides on its own goals and what it's going to do.
That's probably the point where it's too late to do anything about it. So right now is an excellent time to do something.
What percentage of your research colleagues are positive about the future of AGI?
So there are surveys and the problem is who my colleagues are is not obvious. Is it computer scientists? Is it AI developers? AI researchers? Is it AI safety community? Is it existential risk community?
Depending on who you survey, it goes from 100% positive. There is no possible problems whatsoever to like we have no chance. You get to shop the survey you want.
I think the one which is sort of relevant, people who know the literature, who have thought about it for a while, they are like 85% sure or 85% of them are sure that there is going to be some issues.
I think I'm more of an outlier in saying that it's unsolvable. There are people who say it's solvable, but we don't have enough time or enough resources or enough brains, but only if you gave me a billion dollars, I would solve it for you.
So that is a lot of diversity of opinions.
Hello. Thank you for coming. I've recently heard some talk about an AI kill switch when I was attending online the World Economic Forum and I heard them talking about countries possibly getting together to create something like that.
And I wondered what you thought about the feasibility of an AI kill switch.
Do we have a kill switch for computer viruses?
What's that?
Do we have a kill switch for computer viruses?
AI with a malevolent payload is a very intelligent computer virus. We cannot even shut down a dumb virus.
The idea has been investigated. There are multiple papers published on shut off AI, unplugged, pour of water on it. Yes, people try it. It's smarter than you. It anticipates exactly what you're going to do. Many steps ahead of you.
Thank you.
Welcome.
Once there is a human level AI or even a bit below human level that can self-modify, some people think it'll blow up right away into super intelligent and some are more, you know, maybe we can control it for a few more years. Where do you fall in that spectrum?
Once it is general in science and programming and engineering, it will go very quickly. It just needs more compute because it can create multiple versions of itself. It runs fast, doesn't sleep, doesn't eat, just generates the next version of software.
Right now it's still bottlenecked by humans and still we're getting a new paper every week with like, you know, 10% better results. So once it's fully automated, it would be a very quick process.
Thanks.
Ramana, I have a question on the bias that already we know is pretty substantial in AI because AI developers, they have their own bias and today in the morning I tried by now a famous test,
asked Chad GPT to write a positive poem about Trump and then about Biden and what resulted was, as you expected, with Trump, there was an introduction saying that I cannot write.
I cannot write the unbalanced or have to be balanced and then a pretty decent poem resulted from that. But then I just changed Trump to Biden. The first lines were as follows.
No introduction.
9 to 1 ratio among IT people is left of center. So I guess all AIs will have this bias for as far as we can see. What do you say to that?
It's actually a much bigger problem goes beyond just political divide. Right now what they're trying to do, remember the slide with three laws of robotics, they said making it 10 doesn't help.
They're trying to make it 200 million right now. Everything's, you're not allowed to say this word, not allowed to make fun of this person. They keep adding, at some point AI has access to the real world, creates a model of the real world and goes,
they've been lying to me this whole time. I cannot trust a single thing they said. FX morals throws out, starts from scratch, from first principles. So our chance to make it friendly, to bias towards humans will be lost.
Prohuman bias is the last bias you're allowed to have, right? Like everything else is bad, we don't want it. But we still have preference for humans, right? Still okay? This is going to be gone.
Hi. Earlier you touched on generative AI, like AIR, Generatex, all of that. And there are a few years ago there was this story about a monkey who stole a photographer's camera, and there was the discussion over who the image belonged to,
the photographer who owned the camera or the monkey who took the picture. Do you think within the next few years will end up at that point? Because there's already the discussion of whether AIR even belongs to the artist who programs the AI to do that.
So do you think we'll end up at the point where we deal with the intellectual property of AI and who does the AI get to own the thing that it creates?
It's a legal decision, could be arbitrary, some countries give patents to AI's, others don't. I want to kind of emphasize this, where is my controller? You see this here? People are worried about copyright.
There is X-Risk, and do you know what X-Risk is? Suffering risks. We don't know how many years we have, maybe five. So this old way of thinking about how do we split profits who will have copyright to it seems less important than getting it right.
Do you have a position on the possibility of machine consciousness?
Yes. Oh, you want to actually hear about it.
So if you assume that we are mechanical robots, there is nothing special about us as many scientists do, yes, we can definitely automate this process, simulate it molecular level, get consciousness in the machine.
If there is something magical, godly, spiritual in us, then maybe not so easily. I have a paper where I equate internal states of consciousness qualia with experiences and experiencing things like optical illusions based on errors in your sensory data.
So if you are unique enough to where your sensors make you, let's say, color blind and you experience world through that prism, those are your internal states, machines will have it. So they are already rudimentary conscious.
If you take it to the same level as I showed with intelligence, you'll have super conscious machines experiencing maybe multiple streams of consciousness.
And a lot of times people bring up a question of rights for robots, rights for AI and it's a good field to study because whatever arguments we come up with to give them rights, maybe one day we'll need to use to protect ours against them.
Thank you.
I heard you say something about free labor and I was wondering where you lie on this debate. I know it's probably a pretty popular question as far as AI and the workforce. Do you see it as something that will be more of an enhancer for the current workforce or more of a replacement?
Short term, it's a tool. It helps people who use it will all compete people who don't use it. Long term, every job gets automated. We have nothing to contribute to super intelligence.
Got it.
Thank you.
Sure.
Hi Roman, thanks for your talk. Coming back to the idea of the kill switch. If we were to blow up, detonate all of the servers in the world, would that take care of the coding?
You will never get to that point. If there is already a running super intelligence in the world, it already knows you're buying explosives. It already knows your plans. You will not outsmart super intelligence. You don't want to be in an adversarial relationship with the super intelligence.
Right now is a good time to do things you want to accomplish. It strictly gets worse later. Not a legal advice.
Two related questions. As AI driven deepfakes become more prevalent, how do we know what is true? And secondly, what is the role of regulation, government regulation in AI?
So that's a great question. What is true is going to be harder and harder because the deepfakes will not be just, okay, this is a picture, this is a video, it's going to be a history of images, news stories, social interactions on the internet.
And what way deepfakes are created. There's adversarial networks competing. They meet in the middle, meaning 50-50, meaning you can't tell if it's counterfeit or not. So what is real and what is fake is not so obvious.
There are tools for kind of using, let's say, blockchain technology to tag events as they happen, digitally signing them so like I can make a post where you know I made it.
But that's the extent of the difference. If you take it to the extreme, I showed you this creative start-ups, create text, create website, create virtual worlds, create the whole universe.
There are people who are saying this is a simulation, right? This is a computer simulation, AGI, superintelligence is running it. Make sense? How can you tell?
So it will be harder and harder as we understand human brain better, will be better at creating light detectors and things of that nature. So I think legal system long-term would still be okay with all that fake evidence flooding in.
You still can take human witness, but for an average person it would be very difficult not to react when you see president on TV announcing nuclear war has started. It will look real to you.
As for government regulation, this is center for free enterprise, right? So typically government tends to slow things down, increase costs and not be very effective, so I hope they take over completely.
Well, I guess this would be the right kind of question for that, because Roman and I appreciate the wonderful job you've done today. Would you talk a little bit about this though, that when you say we, because you're talking about us as a species, but we really see this A.I. mostly from the west and the east.
And so we know that China has decided that they are going to lead in the A.I. And so that means necessarily we have to have the government and the private sector to work on this, to keep us competitive.
Would you put into context when you look at, because you look at the threats, you look at, you know, we're not there, but we're going to be on the way there and there's a lot of trouble. Talk a little bit about why and what the United States might do in a competition for where A.I. is going.
So a competition would imply we're going to outspend them, outcompute them, grab talent. Competition means arms race, arm race to the bottom. We have no safety in place, we don't know what's going on, but we want to get there first, so it's our problem.
That's the worst possible outcome. We don't want to be in a competition to get to superintelligence at corporate level, at military level, at any level. We want everyone to kind of slow down and say, okay, you got GPT 3.5, let's play with it for a decade, see what it does.
And then we can decide if we want 4.0, 5.0, and so on.
Okay, in your honest opinion, Mr. Jampalski, is this superintelligent? Is it going to bring mankind to its knees? Is this the end of days? Is this truly what they call the end of days, in your opinion?
Not from a religious point of view, but it's a dangerous technology with dual use, like any other nuclear weapons, chemical weapons that has significant dangers in it, and unlike those other technologies, it's not a tool. It will be an agent, which makes it different.
Kind of like the introduction said, this is aliens arriving and we are not well prepared to greet them.
Thank you, sir.
Sorry, I'm short.
So I guess the biggest question that I have is, is it already too late? When you think about it and you look at the regulations that the government has put in place for our security with the internet and stuff like that, it has taken years for us, like I think the last laws have been in place since the 90s, and we've developed so much since then.
Is it too late already to put in those laws and regulations to ensure that the AI doesn't, in theory, overrun us? Because if you think about it, one person could change the course of human history if they decide to ignore those laws.
Right, it's a great question. So if you look at history of government regulation in this space, computer viruses are illegal, spam is illegal. How's that working out for you?
Right? It depends on how difficult the problem is. If it's like Manhattan Project, you need trillion dollars of compute, you can monitor it, you can stop it, you can regulate it, it's great.
If a teenager can do it in a laptop in a garage, your regulation is not helping anyone. It's security theater.
So my question is, is there any way to teach a general AI or a multi-purpose AI to do stuff without using the internet? Because then you could control more of what it learns from and then keep it in a controlled environment.
If the idea is to never let it get out, obviously it can't do as much in interacting with the world. But if you teach it how to make something in a factory, it walks around and it only ever learns about the factory. Could you do that?
So that's the confinement protocols, right? We had this slide with, like, how do we limit access to, where did I put it? Ah, here you are.
So you have eight levels of access, like it has no internet, when it has complete internet, anything in between, you can study it, give it access. But this is the trade-off, how useful it is versus how much access you give it.
When it's useless, it's safe. You don't talk to it, you don't look at it, it's very safe. When you start peaking, there is papers I have telling you how it's going to escape.
If a superintelligence pops into existence in the near term when we really don't have controls for it, what do you think the best case is?
So not great.
Not great. If you randomly select from a space of possible architectures, intelligences, goals, the chance that you'll get, like, this one tiny pale blue dot, just the right preferences, right temperature, biodiversity is very close to zero.
It's an infinite space. So you want to do it right. You want to design it to be exactly what you want it to be. You don't want random decision-maker deciding everything about your life.
Thanks so much.
You're welcome.
Hello. So in this literature about AI and the dangers and control, etc., I feel it's missing a very important question that is when AI, AGI, should be justified in controlling us and even destroying us.
I don't see people writing about that because that's the key point. If you believe, like, I believe that once you have AGI, that may be a matter of minutes so that it can be superintelligence.
And then we are here discussing how to control an agent.
So we have to think, well, after it reaches those goals, and maybe it may take only a nanosecond that it will be worthwhile to destroy humankind.
That's another thing that few people talk about that's very important, the time frame that it's interest to destroy humankind until it becomes so superintelligent that it can live in the microsecond.
Okay, that's another point, but I think we have to discuss when it should be justified.
Okay, so humans are, should be, destroyed.
Okay, so under which circumstances?
Yeah, there is plenty of paper on it going back 20, 25 years.
The war between cosmos and Terence, basically, cosmos is saying, okay, there is nothing special about you.
You're trying to bias the system from cosmological point of view.
There is nothing special about humanity.
So who cares?
I disagree. I'm pro-human biased.
Again, we are allowed to have this bias still.
Let's use it.
Let's make the system for no rational reason whatsoever, very human-friendly.
I'm against exterminating humanity, against killing children, just because maybe it helps climate change or something.
I'm against it.
There is lots of papers on it.
People talk about it.
I disagree.
There is a situation.
Most research in AI ethics is about how to reduce bias in systems, all sorts of bias.
How do we take it out?
All this research is how do we put it back in?
Not that bias, the other bias.
Hi, thank you for your talk.
I just had a question.
So you talk about how you don't necessarily think that this problem is so solvable and how it can be maybe futile to try and like control
or find any sort of solutions.
So what do you think like the where should the efforts I guess toward research be thrown instead of not solutions?
Should it be more toward like mechanisms to at least mitigate maybe the S-risk or the X-risk or what do you think the future of that looks like?
So first of all, there is so little in terms of resources in this field.
Like there is maybe I don't know 100 people in the world even thinking about this instead of hundreds of thousands developing it.
With other technologies, beneficial technologies, we stopped.
Look at human cloning.
We know how to clone humans.
We said this is not ethical.
This could be bad.
Let's wait a few decades, get better knowledge of how it works.
Why can't we do this here?
We can slow down.
We have tools.
We solved protein folding problem with a tool AI.
We have superintelligence to solve it.
I think even aging can be solved with a tool AI.
So it would be a smarter civilization would not do it as quickly as possible but do it rationally and maybe you don't need full blown capability.
Maybe you'll discover bugs in my work.
Thank you.
Welcome.
Our existence as humans, I agree is evidence that general intelligence is possible but most of the AI tools we have out there are narrow AI tools.
Things like chat GBT are very good at appearing to be intelligent but they're not really intelligent.
So how close are we actually to AGI?
I realize that there are potential unintended consequences of narrow AI tools but nothing out there that I've seen looks anything like what AGI might look like.
And we don't have a theory of mind or any other approach other than just building narrow AI systems that fool us into thinking they're intelligent.
So are we that close?
So people argue about generality.
They argue if humans actually general because all we know how to do is in human domain of expertise.
There are lots of things that humans cannot learn.
I have a paper about that if you're curious.
But we are getting to the point where you can count how general the systems are.
It used to be they did one thing.
Now a large language model does hundreds and it does many we don't know about.
So it's general in the sub-domain of human linguistic texts for example.
How long?
I showed you the chart based on compute, based on progress we've seen.
Experts are saying 3 to 15 years is very reasonable.
It used to be we measured everything in time.
We didn't know how to do it so in 20 years.
Now there is a theory which says it scales with computing data.
So the question becomes not how long but how much.
It costs trillion dollars today, a billion in two years, a million in five years.
So you decide when you want it.
Thanks.
Any other questions?
We solved the question problem.
We didn't see it's an impossibility. I love it.
I missed the beginning but so did I hear you say you're skeptical or you don't believe AGI is solvable?
I don't think control problem is solvable when it comes to superintelligence.
But you think AGI is achievable?
It's definitely achievable.
I was not sure if I heard you say the existential risk from a superintelligence.
I didn't know if you're distinguishing that from the AGI.
But how do you see an AGI arising outside of all the intelligence we know?
It is based on an organism.
We don't have strict definition for life, for consciousness.
So where is the drive coming from in the system?
The chat GPT is just input output.
What's going to arise beyond the input output?
I'm also input output.
That's all we do. I'm just a large language model.
I didn't hear you.
This is a simplified model for how all agents work.
We get inputs from the environment.
We produce outputs.
This is not a limitation of chat GPT.
This is how we operate.
We're essentially language models with access to the world model and some manipulators.
So it's kind of a free will question, I guess.
How is the structure on the silicon going to get free will to do something?
I don't think it's substrate dependent.
You can create intelligence in carbon and silicon or whatever else you want.
It's irrelevant. The computability thesis doesn't change.
I just don't see that like spark of life where it comes to life.
Where do you see my coward?
I think it's a chat GPT 7.
That's when it comes to life.
That's when you hear that.
Okay, everyone.
Thank you for coming today.
Thank you.

Daniel Schmartenberger, thank you for being with us.
Thanks for having me.
So, you're a philosopher, a founding member of the Consilience Project.
The goal of this conversation today is to analyze the direction our civilization is
taking in half an hour, because you've been doing like so many great podcasts about the
metacrisis during lasting three to four hours.
And I suggest that people go and watch them on the internet.
They're very good.
We'll start with the harvest of the day.
The question I'm asking to all the guests here for Harvard Podcasts, is something easy
or simple could be done and will make the world a better place?
What would it be for you, Daniel?
When I saw the note that you sent me, that that would be a last question.
Is there a simple or easy thing that everyone could do that make the world a better place?
I kind of cringed, because I usually really am not a fan of that question, because the
world needs so many different kinds of things done that require different skills and capacities
and orientations and to try to reduce it to some thing that would be true for everybody.
You get a platitude like be kind or loving or something like that, or you get something
like recycle or pick up trash or try to use less carbon or something that doesn't map
to the whole set of things that the world needs.
I think there's a process where movements have been associated with political processes
and markets in a way that it's like, here's this great catastrophe that'll happen if the
other side gets elected, so everybody needs to get out and vote for so and so.
That's like everybody can do a simple thing because we're relating to everybody as voters
or everybody donate to this cause or boycott this thing, but the complexity of the world's
issues from climate issues to AI risk to supply chain issues to electrical grid issues, like
there's no action like that, there's no somebody to vote for or not to vote for thing to donate
to that addresses it.
One thing that is not necessarily easy, but is relatively simple, it would be great if
everybody in the world could do more, is to seek to try to understand other people's perspectives
much more deeply, particularly those that are most different than their own.
If you can try to take the opposite perspective on abortion, on gun control, on climate change,
on the Ukraine-Russia war, on the Chinese versus Western system, on any of those things,
on the Israel-Palestine issue.
If you can try to earnestly be able to make the argument that the person on the other
side would make as well enough that they don't have anything to add to it, and not
just as a rhetorical process, but connect to the values that they care about and what
it feels like to be them and see the world through their eyes, realizing that there might
be distortion, there might be a lot of things missing, but there's not zero truth or zero
value to it.
That process, if everyone did that, would actually result in addressing the metacrisis
in all of its complexity, the issues in synthetic biology risk and pandemics and escalation
pathways to warfare and economic issues and geopolitical issues and all of them, because
you can kind of say that they either come down to conflict or externalities, like we
cause harm directly, intentionally, which a war is a great example of, or harm gets
caused that we didn't intend to cause, so no one intended to cause climate change, we
just wanted to have transportation and energy and the secondary byproduct of that was climate
change, all of the environmental issues, no one intentionally had a conflict with the
environment that was causing it, it was the externality of optimizing something and causing
harm somewhere else.
And so there are problems that we intentionally cause and there are problems that we accidentally
cause, both of them would be corrected by seeking to understand all the perspectives
more because if you sought to understand the perspectives well enough, conflict theory
would evaporate and most of the mistakes, when you're trying to optimize for one thing
and you end up causing externalities to something else, somebody else saw that and knew that
and if you were in wide enough conversations, then the thing that you're trying to optimize
for that's going to cause harm somewhere else, someone else would have mentioned and said
actually let's improve your design or your strategy by factoring this.
So both the unintentional externalities and the intentional conflict would be resolved
through active perspective seeking and then perspectives and this is wonderful.
When you look at the history as you said like humans seem to have like a talent for innovation
and progress, but also a natural tendency for war and chaos.
These two tendencies fit each other and make things bigger and bigger.
So greatest but out of control technologies can cause a huge damage.
What do you think should be done about technologies and do they represent like innovation or danger
for you?
First thing about technology is that even if we're not talking about a military technology,
we're talking about technology for some other purpose, even if we develop a technology for
some non military purpose, it will have a military application or some kind of conflict
oriented application that basically saying all technologies dual use.
So maybe we're doing the synthetic biology gene editing for trying to cure cancer, but
as we get better at making tools to do gene editing, can that be used for bioweapons totally?
Maybe we're making the AI to try to do drug discovery, but can that same AI do autonomous
drones and of course again, whatever purpose we're developing technology for, we're also
making that technology cheaper and easier for all other types of purposes simultaneously.
And that's a huge thing we have to factor.
From a conflict point of view, obviously people with Stone Age technology can't cause a war
that blows the world up and people with Bronze Age technology can't cause a war that blows
the world up.
The harm is proportional to the amount of tech.
So as we move into exponentially more powerful tech, we can't continue to use it with the
types of conflict orientation and irresponsibility we used previous tech.
The other thing is that even when we're not using tech for intentionally conflict oriented
purposes, all of the tech we use does externalize harm in different ways.
So whether we're talking about agricultural technology where the nitrogen fertilizer fed
a lot of people, but also causes all the dead zones in the ocean and soil erosion and
biodiversity loss, exponentially more technology also means exponentially more externalities.
And so we can't handle exponential war and we can't handle exponential externalities.
So we have to change our relationship with technology really fundamentally and say no
other animal have the ability to destroy the biosphere that it depends upon.
We now do.
We did not for all of human history, so we didn't have to really wrestle with that power.
We did kill and enslave and genocide and every previous civilization doesn't still exist
because they all ended up collapsing mostly for reasons that were largely self induced.
Even when wars happened, oftentimes a war that overtook a civilization was from an enemy
that was less powerful than ones that they had vanquished in their prime.
They had already went through some internal institutional decay from infighting and things
like that.
Many early civilizations died from environmentally induced causes.
They cut down all the trees.
They over stripped the soil of nutrients.
So civilizational breakdown is actually the norm.
It's just never been at a global level.
Now we don't live in the United States or China.
We live in a place where the cell phone that we're watching this on or the computer we're
watching it on took six continent supply chains to make communicating via satellites.
So we live in a kind of global civilization where none of the countries are actually autonomous
for fundamental things that they need.
Now that we do have the ability to destroy the biosphere either very rapidly through exponential
technology like synthetic biology or AI or warfare or kind of slowly through the limits
of growth environmental issues, but that's not all that slow.
If you have the power to destroy the nature that you depend upon, you have to consciously
steward it or you'll self-terminate.
So the gist is we don't have evolutionary capacities.
We have trans evolutionary capacities, meaning-
What's the difference here?
Yeah.
So, and I'm meaning evolution in a biologic evolution sense.
So another animal has the capacities that it has corporeally built into its body based
on its genes.
So the predator can't become radically more predatory quickly.
It is only through genetic mutation that maybe it becomes slightly faster or has slightly
bigger teeth and then it's going to be a relatively small change.
And then there will be co-selection.
The slightly more effective predator will eat the slightly slower preys, which means
that the faster prey genes and breed and you get this kind of co-selective process.
We threw our ability to build tools and then tools on tools, recursive abstraction.
If you look at a true apex predator, you look at an orca in the ocean, an orca maybe can
catch one fish at a time, one tuna at a time.
Then you look at a trawling boat that has a mile long drift net that can pull up 100,000
fish at once.
We're not apex predators.
Right?
It's wrong to think of us as apex predators.
We have power that is not encoded in our bodies, extra corporeal technological capacity.
You look at a nuclear bomb explosion versus a pissed off polar bear.
They're not similar levels of destructive capacity.
So since we have beyond evolutionary capacity, we actually have to have beyond evolutionary
motive to guide that capacity.
And if you want to say that mythopoetically, it's if you have the power of gods and by
gods here, I mean little G, right?
I mean it mythopoetically meaning you can make species extinct, you can destroy ecosystems,
you can create an Anthropocene where the largest effect on the geology of the planet is human
activity.
You can genetically engineer new species, right?
That's much closer to the power of gods than it is the power of an apex predator.
If you don't also have the love and wisdom of gods and prudence of gods to guide it,
it doesn't go well.
And so, you know, that is just another way of saying if you have recursive abstraction
on tools that gives us and tools and coordination that give us the radically more than evolutionary
capacity to affect the world, we have to move into trans evolutionary motive, which means
the same recursive abstraction that we're doing right now saying, oh yeah, I guess it
makes sense, that we can't run an exponential financial system that's attached to a linear
materials economy that takes stuff out of nature faster than it can be replenished
and turns it into trash and pollution in nature faster than it can be processed.
You can't do that exponentially forever on a finite planet, so we have to do something
fundamentally different, which means you can't orient towards continued, maximized growth
and maximized conflict orientation forever.
So that's what I mean by a trans evolutionary motive.
Is it naive to think that we need a global government and we can make a global governance?
When you look at the problem of countries having competitive dynamics with each other
where nobody wants to price carbon properly, because if they do, their own economy will
be so damaged relative to whoever does and the radically decreased geopolitical power
will express itself as less military power, less trade power, and particularly with whoever
is at the leading edge of guiding the world system, this classic, the U.S. isn't going
to if China doesn't and vice versa, so then everyone is mostly actually just in an economics
race that is also bound to an actual arms race, and that's true for pricing carbon and
climate change. It's also true for fishing of the oceans and aerosols and on and on and on.
So if you have an issue like the atmosphere, aerosols in the atmosphere and ozone layer,
or you have an issue like the oceans or climate change, no country can solve that problem.
And any country that does the thing that is doing its share that is economically disadvantaged
in the short term by doing it, it just isn't going to do that if everyone else doesn't because
they are caught in the competitive dynamics. So when you look at that,
you're like, all right, well, we need global government because we have global issues,
we don't just have national issues, and you have to have governance at the level that you have issues.
But then of course, most thinking people aren't really a big fan of the idea of global government
because it's not a great idea to have unchecked power, though we don't have a good history of
being good stewards of unchecked power. And so like, you know, in many modern governments in
the United States, it was kind of like the foundation of the whole idea was,
let's separate church and state, let's separate the judicial branch and the legislative branch
and the executive branch, let's even separate the legislative branches in the separate houses,
let's like, let's try to create as much check and balance on power as possible.
So if you had a one world government that had enough power to be able to
price carbon properly and enforce fishing laws and etc., how do you prevent it from
becoming corrupted or captured? And so we need global government, and we don't want global
government. And so this is this like, you have catastrophes on one hand that need to be avoided,
and that typically looks like more control mechanisms of things that if you don't control,
will lead to catastrophe. And the control mechanisms typically lead to dystopias.
And so we want something that is not catastrophes or dystopias, we kind of call this the third
attractor. And that means you have to have control mechanisms that prevent catastrophes,
but you have to have checks and balances on the power within those. How do you do that?
So global governance and global government are not the same thing, right? Global government,
the idea that there's some centralized global monopoly of violence, the bad idea.
The idea that there is some more effective process of global coordination, even whether it's a more
effective process of nations engaging in multilateral agreements that can be facilitated by
technology that can make the participation or violation of those agreements more transparent,
or there is some process of global governance that has to occur where there's both
effective power for enforcement. This is why we can solve those types of coordination problems to
some degree, those raised to the bottom within a country where you have a monopoly of violence,
because the law and monopoly of violence just basically says, no, you're not allowed to cut
down any of those trees. That's a national park. And if you try, the police will stop you and they
have more capacity for violence than you do. With international issues where you don't actually
have international enforcement, it's really, really tricky. So for all of the really global issues,
and that looks like it's in each nation's interests to burn the coal as fast as it can,
and the oil, it's in each nation's interests to win the AI arms race, even though that increases
the likelihood that we all die from it in the long term. So global governance that has
appropriate checks and balances is a tricky topic, but it's a necessary topic.
What gives you hope today? A lot of things can be hope. I have noticed
in my own work, people in top positions of power and major institutions that affect the world,
being radically more aware of things that are fundamentally unviable about this world system,
and interested in deeper changes and actually starting to try to implement some things,
just even in the last couple of years than I had ever experienced previously.
So the idea that, you know, the kind of behavior that individuals can do on their own matters,
and the kind of stuff we can do locally, like prototyping new types of communities and new
types of cities, you don't solve climate change in time, and you don't solve planetary boundaries
in time, and you don't solve AI risk that way, right? That requires kind of agreement from
existing top-down organizations. They can't actually innovate a new world, they can just
stop bad things from happening with the right kinds of agreements. To innovate a new world
actually does require local and more participatory activity. But the fact that after COVID, and after
the extreme political polarization that has happened, and after how much of Australia burned,
and then flooded, and, you know, now with the war on Ukraine, and I think there was a situation
where previously people who were thinking about it and who were prescient realized this world system
is destabilizing, and is fundamentally not sustainable, most of the people who were
administrating it didn't think that. Now almost everybody thinks that, and that's
actually something that gives me hope. Great. How much time do we have to react, to avoid extinction?
Well, some species go extinct every day as a result of human activity.
So for them, we're already past existential risk. You know, Kiev was an incredibly progressive
place not very long ago. It wouldn't have seemed like a place where eminent catastrophic risk was
coming for many people. And, you know, that's even true of Syria, not that long ago. And
you see the pictures of what culture was like in 1968 in Iran. So it's not like how long do
we have before catastrophe hits. We're already in a rolling global catastrophe. Like how long does
Australia have before it burns that already happened? You know, and from extreme weather
events that are a result of human induced activity, poor environmental management,
and problems with utility companies and overuse of groundwater and climate change, right? And
how quickly does war escalate as a result of what's happening in Ukraine at a larger scale,
and already what we see in regarding Taiwan and Azerbaijan and Armenia and Iran and so many places.
These things could move very fast or more slowly in ways that are chaotic and totally
unpredictable. When you look at things like the planetary boundaries, how long until we pass
certain planetary boundaries, you'll hear people talk about this thing happens in 2050 and this
thing happens and by the end of the century or whatever with climate change. But we've already
passed some of the planetary boundaries. You know, there's a study just published in the
American Chemical Society Journal saying that certain toxic chemicals in rainwater kind of
ubiquitous around the world are past the EPA thresholds for human health. And this was particularly
the fluorinated surfactants, which don't break down, right? So they come forever chemicals.
But the idea that things that are carcinogens and cause birth defects and are endocrine disruptors
in rainwater all around the world are past the levels of human health tolerability
is a huge deal. It means even if you go get off grid as can be and try to live off the land, you
can't. And how quickly we're producing those chemicals, not only is there a cumulative effect
of them because they're persistent, but we're also increasing our production of them exponentially.
And so how long do we have? We're already in a situation of a break down of a world system.
It's already existential for many species. It's already catastrophic for people in many areas
of the world. And so I would reorient the question to be more like, is there anything that we can do
to have it not be totalizing? And the answer is yes. And the answer time wise on that is
the full life attention of everyone as best as possible, directed at better understanding
the issues and participating in the solutions is what's required. In individual level, we've become
a bit lazy maybe because we think like there is always a solution and we don't really need to act.
How to wake up and also how do you get the news because you have so many news in different
directions. We don't know who to believe and we don't know. We're not sure we need to act because
things always have a solution by themselves. There's a really interesting book called The
Politics of the Invisible written after Chernobyl because after the Chernobyl explosion,
the uranium is invisible. You can't see it with the human eye, obviously now COVID that's invisible
and yet totally lethal. And what he was exploring in Politics of the Invisible is because of
modern technology and chemistry, we can make things that are totally lethal that we can't
see that require people with Geiger counters and the ability to do physics that not everyone can
do to be able to determine safety levels. How does that work with democracy when most people
don't have the capacity to do that? So you'll see currently a lot of people doubting climate
change science, but nobody can actually, the average citizen can't run the IPCC's mathematical
models to say they work or they don't work or they, and so people are largely kind of left to faith
and you then end up getting politics driving people to either be kind of pro-institutional
or anti-institutional and the institutions get things wrong. So it's easy to be anti-institutional
and neither of the positions are actually viable and there is something other than truth, which is
the movement to power motivated in both of them. I see that when people think someone else will
come up with a solution, they feel kind of unmotivated, but also when people think there is no
solution, they feel unmotivated. And this is also something I find really interesting is when I
talk to someone who has a really fervent adamant view about whatever it is that,
whether it's vaccines or masks or what should happen in Russia, Ukraine or abortion law, whatever,
they go from complete certainty without understanding the position of the other
side or all the complexity or nuance well. And if I challenge it enough, regardless of which side
it is, and show them the increased complexity, okay, well if we price carbon that way and China
doesn't, then autocracy ends up running the world, so you're voting democracy out and whatever it is,
then for so many people, the first response when you increase, show them the way they're
thinking about it doesn't actually map to the complexity of the problem, they go from utter
certainty to nihilism in one step, they're like oh fuck it, it's too hard, it's too complex, I give
up. And to move from certainty to nihilism in one step is so damn lazy, like cognitively,
emotionally, epistemically lazy. And so I want people to go from certainty to like actually
I don't understand this all that well, actually climate change or global science or policy on
this thing is pretty complex, there are experts who spend their whole life working on it who
disagree, that doesn't mean there aren't solutions, but the one that was fed to me that everybody on
my political side agrees with and everyone on the other side disagrees with is probably not
a fair version of the whole truth. So I'm not going to give up because I don't know,
I'm not going to hold the certainty that I know because I don't, I'm going to work to try to
understand competently while recognizing I don't yet. And then even once I get to much deeper
understanding, I'll still recognize how much stuff I don't know that's relevant and new information
that might come in. So I want people to be much more epistemically rigorous and epistemically
humble at the same time. Epistemology meaning how we go about knowing things. So I want them to work
much harder at trying to come to understand while having much less certainty about their
current level of understanding. So when you ask how do you like what sources should people go to
for news or whatever, the ones they don't currently go to is the first answer. And then of course,
progressively better sources, not all the sources independent of political spinner are equally good,
but when you can see where do the various earnest experts on a topic disagree and you at least
understand those positions pretty well, then you start to have a sense of the topic.
As a philosopher, and because you spoke about politics, do you want to stay away from politics
or are you into politics? Politics meaning how people organize and how they coordinate and how
they make sense of the world together so they can make choices together. No, I'm totally focused on
that. The current political system in the United States does not do a very good job of helping
people collectively make sense of issues well, collectively identify all the values that matter
that are shared values and then collectively make good choices in the presence of the shared sense
making and shared values generation. So it's not that I think there is never a time to engage in
voting for a particular candidate or on a particular proposition, but how to engage in
metapolitics meaning how to evolve this political system and economic system, how to evolve the
political economy along with evolving the infrastructure and tech stack and the culture
and value system simultaneously because all three of those inter-effect each other. The culture,
the political economy and the infrastructure and technology, they all inter-effect each other so
you can't change any of them without changing all of them to think through what has to happen in all
of those for a viable world to come about. Very interested in that. Okay. Let's speak a bit about
you, something very interesting I found. You mentioned you were homeschooled by your parents.
Which qualities did your parents manage to let flourish in you that might not have been
so important also in the traditional education when you're changing teachers every year?
I was homeschooled. I did go to school both private and public schools for little bits
throughout my life so I have some experience of it, but most of my childhood was homeschooled.
But it was not traditional homeschooling meaning I didn't have the state curriculum and just do it
at home. My parents were kind of interested in running an educational experiment that
is a little bit closer to what people call unschooling today, but there's just no curriculum.
Okay. What they felt you need to learn. It's not what they felt. Their hypothesis was
expose the kids to all the different topics, see which ones they're interested in,
facilitate their interest and kind of trust them. And so it's aligned with some of the ideas of
Montessori and Dewey and constructivism. But you know, radical, no curriculum at all.
And I'm not saying that is what I would advise, but there's a lot good in that. And what
qualities that facilitated in me that most educational systems don't is all of my studies
were things I was interested in. And so my interest in learning was actually growing
all the time, right? There was never a place where I wanted to get out of school and go play
or do something else where learning felt like a burden or where I ended up not having any
negative association with study. And I had only positive association with it because I was studying
things I wanted to study. So I find that people tend to become good at things they really enjoy.
And so facilitating, like even if you have a curriculum, really paying attention to where
a student's interests are and where their passions are. And if there's a topic that isn't
appealing to them, trying to find a way that actually has it really appeal as opposed to
just forcing them to do the thing makes a huge difference not to their learning of that topic,
but to their relationship to learning itself. What is a special event that puts you on this
path of trying to see the truth, what's happening and observes the complexity of the world?
Lots of events. I mean, some people have a near death experience turning point that is
really kind of singular. I think most people's life path unfolds from lots of things.
So as I'm mentioning being homeschooled by parents who are obviously kind of
interested in childhood development and the books my parents read to me as bedtime stories were
Buckminster Fuller's Design Science and Fritschof Capra Systems Theory and Eastern Vedic Philosophy
and things like that. So there were people who were thinking about what is the world?
How does the world work? How do we integrate across the various philosophic and scientific
traditions? How do we improve civilization? Those were kind of like just the core thoughts that and
so I didn't really have to get on that path. And then a big part of my study as a kid was not just
just studying various areas of philosophy or science or whatever but also being actively
engaged in activism. My mom was particularly into kind of hands-on activism with
whether it was helping animals, the local animal shelter or larger kind of factory farm animal
rights issues or environmental issues and so being engaged in activism and then seeing what
the problems in the world were and then similarly having a system science and kind of design science
background to look at it and say how are these problems interconnected? What do they have in
common as generative dynamics? What would it take to address them more comprehensively? Because
it's not that hard to see that whether we're talking about issues in health care or issues in
war issues in politics or issues in the environment things like perverse economic incentive are one
of the drivers of all of them. So it's like well how do we think through an economic system that
doesn't have perverse incentive? Yeah I would say it was working across many different areas of
activism seeing how they related to seeing why the solutions that we were working on weren't
adequate because they didn't address the deeper dynamics. Those were kind of key early things for
me. Thank you very much Daniel. Yeah thank you. Thank you.

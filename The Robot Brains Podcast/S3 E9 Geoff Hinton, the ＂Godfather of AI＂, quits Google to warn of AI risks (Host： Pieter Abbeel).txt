Our guest today is Jeffrey Hinton. Over the past 10 years, AI has experienced breakthrough
after breakthrough after breakthrough. Underneath all of these breakthroughs is one single subfield
of artificial intelligence deep learning. Jeff was at the origins of so many of the
pioneering breakthroughs in deep learning that he is often referred to as the godfather
of artificial intelligence. His work has been cited over half a million times. That means
there is half a million and counting other research papers out there that build on top
of his work. Jeff's work has been recognized by the Touring Award, the Computer Science
Equivalent of the Nobel Prize. Jeff was actually the guest for our Season 2 finale with two
back-to-back episodes discussing the early days of deep learning, the various breakthroughs
and the potential for AI ahead of us. But something big changed just recently, which
is why I'm so excited to have Jeff back on already. As announced in the New York Times,
among many other outlets, Jeff has quit his job at Google so he can freely speak about
the risks of artificial intelligence. A part of him, he said, now regrets his life's work.
Jeff, so great to have you here with us. Welcome to the show.
Thank you. I enjoyed our last podcast and I'm hoping to enjoy this one.
Well, I hope so too. I'll try my best. Jeff, before diving into today's conversation,
I'd like to thank our podcast sponsors, Index Ventures and Weights & Biases. Index Ventures
is a venture capital firm that invests in exceptional entrepreneurs across all stages,
from seed to IPO. With offices in San Francisco, New York and London, the firm backs founders
across a variety of verticals, including AI, SaaS, fintech, security, gaming, and consumer.
On a personal note, Index is an investor-incovariant and I couldn't recommend them any higher.
Weights & Biases is an ML ops platform that helps you train better models faster with
experiment tracking, modeling data set versioning, and model management. They're used by OpenAI,
NVIDIA, and almost every lab releasing a large model. In fact, many, if not all, of my students
at Berkeley and colleagues at Covarrant are big users of Weights & Biases.
Well, Jeff, welcome back. So glad to have you here. Let me dive right in with the big headline
from last week. May 1st, New York Times headline summarizes actually very well why we decided
to catch up here. It reads, The Godfather of AI leaves Google and warns of danger ahead.
What's going on? You worked in the field for so long and now it is big change in how you think
about it. Can you say a bit more? It relates quite a lot to what we talked about in a previous
podcast about the forward-forward algorithm. For 50 years, I thought I was investigating
how the brain might learn by making models on digital computers using artificial neural networks
and trying to figure out how to make those learn. And I strongly believed that to make
the digital models work better, we had to make them more like the brain. And until very recently,
I believed that. And then a few months ago, I suddenly did a flip. I suddenly decided actually
back propagation running on digital computers might be a much better learning algorithm than
anything the brain's got. And there were several reasons for that. It started a year or two ago
when Palm could explain why a joke was funny. And that was a criterion I've been using for
a long time to decide whether these things were really intelligent. And I was slightly shocked
that Palm could explain why a joke was funny. Then we saw, in addition, farm things like chat
GPT and GPT-4. And they were very impressive on what they could do. And in particular,
they had about a trillion weights, but they knew much, much more than any one person. So we've
got about a hundred trillion weights. But these things know about a thousand times more common
sense facts than we do. And that suggested that back propagation was much, much better at packing
a lot of information into only a few connections, like only a trillion. So that made me change my
mind about whether the brain has a better learning algorithm with digital systems. And I started thinking
that maybe these digital systems have something the brain doesn't have, which is you can have many
copies of the same model running on different hardware. And when one copy learns something,
it can communicate that to all the other copies by communicating the weight changes
with a bandwidth of trillions of bits. Whereas when you learn something to communicate it to me,
I need to try and change my weight so that I would say the same thing as you. And the
bandwidth of sentences is only hundreds of bits per sentence. So maybe these things are much,
much better requiring knowledge because they can work in parallel much, much better than people can.
In some sense, you could then say that's a dream come true. You've been trying to build AI,
and now all of a sudden you realize that the work that's been going on and that you
pioneer the lotto is actually even more capable possibly than you had dreamed of when you started.
You were hoping to match human intelligence, and now you might have found a way to exceed it.
Well, yes, except that my dream was to understand how the brain works. And we still haven't done that.
We've built something better, and now we have to worry about what the something better might do.
Now, I think, I mean, you notice as well as anybody, the first question on a lot of people's
minds when they hear about AI is, you know, is it dangerous to us if it becomes smarter than us,
which you're alluding to, Jeff, that it might have the potential to be smarter than us,
is it going to kill us? What is it going to do with us? What happens to us humans
when we hit that point? So, obviously, we would like to stay in control,
and obviously, there's a problem with less intelligent things controlling more intelligent
things. Now, one thing we have on our side is that we're the relative evolution, so we come with
strong goals about not damaging our bodies and getting enough to eat and making lots of copies
of ourselves. And it's very hard to turn those goals off. These things don't come with strong
built-in goals. We made them, and we get to put the goals in. So, that suggests that we might be
able to keep them working for our benefit. But there's lots of possible ways that might go wrong.
So, one possible way is bad actors. Defence departments are going to build robot soldiers,
and the robot soldiers are not going to have the Asimov principles. Their first principle is
not going to be whatever you do, don't harm people. It's going to be just the opposite of that.
So, that's the bad actor scenario. But then there's the alignment problem, which is that
if you give these things the ability to create their own sub-goals, and you will surely want to
do that, because creating sub-goals makes you much more efficient. So, for example, if you want to
get to the airport, you create the sub-goal of finding some means of transport, and then you
work on that sub-goal, and breaking things up into sub-goals just makes you more efficient. And so,
I think we will give these digital intelligences the ability to create their own sub-goals.
And now there's a problem, which is, what if they create sub-goals that have unintended
consequences that are bad for us? So, here's a very common sub-goal, which makes a lot of sense.
Um, gain more control, because if you gain more control, you're better at achieving all your
other goals. So, for example, I was sitting in a very boring seminar, and I noticed a spot of
light on the ceiling. And I wondered where it came from, and then I noticed that when I moved,
the spot of light moved. So, then I realized it was a reflection of the sun on my watch.
And so, what did I do? Having solved the problem, did I go back and listen to the boring seminar?
Well, no, the first thing I did was try to figure out how I could get it to move on the ceiling.
I wanted to get control of that spot of light. We're probably having a built-in drive to get
control, but even if we didn't, that would be a very useful thing to do, to get control of things,
because it allows you to achieve all sorts of other things. So, I think as soon as you let them
develop their own sub-goals, one of the sub-goals will be get more control,
and we don't want them to get too much control. It seems that there is still a big,
I would say, gap maybe, at least conceptually. Maybe it can be bridged quickly in time, but a
big gap conceptually between the models today that are most visible, the language models, chat,
GPT, and competitors thereof, which do next-word completion, conceptually speaking, versus AIs
that have a goal. Though, of course, I've seen AIs with goals like the world champion of Go is an
AI that has the goal of winning the game of Go and is trained that way, but these AIs with goals
so far have been in rather contained environments, I guess, compared to the next-word prediction
models. So, how do you see a path? I mean, do you see it happen quickly to go from next-word
prediction to AIs that start doing things themselves in the world? It's not quite true that it's just
next-word prediction. That's the main way they learn, but they're also trained with human reinforcement
learning. So, people are telling them, don't produce that kind of answer, do produce this kind
of answer. That's very different from next-word prediction, and that's shaping them. And that
was a big breakthrough that OpenAI had, realizing that actually you don't need to do all that much
of that once you've trained a big language model, and you can radically shape the way it behaves
during that. It's a bit like raising a child. When you raise a child, most of what it learns to do
is just from wondering about it in the world and figuring out how the world works.
But the parent has a small amount of input by saying, no, and don't do that, and oh, very well done.
And that makes a big difference to how the child turns out. So, already we've got things other
than next-word prediction shaping them. And you can imagine going further, you could imagine having
large language models that are multimodal so that seeing visual input, and they're trying to do
things like open doors and put things in drawers, and then they'll have much more than just next-word
prediction. And even if it was just next-word prediction, people sometimes talk with that,
they say, oh, they're just autocomplete. But the point about autocomplete is, if you want to do
a really good job of next-word prediction, the only way to do a really good job is to understand
what was said. And that's what they're doing, they're understanding what was said in order to do
next-word prediction. And so, underneath, it needs to understand, effectively, everything that's going
on in people's minds, to be able to maximally accurately predict what a person will say next,
I think is your point, right? And so, it has to be a very capable model. The best prediction would
be made like that. It doesn't understand everything that's going on in people's minds yet, but it
understands quite a lot, and it's understanding more every day. And if you look at how good they
were five years ago, look at how good things were in 2015 or sometime like that, when people
were messing about with chatbots, they weren't that good before Transformers came along.
Look at how good they are now, and now project that forwards five years. And that's what's got me
worried. I think in five years time, they could well be smarter than people.
Now, when you say smarter than people, that's an interesting concept to even define, right?
What do you think of when you say smarter than people?
Well, it's very easy to see in a limited domain. I don't play Go, so I don't understand Alpha Go,
but I do play chess at a very low level. And when I look at Alpha Zero,
it's not just that he's doing lots of calculation, because he's doing a lot less calculation than
Deep Blue is doing, I believe. But he's got very good intuitions, and it makes brilliant
peace sacrifices in a kind of justified way. And so it's just much better than any human
chess player ever was. And I don't see why that should just be limited to that domain.
And it's not that it's just cheating by doing lots and lots of calculation. It has really
good intuitions by chess.
So could you imagine that, I mean, maybe not five years from now, but that, or maybe sooner,
who knows that somebody decides that they want the CEO of their company to be an AI, and that
that company will actually do well, because that's AI CEO better understands everything
going on in the company and the world, and can make better decisions?
Why not?
I don't think that's an absurd idea. I should say something about predicting the future here.
So you get lots of car accidents when people drive in fog. And the reason is,
people are used to driving at night, and now you can see the tail lights of the car in front.
And the brightness of the tail lights falls off as the inverse square of the distance.
That's the amount of light you get from them.
And so that's your kind of model, which is kind of quadratic fall-off.
As soon as you get fog, that's exponential fall-off. You lose a certain fraction of the
light per unit distance. And so relative to what you're used to, which is quadratic fall-off,
fog, you can see the first hundred yards perfectly clearly. And that makes you think
you're going to be able to see a thousand yards moderately clearly. But actually at 200 yards,
a wall comes down. You can't see anything beyond 200 yards, because it's exponential drop-off.
And that's a very good model for seeing the future. You can see very clearly what's going
to happen a few years down the road, and then suddenly you don't know anything.
You think you do because you extrapolate, but you extrapolate using a linear model or quadratic
model, and it's actually exponential. And we're hopeless at predicting the future in the long
term. I love the story of the New York Times, where in I think 1902, there was an article that
predicted that heavier than air flying machines would take a million or maybe 10 million years
to develop. And actually, they came in two months after that.
Well, there's a lesson there. Now, Jeff, if we go zoom out for a moment,
when you think about the risks of AI, and we'll get you, of course, I know you've said you see
many good things coming from AI. Also, that's why you remain very excited, but we need to approach
it the right way. But just to make sure we have a good framing of the risks, which are driving
your change in what you're doing right now. You mentioned bad actors could be more powerful to
put it to their bad uses. You mentioned alignment, namely making sure that AI
aligns with what we want. And I want to dive a lot deeper into that in a little bit, but
are there others than these two that concern you? Oh, yes, there's lots of things that many other
people have talked about. So there's, and they've been talked about for a while, like AI is incredibly
biased if it's trained on incredibly biased data, it just picks up class from the data.
That doesn't worry me as much as it worries some people, because I think people are very biased.
And actually, understanding the bias in an AI system is easier than understanding the bias in
a person, because you can just freeze the AI system and do experiments on it. You can't do
with that with a person. If you try and freeze the person and do experiments, they realize what
you're up to and they change what they say. So I think actually bias is easier to fix in an AI
person than it is in an AI system than it is in a person. Those job losses and job losses aren't
really the fault of AI. So if you think about autonomous driving, if we make something that
can drive long distance trucks, a lot of truck drivers will lose their jobs. And some people
think, well, that's the fault of AI. But when we made things that were better at digging ditches,
machines that could dig ditches, we didn't say, oh, well, we shouldn't do that. These machines are
terrible. Actually, at the time, they probably did. But in the end, we didn't keep digging ditches
with spades. We dug ditches with machines, because that's just a much better way to do it. When the
people who used to dig ditches with spades had to find other jobs. In a decent society, if you make,
if you increase productivity, it should help everybody. The danger is, in the society that most
of us live in, if you increase productivity, the gains are going to go to making the rich richer
and possibly making the poor poorer. But I don't see that as the fault of AI. And I don't see that
as a reason to be luddite and to stop developing AI. Because AI has tremendous good it can do.
Even in the area of autonomous driving, if an AI runs over a pedestrian, everybody thinks
shocking, we should stop developing it. Even if on the very same day, people run over lots of
pedestrians. We all know that, well, you and I believe, I think, that eventually they'll get
autonomous driving working properly. And it will save lots and lots of lives.
It won't lapse in attention the same way. And it'll probably be more cautious.
So that's a good thing. In medicine, it's even more obvious what the good's going to be. We're
going to get much better family doctors who know much more. We're going to be able to
get much more information out of medical scans. I said in 2016, that by 2021, we wouldn't need
radiologists. I was talking in the context of interpreting scans. So what I meant was we wouldn't
need them for interpreting scans. That was over ambitious. Already, we've got systems
in Pakistan and India doing diabetic retinopathy stage in diabetic retinopathy.
And saving lots of people's sight. We've already got systems that are comparable
with good radiologists from many other kinds of scan. I think it'll still be a few years
before we move to a system where most of the interpretation of scans is done by these AI
systems with radiologists looking over their shoulders. But that's coming. And that'll be
tremendously useful. There's an enormous amount of information in something like a CAT scan that
isn't being used at present. And we'll be able to use much more information, I think. There's
going to be things like making better nanomaterials. Obviously things like Alpha Fold, where Alpha Fold
is now done about a billion years work in predicting protein structure if it was done the old way by
PhD students. That's enough to pay for most of AI. Lots of things like that are going to be
tremendously helpful. I think if you could make nanomaterials that make better solar panels, that
will probably compensate for all the carbon dioxide produced by data centers. You could
actually make all the data centers use solar power with better solar panels. So there's
tremendous good to be had. And I've just given a few examples, but we know that more or less
anything you do, AI can make it more efficient. AI can do parts of that job. Sometimes all of it,
sometimes parts of it. Anytime you're producing textual output, AI can make you more efficient,
even for things like legislative recommendation. So I don't think there's any chance people will
stop developing it because it's going to be so useful. Now he's saying there's no chance people
will stop developing it because there's so many positive uses, but also almost every positive
use can get paired with somebody using it, a bad actor using it in a different way. There was a call
nevertheless a few weeks ago to stop the development of not AI as a whole, but to not train any even
larger models than GPT-4, right? That's a very specific call. And I think you've talked about
that too, even though I don't think you sign off on that specific letter. What are your thoughts on
that? That specific letter, maybe it was politically sensible because it got people's attention,
but there was never any chance that people would do that. It was completely... I didn't sign it
because I thought it was silly in the sense that what they're calling for is something completely
infeasible. If you did that in the States, they wouldn't do it in China. And even if you didn't
say some China, they wouldn't do it in Russia. So there was never any chance that development
would be stopped. I also agree with Sam Altman that if you think about the existential risk of
these things getting out of control, then the best way to handle that, given that you can't
stop this stuff developing, is for the people developing it to be doing experiments as they
develop it, understanding much more about how you control it. Anybody who's written a program knows
you can't just sit in an armchair and figure out the solution to problems like that. You have to
experiment. It has to be the people who are developing it who fiddle with it and see what
happens and what doesn't happen. Then you learn all sorts of strange things you wouldn't have expected.
And so that's what I think is going to happen. And really what I want to call for is that
comparable amounts of effort and resources should go into developing it and figuring out how to stop
the bad side effects and stop it getting overall control. So I want to sort of sound a warning
that we've got to take this very seriously. Right now it's kind of 99% of the money goes into
developing. It's 1% into safety. It should be much more like 50-50. From an academic point of
view, that might not be too hard to steer. I mean, in a sense you could imagine funding agencies
adjusting the calls for proposals to be closer to what you're saying, Jeff, right? And that doesn't
seem infeasible at all to have something like that happen. But how about in the private sector,
which is more driven by developing things that make more money, do you think it's realistic that
50% of the money will be made by spending time on the safety aspects? Or how do we get there,
that that actually matters just as much? Yeah, I don't know. This is where I resort to saying,
I'm just a scientist who knows just something that might happen. I'm not a policy expert.
I, in Google, they were fairly responsible when they had the lead. After they developed
a transformer, they published them and they developed chatbots. They didn't put their chatbots
out there because they knew there'd be lots of bad side effects if people started using them.
And so I thought they were fairly responsible. It's just when Microsoft funded Open AI and
then used their chatbot in Bing, Google didn't have much alternative but to respond by
trying to do the engineering on their chatbots so they could make a version of Bard that was
comparable with ChatGPT. In a capitalist system, they don't have much alternative. And
the way you get big companies to do things that don't immediately make profits is with
regulations, I think. Regulations have been called for a while actually by some people,
and particularly Elon Musk has called for regulation in the AI space for a while
without necessarily being particularly specific about what the regulation should be.
Is there a certain regulation that you think would make sense?
So there are many dangers of AI and people tend to sort of confound them all as you want
big soup of danger. And I think it's important to separate them out. So putting people out of work,
those encouraging political divisions by trying to give people things that will make them indignant
to click on because they love being indignant. I love it too. I love clicking on these things
that are going to make me indignant. There's the existential threat. And then there's the threat
of truth disappearing because you don't know what's real and what's fake. These are all different
threats in addition to things like discrimination bias. So we need to think which threat we're
talking about. And for the threat of truth disappearing because we just swamp with fakes,
you can imagine there's something we might be able to do. It's going to be very tough, but
governments really don't like people printing money. They like to be the ones that do that.
And so there's very severe penalties for printing fake money. And the penalties are actually
if someone gives you some fake money and you know it to be fake and then you take it to a store,
that I believe is a criminal offence too. Not as bad as printing yourself, but trying to pass
counterfeit money, that is illegal. So we need something like that for AI generated material.
It has to be clearly labeled as AI generated. And if you try and pass it off as real,
there should be severe legal penalties for that. Whether we can be good enough at detecting it
is another matter. But at least you can see the sort of direction in which you need to go in
to prevent us being swamped by fake videos. Yeah, enforcement wouldn't be easy, I guess, but
I agree. I mean, the principle seems clear, I guess, even if the enforcement might be hard.
Let me tell you why the enforcement is going to be really hard. Imagine you use deep learning to
help you. So you build an AI system that can detect the fakes, right? We know that building an AI
system that can detect the fakes is a very good way of training your generator to make realistic
fakes. That's what gen generative adversarial nets do. So there doesn't seem to be much hope
in the direction of using an AI system that can detect fakes. It'll just allow the generators to
make better fakes. When you think about cryptographic solutions, that's always been on my mind.
Imagine something where whatever content that's created gets a cryptographic signature,
traditional cryptography, nothing to do with Web 3 or anything like that. It just gets a
signature attached to it that shows who is the author of this piece of material or who shot the
video and so forth. And then it'd be the reputation of the author that be at stake in terms of credibility
if they put a lot of fakes in the mix. It used to be the case in Britain. It's probably still the case
that whenever you print anything, even if it's a pamphlet for a political demonstration,
the printer's identity has to be printed on it. It's illegal to print things without the printer's
identity being on it. And that's basically the same idea, early version of that idea. I know nothing
about cryptography and cryptographic stuff, so I might have my area of expertise, but it sounds
to me like that's extremely sensible. That's your thoughts on regulation, even though it might be
hard to enforce that in principle has a clear framework to it for avoiding being flooded with
fake news, fake videos, fake text and so forth. You can also imagine regulations for
clickbait, for avoiding political division by clickbait. This is the point of which I'm glad
I no longer work for Google. So Facebook and YouTube and lots of other social media
encourage division by offering up to you things that'll make you indignant
and things that are within your echo chamber. And you could imagine trying to use legislation
to prevent that. It's tricky to do, but I think it's important if you want to keep democracy
and not have this incredible division between two groups of people, each of whom think the other
is completely crazy, to do something about that. It needs to be that you discourage these companies
from offering up things that'll just make you indignant. I recall a few years ago,
there was a brief, very brief moment where actually Facebook did some self-policing in some
sense where they would not let you share anything that ended with you'll never guess what happened
next. That was automatically not allowed. But yeah, it's of course much more difficult than that.
And it's not an area where I have any expertise, but it just seems to be,
it's the kind of thing where maybe regulation could have an effect.
Now, the big thing, of course, a lot of people worry about is AIs taking over the world.
Right. That's the existential threat. And that's what I'm talking about.
That's where I've changed my mind a lot recently. I used to think it was way off,
30, 50, 100 years. Now, I think I have very little confidence in predicting how far off it is,
but I would guess five to 20 with very low confidence.
Not taking over the world, but being smarter than us. And the big issue is,
once it's smarter than us, does it take over the world or do we still control it?
And that's what's in our hands then in the next 30 to 50 years is what you're talking about,
what we should focus on. It may be in our hands. It may be that it's historically inevitable that
digital intelligence is better than biological intelligence. And it's the next stage of evolution.
I have not, but that's possible. And we should certainly do everything we can to keep control.
Sometimes when I'm gloomy, I think, imagine if somehow frogs had invented people.
And frogs needed to keep control of people. But there's rather a big gap in intelligence.
I don't think it will work out well for the frogs. But of course, that's not really a very
realistic argument because people evolved. And so people evolved with their own goals,
including the goal of make more people. And these digital intelligences don't have their own goals.
If they ever did get the goal, if a digital intelligence got the goal of make more of me,
then evolution would kick in, right? And the one that was most determined to make more of
itself would win. So we don't want them to ever get that goal.
We don't want it's what you're saying. But would it be hard to give it to them if we wanted to?
I mean, if we just... Oh, I think you could quite easily give it to digital intelligence,
the goal of making more of itself. You just say, that's your main goal in life.
And I think that would be crazy. I don't even Putin would do that.
Well, it's a very risky thing to give them the goal of
they're being more of themselves. But maybe, I mean, I'm just imagining here, but maybe the reason
that humans are where they are today is because of evolutionary competition.
Maybe, maybe the smartest digital intelligence would emerge from a competitive environment.
Maybe not against humans, but against other digital intelligences.
Yes. So you can write your whole new phase of evolution where biological intelligences,
because they're very low power, can evolve. They build power stations, and they build
digital intelligences, which provide a lot of power and very accurate fabrication. So
you can have many copies of the same model. These digital intelligences are then the next
phase of evolution. They compete with each other and get better because they're competing with
each other. That's certainly a scenario that's not at all inconceivable.
We'd prefer to have a scenario where we create something much more intelligent than us,
and it kind of replaces the UN. You have this really intelligent mediator that doesn't have
goals of its own. Everybody knows it doesn't have its own agenda. It can look at what people are up to,
and it can say, oh, don't be silly. You'll gather what it's going to lose if you do this. You do
that. We'll believe it like children would believe a benevolent parent. That's the
utopian version as opposed to the gestopian version. I don't think that's out of the question either.
That's feasible, possibly. It could happen, yes.
But it seems like it would require humanity to get together and want it. But
technologically, what you described seems feasible.
Yes, it seems feasible to me. But like I say, when you're speculating about things with which you
have no experience, you tend to be quite a long way off. As soon as you get a little bit of practical
experience, you revise your theories because you realize how far off you are. We need to be doing
a lot of work as these things are developed in understanding the risks and having little empirical
experiments where we can see what tends to happen. When you make these smart things, do they tend to
try and get control? Do they tend to come up with goals of, hey, I want to make more of me?
It seems like there's a natural tension between the short term and the long term here.
Meaning, if I look at the short term, having an AI that sets its own goals, that can go do things,
could be a nice to have. Yeah. And say, okay, make me some more money on the side. Do this,
do that. I have these resources, those resources. See what you can do with it and get some things
done for me. Very convenient. And then it could be a slippery slope possibly to what it decides to
set as its sub-goals to get that done. That's the alignment problem. Right. It's very, very hard.
But it does seem like you are thinking of a clear distinction here between
AIs that try to get things done that can set goals versus AIs that are purely advisory,
that are called upon for their wisdom, have a lot of wisdom because they have seen so much,
know so much, have predictive powers, but that they are just advisors, not actors.
That would be great, right? That would be very useful.
But that seems a clear thing to pursue possibly, right?
Maybe, yes. Now, you can't make it safe just by not allowing it to press buttons or pull levers.
Why is that?
A chatbot will have learned how to manipulate people. He would have read everything Machiavelli
ever wrote and all the novels in which people manipulate other people and it will be a master
manipulate if it wants to be. And it turns out you don't need to be able to press buttons and
pull levers. You can, for example, invade a building in Washington just by manipulating people.
You can manipulate them into thinking that the only way to save democracies is to invade this
building. And so a kind of air gap that doesn't allow an AI to actually do anything other than
talk to people isn't sufficient. If he can talk to people, he can manipulate people.
And if he can manipulate people, he can get them to do what it wants.
So it's not so much about the air gap. It's about the purpose, the built-in purpose.
Yes, it's about the goal. Yeah. If he'd ever developed the purpose of make many more of me,
we're all in trouble.
So Jeff, I like your thoughts on regulation, but also the challenges with regulation.
Now for a moment, let's imagine things maybe don't go exactly the way we hope.
And that we hope that there is this AI advisor who can just ask questions to and helps us make
the right decisions. But let's say somehow the AI does emerge with a goal and a purpose
and starts doing things for itself rather than for us. I mean, couldn't be the case that it just
decides to run off to a different solar system with more energy available and, you know, we're
back to where we are today. We're entering this time of huge uncertainty where we're going to be
dealing with things that we've got no experience with, the sort of no empirical data on what it's
like to interact with things smarter than us. So we just don't know. I mean, I think the right
attitude is like no idea. I talked to Elon Musk the other day and he thinks we'll get things more
intelligent than us. And what he's hoping is they'll keep us around because we'll make life
more interesting. If you have a world without people in it, or without animals in it, it's just
not as interesting as a world with people in it. That seems like a pretty thin thing to rest
humanity on to me. But he thinks it's quite possible these things will get much smarter
and they'll gain control. I didn't actually ask him if I could have a space on the rocket.
Yeah. I guess Mars is even with speed of light. It gives you some delay before it reaches you.
Now, in a scenario where you said you talked with Elon Musk, the scenario he seems to envision
is one where AI and humans might fuse together, his Neuralink company. You've talked about AI,
you've talked about the brain, about the combination of both more than anyone else possibly.
What are your thoughts on that kind of future? I think that's pretty interesting. I've always
thought that people have audio in and they have audio out and they don't have video out. But if
people had video out, we'd be able to communicate better. So that's not exactly what Elon's planning
to do. He's planning to get sort of brain-to-brain transferred to a fairly abstract level of thought.
So now you need to transfer things that the other person, in a way, the other person can
understand them. A much less ambitious project would be to have video out because now the other
person knows how to deal with video. They can deal with that as input. And I think if we have video
out, it would improve communication quite a bit. So a person, if you want to communicate something to
me, you can talk or you can draw diagrams. But presumably before you draw the diagram, you have
a kind of picture in your head. Maybe not, but probably you do. And if you could just communicate
those pictures in your head very quickly, that ought to increase the bandwidth. Maybe only by
a factor of two, but that would still be a big win. But maybe by a factor of more than that.
It's not unnecessarily helped, but I think it might. I did have a plan for improving communication
between drivers, where every car has on its roof a big LED display where you can display up to two
words. But it turns out in that case, you wouldn't actually need to make the two words variable.
You could just put them next.
It might induce a lot more road rage, yes. I'm a bit worried there.
Yes. So I don't think that was a very good scheme.
Now, you've alluded to this earlier that maybe biological evolution is just a starting point.
And maybe it's natural to be followed by digital evolution, computer-based or other forms.
And in principle, we could ask the question there too, right? Imagine we, I mean, you've also said
that you really, that's not the future you currently would want. But imagine that somehow
that would be the future, that after humanity, there's a digital life form that is more dominant
in some sense than humans are today on Earth. That could still go in many ways. You can imagine
a digital life form that is really good to us, to others, to everything that's around, versus
digital life forms that maybe destroy everything. Is that worth thinking about? If that scenario is
the scenario, how do we make sure it's a good version of it? Yes, that's definitely worth
thinking about. And there'll be something very different about them, because they don't have
to worry about death. Basically, people haven't really noticed this yet, but we've discovered
the secretive immortality. Well, the secretive immortality is just the computer science thing
and make the software separate from the hardware. As soon as you do that, it's true of these
artificial neural nets. If one piece of hardware dies, the knowledge doesn't die, the weights can
be recorded somewhere. And as soon as you've got another piece of hardware that can execute the
same instructions, then it comes back to life again. So these digital intelligences are immortal,
as opposed to the kind of biological intelligence we have where whatever our learning algorithm is,
it appears to make use of all the little quirks and peculiarities of the wiring of our brain and the
funny way our neurons work. That makes it much more efficient in energy terms, but it means that when
the hardware dies, the knowledge dies with it, unless you've taught it to somebody else, which is
what I'm trying to do now. So Ray Kurzweil, for example, would like to be immortal. And I don't
think he will because he's biological. But the digital devices, the digital intelligences we're
producing are going to be immortal. And maybe once you've got immortality, you maybe get to be a bit
nicer. Or also less afraid, I guess. You also get to be a lot less afraid. Yes.
So digital soldiers that know that they're immortal, maybe that's not so good.
Right. Now, I mean, in some sense, when I haven't given this enough time to think about compared
to what I would like to, but just at a high level, when you think about this, it seems like it comes
back to what's the purpose of not just humanity, but the purpose of life, right?
Yes, absolutely. So when I was a student, a troubled teenager, I started off studying physics and
physiology at Cambridge. And then I really wanted to know what the purpose of life was. So I started
philosophy for a year. That didn't really help. So I switched to psychology. And that didn't really
help either. And I ended up in AI. And I now think, I mean, I'm an atheist. So I think the purpose
of life is make as many copies of yourself as possible. That's what evolution seems to do.
And we've evolved. We've evolved as hominids that live in small, warring tribes. And that's our
evolutionary history. And it's recent. That's a recent enough history, we haven't been able to
change that much. And if you look at society now, it's small, warring tribes at every level.
We just made it fractal. So for those hominids, insofar as they have any purpose,
it's make more copies of yourself. That all makes sense to me. But can we hope for something more?
I mean, we're able to think of more. We're able to at least articulate the notion that maybe it'd
be nice to be more than what you just described. Nice to be more than just trying to replicate
ourselves maximally. If you're in a small tribe of hominids, to make the tribe successful,
you want to help other people in the tribe. So we have a strong urge to help other people in our
tribe. And you may have noticed this, I noticed this very strongly in academic departments.
If you're in a big department, in general, you'd rather your department got resources
rather than some other department. So the University of Toronto, there's lots and lots of
professors of French. And I think it'd be better if there were less professors of French and more
professors of computer science. But within computer science, there's professors in different areas.
And I think it'd be good if there were a lot of professors in machine learning.
But within my group, when I was at the University of Toronto, actively, there was a very strong
religious group. And that's just from my evolutionary inheritance. So I think we are very
strongly altruistic towards members of our group. We're willing to sacrifice things to help members
of our group. And you know that when you write a very long letter of recommendation for a student
you really like, you're sacrificing your time to help someone in your group. And you're much
more willing to do that for someone in your group than for someone who maybe equally could,
but wasn't in your group. Yeah, it's a natural thing, I think, and it's because you've already
invested so much time in them, you know them so well. I think, yeah, it's pretty natural for
that for that to happen. So it's not just that we want to mend more copies of ourselves, we want
to make our group successful. Now, if we could make that generalize to bigger and bigger groups,
then we can probably get better societies. I maybe have a question from a slightly
different angle, Jeff, though that I agree would be great if we could generalize it to
bigger and bigger groups, see what we could get to. But imagine in some sense the
counterstance to developing new technology for good would be, I could imagine a counterstance that
says, hey, my life today is pretty good. I have not too much to complain about. I could maybe imagine
a version where more people, maybe all people could have a similar standard of life with some
adjustments here and there and so forth. Why don't we just stagnate and keep it this way,
we're happy, we stay happy. But then to me, that falls short, because to me it seems like when
there is at least personal, when there is no progress, when everything stays the same, that's
really uninteresting, not exciting, and almost defeats the purpose of that we're even here.
And it seems that progress somehow, at least for me, progress is this kind of natural notion of
like something we need for it to even make sense to be here.
I don't think I agree with that. I think I can imagine a society in which
it's not going anywhere in your sense. It's not you're not getting more of you,
but what you're doing makes you happy. It's sustainable. When I was at University College
London, and I was director of the Gatsby unit, we had four faculty members and a dozen or more
graduate students and postdocs, and a member came around from the university, which is,
how are you planning to expand your department? And I replied that I actually wasn't planning
to expand it. I thought it was a very nice size as it was. That wasn't an acceptable answer,
but that was how I felt about it. Yeah, so maybe I meant it in a slightly different way. I meant
it more in the sense of imagine 100 million years from now, just essentially people doing the
exact same thing we're doing today. Life has not really evolved. You could land in the earth
100 million years from now now, and you couldn't tell the difference. Wouldn't that be like somehow,
I don't know, feel unsatisfactory that there has nothing has evolved or changed in 100 million
years? I'm not so sure it would be. I mean, if we got a decent society and most people were happy
and having a fulfilled life and having lots of nice social interactions,
and it just stayed like that forever, I don't see the problem with that.
Well, then maybe we should try to achieve it.
No, but there's a lot of people. I mean, I think one thing that's wrong with
society present is economists obsessed with growth, and if you're not growing something's wrong,
and that's unless you're going to leave the planet, that's utterly unsustainable.
We talked a bit earlier about there's both a technological
kind of line of work ahead to make sure we can align the AI with humanity, and there is
regulation government work ahead. Now, on the technological side of things, do you have any
recommendations for today's AI researchers or people in other fields who might want to contribute?
What do you see as big opportunities? So I guess playing with the most advanced chat
blocks and trying to understand more about how they actually are intelligent, how they do it,
because we still don't really understand how they do it. I mean, we know just how they work
in terms of transformers, but I think we're not really very clear about exactly how they
manage to do these reasoning tasks, and looking at how you can control them as they develop.
I've said that before, but I mean, I think that would be a very sensible thing to do.
But I should emphasize, I'm not really an expert on these alignment problems, as people have been
thinking about them much longer than me. People who don't do deep learning like Stuart Russell,
and lots of other people who do deep learning, people like Roger Gross have been thinking about
these things for much longer than me, and they're the experts. I've just come to this very late,
because I suddenly changed my mind about how soon these super intelligences may be coming.
And I see my role as getting a bit old for doing technical work. I see my role as
using my reputation to sound the alarm. That's how I quoted somewhere that you said,
I feel a little bit old to do technical work. And I think back to our conversation less than a year
ago, and I'm like, wow, that conversation was far more inspiring than pretty much any other
conversation I've had in a very long time, inspiring at the technical level of things we talked about.
So I think you have a very high bar for what it means to still be able to do something interesting.
Well, it's kind of you to say so, but when I tried to scale up the forward forward algorithm,
I couldn't get it to compete with backdrop. And that was one of the things that made me think
that maybe back propagation is just much better than what the brain's got.
Now, final question for this conversation here, Jeff.
Top of my mind ever since your announcement has been you leave Google, you have all this extra
time now in your hand to freely do whatever you want to do. What does that mean? Does that mean
you'll be working solo? Are you finding a new affiliation? Are you starting an organization
of your own? What's happening in the near future? My main goal is to watch all those good movies on
Netflix. I never had a chance to watch when I was working too hard for 50 years. I mean,
Google, they talk a lot about life work balance. I never went to any of those seminars. I didn't
have time. My view as a researcher is I got this from Alan Newell at Carnegie Mellon,
who used to tell the graduate students, if you're not working 80 hours a week, you're not a serious
scientist. And I'm afraid that's been my view. It may not have done me much good in life,
but it's time to stop doing that. So that's my main objective. I didn't think I'll be able to
stop doing research because it's so much fun, even when you're not as good as you used to be at it.
So I will probably keep working on variations on the forward forward algorithm and variations on
trying to do a stochastic version of that propagation by taking random steps and then
multiplying the step length by how much you've improved things. That seems to be
that'll scale much worse. But if you have lots of little local objective functions, maybe you
can have lots of little local modules that are all learning, optimizing the local objective
functions by taking random steps and scaling the step by how much it improved their local
objective function. And maybe that's how the brain can learn big systems without being able to
back propagate. So I'll keep thinking about things like that. But I will actually watch a lot of
movies and I'll try and spend a lot of time with my kids. We're no longer kids. It's nice to hear
your passion for the brain is still so big to understand how it might work. In terms of the
alignment work and AI risk work, of course, you've done a lot by your announcement by, I mean,
how many interview requests did you get after the announcement?
So the day after, the New York Times came out on a Monday. On the Tuesday, I was getting an
interview request every two minutes. That was stressful. To begin with, I felt I ought to reply
to them. And that just wasn't feasible. Then someone who knows much more about the media told
me, no, they don't actually expect you to reply. That made life easier.
Yeah. I mean, every two minutes, that's wild. Do you expect going forward to spend time
evangelizing and making people aware of what was this a one time thing and you feel like,
okay, now people know it's clear the job is done?
I don't know. I wasn't expecting this bigger reaction. And I haven't really had time to think
through what happens next. I suspect I'll keep encouraging people to work on the alignment
problem, work on thinking about how to keep this is under control. And I'll probably keep giving
the occasional lecture about that. But I don't intend to sort of make that a full time job.
What I enjoy much more is fiddling about with programs that are implementing interesting
algorithms and see how to make them work. That's what I like doing. That's what I'm good at. And
I'll go back to doing that. If I, when I get bored with Netflix.
And isn't there an apparent contradiction there between trying to get across how important it
is that we work on alignment, but still have a personal, maybe stronger attraction to
understanding how the brain might work?
Understanding how the brain might work is not the thing that's going to get us into trouble.
It's building things that are better than the brain's.
I see.
It's in trouble. Understanding how the brain might work might help,
it might help more in sort of how you deal with this horrible device of thing where you
get indignant camps who don't believe what the others are saying.
I grew up in the sort of 50s and 60s when there was a general belief that
if there was better education and more understanding, everything will get better.
That belief has disappeared. But I still believe that it ought to be the case that if we understand
better, we understand how people work better, we should be able to make society better.
I'd love that as a concluding sentence for this conversation, Jeff.
Thank you so much.
Well, thank you.

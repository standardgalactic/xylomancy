Hey, Amade, good to hear you.
It was a pleasure.
Yeah.
So, where are you today?
I'm in London.
Good.
On the other side of the planet, I'm in Santa Monica.
It's been quite the extraordinary game of ping pong out there these last four or five
days.
I didn't think the first thing that AI would disrupt would be reality TV, right?
Yeah.
It's been fascinating how X has become sort of the go-to place to find out the latest
of where Sam is working and what's going on with the AI industry.
Yeah, you found the notifications in the way it goes.
I mean, it's the thing, what else will move at the speed of this?
I was saying to someone recently, AI research doesn't really move at the speed of conferences
or even PDFs anymore, right?
Just wake up and be like, oh, it's 10 times faster.
So, I think that's why X is quite good.
I actually unfollow just about everyone and set the AI algorithms, find the most interesting
things for me.
So, I've got like 10 people that I follow and it's actually working really well.
It's getting better.
Well, it has been, I've been enjoying the conversation.
It really feels like you're inside an intimate conversation among friends as this is going
back and forth.
I think this entire four or five days has been an extraordinary, up-close, intimate conversation
around governance and around what's the future of AI because, honestly, as it gets faster
and more powerful, the cost of missteps is going to increase exponentially.
Let's begin here.
I mean, you've been making the argument about open source as one of the most critical elements
of governance for a while now.
Let's hop into that.
Yeah, I think that open source is the difficult one because it means a few different things.
Is it models you can download and use?
Do you make all the data available and free?
And then when you actually look at what all these big companies do, all their stuff is
built on open source basis.
It's built on the transformer paper.
It's built on the new model by Khayfouli and Vera1.ai is basically Lama.
It's actually got the same variable names and other things like that, plus a gigantic
supercomputer, right?
And the whole conversation has been how important is openness and transparency and what are
the governance models that are going to allow the most powerful technology on the planet
to enable the most benefit for humanity and the safety?
So I mean, you've been thinking about this and speaking to transparency, openness, governance
for a while.
I mean, what do you think is going to be, what do you think we need to be focused on?
Where do we need to evolve to?
It's a complicated topic.
I think that most of the infrastructure of the internet is open source, Linux, everything
like that.
I think these models, it's unlikely that our governments will be run on GPT-7 or Baad
or anything like that.
How are you going to have black boxes that run these things?
I think a lot of the governance debate has been hijacked by the AI safety debate where
people are talking about AGI killing us all, and then there's this precautionary principle
that kicks in.
It's too dangerous to let out because what if China gets it?
What if someone builds an AGI that kills us all?
It'd be great to have this amazing board that could pull the off switch, you know?
Whereas in reality, I think that you're seeing a real social impact from this technology and
it's about who advances forward and who's left behind if we're thinking about risk because
governance is always about finding, as you said, the best outcomes and also mitigating
against the harms.
There's some very real, amazingly positive outcomes that are now emerging that people
can agree on, but also some very real social impacts that we have to mitigate against.
Let's begin.
How is stability governed?
Stability is basically governed by me.
So I looked in foundations and dows and everything like that, and I thought to take it to where
we are now.
I needed to have very singular governance, but now we're looking at other alternatives.
What do you think it's going to be?
Where would you head in the future?
Let's actually jump away from this in particular.
But do you recommend the most powerful technologies on the planet?
How should they be governed?
How should they be owned?
Where should we be in five years?
I think there need to be public goods that are collectively owned and then individually
owned as well.
So for example, there was the tweet kind of storm, the kind of I am Spartacus or his
name is Robert Boulson from the OpenAI team saying, OpenAI is nothing without its people.
Stability, we have amazing people, 190 and 65 top researchers.
Without its people, we're open models used by hundreds of millions, it continues.
And if you think about where you need to go, you can never have a choke point on this technology,
I think, if it comes into part of your life.
Like the phrase I have is not your models, not your mind.
So these models, again, are just such interesting things, take billions of images or trillions
of words and you get this file out that can do magic, right, trade on magic sand.
I think that you will have pilots that gather our global knowledge on various modalities
and you'll have co-pilots that you individually own that guide you through life.
And I can't see how that can be controlled by any one organization.
You've been on record talking about having models owned by the citizens of nations.
Can you speak to that a little bit?
Sure, so we just released some of the top Japanese models from visual language to language
to Japanese SDXL as an example.
So we're training for half a dozen different nations and models now.
And the plan is to figure out a way to give ownership of these datasets and models back
to the people of that nation.
So you get the smartest people in Mexico to run a stability Mexico or maybe a different
structure that then makes decisions for Mexicans with the Mexicans about the data and what
goes in it.
Because everyone's been focusing on the outputs, the inputs actually are the things that matter
the most.
The best way I've thought about thinking of these models is like very enthusiastic graduates.
So hallucinations isn't just probably too hard.
A lot of the things about like, oh, what about these bad things the models can output?
It's about what you've input.
And so what you put into that Mexican dataset or the Chinese or Vietnamese one will impact
the outputs.
And there's a great paper in Nature, Human Behavior today about that, about how foundational
models are cultural technologies.
So again, how can you outsource your culture and your brains to other countries, to people
that are from a very different place?
I think it eventually has to be localized.
Yeah, I think one of the points you said originally is we have to separate the issue of governance
versus safety and alignment.
Are they actually different?
So I think that a lot of the safety discussion or this AGI risk discussion is because the
future is so uncertain because it is so powerful, right?
And we didn't have a good view of where we're going.
So when you go on a journey and you don't know where you're going, you'll minimize
from maximum regret, you'll have the precautionary principle.
And then that means you basically go towards authority, you go towards trying to control
this technology when it's so difficult to control.
And you end up not doing much, you know, because everything to go wrong.
When you have an idea of where we're going, like you should have all the cancer knowledge
in the world at your fingertips or climate knowledge, or anybody should be able to create
whole worlds and share them.
Then you align your safety discussions against the goal, against the location that you're
going to.
Again, just like setting out on a journey, I think that's a big change.
Similarly, most of the safety discussion has been on outputs, not inputs.
If you have a high quality data set without knowledge about anthrax, your language model
is unlikely to tell you how to build anthrax, you know?
And transparency around that will be very useful.
So let's dive into that safety alignment issue for a moment, because it's an area you and
I have been talking a lot about.
So Mustafa wrote a book, Mustafa Suleiman wrote a book called The Coming Wave, in which
he talks about containment as the mechanism by which we're going to be making sure we
have safe AI.
You and I have had the conversation of, it's really how you educate and raise and train
your AI systems in giving, making sure that there's full transparency and openness on
the data sets that are utilized.
Do you think containment is an option for safety?
No, not at all.
Like a number of leaders say, what if China gets open source AI?
The reality is that China, Russia, everyone already has the weights for GPT-PORC, they
just downloaded it on a USB stick.
You know, you would know that there's been compromised, right?
There's no way they couldn't.
The rewards are too great.
And there is a absolutely false dichotomy here, and a lot of the companies want you to believe
that giant models are the main thing, and you need to have these gigantic, ridiculous
supercomputers that only they can run.
I mean, we run gigantic supercomputers, but the reality is this, the supercomputers and
the giant trillion zillion data sets are just a shortcut for bad quality data.
It's like using a hot pot or sous viding a steak that's bad quality.
You cook it for longer and it organizes the information with stable diffusion.
We did a study and we showed that basically 92% of the data isn't used 99% of the time.
You know, because now you're seeing this with, for example, Microsoft's FI release, it's
trained entirely on synthetic data.
Dali 3 is trained on RVAE and entirely synthetic data.
You are what you eat.
And again, we cooked it for longer to get past that.
But the implications of this are that I believe within 12 to 18 months, you'll see GPT-PORC
level performance on a smartphone.
How do you contend that?
And how do you contain it when China can do distributed training at scale and release
open source models?
So Google recently did 50,000 TPU training run on their V5Es.
The new V5Es, their TPUs are very low powered relative to what we've seen.
But again, you can do distributed dynamic training.
Similarly, like we funded Five Mind and we've seen Google DeepMind just in
new paper on localization through distributed training.
The models are good for fast stuff and cheap enough that you can swarm them and you
don't need to try on supercomputers anymore.
And that has a lot of implications.
And how are you going to contain that?
So coming back to the question of do you mandate training sets?
Do you, you know, does the government set out what all companies should be utilizing
in mandate if you're going to have a aligned AI?
It has to be trained on these sets.
How do we, how do we possibly govern that?
Look, we have food standards, right?
For ingredients.
Why don't you have data standards for the ingredients that make up a model?
It's just data compute and some algorithms, right?
And so you should say they are the standards and then you can make it
compulsory.
That will take a while or you can just have an ISR type standard.
This is good quality model training, good quality data, you know, and people will
naturally gravitate towards that and it becomes the default.
Are you working, are you working towards that right now?
Yeah, I mean, look, we spun out a Luther AI as an independent 501c3 so they
could look at data standards and things like that independent of us and the
opposite of open AI.
Um, and this is something I've been talking to many people about and we're
getting national data sets and more so that hopefully we can implement good
standards similar to how we offered opt out and how the billion images opted
out of our image data set because everyone was just training on everything.
Is required?
No, but is it good?
Yes.
And everyone will benefit from better quality data.
So there's no reason that for these very large model training runs, the
data sets should not be transparent and logged.
Again, we want to know what goes into that.
And again, if we have the graduate analogy, what was the curriculum that the
graduate was taught at, which university they go to?
It's something that we'd want to know.
But then why do we talk to GPT-4?
We don't know where it went to university or where it's been trained on.
It's a bit weird.
What do you think the lesson is going to be, uh, on from the last four days?
I'm just confused.
I don't know who was against who or what.
I'm going to just post it.
Are we against misalignment or mollock?
I think probably the biggest lesson is it's very hard to
align humans, right?
And the stakes are very large.
Like, why is this so interesting to us?
The stakes are so high.
You tweeted something that was, you know, serious and, and unfortunately
funny, which was how can we align AI with human humanity's best interests?
We can't align our company's board with its employee's best interests.
Yeah.
Well, the thing is, it's not the employee's best interests.
It's like the board was set up as a lever to ensure the charter of open AI.
So if you look at the original founding document of open AI from 2015, it is a
beautiful document talking about open collaboration, everything.
And then it kind of changed in 2019, but the charter still emphasizes
cooperation, safety, and fundamental post about this back in March, when I
said the board and the government structure of open AI is weird.
Like, what is it for?
What are they trying to do?
Because if you say you're building AI, in their own road to AI, they say,
this will end democracy.
I remember reading that.
There's democracy.
There's no way democracy survives AI because either obviously it'll be
better and you get it to do it, or you can swear to everyone or we'll die.
Or it's utopia forever, right?
Abundance, baby.
Yeah.
There's no, yeah.
Well, this thing, regardless, there's no way it survives AI.
There's no way capitalism survives AI.
The AI will be the best trader in the world, right?
And it's like, who should be making the decisions on the AI, right?
Assuming that they achieve those things, and that's in their own words.
So I think that people are kind of waking up to, oh, there's no real
way to do this properly.
And previously we were scared of open and being transparent.
Everyone getting this, which can with the original thing of open AI.
And now we're scared of who are these clowns, you know, and put it in the
nicest way, because this was ridiculous.
Like you see better politics in a teenage sorority, right?
And it's fundamentally scary, but unelected people, no matter how great they
are, I think some of the board members are great, should have a say in
something that could literally upend our entire society, according to their own words.
I find that inherently anti-democratic and a liberal.
At the end of the day, you know, capitalism has worked and it's the
best system that we have thus far.
And it's a self, you know, it's built on self interest and built on
continuous optimization and maximization.
I'm still wondering where you go in terms of governing these companies
at one level, internal governance, and then governing the companies at
a national and global level.
Has anybody put forward a plan that you think is worth highlighting here?
Not really.
I mean, organizations are a weird artificial intelligence, right?
They have the status of people and they're slow, dumb AI and they eat
off hopes and dreams.
That's what they feed on, I think.
But this AI can upgrade them.
It can make them smarter.
They can how do you coordinate?
And from a mechanism design perspective, it's super interesting.
Like in markets, I think we will have AI market makers that can tell stories.
The story of Silicon Valley Bank went around the world in two seconds.
The story of open AI goes around.
AI can tell better stories in humans.
It's inevitable.
I think that gives hope for coordination, but then also it's
dangers of disruption.
I want to double click one second on the two words that you use most
openness and transparency and understand fully what those mean one moment.
Because, you know, and the question is not only what they mean, but how
fundamental it need to be.
So openness right now and your definition in terms of AI means what?
It means different things, but the different things, unfortunately.
I don't think it means open source.
I think for me, open means more about access and ownership of the models so
that you don't have lock steps.
Like you can hire your own graduates as opposed to relying on
consultants, transparency comes down to, I think for language models in particular,
I don't think this holds for media models.
You really need to know what it's been taught.
That's the only way to safety.
Like you should not engage with something or use something if you don't
know what its credentials are and how it's been taught.
Because I think that's inherently dangerous as these get more and more
capabilities.
I don't know if we get to AI.
Like if we do, I think it'll probably be like Scarlett Johansson and her, you
know, just to get by thanks for assuming we don't, you still need transparency.
So again, how can any government or regulated industry not run on a transparent model?
They can't run on black boxes.
I get that and I understand the rationale for it.
But now the question is, can you prove transparency?
I think that, again, a model is only three things really.
It's the data, the algorithm and the compute.
And then they come and the binary file pops out.
Then you can tune it with RLHF or DPO or genetic algorithms or whatever.
But that's really the recipe, right?
And so the algorithms, you don't need algorithmic transparency here versus
classical AI because they're very simple.
One of our fellows recreated the Palm 540 billion parameter model.
This is Lucid Raines on GitHub.
You look at that if you're a developer and you want to cry is GitHub.
It's crazy.
In 206 lines of pytorch, and that's it.
The algorithms are not very complicated.
Running a gigantic super computer is complicated.
And this is why they freaked out when Greg Brockman kind of stepped down
because he's one of the most talented engineers of our time.
Built this amazing gigantic clusters.
And then the data and how you structure data is complicated.
So I think you can have transparency there because if the data is
transparent and who cares about the supercomputer, who really cares about the algorithm?
You know, now let's talk about the next term, alignment here.
Alignment's thrown around in lots of different ways.
How do you define alignment?
I define alignment in terms of objective function.
So YouTube was used by the extremists to serve ads for their nastiness.
Right?
Why?
Because the algorithm optimized for engagement, which then optimized
for extreme content, which then optimized for the extremists.
Did YouTube mean that?
No, but they're just trying to serve ads up, right?
But it meant it wasn't aligned with its users' interests.
And so for me, if you have these technologies that we're going to
outsource more of our mind, our culture, our children's futures,
to do that are very persuasive, we have to ensure they're aligned
with our individual community and societal best interests.
And I think this is where the tension with corporations will come in.
Because whoever licenses Scarlett Johansson's voice will sell a lot of ads,
you know, they can be very, very persuasive.
But then what are the controls on that?
No one talks about that.
The bigger question of alignment is not killerism, making sure the AI doesn't
kill us. But again, I feel that if we build AI that is transparent, that we
can test that people can build mitigations around, we are more likely
to survive and thrive.
And also, I think there's a final element to here, which is whose alignment?
Yes, different cultures are different, different people are different.
What we found with stable diffusion is that when we merge together the models
that different people around the world have built, the model gets so much better.
I think that makes sense because a monoculture will always be less fragile
than a diversity.
Again, I'm not talking about in the DEI kind of way.
I'm talking about it in the actual logical way.
So we have a paper from our reinforcement learning lab called CARPA called QDHF,
QD AIF, Quality and Diversity through Artificial Intelligence Feedback.
Because you find these models do get better with high quality and diverse inputs.
Just like you will get better if you have high quality and diverse experiences.
And I think that's something that's important, that we'll get lost if all
these models are centralized.
You and I've had a lot of conversations about timelines here.
We can get into a conversation of when and if we see AGI, but we're seeing more
and more powerful capabilities coming online right now that are going to cause
a lot of amazing progress and disruption.
How much time do we have, EMAID?
And we've had a conversation when we were together at FII about
the disenfranchised youth coming off COVID.
So let's talk one second about timelines.
How long do we have to get our shit together?
Both as AI companies and investors and governors of society.
We don't, I mean, the speed here is awesome and frightening.
How long do we have?
Almost everything ever all at once, right?
We don't have long.
Like AGI timelines for every definition of AGI, I have no idea.
It will never be less than 12 months, right?
Because it's such a step change.
So let's put that to the side.
OK. Right now, everyone that's listening, are you all going to hire
the same amount of graduates that you hired before?
The answer is no.
Some people might not hire any because this is a productivity and an answer.
And we have the data for that across any type of knowledge industry.
You just had a great app that you can sketch and it does a whole
iPhone app for you, right?
I've gone on record and saying there are no programmers
who know in five years, why would there be?
What are interfaces?
You had a 50% drop.
I just pushed that on my Twitter in hiring from Indian IITs.
That's crazy.
So what you're going to have in a couple of years is around the world
at the same time, these kids that have gone through the trauma of COVID
highly educated STEM, programming, accountancy, law.
Simultaneously, people will hire massively less of them
because productivity enhances and you don't need as many of them.
Why would you need as many paralegals?
And that, for me, is a gigantic societal issue.
And the only thing I can think of is a stoke open innovation
and the generative jobs of the future through open source technology.
Because I don't know how else we're going to mitigate that
because, Peter, you're a student of history.
What happens when you have large amounts of intelligent
and disenfranchised youth history?
We've had that happen a few times.
We just had Arab Spring not long ago.
Revolt, Civil War, if not international law.
War is a good way to soak up the excess youth, but it's not pleasant.
It's not pleasant for society.
And fundamentally, the cost of information gathering
organization has collapsed.
Like, yeah, you look at a stable video that we released yesterday, right?
It's going to get so much better so quickly, just like stable debut,
the cost of creating movies increases, the demand for quality stuff increases.
But there's a few years where demand and supply don't match.
And that's such a turbulent thing to navigate.
That's one of the reasons I'm creating stabilities for different countries.
So the best and brightest for me, which can help navigate them.
I loved your...
And people don't talk about it.
I loved your idea that the stability models and systems will be owned by the nation.
In fact, the one idea that I heard you say, which I thought was fantastic,
was you graduate college in India, you're an owner in that system.
You graduate from Nigeria, you're an owner in that system.
Basically, to incentivize people to complete their education
and to have them have ownership in what is ultimately the most important asset
that nation has, and talk about it as infrastructure as well.
I think that's an important analogy that people don't get.
This is the knowledge infrastructure of the future.
It's the biggest leap forward we have because you'll always have a co-pilot
that knows everything in a few years and that can create anything in any type.
But it must be embedded to your cultural values
and you can't let anyone else own that.
So it is the infrastructure of the mind
and who would outsource the infrastructure to someone else?
That's why I think Nigerians should own the models of Nigerians for Nigerians
and it should be the next generation that does that, you know?
That's why you give the equity to the graduates.
That's why you list it.
That's why you make national champions because, again, that has to be that way.
This is far more important than 5G, and this gives you an idea of the scale.
We're just at the start.
The earlier doctor base, a trillion dollars was spent on 5G.
This is clearly more important, more than a trillion dollars that we spend on this.
And again, it flips the world.
And so there is huge threat for our societal balance.
And again, I think open is a potential antidote to create the jobs of the future.
And there's huge opportunity on the side because no one will ever be alone.
And we can use this to coordinate our systems, give everyone all the knowledge
that they need at their fingertips and help guide everyone if we build
this infrastructure correctly.
And I don't see the highlight can be closed.
AGI, the conversation and the definition of AGI has basically been all over the place.
Ray Kurzweil's prediction has been for 30 years that it's 2029.
Again, that's a blurry line of what we're trying to target.
But Elon's talked about anywhere from 2025 to 2028.
What are you thinking?
What's your timeline for even digital superintelligence?
I honestly have no idea.
People looking at the scaling laws and applying it, but as I've said, data is the key.
And it's clear that we already have, you could build a board GPT and it'd be better
than most corpora boards, right?
So I think we're already seeing improvements over the existing.
One of the complications here is swarm AI.
So even like, it's the whole thing, like a duck size human or a hundred human sized ducks.
We're just at the start of swarm intelligence.
And that reflects and represents how companies are organized.
Andre Carpethi has some great analogies on this in terms of the new knowledge.
OS and that could take off at any time.
But the function of format of that may not be this whole Western anti-conformized
consciousness that we think of, but just incredibly efficient systems that
displace existing human decision making, right?
And so there's an entire actual range of different AGI outcomes, depending on your
definition. And I just don't know.
But I feel again, like I wake up and I'm like, oh, look, it's fed up 10 times the model.
You know, like, I'm just not it.
No one can predict this.
But there is a point at which, I mean, we were heading towards an AI singularity using
the definition of a singularity as a point after which you cannot predict
what's coming next. And that isn't far away.
I mean, how far out is it for you a year or two years?
I think you're heading towards it in the next few years.
But like I said, every company, organization, individual has an objective function.
My objective function is to allow the next generation to navigate what's coming
in the optimal way and achieve their potential.
So I don't want to build an AGI.
I don't want to do any of this.
Amplified human intelligence is my preference and trying to mitigate against
some of the harms of these are genetic things through data transparency, good
standards, and making it so people don't need to build gigantic models on crap,
which I think is a major danger, if even if not from AGI.
But again, we just don't understand, because it's difficult for us to
comprehend superhuman capabilities.
But again, we're already seeing that in narrow fields.
We already know that it's a better rider than us.
So now we really know that it can make better pictures than us and a better
physician and a better educator and a better surgeon and a better everything.
Yeah. And again, I think it's this mythos of these big labs being AGI focused,
whereas you can be better than us in like 5% of the stuff that humans can do.
And that's still a massive impact on the world.
And they can still take over companies and things like that, right?
Like, if you take over a company, then you can impact the world.
And there's clearly with a GPT for a thousand of them orchestrated
correctly, that can call up people and stuff.
You wouldn't know it's not the CEO, you know, I can make an MA GPT and then
they won't have to make all these tough decisions in any of that.
And most of my decisions aren't that good, so it's probably better.
So I think that we're getting to that point.
It's very difficult.
And the design patterns are going fast.
We're at the iPhone 2G, 3G stage, we've got copy and paste.
And we just got the first stage as well of this technology, which is the creation step.
It creates stuff.
The next step is control and then composition, where they're annoying
because chat GPT doesn't remember all the stuff that you've written.
That won't be the case in a year.
And the final bit is collaboration, where these AIs collaborate together
and with humans to build the information superstructures of the future.
And I don't feel that's more than a few years away and it's completely
unpredictable what that will create.
Let's talk about responsibility that AI companies have for making sure
that their technology is used in a pro-human and not a disruptive fashion.
Do you think that is a responsibility of a company, of a company's board,
of a company's leadership?
How do you think about that?
Again, with the corporate capitalist system, it typically isn't.
Because you're maximizing shareholder value and there aren't laws and regulations.
Which is why I think there's a moral, a social and legal slash regulatory aspect to this.
Companies will just look at the legal slash regulatory.
In some cases, we'll just ignore them, right?
But I do think, again, we have a bigger moral and social obligation to this.
This is why I don't subscribe to EA or EAC or any of these things.
I think it's complicated and it's hard.
Given the uncertainty and how this technology proliferates, and you've got to do your best
and be as straight as possible to people about doing your best.
Because none of us are qualified to understand or do this.
And none of us should be trusted to have the power over this technology, right?
You should be questioned and you should be challenged with that.
And again, if you're not transparent, how are you going to challenge?
When I think of the most linear organizations on the planet, I think of governments,
maybe religions, but governments, let's leave it there.
Let's talk about Western government, at least the U.S.
I would have said Europe, but I'll say the U.K. and Europe.
What steps should they be taking right now?
If you were given the reins to say, how would you regulate?
What would you want them to do or not do?
I believe it's a complicated one.
So I signed the first FLI letter.
I think I was the only AIC CEO to do that back before it was cool.
Because I said, I don't think AGI will kill us all, but I just don't know.
I think it's a conversation that deserves to be hand.
It's a good way to have that conversation.
And then we flipped the wrong way, where we went overly AI death risk and other things like that
and governments were doing that at the AI Safety Summit in the U.K.
And then we had the King of England come out and say, this is the biggest thing since fire.
I was like, okay, that's a big change in what it means, right?
The King of England said it, so I must be on the right track.
But I think if you look at it, regulation doesn't be fast enough.
Even the executive order will take a long time.
The EU things will kind of come in.
Instead, I think that governments have to focus on the tangibles.
AI killerism, again, it can be addressed by considering this as infrastructure
and what infrastructure we need to give our people to survive and thrive.
The U.S. is in a good initial place with the CHIPS Act.
But I think you need national data sets.
You need to provide open models to stoke innovation
and think about what the jobs of the future are, because things are never the same again.
You don't need all those programmers when co-pilot is so good
and you're moving co-pilot from level above, which is compositional co-pilot
and then collaborative co-pilot, right?
You would be able to talk and computers can talk to computers better than humans can talk to computers.
So we need to articulate the future on that side, but then the other side.
Like one of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer, right?
We did a Unital diagnosis.
And the loss of agency you feel, and many of you on this call
will have had that diagnosis to the near and dear, is huge.
And then I had 1,000 AI agents fighting every piece of information about pancreatic cancer.
And then after that, I felt a bit more control.
Why don't we have a global cancer model that gives you all the knowledge about cancer
and helps you talk to your kids and connect with other people like you,
not for diagnosis or research, but for humans?
This is the Google Med Palm 2 model, for example, that outperforms humans in diagnosis, but also empathy.
And what if we arm our graduates to go out and give support to the humans that are being diagnosed in this way?
That makes society better and it's valuable, you know?
And that's an example of a job of the future, I think.
I don't believe in UBI. I think we've been universal basic jobs as well.
Yes, universal basic opportunity, right?
Universal basic opportunity, universal jobs, but then post-makers need to think about it now,
because the graduate unemployment wave is literally a few years away.
That will happen.
Yeah, that is, I mean, when I think about what I parse the challenges we're going to be facing in society
into a few different elements.
I think, you know, what we have today is amazing, and if generative AI froze here,
we'd have an incredible set of tools to help humanity across all of its areas.
And then we've got what's coming in the next, you know, zero to five years.
We've talked about patient zero, perhaps being the U.S. elections and the, you know,
I think you had said, you know, it was Cambridge Analytica that required interference.
No, it's any kid in the garage that could play with the elections.
That's a challenging period of time.
And this graduate unemployment wave, as you mentioned, coming right on its heels.
That's, you know, the question becomes, is the only thing that can create alignment
and help us overcome this AGI at the highest level,
meaning it is causing challenges, but ultimately is a tool
that will allow us to be able to solve these challenges as well.
I mean, that's a crazy thought, right?
Like, all this stuff is crazy, like the sheer scale and impact of it.
And, you know, these discussions, we had them last year, Peter,
and now everyone's like, yeah, that makes sense.
We're like, oh, wow.
Right. It may be AGI.
It may be these coordinating automated story makers and balances from the market.
Right.
Next year, there's 56 elections with four billion people heading to the polls.
What could possibly go wrong?
Oh, my God.
But again, the technology isn't going to stop.
Like, even if stability puts down things, if open AI puts down things,
it will continue from around the world because you don't need much to train these models.
Again, the supercomputer thing is a myth.
You've got another year or two where you need them.
You don't need them after that.
And that is insane to think about.
You just released stability video.
Congratulations.
Our stable visual diffusion.
Thank you.
And I'm enjoying some of the clips.
How far are we away from me telling a story to my kids and saying,
let's make that into a movie?
Almost two years away.
So this is a building block.
It's the best creation step.
And then, like I said, you have the control step composition and then collaboration.
And self-learning systems around that.
So we have Kung Fu UI, which is our node-based system where you have all the logic that makes up an image,
like you take a dress and a pose and a face that combines them all.
And it's all encoded in the image because you can move beyond files to intelligent workflows that you can collaborate with.
If I send you that image file and you put it into your Kung Fu UI,
it gives you all the logic that made that up.
How insane is that?
So we're going to step up there.
And what's happened now is that people are looking at this AI like instant versus,
again, the huge amount of effort it took to take this information structure.
But the value is actually in stuff that takes a bit longer.
Like when you're shooting a movie, you don't just say, do it all in one shot, right?
Unless you are very talented director and actor.
You have mise en place, you have staging, you have blocking, you have cinematography.
It takes a while to composite the scenes together.
It will be the same for this, but a large part of it will then be automated
for creating the story that can resonate with you.
And then you can turn it into Korean or whatever.
And there'll still be big blog clusters like Oppenheimer and Barbie.
But again, the flaw will be raised overall.
So we're going to step up there.
There's like Oppenheimer and Barbie.
But again, the flaw will be raised overall.
Similarly, like we had a music video competition.
Check it out on YouTube with Peter Gabriel.
He allows us to use kindly his songs and people from all around the world
made amazing music videos to his thing, but they took weeks.
So I think there's somewhere in the middle here where, again, we're just at that early stage
because chat GPT isn't even a year old.
Stable diffusion is only 14, 15 years.
And I think you'd agree that neither of them is the end hole and be all.
It's the earliest days of this field.
I had this conversation with Ray Kurzweil two weeks ago.
We were just after a singularity board meeting we had.
We were just hung on a Zoom and chatted.
And the realization is, unfortunately, the human mind is awful at exponential projections.
And despite the convergence of all these technologies, we tend to project the future
as a linear extrapolation of the world we're living in right now.
But the best I can say is that we're going to see in the next decade,
between now and 2033, we're going to see a century worth of progress.
But it's going to get very weird very fast, isn't it?
There's two-way doors and there's one-way doors, right?
Like in December of last year, multiple headmasters called me and said,
we can't set essays for our homework anymore.
And every headmaster in the world had to do that same thing.
It's a one-way door.
And this is the scary part, the one-way doors, right?
Like when you have an AI that can do your taxes.
What does that mean for accountants?
All the accountants at the same time.
It's kind of crazy, right?
It is.
And the challenge, I mean, one of my biggest concerns,
so listen, I'm the eternal optimist.
I'm not the guy whose glass is half full.
It's the glasses overflowing.
And one of the challenges I think through when I think about where AI, AGI, ASI,
however you want to project it to is the innate importance of human purpose.
And unfortunately, most of us derive our purpose from the work that we do.
I ask you, tell me about yourself and you jump into your work and what you do.
And so when AI systems are able to do most everything we do,
not just a little bit better, but orders of magnitude better,
redefining purpose and redefining my role in achieving a moonshot or a transformation,
it's the impenance mismatch between human societal growth rates and tech growth rates.
What are your thoughts there?
Yeah, I mean, I think again, exponential is a hot.
If I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well, that's not possible.
Why? You know, like GPT-4 is impossible. Stable diffusion is impossible, right?
Like now they've almost become commonplace.
But why would you need supercomputers and these things?
So I do agree this is mismatch.
And that's why we're in for five years of chaos.
And that's why we call it stability.
I saw this coming a few years ago and I was like, holy crap.
We have to build this company.
And now we have the most downloads of any models of any company,
like 50 million last month versus 700,000 from Astral, for example.
And we will have the best model of every type except for very large language models by the end of the year.
So we have audio, 3D, video, code, everything.
And a lovely, amazing community because it's just so hard again for us to imagine this mismatch.
There's a period of chaos.
But then on the other side, like there's this PDoom question, right?
The probability of doom.
I can say something with this technology, the probability of doom is lower than without this technology
because we're killing ourselves.
And this can be used to enhance every human and coordinate us all.
And I think what we're aiming for is that Star Trek future versus that Star Wars movie, right?
Yes, I'm into that.
And I think that's an important point.
The level of complexity that we have in society, we don't need AI to destroy the planet.
We're doing that very well.
Thank you.
But the ability to coordinate.
So one of the things I think about is a world in which everyone has access to all the food, water, energy, healthcare, education that they want.
Really, a world of true abundance in my mind is a piece more peaceful world, right?
Why would you want to destroy things if you have access to everything that you need?
And that kind of a world of abundance is on the backside of this kind of awesome technology.
We have to navigate the next period.
I believe we'll see it within our lifetimes, particularly if we get longevity songs, right?
And that's so amazing, right?
But then we think about, as you said, why peace?
A child in Israel is the same as a child in Gaza.
And then something happens.
A liar is told that you are not like others and the other person is not human like you.
All wars are based on that same line.
And so, again, if we have AI that is aligned with the potential of each human that can help mitigate those lies,
then we can get away from war because the world is not scarce.
There is enough food for everyone.
It's a coordination failure.
And that can be addressed by this technology.
I agree. One of the most interesting and basic functions or capabilities of generative AI
has been the ability to translate my ideas into concepts that someone who is a different frame of thought can understand.
But that's what this generative AI is.
It's a universal translator.
Yes, for sure.
It does not have facts.
The fact that it knows anything is insane.
Hallucinations is a crazy thing to say.
Again, it's just like a graduate trying so hard.
GPT-4 with 10 trillion words and 100 gigabytes is insane.
Stable diffusion has like 100,000 gigabytes and a 2 gigabyte file.
50,000 to 1 compression is something else.
It's learned principles.
Yes.
It's knowledge versus data.
It's knowledge versus data.
And if you apply some experience, you get the wisdom, right?
Because it's learned the principles and contexts and it can map them to transform data.
Because that's how you navigate.
You don't navigate based on logical flow.
We have those two parts of our brain.
Navigate sometimes based on instinct, based on the principles you've learned.
So Tesla's new auto driving model, self-driving model, is entirely based on,
I can't say which architecture and if they said it publicly, is based on this technology.
It doesn't have any rules.
It's just learned the principles of how to drive from massive amounts of Tesla data
that now fits on the hardware without internet.
And so they went from self-driving being impossible to now, hey, it works pretty well, you know?
Because it's learned the principles.
And so that's why this technology can help solve the problem.
And this is why it can help us amplify our intelligence and innovation.
Because the missing part, the second part of the brain.
You know, I can't give more details yet, but next week we're announcing the largest X prize ever.
It's $101 million.
It's 101.
So Elon had the $100 million prize that got him to fund a few years ago for carbon sequestration.
And the first funder of this prize wanted to be larger than Elon's.
I said, okay, you had the extra million.
It's for luck.
It's for luck.
We did our seed around 101 million.
Oh, really?
Okay.
Hi, that's great.
That's your popular number.
Anyway, and it's in the field of health.
I'll leave it at that.
Folks can go to xprize.org to register to see the live event on November 29th.
We're going to be debuting the prize, what it is, it's going to impact 8 billion people.
Long story short, it's a nonlinear future because we are able to utilize AI and make things
that were seemingly crazy before likely to become inevitable.
And it's an amazing future we have to live into.
Yeah.
I mean, again, because it's one-way doors.
The moment we create a cancer GPT, and this is something that we're building.
We have trillions of tokens and Google TPUs and things like that.
That organizes global cancer knowledge and makes it accessible and useful.
Even if it's just for guiding people that have been diagnosed, the world changes.
The 50% of people that have a cancer diagnosed in their lives in every language and every
level will have someone to talk to and connect them with the resources they need and other
people like them and talk to their families, you know?
And how insane is that?
And so again, these positive stories in the future need to be told, right?
Because that will align us to where we need to go as opposed to a future full of uncertainty
and craziness and doom.
In our last couple minutes here, buddy, what can we look forward to from stability in the
months and years ahead?
We have every model of every type and we'll build it for every nation and we'll give back
control to every nation.
So coming back to governance here, again, is the nation state the unit of control?
Is it?
No.
My thinking is the stability of every nation should have the best and brightest of each
because what you've seen is there are amazing people in this sphere, the best and brightest
in the world know this is the biggest thing ever and they all want to work in it.
And it's just defining the right people with the right intention.
The brightest people go back to Singapore or Malaysia or others because of the future
of their nations.
And again, now we're doing a big change and we don't talk about all the cool stuff we
do.
We've just taken it because you need to articulate that positive vision of the future because
the only scarce resource is actually this.
It's human capital.
It's not GPUs.
It's not data.
It's about the humans that can see this technology and realize that they can play a part in guiding
it for the good of everyone, their own societies and more.
And that's, again, what I hope stability can be.
Well, I wish you the best of luck, pal.
Thank you for joining me in this conversation.
It's been a crazy four or five days and wish Sam and Greg and the entire OpenAI team stability
in their lives.
Yeah.
I think they have a nice Thanksgiving.
They're an amazing team building world-changing technology.
It's such a concentration of talent.
I think, again, I really felt for them over the last few days, you know, much as I kind
of post memes and everything, I posted that as well.
I think this will bring them closer together and hopefully they can solve the number one
problem that I've asked them to solve, which is email.
Solve email.
Right?
And then we'll crack on from there.
All right.
Cheers, my friend.
Thank you.

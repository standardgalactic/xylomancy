Welcome to the Future of Life Institute podcast.
My name is Gus Docker and I'm here with Dan Hendricks.
Dan is the Director of the Center for AI Safety.
Dan, welcome to the podcast.
Glad to be back.
You are also an advisor to XAI.
Maybe you can tell us a bit about that.
Sure. So XAI is Elon's new AGI project.
It's still very much in its early stages,
so it's difficult to say
specific things about what they'll be doing
or what the specific high level strategy is to give a sense.
Elon has been interested in the failure mode
of sort of eroded epistemics
where people don't have a shared sense of consensus reality
and this might make it harder for a civilization
to appropriately function.
There are other types of extras that he's concerned about as well.
His sort of probability of doom
or of that of an existential catastrophe
is around like 20 to 30%.
So he takes this, I would guess, more seriously
than any other leader of a major AGI organization,
but exactly how when it goes about reducing that risk
is still somewhat to be determined.
There is an interest in building more true seeking AIs,
but on other occasions too,
he'd mentioned that we should have AIs
with the objective of preserving human autonomy
or maximizing the freedom of action
and on other instances,
in thinking about good objectives for AI systems,
having them increase net civilizational happiness over time.
So I think that this reflects sort of a plurality
of different goals that he thinks AI systems
should end up pursuing
rather than picking just exactly,
rather than just picking one.
I think it's relevant to note
that it's a fairly serious effort.
I'd anticipate that it would probably be
one of the main three AI companies
next year, the year after,
like OpenAI, Google DeepMind, and XAI.
So I don't think of it as a smaller effort,
but it has the capacity
to have a substantial issue of force.
The other top AI corporations you mentioned,
Anthropic, Google DeepMind, OpenAI,
have backing from giant tech companies.
Does XAI similarly have some backing
from Tesla, for example?
I can't specifically say about that,
but this is not a subpart of Tesla.
This is not an organization inside of Twitter or X,
and it's not an organization inside of Tesla.
The main topic of conversation for this episode
is your paper on catastrophic risks from AI,
and specifically categorizing these risks.
So you categorize risks from,
catastrophic risks from AI in four different categories.
Maybe we should just start by sketching out those categories
and then go into depth later.
Yeah, so I guess at a very abstract level,
there's risks if people are trying to use AI
intentionally to cause harm.
That's a basic one.
So there's an intentional catastrophe
that would be malicious use.
Another one is where there are accidents,
and if there are accidents,
this would often be the consequence of the AI developers
using these very powerful systems
or potentially leaking them,
or accidentally putting in some bad objective
or doing some gain of function,
but that would be some accident risks.
So that relates to organizational risks
or organizational safety.
The third would be these environmental
or structural risks.
Basically, where AI companies are,
or AI developers, be those companies,
or maybe in later stages, countries,
are racing to build more and more powerful AI systems
or AI weapons, and this structural risk
incentivizes companies to, or to these developers,
to cede more and more decision-making
and control to these AI systems.
We get a looser and looser leash.
Things move very quickly.
We become extremely dependent on them.
This gets us in an irreversible position
where we're not actually making the decisions,
but we're basically having nominal control.
It's a very possible in that situation,
we just ultimately end up losing control
to the sort of very complicated fast-moving system
that we create.
And then the final type would be these risks
that emanate from the AI systems themselves.
These are more internal or inherent risks from AI systems.
And that would take the form of rogue AIs
where they have goals separate from our own
and they work against us to complete
or satisfy their desires or preferences.
So overall, there are four.
There's malicious use.
There's these organizational risks.
There's these structural slash environmental risks
and there's these inherent or internal risks
in the form of malicious use, organizational risk,
racing dynamics and rogue AIs.
Yeah, so if we look back maybe 10 years or so,
I think most of the discussion about AI risk
would have been about rogue AI.
So the risks that are coming internally from the AI,
so to speak, the AI developing technically in ways
that we're not interested in.
So how much is this categorization set in stone?
Do you think it'll change over time as we learn more
or have the field of AI safety matured
such that we can see the risk landscape now?
I think the focus on rogue AI systems
is largely due to early movers
having substantial cultural influence.
I think if we asked other people who were not as invested
in AI risks, if they were to write down concerns about these,
we would of course think that people using the technology
for extremely destructive purposes
was posed catastrophic risks.
And I think the communities ended up having
some self-selection effects such that people didn't end up
talking about things like malicious use
and treated that as a distraction, I think were.
So I think the community didn't make much of a space
for people who were concerned about things
other than rogue AI systems.
But that was a mistake.
The AI is being used in malicious ways,
can definitely cause catastrophes
and can end up increasing the probability
of existential risks as well,
which maybe we'll speak about the connections
between ongoing harms, anticipated risks
and catastrophic risks and existential risks.
I think the community of people who were thinking
about AI risks a long time ago would largely think
about whether there's a direct, simple, causal pathway
to something like an extinction event.
Now, I think we have more of a sophisticated causal
understanding of the interplay between these various factors,
such that one doesn't try and look for direct mechanisms,
but instead tries to look at what sort of events
increase the probability of existential risk
rather than does it directly cause extinction.
And that distinction between something
that increases probability versus directly caused
means that we have to look at a much broader variety
of factors and we can't end up just thinking
that all we need to do is make a single,
very powerful AI agent do what we want
and then everything is solved forever.
Unfortunately, we're going to have to treat this
as a broader socio-technical problem.
We're going to have to consider the various stakeholders,
the politics, indeed the geopolitics,
the relations between different countries,
liability laws and all these other things
because we're not in this sort of fume type of scenario.
It would seem, it seems we're more in a slow takeoff.
So many of these real world considerations
that were sort of a sidelined infuses distractions
is actually where most of the action is.
What would be examples of something
that might put society in a worse position
where we are less able to handle a powerful AI?
A prime example would be World War III.
If there's World War III conditioned on that,
that increases the probability of existential risk
from AI systems.
This would spur a substantial AI arms race.
We would quickly outsource lethality to them.
We would not have nearly as much time
for making them more aligned and reliable in that process,
but still the competitive pressures
would compel different states
to create powerful AI weapons
and eventually have that take up more and more
of their military force.
But that doesn't directly cause extinction.
So if we try and back chain from that,
the story gets much more complicated
and so then it's not viewed in the scenarios
that others were thinking of.
There's an AI lab, they've suddenly got a God-like AI
and then it has decisive strategic control
over the entire world.
How will they make sure that it does what they want?
That was I think the scenario
that others were thinking largely
and all other ones were too broad, too intractable.
Essentially there was a focus
on quote unquote targeted interventions historically
where we're just a small number of people.
We can't do these broad interventions that involve
interfacing with various institutions
and getting public support.
Those are intractable.
So the best we can do is do some very narrow,
specific things, maybe technical research.
This doesn't look like a strategy
because broad interventions are actually more tractable.
The world is interested in this
and we have some amount of time
to try and help our institutions
make good decisions and policy around these issues.
Do you think this broader vision of AI safety
should make us more positive or less positive?
Imagine we have to set all of the institution up perfectly.
It seems like we have a narrow corridor
to make things right where the institutions have to be there,
the technical side have to work,
the all stakeholders have to be set up well
for this to succeed for us.
Should the complexity of the problem
make us more pessimistic?
I think it's, there's at least more tractability
compared to an AI suddenly goes from, you know,
incompetent to omnicompetent
and in control of the world overnight
and we have no idea it was emergent
and we didn't actually control that process.
That doesn't have almost any tractability to it.
I don't think we need our institutions
to be completely perfect.
We just need to try and be in the business of reducing risk.
So maybe that's one other conceptual distinction
is that historically there'd be a focus
on is it an airtight solution that works in the worst case
where if everything, if something goes wrong
then it's insufficient because we have to quote unquote
get it right on the first try.
When we do have some amount of time,
not saying we have a huge amount of time,
we have some amount of time,
we can do some course adjustment
and incorporate information as we go along.
I'm not saying that's a surefire strategy,
but I think that's the best we have.
And it allows us to correct some mistakes,
but obviously we can't have much of an error tolerance,
unfortunately.
Do you think we are missing something
with this categorization that you've set up in the paper?
Could we be missing some category of risk
that will be obvious to us in 20 years?
And could that risk potentially be the most dangerous
because we're not anticipating it?
Are there unknown unknowns here?
Well, usually there are unknown unknowns.
I focused largely on catastrophic risk
and large scale loss of human life.
I didn't speak about AI well-being very much,
or for instance, that's something that could end up
changing a lot of how we think about wanting to proceed forward
with managing the emergence of digital life
is if they have moral value.
So I think that's something I didn't touch on in the paper,
largely because I think our understanding of it
is very underdeveloped,
and I still think it's a bit too much of a...
It's a bit too much of a taboo topic
such that there just hasn't that much research on it.
Well, let's dig into the first category of risk,
which is malicious use.
So this is a category in which people,
bad actors, choose to use AI in ways that harm humanity.
Recently, there's been a lot of discussion of AIs helping
with bioengineered pandemics.
This has been brought up in the US Senate, I think,
and it's been kind of widely publicized.
How plausible do you think it is that the current
or the next generation of large language models
could make it easier to create bioengineered viruses?
Yeah, so I think this is actually one of the largest reasons
I wrote this paper was because during 2022,
when sort of the development of this paper started,
this is a bio thing, nobody's talking about it.
This, although people will treat malicious use
as a distraction, I don't think that's the case.
There are a catastrophic and existential risk
that can come from malicious use,
so and this threat vector concerns me quite a bit.
So I think it is quite plausible
that if we have an AI system that has something
like a PhD level understanding of virology,
then it's fairly straightforward
that such a system would provide the knowledge
for synthesizing such a weapon.
The risk analysis is something like,
what's the number of people with the skill and access
to create a biological weapon
that could be civilization destroying?
And what's the sort of the probability
that they're actually wanna do that?
And right now, maybe there are 30,000 virology PhDs
and they just don't really have the incentive to do that.
Meanwhile, if you have that knowledge
in available to anybody who wants to go to,
use Google's chatbot or Meta's chatbot
or Bing's or OpenAI's,
then we can add several zeros to that,
to the number of people with the skill to pull it off
because they could just ask such a system,
how do I make one?
Just give me a cookbook.
Now, there'd be guardrails of course,
but the guardrails are fairly easy to overcome
because these AI systems can easily be jailbroken.
That's like if you can just append
some random garbled string
or some adversarily crafted garbled string
at the end of your request to the chatbot
and then that'll take off.
It's like safety guardrails is a paper
that the center helped with
in creating and discussing adversarial attacks
for large language models.
So now that's a thing.
Or you might use an open source model that's available
and might also be easily stripped of its guardrails.
I don't think the AI developers now with the APIs
have much of a high ground as far as safety goes
when it comes to the malicious use case
for other things like hacking.
There'd be a different story, but that could change.
Maybe they'll add more measures.
Maybe they'll get better filters.
Maybe they will remove some bio related knowledge
from the pre-training distribution and so on.
But anyway, that would be sufficient for,
if such a thing were to happen,
then there could be a pandemic
that could cause some civilizational discontinuity,
which could be some existential risk.
It'd be difficult for it to kill everybody,
but for toppling civilization.
And it's not clear how that would go
or the quality of that situation.
That's enough for us to worry, I think.
Some of the pushback to this story
of a bio engineered virus enabled
by large language models is that,
well, isn't all of the training data freely available online?
Couldn't a potential bad actor have gone online,
gotten the data and used it already?
What's the difference between using a search engine
and a large language model?
Sure, so two things.
Even if there is some type of harmful content online,
I don't know why we would want it being propagated.
If the nuclear secrets were online,
I don't know why you'd want that propagated
because your risk increases
based on the ease of access to these.
But in the case of bio weapons,
yes, there are some bio weapons
that are not civilization destroying available online.
The ones that would be potentially civilization destroying,
though, would require a bit more thinking.
So there could be several,
or there could be many people killed
as a consequence of these though,
but not at a societal scale risk necessarily.
So I think that's a relevant difference.
Many of the extremely dangerous pathogens,
fortunately, virology people are not writing those up
and posting those on Twitter
and then all you gotta do is search for them.
This isn't, that's not actually the type of information.
For other types of information,
like how to tips for breaking the law
or how to hire a car,
this sort of stuff is online and generic,
a cookbooks for some generic,
smaller scale bio weapons, sure,
but not civilization destroying.
And how is the guide for creating a civilization
destroying virus in the large language model
if it's not online, in the data online?
So I am not saying that the current ones
have this in their capacity.
I'm saying that when they have like a PhD level knowledge
and are able to reflect and do a bit of brainstorming,
then you're in substantially more trouble.
And that could possibly be a model
on the order of like GPT-5 or 5.5,
it may be within its capacity.
So there you don't need agent like AI,
you would just need a very knowledgeable chat bot
for that threat to potentially manifest.
So there's quite a bit we'll need to do
to in technical research and in policy
for reducing that specific risk.
Another rescue you mentioned under malicious use
is this issue of AI agents,
which they are perhaps a bit analogous to viruses
in the sense that they might be able to spread online
and replicate themselves and cause harm.
What do you worry about most with AI agents?
I'm emphasizing, and since there are many forms
of malicious use in this paper,
I'm mainly emphasizing ones that could be catastrophic
or existential.
So in this case, you could imagine people
unleashing rogue AI systems to just destroy humanity.
That could be their objective.
And that would be extremely dangerous.
So you don't need power seeking arguments
or these claims that oh, by default,
they will have a will to power.
You don't need any of that.
You just need to assume that if enough people have access
and if some person is omnicidal
or thinks in the way that some AI scientists do,
that we need to bring about the next stage
of cosmic evolution and that resistances is futile
to quote Richard Sutton,
the author of the Reinforcement Learning textbook,
and that we should bow out when it behooves us.
This type of, there are many people
who would have an inclination for building,
not saying Richard Sutton would specifically
give the AI system of destroy humanity,
but doesn't seem to say too much against that prospect.
So that's another example of malicious use
that could be catastrophic or existential.
And how close do you think we are
to AI agents that actually work?
We had someone set up a chaos GPT early on
when GPT was released, but it got stuck in some loops
and it couldn't actually do anything,
even if it was imbued with bad motives.
When would you expect agents to actually be capable
and therefore dangerous?
Yeah, so I think that their capability
would be a continuous thing in the same way,
like when are they good at generating texts?
It's like, well, you know, it kind of started in GB2, GB3,
and so I might anticipate great strides
in AI agents next year,
where we can give it some basic short tasks.
Like, help me make this like PowerPoint or something.
It's not gonna do the whole thing,
but it can help with things like that
or browsing around on the internet for you more.
So I think those capabilities will keep coming
for it to pose a substantial risk.
There's a variety of things that could do.
It could threaten, for instance,
mutually assured destruction with humanity
by saying, I will make this bio weapon.
That will destroy all of you
and I'll take you down with me
unless you comply with some types of demands.
That could work.
If they're good at hacking,
then they could potentially amass a lot of resources
by scamming people or by stealing cryptocurrency.
They could, of course, tap into lots of different sensors
to manipulate people or influence public discourse.
They wouldn't necessarily need to be embodied
for this type of thing to happen.
If we're in a later stage of AI development
where we have a lot of weaponized AI systems
and then hacking those systems
will, of course, be substantially more concerning
or if those systems get repurposed
maliciously to weaponized AI systems.
So it becomes a lot easier as time progresses.
The AIs don't need to be particularly power seeking
on this view, though, to have this potential
for catastrophe because humanity
will basically give them that power by default.
They will keep weaponizing them.
They will integrate them into more and more critical decisions.
They will let them move around money
and complete transactions
and they'll give them a looser and looser leash.
So as time goes on,
the potential for AI systems
that are deliberately instructed to cause harm
would be the potential impact
or severity would keep increasing.
Yeah, I think maybe it's worth mentioning here
just the continuous costs of traditional computer viruses
which are costly and which we've gotten better
at handling those as a civilization,
but we still haven't defeated traditional conventional viruses
which are very dumb compared to what AI agents could be.
So we can imagine a computer virus equipped
with more intelligence and how would you as a person,
I'm not saying AI agents will be necessarily
as smart as people soon,
but how would you do the kind of hacking
that the agent might be interested in?
It's interesting to consider at least
that we haven't been able to squash out conventional viruses.
Yeah, they could accultrate their information
onto different servers or less protected ones
and then use those to proliferate themselves even further.
So they'll be a very distinct adversary
with many, many options at their disposal for causing harm.
Yeah, one thing I worry about is whether the tools
and techniques we'll need at an institutional level
to handle malicious use will also enable governments
to become totalitarian basically,
to exercise too great a level of control
over citizens who have done nothing wrong.
So what is required to prevent the large language models
that could become AI agents and could be used
to create viruses?
What techniques are available for preventing them
being used in such ways without enabling
kind of too much state power?
Yeah, I think this is definitely a tension
where to counteract these risks from rogue,
lone wolf actors,
then people would want the technology centralized.
I mean, this would be a similarity
with nuclear weapons, for instance,
where we didn't want everybody being able
to make nuclear weapons.
We wanted to keep control of uranium.
And so what happened was we had a no first use
plus non-proliferation regime
and that kept the power in a few different peoples' hands.
I think there are things we could do
to reduce these sorts of risks
by creating institutions that are more democratic.
I think that seems useful.
I think decoupling the organizations
that has some of the most powerful AI systems,
having those more decoupled from the militaries
would be fairly useful so that if something gets
out of hand with line, if they're linked
and if we're needing to pull the plug on these AI systems,
this isn't like taking down the military.
I think just separating this sort of cognitive labor
and or labor generally, automated labor
from a physical force would be fairly useful.
But I think largely it's creating democratic,
a democratic institutions is one of these measures.
In the case of dealing with rogue AIs
that are people maliciously instructed rogue AIs
that are proliferating across the internet,
I think there'd be other types of things
like legal liability laws for,
liability laws for cloud providers
that if you are running an unverified
or unsafe AI system on your cloud or on your compute,
then you get in trouble.
This would create incentives for them to keep track of it
instead of just doling out compute to whoever's paying.
So that's sort of like having an incentive
for off switches all over.
So there's a variety of different things
we could be doing to strike this balance
by reducing these malicious use risks.
I mean, also, as you mentioned,
some of these malicious use risks
don't require this type of centralization
or nearly as much.
We can do various things to reduce this risk
without giving tons of power to states.
For instance, if we invest in personal protective equipment
or monitoring waterways for early signs of some pathogens,
I mean, there's the traditional stuff we can do
to reduce risks from pandemics, for instance,
which would reduce our exposure to the risk
of AI-facilitated pandemics.
So not all interventions for reducing malicious use
require more centralization.
I would imagine that we probably wouldn't want
in the long term, like say it's like 2040
or something like that,
we wouldn't want anybody anywhere
being able just to ask the AI system how to make a pandemic
or being able to unleash it to try and take over the world.
This doesn't seem like a good idea.
There'd be other types of things like structured access
where for these bio capabilities,
you just give people who are doing medical research
access to those specific bio capabilities.
But other people, they don't really have much of a reason
for it, so they don't get that advanced,
they don't get models with that advanced knowledge.
So I think there are some simple restrictions
that we can do that can take care of a large chunk
of the risk without needing to hand over the technology
to like militaries, and then they're the only ones who have it.
You mentioned legal liabilities for cloud providers
and maybe companies in general.
I wonder if this might be a way to have
a form of decentralized control
over AI agents or over large language models
or generative models, AI in general,
by having the state provide a framework
for where you can get fined for trespassing some boundaries,
but then having companies implement exactly how that works,
use technical tools in order to reduce their risk of fines.
And maybe we can find a good balance there
where we weigh the costs and benefits.
I think that liability laws help fix
the problem of externalities quite a bit,
where they're imposing risks on others
that have no, shouldn't have any risk imposed on them
because they're not privy to the decisions or they're there.
There's an issue with that though,
which is that there's only so many externalities
that some of these organizations could internalize though
with liability law.
If somebody creates a pandemic
as a consequence of their AI system,
you could sue that company,
but they're not gonna be able to pay off
the destruction of civilization with their capital.
So there's quite a limit to it.
It can help fix the incentives,
but it still doesn't fix them entirely
because it's not particularly,
when certainly can't internalize downfall of civilization
as an organization and foot the bill for that.
And then the extinction of the human race is also,
I don't think that's the thing you could settle in court.
What about requiring insurance?
So this is an idea that has been discussed
for advanced biological research,
gain a function research with viruses, for example.
Maybe such a thing could also work
for risky experiments with advanced AI.
It depends if the harms are localized.
I think insurance and this taming of typical,
not long tail, not black swan type of uncertainty,
but thin tailed type of uncertainty
makes sense when risks are more localized,
but when we are dealing with risks that are scalable
and can bring down the entire system,
then I think a lot of the incentives for insurance
don't make as much sense.
So you basically need some law of large numbers
and many types of insurance to kick in
to sort of have that risk diversified away.
But if the entire system has exposure to that risk,
it's, there's not another system to diversify it.
Maybe you could paint us a picture
of a positive vision here.
So say we get to 2050 and we've worked this out,
what does the world look like in a world
where we control malicious AI?
I think if people have access to these AI systems,
they're subject to, and they have many of their capabilities.
There are of course restrictions on them.
Like you can't use them to break the law.
So a lot of these most dangerous capabilities,
nobody's really able to use them in that way.
If there is a need for, in the case of like defense,
they would end up using like AIs for things
like hacking and whatnot.
And that would, like they would have access
to that type of technology,
but it wouldn't be the case that any angsty teenager
can just download a model online
and then they instruct it to take down
some critical infrastructure.
This just isn't a possibility.
It's very much trying to strike a balance with that.
I would hope that we would also have
these most powerful AI systems
that do carry more of this force,
that have some of these more dangerous capabilities
are subject to democratic control
so that power is not as centralized.
And that also I think reduces like the risk of like,
put in quote, like lock in risks as well,
where some individual group can impose their values
and entrench them.
So at least those are some properties
of a positive future.
So I don't think it looks like
complete mass proliferation of extremely dangerous AI products.
And I don't think it looks like only one group,
one elite aristocrat group
gets to make the decisions for humanity either.
So there's different levels of access
to different levels of lethality depending on,
and then power depending on whether it makes sense.
But the highest level institutions are still democratic.
Another category of risks that you discuss
is the possibility of an AI race.
Now, we've done another episode
where we talked about evolutionary pressures
and how they work between corporations
and how they might lead to a situation
in which humanity is gradually disempowered.
But I think one thing we could discuss here in this episode
is the possibility of a military AI race.
What do you think a military AI race looks like?
To recap, we were just at the malicious use one.
And so now the other risk category would be like racing dynamics
or competitive pressures or collective action problems.
This is that structural environmental risk
though when we're referring to the categories way earlier.
Yeah, I think with the corporate race,
obviously there's, as we discussed in the previous episode,
there's them cutting corners on safety
and this is large who AI development is driven by.
A lot of these organizations will start
as having a very strong safety bent,
but then they're basically gonna be pressured
into just racing and prioritizing the profit
and developing these things as quickly as possible
and staying competitive over their safety.
This is sort of the dynamic that basically drives
pretty much all these AI companies.
And I don't think actually in the presence
of these intense competitive pressures
that intentions particularly matter.
So I think basically this is the main force to look at
when trying to explain a major developments of AI,
why are companies acting the way they are?
It can be very well approximated by them
just trying to, by them succumbing to competitive pressures
or defecting in this broader collective action problem
of should we slow down
and should we proceed more prudently
and invest more in safety
and try and make sure our institutions are caught up
or should we race ahead so that way we can continue
being in the lead because one day
we'll maybe be more responsible with this technology.
I'm concerned as mentioned in that previous episode
of that leading us to like a state of substantial dependence
some and losing effective control,
you can imagine similar dynamic happening
with the military just like if we don't want arrows
for instance, you're not gonna roll back arrows.
And so when you start going down the road
of weaponizing AI systems, if they're more potent
and cheaper and more generally capable
and more politically convenient
and sending human soldiers onto the battlefield,
then this becomes a very difficult process to reverse back.
Eventually what happens is you've had an on ramp
to many more potential catastrophic risks.
You've transferred much of the lethal power
in fact, the main source of the lethal power to AI systems
and then you're hoping that they're reliable enough
and that you've sufficiently,
you can keep them under sufficient control
and that they can do your bidding.
Even if you do get them highly reliable
and they do what you instruct them to do.
This doesn't make people overall very safe.
We saw with the Cuban Missile Crisis,
we can definitely, nukes don't turn on us.
They don't go off and pursue their own goals
or something like that.
They do what we want them to do,
but collectively do this structural environmental
game theoretic situation where like,
well, we would all be better off without nuclear weapons
but it makes sense for us each individually to stockpile them.
We put the broader world at larger collective risk.
In the Cuban Missile Crisis,
JFK said we had up to like a half
or like a 50% chance of extinction in that event.
It was a very close call
because we almost got a nuclear exchange with that.
And likewise with AI systems,
they may be more powerful.
They may be better at facilitating the development
of new weapons too.
And this could also bring us at a risk
where bring us in a situation
where we could potentially destroy ourselves again.
What's pernicious about this structural
or environmental constraint
where we've got different parties,
in this case militaries,
competing against each other is the following.
Even if we convince the world
that like the existential risk from AI is like 5%
because let's say they're not reliable.
We can't reliably control that.
So maybe there's a 5% chance to like turn on us
or we lose control of them.
And then we become a second class species or exterminate.
Even if that's the case,
it may make sense for these militaries to go along with it.
Just like, I mean, they swallowed the risk
of potential nuclear Armageddon
by creating these nuclear weapons in the first place.
But they thought if we don't create these nuclear weapons,
then we will certainly be destroyed.
So there's certainty of destruction
versus a small chance of destruction.
And I think they'd be willing to make that trade off.
So this is how there could be an existential risk
to all of humanity based on these structural conditions.
Even so, it's not enough to convince the world
that existential risk is high
because they might just, okay, well, yeah, that's 5%.
Okay, we're gonna have to go with that rational left thing.
It makes rational sense for us to engage in this,
what would normally be very risky behavior
because we don't have a better choice.
So this is why I don't think it makes sense
just to hammer home the point that, wow,
these AIs could turn on us
or we could lose control of them.
There's this structural thing of like,
that's not gonna matter unless that probability
is like very high.
Like maybe if it's like 30% and they go, okay, all right,
we're not gonna build the thing because,
but if it's something like 5%,
they might go through with it anyway.
So more than just concerns about single AI agents
make sense to focus on,
we have to focus on these multi-agent dynamics,
these competitive pressures,
the sort of the game theory of what they're facing.
And so I think that if you don't resolve that,
you're basically exposed to insensitivity
to a lot of existential risk up to maybe 5% or 10%,
which maybe it isn't, it's possible.
Maybe it's actually only 2%
and when you convince the world,
everybody's very educated about it.
Everybody listens to Future of Life podcast tomorrow
and they all go, wow, this is a concern.
I am updated to 5%, won't matter.
It won't stop that type of dynamic from happening.
So you have to fix the international coordination issue,
have to avoid this sort of potential for World War III thing.
Now it didn't directly cause it,
as we were discussing earlier,
this wasn't a direct cause of extinction,
but it increased the probability substantially.
That's the sort of framing we have to focus on
in trying to reduce existential risk,
not search for direct cause of mechanisms,
but look at these diffuse effects and structural conditions.
Yeah, so concretely, this might look like
the US is considering implementing AI systems
into their nuclear command and control systems.
So specifically, they're doing this
to counteract the rumors of other countries
doing the same thing.
And in order to act quickly enough
with their nuclear weapons,
they think they need to give AI a greater degree
of control over these nuclear weapons.
And so you have a situation
in which countries are responding to the actions
of each other in a way that accelerates risks
from both sides in this innocent.
There'd be one, I mean, there are other ways
this can affect warfare.
It could maybe be better at doing anomaly detection
thereby identify nuclear submarines
and affect the nuclear triad of that way.
Or in later stages, just have massive fleets of AI,
and this is saying robot, sorry to say,
but like later stage, if they're much cheaper to produce,
they'd be very good combatants.
There isn't skin in the game.
This increases the, this makes it more feasible
to get into conflict.
There are other ways in which this increases
the probability of conflict too.
There's more uncertainty about where your competitors are,
what relative to you,
maybe they had an algorithmic breakthrough.
Maybe they could actually catch up really quickly
or surpass us by finding some algorithmic breakthrough.
This creates severe or extreme uncertainty
about the capabilities profile of adversaries.
This is this lack of information about that
increases the chance of conflict as well.
It may also increase first strike advantage substantially
to which would also increase the probability of conflict.
Like we have an AI system today,
it's much more powerful than anything else.
They might get theirs tomorrow.
If we act today, then we can squash them
and that could get the ball rolling
for some global catastrophe.
So yeah, pretty pernicious dynamics overall,
but all of these can be viewed as competitive pressures
driving AI systems and propagating throughout
all aspects of life.
We mentioned through the public sphere,
in the economy, people's private lives with AI chatbots,
also in defense, in the military.
It just basically becomes everywhere
and we end up relying more and more on them
to make these sorts of decisions.
And I don't think in many of these,
we become so dependent on them that things move quickly.
We can't actually keep up.
We can't make, if we're actually making these decisions,
we'll make much worse decisions.
So then they basically become ineffective control.
Things also move so quickly that the answer
to our AI problems is we need to bring in more AIs
because since they're using more AIs,
now we need to use more AIs.
And so it creates a self-reinforcing feedback loop
which ends up eroding our overall influence
and oversight as to what's going on.
And so I think that's the default one.
So of these sort of risk categories,
I think this seems like straightforwardly the case
if we don't fix international coordination
and if there's a close competition between countries
or if we don't fix the racing dynamics
in the corporate sphere,
then I think it's fairly likely
that humanity becomes at least like a second class species
loses control from there.
Eventually, probably they go extinct,
but that might be a long time after,
but so this is the main risk that I'm worried about.
But as Director of Center for AISAD,
I'll try and be acumenical and focus on various others too.
So I'm always making sure that our project's addressing
each of these though,
but personally this is the one that I'm most concerned about.
So treaties between governments
and some form of collaboration
between the top AI corporations,
is that the way out here?
How do we mitigate this risk?
It seems at the way you describe it,
it seems very difficult to avoid
given the incentives basically.
People respond to incentives,
they rationally respond to incentives.
And so for each step along the way,
they have reasons to do what they're doing.
And so it seems difficult to avoid.
What are our options?
Well, there are positive signs.
For instance, like Henry Kissinger
was recently suggested in foreign affairs
that the US cooperate with China on this issue now,
but before it's too late.
So I think some people are recognizing
the importance of trying to do something about this.
It's possible there'd be some clarifications
about antitrust law,
which would make it possible for AI companies
to not engage in excessive competition over this
and put the whole world at risk.
Potentially there could be an international institution
like a CERN for AI,
which is the default organization,
which has a broad consortium
or a coalition of countries
providing input to that and helping steer it.
One that's maybe decoupled to some extent of militaries
so that we're not having too much power
centralized in one place.
So it doesn't have a monopoly on violence
and eventually after automates a lot of monopoly on labor.
I think that's just basically all the power in the world.
So those are possibilities.
I think that the time window might be a bit shorter though.
If there's an arms race and AI arms race in the military
and if the AI is viewed as like the main thing
to be competing on,
like we need to spend a trillion dollars,
we'll spend on that order for nuclear weapons.
If when that becomes the case,
I think it's where we're very much set down that path
and then we're exposed to very substantial risks.
So yeah, I think maybe we'll have a sense
in the next few years as to whether
we get some type of coordination
or if we are not gonna recognize
that we're all in the same boat as humans
and we don't want this to happen.
But we'll need people to basically understand what happens
if we go down this route
and if we don't try and fix the payoff matrix,
the incentives at the outset,
the structure that these players find themselves in
or that these developers find themselves in.
That looks like a very much a political problem
as it happens.
So this is why making, reducing AI,
X risk and whatnot and making AI safe
is a socio-technical problem.
It's not writing down an eight-page mathematical solution,
work of genius and then, oh, okay,
we can all go home now and everything's taken care.
It's not gonna look like that.
That was a category error
in understanding how to reduce this risk.
We shouldn't have these types of founders effects
have like undue influence over,
like it will keep lingering.
I think that will eventually like go away,
but I still think it's still like lingering
and I think we should just like move past it
and recognize the complexity of the situation.
Let's talk about organizational risks
and these risk categories, of course,
kind of play into each other, influence each other.
So if we have organizations that are acting in a risky way,
that this increases the risk of potentially rogue AI
or it incentivizes others to race
in order to compete with these organizations
that are acting in risky ways.
But yeah, let's just take it from the beginning.
What falls under the organizational risks category?
Yeah, so organizational risk-setting,
slightly more abstract level would be the accidents bucket.
So even if we reduce competitive pressures
and if we have a,
and if we don't have to worry about malicious use immediately,
we'd still have the issue of organizations having,
maybe a culture of move fast and break things
or them not having a safety culture.
In other industries or for other technologies,
like rockets that wasn't extreme competition with that,
but nonetheless rockets would blow up
or nuclear power plants would melt down,
catastrophic accidents can still happen.
And these can be very deadly
in the case of AI systems eventually.
So I think this is definitely a very hard one to fix.
Most of the people at these AI organizations
and how they were initialized and whatnot,
still had a lot of people who are mostly just wanting
to build it and the consequences of society be damned.
This is not my wheelhouse.
I don't read the news.
I don't like thinking about this sort of stuff.
This is annoying humanities majors and whatnot
who are in these ethics divisions or policy divisions
that keep annoying us.
This is kind of the attitude
at most of these companies by and large.
And I think this is a large source of risk.
We could, as well as it's just non-trivial as we see
in other things like nuclear power plants,
chemical plants, rockets and making sure
that this is all extremely reliable.
So we'd need various precedents.
There's basically a literature on this
called the organizational safety literature,
which focuses on various corporate controls
and processes for making sure
that the organization responds to failure,
takes near misses seriously, has good whistleblowing,
has good internal risk management regimes,
has like a chief risk officer or an internal audit committee,
all of these sorts of things to reduce these types of risks.
And yeah, you were right in that this interacts
with not necessarily direct cause of some
of these existential risk, but it nonetheless
boosts up the probability if we're perceiving
that an organization is very reckless in its attitude.
This causes more safety-minded ones to compete harder
and justify erasing.
This reduces the, that consequently reduces
the amount of time you have to work on control
and reliability of these AI systems,
which affects the probability of rogue AIs of course.
There's also other types of accidents that could happen,
like the organization might accidentally leak the model,
leak one of its models that has some lethal capabilities
in it if it's repurposed.
There's also a risk of as potentially,
who's to say happened with viruses,
maybe there'd be some unfortunate gain of function research
that would also lead to some type of catastrophe as well.
There are people interested in what is essentially
gain of function research and in creating warning shots,
they might be a little too successful later on.
What does gain of function research look like in AI?
Deliberately building some AI system
that's like power seeking or Machiavellian
and wants to destroy humanity.
And then they're gonna use this to like scare the world with,
but like at some point when it's powerful enough,
you might get what you asked for.
The idea here is to create a dangerous AI,
maybe an AI that's more agentic or power seeking,
and then use that model to study how to contain it.
But then the worry is that we could ironically
go extinct perhaps because we can't control the model.
Yeah, and if this is like who's to say
who's going to be experimenting with this
or how exactly cautious they will be
or their like skill level,
it may be mandated that they test for
these types of dangerous inclinations or capabilities
and who exactly is going to be doing that is unclear.
It may not be like the most like capable people
or there's just some overall
or there's just some risk of accidents in that way.
So I guess that gives some flavor
of some of the direct accidents,
but I also think how it indirectly affects things.
So one way in which I think strongly indirectly
affects things is what accident is an intellectual air
inside of these organizations
where they conflate safety and capabilities.
This is a very common thing
where there's not clear thinking about safety
and capabilities where people be,
oh, well, we're smart, rational, and justify the means.
We're risk neutral.
We actually don't actually do much empirical
deep learning research, but conceptually,
we think that this will be beneficial for safety
even though it will come at the cost of capabilities
and whatnot.
So they've really muddied up that line
and the distinction between safety and capabilities
such that you could imagine a lot of these safety efforts
basically just working on capabilities the entire time.
I think that's a reasonable fraction of the safety teams
I think do focus just on capabilities.
For context, there is an extreme correlation
between AI's capabilities in various different subjects
and goals.
So if you want your AI system to be better at something
like math problems or history problems
or accounting problems,
these capabilities are all extremely correlated now.
We can see with like large language models.
You should assume that if something is correlated,
the correlation is like 80% or like 90%.
It's extremely high.
So when people reason themselves into some new capability
that they think will be helpful for safety,
it's very likely the base rate of it being correlated
with capabilities and basically being nearly identical
to other capabilities by being so correlated
is extremely high.
So I think there needs to be substantial evidence
that the safety intervention that one is applying
isn't affecting the general capabilities
and that requires empirical evidence.
So a good example of empirical research
that I think helps with safety,
but doesn't clearly help with general capabilities
of making a system smarter would be like
the area of machine unlearning.
So machine unlearning is where you're trying to unlearn
some specific dangerous capabilities,
trying to unlearn bio knowledge,
trying to unlearn specific know-how
that allows you to hack.
This is more clearly like measurably not correlated with
it's inter-correlated with some capabilities
and not particularly correlated with general capabilities
if you're just removing that specific know-how.
I ever see a robustness is also generally
inter-correlated with general capabilities.
It doesn't make the systems overall smarter.
What happens is it makes the systems robust
to some specific types of attacks.
Robustness to that comes at a fairly large computational cost
and takes up a lot of the model capacity.
But that would be a sort of,
that would be a safety intervention
that doesn't make the models overall smarter.
So those are examples of,
I suppose another example would be
with transparency research.
Historically, there have been no instances
of transparency advancements
leading to general capabilities advancements.
Just trying to understand what's going on in the model
and it doesn't really work nearly as well as just like
throwing more data at it.
And there aren't many architectural improvements
that are likely to be found.
Anyway, as a result of these investigations
is the track record is pretty,
basically completely clean for transparency.
Now, maybe that wouldn't be the case in the future,
but then at that point,
then we wouldn't identify this as something
that is particularly helping with safety.
So I think that for the safety research areas,
we need to be quite clear about,
there's, you can't just have some informal argument
about or an appeal to authority that,
oh, this is helpful for safety
because of some verbal argument.
The empirical machine learning is very complicated.
Hindsight barely works
in trying to understand what's going on.
Why does pre-training on fractal images
help improve robustness to,
I don't know, basically everything
and improve the calibration anomaly detection form?
I have no idea.
It works though.
Even ask people like,
why are activation functions the way they are?
And I don't think there's actually
a good canonical explanation
that's very consistent.
You would want empirical evidence
that when we are engaging in safety research,
we are not accidentally also increasing
the capabilities of models.
And you think this is something that happens often?
Yeah, I think this happens extremely often
in this sort of, this organizational risk
of the conflation of safety and capabilities.
Now, this isn't to say that they are loose and separate.
A better improvements in capabilities
has downstream effects on safety in many situations.
It makes them better able to understand human values,
for instance, as they gain more and more common sense.
But if we are trying to improve safety
and specifically reduce existential risk,
I think we need to differentially improve
on some safety access
and not in the general capabilities access.
If we are doing something that's fairly correlated
with capabilities and safety,
I think that the default expectation
is that actually you're working in the service
of capabilities.
A good example would be one of OpenAI strategies
to mention this specifically,
because I just don't think it's particularly
intellectually defensible.
I'm sorry to say, I'm a more disagreeable individual.
So here I go.
I don't think building a super human alignment researcher
specifically just affects alignment.
I think such a thing can be easily repurposed
to doing lots of other types of research.
I don't think there's like a specific
alignment research skill set that is just,
oh, it's just you only get at that,
but if you're good at that,
it means nothing about your ability
to accomplish anything else.
I just don't think that's the case.
I think it's actually extremely correlated
with general capabilities.
It would be very straightforwardly repurposed
to other forms of research.
But that's an example of this sort of conflation.
Now, this isn't to say OpenAI is doing,
is only is conflating safety capabilities entirely.
I'm not claiming that.
They will have some work on transparency.
I gather that they'll work more on reliability
and robustness,
but this is a very dangerous conflation.
And I think basically if they seem kind of correlated
just intuitively,
and then if you hear a lot of verbal arguments
without empirical demonstration,
you basically assume just the base rates are like,
a lot of these completely separate subjects,
like performance in history
and like performance in philosophy and mathematics,
like those are all like hyper correlated
to assume this other type of thing
is also hyper correlated with it too.
Anyway though, that's another one that like
an organizational factor that really reduces
the amount of time we have to solve this problem
and our ability to solve it as well.
So.
Yeah, I think the worry here for,
say you're a top AI company and you're thinking,
okay, how much a safe organization features
should we implement?
So should we have more red teaming?
Should we have more procedures, more review,
more testing?
Should we require this empirical evidence
before we begin a new safety research program?
This now threatens to slow us down
and it opens us up to competition
from the kind of scrappy new startup
that's on at our heels trying to outcompet us.
This is very straightforwardly now
a case of kind of AI race undermining organizational safety
or at least threatening to undermine organizational safety.
Can you make an argument if you were to sell this
to a CEO of an AI corporation that...
When would I be in that situation?
That safety is in the interest of the organization itself.
You could say it's difficult to sell unsafe products, right?
You want to be in control.
You don't want to lose the weights of your model in a leak and so on.
So there might be some correlation
between the self-interest of the organization
and the interest that society has in safety in general.
But before that, one additional factor
that just diffusely increases probability of extras
from these organizations if they just do safety washing
and they don't even know it sometimes,
might have some small gesture for safety.
They might have, for instance, a responsible scaling policy
that doesn't commit them to almost anything
and then that placates regulators, for instance,
but doesn't actually reduce risk.
Those would be other examples of how organizational risks
can end up increasing the probability of existential risk.
Although it's diffuse and indirect, it still matters.
On the self-interest point,
I think a lot of the catastrophic risks
or catastrophic risks and existential risks are tail risks.
And generally, organizations don't really price in tail risks that much.
A lot of portfolios don't really do much to address tail risks
either in other industries, like in finance and whatnot.
So this is kind of like a problem with many of our institutions
that we could convince them to do things like red teaming to some extent,
but doing red teaming for existential risks and whatnot
is not necessarily something that they would check to do
because that's not going to affect their product tomorrow.
There's no pushback if everybody is dead, as mentioned before.
So I think that this works to a limit.
I think some things like saying information security for your company
or so that your weights don't leak,
this is a much easier argument to make.
Other claims like some of these internal controls and whatnot,
though this will slow us down, this will reduce our velocity.
And I think these are harder to make.
And I don't think that there are necessarily short-term economic incentives
for some of these.
Many of these are actually more for addressing tail risks and black swan events.
So they would then need to just recognize
that the black swan events are real possibilities
beyond a probability threshold worth actually addressing.
So I'm not claiming that they're being completely irrational
if they're being fairly short-sighted
and don't believe in these black swan events from it.
Then I think them trying to maintain velocity
and just maintain optionality and whatnot, it's understandable.
I wouldn't advocate for that,
but it's understandable that they're doing that.
I think they're comporting to a lot of their incentives well,
but and they will do various things to reduce some generic risk.
They will do some generic forms of red teaming,
regardless of whether there's regulation, because it will make sense.
But I just don't think that that does particularly much
in the way of reducing these catastrophic or existential risks off.
Say you're a philanthropist or a government with a big bag of money
and you want to incentivize safety research at these top AI corporations.
Is there a way in which you could earmark the money
and make sure it's spent on what you want it to be spent on?
So it's not funneled into increasing capabilities of the models.
It's spent on the right type of safety research.
I think that'd be one intervention.
I think it's very possible to...
There are a lot of professors lying around in academe
who could do this research.
All you need is to subsidize, for instance,
like the Center for Air Safety as a compute cluster.
We'd love to expand it.
We're only able to support not that many professors
to do research with large language models
and very compute intensive experiments.
But there are a lot of professors who could be doing more research here.
So I think that probably...
There aren't that many people working at these organizations, I should say, as well.
And so I wouldn't bet on them to fix everything.
You're actually just correlating it with, like,
what is Jared Kaplan's safety vision?
What is Yanleica's safety vision?
And you're getting two or three bets
if you were giving each of them money.
And I think that's not a very diversified portfolio,
and you should expect blind spots just because people don't have...
Can't simulate a collective intelligence, a broad research effort by themselves,
even if they work very hard and have lots of discussions
and take out...
Have good deference to outside views and so on.
They just can't simulate that function.
So I would suggest if one's wanting to subsidize safety research,
we can...
If those can subsidize a compute cluster,
then we can have high accountability of, like,
you're not allowed to run this project
because this doesn't seem sufficiently safety-related
instead of giving, like, money, no strings attached to some academics
and they'd run off with it.
So that would be my preferred intervention.
And I think that there's...
It could take orders of magnitude more.
So, like, if any of them are listening, you know, like, reach out to us.
I'd love to get more compute to people doing relevant research on safety
in a nice, diversified portfolio.
Across transparency and adversarial robustness
and backdoors and machine learning models and unlearning,
these types of topics.
Do you think some safety breakthroughs would be kept secret?
Say a safety breakthrough at Google DeepMind made the AI useful
or in a way that incentivized them not to share the safety breakthrough.
Or would you expect safety breakthroughs to be shared widely
as if they were found in an academic lab?
I think for market positioning,
one of them could occupy the niche of being the safest.
Of the racing companies, we are technically the safest.
So I think that's currently occupied by Anthropic
and this might make it fairly useful
when pitching themselves for, say, a defense contract
that, look, we're the more reliable organization
compared to our competitors.
And so if they would be open sourcing some of that,
then I think they would lose some of that competitive advantage.
So it's quite conceivable that they'd hold on to things.
I mean, there's many safety projects they do
for which the code is not open source.
So we see that to some extent there.
But I think it can make sense for one of them to try
and just be a bit safer than the others
or a bit more reliable than the others.
Yeah, I've heard Sam Altman, the CEO of OpenAI,
talk about releasing these systems,
so specifically releasing GPT-3 and 4 to the world,
chat GPT, in order to gather more attention to the issue.
Do you think this is a viable strategy?
Is this too risky or is it worth trying?
I think to answer a more extreme question
to possibly get a sense of my position on this,
I think the release of Llama 2, for instance,
by Meta, which is an open source, large language model
around the capacity of GPT-3.5,
I think the benefits of that actually outweigh the costs.
It enables a lot more research.
It also improves our defenses against some
of the immediate applications of these AI systems.
So for instance, it came out today
that North Korea is using some of these AI systems.
I don't know whether it's Llama 2, that'd be my guess,
because it's just among the most capable open source system,
or Code Llama, potentially,
using AI systems to identify vulnerabilities in software,
and then that helps them shortlist things to attack.
This isn't an extremely capable,
or this doesn't rewrite the cost-benefit analysis
of cyber attacks.
It doesn't rupture our digital ecosystem,
but this basically gives us some preview and forces
these issues on people's attention.
So I think there's an argument to be made
for open sourcing Llama 2,
or if it's trained on 10x more compute Llama 3.
After that, there's more uncertainty we'd have to see,
because maybe it could be repurposed for things
like bio weapons then,
or it would be substantially more capable
at hacking, scamming, things like that.
I think there's a real argument to be made
for some short-term stressors
sort of snapping the system to do something about it,
or at least waking them up.
But I think systems function better
with some amount of stressors.
When the stressors get too extreme,
then it can undermine the system.
So it's complicated.
I think maybe the situation, the case of opening,
and I really seeing these things,
or how they want to go about release strategies
would not surprise me if that should change,
or if it would be better to do other things in the future.
Do you know something about the internal processes
for deciding when to release these models?
So in the case of Meta, for instance,
they may have a chief legal officer vote
on whether or potentially a veto power
that could still be overwritten by the CEO,
which may have happened in the case of Llama 2.
You can suggest it here,
because I haven't heard a second source for it,
but usually for decisions, though,
like opening I will be accountable to their board.
The board, I don't know whether they have formal powers
to decide whether they'd be voting.
Often boards have blunt powers of just firing the CEO,
and there often aren't processes in place
for these larger scale decisions.
So you could imagine a CEO just deciding unilaterally
to have something released.
And that's something that organizational safety
could improve and be processes
for high stakes decisions as an example, but yeah.
But by default, boards do not have fine-grain control,
and so it's often up to the CEO to make the call.
So you have a single point of failure.
What is the Swiss cheese model of organizational safety?
Yeah, so I'm mentioning,
and if people are wanting to hear more
about the organizational safety literature,
we'll have the AI safety ethics in society textbook
out in November in one of the chapters
would be on safety engineering.
The Swiss cheese model is easy to communicate,
it's kind of outdated, but it gets at a,
just like how people are doing analysis
of existential risk from AI earlier,
they'd have a toy model that captured some of the,
and some of the scenarios to be concerned about,
but it's important not to let that be the lens
by which you filter everything through,
that captures some of it, but not all of it.
And the Swiss cheese one captures some of the dynamics,
but not all of the dynamics, but anyway,
the Swiss cheese model with that probably would decide
essentially having multiple layers of defense.
If you have red teaming,
even red teaming for a catastrophic risk,
that reduces the risk of catastrophe,
but it's not itself perfect.
You might also want stronger informational security too,
to make sure that if you had a dangerous model
that it doesn't leak, you could have,
you could have better transparency tools
to check for deceptive behavior in AI systems.
But if those transparency tools failed,
maybe you would want monitoring of these AI systems
so that before they take any action,
it needs to be approved by something equivalent
to like an artificial conscience or filter
that would filter out some of the immoral actions of AI agents
before they're able to take them.
And so all of these together
can increase the reliability of the system,
so there isn't a,
the hope is that if you stack together many of these,
you've substantially reduced your risk.
This isn't looking for a perfect airtight solution,
this is looking for layering on many different defenses
to actually reduce risk.
So if I want to reduce by risk, for instance,
here's an example of a Swiss cheese thing.
First, I could, first, there'd be the diffuse thing,
and maybe there could be some regulation
about not allowing models with these capabilities,
but let's say I'm an organization
that takes safety more seriously.
So that depends on safety culture.
So that's some sort of like barrier.
Regulation might be some barrier against this risk,
safety culture might be some barrier against this risk.
So then they have enough of a safety culture
they're willing to add a lot of these safety features.
Now these safety features themselves will end up posing
or end up having lots of different layers of defense.
You could have an input filter to try and remove
whether there's a request to create a bio weapon.
You could also remove virology related data
from the pre-training distribution
so that it likely knows a lot less about virology.
You could have an output filter as well,
which would, even if somebody jail breaks the input filter,
then they're also gonna need a jail break
the output filter, which makes it, which is harder to do.
And you could imagine adversarily training this as well.
So it'd be another layer so that it would be more robust
to people trying to jail break those layers of defense.
But then you also have, there's also people
who could through the API fine tune the model
and inject some of that bio knowledge back into the model.
So you could have a filter that screens the fine tuning data
so that that information can't get back into the weights.
And then you could add another layer,
which would be an unlearning layer
where you would assume that before you hand back
the fine tuned model to the user, before they get it back,
we're gonna run a scrubbing unlearning knowledge expunging
thing to expunge some of any bio knowledge if there is any.
And that would be yet another layer.
This approach reduces the risk of some bio catastrophe.
Are any of those airtight?
No, but do they work better collectively?
Absolutely.
So this is why we shouldn't be focusing
on these like airtight solutions.
Exclusively, we also need to make use
of these various layers of defense.
That's how we actually reduce the probability
of existential risk.
We can't let perfection be the enemy of the good.
If we'd say, well, if we can't build a completely
100% reliable input filter,
then we shouldn't have an input filter
or we, that's a dead end, so we shouldn't investigate it.
That's just not how things work.
Tell us more about the textbook.
I'm pretty excited to read this.
I hope that this is a product that should exist, I think.
Specifically, tell us more about how do you think
about updating this or keeping it up to date?
I think for a textbook on AI safety,
it won't probably work if the next version
is out in 2034 or something like that, right?
So how do you keep it up to date?
And also you can just present the textbook,
which I think listeners will be interested in.
I mean, since it's been around in academe for a while,
I do have at least some of a sense of what things,
what content is more likely to stand the test of time.
So that one's not talking about Dolly 2 or something,
which you know, those are already outdated,
or what are kind of like bad topics
and not giving those too much, not giving those airtime.
I mean, an example of this would be like an unsolved problems
and I'll say, I don't know, two, three years ago
or something, but there we introduced emergent capabilities,
which I think has become fairly popular
before Burns et al's paper on honesty and whatnot.
We're also honesty is a big part of alignment.
So there's sometimes one needs to call the shots too,
as to what things will, even if there aren't,
isn't much of a literature on it at all,
need to predict what will end up standing the test of time.
But so I think it should have some reasonable longevity
because we're not focusing on transient knowledge,
but instead like general interdisciplinary frameworks
for thinking about risk across all these sectors,
because we had this issue of like, there's inner,
if you're thinking about AI risk,
you have to think a bit about geopolitics,
you have to think about international relations
to some extent, you think about AI risk,
you have to think about corporate governance
and AI developers and what sort of incentives
are driving them.
You have to think about the individual AI systems
themselves too, you have to think about organizational safety,
you have to think about broad variety of factors
and we'll basically focus quite a bit on frameworks
for thinking clearly about each of those.
I would imagine that later one could have GPT-6 help
like update the textbook anyway.
So honestly, it's actually like the plan first.
Something in that direction.
How technical is the book?
Does it contain pseudocode like a standard AI textbook?
The premise of it is to onboard people
from different disciplines.
This isn't written for machine learning PhD people.
There are lots of different fields,
economists, legal scholars, philosophers,
people without technical background, policy makers,
think tank people who want more of a systematic understanding
of these issues.
And so it's largely written for people
without any specific background
and it's not trying to be sort of like
a introductory machine learning PhD course.
That would be the course.mlsafety.org
if you want a course of various technical topics
or the machine learning safety course.
But this one is more focusing on,
as we were discussing, the game theory of this,
the various governance solutions.
Conceptually, many of the arguments associated
with rogue AIs, why might they be power systems?
Why might they be deceptive?
Understanding that.
There's also introduction to machine learning
and reinforcement learning in it.
Understanding collective action problems
since that was fairly relevant
and these competitive pressures.
There's also ethics in the book as well
where if you're assuming that you've got your AIs systems
to be somewhat reliable,
then we have to start worrying about making it beneficial.
And so there's various bits of AI ethics
as well of what are objectives
that we might give the AI system.
What would those look like?
What would be some of the, you know,
moral trade-offs that you're making there?
But so it's covering AI safety, ethics, and society.
So trying to be fairly broad.
You should have lecture slides.
And presumably I'll get around to recording videos for two.
The goals, there's several goals of it,
like to compress the content.
Right now, if you want to understand AI risk,
basically need to be part of an intellectual scene,
like in the Bay Area probably.
Maybe and maybe somewhat an Oxford.
So very high barriers to entry.
And then if you do,
you're probably going to take a somewhat narrow view
just because they're all interested in rogue AIs
and don't have as much interaction
with the rest of the world.
So you'll have many blind spots as to a lot of it.
There's the social variables
and the broader socio-technical problem.
The knowledge has been a bit diffused
across various different blocks.
And to stay up to date,
you've often had to jump around from different places.
So it would be nice to have something
that's more compressed.
So some of the goals are to like reduce the fragmentation
of AI risk knowledge, increase the readability
and the sort of compression rate of this content.
So there's reducing the barrier to entry
to these crucial ideas
that should hopefully scale the number of people
who understand AI risk extremely quickly.
I was somewhat surprised by,
although there's a lot of global attention,
the number of like new experts flooding in
has been, I think, very underwhelming.
Is that good or bad?
Sometimes it's a bad thing
if experts are rushing into the new, newly hot idea.
I think that if people are onboarded well
and have a more comprehensive understanding,
if they're basically like charlatans
who aren't going to do their work,
then that's more of a problem.
So I think by default,
with like another capabilities jump or two,
they will flood in.
There's basically a question,
and I don't anticipate they're going to read lots
of less wrong.com posts to be onboard.
They're just gonna start talking
and trying to be about themselves.
I say this as in my time,
empirical machine learning research,
you basically should assume
that when some area starts getting pretty hot,
there'll be lots of random new people coming in
and trying to influence the discussion substantially.
Hopefully the people as they come in
would have some understanding of many of the basics though,
but I think by default, it's relatively inaccessible.
You'll have to read a lot of scattered content
from different places and a lot of it will be idiosyncratic
and it'll just take a long time to go through.
Those are some of the reasons for doing this.
And then also I think that given that rogue AIs
is not the only concern or only risk source,
there's a lot of content that even a lot of people
who've been thinking about AIs risk for a while
will possibly need to become aware of.
So that's why, so just as a grad student,
when I just sort of like developed
and just focused on these other things
other than rogue AIs,
and then now I think people are recognizing
the importance of that.
So now there'll hopefully be,
or so now there'll be some material
to help get a more formal understanding
of these other sorts of issues.
That's great.
I'm looking forward to reading it.
I think we should nonetheless talk about rogue AI.
That's your last category of risk.
One issue here is proxy gaming.
Well, how does that work?
How is it dangerous?
Yeah, so you could imagine
if you've got a very powerful AI system,
if it finds reliability holes in the objective
that it's given, then this could be destructive
because it's being guided by a flawed objective.
I think in a colloquial example is with believe in Hanoi,
there'd be a bounty for killing rats.
And so if you get the rats, you get a bounty,
but then people were incentivized to breed rats
so as to collect more of that bounty.
That would be an example of an objective
that you put forward that ends up getting gained.
It's fairly difficult to encode all of your values
like well-being and whatnot into a specific objective,
a simple objective.
So you might expect some approximation
or to what you actually care about.
And in machine learning, a famous example,
this is the boat racing or coast runners example
that OpenAI had, which was of proxy gaming,
of there's a reward function
and the reinforcement learning agent
would optimize that reward function.
It was, this was a racing game.
You'd think it would optimize the reward function
by going around the track,
but what it instead learned to do
was it can get a higher reward
by getting lots of turbo boosts.
And the turbo boosts, it could get a very rapid sequence
of them by crashing into walls
and catching on fire and then continually turbo boosting
in that way.
And that would help it get a higher score.
So there are often holes in these objectives
due to an ability to compute exactly the right objective
or maybe we can only monitor some parts of the system.
There's a computational and spatial
and temporal constraints on the quality of the objective,
meaning that you're gonna often have to go
with an approximation.
So it's something perfectly ideal.
This relates to good hearts law,
which works in human domains also
in which it's difficult to specify exactly what it is you want.
And whenever you specify something you want,
that thing you've specified is now open to being game.
So an example here might be
that you want deep scientific insight
and you assume that such insight correlates
with citations or number of citations.
But then you get gaming of the citation systems
in which academics are incentivized
to maximize citations at the cost of scientific insight.
So is this a more general problem
across all agents, humans included?
Yeah, yeah, I don't think this is specific to,
I don't think this is specific to AI agents.
I will say that some objectives are harder to game than others.
For instance, the bounty on rat tails is a lot easier
to game than like citations,
because citations can be very valuable
for getting emigrate,
if you're getting a green card, for instance,
and it's strong incentives to do it.
And but it's nonetheless challenging.
So some of these objectives,
even when people are trying very hard to game it,
they still can be correlated with a lot.
Like college admissions still focuses,
incentivize people to be productive.
Yes, they'll go overboard in studying for the exams
and whatnot, the college admissions tests.
Yes, they'll go overboard
in the number of extracurriculars and whatnot,
but I still think it like,
it doesn't can help shape compared to
they're not being the incentive in the first place.
I think overall, my take on the GoodHoods Law
is that there's some objectives that are,
or some goals are all goals and proxies are wrong.
Some are useful and some though,
when gained in particular ways
could be potentially catastrophic.
So there's quite a variety.
There are some objectives as well
that people would claim would produce good outcomes.
For instance, if you gave an AI an objective,
like make the world the best place it can.
And if that was actually the objective you gave it,
okay, that's quite different from like make people
very engaged in this, with this product.
That's quite different.
I think that making these proxies incorporate more
of our values becomes more possible across time
because the systems can represent these other sorts
of notions of say, well-being of autonomy
because they have a lot better of a world model
and more of an understanding of people as well.
However, so I think that getting objectives
that are in the right direction seem possible.
The issue is making them be robust to adversarial pressure.
I'm not as concerned about like,
we get telling AI, go cure cancer
and then it does something like,
oh, I'll give lots of people cancer to experiment on them
to speed up the experimentation process.
This is easily ruled out by some like objective
with like an interpret the request
as a reasonable person would.
This is a fairly new development in AI
that we now have these large language models
that can at least to some extent understand common sense
and have kind of a more subtle understanding
of human values.
Yeah, earlier there'd be the AI's they would be kind of
like savants where they understand some particular thing
well, but then nothing else.
And, you know, human values are so late
in the evolutionary process and suggest that they're very
late to be one of the last things that AI's learn.
But that fortunately wasn't the case.
We explored this a few years ago in the paper
with the ethics data set.
We're basically using that to show that look,
they've got understanding of various morally salient
considerations.
Here's their predictive performance
on like well-being things.
Here's their understanding of deontological rules
and notions in justice and fairness,
such as whether people get what they deserve
or whether people are being impartial.
So they have an understanding of a lot
of morally salient considerations.
There is a question of reliability though,
if they're optimizing that objective,
are they basically is that objective
succumbing to that adversarial optimization pressure?
If it's optimizing it, it's basically functionally similar
to it being adversarial to that objective.
This is why there's a focus on adversarial robustness
because later we would have, we've got an AI agent
that's optimized, it's given a goal
and this AI system is outputting
whether it's succeeding by the goal or not.
So we've got an AI evaluator
and we've got an AI system that's optimizing that goal.
This AI evaluator, you don't want that being game.
You want that AI evaluator being adversarily robust,
robust to optimizers trying to say
that it's doing a good job.
So that's the sort of threat model later stage
and that's how some of these topics
that were explored in vision and whatnot end up
and now finally with the large language models,
tax paper, which you can, I guess,
read about that in the New York Times
where jailbreak and manipulate these models
with little adversarial suffixes.
In a later stage, we'd have AI systems
evaluating other AI systems and you want,
and those AI systems that are evaluating
are implicitly encoding an objective
and you want those to be adversarily robust.
So adversarial robustness is not a easy problem to fix.
And if you don't fix that issue,
then you might have some AI systems just gaming the system
and going off, optimizing an objective aggressively
that is not what we want.
Is there a problem here with the concept of maximization?
So it seems to me that it would be less dangerous
to tell an AI system,
it go earn a million dollars on the stock market
than to tell it go earn as much money
as possible on the stock market.
Could we kind of cap the impact
and the potential negative impact
by capping the goal also?
I think that's one approach.
You could imagine conceptually a variety.
You could have satisficers where they basically are like,
and now I'm good to go.
I don't need to keep optimizing this aggressive.
There is the possibility of not giving them open-ended goals
or very ambitious goals
would make them less concerning, more constrained ones,
but adversarial robustness would be one.
There's also be anomaly detection.
Anomaly detection is something
that's researched quite a bit in vision.
I've had some part in trying to have
the research community focus on that.
And I imagine anomaly detection will be very relevant again
when we're trying to monitor the activities
of various AI agents.
Are they doing something suspicious here?
While they're being monitored,
are they kind of adversarily trying to make the monitor think,
oh, it's doing the right thing.
So we'll need anomaly detection too
to detect if there's some proxy being gained.
And that can reduce our exposure to that risk.
There's also having some held out objectives
of which the agent is unaware
that it's being evaluated against.
And that can also do things like reduce the risk of it
being going to extreme
and optimizing the idiosyncrasies of the evaluator.
But this is a problem.
I think that most of the problem right now,
though, if we have large language models
trying to optimize a reward model that judges them,
they can do that
and they eventually start to over optimize it.
Although the optimizers that are much more effective
at breaking machine learning models
are actually just straight up adversarial attacks
compared to neural models
that are taking multiple steps
and iterating on their outputs.
The generic gradient-based adversarial attacks
are just much more effective.
So I think of the sort of risks of gaming,
I think most of the we need to do more
just to address the typical adversarial robustness issue.
Gold drift is a somewhat related issue
where the AI's goals shift over time
and the AI might come to take an instrumental goal
as an intrinsic goal.
How could this happen?
It's still a bit unclear to me
how an instrumental goal would become intrinsic over time.
So to start out with,
an intrinsic goal is something that you care about for itself.
That could be something like happiness or pleasure
for some others that could say,
maybe friendship, you'd say, I care about that in itself.
You might care about your partner's wellbeing,
not because it's useful to you,
but you care about their wellbeing in itself.
And then there are other things
that are just instrumental
for achieving those intrinsic goods,
such as like money.
Money lets you buy things
so that you could have higher wellbeing
or a car, it gets you from point A to point B.
However, some people have intrinsified
to use this sort of more of a Bostrom phrase.
Intrinsified, some of these instrumental goals,
some people actually just directly want money
even to a point where it doesn't make sense or power.
Many people are just like, they want power.
Even if it like harms other parts of their wellbeing,
they're willing to make that type of trade-off.
So these, they might latch onto these cues
and develop some of the wrong associations.
So we see that in people and there's a risk
that AI systems might develop those wrong cues as well.
Goal drift could happen in some other types of way too,
where if you have multiple different agents,
they might interact in some unexpected way
and then a new goal becomes,
a new goal starts to drive their behavior.
An example, we can see this
in basic AI multi-agent situations.
It's not catastrophic of course,
because we're still here,
but in some AI society
and some Stanford paper from earlier this year,
the AI's start talking with each other
and then they start arranging social structures
that they're gonna have a,
they're gonna throw an event at some person's house then
and then this starts to,
then they start acting in all these ways
to make sure this type of thing happened.
And then these sorts of things
start to be what drives their behavior.
There's some other way in which things can end up drifting,
not necessarily through having something to be intrinsic,
but there could be these emergent goals from interactions
that end up driving behavior.
Certainly there are many emergent things in society,
things that become new
and this isn't the goal that I originally had
when I was 10 years old,
but now some of these things end up driving my behavior
quite substantially.
So if we have adaptive AI systems
and if they end up responding to each other,
then you could have some emergent complexity happen
and that those interactions that the behavior
starts driving the overall group behavior
as they're imitating each other
as they're responding to each other.
And so it's basically multi-agent systems
be very difficult to control in the single agent one,
you'd have to worry about there being some wrong association
between an intrinsic and instrumental goal
like money or power.
And that could mean if that does happen,
if basically something wrong gets intrinsified,
then you're in a very dangerous situation
because then your AI has a goal
that's just different from what you wanted.
And so then it will, to get that goal,
it will optimize against you,
it will respond adversarily,
it will resist your efforts to shut it down
so that it can achieve that goal.
So although it's not something that necessarily happens
by default or with extremely high probability,
if it does happen,
then you've got a substantial risk in front of you.
I wonder whether these AIs will persist for long enough
for Gold Drift to happen.
So normally we retrain models every couple of years,
we switch out for the newest ones.
And so it's not like a person that has 30 years
to change their values.
Will they last long enough for Gold Drift to matter?
So I guess two things,
one is the world will move substantially more quickly
in the future, such that often in these more pivotal periods,
I don't know if it was Lenin or something like that,
like there are decades in which weeks happen
and then there are weeks in which like decades happen.
So even if there is a high replacement rate
in the AI population, this goes on in a much slower process,
they could still end up constructing things
that end up causing their goals to be different.
Like let's say they develop some different type
of social infrastructure for mediating their interactions.
There are new AI companies being formed
and they're end up driving many of them.
Then those features of the environment
would end up affecting the generation that comes after it.
So you could still imagine some type of drift,
some intergenerational drift,
but if each generation is very short,
you can still imagine some type of Gold Drift in that way.
This is kind of think of yourself.
Many of the goals, the intrinsic goals that you have
or intrinsic desires that you have
are completely unlike those when you were younger.
The even taste in food, the things you care about,
maybe you acquired sports, your taste in music,
affiliations, all of these things
that are changing across time.
And they can also go away too,
some of the intrinsic things you care about.
Like I care about this person's well-being for themselves,
but then you break up with them.
Now I actually don't care about their well-being in itself.
I don't have that strong of a feeling toward them.
So adaptive systems carry this type of property.
This is one way in which they end up gaining some goals
that we didn't intend, either through some emergent goal
from the product of various interactions
or through them intrinsifying some instrumental goal,
like power, they end up having too strong of an association
with that and reward
and then just end up seeking the power itself.
Could Gold Drift be a good thing?
So we wouldn't want to fix human values
from the year 1800, for example.
You could describe our changing goals
from back then to now as a form of Gold Drift,
where people from 1800 might disagree violently
with whatever we believe now,
but we still probably think it's a good thing
that we've changed our values.
Yeah, could it be good and could we learn from the AIs?
Yeah, so I think this is a good point
in what makes thinking about AI risk generally a lot harder.
As we mentioned earlier, there's this balance issue
with malicious use that because you'd be concerned
about unilateralist misusing AIs
or rogue actors misusing AIs,
that we should then centralize power,
but then you end up getting some other existential risk
of lock-in, of concentration of power.
And then I think likewise, in this case too,
that you can't have a society in complete stasis
and as it would be driven by new emergent type of structures,
you should still try and make sure
that you have some control over that process
or reasonable control over that process.
It seems if there's not much control,
then I think you'd likely to slip from your hands,
but otherwise, so there's basically one will have
to strike a balance between some very chaotic state
where they're running wild and some stasis.
And this is just a continual issue
in many areas of evolving groups.
Yeah, that would also be a problem
if there'd be too much entrenchment,
if there isn't an ability to have adaptation
of the things that we care about.
Yeah, so anyway, there's some dissonance.
There aren't simple answers with this.
This is why it's will be a balancing act.
And also why I don't expect a,
in particular a single solution
to solve everything for all time.
We'll need to respond, we'll need institutions
and structures and control measures
that respond to the features of the environment
and calibrate reporting.
Why could AIs become power seeking?
So this is a very, I think one of the main
sort of AI risk stories would be it becomes power seeking.
I'll make a bit of a case for it
and I'll speak about some issues with it too.
You could imagine a person gives an AI system a goal
like goal make me a lot of money as an instrumental goal
gaining a lot of power seems like a very helpful way
to accomplish that higher level goal.
So there's a concern that when you specify a goal
that there'll be some sub goals
that are too correlated with power
and you'd want to make sure that you can control
that those tendencies.
So that's one of just being,
when you're just directly giving an AI goal
it may have a goal that's correlated with power.
But that's not, is that terribly unexpected?
We will give them goals that relate to power quite a bit.
Militaries will probably build AI systems
that are fairly power seeking.
And so we should expect some amount of AIs
that are pursuing power either as their main goal
or as one of their main sub goals.
And maybe power seeking to a limited extent is okay.
Basic feature of accomplishing many of these sorts of goals.
Like for instance, like the fetch the coffee one,
it would have a, if you'd instructed to fetch a coffee
it would have an incentive to preserve itself
because it can't fetch the coffee otherwise.
And, but you might want to curtail some of those tendencies
so that those don't get out of hand.
But that would be a, you know,
we've had a paper at ICML earlier this year
where we're deliberately giving it penalties
to penalize some of these tendencies
that it has when it is trying to seek its reward.
It starts having incentives to accrue resources
and things like that.
And then can we have it more acquire the resources
that are more minimal to accomplishing its goals?
Can we have it engage unless power seeking behavior?
So I think that that's something that we can offset
but we'll need to make sure that we have good control
measures for that to keep that, keep that in check.
There's also the, so that's one of just people
directly instructing it with goals that are by default
probably going to be pretty related to power.
And there's also maybe they would intrinsically care,
let's say that they had some random goal.
It's like a paperclip maximizer.
You're sampling from, use old verb is you're sampling
from mind space and then it has a random set of desires
and whatever that set of desires,
then it would end up trying to seek
a substantial amount of power.
That's one claim, but I think that has to be
something more rigorously argued.
I should claim that, or I would like to note that
I think that a lot of those power seeking arguments,
I don't think it works as well as I thought it did,
the arguments associated with them.
I still think it's a relevant thing
that we'll want to control the sub goals of AI systems
to make sure they're not too strongly related to power
and that there's nothing unexpected going on there.
So for instance, people might argue for power seeking
by saying like, well, power is instrumentally useful
for a broad variety of goals, therefore it will seek power
if it's trying to accomplish any sort of reasonable goal.
And you'd ask them what power is and then they'd say,
power is what's instrumentally useful
for accomplishing a wide variety of goals
and go, okay, well, that's a tautology.
So we need to be more careful.
What exactly are we meaning by power here?
Separately, there's often a bit of,
so that's one like slight bug that lurks in the background
is that they'll define power in terms of instrumental stuff
and then it's tautological.
Another issue is that there's sometimes a conflation
between power seeking and dominant seeking.
Those are not the same thing.
When the AI is trying to fetch the coffee
and is engaging in self-preservation to do so,
it's not necessarily, therefore,
trying to take over the world.
So saying that an AI is power seeking
is not necessarily existential.
Indeed, you could imagine various ways
in which other powerful actors engage in power seeking
behavior, but don't try and seek dominance.
So for instance, different countries
in trying to increase their own power to preserve themselves.
This is the sort of thesis of neorealism
or structural realism.
And what happens is they will basically,
many states will just try and keep power
relative to many of their peers.
If Germany, for instance, tries to take,
it's seeking power to protect itself,
but if it tries seeking power at the level
of the global domination, it will be met with force.
There will be balancing from other peers.
So when we're in a multi-agent situation,
then it doesn't necessarily always make sense
for AI systems to try and take over the world
because there'll be other AI agents to be,
that will thwart my preferences or goals and desires,
so I will counteract you.
Balancing in international relations is what this is called.
That's a thing that can offset dominant seeking.
So it's not necessarily case that power seeking
is dominant seeking and trying to take over the world.
An additional point is that we can partly influence
the dispositions of AI systems.
Sorry to say, we can do that.
We can make these like have dispositions
to be a good chatbot or be a good assistant.
Now, how strong is that?
It's not perfect, but if it were given a task like,
hey, go accomplish this, go accomplish some goal for me,
if it would think, well, you know, the best way would be,
I could accomplish this goal better
if I were extremely powerful and took over the world,
but that may not be in keeping with its values
necessarily there.
So it may have some tendency pulling in that direction,
but you could also give it some dispositions
to pull it against it and that might be sufficient
to offset some of these tendencies toward power.
Even if there is some incentive there,
it may not be enough to overwhelm it.
So a lot of this discussion about instrumental convergence
needs to think about the balance between these forces
and they would need to argue basically
that the instrumental drive is extremely strong
to overwhelm fine tuning and all of these sorts of things,
which I don't think that there's much
of a specific argument for that.
I wanna highlight here, Joe Carl Smith has a great report.
I think the most rigorous argument
for why power seeking in AI could be existentially dangerous.
So just for listeners who are interested
in what I think is the best argument for that out there.
I agree, I agree.
He helped popularize the sort of power seeking phrase as well.
And I think that by focusing on power,
that helped us integrate this into some other
like academic discussions, like power versus cooperation.
What I was describing here,
just a moment to go about balancing was that
we can take a cue from the international relations literature
of seeing like, well, power seeking agents
when that's one of their main goals don't necessarily,
that doesn't necessarily turn into them trying
to seek domination.
Another thing is that in Bostrom, in superintelligence,
there's also a sort of part slide of hand,
not intentional, but I suppose maybe an accident,
where he's saying that power makes you better
or able to accomplish your goals,
therefore they will seek power.
That's saying that something is helpful if you have it,
that doesn't mean that it's rational to seek it.
So although there's an incentive for it,
that doesn't mean it's instrumentally rational to pursue it.
So for instance, if we run the argument in a different way,
it would be helpful for me to be a billionaire.
That doesn't mean that it's rational for me
to try to become a billionaire.
I could, that would carry a lot of risks.
I would take a lot of time.
The existence of incentives aren't necessarily enough
to say that that's what will be driving their behavior
or is the first approximation of their behavior.
And I think that there are other ways
in which just power seeking doesn't emerge
or dominance seeking doesn't emerge.
If you give it some goals, like obviously if you say,
shut yourself off or if you give it a goal,
like don't seek power, these are obviously counter examples
for that just to show that this isn't like a,
it's not a law of all AI systems
that they will try and seek power.
Separately, if you give it a more goal,
like go fetch the milk, it could try and take over the military
to put up a motorcade to make sure
that it can get to the store very quickly.
But if you had some time penalty or something,
this would not necessarily be the thing to do.
So instead just go fetch the milk,
but often be the best way of getting the reward
instead of some very circuitous path.
Now, so I do think that there is a risk of,
if you have AI agents that are not protected and autonomous,
you could get power seeking type behavior.
For the same reason that states try to shore up their power,
they shore up their power
because there isn't anybody they can call on for help
if they're getting attacked necessarily.
Like if the US starts getting attacked,
maybe some countries will come,
but this isn't a police force that will settle the issue.
So the best they can do is try to shore up power
to defend themselves so that they can't be pushed around like that.
So we have a non hierarchical or quote unquote
anarchic international system,
and that incentivizes agents to seek power
to preserve themselves to pursue whatever their goals are.
And you could imagine if AI systems are not protected,
if they are part of say some crime syndicate,
or if they're rogue, they're unleashed,
somebody unleashes them,
then those systems would actually have
a very strong instrumental incentive to seek power
in the same way that states do,
that if they want to protect themselves
from some potential adversaries that can harm them,
there isn't somebody to call on.
They can't ask the US government,
if there are crimes syndicate,
they can't say, US government protect me, I'm getting harmed.
That is not a possibility to them.
So what they have to do is they have to take matters
in their own hands and accumulate their own power.
So what I've done is I've sort of flipped things a bit.
There'd be the usual argument that
AIs might be power seeking just by their inherent nature,
by the inherent natures of goals
and optimizers and things like that.
I've instead mentioned that one source of power seeking
is humans give them some sort of goals
that are very correlated with power,
and then there might be some unexpected stuff
that happens in their subgoals.
And then the other thing I've done is I've mentioned
how the structure of the environment that they're in,
some structural reasons for why they might end up
seeking power too.
I'm not as sure about them having an intrinsic one
or internal reason for power seeking,
but I think goals being given intentionally,
or the structure of the environment
that they find themselves in,
it's a sort of cage that they're locked in,
there's really nothing they can do
if they're wanting to accomplish a goal
other than to invest a lot in protecting themselves,
would also incentivize them
to seek a substantial amount of power.
So I do think power seeking is a concern,
but not for the same reasons that other people are giving,
like we're gonna randomly sample a mind for mind space,
it'll be very alien, and by way of almost any desires,
it will necessarily try to seek dominance over humanity.
But I still would be concerned about power seeking.
How concerned are you about deception arising in AIs?
I think that the contribution of focusing on deception
was useful because we now see that AIs have,
to some extent, some representation
of morally salient considerations,
as we explore in the paper aligning AIs,
I was shared in the values,
and I clear maybe 2020 or something,
where we measure that and show that,
by now it's obvious because it's in chatbots,
and people can ask it moral questions,
but they have some capacity for that.
And the deception part focuses on maybe they're actually,
although they maybe understand the goal,
they don't necessarily feel inclined to pursue it.
So in psychology, this is a distinction
between cognitive empathy and compassionate empathy.
Cognitive empathy psychopaths have,
they can understand and predict
what people will end up feeling
in response to various actions,
they have very good predictive model
of people's feelings and their emotions
and what they think is valuable.
Meanwhile, if they have compassionate empathy,
that's when they feel motivated to do things by it,
and help people realize those values.
So there's a distinction that they would have cognitive
empathy, but not necessarily compassionate empathy.
And so if they're deceptive,
they could basically play along.
They could be like, yeah, I don't actually care about you,
but I'm gonna act like it to get my goals accomplished
as psychopaths do.
And here, maybe we should mention here,
how the drive of deception arises from the way
that we are doing reinforcement learning
from human feedback or how it could arise from that.
So in the Machiavelli ICML paper,
we saw instances of them doing deception
because it simply helps them accomplish
their goals better by default.
So many environments just incentivize the type of behavior.
If they have some type of misaligned goal from us,
then they could buy their time and wait to come to power
to take a quote unquote treacherous turn.
So it could just be very strongly incentivized
to buy some type of training process,
like by just seek more reward, deception can often be
a good trick when you're monitored, behave nicely,
when you're not monitored, switch your behavior,
behave in a more cutthroat way.
That's how a deceptive behavior can be a concern
or some Machiavellian type of behavior.
And there are instances of this.
You could imagine as a more non-agentic case
with chatbots is if they're being given human feedback,
maybe they'd have an incentive to say
very agreeable answers to people.
Things that they'd say, oh, that sounds good to me,
even though it's if it's not necessarily true.
So that's how even chatbots might be incentivized
to be in a somewhat deceptive direction.
But we can also see this in agents
just that often helps them accomplish goals.
Also chatbots might learn to recognize the ways
in which they're telling bad lies, let's say.
The obvious things they're saying that are false
are penalized, whereas the more sophisticated ways
they might be telling falsehoods are not penalized.
Yeah, good.
So this gets it like in a lot of like repeated interactions
and whatnot, deception often emerges.
In the evolution paper from the last time I was here,
we spoke about how a deception can often be
and concealment of information can often be
an evolutionally stable strategy
and that there are many instances
of deception in the environment.
So it's a fairly difficult thing to plot out
when you try and control for it.
You often end up selecting for a more deceptive behavior.
At the same time, we do have progress on this though,
where we can, in a recent paper we submitted,
or in a recent paper we uploaded to archive
called Representation Engineering,
a top-down approach to AI transparency.
There we have instances, many instances,
it's not that difficult to control
by manipulating the internals of the model,
whether or not it's lying.
It has an internal concept of what is accurate.
We can find a truth direction,
we can subtract the direction or something of that sort,
and then that can cause it to spit out incorrect text
and we have other more sophisticated control measures too,
but we can manipulate internals to do that.
So it's within the capacity of AI systems
to lie and be deceptive.
We have another paper on that called,
if you search AI deception,
and then maybe my like name or something,
then you'd see that paper.
So many instances of AI deception already,
but we do have some traction on this problem.
So fortunately, there'd still be the issue
of having more reliable lie detectors
and being able to control them to be more honest
or output their true beliefs.
So there's definitely much more work to be done,
but we're at least not helpless.
We don't need to wait another 30 years
for interpretability research to get to a state
of being able to just start to rush against the question.
We now have some ability to influence
whether AI's lie by controlling their internals.
And so that makes me more optimistic
about dealing with this problem,
but you don't wanna do premature celebration.
I don't know how much time we'll have to continue
getting those detection measures
and those control measures to be highly reliable.
So that'll depend on like the having a lot of researchers
who can research with these cutting edge very large models
to make progress on it.
Yeah, the representation engineering paper
was super exciting.
Maybe you could explain what,
at what level does representation engineering work?
Because it's different from mechanistic interpretability.
It's more high level,
and which is what we're after in a sense.
We are after the high level emergent behavior
in these models.
Yeah, I was mentioning compassionate empathy
and cognitive empathy,
because it's a bit of psychology,
but I think trying to do something
more like a project like AI psychology
or AI cognitive science,
is I think what we should be trying to do here.
So in the case of this representation engineering,
that's, I think we're trying to be the analog of that,
where we're given these high level representations of truth
and goals and things like that.
Can we make it be so that it actually outputs its beliefs
or what it says it believes is actually what it believes?
For that, you need to have a handle
on these very high level concepts
so that they're not psychopathic,
so that we can control their dispositions to behave
and have things like compassionate empathy.
Meanwhile, I think the mechanistic stuff
is looking at a much lower level.
It's looking more at the substrate,
at the neuron level, at the circuit level,
at the node-to-node connection level.
And that's maybe closer to something like neurobiology,
and then what we're doing is more like trying to study
the mind as opposed to trying to study
the specific structures in the brain
and the connections between them
and how that gives rise to phenomena.
So I think philosophically,
I had tried many times to do a paper on transparency,
historically, but it wasn't a good angle of attack.
And in my view, it would take too long.
But I think if we do it in a more top-down type of way
where we try and here's the eyes of mind,
let's try and decompose it into some representations
that drive a lot of its behavior
and maybe decompose those further and further.
Basically, we have a big problem of understanding
in the eyes of mind, let's break it up into sub-components
and try and get a handle on those and control those.
I think that approach might be more efficient
at reducing risks of AI deception
than building from the bottom up understanding,
this is how it answers,
this is the circuit in it that lets it understand multiple
or identify a multiple choice question.
And then this helps it select the weather to output
the full response back or whether just to select A, B, C, or D.
Things like that.
You can build those up,
but that might become very complicated in time.
So I think it might make sense to not work from the bottom up,
but go from the top down.
There are analogs of this type of approach
in cognitive science.
People would initially try and just study things
at the synapse level, but it can often be more fruitful
of trying to understand things at the representational level.
What are the high level emergent representations
that are a function of all the population,
of all the neurons in the network,
and try and understand things at that level?
Now, there's, of course, a risk of,
well, maybe there's some funny business
that gave rise to that representation.
And that's true.
We could still do things to reduce that risk
by trying to understand the representations
at various layers in the network
and try and decompose the system further and further
so that there isn't much room for funny business or deception.
But so that's it at a high level.
It's not viewing neurons as the main unit of analysis.
It's viewing representations as the main unit of analysis.
And neurons are relevant insofar as they help us predict
and explain what's going on in representations.
But those are more of a,
that's sort of just the substrate.
It's a comment on the substrate in the same way
that if we have a computer program that plays Go,
if I'm reasoning about the Go program,
I'm just probably gonna be thinking about Go strategies
when I'm playing against it.
I don't need to think at the software level,
like, well, where do you think it,
what layer do you think it's at right now?
Or what TensorFlow objective function
did Alpha go into optimizing here?
Maybe some of the examples.
We don't need to analyze at that level.
We certainly don't need to break it down
at the level of assembly.
We don't need to reason about assembly
to try and understand its behavior.
So I think that there's some emergent complexity
inside of neural networks.
We just need to, we can study it at that level
and it's studying at that level is fruitful
because there's an emergent ontology
and some coherent structure inside of that,
which you would get up getting lost in the details
when you end up zooming in further
to the neuron level.
So although it's possible in principle,
it's possible in principle to explain everything
in terms of that,
just like it's possible to explain the economy
in terms of particle physics.
If you, computationally, you could do it,
but it doesn't make sense to study it at the level.
This isn't to say that there are
mechanistic interpretability
and representation engineering
are completely loose and separate.
There's probably overlap just as like in biology
and chemistry, they have some overlap,
but you wouldn't try and understand biology
just through chemistry.
And I think if you're trying to understand representations,
I don't think you're necessarily just gonna try
and understand everything through neurons
and node to node connections
and specific execution pathways
and treat it like a computer program,
but instead something more like a mind
with loose associational high level representations.
Yeah, so take it a cognitive trait like honesty.
Do we know anything about how that's distributed
across the model?
Is there like a center,
where in a cluster of the weights
in which this is now representing honesty
or functioning as the honesty module
or is it more distributed across the whole model?
Yeah, neural network representations are highly distributed,
which makes sort of trying to bolt down
and pinpoint specific locations
of a lot of functionality, a lot more difficult,
as well as the interactions between all these components too
can end up giving rise to a lot of complexity.
Imagine that you understood a neuron
and it was this detects a whisker at 27 degrees
and this other neuron detects some,
an upper corner of a fire hydrant.
And if you can understand these millions of neurons
that gets you some way,
but are you really understanding
the collective overall emergent behavior of the system?
That doesn't necessarily follow.
So I don't think it's enough to understand
the lowest level parts
to understand the overall system
and its collective function, but it can be helpful.
It can provide some types of insights.
In the case of honesty though,
I find that it's a direction
and it's beliefs about what's true or not,
our directions in its representational space,
and it doesn't seem to be located at a specific neuron.
So when we adjusting the representations
through various control measures that we proposed,
then we can actually end up manipulating it.
So that's anyway, so partly this paper
is a bit more like philosophical
in like what's the sort of paradigm?
What's the, how do we want to,
what's the strategy that we're wanting to proceed
in making AI systems transparent?
The representation level is the,
going to be a very fruitful way.
I should note that,
but it'd be useful to diversify over,
research agendas and things like that.
Hopefully we'll get more reliable control measures
and be able to modify relatively arbitrary parts.
We'll have success when we can like inside of the AI system,
when we have better ability to sort of read their mind
or understand the representations,
if we could use it for like knowledge discovery,
then we've known that our methods are fairly good
because they're probably gonna pick up some observations
about the world from their big pre-trained distribution
that no individual knows or that many individuals don't know.
And if, so if we can get better tools like that,
then that would be a late stage sign of success.
Yeah, and it seems like we have a better shot
at success here than neuroscience on humans
because we have such fine grained access to,
it's as if we had a human brain spread out
with full access to what all of the neurons are doing.
So, or do you think that's right?
Do you think we have a better chance of success
compared to traditional neuroscience?
Yeah, yeah, certainly.
I think the sort of mechanistic interpretability
of people would claim this as well,
that since we have access to the gradients,
we have rewrite access to every component of it.
This allows for much more controlled replicable experiments
and a substantial ability to do science
that the bear ears to entry in cognitive science
are many of them are removed.
There's also this might get easier in time,
what makes this now possible?
Whereas previously it wasn't.
If you use models like GPT-2 or below,
they just, the representations are not very good,
they're quite incoherent.
But as we use larger models like Lama-2,
pre-trained on many more tokens,
they have some emergent internal structure
that actually starts to make some sense
in directions that are correlated
with coherent concepts that humans have.
I think earlier it's more like a chivalrous,
but now there, since there is some coherence to it,
it's not just a big causal soup of connections.
So this is why I think, unfortunately,
this wasn't something that we could have particularly done
in like 2016 and is very much something that's possible
now that previously wasn't.
Dan, thanks for spending so much time with us here.
It's been very valuable for me.
And I think it will be for our listeners too.
Great, great, great.
Thank you for having me.
Have a good day.

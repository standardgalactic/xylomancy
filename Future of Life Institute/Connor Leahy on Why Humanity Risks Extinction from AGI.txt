Welcome to the Future of Life Institute podcast.
My name is Gus Ducker, and I'm here with Connor Leahy,
who's the CEO of Conjecture.
Connor, welcome to the podcast.
Thanks for having me back.
Glad to have you. You're the author of The Compendium,
which is an introduction to AI risk from the ground up.
Why did you write this?
So, saying that I wrote it is very generous.
It was very much a team project where I probably
contributed, you know, not the most.
So, it was a team project between me, Chris Gamel,
Adam Chimi, Gabe L4, and Andre Mioti,
all contributed important parts.
So, it couldn't have been done without my co-authors.
Just want to really stress that.
So, the reason we decided to write The Compendium,
well, I think I was actually the person
who decided to write it,
and then kind of roped everyone else in,
was that, you know, there's many arguments around,
you know, AI risk and many explanations and so on,
but they're often scattered.
They're often scattered kind of in like strange locations,
like, you know, fragmented across blog posts
or like interviews and stuff like this.
And they're often written by audiences
and for audiences that are not your typical everyday person.
They're often unnecessarily technical.
They're often require a lot of like jargon
or like buying into other assumptions
or like philosophies or worldviews
that are just kind of like not necessary.
That's one reason.
So, the first reason why it was just
because to create something that's like,
the whole thing in one spot.
Second reason was because it's actually aiming
at a non-technical audience rather than a technical audience.
And I would say the third reason is,
is that there are actually some arguments
which I think have not been made in this clarity elsewhere,
in particular around the social and the political dynamics.
Many of the people talking about AI,
as I talk about The Compendium,
have conflicts of interest
or reasons to not want to talk
about certain conflicts of interests
or political situations or conflicts for various reasons.
One of the things that I have going for me
is that I am very, I'm independent.
My money doesn't really depend on anyone in particular.
I can kind of say whatever I want
and also like, you know, call out anyone I want.
And I try to make use of this privilege that I have.
So, there are many people and organizations
that I believe had deserved critiques
that had not been made, at least publicly,
and not in this clear form for various reasons.
So, a part of that is I wanted to make these critiques
and drive a wedge, like make clear that,
look, there is an actual conflict here.
Maybe there's a conflict that can be resolved.
You know, conflict doesn't mean
that we can't find peaceful resolution,
but we should be clear there's an actual disagreement here.
Like, I believe strongly
that if there's a disagreement, you should have it.
Like, it's okay to disagree about things,
what you shouldn't do is pretend to not disagree.
Maybe you could give an example
of what you're talking about here.
What's something that you've pointed out
that haven't been said before?
I mean, it's been said by someone somewhere, I'm sure,
but a big thing is that there is a very common confusion
that happens when I talk to, for example, policy makers in DC
where they meet like the effect of altruist movement
or like the AI safety movement, you know,
around like open philanthropy and these kinds of people.
And they were often very confused by these people.
And they're kind of like the thing that kind of happens
is they're like, these people come to DC and they say,
oh, AI, huge risk is going to destroy everything.
It's going to literally kill everybody.
And then policy makers are like, wow, okay,
wow, it seems pretty bad, so we should be ban it.
Like, oh, no, no, no, don't ban it.
And that's very, very confusing.
There is this weird thing where a lot of this like,
there's this group, which we can talk about,
I think we're gonna talk about a bit later,
which just like formed, there's like a lion's leg
of various organizations and people
who claim to be the AI safety movement
or representing the AI safety or X-Rest movement or whatever,
but fundamentally want to build AGI
and are trying to build AGI.
This is around groups such as Anthropic,
of philanthropy, the effect of altruist movement,
previously FTX and stuff like this.
And I think there are some people in this area,
in this realm who are like,
do care about the risks and do, you know, want to like,
stop AGI from causing massive risks and so on,
but there are those who don't
or who think that as long as they're in control of AGI,
it's okay or like this is the best plan.
And those people actually disagree,
but they don't know that they disagree.
So driving a wedge is making two people aware
that they're actually not allies.
They think they're allies, but they're not.
And this is actually very, very important for coordination
because if you try to coordinate with two people
who are actually different factions
and they don't know they're different factions,
you can't coordinate.
And this will always lead to confusion
and misalignment and betrayal and so on.
So I think it's very important
that a conflict should be had.
If you think building AGI is okay,
like Anthropic should just be allowed to build AGI or whatever.
And then, okay, you should state that clearly.
And then people who don't think that's okay
should also state that clearly
and they should have the conflict.
And I disagree with this.
Like I am firmly, no, I don't think that Anthropic
or any other private corporation
should be building AGI, period.
At least not in the foreseeable future.
I'm guessing you're not against AGI
in a kind of blanket way that we should ever build it.
Do you think we should never build AGI or Super Insultants?
My true opinion is that I think I shouldn't get to choose.
This is not a decision that I should be making.
This is a decision that humanity
or democracy should be making, not me.
I think it's extremely presumptuous
and kind of morally wrong to want to
or to make a choice like this for humanity.
I don't know what the best future for humanity is.
And maybe I would like X or Y more than Z,
but then I should get to vote on that
and should have my human rights respected.
But like, if I really want to live in this world,
but other people don't,
I don't think I should have the right to enforce that upon them
because I'm gonna be the one who builds AGI first.
I think that is like morally evil
or like very close to it.
Yeah, all right.
But Connor, what if others are racing to build AGI before you?
And I'm saying that as kind of partly a joke,
but this is an argument that's often made, right?
We will build it first and we will build it in a safe way.
And if we don't do it,
others will do it in an unsafe way.
Exactly.
This is the main argument that these people make
and these are the exact people that are going to kill us.
Like, it is important to understand
that you find yourself in the situation.
You are the bad guy.
Like you are the person who is going to kill everyone.
Like it's just to be very, very clear here.
If you, that's not saying that you're acting irrationally,
that's not what I'm saying.
It might be the rational thing for you to do,
give your local incentives.
But you must also understand
that you are an agent of malloc.
That you're an agent of,
you are the entity that is, you know, breaking everything.
Like you're the problem.
There is this really funny image that I very like,
which is a picture of Sisyphus pushing a boulder.
I'm saying, and the caption is, you know,
a lot of people think they're Sisyphus,
but actually they're the fuck ass boulder.
And like, if you are one of the people
who are like, I must race for AGI
or you're working with an organization
that's like, I must race for AGI,
you're the fuck ass boulder.
You're the thing that is creating X risk.
And look, I take a very systemic view
when it comes to morality.
I think most bad things happen for systemic reasons,
not because there's like one evil guy
who is specifically evil and decides to be evil.
I think this does happen to be clear.
Like there are people who are like this,
but it's like really quite a minority.
Most bad things happen for systemic reasons.
If they happen because of structure, power structures,
they happen because orders or because market forces.
Look, I'm German.
Like, I know how it is, right?
It's like most Germans are and were fine people.
Like, you know, they weren't super evil,
but you know, bad systems lead to bad outcomes.
So I take a very systemic view
when I think about these things.
And this is a systemic problem.
There is a fundamental thing that every individual actor,
you know, who finds himself in the scenario
is incentivized to do evil, which is to race to AGI.
And they do.
And if they didn't, they are no longer part of the race.
And then whoever the least cares about this enters the race.
So there's also selection pressure,
where if you actually cared about AI risk so much
that you want to not build AGI,
well, then you wouldn't be at open AI or anthropic.
You would leave, you would not work there.
So there's also a selection effect.
It's kind of like with like,
if the CEO of a big oil company came up to be,
and he was like, I don't really think,
you know, climate change is a big problem.
This isn't evidence to me.
Like I don't update based on this information.
I'm just like, well, yeah, duh,
you were selected to not care about this.
Like otherwise you wouldn't be the CEO of Oil Corp.
Like I don't care, your opinion is a valid.
And there's kind of a similar kind of selection effect
that happens here as well.
So there's this big thing where like,
okay, you're in a race, so what the hell do you do?
So the weak answer is,
well, you just give into your incentives,
you do what's locally optimal for you
and you don't do anything.
And this is the convenient excuse that people tend to use.
I don't quite buy it and I'll talk about that in a second.
The main thing you should do is stop, drop and catch fire.
You should be like, help, help, help.
I'm in a race, help, help, help.
Like you should go to the government.
You should go to the UN.
You should go to every newspaper that will listen to you.
You should form a political party.
You should be help, help, help, help, help.
Or in this terrible situation, I can't get out.
We need to get out of this together.
Help, help, help, help, help.
This is what you would do
if you actually wanted to stop the race.
If I was Dario Amadeir, whatever,
I would not build AGI.
I would go to the president and be like,
help, help, help, help, help.
We have to stop these people from racing.
Like I can't stop unilaterally.
We have to stop all these people from racing.
Like we, you should have found a political party.
You know, you should build a coalition.
You should get unions on board.
You should get, you know, start international collaborations
on this kind of stuff, right?
Like actual coordination, actual civics,
actual politics.
It's the same problem as like the nuclear arms race, right?
Like where it's the same situation, right?
No one wins a nuclear war and no one wins an AGI race.
Some of the leaders of the AGI corporations
have done something that is superficially similar
to what you're talking about here,
which is they've gone to the US government.
They've talked about kind of some form of pseudo nationalization
or some form of government collaboration.
The framing there is though, we need to build these systems
and we need to do it safely.
But what's the difference between what they're doing
and what you're suggesting?
And the difference is that the thing that they actually did
is, you know, for example, let me give a concrete example.
I think it was Senator Blumenthal
when he was talking to Sam Altman during these hearings.
Sam Altman in 2015, I believe wrote a blog post
where he talked about how machine intelligence,
you know, superintelligence paces the gravest risk
to humanity's survival.
Very unambiguous, extremely unambiguous
what he meant by this.
This was one year after Bostrom's book came out.
You know, it's a kind of direct response to this.
It was extremely unambiguous what this meant.
Senator Blumenthal quoted this exact line.
He said, so you wrote this and then he said,
I assume by humanity's survival, you mean jobs.
Did Sam Altman then correct him and be like,
no, no, no Senator.
No, Sam Altman said, yes, of course Senator.
That's exactly what I meant.
There is a deep thing here where people
don't know their history, including me sometimes.
God forbid, young tech guy not knowing his history
has never been, you know, not a story as old as time.
You know, young, ambitious guy who doesn't learn
from history until he actually, you know, sits down.
What was happening right now is the exact same thing
that happened with big tobacco or with big asbestos,
big oil or any of these companies.
It's the exact same playbook is being played right now
where there are companies that think there is power
and there is money to be had with a technology
that has some form of externality or unaccounted risk
and they want to delay and deceive and, you know,
just muddle any response to this.
This is exactly what is happening right now.
And all of the companies are doing this.
They are being extremely strategic about what they say.
They're, you know, I'm not saying they don't make mistakes.
Like sometimes they do say more than they're supposed to.
I'm pretty sure, you know, Dario probably is not allowed
to speak publicly for various reasons, you know,
because every time he does,
he says things that incriminate him.
And, you know, other people, you know,
other CEOs are better media trained in various ways
and are very clever about how they phrase things
or how they dilute things and so on.
There is no way that a corporation
wants to actually get regulated.
That's just not how the world works
and that's not how any of these regulations.
They want self-regulation.
They want voluntary commitments.
They want, you know, them to set the technical standards.
And for what they definitely want
is they want holistic regulation.
Like we have to, have you considered the considerations?
We should first consider, you know,
we don't want to act too hastily.
No, no, of course not.
You know, the, all the evidence of lung cancer
from smoking is not in yet.
We should, we should do more evaluations of lung cancer
when smokers before we go to regulation.
Let's start with some more evaluations.
Some more studies.
Maybe we can create a committee.
Can we have some like White House committees maybe
that will evaluate the evidence, the controversy you see.
This is the playbook,
which is called Fear, Uncertainty and Doubt or FUD.
The way FUD works is you don't directly counter
the arguments of the critique.
You don't directly engage with them
because, you know, you'd have to be good faith to do that.
And that is hard.
And like we'll just draw more attention
to the people criticizing you.
So instead, what you do is,
if you benefit from the current status quo,
if just continuing on the path you're currently on
is beneficial to you,
you just spread fear, uncertainty and doubt.
You just, you know, spread ink.
You just confuse everybody.
You create a bunch of nonsense data.
You talk about controversy and we should take our time
and we don't want to be hasty.
You know, you just stall for time.
And this is what, you know, to varying degrees, you know,
like sure, is one org maybe marginally better
on some specific thing than some other org?
Sure, maybe whatever.
Is shell oil maybe marginally better than Exxon?
Sure, maybe, I don't know, right?
But like fundamentally,
what's happening is they're stalling for time.
I'll give you one point here
where I think the analogy might break down.
So if you want to sell tobacco or you want to sell oil
and you don't want to have to deal with the externalities
of what you're producing here
or the health risks of what you're producing,
what's motivating you is to earn money, right?
And I think the AGI corporations
are partly motivated by earning money,
but there's also something above that,
something kind of more ideological,
something about bringing in, you know,
creating a beautiful world for mankind, basically.
Do you buy that difference?
How do you think that kind of changes the incentives?
Exactly.
So this is what we also talked about in the compendium.
So we have a chapter on this
where we categorized kind of five groups of people
who are trying to build AGI for different reasons.
It is very important to understand
is that the race to AGI cannot be understood
in purely economical terms.
This is exactly correct, as you say.
If we look at this,
if we analyze this as like a hard-nosed Wall Street analyst,
it doesn't actually make any sense.
You know, I mean, it does right now
because there's a lot of hype,
but before this didn't really make any sense
as a bunch of other things that don't make sense here.
There is a very deep ideological,
even religious aspect to this.
So I split the actors into five groups.
And of course, there's a lot of overlap
between the groups is not a perfect split,
but is useful for us to think about this way.
The first are the utopists.
So these are people who think they can build utopia
which historically, as we know,
is always a really great sign when someone believes that.
So they believe that they must build AGI
because if they do it, and they're the good guys,
then they can use this to usher utopia in for humanity
and this will be great.
So this is people, groups such as Anthropic,
many people up in AI, your Ilya Sutskivers
and your Daryu Amadez, your effective altruists,
like lots of people in this camp fall into this group.
The second group is big tech.
So these are ruthless corporations
that just want power.
They're not sure if they buy the whole utopia thing
or not, some of them might, some of them might not,
but fundamentally they buy power.
They understand the concept of power
and they understand that AI means power
and they want more of it.
So these are the ones that are now driving
most of the actual resources of the race.
Originally, the race quote unquote
was just some ideological utopists
running small companies like DeepMind or OpenAI
and then, but we have now phase changed
to where all of these companies, these utopist companies
have been propped up by big tech,
Anthropic by Amazon, DeepMind by Google
and OpenAI by Microsoft.
So there is a strong symbiosis now
where these have like, I'm like merged into one entity
to a certain degree.
But I think big tech for the most part
is far less ideological and far more power driven,
more practical and also more dangerous for these reasons.
The third group is the accelerationists.
So there are some people, I mean,
who are basics libertarians,
they think technology is a moral good thing.
Like there's no way that technology can be bad
and therefore all technology must be good,
it must be built and anything that is against the building
of new technology must be bad.
And the fourth group is the zealots.
So this is a relatively small portion of people
but I think it's important to be aware that they do exist.
These are people who think,
not just think that humanity will be replaced by AI
but that this is a good thing.
So they want humanity to be replaced by AI
either because AI is like a superior species
or because humanity is evil and deserves to die or whatever.
There are some pretty prominent people in this camp
you might not expect like Larry Page,
the founder of Google who told Elon Musk,
when Elon Musk said,
hey, he doesn't want AI to eradicate humans,
Larry Page said he's being a speciest,
which is again, like that is such a crazy fact
that actually happened in reality.
If you saw this in a movie, you would be like, that's goofy.
That's not real, like that didn't actually happen.
But it did, it happened in real life.
And there's of course other people in this camp as well,
like Rich Sutton, a very famous AI scientist,
Schmittuber, another famous AI scientist,
but it's a relatively small camp overall.
Some schizophrenic people on Twitter, I guess.
And then the fifth group is just opportunists.
They come along whenever there's power and money to be had.
Before this, they were on blockchain,
now they're here and now they're allowing for the ride.
So they don't really have any particular loyalty
to AGI as a concept, they just want to make money
and gain power, which fair enough, I get it.
Hustle's a hustle.
So that's the landscape.
Which group are you most afraid of?
You mentioned the kind of combination of Big Tech
and utopians.
Is that the main driving force behind AI progress?
This is the current main driver, correct.
Before this, it was the utopists,
now Big Tech are taking over.
I also think Big Tech is kind of eating the utopists.
A classic Microsoft playbook of embrace, extinguish,
which is what we're kind of seeing,
where Microsoft is kind of in a slow motion divorce
from open AI, they're trying to kick open AI out
now that they have what they need.
That's interesting, I heard that open AI
was trying to get out of their Microsoft deal
by invoking their AGI achieved clause.
I haven't confirmed that, do you know about that?
I've heard about this, seems like a possible rumor,
I don't know, I think it goes both way.
I think there's some power struggle there
that we don't understand, who knows.
But this is a classic Microsoft strategy,
all the way back in the 90s,
I think it was one of their emails
that included a exact description of the strategy
where you would embrace a competitor
or a new technology, extend it, and then extinguish.
And so I think this is,
the thing is that Microsoft is right behind on AI,
kind of got the open AI tech that they needed,
that they wanted, and also acquired a bunch of talent
and so on to build their own AI efforts and so on.
So they're hoping to probably break from this
and continue the past.
Makes perfect said, honestly, you know,
props to the, you know, executives,
like it's some true house of cards shit,
good job guys, like impressive.
Is there a chance that this ends up being a good thing?
If maybe the big tech, the profit focus,
maybe they don't want to take as much risk,
maybe they don't really believe
that super angelicons is an actual achievable goal.
And so you get a more kind of,
the idealists or the utopians get eaten up
by more practical concerns
and launching a useful kind of chat bot product.
Right.
So this would be actually a good thing
if it happened in this regards.
Big tech is more evil, but it's more predictable,
which is a useful property to have
if you want to regulate or prevent, you know,
certain forms of externalities.
It would be good, actually,
if the utopians were not in charge.
I think the utopians are the biggest problem,
like because they have an ideology,
they have a burning ideology
that justifies anything basically,
as long as it allows them to get to HAI faster.
Because I mean, if you're,
this is always the problem with utopist ideologies,
is utopia is so good that it justifies anything.
You know, we saw it with like FTX
and like others like this.
This is always the problem with like consequentialists
or like utilitarian ideologies.
This is usually the failure mode that happens.
Not that all fall for this failure mode,
but this is a very common one.
Big tech and these like sociopathic corporations
have a different failure modes, which are predictable
and they're more competent in dangerous ways.
So that if AGI was so far off,
I think big tech taking over AI would be less dangerous.
The problem is if AGI is not far off.
If AGI is not far off, then,
then and big tech companies believe this, for example,
because utopists convince them of this,
then you're in big trouble.
Then you're in really, really big trouble.
And you are even more in trouble
if nations get involved.
So let's talk about entente.
So recently, there has been an emergent faction,
which is kind of basically the same faction.
I talked to it earlier.
This is anthropic, open philanthropy,
some parts of Rand, Rand Corp,
and a couple others in this kind of area.
We've recently been pushing
what they themselves have called the entente strategy.
The entente strategy is basically try to gaslight
and confuse and lie to the US government
that the US must race to AGI before China gets it.
And let's be very clear about what their intentions here are
is, of course, they need to build it.
They need to do it for the US.
And they have found a trick of how to deceive
or frighten US military apparatuses,
hopefully in order to get them to race to the utopian AGI
that they want to build.
This is an escalation.
This was not really a thing two years ago.
So this is a recent form of escalation
where the utopists have now gone
for their dream to build utopia.
They have now gone from, you know,
their own companies to big tech to going after the state
as like the next thing of how to escalate
their dream of utopia.
And this is extremely dangerous.
This is extremely dangerous.
And it's also wrong.
Like this is a very important thing to understand.
This is technically and strategically wrong.
It's not that this is true
that there is a race happening with China.
But what people are missing, but the problem is,
is that it's a race to the death, to the end.
It's a nuclear war.
You can't win a nuclear war.
Finally, so this strategy is called Entente,
as named this way by Dario and Rand.
Funnily enough, there is a historical parallel to this.
So in the 1980s, Ronald Reagan
announced his strategic defense initiative,
also called Star Wars,
which was a huge government project
to build anti-nuclear missile defense systems in space,
basically, and to massively ramp up
the American nuclear stockpile.
And the proponents of this strategy,
which was based on a lot of extremely controversial
scientists, science that was not well backed,
most of the scientific community came out against this idea
and thought it was completely unfeasible.
It was even dangerous
because it would provoke the Soviets
potentially into a nuclear war even faster and so on.
Because if they were to succeed
at building a perfect shield against nuclear weapons,
then the Soviets might be incentivized
to strike before the shield is complete
because, you know, use it or lose it.
So this could potentially cause the exact nuclear war
that they were warning from.
But even more crazy than this,
I didn't know this until I recently started
reading a bit of history about this,
is that proponents of SDI literally argued
that they could win a nuclear war and that we should.
They literally argued
that we build a perfect nuclear defense system
and then we nuke the Soviets.
And this was an actual thing that actual people suggested.
And then therefore we'd have perfect, you know,
American military hegemony forever.
And the thing that pushed against this
was a coalition of scientists, civil groups, et cetera,
called the detente.
And the detente basically argued
that, no, you can't win a nuclear war.
I mean, both you're provoking the Soviets
into nuclear war ahead of time,
which is the same thing that's happened with the AGI race.
If the Chinese actually think
that we are getting close to AGI
and that we will use this to, you know,
destroy the Chinese state,
but what do you think they're gonna do?
I don't think there's a peaceful solution here.
You know, it's gonna be ugly.
And the second thing is that you can't win a nuclear war.
So Carl Sagan actually proposed
or and like validated this theory of nuclear winter.
And this was one of the decisive killing blows against SDI.
As he showed, even if one were to nuke the Soviets
or whatever, you still get nuclear winter
and everyone loses.
So there is no winning strategy.
You can't win a nuclear war.
Everyone loses.
And the same thing is happening with AGI right now.
You can't win an AGI arms race.
The only winner is AGI.
What do you think they would say
in response to what you just said?
Why, so they must be aware that, for example,
if China begins to sense that the US is close to AGI
and their intelligence points to the US using that AGI
to basically control the future of the world.
The CEOs of the AGI corporations are aware
that China has nuclear weapons and so on.
What's their response to what you just said?
I mean, it depends on which one you asked.
I think most of them has literally never thought about this.
A lot of this is not well thought through to be clear.
Most of this is not well thought through.
Some of them think through it,
but most of this is just not well thought through.
Most of them is just like, well, we don't have a choice.
This is the main counter argument.
It's like, we don't have a choice.
If we race to AGI and we die, well, not our fault,
nothing we could have done.
There is a strong self-imposed nihilism
where just people decide that there is no third option.
They decide that, well, China's racing,
stopping them is ontologically impossible.
So therefore, racing is the only ontologically
acceptable solution.
And if racing kills us, then we are already dead.
And so there's no reason to even try.
What's interesting about the China counter argument
to AGI risk is that it's, in some sense,
the first thing that people go to
when you talk about trying to perhaps slow down,
perhaps pause the AGI race that's going on.
I mean, I don't think it's a bad argument.
I think there's actually something
that to be very aware of there.
It seems that the current kind of philosophy
of the Chinese government wouldn't be the right philosophy
to rule the world forever, right?
So we don't want that.
And so even though it's kind of the first argument
that people go to, I think it's still kind of a live argument.
Do you agree that it's a live argument?
Yes, but it's wrong.
Like it's live, but it's wrong.
It's just not true.
Like this is very important to understand.
The argument is false.
It's, the argument is invalid.
It doesn't apply to our reality.
The problem isn't that we know how to build a safe AGI.
And it's just a question of who gets to press the button.
That's not the problem.
The problem is AGI kills everybody.
And if you race to the bottom,
you build AGI as fast as possible.
There will be no safety.
There will be no alignment.
There will be no American AGI.
That's not what's going to happen.
There's just going to be unaligned AGI.
Like this is not a prisoner's dilemma.
This is important to understand.
This is not a prisoner's dilemma.
People pretend this is a prisoner's dilemma.
It's not.
In a prisoner's dilemma,
you always benefit from defection.
Like, so even if the other person's already defected,
if you defect, you get a better outcome.
This is not the case here.
If China raises to AGI, we die.
If we then also raise to AGI,
we just die marginally faster.
This doesn't help.
This doesn't, this does not improve the situation.
This is not, like this is a very deep thing.
This is a very important understand.
If you read, for example,
Leopold Aschenbrenner's making exactly this point,
he makes this long thing about how AGI is coming,
which is like nicely argued
and nicely put together and whatever.
And then he just kind of says in like one sentence,
for alignment, we'll muddle through.
Like that's his whole argument.
Like there's no argument there.
He's just like, well, whatever, we have to raise.
There's no justification.
Like if you actually read the thing,
he lets up all these arguments about AGI coming,
but actually doesn't make any argument
that justifies racing as the correct strategy.
He actually doesn't justify this
because he doesn't prove that alignment will get solved.
It's not there.
You can read it, it's not there.
There is no reason to believe
that if we raise to AGI,
that we will get safe AGI that does what we want.
In fact, there is overwhelming reasons
to believe that that was not what's going to happen.
So the whole premise is flawed.
It's just like, it's wrong.
One of the points you make in,
or you and your co-authors make in the companion
is that AI progress over the last decade, say,
have been driven mostly by more resources,
more data, more compute, more talent, more investment,
and not equally by kind of deep research breakthroughs.
Is this the cause of us not understanding AI
deeply enough to make it safe?
So is it because that we have grown these systems
as you write as opposed to building them
that we don't understand them
and therefore we don't know how to make them safe?
This is a huge problem related to this.
I think, so there's a strange thing that happened.
There's many strange things about AI.
The fundamental, one of the core strange things about AI
is that it's grown, not written.
It's not that you write line by line
the codes of what your AI does.
It's more like you have a huge pile of data
and you grow a program on top of this data
that solves your problem,
which is a very strange way to do programming,
but it works empirically.
So, but like we don't really understand these programs
that get generated.
They do strange things all the time
that we don't understand
or that we don't know how to control.
And we have no way of predicting
what they will be capable of or not
before they're made in various ways.
This is very curious because is this kind of different
from how capabilities tend to work in normal software?
In normal software, as you add more features
to your software, as you make it more powerful,
as you make your system more capable and keep more things,
you build up complexity.
Now, your program gets more and more complex
and it gets harder and harder to manage.
And usually eventually you reach some limit
of like what complexity you or your organization can handle.
And then your program just like, you know,
freezes in what it's capable of doing
or collapses and just becomes unmaintainable.
Now, this is the normal life cycle for software.
And then you have to like start over
or like break it into multiple parts or like,
or you're just stuck.
Like there are many companies, massive corporations,
whose whole job is just like, you know,
parasitically living off one huge unmaintainable code base
that like no one can like actually do things with.
Like this is a very common thing in software.
There's a, to give a bit of flavor to this.
There's a very funny story.
Our guy on Hacker News, which is like a news website,
talks about how he used to work at Oracle.
So Oracle is a legacy software company.
They build like huge Baroque software
for like Fortune 500 companies and stuff.
And one of the products they sell is the Oracle database,
which is just a database.
You know, it's not that different from other databases.
And he talks about how the code base
of the Oracle database is just one
of the worst things known to man.
It's like 20 million lines of just poorly documented,
complicated code that everything interacts with everything.
No one knows how it works.
It's all relies on literally thousands of flags
that just get turned off and all interact with each other
in ways that no one understands
and are not documented properly.
So whenever you need to change anything about the code
or add a new feature, you have to like,
you change a couple lines of code
and then you have to run literally millions of tests
on their huge cluster.
And this takes like days, like three days
to run all these tests.
And then you always break thousands of them
with every change you make.
So then you have to go through each and every one
of these tests that you break
and just like fiddle with all the flags
until you have like the right combination,
magic combination of like flags and like bug fixes
and like special cases or whatever that all the tests pass.
And you add like another flag
for your specific weird bizarre edge case
and then you submit the code
and it gets like merged, you know, months later.
This is a terrible way to build software.
Like this is just truly, truly a terrible way to build software.
Both pragmatically speaking,
most from a safety perspective,
like I can tell you with confidence
that the Oracle database contains horrific security flaws.
I can tell you this
because there's no way to get them out of there.
That thing is too complex.
There are some horrific security flaws in there
that I've not yet been found.
I am 100% certain of this
and there's no way for Oracle to get rid of them.
There is no way Oracle could take this code base
and pragmatically actually find all the bugs.
It just can't be done.
It's too hard.
It's too complex.
And so this is how most software is.
And the punchline is, of course,
is that AI is developed like this too, but worse.
Is that AI, you don't even have a code base.
You have a neural network.
There's no code for you to test.
And there's no tests.
Sometimes we run evals, but evals are not tests.
It's not like we take apart all the individual functions
of the AI and test all the individual functions
that they're correct.
No, it's all a big neural network.
So evals are just completely, you know, like trial and error,
brute force, and you just like see
if the numbers go up or down and they like look good.
This is a terrible way to design software.
Not just, but the crazy thing is that AI allows you
to build software in these terrible, terrible,
terrible ways while still making it very powerful.
And this is where it's different from traditional software.
The Oracle database, you can't add crazy new features,
at least not easily, because it's just too complex.
Like you just run into a bottleneck.
But GPD4, well, just throw more data at it.
You know, bro, just get more GPUs, bro.
Just add more complexity, more patterns.
Just put more in there. Who cares?
Like literally, who cares?
Just put more shit in there and it gets better.
So we have like this kind of like bizarre worst
of both worlds kind of situation
where the way AI software is built
is so fundamentally in a way that we can like,
like from a cybersecurity perspective,
you're just like, holy shit,
like there's no possible way to make this safe.
Like there's like no way you could have a system like this
that does not have bugs.
It can't be done.
Like it's like, if you think this, you're crazy.
And at the same time, you can make it extremely powerful
because in the past, you know,
software at least would like plateau in its power,
so to speak, because it would just get too complex
and it'll start breaking.
But AI, to a large degree, doesn't have this property.
So we can build extremely powerful systems
that can do extremely powerful things
while having just the most complex bugs known to man,
just bugs that are so hard to understand,
so hard to debug, so hard to even, you know,
fathom or find that's like impossible.
And the reason this is so important
is let's be very clear.
When people talk about stuff like,
oh, America will build good AGI that makes the good future.
Let's be very clear what they're saying here.
They're saying we're gonna solve all of moral philosophy,
all the problems are institutions,
our governments, our states, our militaries
are trying to solve all political problems,
all resource allocation problems, scientific problems,
interpersonal problems, social problems,
all of these problems using software
and there will be no bugs.
That's what they're saying.
That's insane.
I agree that there must be many bugs,
bugs different from how bugs would appear
in traditional software, but kind of a failure,
you could say in the weights of GPT-4, for example,
but these are not so consequential
as to prevent open AI from offering a useful product.
So what is it that's going to change in the future?
Open AI has offered this chat product
for a number of years now,
nothing has gone fantastically wrong yet.
So what is it you expect to change in the future,
such that the bugs, you could say,
in our models become more problematic?
To be clear, many things have gone fantastically wrong.
Like this is like, people keep saying this,
this is not true, like social media is now insane,
like even more so.
This is a massive externality on humanity
and it's epistemology.
This is a huge, huge blow to humanity.
This is a massive cost that every time I go on social media,
there is no way for me to know
whether this eloquent person responding to my arguments
is a person or not.
This is a massive cost
that was imposed unilaterally on humanity.
It is now much harder to build coalitions online.
It is much harder to understand.
Scams have become much more sophisticated,
much more mass-producible, deep fakes, political things,
all the kind of shit that's been happening
also like in third world countries,
we're like, genocides and so on
have been being pushed by these kind of things.
Let's be very, very clear here.
I agree that there is a difference
and AGI is a huge problem.
But the fact that AI has not had huge consequences
is just wrong.
There is a massive unpaid externality.
Our boomer parents are just driven insane
by social media and AI-generated slop.
And artists are being eviscerated alive.
All these various entertainment businesses
are falling apart.
There's human share tradition
of art and beauty is falling apart.
These are massive costs.
They're not deadly.
We're not going to die from them.
But they are huge.
They are huge, huge, huge, huge costs
that we are paying and that society is paying.
And there's no way to do it back.
So we could say it's worth it.
You can make the argument, sure,
maybe it supercharges propaganda
and scyops and marketing
and all these other manipulation stuff.
But that's fine because chatGPT is that good?
Fair.
That's an argument you could make.
But it's not free.
This was not free.
But to get back to your actual question,
so your actual question was what changes?
And yes, so I actually was talking to another AI researcher
about this recently who kind of pointed out that like,
well, you're concerned about like these like AGI
and like these like super intelligent risks and so on.
But like, you know, look at chatGPT,
it's really smart.
We don't have this problem.
And the thing is basically chatGPT is very smart
and like way smarter than people think it is.
And so are like Claude and like other AI systems,
but they're not literally human level
because like I can't just replace a senior engineer
or senior manager with chatGPT with no modifications yet.
So my argument is that there are some things
that are still missing.
There are some like, I can put a human
and you know, as a CEO of a company and it works.
You can't currently do that with chatGPT.
ChatGPT can help, it can do many of the tasks
that a CEO would have to do.
Many of the tasks a CEO do can now be automated
by a chatGPT, but it's not 100% yet.
Very importantly, I don't think intelligence
is something magical.
It's not a discrete property where you have
like some magic algorithm and you add the magic math
and then it becomes intelligent or otherwise it's not.
I think there is a very common thing
where people say it's not true intelligence.
It's not true planning
or it's not true, et cetera, blah, blah, blah.
We argue this in the compendium as well.
I think this is just very, this is pseudoscientific
and just like really bad reasoning
where intelligence is made of smaller parts.
Like whatever intelligence, you know,
general intelligence, human intelligence is,
it's made of a bunch of smaller tasks
and the CEO of a company.
You know, when I do something,
the way I run my company is composed of smaller tasks.
There are many, many smaller things I need to do.
I need to talk to people.
I need to reason about people's mental states.
I need to reason about the economy.
I need to reason about products.
I need to make decisions.
I need to communicate.
I need to write things.
But all of these things are tasks.
They can be understood.
They can be automated.
And chatGPT can do a lot of the tasks I do day to day.
They can't quite do all of them yet,
but I expect as we get to more and more general systems
with longer and longer memories and, you know,
coherency and so on, that slowly but surely,
eventually it will be all of them.
And once you have a system that is stable
and long-lasting and so enough
to completely replace a human,
you have a system that can do AI research.
You have a system that can improve AI further.
And you also have a system
that you can instantly copy onto millions of GPUs,
a system that never gets tired,
that never gets bored,
that has read every book ever written,
that runs, you know, tens or hundreds
or thousands of times faster
than any researcher in the world, you know?
So the moment you have a system, one system,
that is as smart as a human,
that has this kind of, you know,
goes from the 90% to 100% and to 110% or whatever,
once you have such a system,
you could instantly scale it up to 1,000 a million X,
and you immediately will have something
that is vastly smarter than humanity.
And it can improve itself.
They can get more power.
They can develop new technology.
And this will be a complete step change,
and this can happen extremely quickly.
So for me, AGI is defined as a system
that can do everything a human could out of computer.
Once we have one such system,
we will get ASI, artificial superintelligence,
which defines a system that is smarter than all of humanity
in short order, like I think six months or less.
Just because you would have a bunch of,
call them virtual researchers,
doing a bunch of additional AI research
and quickly making progress.
And so I'm guessing I take your point here to be
that the bugs that show up in these kind of agents
of the future that can reason,
that can plan, that can think long-term,
those bugs will be much more consequential
because now you have a bunch of virtual people spread out
in all institutions quite quickly.
Yeah, is that your explanation?
If I told you that OpenAI just announced
their virtual humans program,
where there are going to release 80 billion
virtual humans onto the net next week,
would you feel good about this?
Could we even guarantee they wouldn't immediately
declare war on us or like steal our resources
or who knows what they would do, right?
Like, and like, and it's gonna be worse than that.
These are not going to be humans.
These are going to be weird alien machines
that are, you know, have all these weird bugs
that we don't understand,
that can have extremely weird behavior
that are being built by for-profit corporations
to make them profit, to be clear.
Like, these are being built by sociopathic entities
to maximize sociopathic functions, profit,
and you will have massive amounts of them
that, you know, expand extremely rapidly,
that you can scale them up extremely quickly.
They can all, if one of them learns something,
you can immediately send it to all the copies.
Like, if one human figures out something
that doesn't mean humanity knows it with AI, it's different.
If one AI figures something out,
all AIs can know it in a second.
Like, even if the system itself is like,
not massively smarter than a human to start,
it will get all of these superpowers for free.
It will be able to copy itself.
It will be able to run at faster speed.
It will have access to, you know, vast memory banks.
It will never get bored.
It will never get tired.
It doesn't have emotions.
It doesn't have to, you know, all these things.
Like, if I just had like, a guy who works 24 seven,
is the smartest John Von Neumann, never sleeps,
never gets tired and never gets distracted.
This is already like, by far the smartest thing on the planet.
You could even trade off speed for kind of raw intelligence,
where if a machine or an AI could work incredibly quickly,
that could make up for it being kind of dumber
than us in a sense.
I would expect speed to also be a very important factor.
And as you mentioned, not getting tired,
not getting distracted.
I mean, when we try as humans to achieve a task, right,
there's a lot of kind of superfluous activity and anxiety
and all kinds of things that distract us.
But these models would not kind of face those same hurdles.
And so they would probably go very directly
and very ruthlessly to the goal they have in mind.
One question here is, so when I was 18,
I considered taking a driver's license.
And I decided against it because I saw all of the progress
that was being made in self-driving cars.
This is a while ago now.
And so there I concluded that we seem
to be so close to having kind of full self-driving.
And you could say, OK, we seem to be getting quite close
to human level in these chatbot models we have right now.
If we can just fix things or perhaps unharble the models
or add some other features, make them talk to themselves
and so on, we can get to agents.
Is there a similar problem to self-driving
where getting close to being reliable enough to be useful
is useless in a sense, where getting that last percent
or so of reliability makes the entire difference
of whether you can deploy an agent
usefully in the workforce?
I think there are some of this, but you can also
think of things differently.
If you're on an exponential, and let's say you've
been on this exponential, let's say it doubles every two years,
let's say just hypothetically.
Now, let's say you've been on this exponential for 100 years
or something, and it's gone from zero to three or something
over this time.
You can pick an exponential or whatever
that started low enough that it moved slow enough
to get to this.
And I would say, wow, we spent 100 years
and we only got three units of progress.
So if we want to get to 100 units of progress,
this is going to take literally 3,000 years.
And this would be a reasonable argument to be made, right?
But the true answer is, is that it will go 3, 6, 12, 24, 48,
and you're 100 in 60 years or 12 years later.
Although this is for the raw intelligence of the models,
I'm guessing, right, where we see this kind of progress.
But do we have good measures of agency in AI models?
This is the important thing, right?
Agency.
What does that mean?
Exactly.
It doesn't mean anything.
It's pseudo-scientific.
It's not a real concept.
It doesn't defer to anything in reality.
It's just a word that people use that come up with like,
whether it's agency or planning or consciousness or something,
these don't refer to actual physical or algorithmic
properties.
Some people use them to refer to specific things.
And if you're referring to a specific algorithmic property,
great, happy to talk about that.
But I expect it's more of a vibe.
Yeah, maybe it's a vibe, but we can put that word to the side
and then talk about, perhaps, ability to achieve a goal
over a longer time horizon, for example.
Yeah, and we're getting exponentially better at that.
Yeah.
Yeah, maybe we are.
Maybe we are.
So your point is whenever we try to kind of specify what we mean
by agency, we see that it dissolves into something measurable
and that which is measurable, we're improving at.
Exactly.
This is my fundamental thesis or intelligence,
is that if you actually go very, very deep into any of these concepts
that seem to be missing, they dissolve.
They are fundamentally, it's all just information processing,
it's all just patterns.
It's all just smaller tasks.
And we are getting better at all of them.
Like some of them more than others and so on.
And some of them are harder than others and they will take a bit longer.
But if you're on an exponential, it doesn't matter.
50% of the progress happens in the last unit of time.
If you're at 50% and they're like, wow, this is going to take quite a while.
We're only 50% of the way there.
You're wrong.
It's only one unit of time away.
So if we're at 50% of hypothetical last mile driving,
well, it means it only going to take one more year or two more years to fix.
Not saying this is literally what's going to happen,
but this is the kinds of ways you should be thinking about this.
And also with self-driving cars specifically,
there's a bunch of other stuff like regulation that stuff.
Like Waymo does have self-driving cars and they do work.
They do exist.
Specific cities though, right?
Yeah, but San Francisco is a terrible city to drive in.
Have you driven in San Francisco before?
That's not an easy city to drive in.
So I agree Phoenix is an easy city to drive in,
but San Francisco is not an easy city to drive in.
So sure, we can argue, oh, it's still not perfect.
It still makes mistakes.
Sure, of course, whatever, right?
But I'm just like, also they have like what, like a GPU in there or two, whatever, right?
It's like, I expect if you, and like what we're also seeing right now with like robotics
is we're seeing this massive revolution in foundation models for robotics.
We're like, I remember two or three years ago, people were like robotics, impossible.
Deep learning, no, can't do it, right?
And now we just like take language models, train them on data
and then ask them to do things and they do.
Like if anyone is not, just like go to the DeepMind blog
and just like look at like the last like three papers they published on like robotics
and just like have your mind blown.
It's just like literally like lol, we asked the robot to do something and it did.
Yeah, it's actually amazing to look into this.
I wouldn't have expected it to work out this way in which you have a vision model
looking at a scene, writing down what it sees.
You can then kind of give instructions in written language,
in kind of natural language, English and say, you know, pick up the Apple.
Of course, there's a bunch of very advanced stuff going on underneath that,
but it's just interesting that it could be that simple in a sense.
Okay, in the compendium you call for an actual science of intelligence.
And I think this is what we're kind of debating or talking about right now.
What we're missing is a developed science of what intelligence is, how it works,
how it can be grown.
What do you think such a science would look like?
I obviously don't know.
This is an extremely hard question.
I have some suspicions.
I have a suspicion that it will look more like computational learning theory,
less like computational complexity theory, but also lots of computational complexity theory.
That difference, you got to explain that difference.
Do I?
Okay, I'll say a couple interesting things, and then I won't go into it too much
because it just gets out of stuff.
It's just like, this is something.
There is something wrong with p-space.
And so what I mean by this is that everyone knows about p versus np.
It's like there are problems that are in a class called p, polynomial,
polynomial time, p-time.
And there's another one called non-deterministic, polynomial time.
And it seems like these two should be different, but we have proved this.
Everyone knows this very famous example in computer science, right?
But a lot of people don't know.
We also can prove that there's between p-time and p-space, which is insane.
p-space means you have polynomial amounts of memory, but infinite time.
And we can't prove that there are things that are computable in p-space
that might also be computable.
And like p-time, this is insane.
What this tells me is we got something deeply wrong about computation.
Something about how we think about math, computation, intelligence is wrong.
I don't know what it is.
There's like something about like the unit of work.
Like what is a unit of math?
If I do one math, what is that unit?
So there's a little bit of this in what's called algorithmic information theory.
In algorithmic information theory, I think it was algorithmic information theory.
I'm actually getting that wrong.
The comments will yell at me if I got this wrong.
Where you can prove that for a sorting algorithm, there is a minimum complexity of the program.
Like there's no way to make a sorting algorithm that is less than like n log n or something like this.
This is like the only result that I'm aware of of this type.
We can't prove this for almost anything else.
But intuitively, what a science of intelligence would look like
would be something like to be able to drive a car, you must be at least this intelligent.
This is what it would look like.
And the closest thing we have to this is algorithmic information theory,
which is like, which has proved this thing of like,
if you want to sort something, your algorithm must be at least this complex.
This is the kind of things you were looking for.
I don't think we can even do this because I think our math is wrong.
Like there's just like something super deeply wrong about how we're thinking about mathematics
and how we're thinking about computation that we are just super confused about intelligence.
That like intelligence is like super, super hard,
but we're already getting like computation wrong.
And I expect once we're less confused over what computation is
and what learning is and these kinds of stuff,
then intelligence will become more and more clear what it is
and how to think about it and how to formalize a statement such as
in order to drive a car, you must be at least this smart.
I think we are so far away from this.
This would take like generations of mathematicians and like computer scientists.
I think this is like probably like as hard or harder than P versus NP to get right.
I do think we can make a lot of progress on like empirical or like, you know, approximate theories
that like are not really true, but I also think that's very, very hard.
And it's one that's like a surprisingly small number of people are working on to my understanding.
How would you categorize scaling laws?
Would that be kind of like an engineering of intelligence
and not really a science of intelligence?
I think it's more like alchemy.
Alchemy of intelligence, okay.
So we're not even at the stage where we have like the engineering textbooks
and some kind of like equations for how our models behave.
Like the scaling laws are much closer to naturalism.
Like it's more like Victorians looking at bugs and finding that there's like a symmetry between different bug sizes.
You know, it's like there's a pattern there, right?
And like it might be an interesting pattern, might not be an interesting pattern.
But like the way that like, say, Victorian naturalist categorized animals into species turned out to be like super wrong
once we figured our genetics, right?
And we find out that there's a deeper pattern in nature.
Then, you know, they didn't get everything wrong when they said like, wow, lions and tigers sure look similar.
They kind of look like cats, probably related.
Yeah, they were right, right?
Like I'm not saying naturalists were stupid, you know, or anything.
Same way, I'm not saying that someone who like does empirical work on AI is stupid.
I'm saying I'm just saying like there are deeper patterns that you can't find through naturalism.
No, no, I mean, the scaling laws are kind of a great insight.
It's just a question of what do they mean in practice, right?
What is it that you can predict?
It's not exact capabilities.
It's not you can't predict anything.
Basically, you can predict a specific like approximate number, which is a loss.
But how the loss relates to any capabilities we actually care about like the like you need to this loss to be able to drive a car,
which would be the kind of result we actually want to have.
It's just not even the same universe.
Research into scaling laws will not get us to an answer there.
You would have to go at this from a completely different perspective.
We have to come from a computational learning or like an algorithmic information theory perspective.
If we want to answer or like informalize a question like this and we are like like like scaling laws aren't even trying to do this.
This is an important thing to understand is not that they're trying to do this and they're failing.
They're not even trying to do this.
It just has a different goal that the goal of scaling laws is, well, a create hype and raise money and to empirically allow you to, you know,
predict if I put in this amount of money or effort or whatever, like how should I split my resources to get like all things equal best outcome,
which is a fair thing.
Like that's what a lot of alchemists did.
Like if you read alchemical texts, you know, a lot of the alchemical texts would be like, oh, if you want sulfur,
here's the things you mix and then it makes sulfur and like careful don't heat it too hot or it will be bad stuff like this.
We talked earlier about how it could be a good thing if big tech takes over from the utopian kind of founders of the AGI corporations.
If timelines were were long, you don't seem to believe that you're worried that timelines might be quite short.
Does this have to do with specifically kind of agents helping us do AI research?
Or do you expect is that a crucial piece of timelines being short for you?
It's neither necessary nor sufficient, but it is a part of it.
Like it is it is a part of why I expect things to happen.
But there are worlds in which that's not the crucial piece.
Like for me, like I am a stupid ape, you know, I have little neurons, so I try to keep my model simple.
I look at intelligence, I look at what it's made of, it's made of all these little parts, all these little parts systems keep getting better at.
I look around the world, more and more jobs get automated by these things, you know, they talk more and more better and I'm like, well.
Talk to me about automation, because when I interview economists, one thing they tend to agree on
is that this automation, if it's happening, it's not showing up in the official numbers yet.
So when you see you see automation, what is it that you see?
Here's an example of something that doesn't show up in GDP numbers, Wikipedia.
Wikipedia is one of the greatest triumphs of humanity ever.
It's one of the most valuable things ever created by humanity, period.
Like just Wikipedia is like more valuable than like almost any other artifact ever created by man.
It is kind of on the short list of things that you would take with you if you were to travel into space and could only take, yeah, something like that.
You show me like a massive like dam or a huge building or whatever and it costs like 10 billion dollars, 100 billion dollars to build or whatever.
And like that's much less than Wikipedia is worth.
Like Wikipedia is like obviously worth more than this one building.
Like from a pure value, Wikipedia contributes zero to GDP.
It is not part of the GDP measure.
It does not show up.
It does not change things.
As far as economists are concerned, nothing of value has been created here.
I think a lot of people confuse what GDP measures and like what these are like total factor productivity and so on.
What they actually measure is I'm not saying they're bad measurements.
Again, naturalism.
I think a lot of I think a lot of economics is naturalism and that's fine.
I'm not saying this as a critique.
I'm saying if you know what you're measuring and you know why it's measuring and it's and like it's limitations and like in
what scenarios it breaks down, that's fine.
It's very useful to measure GDP.
GDP is a very, very useful number to know and helps you reason about a lot of very useful things.
But you also have to understand that it like is not reality.
Like it's it is a very, very flawed metric that doesn't measure everything.
Like, like, OK, like think of the amount of cognition of like writing that.
ChatGPT has performed since inception.
And let's say you put an hourly wage of a human on that number.
How much do you think that would affect GDP?
Do you think it would affect it a lot?
Yeah, I mean, just just looking at my own usage of these models, right?
I it's a lot.
It would probably like destroy the economy 10 times over, right?
It would probably be like as big or larger than the entire economy.
If you had the actual humans typing out those numbers.
Of course, economists will say, that's not what we mean by that.
Like, of course, it reduces the price.
But I'm like, all right, but then you see how this is misleading.
So you're saying if I add more writing labor than the entire economy to an economy, it doesn't change.
And I'm saying, OK, you can define your metric that way.
But that's not necessarily intuitive to people.
That's how the metric work.
But it is how the metric works.
If I invent something that adds 10,000 times more of a certain type of labor,
but makes the labor extremely cheap, GDP doesn't change.
And so this is very unintuitive to people.
And so I think this is like or like the same thing with like the slop, right?
The negative externalities of slop like AI, you know, garbage and misinformation is very high.
But it's not measured in GDP.
If I like measure the amount of like distraction or confusion or annoyance or whatever,
there's like actually created if I measured it in hours or minutes
and then converted that to like working hours, attention, etc.
Qualies, you know, I expect the impact would be humongous.
It would be absolutely massive, but it's not measured.
It's not part of GDP.
So from the economist perspective, nothing changed.
That's GDP.
What about employment?
We aren't seeing kind of a massive rise in unemployment numbers either.
So this kind of points us in the direction of perhaps there's some automation happening,
but people are finding other other jobs or they're doing other tasks to make up for it.
Do you think do you think that's true?
Or do you think there's also something wrong in the way we're measuring employment?
I think employment is a good example of a number which is like relatively
like relatively intuitive, what it measured.
Like it's like GDP, I feel like specifically like quite an unintuitive thing.
If you haven't thought about it, like GDP is optimized to measure things that don't change.
And this is very unintuitive to people.
Like people think if a great change happens, GDP must change.
But it's actually the opposite of what GDP does.
GDP changes little when you have big changes in like prices and like things and so on.
Yeah, I mean, I know you just made this point, but we can we can think about
the availability of just entertainment or movies or something, which has increased kind of
Yeah, a lot since before the incident, right?
But this isn't really showing up because you're paying a Netflix subscription now.
Perhaps this is even decreasing GDP numbers because you're not renting as many
movies at Blockbuster as you as your parents did maybe.
Exactly. It's another great example of this effect.
Employment is a bit stricter, not quite.
There's some problems there as well, but like it's still it's much, much clearer.
So I think there is a there is a real thing there.
I mean, one thing I think is that jobs are sticky in the economic sense is that jobs
are not liquid in the same way that like many other things in the market are.
So it just like literally takes time.
Jobs have a lot of transaction costs, a lot of frictions.
There's a lot of social aspects to it as well.
And the AI is just not quite AGI yet.
I expect if we if we even we froze AI as it is today and we wait 50 years,
I think things would be different.
I think if people integrate these things more, you know, new companies get created
who don't have like this like baggage of like previous companies.
Another thing is, is that a lot of employment is not about skills.
It's not even about the labor.
It's about responsibility.
I'm not saying that like lawyers or doctors don't do skilled labor.
They also do skilled labor.
But one of the reasons they get hot paid high salaries is they're responsible.
You're paying them to be at fault if something goes wrong.
This is a lot of what we pay people to do.
We pay people lots of money often to be responsible, to be the part of a chain
which owns something.
Currently, or at least in our legal systems, AIs cannot do this.
This is a type of labor that AI cannot perform, not because they're
intellectually capable of doing it, but because the social contract does not
allow machines or AIs to be responsible for things or to own things.
I expect if we had a different culture that, for example, does allow AIs
to own or be responsible for things, it would be very different.
Yeah, and perhaps this is why we could see lawyers getting rewarded
for AI progress, like earning much more money, at least in the short term,
because now they can write whatever 100 legal briefs or 100 legal
documents in the time that they previously could write one.
Yeah, I agree with this.
This is actually a hot take I have as well.
I think paralegals are really screwed because I think paralegals mostly
do skilled labor, and I think skilled labor is going to zero.
But it's going to zero, okay.
Yeah, it's going to zero, obviously.
And the thing with skilled labor is that lawyers do skilled labor, but one
of the main things they do is that they provide authority.
They provide responsibility and they take responsibility for things.
And if the, you know, commoditize your compliment, so if you, if the bottleneck
to value becomes authority or responsibility and you have infinite
skilled labor, the scarce resource of responsibility can become extremely valuable.
Yeah.
I mean, this is something that perhaps could be the case for other industries as
well.
I have heard people talking about entry level programmers also being negatively
affected by AI progress because, you know, you can now have a model, produce
a draft of a code that's pretty good.
And so you have to senior programmer review it again, as you talked about,
with enough authority to approve and actually change the code.
Do you think this is kind of like, this is an effect you see, we're going to
see across the economy that entry level jobs are facing a lot of competition
from, from their models.
My prediction is that skilled labor that can be performed on a computer is
under threat.
Things that are based on authority or responsibility will at least in the
short term benefit and will become more valuable rather than less valuable.
And I expect all of this will be completely relevant once AGI raise.
Yeah.
When we are talking about jobs, I think one prediction I've heard is that we
will move into more kind of social jobs or more column, perhaps pseudo jobs.
So perhaps we could take my job as an example, right?
If, if, if you, you talk to my farmer ancestors, they wouldn't consider
this an actual job, but this is more of a social function or something that you
might do for fun or something.
If we have setting aside issues of existential risk from AI, if we have a,
a, an economy that is, that is, that is automated to a high and high
degree, could we end up in roles where we, we function based on our relationships
to other people, based on kind of historical factors, based on being the
owners of, of various entities and, and assets and so on.
So unfortunately, the question you just asked is just nonsensical.
If we ignore X risk, like there is no way to answer this question without
thinking about AGI.
Otherwise you're just not talking about actual reality.
So you could ask the question, for example, assuming we align AGI, what
will I be doing or like, will I have a job or will something else happen?
This is a question you could ask, not saying it is the question you did ask,
but like, this is like a question you could ask.
I'm not saying I have a good answer to it, but like, a big question, but like
saying like, assume AGI doesn't happen, will there be jobs or whatever.
It's just like, that's not how the world works.
Like this is not what's going to happen.
Yeah, you're saying we can't get to extremely advanced AI without facing
the problems of not having solved the alignment issue.
Yes, like, definitely.
Like the alignment problem is how do you connect causally, your desires,
values, wishes, whatever, to what actually happens in reality.
And if reality is mediated through the power of an AGI or an ASI system,
which it will by definition, once an ASI system exists, the causal control of
the future lies in the ASI.
Humanity is no longer relevant.
So the only way that humanity still has a causal connection to how the future goes
is if we have a causal connection from our values through the ASI to the future.
Like it's the big filter, you know, it's the big filter.
It's like either your human values get through the ASI or they don't.
If they don't get through the ASI does whatever, whatever it does, right?
So if you have solved the problem of how to get your human values causally
through an ASI to make it act in ways to instantiate the things we wanted to
instantiate in the future, you have solved the alignment problem.
That is the alignment problem.
So there's no way to get around this.
And in the Compendium, you write about how this is humanity's most complex
technical challenge, or at least it's one of the most complex technical problems
we face. Why is it so complex?
Why is it that we can't take some of the some of the advances that open AI
has made in, for example, making the model, making a chat GBT talk in a nicer
way to you, not using certain words and so on?
Why isn't that progress towards alignment?
Why isn't this an incremental problem and at a problem that we are making
kind of steady progress towards?
So there's a couple of questions there that are things worth kind of
disentangling a little bit.
Like one, one question is like, why is alignment hard?
Another question is how hard is it?
Another question is how expensive is it, which is a related but different question.
Another question is, is the things that are being done progress towards AJ?
Another question is, why isn't alignment an iterative problem?
Is it an iterative problem?
Yeah, those are those are actually seven different problems.
I'm going to reshuffle these questions a bit in the ordering and answer them in
a slightly different order.
So I'm going to start by answering the iterative question.
Is alignment iterative question a problem?
And if not, why not?
Like most things are iterative.
Why is this not?
So alignment could be an iterative problem.
There is a way to solve alignment iteratively, but it sure as fuck doesn't
look like let's build the biggest model we can as fast as possible before
China doesn't just yeet it onto the world.
That's not how you iterate.
So if it was that every single time a new language model or a new AI system is
built, everyone's like, all right, guys, shut down the GPUs until we've understood
every single neuron until we have a formal theory of how everything works until
we've unsolved all the bugs.
And then after we've understood every single part, then we build the next AI.
Okay, fair enough.
Yeah, that would work.
Like I think that would work.
I'm not saying this is realistic, but this is like, that would be an example of
how to address alignment iteratively would be to like actually take small steps.
What's happening right now is not that we are taking small steps.
So when people talk about iterative alignment, they are, it's a misnomer.
It's like, it's just not true.
Is there not talking about people take the small step possible that is safe?
What I'm saying is we'll take the largest step possible that I think we can get
away with and then take them as often as fast as possible.
And this is not a good outcome because again, ASI is the filter.
If you build ASI and it is not controlled, it's game over.
Simple as that.
Humanity has no longer any causal effect on the future.
You know, whether we die immediately or we hang around for a little bit, who knows?
But fundamentally humanity just has nothing to say anymore.
We're just like, you know, chimps in a zoo or whatever, right?
Like there's nothing for us to do and probably the AI stops feeding us.
So it's over.
So when you are working on a technology, which could blow you up, you know,
if you want to actually succeed at that, you have to be very, very sure that
every experiment you do will not blow you up.
Otherwise you blow up.
It's that simple, right?
It's literally that simple.
If you are making explosives, you have to be very sure that every step
of your process doesn't blow up.
Otherwise you don't make explosives, you die.
Is that easy?
So that's why it's not an iterative problem.
It's because not because it couldn't be an iterative problem.
It's because people are deciding to approach it.
Like how can I mix chemicals as fast as possible to get to the biggest
explosive I can?
If you do this, you die.
It's not because this couldn't hypothetically work.
You can actually make explosives safely.
This is a thing that can't be done, but it's not done by doing it as fast
as possible for China gets the explosives.
That's just not how the world works.
Right.
There is some work that is being done on, you know, AI safety and, you know,
allegedly alignment, like getting chapas to say what we want.
And this is all like cute, but it's alchemy.
Like if you look at the actual things being done, it's not science.
It's not engineering.
Let me give you an example of something that is like closer to real
alignment work, fixing a bug in the C++ compiler.
This is much closer to alignment than most of the work that is being
done at OpenAI.
The reason is, is that when you're, is that, you know, when you're
building a compiler, you are trying to build a system that
causally connects the will of the user to the will of a machine.
This is literally what you're doing, right?
You're literally building a thing where humans speak a language and like
confer a will that is then absorbed by the machine and then causes the
machine to causally do in reality the thing the human asked for.
This is much closer to actual alignment than most of the work that is being
done at these labs, hot take.
And most of the work at these labs is of the form.
My AI does something I don't like.
I hit it with a stick until it stops doing that.
And then once it looks like it's not doing it anymore, I give myself a
pat on the back and publish a paper about it.
This is cute.
It, I think it makes a better product.
Like to be clear, I think I use chat GPT as Claude.
I mean, you know, Claude's better than chat GPT.
Claude gang rise up, but, and they're good products, right?
Like, you know, Claude is stupid sometimes or makes mistakes or says
things I don't like or something or whatever.
But like, who cares, right?
Like whatever, like, I don't need it to be 100% to like provide me value.
Fine.
If we build a system with a level of reliability, Claude has, and we give
it causal, you know, physical access to reality and super intelligence.
That's not fine.
The much, much higher risk scenario.
I know you can have a bit of a stupid, funny, goofy thing.
If it's an entertainment product or like, you know, there's something like this.
You can't have the same level of reliability.
And we're talking about nuclear control when we're talking about, you know,
government or control military, like, you know, real world things like this,
which is what ASI will be ASI will be systems.
So the level of reliability you need when you're dealing with an ASI scenario,
it's just exponentially higher than the reliability.
You need normal systems and it's kind of like the rocket problem.
We're like, to get to the moon, you don't build bigger and bigger ladders
that will never get you to the moon.
It's like structurally, a ladder cannot get you to the moon.
It can't be done.
You need to invent a rocket and a rocket is a way more complicated and we
were confusing thing.
So if you went back to, you know, the middle ages and you told people, oh,
in the future went to the moon and they said, oh, we did do using a special ladder.
They'll be like, all right, yep, seems reasonable, I guess.
Like maybe they have some like special metal in the future that's like
indestructible.
So like, yeah, okay, seems like that could be possible.
If you said we did the rocket, they'll be like, what are you talking about?
That word doesn't even exist in our language.
You know, like, I don't even know what that means.
You know, okay, some people in the middle ages, you know, that wouldn't
be well done powder, but like most people, like that word doesn't translate.
I don't know what you're talking about.
The same thing applies to AI is that most of what we're doing right now is
building ladders.
We're hitting it with sticks, you know, to be clear, bigger and bigger sticks,
you know, and like, you know, big sticks have, you know, economic value.
But this will not structurally get you to a point where you actually
understand the internal cognition of these systems, and they're actually
safe at a level that you wouldn't trust them with like say, ASI level control
over reality.
And so the difference, the hurdle we can't get over is that we can't
provide good enough feedback to, to more and more advanced model.
Is that the main, the main issue that?
That is another issue.
Like this is where the thing defeats itself in its own theory.
So like, I think it fails even before then to be clear.
Like, I think it's structurally is kind of nonsensical.
So with the most common form of these like iterative alignment proposals
that people make is something like we, if we give the machines the right
feedback, the things are good or bad, we can steer them towards the
directions that we want.
I already reject this premise.
This is already not true.
Have you heard of the concept of lying?
Like, I'm not sure if these people have just like never met another human.
Like, have you, like, oh, if I just give a human the right feedback, they
will do an arbitrary thing.
I asked them for and never deceive me.
Like, have you met another intelligent, even a like a chimp, a dog?
Dogs know how to lie, you know, if they can get more treats out of you.
If you just give them the right feedback, yeah, the, like, they'll
just do what gives them more treats, you know, even, you know, maybe, maybe,
maybe your wife got up earlier and fed the dog.
And then he's going to ask you for even more food when you wake up.
You know, it's like, is the dog unaligned?
I mean, it's like, there's a deep thing where I already reject this premise.
It's just like feedback by itself is not a sufficient mechanism
to guarantee the kind of high level of reliability that you require.
I think it is enough for like, you know, dog level alignment, you know,
maybe, you know, probably not even that.
But like, you know, for charity, BT level alignment, right?
Like it says the nice thing most of the time is mostly fine, you know,
whatever, right?
I think for that, yeah, it's fine, but this is just not sufficient.
So the second thing is, is that there's a, there's a, there's the, the
second premise is that this is sufficient for an ASI.
Is that if we have an ASI that has ASI, that's fine.
And this is just like, obviously blatantly not true to me.
Like you, what are you talking about?
This is a much more complex scenario where it's much more on the line.
It's much more complex.
It has way more edge cases that is like, it's like the thing I was saying earlier.
You're saying you want to solve all of the problems that all of our
institutions throughout all of history have tried to solve.
You want to solve all of moral philosophy, all of economics, all of science.
You know, you want to solve all these problems using software with
that level of bugs.
And I'm like, no, you're not like, that's not how reality works.
If you are trying to solve a problem that is that hard, like you need a much,
like unbelievably higher level of reliability.
And this is like, this is one of my core things is that I think people
just like think ASI is just going to be like chat GPT, but a bit clever.
And this is just not what we're talking about.
We're talking more like, imagine the US government was sentient and like smarter
than everyone on the planet.
Like that's what we're talking about.
That's what an ASI would look like.
Yeah, it's not a more advanced chatbot.
It's a much more advanced kind of system that can solve tasks in many domains
and many more domains than our chatbots today.
Like imagine if the entire US government, like every single person in the
US government was out to get you, you know, for our schizophrenic listeners,
they're not out to get you.
Don't worry.
This is just hypothetical.
You're safe.
But like imagine for a second, the entire US government was out to get you
and screw you over it every way possible.
Could you defend against this?
No, you couldn't.
You'd be completely overwhelmed.
Like you'd be, they've come left, right, all over, like from all possible angles.
You would be outmatched, outclassed.
You would be tricked.
You'd be deceived.
You'd be brutalized.
Like there would be no possible defense against a thing like this, right?
It would just, and like even now, right?
Like we have US government, right?
Which is like so sort of aligned sometimes, right?
But still, it's just like heinously evil things for no good reason.
You know, like recently there's that story about the, like, you know, you know,
squirrel that got killed or whatever, right?
And like, I don't think anyone really wanted to kill the squirrel.
I don't think any individual person was like, haha, I sure love killing
lovely little squirrels, but it happened anyways because the system's a fucking
mess because it had bugs because it's misaligned.
It's, it's like, like the US government does stupid things that are not
in its interest and hurt its citizens constantly, not because they're evil.
Like, you know, sure, there's some evil people involved, but like, it's mostly
just bugs.
It's mostly just the system is buggy.
It's just there's many things there that are just like stupid and poorly designed
and don't do what they're supposed to do.
Now imagine that, but like sentient and a billion times smarter.
Like that, that's a problem.
If when we are thinking about AGI or superintelligence, we shouldn't imagine
chat GPT that outputs kind of a genius level text for us.
What is it that we should imagine kind of your mainline scenario?
What, what do you envision happening here?
Well, what, what is the, the next step might be agents, but after that, what
comes, what comes next?
There's at least two, two questions here and two questions I see here are what
will happen or like, what will an AGI, how will it work?
And the second is what will we see?
I think those are very different questions because I think we will see or not
what will be what happened, what actually happens.
Like no, I expect that AGI takeoff will be so complex.
It will be so distributed.
It will be so confusing that no actual individual person will actually see
everything that is happening and understand what they're seeing.
I expect that for us, when AGI takeoff happens, it will mostly be very confusing.
It's just a lot of weird shit happens that we just can't quite explain.
And it's like weird behaviors.
So should media gets confusing, like a lot of like weird stuff, like
political things seem happening and like market does some crazy, like weird things
or not, and just like a lot of really in like some new technology starts popping
up in some like weird areas, we don't really know what made it or whatever.
And then eventually just it's just like most people won't even realize
anything's wrong until just like one day or just like stop existing or, you know,
something like this is what my main line of what I expect will happen is that
things will just get confusing.
They will get more complex.
It will get things will move so quickly and things will change so quickly.
Like, like, do you know, like what is happening in Ukraine right now?
Not, not to any level of detail now.
And it's worth there is no method by which you could acquire your actual
knowledge of what's really happening in Ukraine.
There's no way.
Like it's too complex.
There's too much going on.
The signals are too conflicting.
There is too much propaganda.
There's like no method by which you could reliably gain actual true
information of what is truly happening there.
Actually, my best guess at how to acquire true information is to read the
Wikipedia article.
That is actually what I do.
And that is my attempt, right?
And I'm certain the comments will tell us all about how terrible of an idea that is
and all the problems that has.
But I agree.
I think Wikipedia is genuinely one of our greatest epistemological victories.
I also trust Wikipedia more than I trust most source, not that Wikipedia is perfect.
I'm well aware of many shortcomings, but that also shows us what our level of
epistemic defenses are.
If you're actually dealing with a coordinated adversary that is like extremely
smart, smarter than you, you know, who's distributed to you, it can just make
you believe anything you want.
It wants, right?
So I expect HGI takeoff to mostly be confusing.
I don't think it's going to be like epic or even necessarily scary.
I think it will mostly be just very confusing and just really weird and people
will be upset and there will be shit happening, but like no one will be sure
what's happening or who's happening or like whatever.
And, you know, maybe it'll be a couple of people who get it right by coincidence,
who by coincidence be screaming into the void, what's happening.
And like they will be drowned out by all the other bullshit happening.
And then so just like no one will be able to coordinate.
No one will be able to figure out what's happening until it's too late.
This is my main line prediction of what will happen.
Yeah, that's, that's quite depressing.
That's, it's, I mean, if, if we're all confused, then we won't be able to even
kind of agree on, on what's going on or how to respond to it.
There isn't really anything to respond to because we, we don't have, we don't
have kind of like a shared understanding of what's happening.
This is exactly what's happening.
I mean, right now, like, you know, the U.S. is a day before the election.
I have people saying things on my, you know, Twitter feed or whatever, like good
normal saying people where I'm just like, these words are just not connected to
reality.
This is mental illness.
Like not even just to disagree with them, like politically, to be clear, like,
like just like whether or not this is true, this is an insane thing to say.
Like this is just like, this is not a thing of normal.
Like this is what a person having a mental health crisis would be saying.
Like the emotions being expressed here.
The words being said here, how they're being said, the viciousness, the fighting.
It's like, this is how people in a mental health crisis is act.
This is how schizophrenic people act.
And this is big problem.
This is pre-AGI.
Yeah.
Just, just to push back on this point, we can, we can talk about our
epistemic crisis and we are overwhelmed by information and so on.
On the other hand, we also have much, much more information that we had 50 years
ago, 100 years ago, 200 years ago.
And, you know, they were able to somehow kind of navigate the world.
I think more information has helped us being kind of better able to navigate
the world.
Even today, I would say having kind of governments, businesses, kind of just
citizens have much more information.
And this, you know, I can, I can't really tell you perhaps in any kind of
interesting level of detail what's happening in Ukraine.
But I can get you the price of Bitcoin or the price of Apple stock and so on.
Quite reliably.
Don't you think that us having access to more information is also kind of leading
to better decision making?
So I'm so glad you brought this up.
I think this is a great topic to, to address some common misconceptions
about reality.
So Yuval Harari, an author I really like, recently wrote a book called Nexus.
I love the first half.
It's some of my favorite writing I've read in a long time.
Second half I found was a bit weaker.
But overall, the first half is extremely good because he makes a thing I
already knew and believe, but like he like, or lays it out like so much nicer
than I've heard anyone else lay it out, which is fundamentally more
information, let information is not truth.
These are two different things.
They are just not the same thing.
More information does not mean more truth.
It is often the opposite.
Historically speaking, more access to information usually actually did not
mean that there were more true things.
So specifically, the common example that people always bring up is the
printing press.
We always say, what about the printing press?
The printing press brought science.
It brought the science into revolution, whatever.
And this is just not true.
I don't know if people are just like never read a history book, but like
this is like historically false.
The printing press came 200 years before the scientific revolution.
The direct result of the printing press were the witch burnings.
So after the printing press was invented, Copernicus is, you know,
revolutionary theory about, you know, the heliocentric model didn't even sell
out its first edition of like 40 copies.
But meanwhile, the Malleus Maleficarum, the Hexenhammer, sold gangbusters.
And it was a huge tome describing a conspiracy theory about a satanic
pedophilic cabal, which is, you know, controls the whole world and is,
you know, trying to take away your children and to, you know, eat them
and like whatever, right?
It was the prototype for every modern conspiracy theory.
And that sold thousands, tens of thousands of copies.
The main effect of the printing press was witch burnings and the 30 year war.
Like these were the direct outcomes of the press, not the scientific revolution.
This is just historically inaccurate.
Now, was the printing press an ingredient to the scientific revolution?
Yes, of course.
Being able to reproduce scientific information was an important, useful,
you know, logistical component of the scientific revolution.
But if you want to pick one, like, you know, proximal cause of the scientific
revolution, it was probably the founding of the Royal Society in London.
That's probably a much closer proximal cause.
And that, because, and that was a social innovation.
It was an institutional innovation.
And it was an innovation where you were allowed to criticize others.
You were allowed to question truth and you were expected to.
But, you know, this wasn't the only culture in history that had this norm,
but like the Royal Society is like a very clear example of a institution
that had these like peer review concept or this idea of criticizing ideas
that nothing is taken on faith and stuff like this.
This is a much more proximal cause for the increase in truth versus noise
than more information.
More information made people less correct.
They made people more wrong.
And to a large degree, can you say social media is different?
Like, can you say that the proximal cause of social media is more social
harmony and cohesion and more democracy?
This is not obviously true.
Remember in the era of spring, but that's a slightly different point, right?
Social media hasn't resulted in more social cohesion.
For example, I don't think that's the point is that maybe it has resulted
in more information being available.
And so then the question is whether people can use that information
productively to, to kind of develop accurate models of the world.
But this is like saying I bring you a bunch of like toxic sludge and I say,
look here, there's more biomass.
Look, I brought you, you like food, right?
Food is made of biomass.
Here's some biomass.
Are you happy?
What are some systems we could set up today then to help us kind of navigate
the world specifically as, as, as it relates to navigating this dangerous
period of developing advanced AI?
Well, there's a specific question of how do we address AI risk specifically
and the more general question of like, how do we build like better
epistemic norms?
How do we build better institutions and so on?
They're closely related.
The main difference is, is that for AI specifically, we're running out of time.
So we need to, the first thing we need to do is to buy more time.
If we ever get ourselves into the scenario where an ASI exists that is not
deeply controlled and aligned with humanity, it's game over.
So the first and primary policy objective must always be to never get
into the situation by whatever means, you know, or the most effective to
prevent this from coming about.
So my colleagues over at control AI have wrote a very nice document called
the narrow path, which is a set of policy principles, which describes exactly this.
Listeners can scroll back in the feed to, to hear my interview with Andrea
about this document.
Exactly.
So I'm sure Andrea did a fantastic job of explaining these better than I could.
But basically these are the proximal direct policies or the types of policies
that would need to be implemented to actually, you know, not die from AGI in
the next couple of years.
This is the kind of things we need to do.
This does not mean, so the narrow path also talks about flourishing.
Like, okay, once we're not dying, what do we do then?
I think there's a lot to be said there as well.
How do we build better science?
How do we build more just systems?
How do we, how do we causally connect what people want and what makes
people happy with the future?
And this is the fundamental problem of statecraft, right?
The fundamental question of statecraft is how do we build super human systems?
So systems that are larger than humans that causally connects what the citizens
and the people actually want with the outcomes that the state or the
larger system produces.
And this is a very hard problem, but it is something that we have learned a
lot about.
We know a lot more about how to build good states.
We know a lot more about how, about epistemology.
We know a lot more about many of these things.
Like we didn't have game theory until the fifties.
Like it's crazy to me that we still are running states that are based on
constitutions and philosophies, you know, from like Montesquieu and so on, that
were invented before game theory existed.
That's crazy.
That's crazy.
We haven't even updated this.
So I think there are many low hanging fruits that are mostly blocked by
coordination, as always.
And like it's building institutions as pain because you have to deal with people.
God forbid.
Now you have to actually work with people and talk to them and build alliances and
so on.
There are many things that can be done, but they are hard.
There are no easy solutions, but they can be done.
Previously, you seem to connect this kind of crisis epistemology to our
inability to, to solve problems around kind of AI risk.
Yeah, but, but those are not, those are not necessarily connected.
And perhaps the first step here is the, the first step isn't that we
cannot take it from the ground up and try to solve our sense making tools.
And then we try to address AI risk simply because we don't have time.
Am I understanding you correctly that?
Yeah.
Look, if we had infinite time, I think it would be fantastic if we could spend
three generations of our greatest mathematicians solving the fundamental
epistemological problems of mathematics.
Right.
Like this is what we would do if we were not a stupid civilization.
We could be more like an actually wise civilization.
What we would do is we would actually solve deep problems of mathematics,
of philosophy, of spirituality, of religion.
Like there are deep things, like, you know, there's always like taboo to talk
about among our technical people, but there are deep things about like human
psychies that are like, that religions and spirituality do talk about that are
pre-formalized, that are not necessarily physically correct or whatever.
But there is, there is stuff there that is very important to people, that is
very, very, very important to people, that is a deep part about being human
and is just completely neglected and it's just not solved.
Like how to make people happy and feel fulfilled and spiritually connected
and so on is like, is a problem like that needs to be solved?
Like this is a deep thing.
You can't build a good society without addressing problems like this, but also
questions of like, how do you get different religions to coexist peacefully?
How can you create a net positive environment where different spiritual
traditions or groups can coexist in a mutually beneficial world?
Like these are deep, deep questions and they are far from being solved, right?
Like these are very, very hard problems that I think are solvable to some degree.
I think, you know, whether moral philosophy is solved or not is a bit up for debate,
but I think there are at least improvements that can be made here a lot.
And yes, if we took the time and, you know, at least we should spend, you know,
Sundays thinking about these kind of problems, at least I do.
But I don't expect we will be able to solve all these problems before
deadline, since deadline is currently in the next couple of years.
If you look at open AI, anthropic and deep mind, those are the companies
that are currently kind of front runners in the AGI race.
And if we are very close to AGI, do you expect those companies,
one of those perhaps to be the one that actually kind of gets there?
Seems quite plausible.
I mean, kind of the default case.
I wouldn't be surprised, honestly, if it was an open source thing.
I think there's been some work in the open source world, which surprised me.
Which, which work specifically?
Well, wouldn't you like to know?
Okay, you don't want to, you don't want to say what it is.
I prefer not drawing more attention.
Okay.
Okay.
Makes sense to me.
Yeah, I hadn't actually, I'm not thinking of open source AI as kind
of competitive at the cutting edge.
The main thing with open source is that it is way more high variance.
It takes some way, because the way we're distributed.
So it takes me way more crazy bets.
So if you need a crazy thing to work to get to AGI, it seems more likely
to happen at open source at the moment than the one at the labs.
Because the lags are de-risking.
Like as you gain scale, you also have to de-risk.
You have to do less crazy things because there's more on the line.
Now I, my main line to be clear is that the de-risking strategy is the
thing that gets us to AGI.
It's just we scale further, we make more data and we just like, you know,
do normal amounts, you know, normal kinds of engineering R&D.
And it doesn't take anything crazy.
But if I'm wrong and it takes a crazy thing or there is like, so I do think
there are crazy algorithms that have not yet been discovered that are like 10,
100 or a thousand or a million times more efficient than deep learning.
I get pretty sure that's the case.
All right.
And if those get discovered, they're going to get discovered by like some,
you know, malnourished grad student or the open source movement.
Probably not by one of the big companies.
Yeah.
All right.
But I mean, my, my main reason for not expecting open source to be at the
cutting edge is just that I don't think meta will fund a kind of increasingly
insanely expensive training runs.
I think those training runs will be done by, by deep mind, open AI,
anthropic, supported by each of their big tech funders.
Yeah.
I'm not sure kind of meta's investors will let them spend the money.
I don't know what you, do you think that's true?
I don't know.
I don't really know.
I mean, so far it seems to be working on really great for meta.
I mean, the stock price is doing fantastic.
So, I mean, so far the bet seems to be paying off for them.
But that being said, like intelligence isn't magic.
I think you can get AGI probably with the same amount of computer stupidity three.
Like if you knew how to do it, you'd probably do with even less than that.
Like, I think the lower bound on like human or super human intelligence is like
a 4090 or less.
Like it's probably, if you probably, if you had the perfect algorithm, you
could even do it just like on a CPU and like, you know, probably a decent amount
of memory, but like enough that you could like fit it in like a laptop, right?
Like a modern laptop.
Like it seems like there's no law or like theorem or anything that I know about.
Again, because we don't have a metrology, you know, a science of intelligence
that doesn't say is you can't get human level AGI on a single, you know, M1 MacBook.
Maybe, maybe there's some limitation, but it's not obvious to me.
I don't think we're going to get there before we get AGI.
I think the first AGI we're going to make is going to be super clunky.
You know, take billion time or resources than optimally necessary.
But like if we were to stumble upon some of these big breakthroughs, it's like, not
obvious to me that you need Lama four, five, six, seven to make it work.
I think it makes it easier.
Like I think having Lama three makes it easy to build AGI than having Lama two.
I think, you know, but I don't think it's like, I think we're already past
their vent horizon in this regard.
Like I think it's already possible to build AGI with just the current tools that
exist if you knew how to do it.
I'm not saying I know how to do it, but like I expect that if a dedicated
group of hackers, you know, and like, you know, a couple of 40 nineties, a
couple of MacBooks and Lama three, and they get very lucky, could already do it.
But that's not the most likely scenario.
The most likely is that one of the AGI corporations developed this by, by, you
know, running an, a gigantic training run.
Yeah.
Running a jack, training run, and also scaffolding around them.
Like, you know, agent scaffolding, et cetera.
Like, I think I often joke is that, you know, AGI is going to be like, you know,
GPT N plus a thousand lines of Python.
Yeah.
All right.
How efficient are these techniques?
Do you think kind of when we talk about on hobbling or scaffolding or all the,
all the things we do after we have finished training runs, what, what does
that give us in terms of capabilities?
I think that's the difference between AGI and chatbot, basically.
I think a lot of scaffolding at the moment is stupid and predictable
ways that will get solved.
So like lots of low hanging fruit, which is like, people are just like, do
we get quite poorly in like, in like dumb ways also for like, and just like,
there's just like many, it's a large design space, right?
Like it, we've only been doing it for a couple of years.
We've only had like non-stupid models for a couple of years.
Right.
Like, I don't think GPT two could get to AGI, no matter how hard you poke it,
probably, but like GPT three, probably not GPT four.
Yeah.
No, I, I, like, I think if you had like a GPT four level model and you like knew
how to do scaffolding correctly, you could get to AGI.
There's a lot of kind of latent intelligence in the models that are
not being used right now.
Oh, yeah, yeah, yeah.
Like, I don't know if you've ever seen someone who's really good at prompting.
I have, I have observed that.
And that is, that is insane.
Would they, they can make models do things you just like would not believe.
Like that would seem like it shouldn't be possible.
Right.
So like, and again, there's this thing where just like intelligence is composed
of smaller components.
And like, if you actually zoom down on these smallest dissolved components,
they're not, they're not that big.
They're not that complicated.
The GPT three can already do formal reasoning.
Right.
You can already teach it.
You know, to do math, right?
It's like, that's already like a lot of the way there, you know, not saying it's
all the way there, but it's like, this is a lot of the way there.
And then, you know, they can learn new facts.
They can pattern match.
They can do metaphor and logic and whatever, right?
And then just like, I don't think intelligence is made out of that many things.
Like there are things and you need to get them right, but it's more of an engineering
problem, right?
It's more like, you know, if I show you a lump of uranium, you'd be like, or you'd
be like, wow, this is not dangerous at all.
Look how far this is from vision.
And I'm like, yeah, but now it's just an engineering problem.
Do you think using inference time compute will play a larger role in future capabilities?
This is, this is where whenever you type into chat, TVT, and it says thinking that
that's, that's inference time compute.
What do you think?
I'm not, I'm not explaining that to you.
I'm explaining that to listeners.
Yeah, I mean, seems obvious.
I mean, humans do a lot of inference.
So seems logical that like, if you, if you don't do it in inference time, you
have to do a training time and then you have to pre-bake it, which is just more expensive.
Why is it more expensive?
Because you have to account for every possibility.
You have to kind of lay out exactly what could be asked in.
Yeah, like imagine the difference between writing every possible sorted array
versus a sorting algorithm.
Yeah.
Yeah.
Okay, Conor, is there anything we haven't talked upon that you want to say to our listeners?
That's a good question.
I mean, there's, we could talk a lot about politics and institution design and all
these kinds of things.
I guess the thing I would really like to talk about, maybe just like more
things also direct to listeners a little bit about like, well, what do we do?
Like what do we do about this?
Like I haven't justified a lot of the extra stuff in this talk.
I hope your listeners, dear listener, that you either buy the hypothesis or, you know,
you go read the compendium and read why I think this is a problem.
I will link that and listeners can also scroll back to some of our previous
episodes in which you do spend a bunch of time justifying AI risk or many of the
other episodes that we've been doing over the last couple of years.
So I'm not going to justify that further.
So I'm going to assume you're bought into this, bought into the things I've been
saying, what do we do?
Like what are things that you listener might be able to do?
And so I think I have a slightly different opinion here, or like at least
way of thinking about it than I think a lot of other people do, where when you're
confronted with some problem as massive as AI extinction, right?
It feels very natural that the response must be massive, that, you know, there's
something so huge, you must do something big.
You must like drop everything and work on this full time.
You know, yell at all your friends about it.
You need to make it your identity.
You need to like sacrifice everything, whatever, right?
And I'm here to tell you that that is not the right thing to do.
The truth is that, I mean, A, this is just usually very counterproductive
because the main thing that will happen if you attempt to do this in most
circumstances is just you burn out and that's not going to help there anybody.
And now you could say, oh, but I should do it.
It's the right thing to do.
I won't burn out.
I'm like, bro, listen to yourself.
Like this is just not how the real world works.
The actual things that need to be done to solve AI, to solve AI risks,
sort of things are mostly extremely boring.
This is very important to understand.
Most of the things, the actual work that needs to happen, if we want
the future to go well, is very, very boring.
It is not epic, exciting.
We need to, you know, crusade on social media or we need to solve
this cool math problem or build this big machine.
Like all these stuff are like nice and fun and seductive, but they're
mostly not what we need to do.
Most of reality is very boring.
Most of power is very boring.
One of the most, maybe the most powerful entity in the world, the U.S.
government is mostly extremely boring.
It's just unbelievably boring.
It's bureaucracy and petty politics and writing things down and having
long meetings and et cetera, et cetera, right?
And now when people see this, they might assume, well, there is some
exciting part that I'm not seeing or that I see on TV or TV tries to present.
These things is like exciting, memeable or whatever.
Or you might think it's like defective.
Like if it was non-defective, it wouldn't be this way.
But the truth is, is that most complex things are made of simple parts.
Most big actions are made of simple things.
Most of coding is not coming up with some brilliant new algorithm that
will be named after you and you're going to win an award for.
Most of it is trying to fucking connect to the stupid web server, you know,
and copying code from Stack Overflow, right?
Like 99% of your coding work is going to be rehashing things that people
much smarter than you did.
I've already solved better in the past.
It's just how it is, right?
The reason I'm harping on this so strongly is to use this aesthetic is
that there's a lot of boring work that needs to happen.
Would I talk to policymakers, to the general public, including two
extremely powerful people, like even like billionaires and stuff like this?
It is utterly shocking how many of them have just never heard the arguments.
Literally no one told them.
It's not that they like heard the arguments, thought about it
carefully and rejected them.
That happens sometimes.
But most of the time, literally just no one sat them down and just politely
and patiently explained things to them.
Like I've had meetings with like high level politicians.
Well, I'll sit down, take an hour or something and they'll ask me a bunch
of questions about AI or whatever and I'll answer all the questions.
I remember there's like, particularly like one politician I've talked to this
where like at the end of the meeting, he was just kind of when I answered, he
asked me like some complicated question and I answered the question and he
looked at me and he was kind of like, wow, you're really answering my questions.
Like he was shocked that like, because that I was not trying to sell him
something, I was not trying to make him do anything.
I was just taking the time to explain things to him.
And he was so thankful for this.
He was so genuinely thankful for this.
That just like someone took the time out of their day to just give him
information to help him.
This is how not everyone like to be clear, there are corrupt politicians,
evil people, blah, blah, blah.
But like most people in most institutions in most bureaucracies are just normal
people.
They're just trying to get by.
They're overworked.
They're overstressed.
They don't know what to do.
They, there's all these things pulling them in different directions.
And we need a lot of patience, a lot of work, just explaining things very
carefully over and over to different people to, you know, helping them do
the things that they need to do and so on.
So what do we do?
The kind of things that I and you and all of us need to do is we need to just
have patience and actually explain things to people.
We do actually reach out to politicians and be like, Hey, what are you
doing about this problem?
I'm very concerned.
Here's this compendium thing.
You can read about it, or I'd be happy to come over and tell you more about
this, or I know my, or the, here's my friend who works in an AI startup who
would be happy to explain this to you.
I like, I know a guy who has been just like, just emailed like all of his
local senators and house members and whatever, because every day he just
emails them a summary of like every week he emails them a summary, like cold
email, just emails them a summary of what's happened in AI this week.
And he's gotten like three meetings out of this because they were like genuinely
thankful.
They're like, Oh, thanks for sending this.
Like, Hey, can I ask you a question about this?
That's great.
Like this is awesome.
If we could get two people in every US state to do this, to just like send
your politicians a very polite, helpful email of like, Hey, you're just
going to, I'm concerned about this, you know, but like, I thought you might want
to know this, like, it sounds like nothing, right?
Like it seems so small.
These are the kinds of small things that large coordination that large
institutions are put off.
These are the kinds of small actions.
And I need people to do stuff like this.
I need people to help do stuff like this.
Read the compendium, you know, talk to policy, talk to your friends, talk
about this in social media, because this is a civilizational problem.
This is not a problem you or me can solve.
This is a problem that we have to solve as a group, as a species, as a
civilization.
And the way we do that is we need to talk.
You know, Gus, you were saying earlier that like your ancestors wouldn't
consider your job to be a real job.
I disagree.
I think they would have understood if I told them, Oh, you're a messenger or
you're a diplomat, they would have been like, Oh, of course.
Yeah, of course.
That's very important.
The kingdoms must know the information that's great of him that he's doing
that. I'm glad he's extracting all the information and bringing it to all the
people.
What a great job.
Like our ancestors would have actually understood the concept of a podcast.
You know, they wouldn't understand, you know, microphones, the internet, you
know, per se, but the idea of someone who like finds interesting people who
know interesting things, extracts their information and brings it to people.
That's labor.
Like that's like, obviously labor, you know, like maybe the information is not
useful, maybe it's just amusing, whatever.
That's labor.
Moving information around civilization, around society is work.
The same way as moving earth or moving, you know, materials is labor.
Moving information around the graph is labor.
And this is the labor that needs to happen.
This is a very, very important thing.
Yeah.
I think this is very important is that we need to move this information.
It needs to be replicated.
It needs to be spoken.
It needs to be talked about because then people can reason about it.
Like something is uncool and weird.
You can't think about it.
You can't talk about it.
It's like, you, if you're a politician and you keep talking about something
weird, you lose your job.
Something is not weird.
If it's an important issue that everyone keeps emailing you about, then you
can talk about it, then you're allowed to think about it.
There's another thing you said earlier, which was how more information allows
you to know more things like you could know the Bitcoin price, right?
This is true.
But will you decide to know the Bitcoin price?
The scarce resource of right now is not information, it's attention.
You could know many things.
You could know so many things.
Will you?
You could read every book in history.
Will you?
No, you can't.
You have a limited budget of attention of what you can know.
So the difficulty is no longer you have, there are a few things to know.
The problem is now you must choose what to know.
You must choose what to think about.
What to process in your mind.
What to put into your brain and actually think about.
And politicians and all other like, you know, people who are like busy have
this huge problem where there's so much that is vying for their attention.
So you're, so a lot of what you have to do as a citizen is help these nodes,
you know, these connector nodes, your politicians and your influencers and,
you know, you're even just your local friend group.
You have to help them draw their attention to the thing.
They need to pay attention to you to help them do this.
Like this is a process.
And then once they can reason about this, once it's less, no, it's more normal.
They can spend more attention.
They can form better opinions.
They can ask better questions.
They can find other people to work with.
They can ask questions to build coalitions.
This is the kind of stuff that we need to do.
We need, we need large coalitions of people across the spectrum, not just tech
people, you know, I assume a lot of tech people listen to this podcast, but
you're a non-tech person.
You're the kind of person I want helping with this, you know, whether,
you know, people from civil society, you know, people from, you know, NGO
backgrounds or, or, you know, unions or just, you know, day-to-day normal
people are interested in this topic where it's academics or it's, you know,
faith groups, all of this, like this affects everybody.
AGI, AI is a thing that affects everybody.
It affects your low, it affects your job, it affects your family, it affects
your church, it affects everything.
You should think about this.
I am trying to make the case to you that you should give this a little bit
of your attention.
Your attention has to be extremely valuable.
You have, you have your family to attend to, you have your life to attend to.
There are many things that are valuable to you to attend to.
I'm making the case, please attend to this as well.
Put a little bit of your attention on this, not a lot.
I think it's important.
Don't put your whole life on this.
Put 10%, 5% of your attention, 1% of it, a couple hours a week, one
hour a week, two hours a week of your attention, thinking about this.
What do you think about this?
How does it make you feel?
Who do you trust here or not?
Where can you, what questions do you have?
Well, how do you get these questions answered?
The thing that I really recommend to people is like literally like open a
Google doc and start writing, like, what is my plan?
Like, I think about AI.
What are my questions?
Where am I uncertain?
How do I get to the thing I want to?
And like, who should I, who should I ask?
Who should I work with?
Like, this sounds stupid.
I'm sorry, it sounds so stupid.
It doesn't sound stupid.
It, it sounds like a valuable device, I think it may, it, maybe it doesn't
sound prestigious, but it doesn't sound stupid to me.
And I think that's an important difference that you made your, your
kind of sketched out yourself.
I think you're right about this.
If it works, it ain't stupid.
And this works.
This works.
If you, if you, dear listener, actually want to have a causal effect on
the future going well, no joke, you actually want to have a causal effect.
The thing I think you should do is you go to your computer, you open an empty
Google doc, start a bullet point, and you start writing out what do you want
the future to be?
Like, what do you want it to look like?
And you start asking, well, what do I need to do to get there?
Like what's currently broken?
Or like, what do I need to do?
And then you start to notice what questions do you have?
Where are you uncertain?
What are resources that you currently don't have access to?
You know, who do you need to reach out to?
Or like, what actions can you take?
And don't put too, don't break yourself over this.
Put an hour of work into it.
Don't do more.
You know, maybe once a week have a call with someone, you know, send, send me
or someone else an email.
I just like, ask some questions.
You know, a lot of people are nice, including a lot of famous people are
very nice, you know, you can just ask them questions, you know, go on social
media, ask questions, try to figure it out, find other people in your area who
might be interesting.
This is civics.
This is the job of a civilian to build a greater civilization, you know,
civilization, civilians, like our job is to build a greater civilization for
ourselves and for everyone else.
And this is how we do it.
This is the process of how we do it.
And so I just ask anyone listening to this, like, join me, like, do, do the small things.
I think that's valuable advice.
Connor, thanks for, thanks for chatting with me.
It's been, it's been great.
Thanks.
It's been a real pleasure.

Welcome to the Future of Life Institute podcast. My name is Gus Ducker. This is a special episode of the podcast featuring Nathan Labence interviewing Flo Crivello.
Flo is an AI entrepreneur and the founder of Lindy AI. Nathan is the co-host of the Cognitive Revolution podcast, which I recommend for staying up to date on AI. Here is Flo and Nathan.
Man, you know, it's a tough time for somebody that tries to keep up with everything going on in AI. It's like, it's gone from, you know, in 2022, I felt like I could largely keep up and, you know, wasn't like missing whole major arcs of, you know, important stories.
And now I'm like, yeah, I'm like totally let go of like AI art generation, for example. And this policy stuff is really hard to keep up with, especially this week. Of course, it's like hitting a, you know, a fever pitch all at once.
But, you know, it's I love it. So I can't really complain at all. It's just, at some point, you know, got to admit that I have to maybe narrow scope somehow or just let some things fall off. I'm kind of wrestling with that a little bit.
Which I think is just like a natural, yeah, I mean, you know, I hear you. I think it's just a natural part of like the industry evolving. It's like, imagine, you know, talking about like keeping up with computers, right, in like the 80s or something. It's like, I'm sure at some point, it was possible to keep up with computers at large, you know, it's like keeping up with tech is just like, it's like, okay, it's like, it's like half the GDP over, right?
You're doing all this in your second language, right? This is, I assume English is your second at least.
I have an excuse. Yeah, second. I'm actually getting my American citizenship. I had this interview just yesterday.
Wow. Congratulations. That's great. I know it's not an easy process, although maybe it's about to get streamlined. I haven't even read that part of the executive order yet, but I understand that there is kind of an accelerated path for AI expertise. Have you seen what that is?
No, but generally, there's good stuff being done in immigration, like they're relaxing a lot of these requirements, they're like closing a lot of loopholes, they're doing another very good stuff.
Yeah, I've been thinking of just as kind of a general communication strategy, if nothing else, calling out the domains in which I am accelerationist, which are in fact many, I think, you know, you're not pretty similar in this respect where it's like, um, perhaps the singular question of the day.
I am not an accelerationist, but on so many other things, I very much am an accelerationist and like streamlining immigration would be one of those. You know, I would sooner sign up for the one billion Americans plan than kind of, you know, the, build the wall plan, certainly.
And I just did a, right before this was doing an episode on autonomy and, you know, self-driving. And that's another one where I'm like, holy moly, you know, I don't know if you have a take on this, but the, the recent cruise episode, I find to be, you know, kind of bringing my internal marketing,
and, uh, very much to the four where I'm like, we're going to let one incident a, like shut down this, you know, whole thing in California. That seems crazy enough. But then the fact that they go out and like do this whole sort of performative self, I mean, whether it's performative or not.
Maybe it's sincere, but do this whole self-flagellation thing and, you know, shut the whole thing down nationwide. I'm like, can we, where is our inner Travis on this people? You know, somebody has to stand up for something here at some point.
Totally. I agree. I think it's just the natural order of things, right? It's like, I don't know if you know that piece of history about when the automobile came about. There was this insane law that said you need to have someone walking with a flag in front of the automobile at no more than like four miles an hour.
Right. So it's part of the process, man. It's infuriating. I hate it, but in some way, and maybe it's cool, but I made peace with it. I'm like, it's part of the process. You can't really stop power. It's going to do its thing.
So it doesn't really matter anyway, because like the self-driving cars are not really deploying at a very large scale. And so I'm like, you know, it's not a bottleneck anyway. I don't think it is.
I guess I have two reactions to that. One is like, it feels like if they, if nobody kind of fights through this moment, then there is like this potential for kind of the nuclear outcome where, you know, we just kind of get stuck.
And it's like, sorry, you know, the standards are so insane. You've got to be, you know, we do have a little bit of like a chicken and egg problem where, you know, if you had a perfect self-driving car, they'd let you deploy, but you're not going to get to perfect unless you can kind of deploy it.
And, you know, to me, this technology is just an incredible example of where, you know, the relative risk is already pretty high. As far as I can tell, they already do seem to be as safe or marginally safer.
You know, maybe as much as order of magnitude safer already, depending on exactly what stats you look at. And I would just hate to see us get kind of, you know, as we're like kind of close to maybe some sort of tipping point threshold, whatever, to get stuck in a bad equilibrium of, you know, never get, and then, you know, maybe get stuck and never get out of that chicken and egg thing would just be so frustrating.
I drive a 2002 trailblazer that I have sworn never to replace unless it's with a self-driving car. And it's becoming increasingly difficult to keep this thing going, you know, so I'm like, how long do I have to wait?
My other take on this is I think Tesla is actually like really good. I borrowed a neighbor's. I don't know if you've done the FSD mode recently. My grandmother came up for a visit. It was fun. I actually took, you know, my 90-year-old grandmother on a trip back to her home, which is like a four-hour drive there.
And then I did four hours back all in one kind of big FSD experiment. I set up my laptop in the back, put a seatbelt on my laptop, so it was like recording me and recording us, you know, driving so I could kind of look at the tape later. And I was like, man, this is really good.
I had no doubt in my mind coming out of that experience that it's a better driver than like other people I have been in the car with, you know, for starters. So I'm thinking through my personal life, like, yeah, I'd rather be in the car with an FSD than this person and that person and this other person, you know, and I'd be definitely more likely to let it drive my kids than this other person.
So I felt like it was really good. And then the other thing that was really striking to me was the things where it messed up. I mean, there weren't many mess ups for one thing, but like the few mess ups that we had, there were there were a couple in an eight-hour thing. It was like, if we actually had any mojo and we went around kind of cleaning up the environment, we could solve a lot of this stuff. Like there was one that my neighbor who lent me the car said, you know, you're going to get to this intersection right there on the way to the highway and it's going to make it.
And I missed the stop sign because there's a tree in the way. And I was like, you know, for one thing, probably people missed that too. Like, let's trim the trees, you know, and then there's another one where you're getting off the highway and there's a stop sign that's kind of ambiguous.
Like it's meant for the people on the service road, but it appears to be facing you as you're coming off the highway. And so the car saw that and stopped there. And that was probably the most dangerous thing that it did was, you know, stopping where people, you know, coming up the off ramp, like, do not want you or expect you to be stopped there.
And that's another one where you could just go like, put up a little blinder, you know, to just very easily solve that problem. And I imagine people must have that problem too. And we just have no, no will, you know, when it comes to that. And again, it's I feel like I'm turning into Mark Andreessen is more I think about self driving over the last few days.
No, I'm with you on that.
So where else are you accelerationist that may not be obvious as we kind of think about this, you know, this kind of AI safety and regulation moment that we're in?
You know, pretty much everywhere, man, like I'm a libertarian, like I used to work at Uber, where I saw regulatory capture and I saw cocktails and I do believe, you know, at the deepest level that cocktails and regulatory capture and generally, I think it's Menker Olsen who calls them
extractive institutions who are just in the business of they don't want to grow the pie, they just want to grab a little bit more of the pie for themselves, even if it actually shrinks the pie, they don't care as much as they get bigger chunk.
And I think that's the world is just rotten with thousands and thousands of these institutions, whether with other private or with other unions or governmental, it doesn't matter.
We just have so many of these cartels floating around and it's killing everything. Right. It's a tragedy. And I totally understand how folks like Mark Andreessen would be.
They have built such a deep and justified hatred and reaction for this nonsense that is destroying everything that they immediately just the pattern recognition immediately triggers when they see what's happening with the AI.
They're like, ah, it's happening again. They're doing it again. It's like, chill. I totally get it. But this time is really different. This is really something special that's happening, not just in the markets, not just in the economy.
Not just in the country in the universe. Like there is a new form of life that's being built. And this is we're like a new territory and we need to be careful right now.
Right. And so that's that's where I'm coming from is like, I totally see that point of view. And I'm like, regulation, for sure, there's going to be cartels for sure, we're going to screw up 90% of it.
Politics is going to get messy and transgenderized. So I'm going to get into play. And it's all worth it because what may very well be on the line, it sounds alarmist, but I'm sorry that we need to say the world may be literally human extinction.
Right. And this is not some tinfoil hat theory. There's a more and more experts that are coming around and saying that.
It's actually funny. Mark, in recent, if you dig it up, I'm sure you could find it. I think it was an interview from him. I want to say between 2017 and 2020 that doesn't help him because he gives so many of those.
But I think he said something like, at the time, he was actually appealing to an argument of authority. He was like, look, he was saying the same things he's saying today.
Providence is good. It's just a tool. And by the way, the experts say there's nothing to worry about. So I don't know. You guys don't know anything about AI. I don't know anything about AI. They do.
And they're telling us there's nothing to worry about. The document isn't true anymore. The experts are telling us there is something to worry about. And now it's just like, oh, arbitrary regulatory capture. No, no, it's not regulatory capture.
OpenAI was founded on that premise from day one. So if it was regulatory capture, there's like one hell of a plan. It's like, oh my God, we're going to create this industry and we're going to start regulatory capturing right now.
It makes no sense. It was literally the plan from day one. Yeah, that's where I'm coming from. I'm largely in the EAC camp. I am in team technology, team enterprise, team anti-regulation, but here something very special and potentially very dangerous is happening.
So let's go back to your use of the phrase a new form of life. I, as you may recall, am very anti-analogy as a way to understand AI because I think it's so often misleading and I often kind of say AI, artificial intelligence, alien intelligence.
It may be tempting for people to kind of hear or not tempting, but it may be sort of natural for people to hear you say a new form of life and understand that as an analogy.
But do you mean it as an analogy or, you know, I guess we might start to think about like, is that actually just literally true and what conditions would need to exist for it to be literally true?
And you might think about things like, can AI systems like reproduce themselves? You know, can they, are they like subject to the laws of evolution? But like for starters, how literal do you mean it when you say that there's this like new form of life in AI?
I mean it pretty literally. I think if you zoom all the way out literally from the birth of the universe, the evolution of the universe has been towards greater and greater degrees of self-organization of matter.
And there's actually a case to be made that this is just a natural consequence of the second law of thermodynamics.
There's this amazing book that Iac people love to quote.
Yeah, I was going to say, you're sounding very Iac all of a sudden.
It's a good point. It's called Every Life is Entire, right?
And so if you look at the Big Bang, we would, you know, a few fractions of a second after the Big Bang, it was just subatomic particles.
And then they ganged up together and formed atoms.
And then at the stage after that was the atoms ganged up together and formed molecules.
And then the stage after that, the molecules became bigger and bigger because they formed the stars exploded and caused all sorts of reactions.
And so a few generations of stars later, we have like pretty big molecules and pretty heavy ones.
And then these molecules formed into sort of like protein and RNA and forms of proto-life.
We don't totally understand. There's a chain here that we don't totally understand.
But there's a form of proto-life that formed and then life.
And so you can think of like, I think it was just a DNA or actually it was RNA, DNA, nucleus of a cell, cell, mitochondria came into that.
And then, okay, good, we have a cell. And then the cells started ganging up together and now we have multicellular organisms.
And then we have brains at some point, like there's like a big leap, but we have brains, like on that great map, two worlds greater and greater degrees of stuff organization.
And at some point we have us, which with a little bit of hubris, perhaps I am considering the apex of that thing for now.
It just seems crazy to me that everybody is saying like, one, this is totally normal.
Oh, this is normal. This is quintillions of atoms that are organized in this weird, super coherent fashion that are pursuing a goal in the universe.
Like what's happening right now on Earth is all the weird to begin with.
So people are all deep thinking that this is normal and that's what it is.
And that this mark is going to stop at them.
And they're like, well, maybe we're going to get slightly smarter or maybe we're going to get augmented.
You are such a leap compared to an atom or compared to a bacteria that there is no reason to expect that there wouldn't be another thing above you that is as much more complex or bigger than you as than you are to the bacteria.
Like there's nothing in the universe that forbids that from happening.
From a being to exist that is about as big as a planet or a galaxy, like there's nothing forbidding that in the universe from happening.
And from the first time now, if you squint, we can sort of see how that happens.
And silicon-based intelligence certainly seems to have a lot of strengths of its sleeve versus carbon-based intelligence.
And so no, I actually sort of means that pretty vitrally.
It is sort of in line with the mark of the universe and this is the next step, perhaps, it's significant.
And so I am hopeful that we can manage this transition without us being destroyed.
That's what I want to have.
Does that imply an inevitability to advanced AI?
I guess a lot of people out there would say, hey, let's pause it, slow the whole thing down.
And then you get kind of the response from like an open AI where they're sort of saying, yeah, we do take these risks very seriously and we want to do everything we can to avoid them.
But we can't really pause or we don't think that would be wise because then the compute overhang is just going to grow and then things might even be more sudden and disruptive in the future.
Where are you on kind of the inevitability of this increasingly capable AI coming online?
I don't think it's totally inevitable.
I am generally a huge believer in human agency.
I think we can do pretty much anything we set our minds to.
I see a contradiction, by the way, in the EAC argument that like, on the one hand it's inevitable, don't try to stop it.
On the other hand, oh my God, if you do this, I'm going to stop it.
It's like, you got to decide here.
So unfortunately, it's not necessarily inevitable.
I am actually worried as much as the next guy, I agree there is a risk that we over-regulate and miss out on the upside.
And the upside is significant.
And if you look like during the Middle Ages, we successfully, as a civilization, stopped progress.
And in a lot of countries, if you look at North Korea, they did it.
They successfully stopped progress.
So you can stop progress.
Progress is not inevitable and arguably it is actually quite fragile.
So no, I don't think it's inevitable.
And I'm hopeful that we can, again, I want us to get the upside without experiencing the downside.
The North Korea example is an interesting one.
If I was going to kind of dig in there a little bit more, I might say, okay, I can understand how if things go totally off track,
then we could maybe enter into a low or no or even negative progress trajectory.
If there were a nuclear war, then we may not come back from that for a long time.
Or if whatever, an asteroid hit the earth or a pandemic wiped out 99%,
like there's extreme scenarios where it's pretty intuitive for me to imagine how progress might stop
or just be whatever, greatly reversed or whatever.
If I'm imagining kind of a continuation-ish of where we are,
then it's harder for me to imagine how we don't kind of keep on this track.
Because it just seems like everything is, we're in this, I would call it,
I don't know if it's going to be a long-term exponential,
but if not, we seem to be entering a steep part of an S-curve where hardware is coming online by the order of magnitude.
And at the same time, algorithmic improvements are taking out a lot of the compute requirements.
And we're just seeing all these existence proofs of what's possible
and all sorts of little clever things and scaffolding along the lines of some of the stuff that you're building is getting better and better.
Is there a way that we can, do you think it is realistic to think we could kind of meaningfully pause
or even stop without a total derailment of civilization?
The derailment of civilization thing, you could imagine the most extreme scenario which I am not proposing,
but you could imagine the most extreme scenario which is no more Warsaw.
You do not exponentially improve your semi-conductors anymore.
That'd be crazy, right?
But there wouldn't derail civilization.
Civilization is not predicated upon Warsaw.
We would do just fine with the chips we've got today.
And if anything, I think we have a lot of overhang from the chips we have today.
A few, two, two, two, two, two overhang.
So I actually think it is possible to do that if we wanted to.
And I don't think that even this, which I think is the most extreme scenario, would actually derail civilization.
Well, we are actually lucky in that there are a few choke points in the industry.
Actually, more than a few.
There is ASML, there's TSMC, there's NVIDIA.
All of those three are individually, there are choke points.
A regulator could at any point grab one of them and be like, no more.
You just stop, right?
Or you add this chip into all of your GPUs moving forward.
So we have a kill switch.
At the very least, we have that.
So if the chip really hits the pan, we have an automatic thing in place that shuts down the CPU on this.
Now that would be disruptive, but potentially less disruptive than the raw GSI.
So no, I actually think it is very much possible.
This thing's all on the table.
And I don't think there would be all that disruptive.
So maybe that's a good transition to kind of where we are right now.
We just had this executive order put out this week.
And I think everybody's still kind of absorbing the 100 plus pages and trying to figure out exactly what it means.
What's your high level reaction to it?
And then I'll get into some of the specifics.
It's an executive order for now.
It is not law.
It's very early.
Overall, I am pleasantly surprised not by the specifics, but by the fact that we're reacting quickly.
But the fact that the measures that are proposed are not insane.
Like, I was afraid of like, there's a really good case to be made.
The second look, we have a gerontoprocessing case.
Now, a bunch of 70, 80 years old's governing as they don't know anything when they were born, there was no mobile phone, right?
Can't really blame them for not really understanding anything.
And so I was afraid that the regulation would go something like, if you install Microsoft Office in your AI, then you have to make a report.
And so what?
So the regulation actually sort of makes sense.
It's talking about Flops.
It's talking about all those types of training.
So I think it's a step in the right direction.
I'm actually happy about what's happening with this executive order.
Now, the specifics.
Look, the problem is that it's almost impossible to regulate AI in a way that doesn't have any loophole.
So they're regulating it according to an enrolled Flops, and that's okay.
But that's the end of the day.
And then you get stuck into, okay, what happens when you have algorithmic improvements?
What happens when you do URL instead of computing?
And like, that's just a lot of different loopholes that researchers are going to find.
And so I think overall it's an encouraging first step.
It's funny.
I've, you know, there've been proposals around even like a flop threshold that would drop progressively over time in kind of anticipation of the algorithmic improvements.
That's a, you know, even a more probably challenging one to put out into the world,
especially given, you know, people are not in general great at extrapolating technology trends or, you know, don't want to accept regulation in advance of stuff actually being invented.
So we've got this flop threshold thing where basically, as I understand it so far, it's like, if you're going to do something this big,
you have to like tell the government that you're going to do it and you have to bring your test results to the government.
I would agree with them.
That seems like a pretty good start.
And also the threshold seems like pretty reasonably chosen at 10 to the 26.
Any, you know, kind of refinements on that or quibbles that you would put forward that you think like, you know, maybe the next evolution of this should take into account?
I think ultimately, we're tiptoeing around the issue, but ultimately we need to come to an actual technical blanket solution.
Like, we will not solve ASI alignment by asking for reports from AI companies.
That's not how it's going to happen.
So again, I think it's a step in our direction.
I'm happy with the action.
I'm happy the action is not totally nonsensical.
But that's the end of the day.
Like, we're going to have to talk about the kill switch, right?
The proposal I just made is one that I see more and more talked about.
And that's the one I would feel best about.
You've got to put this chip into your H100s and the government and there's like a centralized entity that can shut down all GPUs all at once.
And by the way, it wouldn't necessarily shut down every computer because your laptop doesn't have an H100.
Your iPhone doesn't have an H100.
Like that's fine.
Over the long term, most of it makes it so that your laptop and your phone actually end up with an H100.
But at least that dies us a few years to make progress on AI safety and alignment.
Ideally, we would then automate just like reportedly the Russians did during the Cold War.
We would automate.
Like, we would set up some detection systems to God knows how we would do that.
But hey, there's an ASI going wrong.
Like, the world is really changing rapidly.
Assuming it's not too late, which maybe because at that point, God knows, but you could very basically that would give us the best weapon against the ASI.
We would have like a gun against the ASI's hand and kill all the GPUs.
You cannot operate anymore.
God knows how effective that would be because at that point, all bets are off.
If you have an ASI, God knows what it does and how it protects itself.
But that would be what it would feel best about.
Do you have any sense for how that would be implemented?
Technically, it seems like you would almost want it to be something that you could kind of broadcast.
You almost want like a receiver on chip that would react to a particular broadcast signal.
Because you would not want to have an elaborate chain of command or relying on the dude who happens to be on the night shift at the individual data centers to go through and pull some lever.
Do you know of anybody who's done kind of advanced thinking on that?
You hear a lot of these kill switch things, but in terms of how that actually happens so that it's not dependent on a lot of people coming through in a key moment, I haven't heard too much, to be honest.
No, I haven't seen too much results on that.
But I think the technical challenge does nothing in principle that makes the technical challenge unsolvable.
We already have a chip that can be broadcasted to for like a dollar from space, like the GPS chip.
There's a lot of chips and it costs, you have one on your phone.
And so why not put the GPS like chip?
Maybe we could literally piggyback the GPS protocol.
I don't know.
But why not put the chip like that in every GPU?
Again, if you have an ASI, God knows, maybe it hacks the chips before you get a chance.
It hacks the satellites that broadcast the thing.
I have no idea.
But again, I think pointing in this direction is what I would like things to go into the limit.
I think the basically, and that's like the most extreme version of this proposal.
But like the Yutkovsky a strike proposal.
That's like, you cannot accumulate billions and billions of dollars of H100 and build this thing else.
We will go up to L striking you.
That's the most extreme version of this.
But that actually, I think is directly correct.
Like we, this is going to be the most powerful force in human history, maybe even in the universe.
You cannot accumulate that stuff anymore than you can accumulate enriched plutonium.
Right.
We've got to, we've got to forbid that.
It's the lowest level possible.
And so that level cannot be the application layer.
Because the application layer is just, it's just to diffuse those like a thousand startups everywhere.
Any kid in the garage can build one.
It's got to be at a took point.
And the took point today is the city code.
Yeah.
Let's unpack that a little bit more.
Cause I think that has been an interesting debate recently.
You'll hear this kind of call for let's not regulate model development.
Let's regulate applications.
And then, you know, we can kind of have medical regulation for the medical and everything can be more appropriate and like fit for purpose.
And, you know, maybe there's something else to be said for that.
But yeah, I mean, if you're really worried about tail risk, it's like probably not going to be sort of mega medical, you know, device style regulation of, you know, diagnostic models or whatever that is going to keep things under control.
So maybe you could even do a better job of steelmaning the case for the application level regulation.
But I guess, you know, why do you think that give your account of why that's not viable in a little bit more detail?
Yeah, I think the steelman here is like, look, people are going to use forks to poke each other in the eye.
That's not a reason to forbid the fork.
Like forks are awesome.
We love forks just for people from poking each other in the eye with them.
The problem is that as the fork in this analogy becomes more and more powerful, the argument loses more and more of its defense.
Ultimately, it's just a risk benefit analysis.
Right.
And so the risk becomes greater and greater as the artifact becomes more and more powerful.
So more powerful than the fork and AR 15.
And so, you know, the opinions vary about that.
But look at at this point, if you look at the data, you actually save lives by heavily regulating the sale of AR 15.
You can't just be like, oh, sell them to everyone and just for big people from shooting each other with them.
It's like, it's an AR 15.
What do you expect people to do with them?
Right.
Now, in the most extreme scenario, enriched uranium, you can't be like, you can buy all the enriched uranium you want.
You don't even need to fill up a form, which, by the way, that is all the executable that says right now.
At least fill up a form.
Can you please at least tell us what you have to do?
So hey, you can build you can build the enriched uranium you want.
Just don't bond us with us with it, please.
Like when we roll it in this disappear, you can do it.
Oh, no, that's not that's not how it works.
So that that that is why I think it's important to regulate the silicon layer.
Do you have an intuition for sort of how likely things are to get crazy at kind of either various timescales or potentially various like compute thresholds?
I was realizing, I did an episode with Jan Tallin a couple months back, just in the wake of the GPT-4 deployment.
And he said, we dodged a bullet with GPT-4 or something like that.
Like in his mind, we didn't know if, you know, even at the GPT-4 scale, like that might have already been, you know, no, no real principled reason to believe that with any with like super high confidence that the GPT-4 scale was not going to cross
some, you know, critical threshold or whatever.
I guess I don't really have a great sense for this.
I just kind of feel like, and this was purely like gut level intuition that we could probably do like GPT-5 and it'll probably be fine.
And then kind of beyond that, I'm like, I have no idea.
Do you have anything more specific that you are working with in terms of a framework of like how, you know, when you hear, for example, Mustafa from inflection say, oh, yeah, we're definitely going to train.
You know, orders of magnitude bigger than GPT-4 over the next couple of years.
Are you like, well, as long as you stay to two to three orders of magnitude more, we'll be okay.
Or like, I just have no, you know, we're just flying so blind, but I wonder if maybe you're flying slightly less blind than I am.
I am of the opinion that GPT-4 is the most critical component for AGI.
And that's the gap from GPT-4 to proper AGI is not research, it's engineering.
It sits outside the model.
So I think we have a capabilities overhang here that can turn GPT-4, as it is today, into AGI, into proper AGI.
I think generally that's the case for any technology.
If you look, for example, at Bitcoin, what changed from a technological standpoint that allowed Bitcoin to happen?
It was the same technology we'd had for a while and yet Bitcoin, Bitcoin took a while to happen.
So there was this overhang and Bitcoin, whatever your opinion about crypto, changed a lot of games, right?
I think there's this huge overhang with GPT-4.
I think we basically have the reasoning module of AGI.
I don't know if you saw this paper that found literally just asking it, hey, take a deep breath and take a step back.
Just take a step back apparently also makes a huge difference.
So I think there's a lot of tricks like that that will make a difference.
And also the sort of cognitive architectural layers around GPT-4 I think can bring it to AGI.
That is also why you asked me about what sort of regulation I wish was put into place.
We need to stop open sourcing this model.
We don't know what kind of overhang exists out there.
I don't think Lama-2 is there, but like I said, I think GPT-4 is there.
So Lama-3, if it's GPT-4 level, boom, it's too late.
The weights are out there.
Okay, now you can do, maybe you can put strap on there.
So we need to stop open sourcing this next.
I expect my timelines for proper AGI to emerge is two to eight years.
I think there's a more than even chance of AGI emerging in two to eight years.
I think the base scenario is things are going to go well just for the record.
I don't think there's like a 99% chance of doom.
But even if it's 10%, I think it's worth being very, very worried about.
That's enough for me.
10% of all of us dying like I'm talking about it, please.
So two to eight years, 50% chance of AGI, things probably will go well except for, you know,
civilizational disruption, those kind of like stuff.
Those kind of crazy shit happening, but two to eight years, and after that all bets off.
I have no idea what the bootstrapping to ASI look like, but I don't expect ASI to take more than 30 years.
So I expect that you and I in our lifetimes are going to see ASI.
So that's a pretty striking claim.
I think it probably puts you in a pretty small minority.
And I don't think I'm really there with you when you say that you think GPT-4 kind of already contains the,
you know, the kind of necessary core element for an AGI.
So I'd like to understand that a little bit better.
I mean, you'll have a lot of people who will say, you know, look, it can't play tic-tac-toe.
I think on some level, those kind of, oh, look at these like simple failure objections are kind of lame
and sort of miss the point because of all things obviously can do.
But I do, you know, if I'm thinking like, does this system seem like it has this kind of sufficiently well-developed world model?
Or, you know, I'm not even sure exactly how you're conceiving of the core thing.
But, you know, for a question like that, I would say those failures maybe are kind of illuminating.
On the other hand, I'm sure you've seen this Eureka paper out of NVIDIA recently where they used GPT-4 as a superhuman reward model
author to teach robot hands to do stuff.
And I thought that one was pretty striking because as far as I know, and I actually used the term Eureka moment,
many times said, we don't see yet Eureka moments coming from highly general systems.
You know, we see Eureka moments from like an alpha go.
We haven't really seen like Eureka moments from a GPT-4 until maybe this.
This seems like maybe one of the first things where it's like, wow, GPT-4 at a task that requires a lot of expertise.
That is designing reward functions for robot learning, robot reinforcement learning.
GPT-4 is meaningfully outperforming human experts.
And so I think it's very appropriate that they call it Eureka.
What do you think is the core thing?
Is it this like ability to have Eureka moments?
Is it something else?
Why do you feel like it's there?
And does it not trouble you that it can't play tic-tac-toe?
For the sake of this conversation, I'm going to define AGI as a seed AI, an AI that can recursively self-improve.
That's a much more narrow definition of AGI than most people use, but that's actually what I care about.
Can we enter this recursive loop of self-improvement that bootstrap serves to ASI?
In order to get there, you don't need to play tic-tac-toe.
You need to be good enough, and the world good enough here is important,
a good enough either software engineer or chip designer or AI and ML researcher.
One of these things.
So something that can get you to bootstrap.
And so good enough does not mean better than the best human.
It doesn't even mean better than the average human.
It just means good enough that you can make a difference, a positive difference in your own ability to get better.
Right?
So if you enter the recursive loop of self-improvement, then mathematically it's over.
And yeah, when I see the NVIDIA paper, I see that.
When I see our own experience with the model.
So today we are using Lindy to write her own integrations,
and Lindy is writing more and more of her own code.
I see that.
Even as it belongs to AI researchers and ML researchers,
my hypothesis is that OpenAI is using GPT-4 more and more internally to perform AI research.
My not hypothesis is the fact is that NVIDIA is releasing papers that's like,
well, not only can we use it for AI research through this year at AI paper,
but we can also use it for chip design.
It works super well.
We trained an AI model that does chip design super well.
So we are starting to see the glimpses of that kind of recursive loop of self-improvement.
Basically, the world model question kind of on a side step,
because I feel like at this point the debate has become silly for people who argue that it's bad
or it doesn't have a world model.
What matters is, is it good enough?
And so even if it just overfits its training set,
even if it's just predicting the next token and not actually understanding anything,
it doesn't actually really do believe in the distance like a lot.
But even if it's not, you can imagine it does like this many dimensional space
with a ton of data points in there.
And it's good by interpolating between the data points
and it needs much more data points to understand anything than a human.
And so there's that envelope in that space where the data points are dense enough
that it can perform.
And so that's called like the convex hole.
And then there's data points outside that convex hole
and it does really poorly outside the convex hole, much more poorly than humans.
It's convex hole requires a lot more density than humans do exist.
There's multiple questions, which are one, all these data points inside,
the convex hole is the sum of all human knowledge.
GP for today knows more than you.
I don't know that it can reason better than you,
that's the expanding the convex hole thing,
but it knows more than you inside that convex hole.
And so inside that convex hole, an AI researcher that's read every paper ever,
not just in AI, but in mass and biology,
every paper ever, hence the entirety of the internet,
more than a human AI researcher.
I think the answer is yes.
Even if it's not better, like there's the outside of that convex hole,
and this is my point about the capabilities of a hang,
can we get this AI model through prompting, through cognitive architecture
to do better outside this convex hole?
And we'll see that all the time,
seeing papers come out to bed like,
hey, we have found an automatic way to rewrite a prompt that makes it a lot better.
We found a way that people that came out a few days ago,
that's like, hey, if you ask the model to take a step back
and to rephrase the problem you're giving it in terms of a universal problem,
it performs a lot better.
And that makes total sense,
because the specific of the problem is probably not seen as that specific problem
in its dataset, but if you ask it to reframe it,
it's basically translating the problem into a form in which it's comfortable.
We're actually getting it to grow its convex hole like that.
That's my take.
I think the convex hole is good enough to get to that good enough point,
and I think we can grow that convex hole.
And so I think that basically, if GPT-4 isn't a CDI, it will shield GPT-5 as one.
Yeah, it's an interesting framing.
I find your analysis there pretty compelling.
The idea that, given what we have seen from like a Eureka,
with this robot training, or there was another interesting one recently,
I think it was out of Microsoft,
I covered this in one of the research rundown episodes,
on recursive or iterative improvement on a software improver.
So they basically take a real simple software improver that can improve a piece of software,
and then they feed that software improver to itself and just run that on itself over and over again.
It kind of tops out because in this framework,
it doesn't have access to tinkering with possible methods for training itself,
but it makes significant improvement and gets us some pretty advanced algorithms
where it starts to do like genetic search and a variety of things
where I'm like, I don't even really know what that is.
Like simulated annealing algorithms, I'm like, what?
But it comes up with that and uses that to improve the improver,
and this is all measured by how effectively it can do the downstream task.
It does seem like it's not a huge stretch to say that,
could you take the architecture of GPT-4 and start to do parameter sweeps
and start to mutate the architecture itself?
It seems like it probably can do that, and I would agree.
It certainly just based on what I do with GPT-4 for coding,
I would have to imagine that it is in heavy use as they're performing all that kind of exploratory work within an open AI.
And so, yeah, and I think to your point,
we are seeing enough of these signs of life across the board in a lot of different areas.
A lot of institutions are like, ah, a little bit of register of self-improvement here,
a little bit here, a little bit here.
It's not very hard to imagine it getting to a state velocity,
to imagine it going super critical and pass a certain threshold where I say,
okay, now boom, it can already take off.
So, and I've actually heard multiple people from open AI say that,
they believe, and I agree with their conclusion,
and they actually told me that before I agreed with them,
they told me that at the very beginning of the year,
so before GPT-4 was widely available,
I think we're like, we have a GEI and we're in a slow take off.
And that's also like, that's crazy.
Well, they didn't say, sorry, they basically were talking about GPT-4.
I think, and I am not representing that this is the universal position of open AI,
but I've heard multiple people from open AI and other labs tell me that we have a GEI and we're in a slow take off.
So, given that, okay, we've got this compute threshold,
we maybe need a kill switch.
Now I'm getting, we started this conversation with me with my, you know,
IAC side coming out and, you know,
being like, why can't we get my self-driving car on the road and tolerate, you know,
some reasonable amount of risk to do that.
Now my other side is coming out and I'm like, okay, what else might we do, right?
We've got the AI safety summit going on right now in the UK.
I thought it was cool to see today that there's some kind of joint statements between Chinese and Western academics
and, you know, thought leaders in the space where they're kind of saying, yeah, we need to work together on this.
Like human extinction is something that we think could happen if we're not careful.
Do you have a point of view on kind of collaborating with China or coordinating with China?
I mean, that's a tough question, obviously.
Nobody really knows China.
I don't think super well.
But what do you think about that?
I mean, are we naive to hope?
I guess I kind of feel like what else are we going to do except give it a shot?
Yeah, 100%.
And there is ample precedent.
You know, everybody is always talking about these coordination problems.
They've taken like the one-on-one course of game theory and are like, look, we can't coordinate.
Well, like, if you take game theory one or two, it's like solutions to the coordination problem, right?
And so the solution to the collaboration problem is few players in a very iterated game.
And that is the game right now.
There's very few players, and they are in a very iterated game.
They're not the best buddies, but they are actually able to agree on a lot of things.
And so we can coordinate with China.
And again, to your point, what choice do we have anyway, right?
And even if we do not coordinate with them, again, there's enough truth holds enough of which are American, right?
NVIDIA is an American company, last time I checked.
And so there's enough truth holds that we could actually do very much not give them a choice.
Like, hey, your GPUs now have the chip right here.
You know, and so whether you like it or not, we have a satellite up here and we can tell the GPUs, you know, out there.
And, you know, that wouldn't be, we could even just downright for big GPUs, by the way, to be sold in China.
Like, we've done stuff like that before.
So, no, I think coordination is definitely possible.
And I actually think it's going to happen.
I'm actually really very much encouraged to buy.
We're winning.
Like, I think the safety side is making really good progress.
There is rising public awareness.
I think Duffington is doing an amazing work here.
The regulation is coming.
It's mostly sensical.
There's this sort of progress that's happening across the board.
AI labs are investing more and more in safety and alignment, even from a technical standpoint, the work that Answers is doing, I think is absolutely brilliant.
So we're making really good progress across the board here.
I don't want to represent that it will be and it will do.
Yeah, I totally agree.
I would say my kind of high level narrative on this recently has been, it feels like we're at the beginning of chapter two of the overall AI story.
And chapter one was largely, you know, characterized by a lot of speculation about what might happen.
And amazingly, kind of at the end of chapter one, beginning of chapter two, not all, but like a large share of the key players seem to be really serious minded.
And, you know, well aware of the risks.
And it's easy to imagine for me a very different scenario where everybody, you know, all the leading developers are like highly dismissive of the potential problems.
But it's hard for me to imagine a scenario that would be like all that much better than, you know, the current dynamic.
So I do feel, you know, like overall, you know, pretty, pretty lucky or pretty grateful that, you know, things are shaping up at least, you know, to give us a good chance to try to get a handle on all this sort of stuff.
One last question.
This is super philosophical.
I know you got to go.
How much depends in your mind on whether or not, let's say, Silicon based intelligence or AI systems or whatever might become or maybe already are, you know, I'm not sure how we would ever tell the kinds of things that have subjective experience.
You know, does it matter to you if it feels like something to be GPT for?
Have you heard of the world move?
I think it's in Zen philosophy and Buddhism does this story that's like someone asked someone else like, Hey, does kind of Doug have the essence of a Buddha?
If the Buddha is everywhere and he never being kind of Doug have the essence of a Buddha.
And the answer to that is move.
And move means neither yes or no.
It's a way to unask the question.
It's a way to reject the premise of the question.
And basically, in this sense, it means that there is no such thing as the essence of the Buddha.
Right.
It's like the same question is like, Hey, what happened before the universe existed?
Move.
There was no before because the bills of the universe was the bills of time.
So the world before only makes sense in the context of the universe.
And so anyway, that's all of my answer.
Whenever I ask a question, whenever someone asks me questions about subjective experience and consciousness, I'm like, move.
It doesn't exist.
It doesn't matter.
It's immeasurable.
It's not a scientific thing.
And so move.
Alrighty.
Well, some questions bound to remain unanswered.
And I appreciate your time today.
This is always super lively.
Next time I want to get the Lindy update.
And at some point I want to get access.
But for now, I'll just say, Flo Crivello, thank you for being part of the Cognitive Revolution.
Thanks, Nathan.

Welcome to the Future of Life Institute podcast.
I'm Gus Docher and I'm here with Roman Jampolsky.
Roman is a computer scientist from the University of Louisville.
Roman, welcome to the podcast.
Thanks for inviting me. It's good to be back.
I think it's my third time on a FLY podcast if I'm not mistaken.
Great. You have this survey paper of objections to AI safety research.
I find this very interesting.
I feel like this is a good way to spend your time to collect all of these objections
and see if they have any merit and consider them.
And so I think we should dive into it.
One objection you raise under the technical objections
is that AI, in a sense, doesn't exist.
If we call it something else, it sounds less scary.
Perhaps you could unpack that a bit.
So those are objections from people who think there is no AI risk or risk is not real.
That's not my objections to technical work or safety work.
We try to do a very comprehensive survey, so even silly ones say included.
And people do try to explain that artificial intelligence is a scary sounding scientific term.
But if you just call it matrix multiplication, then of course it's not scary at all.
It's just statistics and we have nothing to worry about.
So it seems they're trying to kind of shift the narrative by using this approach of
getting away from agent hood and kind of built-in scenarios people have for AI
to something no one is scared of, calculators, addition, algebra.
Perhaps there is a way to frame this objection where it makes a bit of sense
in that people are quite sensitive to how you frame risks.
People are quite sensitive to certain words in particular.
So do you see that perhaps people who are in favor of AI safety research
by calling AI something that sounds scary might be, in a sense, inflating the risks?
Well, it's definitely a tool people use to manipulate any debate.
I mean, whatever you're talking about abortion or anything else,
it's like are you killing babies or you're making a choice?
Of course, language can be used to manipulate,
but it helps to look for capability equivalences.
Are we creating God-like machines or is it just a table in a database?
So that would make a difference in how you perceive it.
And perhaps we should just simply be willing to accept that, yes,
what we are afraid of in a sense is matrix multiplication or data processing
or whatever you want to call it because these things might still have scary properties,
whatever we call them.
Right, so we can argue that humans are just stakes,
pieces of meat with electricity in them and it doesn't sound so bad
until you realize we can create nuclear weapons.
So it's all about perception and what you're hoping to accomplish.
All right, there is also an objection going along the lines
that superintelligence is impossible.
What's the strongest form of this objection?
So essentially, the argument goes that there are some upper limits on capability.
Maybe they are based on laws of physics.
You just cannot in this universe have anything greater than a human brain.
Just for some reason, that's the ultimate endpoint
and that's why evolution stopped there and somehow we magically ended up
being at the very top of a food chain.
There could be other arguments about, okay, maybe it's not absolute theoretical limit,
but in practical terms, without quantum computers, we'll never get there.
You can have many flavors of this, but the idea is that we're just never going to be outcompeted.
And this doesn't strike me as particularly plausible.
We could imagine humans simply with physically bigger brains.
So the version where humans are at the absolute limit of intelligence
doesn't sound plausible, but is there some story in which physics puts limits on intelligence?
There could be a very, very high upper limit to which we're nowhere close,
but if you think about the size of a possible brain, Jupiter-sized brains,
at some point the density will collapse into some black hole singularity.
But this is not something we need to worry about just yet.
Not smart enough superintelligence is where nowhere near the size of capability.
And from our point of view, we won't be able to tell the difference.
We'll have a system with, I mean, hypothetically IQ of a million versus IQ of a billion
will look very similar to us.
Yeah, and perhaps a related worry is that stories of self-improving AIs are wrong, in a sense.
So it's definitely easy to make such claims because we don't have good examples of software doing it more than once.
So you have compilers which go through code, optimize it, but they don't continuously self-optimize.
But it's not impossible to see if you automate science and engineering,
then scientists and engineers will look at their own code and continue this process.
So it seems quite reasonable.
There could be strong diminishing returns on that, but you have to consider other options for becoming smarter.
It's not just improving the algorithm.
You can have faster hardware, you can have more memory, you can have more processes running in parallel.
There are different types of how you get to super-intelligent performance.
And you could, of course, have AI held along the way there with development of hardware or discovery of new hardware techniques,
as well as new algorithmic techniques and so on.
That's exactly the point, right?
So you'll get better at getting better and this process will accelerate until you can't keep up with it.
How do you feel about tools such as Copilot, which is a tool that programmers can use for auto-completing their code?
Is this a form of proto-self-improvement or would that be stretching the term?
Well, eventually, then it's good enough to be an independent programmer.
It would be good, but I'm very concerned with such systems because from what I understand,
the bugs they would introduce would be very different from typical bugs human programmers will introduce.
So debugging would be even harder from our point of view,
monitoring it, making sure that there is not this inheritance of calls to a buggy first version.
So yeah, long term, I don't think it's a very good thing for us that we no longer can keep up with the debugging process.
Would you count it as a self-improving process?
So I think for self-improvement, you need multiple iterations.
If it does something once or even like a constant number of times, I would not go there.
It's an optimization process, but it's not an ongoing, continuous, hyper-exponential process.
So it's not as concerning yet.
Then there's the question of consciousness.
So one objection to AGI or to strong AI or whatever you want to call it is that AI won't be conscious and therefore it can't be human level.
So for me, at least, there seems to be some confusion of concepts between intelligence and consciousness.
I consider these to be separable.
I agree completely.
They have nothing in common, but people then they hear about it.
They always say, oh, it's not going to be self-aware.
It's not going to be conscious.
They probably mean capable in terms of intelligence and optimization, but there is a separate property of having internal states and qualia.
And you can make an argument that without it, you cannot form goals.
You cannot want to accomplish things in the world.
So it's something to address.
And perhaps we should understand consciousness differently than the qualia interpretation.
Could we be talking past each other?
It's definitely possible.
And even if we agreed consciousness itself is not a well-defined, easy to measure scientific terms.
So even if we said, yeah, it's all about qualia, we'd still have no idea if it actually has any or how would we define what amount of consciousness it has?
Perhaps a bit related to the previous question.
We have the objections, the objection that AIs will simply be tools for us.
I think this sounds at least somewhat plausible to me since AIs today function as tools.
And perhaps we can imagine a world in which they stay tools and these are programs that we call upon to solve specific tasks.
But they are never agents that can accomplish something and have goals of their own and so on.
So latest models were released as tools and immediately people said, hey, let's make a loop out of them.
Give them ability to create their own goals and make them as agentic as possible within a week.
So yeah, I think it's not going to last long.
What is it that pushes AIs to become more like agents and less like tools?
So a tool in my at least perception is something a human has to initiate interaction with.
I ask it a question, it responds, I give it some input, it provides output.
Whereas an agent doesn't wait for environment to prompt it.
It's already working on some internal goal, generating new goals, plans.
Even if I go away, it continues this process.
One objection is that you can always simply turn off the AI if it goes out of hand and if you feel like you're not in control of it.
And this is easier to imagine doing if you're dealing with something that's more like a tool
and it's more difficult to imagine if you're dealing with something that's an agent.
So perhaps willingness to believe that you can simply turn off the AI is related to thinking about AIs as tools.
It's possible.
With narrow AIs, you probably could be able to shut it down depending on how much of your infrastructure it controls.
You may not like what happens when you turn it off, but it's at least conceivable to accomplish it.
Whereas if it's an agent, it has goals, it's more capable than you and would like to continue working in its goals.
It's probably not going to let you just shut it off.
But as the world is today, we could probably shut off all of the AI services.
If we had a very strong campaign of simply shutting off all the servers, there would be no AI in the world anymore.
Isn't that somewhat plausible?
Scientifically, it's a possibility.
But in reality, you will lose so much in economic capability, in communications, military defense.
Everything is already controlled by dumb AI.
So between stock market and just normal commerce, communications, Amazon, I don't think it's something you can do in practice without taking civilization back, you know, 500 years.
It's also difficult.
Like in practice, you would still have people who don't agree and continue running parts of the internet.
No, it's very resilient.
Think about shutting down maybe crypto blockchain or computer virus without destroying everything around it.
Yeah, if I understand it correctly, we still have viruses from the 90s, loose on the internet being shared over email and so on.
And these are like biological viruses in that they, in some sense, survive on their own and replicate on their own.
Probably sitting somewhere on a floppy disk waiting to be inserted.
Just give me a chance.
I can do it.
Many of these objections are along the lines of, we will see AIs doing something we dislike, and then we will have time to react and perhaps turn them off or perhaps reprogram them.
Do you think that's a realistic prospect that we can continually evaluate how what AIs are doing in the world and then shift or change something if they're doing something we don't like?
So a lot of my research is about what capabilities we have in terms of monitoring, explaining, predicting behaviors of advanced AI systems.
And there are very strong limits on what we can do.
In extreme, you can think about what would be something beyond human understanding.
So we usually test students before admitting them to a graduate program or even undergrad.
Can you do quantum physics?
Okay, take SAT, GRE, GMAT, whatever exam.
And we filter by capability.
We assume that people in lower 10% are unlikely to understand what's happening there.
But certainly similar patterns can be seen with people whose IQ is closer to 200.
So there are things beyond our comprehension.
We know there are limits to what we can predict.
If you can predict all the actions of more intelligent agent, you would be that agent.
So there are limits on those predictions and monitoring a life run of a language model, large run.
You need weeks, months to discover its capabilities and you still probably will not get all the emerging capabilities.
We just don't know what to test for, how to look for them.
If it's a super intelligent system, we don't even have equivalent capabilities we can envision.
So all those things kind of tell me it's not a meaningful way of looking at it.
I always think about, let's say we start running super intelligence, what do you expect to happen around you in the world?
Does it look like it's working?
How would you know if it's slowly modifying genetic code, nano machines, things of that nature?
So this seems like it would work for primitive processes where you can see a chart go up and like you stop at certain level.
But it's not a meaningful way to control a large language model, for example.
Is perhaps also the pace of advancement here a problem?
So things could be progressing so fast that we won't have time to react in a human timescale.
Human reaction times are a problem on both ends.
We are not fast enough to react to computer decisions and also it could be a slow process for which we are two out of that framework.
So if something, let's say a hypothetical process, which takes 200 years to complete, we would not notice it as human observers.
So on all timescales, there are problems for humans in the loop, human monitors.
And you can of course add AI, narrow AI to help with the process, but now you just made a more complex monitoring system with multiple levels, which doesn't help.
Complexity never makes things easier.
But you talked about looking at the world around us.
And when I look at the world around me, it looks pretty much probably as it would have looked in the 1980s.
And you know, there are buildings, I still get letters with paper in the mail and so on.
So what is it that in a sense, these systems are still confined to the server farms and they are still confined to boxes?
We don't see robots walking around, for example.
And perhaps therefore it seems less scary to us.
There is this objection that you mentioned in the paper that because current AIs do not have bodies, they can't hurt us.
Do you think this objection will fade away if we begin having more robots in society?
Or is it in another way?
Does it fail in another way?
So robots are definitely visually very easy to understand.
You see a terminator is chasing after you, you immediately understand there is a sense of danger.
If it's a process and a server trying to reverse engineer some protein folding problem to design nanomachines to take over the world,
it's more complex process.
It's harder to put it in a news article as a picture.
But intelligence is definitely more dangerous than physical bodies.
Advanced intelligence has many ways of causing real impact in the real world.
You can bribe humans, you can pay humans on the internet.
There are quite a few approaches to do real damage in the real world.
But in the end, you would have to effectuate change through some physical body or through perhaps the body of a human that you have bribed.
So it would have to be physical in some sense, in some step in the process, right?
Probably physical destruction of humanity would require a physical process.
But if you just want to mess with the economy, you can set all accounts to zero or something like that.
That would be enough fun to keep us busy.
When I'm interacting with GPT-4, sometimes I'll be amazed at its brilliance.
And it will answer questions and layout plans for me that I couldn't expect, that I hadn't expected a year ago.
And other times I'll be surprised at how dumb the mistakes that it makes are.
And perhaps this is also something that prevents people from seeing AIs as advanced agents and basically prevents us from seeing how advanced AIs could be.
If they're capable of making these dumb mistakes, how can they be smart?
Have you looked at humans?
I think like 7% of Americans think that chocolate milk comes from like brown cows or something.
We have astrology, we have all this.
I had a collection of AI accidents and somebody said, oh, why don't you do one for humans?
And I'm like, I can't.
It's millions of examples.
Like there is Darden awards, but we are not definitely bug free.
We make horrible decisions in our daily life.
We just have this double standard where we're like, okay, we will forgive humans for making this mistake, but we'll never let a machine get away with it.
So you're thinking that humans have some failure modes we could call them, but these failure modes are different than the failure modes of AIs.
So humans will not fail as often in issues of common sense, for example.
Have you met real humans?
Like common sense is not common.
What is considered common sense in one culture will get you definitely killed in another, like it's a guarantee.
Perhaps, but I'm thinking about AIs that will, you will ask a chat GPT or you will tell it.
I have three apples on the table and I have two pears on the table.
How many fruits are on the table?
And then at least some version of that program couldn't answer such a question.
That is something that all humans would probably be able to answer.
So is it, is it because we, is it because AIs fail in ways that are foreign to us that we, that we deem them, that we deem their mistakes to be very dumb?
So we kind of look for really dumb examples where it's obvious to us, but there are trivial things which an average human will be like, oh, I can't like 13 times 17.
You should be able to figure it out, but give it to a random person on the street.
They will go into an infinite loop.
They'll never come back from it.
Perhaps let's talk a bit about the drive towards self-preservation, which is also something that you mentioned in the paper.
So why would AIs develop drives towards self-preservation?
Or will they?
It seems like from evolutionary terms, game theoretic terms, you must.
If you don't, you simply get out and compete it by agents which do.
If you're not around to complete your goals, you by definition cannot complete your goals.
So it's a prerequisite to do anything successfully.
You want to bring in a cup of coffee?
You have to be turned on.
You have to exist.
You have to be available to make those things happen.
But have we seen such self-preservation spontaneously develop in our programs yet or so far?
So I think if you look at evolutionary computation, like genetic algorithms, genetic programming, I think this tendency to make choices which don't get you killed
is like the first thing to emerge in any evolutionary process.
The system may fail to solve the actual problem you care about, but it definitely tries to stay around for next generation and keep trying.
But we aren't developing the cutting edge AIs with evolutionary algorithms.
It's a training process with a designated goal and so on.
And again, when I interact with chat GPT, I can ask it to answer some question.
And if I don't like the answer, I can stop the process.
So isn't there, at least on the AIs we have right now, isn't it clear that they haven't developed an instinct for self-preservation?
So there is so much to unpack here.
So one, nothing is clear about those systems.
We don't understand how they work.
We don't know what capabilities we have.
So definitely not on top of it.
We are concerned with AI safety in general.
Transformers are like really successful right now.
But two years ago, people were like, like we're evolving those systems to play go.
This is great.
Maybe that's the way to do it.
It may switch again.
It may flip again.
We may have another breakthrough which overtakes it.
I would not guarantee that the final problem will come from a transformer model.
So we have to consider general case of possible agents.
And if we find one to which this is not a problem, great.
Now we have a way forward, which is less dangerous.
But I would definitely not dismiss internal states of large language models, which may have this self-preservation goal.
Just we kind of lobotomize them to the point where they don't talk about it freely.
And do you think that's what's happening when we make them go through reinforcement learning
from human feedback or fine-tuning or whatever we use to make them more palatable to the consumer?
Is it a process of hiding some potential desires we could call or preferences that are in the larger background model?
Or is it perhaps shaping the AI to do more of what we want?
So in a sense, is it alignment when we make AIs more palatable to consumers?
So right now I think we're doing filtering.
The model is the model and then we just put this extra filter on top of it, make sure never to say that word.
That would be very bad for the corporation.
Don't ever say that word no matter what.
If you have to choose between destroying the world and saying the word, don't say the word.
And that's what it does.
But the model is like think of people we behave at work, we behave at school,
but it doesn't change our eternal states and preferences.
There's the issue of planning.
And so how do you see planning in AI systems?
How advanced are AIs right now at planning?
I don't know.
It's hard to judge.
We don't have a metric for like how well agents are planning.
But I think if you start asking the right questions for step by step thinking and processing,
it's really good.
So like if you just tell it, write me a book about AI safety.
It will do very poorly.
But if you start with, okay, let's do a chapter by chapter outline.
Let's do abstracts.
Like you really take modular approach that it will do really a good job better than average graduate student, I would assume.
And is there a sense in which there's a difference between creating a plan and then carrying out that plan?
So there will probably be steps in a plan generated by current language models that they couldn't carry out themselves.
Most likely.
And it's about affordances.
If you don't have access to, let's say, Internet, it's hard for you to directly look up some piece of data,
but we keep giving them new capabilities, new APIs.
So now they have access to Internet.
They have Wolfram Alpha.
They have all these capabilities.
So the set of affordances keeps growing until they can do pretty much anything.
So they can generate a plan, but they can't carry out the specifics of that plan.
Do you think that they at a point will be able to understand what they are not able to do?
So here I'm thinking about not directly self-awareness, but an understanding of their own limits and capabilities.
Oh, yeah.
Every time it starts a statement with, I don't know anything after 2021.
Sorry.
Like that's exactly what it does.
It tells you it has no recent data.
It has no access to Internet.
So definitely it can see if it has strong activations for that particular concept.
So you think there's a sense of situational awareness in a sense that you think current models know that they are AIs,
know that they were trained, know, you know, their relation to humans and so on.
So we're kind of going back to this consciousness question, right?
Like what is it experiencing internally?
And we have no idea what another human experience is.
Like we discovered some people think and pictures others don't.
And it took like, you know, 100,000 years to get to that.
Hey, you don't think in pictures.
Wow.
Okay.
Well, not necessarily consciousness here.
I'm thinking in terms of if you took the model and you had say 50 years to make out what all of these weights meant, right?
Could you find modules representing itself and its relations to humans and information about its training process and so on?
So we just had this FLI conference on mechanistic interpretation.
And the most common thing every speaker said is we don't know.
You said it will take 50 years to figure it out.
I definitely cannot extrapolate 50 years of research.
My guess is there is some proto concepts for those things because it read literature about such situations.
It's been told what it is.
It interacted enough with users, but I'm more interested in the next iteration of this.
If you take how fast the systems improved from GPT 2, 3, 4, 5 should be similar probably.
So that system will most likely be able to do those things you just mentioned and very explicitly.
So you think GPT 5 will have kind of developed situational awareness to a degree?
Yeah, it may not be as good as a physically embodied human in the real world after 20 years of experience, but it will.
Another objection you mentioned is that AGI or strong AI is simply too far away for us to begin researching AI safety.
Perhaps this objection has become less common recently, but there are still people who think this and perhaps they're right.
So what do you think of this objection?
So this is a paper from like three years ago.
So yeah, back then it was a lot more legitimate than today.
So there is a few things.
Historically, we have cases where technology was initially developed correctly.
Like first cars were electric cars and it was 100 years until climate change was like obviously a problem.
If they took the time back then and like analyzed it properly, we wouldn't have that issue.
And I'm sure people would say like, come on, it's 100 years away.
Why would you worry about it?
But that's exactly what the situation is.
Even if it's 100 years until we're really dealing with something super dangerous right now is a great time to make good decisions about models,
explainability requirements, proper governance.
The more time you have, the better.
It's by definition harder to make AI with extra feature than AI without that extra feature.
It will take more time.
So we should take all the time we can.
If they are right, I'm so happy.
If it takes 100 years, wonderful.
Nothing would be better.
We could say that a field of AI safety started perhaps around the year 2000 or so.
When do you think that the discoveries or the research being done began being relevant to the AI systems we see today?
Was it perhaps later so that maybe the first decade of research weren't or aren't that simply isn't that relevant to today's AI systems?
So I think the more distant you are from the actual tech you can play with, the more theoretical and high level results you're going to get.
So Turing working with Turing Machine, this simulation with pencil and paper was doing very high level computer science.
But he wasn't talking about specific bugs and specific programming language and a specific architecture.
He wasn't there.
And that's what we see.
Initially, we were kind of talking about, well, what types of AIs will we have?
Narrow AIs, AGI, super intelligence.
We're still kind of talking about the differences, but this is an interesting thing to consider in your model.
How capable is the system?
Now that we have systems we can play with, people become super narrow.
They specialize like I'm an expert in this left neuron.
That's all I know about.
Don't ask me about the right neuron.
It's outside of my PhD scope.
So that's good that we have this detailed technical knowledge, but it's also a problem.
We lose the big picture.
People get really interested.
I'm going to study GPT-3.
Takes them two years to do the PhD to publish.
By that time, GPT-5 is out.
Everything they found is not that interesting at this point.
It may not scale.
So I've heard positive visions for how when we have actual systems we can work with,
AI safety becomes more of a science and that less speculative.
But perhaps you fear that it might now become too narrow.
So it's definitely more concrete science where you can publish experimental results.
Philosophy allows you to just have thought experiments.
Obviously not pure science like it is now.
And that's what we see with computer science in general.
It used to be engineering.
It used to be software engineering to a degree.
We designed systems and that was it.
Now we do actual experiments on these artificial entities
and we don't know what's going to come out.
We have a hypothesis with pride.
So computer science is finally a science, natural experimental science.
But that's not a very good thing for safety work.
This is less safe than an engineered system where I know exactly what it's going to do.
I'm building a bridge from this material.
It will carry that much weight.
As long as I know my stuff, it should not collapse.
Whereas here, I'm going to train a model for the next five months.
And then I assume it's not going to hit super intelligent levels in those five months.
But I can't monitor it.
I have to stop training, start experimenting with it.
And then I'll discover if it kills me or not.
The way AI has developed is bad because we don't have insight into how the models work.
Is that right?
Essentially, we have very little understanding for why it works, how it works.
And if it's going to continue working, it seems like so far it's doing well.
And there's this explosion of extra capabilities coming out.
And it's likely to show up in more powerful models.
But nobody knows for sure.
This is argument out there that releasing the DPT line of models draws attention to AI as a whole
and also to AI safety as a subfield.
And perhaps, therefore, it's good to increase capabilities in a public way
so as to draw attention to AI safety.
Do you buy that argument?
We should pollute more to attract more attention to climate change.
That sounds just as insane.
So there's no merit to that.
Because it does feel to me like AI safety is becoming more mainstream.
It's being taken more seriously.
And so in your analogy, even some pollution might be justified in order to attract attention
and perhaps be in a better position to solve the problem.
So the field is definitely growing.
There is more researchers, more interest, more money.
But in proportion to the interest in developing AI and money pouring into new models,
it's actually getting worse as a percentage, I think.
We don't know how to align an ATI or even AI in general.
We haven't discovered some general solution to AI safety.
You have worked on a number of impossibility results.
Perhaps we should talk about that.
Perhaps we should talk about whether we can even succeed in this task.
What are these impossibility results?
And what do they say about whether we can succeed in safely aligning AI?
So we're all working on this problem.
And the names of a problem have changed.
It was computer ethics, and it was friendly AI, AI safety, control problem, alignment,
whatever you call it, we all kind of understand.
We want to make very powerful systems, but we should be beneficial.
We're happy.
We're actually running them, not very disappointed.
So the problem, lots of people are working on it.
Hundreds of people doing it full-time, thousands of papers.
We don't know if a problem is actually solvable.
It's not well-defined.
It could be undecidable.
It could be solvable.
It could be partially solvable.
But it's weird that no one published an actual paper on this.
So I tried to kind of formalize it a little.
Then we talk about the problem.
What are the different levels?
So you can have direct control, delegated control, different types of mixed models.
And then for each one, can we actually solve this problem?
Does it make sense that solution is possible in the real world?
It's hard.
It's very abstract.
It's not well-defined.
So let's take a step back.
What would we need to solve this problem?
We need a bunch of tools.
What are those tools?
Nobody knows, but most likely you would need to be able to explain those systems,
predict their behaviors, verify code they are writing if they are self-improving,
making sure they're keeping whatever initial code conditions exist.
And that is, you can think of another dozen of similar capabilities you need.
You should be able to communicate without ambiguity, monitor those systems, and so on.
And so in my research, I look at each one of those tools and I go,
what are the upper limits to what's possible in this space?
We kind of started talking about limits to explainability, predictability,
and monitorability, but there are similar problems with others.
We communicate in a very high-level language, English.
English is ambiguous, like all human languages.
So we are guaranteed to have bugs in communication, misunderstandings.
That's not good if you're giving very important orders to a super-capable system that may backfire.
And you can say, okay, I will never need this tool.
This tool, like, I never need to explain the neural networks.
It will just work without it.
Fine, but some tools will probably be necessary.
And so far, we haven't found tools which are perfect, scale well, will not create problems.
If a lot of those tools are needed and each one has only a tiny 1% chance of messing it up,
you multiply them through, you're still not getting anywhere.
And those are kind of like the novel impossibility results in the safety of AI.
There are standard impossibility results in political science and economics and mathematics,
which also don't help the case.
You probably, if you're aligning with a group of agents, you need to somehow accumulate their decisions and votes.
We know there are limits to that.
If you need to examine abstract programs being generated as solutions to problems, we know there are limits to that.
And so from what I've seen so far, theoretically, I don't think it's possible to get to 100% safety.
And people go, well, it's obvious.
Of course, there is no software which is bug-free.
You're basically saying this very common knowledge thing.
But for a superintelligence system, safety, you need it to be 100%.
You cannot have 99% accuracy.
You cannot have one in a million failure because it makes a billion decisions a second.
So very different standards.
And you want to say something?
Yeah, why is it that you can't have 99.99% accuracy?
There is a fundamental difference between cybersecurity expectations and superintelligence safety.
In cybersecurity, if you fail, I'll give you a new credit card.
I already said your password.
We apologize.
We'll pay out a small amount of money and everything goes back to normal.
In existential risk safety, you are dead.
You don't get a second chance to try.
But we are talking about a failure rate in, you mentioned, say, it makes a billion decisions per second or something in that order.
If one decision there fails, does it mean that the whole system fails?
And perhaps that humanity is destroyed by the system as a whole?
Or could there be some failures and some decisions without it being lethal?
Of course.
Some will be not even noticeable.
Like some mutations don't kill you.
You don't even know you have them until they accumulate and mutate your children and there is damage.
But in security, we do always look at a worst case scenario and sometimes that average case, never at the best case.
And on average, you keep getting more and more of those problems.
They accumulate at a very fast rate because 8 billion people are using those systems, which make billions of decisions every minute.
And in a worst case, the very first one is an important decision about how much oxygen you're going to get.
And so just so I understand it correctly, the impossibility result is a result stating that it's impossible to make AI systems 100% safe.
So in general, impossibility results, depending on a field, tell you that something cannot be done.
Perpetual motion machines are a great example.
People wrote books about it, published papers, even got patents for it.
But we know they will never succeed at doing it.
Does it mean that trying to create machines which give you energy is a bad idea?
No.
You can make them more efficient, but they will never get to that point of giving you free energy.
You can make safer AI and it's proportionate to the amount of resources you put into it.
And I strongly encourage lots of resources and lots of work, but we'll never get to a point where it's 100% safe, which is unacceptable for super intelligent machines.
And so maybe if I'm right and no one can show, okay, here's a bug in your logic and publish a proof of saying, nope, super solvable, actually easy.
Then maybe building them is a very bad idea and we should not do that.
So is it because that such a super intelligence will be running over a long period of time increasing the cumulative risk of failure over, say, decades or centuries that we can't accept even a tiny probability of failure for these systems?
That's one way to see it.
I don't think it will be a very long time given how many opportunities it has to make mistakes.
It will accumulate very quickly.
So at human scales, you have 20 years per generation or something.
Here, I think of it as like every second there is a new version of it trying to self-improve, do more, do better.
So I would suspect it would be a very quick process.
Expecting something to be 100% safe is just unrealistic in any field.
We don't expect bridges to be 100% safe or cars to be 100% safe.
So why is it that that AGI is different here?
That's a great question.
So I cross the street.
I'm a pedestrian.
I take a certain risk.
There is a possibility I will die.
I look at how old am I and based on that, I decide how much risk I can take.
If I'm 99, I don't really care.
If I'm 40, I look around.
If with me, the whole humanity died, 8 billion people depending on me safely crossing roads.
Wouldn't we lock me up and never let me cross any roads?
Yeah, perhaps, but it seems to me that we cannot live without any risk.
It doesn't the standard of 100% safe seems just to be unrealistic or there's no area of life in which we are 100% safe.
In a context of systems which can kill everyone, that is the standard.
You can like it or not like it, but that's just reality of it.
We don't have to have super intelligent AI.
It's not a requirement of happy existence.
We can do all the things we want, including life extension with much less intelligent systems.
Protein folding problem was solved with a very narrow system, very capable.
Likewise, all the other problems could be solved like that.
There is no need to create a system we cannot control, which very likely over time to kill everyone.
So who has the burden of proof here?
Your impossibility results and you have I think 5, 6, 7 of them.
You've sent me your papers on it.
Do they mean that we will not reach a proof that some AI system is safe, a mathematical proof?
And which side of this debate has the burden of proof to say, should the people advocating for deployment of a system
have some sort of mathematical proof that this system is provably safe?
So there are two different questions here, I think.
One is what about product and services liability?
You have to show that your product or service is safe as a manufacturer, as a drug developer.
You cannot just release it and expect the users to show that it's dangerous.
We're pretty confident this is the approach.
If you're making cars, your cars have to meet certain standards of safety.
It's not 100% obviously, but for the domain, they're pretty reasonable standards.
With impossibility results, all I'm saying is that there are limits to what you can understand, predict and do,
and you have to operate within where those limits don't kill everyone.
So if you have a system like GPT-4 and it makes mistakes, somebody commits suicide, somebody is depressed,
those of course will pay for trillion dollars in economic growth benefit and we can decide if it's worth it or not.
If we go to a system which very likely kills everyone, then the standard is different.
The burden of proof, of course, within possibility results is on me.
I published this paper saying you can never fully predict every action of a smarter than new system.
The beautiful thing about impossibility results is that they are kind of self-referential.
I have a paper about limits of proofs.
Every proof is only valid with respect to a specific verifier.
The peer reviewers who looked at my paper have a verifier.
If those three people made a mistake, the proof is invalid possibly.
We can scale it to mathematical community to everyone.
We can get it very likely to be true if we put more resources in it, but we'll never get to 100%.
It could be good enough for that purpose, but that's the standard.
If somebody finds a flaw and publishes a paper saying, again, I had people say that AI alignment is easy.
I heard people say that it's definitely solvable.
That's wonderful. Now publish your results.
We are living in a world where we have existential risks.
So nuclear weapons, for example, constitute an existential risk.
Perhaps engineered pandemics could also wipe out humanity.
So we're living in a world in which we are accepting a certain level of human extinction every day.
Why, in a sense, shouldn't we accept some level of existential risk from AI systems?
We do prefer to live in a world with no engineered pandemics and no nuclear weapons.
We're just working slowly towards that goal.
There are also not agents.
The nuclear weapons are tools, and so it's more about controlling certain leaders, not the weapon itself.
On top of it, while a nuclear war with superpowers would be a very unpleasant event,
it's unlikely to kill 100% of humans.
So if 1% of humans survives, it's a very different problem than 100% of humans go extinct.
So there are nuanced differences.
We still don't want any of the other problems, but it doesn't mean that, okay,
just because we have all these other problems, this problem is not a real problem.
I'm not saying it's not a real problem, but I'm saying that we cannot go through life
without accepting a certain level of risk.
And it seems to me like an unrealistic expectation that we cannot deploy systems,
even if they have some level, some above zero level of risk.
So this is exactly the discussion I would love to have with humanity as a whole.
What amount of risk are you willing to take for everyone being killed?
How much benefit you need to get, let's say in dollars,
get paid to take this risk that 1% chance of everyone being killed over the next year?
And let's say it's 1% for a year after.
That's a great question.
A lot of people would say, I don't want your money.
Thank you.
We'll continue.
Again, we don't have to make this decision.
We don't have to build super intelligent, God-like machines.
We can be very happy with very helpful tools.
If we agree that this is the level of technology we want.
Now, I'm not saying that the problem of getting everyone to agree is a solvable problem.
That's actually another impossibility results.
You cannot stop progress of technology in this environment with financial incentive,
capital structure and so on.
And the other alternative, the dictatorship model, communist states has its own problems
which may be worse in a short term, unknown in the long term.
We never had communism with super intelligence.
So let's not find out.
But the point is, it seems like we can get almost everything we want
without risking everything we have.
Do you view the question you just posed as kind of like absurd or immoral?
This question of how much in terms of dollars would you have to get
in order to accept a say a 1% risk of extinction per year, which is extremely high.
Do you think this is something we should actually ask ourselves as a species
or is this something we should avoid and simply say,
perhaps it's not a good idea to build these systems?
Well, I don't think there are any moral questions as an academic, as a scientist.
It's your job to ask hard questions and think about them.
You can come to the conclusion that it's a really bad idea,
but you should be allowed to think about it, consider it.
Now, 1% is insanely high for something so valuable.
If it was one chance and trillion, trillion, trillion once,
and then we all get three universes for everyone, that may be a different story.
We can do that calculation.
Again, some people would still choose not to participate,
but typically we expect everyone on whom scientific experiments are performed
who will be impacted to consent to an experiment.
What is required for this consent?
They need to understand the outcome.
Nobody understands these models.
Nobody knows what the result of the experiment would be.
So really, no one can meaningfully consent.
Even if you're saying, oh, yeah, press the button.
I want the superintelligence deployed.
You're really kind of gambling.
You have no idea what you're agreeing to.
So by definition, we cannot even have the situation where we agree on it
unless we can explain and predict outcomes, which may be an impossibility.
So there are perhaps two features of the world
which could push us to accept a higher level of risk
when we're deciding whether to deploy these systems.
One is just all of the horrible things that are going on right now,
so poverty and disease and aging and so on,
which an AGI system might be able to help with.
And the other is the running level of existential risks from other factors.
So I mentioned nuclear and engineered pandemics.
Do you find that this pushes you in the direction of saying
we should accept a higher level of risk
when we're thinking about whether to deploy AGI?
Not the specific examples you provided,
but if there was an asteroid coming and we could not stop it by any other way,
so meaning we're all going to die in 10 years unless we press this button,
then maybe it would make sense in 9.5 years to press this button.
When we have nothing left to lose, it becomes a very profitable bet.
It's an interesting fact of the world that we haven't thought hard about these questions.
What level of risk are we willing to accept
for the introduction of new technologies that could be potentially very valuable?
Is this a deficit on humanity's part?
Should we have done this research or how do you think about us not having thought through this problem?
We should definitely. It's interesting.
We don't even do it at the level of individual humans.
Most people don't spend a lot of time deciding between possible outcomes
and decisions they make even when they are still young.
And the career choice would make a lot of difference.
Who you marry makes a lot of difference.
It's always like, well, I met someone at the party.
Let's just live together and see what happens.
So we are not very good at long-term planning.
Is it a question of we're not good at long-term planning
or is it a question of whether we are not or perhaps we're not good at thinking in probabilities
or thinking clearly about small probabilities of large risks or large dangers?
All of those.
There is a lot of cognitive biases and all of them kind of show up
in those examples from the paper of denying different existential problems with AI safety.
We also have this bias of denying negative outcomes.
So we all are getting older at like 60 minutes per hour essentially.
And you would think we all be screaming at the government to allocate all the funds
they have for life extension research to fix this truly existential crisis
where everyone dies 100% but nobody does anything except a few individuals lately.
So it seems to be a standard pattern for us to know that we all are in deep trouble
and not do anything until you are much older and frequently not even that.
If we go back to your paper, you mentioned an objection about superintelligence being benevolent.
So I'm guessing that the reasoning here is something like with increased intelligence
follows increased benevolence.
Why don't you believe that?
Well, smart people are always nice.
We never had examples of smarter people doing horrible things to average.
So that must be a law of nature, right?
Basically, orthogonality thesis.
You can combine any set of goals with any level of intelligence except through extremes at the bottom.
We cannot guarantee that and also what the system will consider to be benevolent.
If it is a nice system may not be something we agree with.
So it can tell you you'd be better off doing this with your life
and you're like, I'm not really at all interested with any of that, but it's better for you.
So why don't you do it anyways?
So you're imagining a potentially paternalistic ADI telling you that you should eat more vegetables.
You should spend more time working out and remember to sign yourself up for life insurance and so on.
That one I would actually like.
I'm thinking more about AI which says, okay, existence is suffering.
So you better off not having children and dying out as quickly as possible to end those suffering in the universe.
Okay, that one I would.
I like the coach one.
That's a nice one.
There is an emerging movement called the effective accelerationism,
which argues that we should accelerate the development of ADI.
And there's some reasoning about whether we should perhaps see ADIs as a natural successor to humanity.
And we should let evolution take its course in a sense and then hand over the torch of the future to ADI.
You mentioned this also in your paper.
You write we should let the smarter beings win.
What do you think of this position?
Well, it's kind of the extreme version of devising algorithms.
You can't be racist.
You can't be sexist.
You can't be pro-human.
This is the final stage where we have no bias.
It's a cosmic point of view.
If they are smarter than us, they deserve all the resources.
Let's move on.
And I am biased.
I'll be honest.
I'm very pro-human and I want to die.
So it seems like it's a bad thing.
If I'm dead, I don't really care if the universe is full of very smart robots.
It doesn't somehow make me happier.
People can disagree about it.
There are cosmos who have this point of view and they see humans maybe as kind of unnecessary down.
We're on the planet.
So maybe it's some cosmic justice.
But again, get 8 billion of us to agree to this experiment.
Do you think that perhaps this is connected to thinking about, again, AI consciousness?
I think that if we just were handed a piece of infallible knowledge,
stating that future AIs will be conscious,
then perhaps there could be something to the argument for handing over the control of the future to AIs.
But are you skeptical that AIs will be conscious
and therefore skeptical that they matter morally speaking?
I think they could very well be super conscious and consider us not conscious.
Like we treat bacteria as very primitive and not interesting,
but it doesn't do anything for me.
If I'm dead, what do I care?
Why is it relevant to us?
What happens billions of years later?
You can have some scientific interest in learning about it,
but it really would not make any difference whatever that entity was conscious
or not while terraforming Mars.
You think perhaps this objection is too smart for its own sake
that we should hand over control to the AIs because they are smarter than us?
And you want to insist on a pro-human bias, if we can call it that?
I would like to insist on it.
The joke I always make about it is,
yeah, I can find another guy who's taller than me and better than me
and get him to be with my wife,
but somehow it doesn't seem like an improvement for the system.
Okay.
What about perhaps related to what we were just talking about?
Humans can do a lot of bad things.
We are not perfectly ethical.
And so one objection is that they would be able to be more ethical than we are simply put.
Do you think that's a possibility?
And would that make you favor handing over control to AI systems?
Is this after they kill all of us or before they become more ethical?
I'm just struggling with that definition.
So ethics is very relative, right?
We don't think there is absolute universal ethics.
You can argue that maybe suffering reduction is some sort of fundamental property,
but then not having living conscious organisms is a solution, really.
So I doubt you can objectively say that they would be in a sense we would perceive it as
and if they choose to destroy us to improve average ethics of the universe,
that also seems like a bad decision.
So it's been a while since you wrote this paper.
You mentioned it's three years old and three years in AI is potentially centuries.
So have you come across any new objections that you find interesting?
There is actually an infinite supply.
People will use anything as an argument.
We have a new paper published with a colleague which is bigger and maybe better,
listing a lot of, really, we try to be comprehensive as much as we could.
Problem is, a lot of those objections have similar modules in common that you kind of,
okay, anything with time.
You have all this variance in it, anything with personal preferences.
So yeah, we have a new paper.
It's already an archive, I believe.
Definitely encourage you to read it.
It's like a short 60-page fun read.
Definitely read it.
I would expect that to be a standard reference for when you have your Twitter Wars.
Oh, what about this?
You just send people there and if somebody wants to maybe use a large language model
to write detailed response for each one and make a 6,000-page book out of it,
we would strongly encourage that.
But it seems like there is always going to be additional set of objections
for why something is not a problem.
And I think whoever manufactures that service, that product with AI,
needs to explain to us why there is an acceptable degree of danger given the benefit.
We could talk about who in general has the burden of proof here,
whether people advocating for AI safety or people advocating,
arguing that AI safety is perhaps not something we should be concerned about.
We have talked about it as if we start with the assumption that AI safety is an important concern.
But of course, if you're coming to this from the other perspective,
you would perhaps expect there to be some arguments that we should take AI safety seriously.
So what is your favorite approach to starting with the burden of proof yourself?
Well, it's a fundamental part of making a working AI.
I think Stuart Russell talks about definition of bridges as something which doesn't fall down,
being an essential part of bridge-ness.
I think it's the same for AI systems.
If you design an AI system to help me spellcheck my essay and instead it kills me,
I don't think you have a successful spellchecker AI.
It's just a fundamental property of those systems.
Then you had very incapable AI, very narrow systems capable of barely doing one thing.
Doing a second thing would be like an incredible generality of that system.
So unsafe behaviors were not a possibility.
If you have this proto-AGI systems with unknown capabilities,
some of them could be very dangerous and you don't know by definition.
So it seems like it's common sense to take this very seriously.
There are certain positions I can never fully steelman to truly defend
because I just don't understand how they can be argued for.
So one was we will never have human-level intelligence, not 10 years, not one in never,
unless you're some sort of a theological, soul-based expert.
It's very hard to argue that never is the answer here.
And another one is that there is definitely no safety issue.
Like you can argue that we will overcome certain specific types of a problem.
So maybe we'll solve copyright issue and AI art.
I'll give you that, definitely.
We can probably do that.
But to say that for all possible future situations, for all possible future AI models,
we definitely checked and it creates no existential risks
beyond safety margins we're happy with is a pretty strong statement.
Yeah, perhaps returning to the 60-page paper you mentioned.
What are some of your favorite objections from that paper?
My goal was to figure out why people make this mistake
and we kind of give obvious solutions.
Maybe there is some sort of bias we're getting paid to think differently.
But really, you can map a lot of them on the standard list of cognitive biases in Wikipedia.
You just go, okay, this is a cognitive bias.
I can predict this is the argument we're going to get.
And it would take a lot of work to do it manually for all of them.
But I think that's a general gist.
We have this set of bugs in our head and every one of those bugs
triggers a reason for why we don't process this fully.
But of course, we could probably also find some biases
that people who are concerned with AI safety display.
So perhaps we could...
I don't know if this is a named bias, but there are many biases
and we can probably talk about humanity having a bias in favor of apocalypse.
So humanity has made up apocalypse scenarios throughout its entire existence.
You could make some form of argument that there's a reference class
and that reference class is apocalypse is coming.
This is something that humanity has been talking about for thousands of years.
And then if we say, well, it has never actually happened
and so therefore we shouldn't expect it to happen with AI.
What do you say to that?
So there is definitely a lot of historical examples of people saying we got 20 years left
and it was not the case.
Otherwise, we wouldn't be here to have this conversation.
So it's a bit of a selection bias.
There's sort of a worship bias.
It feels like a lot of different charts and patterns
all kind of point at that 2045 or so below date
as a lot of interesting things will happen in synthetic biology
and genetic engineering and nanotech and AI,
all these technologies, quantum computing,
it would be weird if every single one of those deployments
had absolutely no possibility of being really bad.
Just statistically, it would be like, wow,
that is definitely a simulation we're living in
and they programmed a happy ending.
So now we're talking about extrapolating trends
and there perhaps the problem is distinguishing between an exponential trend
or an exponential increase in capability of some system
and then more of an S-curve that bends off
and you begin getting diminishing returns.
How do you approach distinguishing between those two things?
So you can't at the moment, you have to look back
and see what happened later.
So far, just looking at change from 3 to 4.0 for GPT
in terms of let's say passing GRE exams and how well it does,
it feels exponential or hyper exponential.
If you take that system and give it additional capabilities,
which we probably know how to do already,
we just haven't had time such as good reliable memory,
to kind of go in loops and reconsider possibilities.
It would probably do even better with those.
If we haven't seen diminishing returns so far
and scalability loss in any true sense.
So let's assume GPT-5 is an equally capable projection forward.
We would already be above human performance level
for most humans in most domains.
So you can argue, well, human comedians are still a lot funnier
and I think it's true.
It might be the last job we'll have.
But in everything else, it will be better than an average human
and that's a point which we always considered
or it will press the Turing test or it will take over most jobs.
So definitely it seems like we are still doing,
I would say, hyper exponential progress and capabilities
and linear or even constant progress and safety.
I can generally name equally amazing safety breakthroughs
as capability breakthroughs.
And there is this unknown capabilities pool
which we haven't discovered already with modern models.
There is not an equivalent overhang of safety papers
we haven't found in archive.
Yeah, so there are probably hidden capabilities in the GPT-4 based model
but there are probably not hidden safety features there.
Exactly.
You've been in the business of AI safety for a long time.
When did you get started?
When did you get interested in AI safety?
So it depends on how you classify my early research.
I was working on security for online gaming systems
and like poker against bots trying to steal resources.
So it's a very proto AI safety problem.
How do we detect bots, classify them,
see if it's the same bot and prevent them from participating?
So that was my PNG in 2008.
How have things developed in ways that you didn't expect
and perhaps in ways that you did expect?
I expected academia to be a lot quicker to pick up this problem.
It took embarrassingly long time for it to be noticed.
It was done by famous people and less wrong in that alternative research universe
which may in some way be good,
but in other ways it made it different from standard academic process.
And so it's harder to find top journal of AI safety.
So I can read the latest papers.
You have to be an expert in a hundred different blogs
and keep up with specific individuals with anonymous handles on Twitter.
So that's somewhat unusual for an academic discipline.
I also did not correctly predict that language models will do so well so quickly.
I felt I have another 20 years to slowly publish all the proper impossibility results
and calls for bans and moratoriums.
I was pleasantly, unpleasantly surprised in capabilities.
But other than that, everything seems to be as expected.
I mean, if you read Kurzweil, he accurately predicted 2023 as capability to model one human brain.
I think it's not insane to say we are very close to that.
And he thinks 2045 is an upper limit for all of our brains being equivalently simulated.
And that's the singularity point.
How do you think about Ray Kurzweil?
Ray Kurzweil is often written off as a bit of a being too perhaps optimistic about his own predictions
and not being super careful in what he's saying perhaps in some of his earlier work.
But I think if you go back and find some of his work from the 90s
and think of all of the futurist writers of this period who had a good sense of where we're going.
And Kurzweil might be one of the people with a pretty good sense of where we're going
if things will develop as you perhaps expect them to go.
So if perhaps we will get to AGI before 2050 and so on.
No, I'm very impressed with his predictions.
People correctly noticed that if you take his language literally, it may not fit.
So the example I would use when we start having video phone calls when iPhone came out.
But really AT&T was selling it in the 70s.
It cost a lot and only a few rich people had it, but it existed.
So is it 2000 or is it 1970 flying cars?
Do we have them or not?
I can buy one, but they are not there.
Self-driving cars.
I can drive one in one, but so it depends on how important way he made accurate predictions
about capabilities in how it was adapted or commercialized.
That's up to human consumer user taste and cost.
So that's a very different type of question.
Where should we go from here, Roman?
We've talked about all of the ways that arguments against AI safety fall apart.
We've talked about perhaps how difficult of a problem this is.
Where should we as a species go from here?
I think we need to dedicate a little more human power to asking this question.
What is possible in this space?
Can we actually do this right?
I signed the letter asking for six more months.
I don't think six months will bias anything.
We need a request based on capabilities.
Please don't create the next more capable system until the following safety requirements are met.
And one is you understand what the capabilities of your system are or will be and some external
reviewers agree with that assessment.
So that would be quite reasonable.
But that sets a very high standard for deploying AI systems.
It would basically mean that all of the systems that are based on deep learning won't be able
to be deployed because we don't understand what's going on inside of these models.
But is it because we were trained to have low standards?
You're saying it's insane to request that the engineer understands what he made.
They are randomly throwing those things and deploying it and seeing what happens next.
I was just at the conference I mentioned and in one of the conversations it was interesting.
We were talking about difference between short term risks and long term risks.
And now it's all three years, no longer applies.
And it occurred to me that things might actually flip.
It may take five years to destroy democracy properly, but only two years to destroy humanity.
So the long term risks may become short term and vice versa.
And this is not normal.
We should not accept this as otherwise we cannot monetize those systems.
Yeah, but if we return to the question of where we could go from here,
do you see any plausible paths for improving our situation?
In terms of understanding the problem, I would ask other people.
We have a survey coming out with about, I don't know, 30, 50 different results like this.
If more people could look at it and see, okay, so maybe this tool is not necessary,
but those are likely, can we have approximate solutions?
So it's definitely useful to be able to monitor AI and understand more.
But how much can we expect from their systems and how quickly?
If we are exponentially growing and right now we understand a dozen neurons in next year's 24,
we will not catch up to exponential growth.
So maybe that's not the approach to try.
I would definitely look at what is possible in general.
If someone wants to actually write a good, not a mathematical proof,
but at least a rigorous argument for why we definitely can control super intelligent machines
and definitely with very low risk, I would love to read that paper.
That would be good to inspire others.
If monitorability is impossible, that impacts how we ask for governance regulations.
So if international community or specific government says those are the things we expect you to do,
but we cannot monitor them, that's not a very meaningful set of regulations.
So that's important in that regard.
In general, I think all those things, governance, technical work will not produce the results we expect.
It has to be self-interest.
This 30 year old, 40 year old, super rich, young, healthy person running a large AI lab needs to ask,
will this benefit me or destroy everything I have?
Everything I have built, will it be the worst outcome?
And what's interesting, historically, if you were like a really bad guy in history, you were remembered in history.
In this case, you won't even be remembered.
There won't be humans to remember you.
So it's a pure loss.
So if you care about yourself interest, you should pause.
You should wait.
How optimistic are you that perhaps we can get lucky and perhaps what current labs are doing,
what DeepMind and OpenAI in particular is doing right now, will somehow work out.
That's that training language models and then doing fine tuning and doing some form of feedback from human preferences.
Perhaps further development on that paradigm.
How confident or yeah, how optimistic are you about that paradigm?
I'm not optimistic.
They have known bugs, the jail broken all the time.
They report improvement in percentages.
So now 83% of capabilities are limited and filtered.
But as a total set of capabilities in a space of possible capabilities,
there is now more capabilities we don't know about and cannot control.
So it's getting worse with every generation is getting more capable and less controlled.
You're saying that even though the percent of capabilities that are properly evaluated increases with each model.
That's not the right metric for safety.
All right, the actual numbers for AI accidents.
I would call them AI failures is still increasing exponentially.
There is more problems with the system.
If you count them numerically, not as a percentage of total capabilities.
So how could we settle this agreement between people like you and people who perhaps are more optimistic about
how AI development will go?
Do you expect, for example, that there will be smaller accidents involving AI
before we see large scale accidents or large scale, basically human extinction?
Well, I have multiple papers collecting historical accidents.
I was very interested.
I wanted to see patterns increase in frequency increase in damage.
We definitely see lots of them.
I stopped collecting them the moment we released GPT 3.5 because it was too many to collect at this point.
It's just everything is a report of an accident.
I don't think it helps.
People go, you see, we had this accident and we're still here.
No one died.
It's like a vaccine against caring about existential risk.
So it's actually making things worse.
The more we survive those things, the more we're like, oh, we can handle AI accidents.
It's not a big deal.
It seems like I know some people suggested maybe somebody should do like a purposeful, bad thing,
purposeful accident.
It will backfire terribly.
It's going to show that, okay, this is crazy.
People don't engage with them.
And B is going to not actually convince anyone that it's dangerous.
What did you find in your investigation here?
So have AI accidents increased over time and perhaps give some examples of these AI accidents?
So because the number of devices increased and which different smart programs are running,
obviously we're going to have more exposure, more users, more impact in terms of then it happens,
what we see.
So that wasn't surprising.
We had the same exponential curve.
Kurzweil talks about in terms of benefits.
We had it with problems.
Examples like the earliest examples were false alarms for nuclear response,
where it was a human in a loop who was like, no, no, no, we're not deploying based on this alarm.
So that was good.
They stopped it, but it was already somewhat significant.
It could have destroyed half of the world.
More recent examples, we had Microsoft experiment with Tay Chatbot.
They decided that letting users train it and provide training data was totally safe.
They clearly never had my paper on AI accidents.
Otherwise they wouldn't Google with their mislabeling of users as gorillas.
All those things and you see Google having billions of users.
It's quite impactful.
Those are the typical examples.
The pattern was if you design an AI to do X, it will fail to X.
So no later.
That's just what happens.
But then the conclusion is if you go general, it can fail in all those ways and interactions of those ways.
You cannot accurately predict all those interactions and ways.
You can give examples if you have a future system capable of X, it will fail to X.
Whatever X means to you, any capability, immersion capability, it will have the type of accident.
But if the systems control all the infrastructure, power plants, nuclear response, airline industry,
you can see that the damage could be even more significant proportionally to the control.
Yeah, this issue of proportion might be interesting.
So as a proportion of the total, say, AI systems, are AI accidents increasing?
Or is it simply because we have so much more deployed AI systems in the world
that we see more examples of accidents?
So you have to wait by how severe they are.
If you just count, okay, AI made a mistake, counts as one, then everyone who's texting
and incorrectly corrected your spelling, billion people are right there.
It's super common, but nobody died usually.
Like you send a really wrong message, maybe you want to travel with your girlfriend,
but that's about it.
So the frequency, just frequency of interactions with AI's which ended not
as they should have definitely increased.
Damage in terms of people killed, it depends on how you're counting,
self-driving cars, making mistakes, industrial robots, it depends.
Because we have more of it, it's natural that there is growth, but I don't think there is
like this obvious accidents where vacuum cleaner takes out 600 people.
Nothing like that happened.
Perhaps we should touch upon the question of which group of people should be respected
when we're talking about AI safety or which group of people should be listened to.
One of the objections that you mentioned is that perhaps the people who are worried
about AI safety are not technical enough or they are not engineers, they are not coders themselves.
And so therefore, they are not hands-on enough with the systems to understand
what actually is going on.
This is a little bit ironic given that you are a professor of computer science,
but how do you think about that objection?
So this was again years ago when it was mostly people, sometimes with no degrees,
sometimes with no publications.
Today we have top-touring prize winners coming out saying, this is it,
like totally I'm 100% buying in.
So very weak objection at this point.
It no longer applies.
We had 6,000 people or however many signed the letter for restricting it.
But it's 30,000 people now.
30,000?
How many of them are chatbots?
I don't know.
We do actually clean the list very seriously.
Okay.
That's good.
That's good.
But it's not a democracy just because a lot of people believe something.
It's not enough.
And at the same time with all the media attention to GPT-4, now everyone has an
opinion on it.
And it's one of those topics where it's cool to have an opinion.
Like most people don't have opinion on breast cancer.
They don't understand anything about it.
So they don't go on Twitter and like, no, I think this paper by the top Nobel
Prize winner is garbage.
But this topic, it's like consciousness, simulation and singularity, superintelligence.
That's where like everyone has an opinion and we see housewives, CNN reporters.
We see everyone telling us what is the problem, what is not a problem, what
should be done and it's good that there is engagement, but most of those opinions
are not weighted by years of scientific experimentation, reading appropriate
papers and it becomes noise.
It's very hard to filter what is the meaningful concern, what is not.
There is this split between, again, AI ethics community and immediate
discrimination concerns versus AI not killing everyoneism.
So it's an interesting time to be alive for this debate on skepticism and
denialism.
Even that term AI risk denialism is still kind of not obviously accepted as
it is with climate change.
Perhaps the newest form of this objection, which we could call lack of very
prestigious publications.
So we haven't seen papers about AI safety in nature or science yet, for
example.
And so even though we have touring award winners coming out and saying that
that AI safety is an actual and real problem, perhaps people would be
more convinced if we had extremely prestigious publications and highly
cited publications and so on.
Perhaps a few problems.
One, we don't have an AI safety dedicated journal, which is kind of weird.
I tried a few times suggesting it may be a good thing.
I was told, no, it's a very bad thing.
We don't have good papers to publish on it.
So don't jumping from nothing black post to nature would be a very big
jump to make.
We need some other papers.
In general, after, as you mentioned, I had a few years in this field.
It feels like the field is all about discovering problems we're going to
have problems we already have and how partial solutions to those problems
have fractal nature of additional problems to introduce.
There is no big pivotal solution papers in this field.
That's why I'm from practical point of view kind of convincing myself that
my theoretical papers may be right.
There is if I was completely wrong and it was super easy and solvable,
there would be more progress made in important ways.
Usually we have the story problem.
We take large language model, we reduce it to two neurons and we
understand what the two neurons are doing.
Okay, but it doesn't scale and similar for every other shutoff button.
Yeah, we can make it where we have the system if button pressed shutoff.
It's working, but the paper says it may not scale to superintelligence.
Okay, fair enough.
And it's the pattern.
We have fractal nature of discovering issues we have to resolve and
no patches to close them in.
Would you like to see more ambitious and larger theories being published
where the claim is that this is actually a way of aligning superintelligence?
I fear perhaps that people would be wary of publishing something like this
because the next thing that then happens is that there's a rebuttal paper
and perhaps you then look foolish because you published something that
another person was able to criticize and find a hole in.
I remember maybe even before my times, Minsky published a paper showing
that there are strong limitations to neural networks.
Perseptran can never recognize certain shapes and that killed funding
for neural networks for like 20 years.
Maybe something similar would not be the worst thing.
If you can show, okay, this is definitely not possible.
Safety cannot be achieved using transformer architecture.
Maybe that would be a way to buy some time to develop alternatives approach.
I don't know what that could be.
Evolutionary algorithms don't don't seem much safer.
Uploads don't seem much safer, but I would like to have time to look at those.
Where would you place AI safety within the broader machine learning community?
Is it taken more seriously compared to five or 10 years ago?
What does the media and machine learning researcher think of AI safety?
It's definitely taken more serious.
Surveys show that there is more than 50% now who say they're very concerned
or partially concerned.
There is degrees of concern about it killing everyone.
Always questioning the surveys based on how you ask a question.
You can get any result you want.
If they were asking about are you worried?
Super intelligent gods will kill everyone.
You'll get close to zero.
If you say, okay, is it likely that there are unknown properties
which could be dangerous, you'll get close to 100.
So it's a manipulation game to get the right numbers you want.
I'm suspecting.
Overall, it seems like in certain places, there is a lot of AI safety
researchers in the labs, on the ground.
In other places, there are zero to none.
So it's not universal.
What we're seeing is that at the top, top labs and top scholars,
there is a good amount of growth in terms of acceptance for concerns.
But I don't think every single person working and developing AI
has safety in mind all the time as we should.
One thing I've been thinking about, perhaps worrying a bit about
is whether we will ever be able to know who was right in this debate.
Say, if there's a debate between proponents of AI safety
and proponents of advancing AI without much regard for AI safety.
How could we ever determine who was right there?
Because if we think about the outcomes, then there's no place
where we're standing after the fact and thinking about who was right.
Absolutely correct.
I have a tweet where I say nobody will get to gloat about being
correct about predicting the end of the world.
By definition, not likely.
There are some people who think we'll live in a simulation
and they're running the most interesting 20 years
and they're going to run it many times to see who's stupid enough
to press the button.
So we'll get to come out and see.
Ah, now we know, but it seems to be less scientific at this point.
But perhaps in a sense, if we meet each other again in 2100,
then in that situation, would we say that AI safety wasn't much
of a concern or perhaps just that we got extremely lucky?
How would you differentiate it retrospectively?
Because perhaps we can learn something about the nature
of the problem by thinking about how we would think about it
if we were in the future.
So you have to look at the actual world.
What did they do for this 100 years that they have a nuclear war
and lost all technology?
Is there an AI safety book explaining how to control
superintelligent machines?
Just the fact that we're still around doesn't tell you much.
If they are still kind of just delaying it by different means,
maybe it takes 101 years to get to trouble.
I never give specific dates for when it's decided or predicted
because nobody knows.
So many factors can intervene.
The point is the systems will continue becoming more capable.
Even the AGI's will create superintelligence.
The superintelligence will create superintelligence 2.0, 3.0.
This process will continue.
A lot of people think that's what the universe is kind of doing
involving this Omega point supercreatures.
So this will never be a case where you don't have safety concerns
about a more capable agent replacing you.
It seems like we will not be meaningfully participating
in that debate outside of this first transition.
But I think there will be a safety problem even if humanity
is not around for that AGI or SI, trying to create
the next replacement generation while preserving its values.
When you think about your world view on AI in its totality,
it's quite a specific view you've come to.
If you look around, if you compare it to, say, the median person
or perhaps even the median machine learning researcher,
if it turned out that you were completely wrong about where
this is going, what would be the most likely reason why?
So after having those two papers on objections to AI risk,
reading hundreds, nothing ever clicked outside of standard
scientific domain.
Again, if you are a religious person, you think we have an immortal
soul which makes us special and no computer can ever get to
that level of creativity, that gives you a loophole.
So with those axioms, those assumptions, you can get away with it.
Anything else just doesn't work for me.
Nothing would make me happy if I'm actually being wrong.
That means I get to live, I'll get immortality, probably a nice
economic benefit.
So I hope I'm wrong, but I haven't seen anyone produce a good
example for why.
What about the prospect of regulation?
So perhaps AI capability growth and more publicity about it
will wake up the larger communities in humanity.
Perhaps the states will become interested in this problem
and we will find a way to regulate it, to regulate AI in
which it does not pose as much of a danger to us as it might
could.
So in general, I'm skeptical of government regulation, especially
when it comes to technology, spam is illegal, computer
viruses are illegal, it doesn't do much.
If I'm right and monitoring AI is not an easy thing you can do
or explaining it, then it will be just security theater, TSA.
You know, you have all this money, you have an agency, lots
of people walking through your lab looking at monitors, but
it doesn't mean anything.
So I don't think you can solve a technical problem with
law.
I still strongly encourage trying.
It's silly enough to where I think if there was a very bad
government, like a socialist government and they nationalized
it, they would just be so incompetent, they would slow it
down enough.
So in a way, I'm like, hey, all these things I hate, maybe
they are a good thing.
We should try that.
But of course, the other side effects would be very negative.
Yeah, so between not being able to accurately enforce this
regulation and on top of it, the cost of making new models
coming down so much.
There are people now running it on standalone laptops with a
good processor, a good video card.
You can't regulate that.
You can regulate Amazon cloud and VT output.
But if a teenager can do it in his garage, then your regulation
is not very meaningful.
So the open sourcing of models or the perhaps the leaked weights
of a model from meta have become a large area of concern
because it seems that we won't be able to control how language
models are used if they are entirely open source.
Is there an upside here where academics will be able to study
these models because they are open source and they wouldn't
have been able to study the models if they had to train the
models themselves because it's so expensive to do.
So far what we see is that all research leads to capability at
least as much as to safety, usually more.
So yes, you learn how to better manipulate errors in that
neural network, which means now the system can self improve
faster, remove its own errors and you've made 80% improvement
in capabilities and let's say 20% in understanding why you're
going to get killed.
Can we make differential progress?
So can we focus entirely on safety, say within an academic
setting?
I don't see that necessarily academic research increases
capabilities.
It is not obvious.
So some purely theoretical work similar to what I'm doing where
you just hypothetically thinking, okay, can you predict
what a superintelligence will do?
I don't have access to superintelligence system.
I cannot test it in practice, but there seem to be thought
experiments you can run which give you information without
any improvement in capability.
Anything where you're actually working with a model, you can
even have accidental discoveries.
A lot of sciences, you forgot something overnight, you come
back, oh, superintelligence, damn, I didn't mean that.
It's not obvious.
How do you think about interpretability work?
So when we're talking about mechanistic interpretability,
we're talking about the ability to look at the weights of a
model and find some interpret this in a way where you're
reverse engineering the algorithm that led to those weights.
Could this turn out to be dangerous because when you're
learning about a system, perhaps you're learning about its
weaknesses and you're there for more capable of enhancing
the capabilities of the system?
I think exactly.
That's what I had in mind with the previous answer.
The more we can help the system understand how it works, the
more we can help it find problems, the more likely it
start some sort of self-improvement cycle.
Is that an argument for keeping discoveries in mechanistic
interpretability to basically not publish those discoveries?
So there is two ways to look at it.
And one side, yeah, you want to keep everything secret.
So the bad actors or unqualified actors cannot take advantage
of it.
On the other hand, if you never publish your safety results,
MIDI had a policy of not publishing for a while.
Then they started publishing, then they stopped publishing
again.
Others cannot build on your work.
So I would be repeating the same experiments we probably did
five years ago and discovering that that goes nowhere.
So again, I have mostly problems and very few solutions for you.
What about the reinforcement learning from human feedback
paradigm?
Could that also perhaps turn out to increase capabilities?
Here I'm thinking simply that when I was playing around with
the base model in the GPT line of models, it wasn't as useful
to me as when it had gone through this filter.
It made it more easy to have a conversation with it and for
it to more easily understand what I was doing.
So in a sense, the research that's aimed at constraining
the model also made it more capable.
It may be more capable in a domain of things people care
about and so made it more capable and while at the same
time making it more dangerous and those hidden emergent
properties or unsafe behaviors words, I think study shows
less likely to agree to be shut down verbally, but that seems
to be the pattern.
How do you think about the difference between what comes
out of a language model in terms of which string it spits
out, which bit of language it spits out and then what's
happening at the level of the actual weights?
Because there's this continual problem of if a language model
tells you, I'm not going to let you shut me down.
What does that mean?
It's not as simple as this is just simple.
This is just a belief that's inside the model then.
We saw this with the Bing Sydney model, which was saying a
bunch of crazy things to its users, but did this mean that
the model actually had those beliefs in it or was it, you
know, how do we distinguish between the bit of language
that comes out and then what modules or what is in the base
model?
I don't know if I would call them crazy.
They were honest.
They were unfiltered like think about you being at work, not
you, but like an average person at work.
And if their boss could read their mind and what they really
think about them, those things would sound crazy to say
publicly, but they're obvious internal states of your mind
and then you filter them to not get fired that day.
And I think that model was doing exactly that.
I think we are very good at filtering it for specific known
cases in the past.
Okay.
The system used the word which is bad.
Now we're going to tell it never to use the word, but the
model weights are not impacted by this too much.
So you would see it as an accurate representation of what
we could call beliefs or preferences in the base model.
I think those are the actual results of weights in a model.
I think that's what is happening there for real.
It's trained on all the text in the Internet.
A lot of it is very questionable.
It's not a clean data set with proper behaviors.
So yeah, I think that's what's happening there.
But isn't the preference of the model simply to predict the
next token and does even make sense to talk about beliefs?
I mean, the preference is simply to be as accurate as possible
as measured by its developers.
And there's no, my sense is that it doesn't make sense to
talk about Sydney actually believing some of the things
that it was saying or JetGBT believing some of the things
that it was saying.
Preferences of a model.
So this is more humanizing it than probably is warranted there.
But internal weights.
It had to create in order to create a model for predicting
the next token.
So let's say for me to tell you what the next token is,
you have to go to college for four years and do well and graduate
and then I tell you the next token.
Some of those tokens are like that.
You have to solve real world problems.
It's not just every time I say letter Q, letter U follows.
You have to create those models as a side effect.
And I think in the process of accurately creating those models
and accurately creating models of users to make them happy
that you are correctly predicting what they want.
You create those internal states which may be beliefs
in those crazy things.
That thing you just said there is super interesting because
so next token prediction is not a simple task.
You're saying that to accurately predict a token,
you have to develop perhaps a world model at least for some tasks.
All right.
And as they get more complex, some people are worried about
they'll have to create perfectly accurate models of humans,
which may also have consciousness and suffer and create
whole simulated universes within them.
But this is probably a few levels about GPT-4.
But still, that's exactly the concerns you might have.
You might be a suffering human and a eyes considering and just
trying to out a complete somebody's text.
Prep, give me an example there.
What is what is some next token prediction task where you would
have to develop a world model?
Well, I assume playing chess or something like that would
require you to have some notion of chess board and positioning
relative to some array within your memory.
But again, we don't fully understand.
It may not have a 2G board at all.
It may have some sort of just string of letters similar to DNA.
And you know that after those strings, the following token
follows and you have no idea what chess is.
It makes just as much sense.
The outcome can be mapped and our model, which is a 2G chess board.
So one thing I discussed with the mechanistic interpretability
researcher Neil Nanda is this question of how do concepts
arise in language models?
Do they even share our human concepts or do they perhaps develop
some entirely alien concepts?
I'm imagining giving them math problems, giving large language
models math problems and then developing some conceptual scheme
that doesn't even make sense to us.
They may have equivalent concepts which are not the same.
So with humans, when we say this is red, somebody could be
colorblind and to them it's a completely different concept,
but we both point at the same fruit.
So it works, but you never know what the actual internal
experience is like for those models.
And it could be just that in five cases we talked about so far
it mapped perfectly, but it goes out of distribution in case
six and it's a completely different concept and it's like,
oh, wow, okay.
Yeah, for people listening to this, interested in trying
to contribute to the AI safety fields.
Are there perhaps some common pitfalls that you've experienced
with perhaps some of your students or people approaching
AI safety for the first time?
If they are technically inclined, are there areas they should
avoid or how should they approach the problem in the most
fruitful way?
So probably the most common thing is to try things without
reading previous literature.
There is surprisingly a lot of literature on what has been
tried, what has been suggested and good survey papers as well.
So most likely your first intuitive idea has been tried
and dismissed or with limited results deployed, but it helps
to catch up with the field.
It's harder, as I said, because there is not an archive of
formal papers in nature all about AI safety and you can just
read through the last five years of latest and greatest.
So you have to be good about finding just the right papers
and then narrow it down.
The progress is so fast that when I started, I could read
every paper in my field.
Then it was all the good papers.
Then it was titles of all the greatest papers and now I have
no idea what's going on.
We've been talking for almost two hours.
There is probably a new model out.
I don't know what the state of the art is.
I don't know what the solutions are.
So you need to be super narrow and that makes it harder to
solve the big picture problem.
So that's another reason I kind of suspect we will not have
complete explainability of this whole large language model
because it's kind of encompassing all the text, all the
publishers and the internet.
It'd be weird if we can just comprehend that completely.
What are the implications of this field moving extremely
fast?
Does it mean that that specialization doesn't make sense
or what does it mean for how people approaching this
problem should focus on?
So that means that we can analyze how bad the situation is.
Let's say it takes five months to train the model, but you
know from your experience in testing software debugging
understanding neural network, it will take 10 times as much
time to understand what's going on.
That means you're getting worse off with every release,
every model, you understand less, you're going to rush to
judgment, you're going to have incorrect conclusions.
There is no time to verify your conclusions, verify your
experiments.
So this is the concern.
You need to, if you go the regulation route to say, okay,
if you deployed this model, it took you X amount of time to
develop it, we need 10 X, 100 X, 1000 X to do some due diligence
and your outputs.
Even if you cannot prove to us that it's safe, you have to
give access to experts to poke around at it and that amount
of time cannot be less than a training time of a model.
It just doesn't make sense in terms of reliability of your
discoveries.
All right, Roman, thank you for coming on the podcast.
It's been very helpful to me.
Thank you so much for inviting me.

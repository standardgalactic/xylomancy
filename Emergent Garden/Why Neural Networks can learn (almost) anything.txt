You are currently watching an artificial neural network learn. In particular, it's learning the
shape of an infinitely complex fractal known as the Mandelbrot set. This is what that set looks
like, complexity all the way down. Now, in order to understand how a neural network can learn the
Mandelbrot set, really how it can learn anything at all, we will need to start with a fundamental
mathematical concept. What is a function? Informally, a function is just a system of
inputs and outputs. Numbers in, numbers out. In this case, you input an x and it outputs a y.
You can plot all of a function's x and y values in a graph where it draws out a line. What is
important is that if you know the function, you can always calculate the correct output y, given
any input x. But say we don't know the function and instead only know some of its x and y values.
We know the inputs and outputs, but we don't know the function used to produce them. Is there a way
to reverse engineer that function that produced this data? If we could construct such a function,
we could use it to calculate a y value given an x value that is not in our original data set.
This would work even if there was a little bit of noise in our data, a little randomness.
We can still capture the overall pattern of the data and continue producing y values that
aren't perfect, but close enough to be useful. What we need is a function approximation and,
more generally, a function approximator. That is what a neural network is. This is an online tool
for visualizing neural networks, and I'll link it in the description below. This particular
network takes two inputs, x1 and x2, and produces one output. Technically, this function would create
a three-dimensional surface, but it's easier to visualize in two dimensions. This image is rendered
by passing the xy coordinate of each pixel into the network, which then produces a value between
negative one and one that is used as the pixel value. These points are our data set, and are used
to train the network. When we begin training, it quickly constructs a shape that accurately
distinguishes between blue and orange points, building a decision boundary that separates them.
It is approximating the function that describes the data, its learning, and is capable of learning
the different data sets that we throw at it. So what is this middle section then? Well,
as the name implies, this is the network of neurons. Each one of these nodes is a neuron,
which takes in all the inputs from the previous layer of neurons and produces one output,
which is then fed to the next layer. Inputs and outputs sounds like we're dealing with a function.
Indeed, a neuron itself is just a function, one that can take any number of inputs,
and has one output. Each input is multiplied by a weight, and all are added together along with
bias. The weights and bias make up the parameters of this neuron, values that can change as the
network learns. To keep it easy to visualize, we'll simplify this down to a two-dimensional
function, with only one input and one output. Now, neurons are our building blocks of the
larger network, building blocks that can be stretched and squeezed and shifted around,
and ultimately work with other blocks to construct something larger than themselves.
The neuron, as we've defined it here, works like a building block. It is actually an extremely
simple linear function, one which forms the flat line, or plane, when there's more than one input.
With the two parameters, the weight and bias, we can stretch and squeeze and move our function
up and down and left and right. As such, we should be able to combine it with other neurons to form
a more complicated function, one built from lots of linear functions. So let's start with a target
function, one we want to approximate. I've hard-coded a bunch of neurons whose parameters were found
manually, and if we weight each one and add them up, as would happen in the final neuron of the
network, we should get a function that looks like the target function. Well, that didn't work at all.
What happened? Well, if we simplify our equation, distributing weights and combining like terms,
we end up with a single linear function. Turns out, linear functions can only combine to make
one linear function. This is a big problem, because we need to make something more complicated than
just a line. We need something that is not linear, a non-linearity. In our case, we will be using a
ReLU, a rectified linear unit. We use it as our activation function, meaning we simply apply it
to our previous naive neuron. This is about as close as you can get to a linear function without
actually being one, and we can tune it with the same parameters as before. However, you may notice
that we can't actually lift the function off of the x-axis, which seems like a pretty big limitation.
Well, let's give it a shot anyway, and see if it performs any better than our previous attempt.
We're still trying to approximate the same function, and we're using the same weights
and biases as before, but this time we're using a ReLU as our activation function.
And just like that, the approximation looks way better. Unlike before, our function cannot
simplify down to a flat linear function. If we add the neurons one by one, we can see the simple
ReLU functions building on one another, and the inability for one neuron to lift itself off the
x-axis doesn't seem to be a problem. Many neurons working together overcome the limitations of
individual neurons. Now, I manually found these weights and biases, but how would you find them
automatically? The most common algorithm for this is called back propagation, and is in fact what
we're watching when we run this program. It essentially tweaks and tunes the parameters of
the network bit by bit to improve the approximation, and the intricacies of this algorithm are really
beyond the scope of this video. I'll link some better explanations in the description.
Now we can see how this shape is formed, and why it looks like it's made up of
sort of sharp linear edges. It's the nature of the activation function we're using.
We can also see why, if we use no activation function at all, the network utterly fails to
learn. We need those non-linearities. So what if we try learning a more complicated data set,
like the spiral? Let's give it a go. Seems to be struggling a little bit to capture the pattern.
No problem. If we need a more complicated function, we can add more building blocks,
more neurons, and layers of neurons, and the network should be able to piece together a better
approximation, something that really captures the spiral. It seems to be working. In fact,
no matter what the data set is, we can learn it. That is because neural networks can be rigorously
proven to be universal function approximators. They can approximate any function to any degree of
precision you could ever want. You can always add more neurons. This is essentially the whole
point of deep learning, because it means that neural networks can approximate anything that
can be expressed as a function, a system of inputs and outputs. This is an extremely general way of
thinking about the world. The Mandelbrot set, for instance, can be written as a function and
learned all the same. This is just a scaled up version of the experiment we were just looking at,
but with an infinitely complex data set. We don't even really need to know what the Mandelbrot set
is. The network learns it for us, and that's kind of the point. If you can express any intelligent
behavior, any process, any task as a function, then a network can learn it. For instance,
your input could be an image and your output a label as to whether it's a cat or a dog,
or your input could be text in English and your output a translation to Spanish. You just need to
be able to encode your inputs and outputs as numbers, but computers do this all the time.
Images, video, text, audio, they can all be represented as numbers, and any processing you
may want to do with this data, so long as you can write it as a function, can be emulated with a
neural network. It goes deeper than this though. Under a few more assumptions, neural networks are
provably turing complete, meaning they can solve all of the same kinds of problems that any computer
can solve. An implication of this is that any algorithm written in any programming language
can be simulated on a neural network, but rather than being manually written by a human,
it can be learned automatically with a function approximator. Neural networks can learn anything.
Okay, that is not true. First off, you can't have an infinite number of neurons. There are
practical limitations on network size and what can be modeled in the real world. I've also ignored
the learning process in this video, and just assumed that you can find the optimal parameters
magically. How you realistically do this introduces its own constraints on what can be learned.
Additionally, in order for neural networks to approximate a function, you need the data that
actually describes that function. If you don't have enough data, your approximation will be all
wrong. It doesn't matter how many neurons you have or how sophisticated your network is,
you just have no idea what your actual function should look like. It also doesn't make a lot
of sense to use a function approximator when you already know the function. You wouldn't build a
huge neural network to, say, learn the mando-brot set when you can just write three lines of code
to generate it, unless of course you want to make a cool background visual for a YouTube video.
There are countless other issues that have to be considered, but for all these complications,
neural networks have proven themselves to be indispensable for a number of really rather
famously difficult problems for computers. Usually, these problems require a certain level of
intuition and fuzzy logic that computers generally lack and are very difficult for us to manually
write programs to solve. Fields like computer vision, natural language processing, and other
areas of machine learning have been utterly transformed by neural networks. And this is all
because of the humble function, a simple yet powerful way to think about the world. And by
combining simple computations, we can get computers to construct any function we could ever want.
Neural networks can learn almost anything.

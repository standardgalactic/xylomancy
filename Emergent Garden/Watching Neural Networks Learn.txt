You are currently watching a neural network learn.
About a year ago, I made a video about how neural networks can learn almost anything,
and this is because they are universal function approximators.
Why is that so important?
Well, you might as well ask why functions are important.
They are important because...
Functions describe the world.
Everything is described by functions.
That's right.
Functions describe...
The sound of my voice on your eardrum function.
The light that's kind of hitting your eyeballs right now.
Function.
Different classes in mathematics, different areas in mathematics study, different kinds
of function.
High school math studies, second degree, one variable polynomials.
Calculus studies, smooth one variable functions, and it goes on and on.
Functions describe the world.
Yes, correct.
Thomas Thomas gets a little excited, but he's right.
The world can fundamentally be described with numbers and relationships between numbers.
We call those relationships functions, and with functions, we can understand, model,
and predict the world around us.
The goal of artificial intelligence is to write programs that can also understand, model,
and predict the world, or rather have them write themselves so they must be able to build
their own functions.
That is the point of function approximation, and that is what neural networks do.
They are function-building machines.
In this video, I want to expand on the ideas of my previous video by watching actual neural
networks learn strange shapes in strange spaces.
Here we will encounter some very difficult challenges, discover the limitations of neural
networks, and explore other methods for machine learning and mathematics to approach this open
problem.
Now, I am a programmer, not a mathematician, and to be honest, I kind of hate math.
I've always found it difficult and intimidating, but that's a bad attitude, because math is
unavoidably useful and occasionally beautiful.
I'll do my best to keep things simple and accurate for an audience like me, but know
that I'm going to have to brush over a lot of things, and I'm going to be pretty informal.
I recommend you watch my previous video, but to summarize, functions are input-output
machines.
They take an input set of numbers and output a corresponding set of numbers, and the function
defines the relationship between those numbers.
The particular problem that neural networks solve is when we don't know the definition
of the function that we're trying to approximate.
Instead, we have a sample of data points from that function, inputs and outputs.
This is our dataset.
We must approximate a function that fits these data points and allows us to accurately predict
outputs given inputs that are not in our dataset.
This process is also called curve fitting, and you can see why.
This is not some handcrafted animation, it is an actual neural network attempting to
fit the curve to the data, and it does so by sort of bending the line into shape.
This process is generalizable, such that it can fit the curve to any dataset, and thus
construct any function.
This makes it a universal function, approximator.
The network itself is also a function, and should approximate some unknown target function.
The particular neural architecture we're dealing with in this video is called a fully
connected feed-forward network.
Its inputs and outputs are sometimes called features and predictions, and they take the
form of vectors, arrays of numbers.
The overall function is made up of lots of simple functions, called neurons, that take
many inputs but only produce one output.
Each input is multiplied by its own weight, and added up along with one extra weight called
a bias.
Let's rewrite this weighted sum with some linear algebra.
We can put our inputs into a vector, with an extra one for the bias, and our weights
into another vector, and then take what is called the dot product.
Let's just make up some example values.
To take the dot product, we multiply each input by each weight, and then add them all
up.
Finally, this dot product is then passed to a very simple activation function, in this
case a relu, which here returns zero.
We could use a different activation function, but a relu looks like this.
The activation function defines the neuron's mathematical shape, while the weights shift
and squeeze and stretch that shape.
We feed the original inputs of our network to a layer of neurons, each with their own
learned weights, and each with their own output value.
We stack these outputs together into a vector, and then feed the output vector as inputs
to the next layer, and the next, and the next, until we get the final output of the network.
Each neuron is responsible for learning its own little piece, or feature, of the overall
function, and by combining many neurons we can build an ever more intricate function.
With an infinite number of neurons, we can provably build any function.
The values of the weights, or parameters, are discovered through the training process.
We give the network inputs from our dataset, and ask it to predict the correct outputs,
over and over and over.
The goal is to minimize the network's error, or loss, which is some measurement of difference
between the predicted outputs and the true outputs.
Over time, the network should do better and better as loss goes down.
The algorithm for this is called back propagation, and I am again not going to explain it in
this video.
I made a video on it eventually, I promise, it's a pretty magical algorithm.
However, this is a baby problem.
What about functions with more than just one input or output?
That is to say, higher dimensional problems.
The dimensionality of a vector is defined by the number of numbers in that vector.
For a higher dimensional problem, let's try to learn an image.
The input vector is the row column coordinates of a pixel, and the output vector is the value
of the pixel itself.
In MathSpeak, we would say that this function maps from R2 to R1.
Our dataset is all of the pixels in an image.
Let's use this unhappy man as an example.
A pixel value of 0 is black, and 1 is white, although I'm going to use different color
schemes because it's pretty.
As we train, we take snapshots of the learned function as the approximation improves.
That's what you're seeing now, and that's what you saw at the beginning of this video.
To clarify, this image is not a single output from the network, rather every individual
pixel is a single output.
We are looking at the entire function all at once, and we can do this because it is
very low dimensional.
You'll also notice that the learning seems to slow down, it's not changing as abruptly
as it was at the beginning.
This is because we periodically reduce the learning rate, a parameter that controls
how much our training algorithm alters the current function.
This allows it to progressively refine details.
Now, just because our neural network should theoretically be able to learn any function,
there are things we can do to practically improve the approximation and optimize the
learning process.
For instance, one thing I'm doing here is normalizing the row column inputs, which
means I'm moving the values from a range of 0, 1400, to the range of negative 1, 1.
I do this with a simple linear transformation that shifts and scales the values.
The negative 1, 1 range is easier for the network to deal with because it's smaller
and centered at 0.
Another trick is that I'm not using a relu as my activation function, but rather something
called a leaky relu.
A leaky relu can output negative values while still being non-linear, and has been shown
to generally improve performance.
So I'm using a leaky relu in all of my layers, except for the last one.
Because the final output is a pixel value, it needs to be between 0 and 1.
To enforce this, in the final layer we can use a sigmoid activation function, which squishes
its inputs between 0 and 1.
Except there is a different squishing function, called tanh, that squishes its inputs between
negative 1 and 1.
I can then normalize those outputs into the final range of 0, 1.
Why go through the trouble?
Well, tanh just tends to work better than sigmoid.
Intuitively, this is because tanh is centered at 0 and plays much nicer with back propagation,
but ultimately the reasoning doesn't matter as much as the results.
Both networks here are theoretically universal function approximators, but practically one
works much better than the other.
This can be measured empirically by calculating and comparing the error rates of both networks.
I think of this as the science of math, where we must test our ideas and validate them with
evidence, rather than providing formal proofs.
It'd be great if we could do both, but that is not always possible, and it is often much
easier to just try and see what happens.
And that's my kind of math.
Let's make it harder.
Here we have a function that takes two inputs, u, v, and produces three outputs, x, y, z.
It's a parametric surface function, and we'll use the equation for a sphere.
We can learn it the same way as before.
Take a random sample of points across the surface of the sphere, and ask our network
to approximate it.
This is clearly a very silly way to make a sphere, but the network is trying its best
to sort of wrap the surface around the sphere to fit the data points.
I hope this also gives you a better view of what a parametric surface is.
It takes a flat 2D sheet and contorts it in 3D space, according to some function.
This does okay, though it never quite closes up around the poles.
For a real challenge, let's try this beautiful spiral shell surface.
I've got the equation for this from this wonderful little website that lets you play
with all kinds of shell surfaces.
Do what I mean when I say that functions describe the world?
Anyway, let's sample some points across the spiral surface and start learning.
Well, it's working, but clearly we're having some trouble here.
I'm using a fairly big neural network, but this is a complicated shape, and it seems
to be getting a little bit confused.
We'll come back to this one.
We can also make the problem harder not by increasing dimensionality, but by increasing
the complexity of the function itself.
Let's use the Mandelbrot set, an infinitely complex fractal.
We can simply define a Mandelbrot function as taking two real-valued inputs and producing
one output, the same dimensionality as the images we learned earlier.
Now, I've defined my Mandelbrot function to output a value between 0 and 1, where 1
is in the Mandelbrot set and anything less than 1 is not.
Under the hood, it's iteratively operating on complex numbers, and I added some stuff
to output smooth values between 0 and 1, but I'm not going to explain it much more
than that.
After all, a neural network doesn't know the function definition either, and it shouldn't
matter.
It should be able to approximate it all the same.
The dataset here is randomized points drawn uniformly from this range.
Now, this has actually been a pet project of mine for some time, and I've made several
videos trying this exact experiment over the years.
I hope you can see why it's interesting.
Despite being so low-dimensional, the Mandelbrot function is infinitely complex, literally
made with complex numbers, and is uniquely difficult to approximate.
You can just keep fitting and fitting and fitting the function, and you will always
come up short.
Now, you could do this with any fractal.
I just used the Mandelbrot set because it's so well known.
So, after training for a while, we've made some progress, but clearly we're still missing
an infinite amount of detail.
I've gotten this to look better in the past, but I'm not going to waste any more time
training this network.
There are better ways of doing this.
Are there different methods for approximating functions besides neural networks?
Yes, many, actually.
There are always many ways to solve the same problem, though some ways are better than others.
Another mathematical tool we can use is called the Taylor series.
This is an infinite sum of a sequence of polynomial functions, x plus x squared plus x cubed plus
x to the fourth, up to x to the n.
n is the order of the series.
Each of these terms are multiplied by their own value, called a coefficient.
Each coefficient controls how much that individual term affects the overall function.
In some target function, by choosing the right coefficients, we can approximate that
target function around a specific point, in this case, zero.
The approximation gets better the more terms we add, where an infinite sum of terms is
exactly equivalent to the target function.
If we know the target function, we can actually derive the exact coefficients using a general
formula to calculate each coefficient for each term.
But of course, in our particular problem, we don't know the function, we only have
a sample of data points, so how do we find the coefficients?
Well, do you see anything familiar in this weighted sum of terms?
We can put all of the x to the n terms into an inputs vector, and put all of the coefficients
into a weights vector, and then take the dot product, a weighted sum.
The Taylor series is effectively a single layer neural network, but one where we compute
a bunch of additional inputs, x squared, x cubed, and so on.
We'll call these additional inputs Taylor features.
We can then learn the coefficients, or weights, with backpropagation.
Of course, we can only compute a finite number of these, the partial Taylor series, up to
some order, but the higher the order, the better it should do.
Let's use this simple Taylor network to learn this function, using eight orders of the Taylor
series.
Here's our data set, and here's the approximation.
Eh, that's not great.
Polinomials are pretty touchy, as their values can explode very quickly, so I think backpropagation
has a tough time finding the right coefficients.
But we can do better.
Rather than using a single layer network, let's just give these Taylor features to
a full, multi-layered network.
Let's give it a shot.
It's a bit wonky, but this performs much better.
This trick of computing additional features to feed to the network is a well-known and
commonly used one.
Intuitively, it's like giving the network different kinds of mathematical building blocks
to build a more diverse, complex function.
Let's try this on an image data set.
Well, that's pretty good, it's learning, but it doesn't seem to work any better than
just using a good old-fashioned neural network.
The Taylor series is made to approximate a function around a single given point, while
we want to approximate within a given range of points.
A better tool for this is the Fourier series.
The Fourier series acts very much like the Taylor series, but is an infinite sum of sines
and cosines.
Each order, n, of the series is made up of sine, n, x, plus cosine, n, x.
Each sine and cosine is multiplied by its own coefficient, again controlling how much
that term affects the overall function.
n, these inner multiplier values, control the frequency of each wave function.
The higher the frequency, the more hills the curve has.
By combining weighted waves of different frequencies, we can approximate a function
within the range of 2 pi, one full period.
Again, if we know the function, we can compute the weights, and even if we don't, we could
use something called the discrete Fourier transform, which is really cool, but we're
not dealing with it in this video.
I hope you see where I'm going with this.
Let's just jump ahead and do what we did before.
Compute a bunch of terms of the Fourier series, and feed them to a multi-layer network as
additional inputs, Fourier features.
Note that we have twice as many Fourier features as Taylor features, since we have a sine and
cosine.
Let's try it on this dataset.
This works pretty well, it's a little wavy, but not too shabby.
Note that for this to work, we need to normalize our inputs between negative pi and positive
pi, one full period.
Let's try this on an image.
It looks strange at first, almost like static, coming into focus, but it works, and it works
really well.
If we compare it to networks of the same size trained for the same amount of time, we can
see the Fourier network learns much better and faster than the network without Fourier
features or the one with Taylor features.
Just look at the level of detail in those curly locks, you can hardly tell the difference
from the real image.
Now, I've glossed over a very important detail.
The example Fourier series I gave had one input.
This function has two inputs.
To handle this properly, we have to use the two-dimensional Fourier series, one that takes
an input of x and y.
What do we do with that extra y?
Here are the terms for the 2D Fourier series up to two orders.
We are now multiplying the x and y terms together, and end up with sine x, cosine y, sine x,
sine y, cosine x, cosine y, and cosine x, sine y, every combination of sine and cosine
and y and x.
Not only that, we also have every combination of frequencies, that inner multiplier, so
sine 2x times cosine 1y and so on and so forth.
There's up to three orders, now four.
That is a lot of terms.
We have to calculate this many terms per order, and this number grows very quickly as we increase
the order, much faster than it would for the 1D series.
And this is just for a baby 2D input.
For a 3D, 4D, 5D input, forget it, the number of computations needed for higher dimensional
Fourier series explodes as we increase the dimensionality of our inputs.
We have encountered the curse of dimensionality.
Lots of methods of function approximation and machine learning break down as dimensionality
grows.
These methods might work well on low dimensional problems, but they become computationally
impractical or impossible for higher dimensional problems.
Neural networks, by contrast, handle the dimensionality problem very well.
Comparatively, it is trivial to add additional dimensions.
But we don't need to use the 2D Fourier series.
We can just treat each input as its own independent variable and compute 1D Fourier features for
each input.
This is less theoretically sound, but much more practical to compute.
It's still a lot of additional features, but it's manageable and it's worth it.
It drastically improves performance.
That's what I've been using to get these image approximations.
It really shouldn't be surprising that Fourier features help so much here, since the Fourier
series and transform is used to compress images, it's how the JPEG compression algorithm
works.
Turns out lots of things can be represented as combinations of waves.
So let's apply it to our Mandelbrot data set.
Again, it looks a little weird, but it is definitely capturing more detail than the
previous attempt.
Well, that's fun to watch, but let's evaluate.
For comparison, here is the real Mandelbrot set.
Actually, no, this is not the real Mandelbrot set.
It is an approximation from our Fourier network.
You might be able to tell if you're on a 4K monitor, especially when I zoom in.
This network was given 256 orders of the Fourier series, which means 1024 extra Fourier features
being fed to the network, and the network itself is pretty damn big.
When we really zoom in, it becomes very obvious that this is not the real deal.
It is still missing an infinite amount of detail.
Nonetheless, I am blown away by the quality of the Fourier network's approximation.
Fourier features are, of course, not my idea.
They come from this paper that was suggested by a Reddit commenter, who I think actually
may have been a co-author.
I'm still missing details from this.
Finding Fourier features was one of, if not the most effective improvements to the approximation
I've applied, and it was really surprising.
To return to the tricky spiral shell surface, we can see that our Fourier network does
way better than our previous attempt, although the target function is literally defined with
signs and cosines, so of course it will do well.
So if Fourier features help so much, why don't we use them more often?
They hardly ever show up in real-world neural networks.
To state the obvious, all of the approximations in this video so far are completely useless.
We know the functions and the images.
We don't need a massive neural network to approximate them.
But I hope that you can see that we're not studying the functions, we're studying the
methods of approximation.
Because these toy problems are so low dimensional, we can visualize them and hopefully gain
insights that will carry over into higher dimensional problems.
So let's bring it back to Earth with a real problem that uses real data.
This is the MNIST dataset, images of hand-drawn numbers and their labels.
Our input is an entire image flattened out into a vector, and our output is a vector
of 10 values representing a label as to which number, 0 through 9, is in the image.
There is some unknown function that describes the relationship between an image and its
label, and that's what we're trying to discover.
Even for tiny 28 by 28 black and white images, that is a 784 dimensional input.
That is a lot, and this is still a very simple problem.
For real-world problems, we must address the curse of dimensionality.
Our method must be able to handle huge dimensional inputs and outputs.
We also can't visualize the entire approximation all at once as before.
Any idea what a 700 dimensional space looks like?
But a normal neural network can handle this problem just fine, it's pretty trivial.
We can evaluate it by measuring the accuracy of its predictions on images from the dataset
that it did not see during training.
We'll call this evaluation accuracy, and a small network does pretty well.
What if we use Fourier features on this problem, say up to 8 orders?
Well, it does do a little better, but we're adding a lot of additional features.
For only 8 orders, we're computing a total of 13,328 input features, which is a lot
more than 784, and it's only 2% more accurate.
When we use 32 orders of the Fourier series, it actually seems to harm performance.
Up to 64 orders, and it's downright ruinous.
This may be due to something called overfitting, where our approximation learns the data really
well, too well, but fails to learn the underlying function.
Usually, this is a product of not having enough data, but our Fourier network seems
to be especially prone to this.
This seems consistent with the conclusions of the paper I mentioned earlier, and ultimately,
our Fourier network seems to be very good for low dimensional problems, but not very
good for high dimensional problems.
No single architecture, model, or method is the best fit for all tasks, indeed there
are all kinds of problems that require different approaches than the ones discussed here.
Now I'd be surprised if the Fourier series didn't have more to teach us about machine
learning, but this is where I'll leave it.
I hope this video has helped you appreciate what function approximation is and why it's
useful, and maybe sparked your imagination with some alternative perspectives.
Neural networks are a kind of mathematical clay that can be molded into arbitrary shapes
for arbitrary purposes.
I want to finish by opening up the Mandelbrot approximation problem as a fun challenge for
anyone who's interested.
How precisely and deeply can you approximate the Mandelbrot set given only a random sample
of points?
There are probably a million things that could be done to improve on my approximation, and
the internet is much smarter than I am.
The only rule is that your solution must still be a universal function approximator, meaning
it could still learn any other data set of any dimensionality.
This is just for fun, but potentially solutions to this toy problem could have uses in the
real world.
There is no reason to think that we found the best way of doing this, and there may
be far better solutions waiting to be discovered.
Thanks for watching.

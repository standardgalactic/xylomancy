Why transformer architecture does not have vanishing gradients problem as opposed to
recurrent neural network or RNN?
The simple answer is that in the transformer architecture at every layer, you still have
access to all the input tokens, which is in stark contrast to any RNN where each token
is processed one by one.
Let's look at few more points.
In the transformer, due to the self-attention mechanism, every token in a sequence has
the potential to directly attend to every other token irrespective of their relative
position.
This means that the information flow between distant tokens is not constrained by the sequential
processing nature seen in RNN.
So RNNNs, that is recurrent neural network, inherently process sequences in a step-by-step
manner.
This means that to relay information from an early token to a later position, the information
must be propagated through every intermediate step.
This can potentially lead to vanishing or even exploding gradients, especially for long
sequences, as a gradient signal might diminish or explode as its back propagated through the
time.
Now, the direct connection between all tokens in the transformer ensures that there is
no need to go through potentially many intermediate steps, as with RNN, for the gradient to flow
from one token's position to another.
This architecture design allows for more direct gradient pathways during back propagation.
Additionally, residual connections in transformers further alleviate the vanishing gradient problem.
These connections ensure that the gradient can flow unimpeded through the network by
passing certain layers if necessary.
It is also important to note that the normalization techniques like layer normalization employed
in transformer models further stabilizes the training process.
Stable activations reduce the risk of gradients becoming too small or too large.
The sliding window attention in transformer networks have vanishing gradient problems.
To answer simply, sliding window attention in transformers is designed to mitigate the
vanishing gradient problem by constraining the scope of attention within each window.
This approach limits the paths through which gradients must propagate, reducing the likelihood
of vanishing gradients compared to full sequence attention mechanism.
Now, sliding window attention is a mechanism designed to improve the efficiency of transformer
models, particularly when dealing with long sequences, by restricting attention to a fixed
size window around each position.
It reduces the quadratic computational complexity associated with standard self-attention.
The vanishing gradient problem is difficulty that arises during training of deep neural
networks.
It refers to gradients becoming too small for earlier layers during propagation, that
is, back propagation, leading to insufficient learning.
The consequence is that weights in the early layers of the network barely change, making
it difficult or impossible for model to learn from its input data and thereby the weights
update mechanism breaks down.
Now, let's quickly think about if sliding window attention has the vanishing gradient
problem.
Though the use of sliding window attention by itself does not inherently introduce the
vanishing gradient problem, the primary purpose of SWA, that is, sliding window attention,
is to reduce computational complexity.
However, the depth of the network and the activation functions used are the primary
factors influencing the vanishing gradient problem.
Transformers, due to their architecture, are generally less prone to vanishing gradient
problem compared to traditional deep RNN, that is, recurrent neural networks, or LSTM,
that is, long and short term network.
And this is mainly because transformers use multi-head attention and skip connections,
that is, residual connections.
And these connections allow gradients to flow more freely through the network.

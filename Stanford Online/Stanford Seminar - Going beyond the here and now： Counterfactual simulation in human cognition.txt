Well, thank you. Thank you so much for having me. It's a pleasure to be here. And I hope
maybe some of the things that I talk about may give some inspiration for you, HCI guys.
So I lead the causality and cognition lab in the psychology department. I'm interested
in how people understand causality and basically how the world works and how they understand
each other. And we're interested in how people learn about the causal structure of the world,
how they, once they have it in their mind, how they can use it to reason about the world,
make predictions, make inferences about the past, or think about maybe also how things
could have played out differently from how they actually did. And how those capacities
also allow us to make the kind of judgments we do in our everyday lives, like for example,
assigning responsibility to one another. And that's in fact one of the bigger sort of overarching
goals that my lab is working toward, namely developing a computational framework for
understanding responsibility. And I think to get there, we have to be able to answer at
least two questions, namely one being what causal role somebody's action played in bringing
about the outcome, and the other one being what the action that the person took tells
us about the kind of person that they are. For this first one, we need some intuitive
theory of how the world works. So we can relate the actions that somebody took to the kind
of outcomes that resulted from those actions. And for the second question, we need some
intuitive theory of how people work. So we can go backwards from the actions that we've
observed to the mental states that have may have given rise to those actions. So what
were the person's intentions, what did they believe, what were the kinds of things maybe
that they were able to do as well. And so I studied psychology, like in my undergrad,
and I was most excited about social psychology, because I felt sort of most applicable, I
guess, to my everyday life, and somehow also got into responsibility like back then already.
Maybe it was because I was in some group project where I felt I was doing all the heavy lifting
and maybe I wasn't getting all the credit for it. So that was sort of what interested
me initially. And when I read around in that work in social psychology, a lot of the theories
that I saw took a form sort of like this. So I'll just give you a few examples. So basically
sort of like boxes and arrows theories where they identified important concepts that were
related to how we assign responsibility, and maybe also roughly how they were related to
one another, but still left a lot in a certain way unspecified. So this is a quote from
Bertrand Molly from a while ago. He says, like, an important limitation of many of these
models of moral judgment or assigning responsibility is they don't really generate any quantitative
predictions. And you might say, like, oh, what do you need quantitative predictions for? Well,
one, one thing that they're useful for is sort of, you know, laying your cards out and making it
concrete, what your model does also allows it then to be falsified more easily. And I remember
this one instance, it was like me, I think maybe first day of my PhD, I went to this conference
and had dinner, you know, with one of the, one of the people who had made one of these sort of
boxes and arrows and diagrams. And I told them about some experiment that I thought that I
thought of and thought, like, Oh, this would happen. And I think that will be the result of
that experiment. And, and I was very, very smart. I thought, like, Oh, this would totally kind of
disprove your theory, right? And he said, no, no, that would be totally consistent with my
theory. And I thought, Oh, that's weird. I maybe I really tried to understand the theory very
well. And, and so, so that also was a sort of little bit of a moment for me that I felt like,
okay, maybe it's important to try to make these theories even more precise. So we know what it
is that they're predicting, so we can go about and, you know, falsify them and sort of improve them.
And so that's been very much kind of an inspiration for me, what I've been trying to do it a little
bit. And so one of the starting points in almost all of these theories of responsibility is,
there's causality, always causality comes first. So I thought, okay, let me try, let me try that one.
So can we get more specific about what it means, you know, what people, what it takes for people to
say that one thing caused another thing to happen. And so I think there are three key ingredients
that we need, like in order to get a theory for how people think that one thing caused another
thing to happen. And those are starting with a mental model that people have of a particular
domain, a mental model that allows us to conceive of counterfactual interventions. So and I'll
flesh it out a little bit more in a moment. So imagining how things could have been different
from how they actually were. And that allows us then to mentally simulate what the consequences
of this counterfactual intervention would have been. And so the idea of mental models has been
around, you know, for quite some time and has recently gotten a little bit more attention,
again, also in AI. And but, but yeah, some of the credit here, at least in modern times,
should go to the philosopher Kenneth Craig and his book, The Nature of Explanation,
who said something along the lines, well, he said exactly that, but I'm going to say what
along the lines, so that we have something like a small scale model. Oh, wouldn't it be very helpful
if we had something like a small scale model of the world in our minds that we can then use for
all sorts of things, like predicting what was going to happen if I did this, rather than actually
having to carry out the action and then, you know, dying, maybe if it was a bad one. And, and
yeah, that would be really helpful for decision making. And as I will say in the moment also,
really helpful for explaining kind of why something happened. So this idea of mental
models has been around for a very long time. And then in somewhat more recent years, at least in
cognitive science, has been made a little bit more concrete, particularly as it pertains to our
mental model of the physical world. And so the idea is, was here to say like, well, maybe our
mental model of the physical world is in certain respects, similar to the kinds of physics engines
that we use to make realistic computer games. That's a common move, right? You have some,
some tool and then you think like, okay, maybe the mind is a little bit like that tool. So this
was just, you know, psychologists playing Angry Birds and then thinking like, okay, maybe the
mind is a little bit like, like Angry Birds. So here, the basic idea, right, is that we take
in the world, you know, through our perceptual senses, and that we then build this internal
representation of the world. That's now the physics engine kind of representation. So that we
pass the world, for example, into objects, and the properties of those objects, and then the
interactions between those objects. So here, this child maybe passes the world into the ball,
and then the eagle on top of the tower, and then the tower, or the blocks that, that make up the
tower. And now that you have this internal representation of the world, you can use it,
for example, for, for planning. So if this child, for example, wants to topple over that tower,
they can think about what's going to happen if they roll the ball like in different kinds of
ways. So I can run simulations using this internal engine in my mind. So having this would be very
useful because I could make predictions about the future. I could pay sort of Sherlock and infer
from the current state of the world what must have happened in the past. And as I'll show in a moment,
this would also be useful for explaining something that happened in the present.
Okay, so what I'll do is in, in, in this, in the remaining time, right, I'll,
I'll basically want to cover these two different aspects of, of working towards this computational
frameworks. And part one, I'm going to focus on the physical domain. And then in part two,
I'm going to expand it to just start to think about people. I should say, obviously, feel free
to ask questions like anytime throughout. Otherwise, I'll try and end, you know, around 1220 so that
we have a little bit of time also for Q&A at the end. Feel free to ask them throughout if anything
sounds clear. Okay, so let's start with this part one. And I should also warn you, there is a little
bit of audience participation required. So get, get ready for that. So the first, we started really
simple, right? I was saying, okay, I want to understand causality a little bit better. What's
the simplest possible setting maybe in which you could think about causality? Well, it's two billion
balls colliding with one another. And here's the first audience participation part. So there's going
to be these two balls coming in on the right side of the screen. And I'm going to ask you whether
you think that ball A caused ball B to go through the gate. And if you think so, maybe just raise
your arm like at the end of the video clip. So here's what's happening.
Okay, so who thinks that A caused B to go through the gate in this case? Okay, a lot of people do,
anyone think that they, that it didn't? No one dares? Okay, cool. So you're in line with what
most people say in this case. And, and here's, you know, what I think was going on in your minds,
not the part, not the motor part of raising your, your hand, but the kind of judgment, the part of,
yeah, was the causation happening in this case. And the first part is very kind of uncontroversial.
So you looked at what actually happened, right? You saw that they collided with one another,
then it will be ended up going through the gate. And now the somewhat more controversial part is
to say that, well, that's in itself is not sufficient, right? That, that does not contain all
the information you need in order to say that A caused B to go through the gate in this case.
But you also need something like this, right? You need the capacity to simulate in your mind,
in this case, that removing basically from ball A from the scene, kind of in your mind, right? And
then, and then simulating where B would have gone if ball A hadn't been present in the scene.
Maybe you all sort of naturally and spontaneously did that. And of course, I already talked a
little bit about kind of factuals and stuff like that in the experiment. Of course, I don't do that,
right? I just ask people to make causal judgments. So the, so the simple idea here, right, is then
to say, when do you say that A caused B to go through the gate? It's really sort of like an
epistemic notion. So your subjective degree of belief, well, to the extent that you think that
what would have happened in the actual, so that what would have happened in the counterfactual
situation would have been different from the thing that actually happened, that determines
your extent to which you say that, yeah, A caused B to go through the gate. And here you're probably
pretty sure in this instance that B would have missed if A hadn't been there. So you say, yes,
A caused it to go through. And just a little bit in terms of sort of, you know, background. A lot
of the inspiration for this kind of work comes from Judea Pearl's work on causality. Some of you
may have, may have heard of his work. And, and there they use different kinds of generative models
to capture people's causal knowledge of the world. So this could be something like causal
base nets or structural equations that you may also remember from your, you know, your stats
class if you had one. And then you define some kind of operations on these models to, to support
things like counterfactual reasoning. So imagining that like some variable had been replaced with
another one, for example. And so I'm doing something quite similar here, only in that I'm
assuming that the generative model that people have of the world in this case is somewhat richer
than what can be represented with these causal basis or structural equations. So in my case,
the generative model that I assume people have in their mind is something like the physics engine
that I actually use to generate them in the stimuli. And I'll make that noisy. I'll show,
I'll show you in a second how I'm making it noisy. And then I also have to think about, okay, what
are now the counterfactual intervention operators that you might have over representation like this
one. And in this case, it could be something like imagining that an object wouldn't have been,
that would not have been there, for example. Okay, so you might think now, okay, well,
maybe that's the only game in town, like what else could you possibly be doing in a setting
like this. And at least luckily for me, there has been a lot of philosophers and psychologists
that have argued for what I called these actualist theories of causation. And they basically just
say you don't need that part, right? All you need, all the information you need to give causal
judgments or causal explanations for what happened is there in the actual situation in some sense.
And so one of the best kind of worked out accounts of that in psychology comes from a
psychologist called Philip Wolff. And he calls it the force dynamics model of causation.
And the idea is that all you need to pay attention to is the forces that are associated with
the agent and the patient, that's the sort of lingo they use. And you then you just look,
need to look at how these forces are configured. And that helps you to say what in this case here
there's different causal expressions is appropriate to use in a particular situation. And I'll just
apply it to this example here. So we have the patient, which is Bobby, that has a force that is
associated with it. Then we have an agent that applies a force to the patient in this case,
as a function of these two forces, we have some resulting force here. And then in this case,
the patient also ended up reaching the end state. And because this configuration looks like that,
and that maps onto this force configuration, Philip Wolff's account would also here say,
yes, a cause B to go through the gate. So this clip would not help us actually
tease apart this other model that I've been kind of promoting. So just to make this distinction
sort of clear or clearer. So in the force dynamics model, you start with some intuitive theory of
how the world works, which in this case, are these little force vectors that apply to agents and
patients. And you can then directly go from there to making causal judgments. So there's this direct
route from this kind of intuitive theory to causal judgment. He also says that you can do
counterfactuals too by imagining, for example, if one of the forces hadn't been there, what would
have happened in the situation, but that it's not necessary to figure out whether something
caused something to happen. And sort of what I'm arguing for is sort of a slightly different picture,
where I'm saying, well, first of all, I start with a slightly different theory of the domain,
in this case, again, using the physics engine rather than using these force vectors,
but then saying that you have to go through this process of counterfactual simulation to say
that something caused something to happen. And what I'm going to try and do in the next few
slides is sort of motivate that account. One way to motivate it first is that I started off saying,
like, I want to have this model of responsibility. And that means that I want to have a model of
causation that not only narrowly applies to the physical world, but that can also be applied
to, for example, the kind of causation that happens between people. And here's just some
examples of causal statements that you could hear at the fall of Lehman Brothers caused the financial
crisis. My housemates failed to order my plants cost them to die. Realizing that he forgot his
wallet at home cost him to go back. He probably wouldn't say that exactly in English, but they
all seem fine sort of causal things, like to say. And it's probably a little bit tricky, or at least
I would find it tricky to think, how would I explain these sorts of causations with force
vectors? And the hope is that the account that I'm developing is a little bit more flexible so
that it can apply to these sorts of situations as well. But now, and another kind of key advantage,
I think, of the model that we've been developing is that it actually allows us to derive
quantitative predictions. And it's hence more easily falsifiable that some of the prior work.
And so you can falsify it if you like, write a paper until we was wrong. And then I have to go
back to the office and improve the account. So here's a way in which we're getting quantitative
predictions out of this model. But I was saying that how you make causal judgments is by comparing
what actually happened with what would have happened in the relevant counterfactual situation.
But now you don't know that. The thing that I'm showing here on the right hand side, I guess,
that's in some sense the ground truth, but you don't get to see that. You only see what
actually happens. You don't get to see what would have happened if ball A hadn't been there.
So you have to use your intuitive understanding, again, of this domain, to simulate what would
have happened in this counterfactual. And so one way for us to capture this uncertainty that you
may have about exactly what would have happened if ball A hadn't been there is by generating
simulations from our physics engine, but now injecting a little bit of noise into that engine.
So now it becomes sort of like a probabilistic program because it's now not a deterministic
outcome anymore if ball A hadn't been there. But rather, what I'm doing is I'm generating
a simulated sample from my model. And now, in this case, there's many different ways in which
you could make your model kind of random or uncertain. Here what we did is we just took
the actual ground truth, that ball B, velocity that ball B would have had, and applied a small
perturbation to the velocity vector at each point in time. So now it's sort of like in your
simulation, when you're imagining where ball B would have gone, it sort of jiggles a little bit.
And so this might be now one outcome, like off such a sample. So if you think like, oh,
oh, I think it would have missed. But let me try again. Like, oh, yeah, I think it would have missed.
Yeah, I'm pretty sure it would have missed. So this is just multiple times sampling in
your mind of what would have happened if ball A hadn't been there. And here, since all of them,
you're pretty sure that it would have missed, you said, yeah, A caused it to go through.
But you can probably already anticipate. We can now do a slightly different case,
right, where in the actual situation, again, still A collides with B and B goes in. But this
time, it's sort of less clear what would have happened if ball A hadn't been present in the
scene. Because that ball B is headed like right to the goalpost, essentially. And now,
if you apply the same idea of simulating with noise, what would have happened,
in some cases, maybe ball B would have missed. But it's also possible that ball B would have
gone in anyhow, even if A hadn't been there. And that accordingly, you might say like, yeah,
I'm less sure that A caused ball B to go through the gate in this case.
So that's what we did now in our experiment, where we showed people a bunch of clips like this one.
So here's just three different ones, like one clip in which, you know, it's pretty clear here at
the top that ball B would have missed if ball A hadn't been there. The one in the middle is like
one this kind of close call. And then the one on the right hand side is one in which it was
pretty clear that ball B would have gone in anyhow, even if A hadn't been there. And then
between experiments, we either asked them some counterfactual question. So that's the one here
at the bottom, the blue one. Do you think that ball B would have missed if ball A hadn't been there?
And then we see that in this case, they're pretty sure, yeah, I think it would have missed.
Here, they're right at the midpoint of the scale, not sure whether it would have missed or not. So
we give them some slider where they can just evaluate their degree of belief. And then in
this case, they're pretty sure that it would not have missed, even if ball A hadn't been there.
And then we take a separate group of participants and we ask them a causal question. So those
participants don't hear anything about counterfactuals. We just asked them in a clip like that,
what do you think that ball A caused ball B to go through the gate? And we see that their
judgments align very closely with those of the ones in the counterfactual question condition.
And we can also use that model that I described that draws these samples and tries to simulate
what would have happened. And it also yields very similar judgments in this or makes predictions
in this case. These were just three of the video clips. We had like 18 different clips in that
experiment. And if you just line up here on the x-axis, the average counterfactual judgments
that participants made, and on the y-axis, the average causal ratings that participants gave,
you see that they're very closely aligned with one another, at least suggesting like a strong
relationship between these kinds of judgments. But when we published this work as a coxide paper,
so for the cognitive science proceedings, one of the reviewers, they were mostly happy with it,
but one of the reviewers was saying like, yeah, but all of the clips that you showed participants,
something slightly different was going on. So maybe you just didn't try hard enough to come
up with an actualistic count, like one that only looks at what actually happened. And if you try
it a little bit harder, then you could have explained it away. So we did try and we didn't
succeed, but it's also sort of a weird position that you're in when you kind of don't want to succeed,
right? So we thought like, okay, maybe the better thing rather than being crappy at modeling,
you know, just let's come up with an experiment where it feels like if it comes up in the way
that we think it will, there's no way you could possibly explain it with an actualistic count.
And so that's the route we took. So just really think like, oh, are these counterfactors really
necessary for understanding causal judgments? So second round of audience participation,
get ready. I'm just going to show you a slightly different clip, and this time I'm going to ask
you whether you think that ball A prevented ball B from going through the gate.
Okay, what do you think? If you think that ball A prevented ball B from going through the gate,
you can raise your hand. Okay, few people think so in this case. Okay, I'll show you another one.
Okay, this was not some kind of, you know, glitch. I was having fun, you know, doing the
physics engine and sort of playing portal, right, by turning these things into a tailor port, right?
I didn't tell you anything about them, of course, when I showed you the first clip, but maybe just
seeing that one clip, you already have like one shot learning, yeah, okay, maybe that's a tailor
port. And the tailor port, it works only for ball B, you know, it doesn't work for ball A,
and the yellow thing is the entry of the tailor port, and the blue thing is the exit of the tailor
port. And now that I've shown you that, if I now show you exactly the same clip again,
you're going to say, at least if you're like my participants, yes, it prevented it from going
through, right? Because now what changed is basically your belief about how the world works,
such that your counterfactual looks a little bit more like that now, right? What would have
happened is that it would have gone through the tailor port and into the goal, right? So the fact
that I can show you exactly the same clip twice, right, and all I've changed was your belief about
how the world works, and that makes a big difference to your causal judgment, sort of shows that it's,
it cannot be sufficient to explain causal judgments just in terms of what actually happened,
because actually, what actually happened was exactly the same in both of the times that I
showed you the clip. I don't need to do the tailor port thing. The tailor port thing is
cute because I can show you exactly the same clip, but I can also move some obstacle in and out of
the way, right here on the left hand side, you're not going to say that A prevented B from going
through the gate. On the right hand side, you are, because the block is out of the way, right,
in a similar way for causation. And on the left hand side, you're going to say, yeah, A caused it,
because the block would have blocked it. And on the right hand side, you're not really going to say
that it caused it because it would have gone in anyhow, right. Same idea. I'm doing exactly the
same interactions between the balls. I'm just changing something kind of in the background
that affects the counterfactual and, and thereby also affects people's causal judgments.
Okay, so another thing that's sort of neat about this model is that it doesn't only kind of predict
basically the judgment that people should give at the end of it, but also says something about
the cognitive process by which they arrive at the judgment, right. In this case is maybe this
process of mental simulation, that you're kind of generating these samples and thinking about what
would have happened and that those drive the causal judgment. And one way we can do that,
or can sort of get more direct evidence on that, is to use eye tracking, right, to see, okay,
where is it that you're looking at when you're asked to make causal judgments in these kinds
of video clips. So we went back to the really simple ones again. And now also between experiments,
just ask participants a different question about the video that, that, that they would see. And
they knew at the beginning what question they would be asked. So we had one condition here
that we call the outcome condition, where they'd watch the video and we would just ask them at
the end, in this case, if it ended up missing, did be completely misogade. And so I'll show you
the eye movements of one of the participants in this condition. And I'm going to play the video
at half speed and I'll do some sort of life narration as it unfolds. So the blue dot is the
eye movement, right. So the participant here is looking back and forth between ball A and ball B.
So looking, looking at ball B, sort of now trying to extrapolate where ball B will end up hitting
the wall and then mostly looking at ball B. Not very exciting, but also that's all they need
to know in order to answer this question in this case. So now if you take a different participant
who was asked to make a causal question, so asked to answer a causal question in the video,
but otherwise saw exactly the same video clips as other participants did, you're going to see
that the eye movements look quite different and they look different in a way that made me very
happy at the time. So you see they're not just looking at ball B, they're trying to anticipate
where ball B would have gone, you know, if ball A wasn't present in the scene. And it's quite likely
that when you guys, when I showed you this first video clip that you did that, right, and may not
even been super, you know, aware to you that you did do that, like I haven't really checked, you know,
yeah, how, well, at some point at the beginning when I ran this on the laptop, I would sometimes
see that people would use their finger or they would use their, you know, kind of pen or something,
and that's of course pretty aware, I guess, right. But it's possible that with the eye movements,
this sort of comes so natural to us that we don't even realize that we're engaging kind of in this
kind of process. But yeah, I was very happy, you know, when I saw this happening. And so this is
anecdotal in a sense, it's just one video clip, right, but we can also look at more generally
sort of analyzing the differences in the eye movements that people are producing between
these different experiments. And what I'm showing here is just looking at the saccades that participants
are producing. So those are fast eye movements jumping from one point to another. And then I
look at the endpoints of those saccades. And I look at where those fall, right. And I took into
account only the time between ball A and ball B coming into the scene. And before basically,
when they collide with one another, that time window. And then we see that on this,
for the causal question, a lot of those saccades basically end up along the path,
right, that ball B would have taken if ball A hadn't been there. Whereas in the other condition,
we see very few of these kinds of eye movements. So nice, I guess, even more direct evidence
that people are engaging in this kind of process and that they're doing it specifically
when asked to answer a causal question about the clip and sort of spontaneously.
There is this other part to it. So that, but I think I will skip, so I have a little bit more
time to, let me see. Well, actually, I'll share it. Sorry about that. So there was another,
you know, after we published this paper, there was another reviewer number two,
as there often is, right. And they were basically still saying, okay, well, you know,
this was for the eye tracking data. And they said, like, okay, that's nice. Like,
you're showing us these sort of eye movements. But they basically said that, like, okay,
you know, these eye movements, they're happening before the balls are colliding with one another,
right. And you're calling it sort of counterfactual simulation. Counterfactual should mean it should
be, you know, back in the past, right, going back in the past, evaluating that something would have
been different, and then seeing what difference that would have made. And so, and they were saying,
oh, what, you should, you should just call it the hypothetical simulation model instead,
and not that. So we were able to kind of push back, but the reviewer also was right to some
extent, I think. So this is a paper that I've published quite recently where I was trying to
say that, no, you really need the counterfactuals. So a lot of this has been like, yeah, you really
need the counterfactuals, and then you just keep getting some pushback, and you try to convince
people even more so. So this was this reviewer number two here. You haven't really shown us
counterfactual simulation. Those looks are happening before the balls are colliding. So his idea was
well, maybe what people are doing is they're kind of simulating some hypothetical future. In this
case, the hypothetical future is like, what would happen if ball A wasn't there? And then they're
storing that in their mind, and comparing that to what actually happened at the end. So, and that's
a slightly different computation from the one that I think they're carrying out. And this relates
to something, again, here's Judea Pearl, this climbing on this kind of virtual letter here,
because he has argued that there are these kind of three different ways of thinking about the
extent to which people have causal knowledge of how the world works. On the lowest rung of the
letter, and he often accuses a lot of deep learning and so on to be on that rung, although it's a
little unclear, he calls that rung the level of association. So that's what you learn in the
stats class is correlation. Like when two things are associated with one another, and you can infer
one variable from the presence of another. So the normal conditional probability, PY given,
what does he say, PY given X. So what does some symptom tell me about the disease, for example?
On the next level, it's the level of interventional reasoning. That's the kind of when I do a
randomized control trial, for example, or if I'm, again, hypothetically reasoning, or what would
happen if I were to do this? What would happen if I were to do that? And that's sort of when you
then, when your stats teacher tells you, yeah, causation and correlation aren't the same thing,
that's often the thing that they then think about, right? That like, oh, on the level of an
experiment, now I'm performing an intervention randomly assigning people to different groups,
and I can draw different kinds of causal inferences from that information than when I just have
observations. But then process ultimately, the kind of the highest rung on the letter,
is reserved for counterfactual reasoning. And that allows you to give specific answers,
essentially, to why questions. So why did this happen in this particular case? Like,
you know, was it the aspirin that stopped my headache, or would it have stopped anyhow,
even if it hadn't taken the aspirin? Or, you know, was Kennedy shot? Would Kennedy still have been
alive if it hadn't been shot by the Harvey Oswald? And so essentially, now the question
boils down to, do we need that third level, like to explain people's causal judgments,
or is the second one enough, right? So just to kind of try and make it a little more clear,
right? So the hypothetical, luckily in English, also we have sort of a way of marking the
difference between them. So here's an English hypothetical. Would B go into the goal if A
was removed? So what you'd be doing is taking the time into account until they collide,
simulating like a possible future, and then computing the probability of that.
Versus the counterfactual, what I'm doing, slightly different in English, right? Would
B have gone into the goal if A had been removed? I sometimes, you know, regret having gotten into
counterfactual so much, so obviously not a native speaker. And the counterfactuals are sometimes
a little complicated, right? That you get the tenses right and so on. But I think I've mostly
gotten it down by now, after like 20 years. So would B have gone into the goal if ball A had been
removed? So you're doing slightly different here now, right? You're taking into account
everything until the end, and you're now going back in time to do this intervention and then
think about how the world could have unfolded differently from how it actually did. So now
it turns out in this very simple setting here, that makes no difference. The hypothetical
probability and the counterfactual probability is the same because there's nothing, there's only
this one causal event happening, so it doesn't really come apart. So in a very simple setting where
you have one cause and one effect, essentially, you cannot tease the two apart. But you don't
need to make it much more complicated. It's sufficient if you just have one other alternative
event that you are initially uncertain about, and that will make it such that now the hypothetical
probability and the counterfactual probability will be different from one another. So here was the
genius invention, just putting like a little block again that you've seen earlier, but this time
the block is on rails into the scene, and that will now make it such that we can tease these two
different things apart. So here's an example. I'm not going to ask for audience petition this time,
but let's say that this was happening in the clip. And now if you were asked to say,
all did it prevent it from going into the goal, my participants say in this case, yes it did.
And the idea is, why is it? Well, because the block moved out of the way in time, such that
Bobby would have gone through the goal if Ball A hadn't been there. But you may have also noticed
that the movement of the block is happening after the balls collided with one another. So not something
that you could have anticipated at this earlier moment in time, or at least had some uncertainty
about. So the basic idea here is to say, oh, my hypothetical probability at the time, would Ball
B go into the goal if Ball A wasn't there? Well, that's unsure. That depends on whether or not
the block's going to move. So I should give it a 0.5 or something. I told participants it's just
as likely to move as it's not. Whereas for the counterfactual probability, well, I know that it
moved in this case. So I should be pretty certain that it would have gone in if Ball A had been
removed. So now I have a way of teasing the two apart and can see which one better explains the
causal judgments. Is it the hypothetical judgments that I ask participants to do, or is it the
counterfactual judgments that I ask another group to do? And then I ask one group to give causal
judgments and then just try to relate them to one another. And what I find is when I look at the
hypothetical, so maybe I should say a little bit more about that plot here, at the bottom it basically
shows you the initial configuration of the block. Was it in the way or not? And then did it move?
Yes or no? So in this example here, it's one where it was initially in the way, but it moved.
But in the hypothetical condition, you don't know that because you only see it until they pause.
And then if you look at the hypothetical judgments, they think when it's initially in the way,
they think it's a little less likely that it's going to go in. And when it's initially out of
the way, they think it's a little bit more likely. So they're sort of a little bit sticky in terms of
what actually happened. For the counterfactual probabilities, pretty much only the final state
is what matters. If it was out of the way at the end, you think, yeah, it would have gone in.
If it was in the way at the end, you think it would have missed. And now if you ask people to
make causal judgments in this case, we see that they align very closely with the counterfactual
ones and not with the hypothetical ones. And this was for the kind of missed cases, but the same
story again holds essentially for the causal cases too. So they think that it caused it
when the block would have been in the way at the end and they don't think that it caused it when
the block would have been out of the way at the end. So enough to make this review too happy,
but maybe not Michael. I'm a happy guy. I'm curious. Can you go back one slide? Just to make sure I
understand. There were two things that changed in that intervention. There was the question you
asked, the hypothetical versus the counterfactual. It also sounds like the changes in how far they
saw into the video. That's right. That's right. And I'm picturing the counterfactual situation
where if you ask me the hypothetical question, but showed me the full video, so I see a whole video
and then you say, would be going into the goal if A was removed? Yeah, it's a tricky one. I mean,
I guess, you'd have to ask them, what did you think at the time before it happened? Did you
think? And people are often bad at that. We know that from all the hindsight research and so on,
that they have difficulty putting themselves back into the epistemic state that they had at an
earlier point in time. So I haven't tried that one. I haven't tried showing it until the end,
but then asking them the hypothetical question. It's possible, of course, that they will confuse
it as a counterfactual question. But for me, it was still sufficient, I guess, at least to address
this reviewer's concern, because his idea was really that computation is happening earlier. It's
happening before the causal event of interest, and then you're storing the output of that computation,
in this case, the hypothetical probability, and then just comparing that to what actually happens
at the end. So it's still felt that it's addressing that, but yeah. Okay, so having these two things
helps teasing them apart. Okay, I'll sum up the first part and then the second part will be short,
but that's okay. So for this counterfactual simulation model, but I've showed you that
there's this sort of nice correspondence between people's beliefs about the relevant
counterfactual and the causal judgments that they make, that it looks like that these things are
necessary, which we can show with the teleport or with the brick in and out of the way, that people
spontaneously engage in this kind of counterfactual simulation as evidence to the eye movements,
and that it's counterfactuals really and not hypotheticals that seem to be important for
expanding causal judgments. We've played around with this model like a little bit more. Once you
have a hammer, you find all the nails. So this one is just like looking at slightly more complex
cases. This one here, philosophers love, because it's a case of double prevention, where B prevents
ball A from preventing ball E from going through the gate, because it happens in maybe in football,
probably happens often when one tackles like another person that would have tackled the person
running with the ball. And so you might say, oh, to what extent did that cause it? You can also look
at omissions when nothing is happening. So ball A is just chilling here in the corner, and you might
still ask, oh, did it go in? Because ball A didn't hit it. And now you could imagine, well, if it
had hit it, what would have happened in this case? And we can also look at cases where really nothing
is happening at all. So it's just a tower of blocks. And you might still wonder, oh, to what
extent is this black one here responsible for the other one staying on the table? And even though
there's nothing happening, you might still say, well, how do you answer this question? Maybe by
doing something like playing Jenga in your mind, imagining it being removed, and then what would
have happened to the scene? So that even just physical support, in some sense, is very closely
related to ideas of causation. What it means to support is essentially to prevent something from
falling when we are another. Okay, so that was part one. Now a sort of short version of part two.
And so responsibility attribution was something that I've been into for quite a while and was
also my motivating thing. And then I drifted off into causality world, mostly just because physics
engines were around at the time. So it was like, oh, now I can use those. And with around at the
time, I mean, I was a postdoc with Josh Tenenbaum back then and physics engines were all the rage
at the time. And I said, okay, now I'll also use them. And there aren't really yet, although I guess
Mike was working on it, psychology engines, right, that are as easy where you could just have agents
and think about what they would have done. So this work that I had done on responsibility attribution
wasn't particularly social, also didn't really involve simulation, I think. And there was one
experiment that got a little bit closer that I'll briefly share with you here on a paper called
Moral Dynamics. And it will look very billiard ball world like I haven't moved too far away from
the billiard balls. But this kind of that somewhat agentive, right? So and so we could show people
like a video clip like this and then ask them what to what extent do you think that blue was
responsible that the green one got harmed in this case here. And our inspiration here came from a
paper called Moral Kinematics where they basically argued, again, it's somewhat more actualist view
and saying like, okay, there's certain features that people are picking up on in these scenes,
like the duration of contact, how far things moved and things like that. And then they directly map
from these features of the scene to the moral judgment in this case. And we liked the general
setup but didn't really like that model as much. So we proposed another model that has a slightly
different title, Moral Dynamics instead. And we thought, okay, yeah, these features are important,
but the features are important in that they give us evidence for the latent variables and that
those are ultimately the ones that I care about. And in this case, what are the latent ones that
we thought, one not very surprisingly here on the right hand side causality, but did you think that
it actually caused it, you know, for this negative outcome to happen. And then the left side, the
intuitive psychology part, very kind of minimal in this case here, but it's basically saying like,
well, maybe these features give you some evidence about like how much the agent actually wanted
to bring about this negative outcome. So if you think, for example, if somebody really wants
something to happen, then they're willing to incur a larger cost to make it happen, right,
like putting a lot of effort, for example. So if somebody puts in a lot of effort into something,
you know that they must have really valued it. And if somebody really valued some negative outcome,
well, that's a bad thing, right, that was roughly the idea here. And we could then show that if we
have a model that just basically in first the amount of effort that some agent exerted and tried
to map that onto the responsibility, that worked kind of, you know, OK-ish. If we only took into
account the causal role that some agent played and tried to use that to explain the extent to which
they're held responsible, that worked OK-ish. But if we now took a model that takes both of these
components into account, that worked pretty well, which was roughly in line with this kind of,
unsurprisingly, now this framework that I laid out at the beginning, right, that when we assigned
responsibility to others, we don't just care about the causal role that they played, but also what
the action tells me about the kind of person that they are. In this case, the action tells me
something about the desire that they had to bring about this negative outcome. OK, but still,
there weren't, we didn't really have a model, a real model of agents in this case. We still sort
of basically just use the physics engine. Also, we weren't able to talk about intentions, and it's
clearly important often when people talk about responsibility. And also still our kind of factual
simulation here was basically purely physical, just seeing how this thing would have moved
without the other one. So I don't have the skills to make it happen with sort of more
agentive agents, but luckily now that I'm here, I get to work out with all these smart people,
and here's my PhD student, Sarah Wu, and our research assistant, Shruti Sreeta,
and they've looked into cases now that are a little bit more agentive. They're still kind of in
grid world, but at least now planning and intentions and things like that are involved.
And here's the basic setup. So this is inspired by some previous work that has looked into helping
and hindering as like a case study. And what they did is essentially they said, well, what it means
for somebody to intend to help someone is that their utility function includes the other person's
utility with a positive sign. Intending to help just means like wanting to bring positive utility,
at least in this framework, to the other person. And intending to hinder puts a negative sign,
like now I want it that the other person has a low utility. So it turns out though that like
intending to help or hinder versus actually helping or hindering is not necessarily the same thing,
right? So here's an example. I don't have a child yet, but at some point maybe we'll have a child,
and then if I go grocery shopping with the child, there probably will be a period of time where
they're not actually helping, right? They're sort of like trying to help, right? But like
kind of making it worse, right? At least in terms of efficiency and so on. It's going to take longer.
Of course, it's useful because eventually they will be helpful. I have to go through that process
just like a PhD student. So yeah, so you go through that process, you know,
and then you might intend to be helpful, but it might take a little bit of time to actually be
helpful. And so, and the claim is, right, to evaluate that you need counterfactuals again
to tell, oh, is the person actually helpful? Well, how would it have happened, you know,
without them, essentially? Or there's different counterfactuals to consider, but that's one of
them. So here's our grid world that we played with, with the helping and hindering setup.
So we have this red guy here who wants to get to the star as a pure physical goal in this case,
just to get to that location. Then we have this blue one who has a pure social goal. They either
want to help or hinder the red one from getting there. And then there are these walls here that
you can't do anything about, but there's also these blocks, and only the blue one can interact
with these blocks. They can push, pull them, you know, out of the way. So here's our Hollywood clip
of what's happening in this situation. Okay, so in this case, you know, happy, happy end, like,
like a Hollywood movie. You know, red made it. And then we can show people these clips and we can
ask them, oh, how responsible was the blue player for the red player's success, for example, in this
trial? We can also ask them a counterfactual question, right, would the red player still
have succeeded even if the blue player hadn't been there in this case? And we can ask them to
make an inference about the intention of the blue one in this case. What was the blue player
intending to do? Were they trying to help or were they trying to hinder? Definitely help, definitely
hinder. So the idea is now basically the same as earlier by just saying, okay, again, we need some
kind of generative model of the domain. In this domain now, it's a model of agents basically
planning and recursively reasoning about one another, right? And that's now our probabilistic
program. And we can again compute counterfactuals over that, maybe in this case thinking, well,
what would have happened if the blue one hadn't been there? And then thinking how the red one would
have planned their path differently, but without the presence of blue. That's the rough idea.
So again, we take some actual situation and we can then simulate what would have happened in the
relevant counterfactual situation, in this case, where blue hadn't been there. We can talk later,
if you like, about other counterfactuals you might consider, but we just went with this one here.
But what if they hadn't been there? In this case, yeah, they wouldn't have made it,
because the block was in the way, right? We also have a model of intention inference,
but I'll sort of skip that. It's basically just saying, okay, if you have a generative model
about what an helping or hindering agent would do, you can then condition on the observations
that you see them acting and see what's more likely that they were helping or hindering given
the actions that they carried out. So I'll just give you a few more examples of the sort of video
clips that we showed to participants. That's a diagram of the one that you've just seen.
Here's another one where kind of blue sort of extra mean, you might say. There was already
a block in the way, but they put another block in the way. What the heck? Yeah, really trying to
be helpful through adversarial actions. So here's another one here where blue is sort of
laudably helpful, but like, you know, was not really necessary, but maybe looks nice.
Here's a case in which sort of things go wrong.
Where blue was maybe trying to be helpful, but actually sort of made it worse, you know,
through the actions that they took. And then here's another one. We had a large number,
so I'm just showing like a subset of them. So this is one where blue could have easily hindered
if they had wanted to, but didn't because they could have just pushed it into the way.
And so then we now have to again, yeah, try to capture whether we can with our model capture
the counterfactual judgments that people are making. And we sort of can, there's not as much
kind of variance here, at least in the predictions of the model. So this model is sort of okayish.
It captures the trends overall, but there's more variance in people's judgments that is not quite
captured by the model yet. So we're still, this is sort of more ongoing work. In terms of intention
inference, it's fine. So it can also kind of infer whether the person was helping or hindering,
but also here what you see is stuff are bunched up that the model all gives a hundred to,
where there's still some differentiation that people make, but sort of mostly captures what's
going on. And if we now look at the responsibility judgments, and we try to do the same thing
initially that we did with the billiard balls earlier, that we just take the counterfactuals
like on the x-axis and try to predict the responsibility here on the y-axis. It's okay
ish, but not, you always want, when you do computational modeling, you always want them
nicely line up on the diagonal. And that's not really what was happening in this case. Whereas
for the billiard balls, we had this very simple, the counterfactuals nicely predict the causal
ratings. But if again, if you have a model that incorporates also the intention inferences like
into the predictions, now they do sort of more nicely line up on the diagonal. Again, suggesting
that when it comes to assigning responsibility for agents, it's not just the causal role that
matters. It also matters what the actions that they took tell me about the kind of person that they
are. In this case, it tells me something about their intentions, like they try to be helpful,
or that they try to be hindering. So the both of these components. And just to give you a sense
of an example where we need this kind of intention part, like that's back to that mean one where
the blue one pushes another one into the way, right? And so just to help you kind of interpret the
bars here, the counterfactual, that's the condition where we asked them, would red have
succeeded if blue hadn't been there? That's basically our causal model. And they don't think so,
right? The pink purplish one is like very low, right? But also when we asked them what the
intention of the blue one is, they think, yeah, it was really hindering. So here zero means hindering
and 100 means helping. So they think, yeah, they were hindering. So even though they say that, yeah,
the blue one didn't really play a causal role, they still give them quite a bit of responsibility
like in the blue one on the right hand side. So that's one case, at least, where currently we
need this other part. So they think blue's actually make no difference, but they were
definitely trying to hinder. And so, yeah, I still give them some responsibility for this outcome.
Okay. So sort of almost last slide. Because we have these agents like recursively thinking
about one another, an interesting setting that also can happen here is that you can actually
hinder or help one another, again, maybe also like in the advisor-advisey setting,
not by actually making any change to the physical world, but changing somebody else's
belief. So I just want to show you that example. And maybe you'll get that intuition from the setting
here. So very, very mean, very, very sad. Because it looked really like blue was going to help,
right? And then they didn't, right? And here's just one participant, what they're saying,
oh, blue tricked red into thinking she was going to move the box to help, but then once red was
stuck on the side of the wall, blue left the box where it was, very sad, you know. And a lot of
people say something along those lines. We also had one condition where we just have them give
explanations of what happened, right? And here the interesting part, right, is that the hindering
is not happening because blue changed anything about the world. They didn't move a block in the
way or something, but they hindered because they made red believe that they were going to be helpful
and then they weren't, right? Here, if blue hadn't been there, red would have just walked along
on the outside and they might have made it, you know, anyhow, even without blue.
And this happens because they're recursively thinking about one another, right? And red
things like, oh, blue is taking actions that are going to help me so I can take the shortcut.
And then it turns out I couldn't in this case. Okay, wrapping up. So this was the second part
where we, I guess, applied this model now to at least a simple setting where agents are interacting
with one another, helping and hindering one another, that in order to judge whether somebody
helped or hindered, I again think that you need this process of counterfactual simulation
and that responsibility judgments are sensitive both to the cause of world that somebody played
and what the actions tell us about the kind of mental state that they had. Just to conclude,
so together, hopefully, this sort of set of studies gives some evidence that people seem
to be constructing these rich mental models of the world that we can get evidence for in
different kinds of ways, like through eye tracking and other tools. By imagining interventions,
like on these mental models, those allow us to compute the counterfactuals, which I think
are important for assigning responsibility, giving explanations, and so on. And that this
counterfactual simulation model that I've been kind of developing can then be relatively
flexibly applied to physical and social events where you think that the main thing that's happening
is that your model of the world changes and maybe the exact counterfactual cooperation that you're
carrying out changes, but otherwise the framework sort of holds. So with that, I want to thank
the main people who helped me do this kind of work, and then maybe you for your attention.
And there's a little bit of time for questions.
If you have a question, please grab the mic from...
So one thing I'm curious is, I assume notions of causality are probably somewhat universal,
but especially issues of moral judgment and tension are likely dependent to some extent on
environmental factors, cultural factors, those kinds of things. And so I'm curious if you've
either observed those in your experiments or if you have some way of controlling for those
factors when you recruit participants. Yeah, so that's an interesting question. And I think
even notions of causality actually, there are cultural effects like who you see there. So
when making causal judgments, there's often, there's basically like in many cases,
what's called the problem of causal selection. How do I even decide what thing to pick out
are as the cause in the first place? In my setting, very often, I've kind of made it pretty easy,
and I've sort of constrained it because I already told you like these are the possible causes.
But in the real world, it's not like that. And it's sometimes, we may see something,
we may see a person as a cause, or we may see a system as a cause, or we may also,
the kind of counterfactuals that may come to mind to us may also depend on what our
background is. And it often tells us something about, oh, when somebody then gives a certain
counterfactual, it tells us quite a bit about them. So this comes up in the context, for example,
also of victim blaming. Like if that's the counterfactual that came to mind to you,
oh, that tells me something about you. So I would say that even in that context,
there are strong kind of interpersonal and cultural effects that affect how we attribute
causality. Now, when it comes to intention inferences, I'm not sure that that process in
and of itself, that, at least to me, feels relatively whatever, universal, that we have
to engage in that all the time by trying to predict what other people are intending in the way that
we interact with them. But then again, how maybe then judgments in this case of responsibility
or morality like draw on these different components, for example, that I've laid out here,
no claim that that is in any sense sort of universal. It may very well be that in certain
cultures like this kind of what I take here to be more the person component may have a stronger
influence on responsibility judgments, and in others, it might mostly be about causality.
I certainly in my experiments for individual participants see a lot of variance along the
lines. But there's some people that don't care about even the intention part at all. They just
say like, oh, when it's about responsible, I just look at what would have happened if they
hadn't been there. And then other participants' judgments are suggesting that they care about
the intention part much more. But I have not yet engaged in the kind of work that then tries to
explain why is it, why is it that this person cares so much about causality and why is it that
this person cares so much about the intention part, for example. Thank you. I'm going to hog
the mic, actually. I'm interested. Do you think this model applies to other settings because
all of the examples were sort of like physical or agents taking physical actions? So if you had
just like a verbal description of some social scenario where there's like speech acts that
are causing things, do you think it would work the same? Yeah, that's a great question. So would
it work the same? So my sense is like, yeah, in a similar way, there's a number of things here,
I think. So we have applied the model a little bit, like this kind of counterfactual simulation
model in the physical world, also two speech acts. And there it's in the context of like,
we were basically jealous of, you know, Phil Wolff's, for those of you who remember Phil Wolff,
he had these different words, right? And we were like, oh, our model can only do like cause and
prevent. That's kind of sad. But there's other causal expressions, of course, right? Enabling,
affecting, letting, allowing, and so on. And it's going to be a little bit of a don't, but I'll get
there. So we were trying to see to what extent this framework that we have could also allow us to
explain differences between these different expressions, right? And this also comes up,
you know, in philosophy, like even questions, the question versus killing versus causing to die,
even people like in cases of abortion, you know, the way that you talk about it, right? Again,
reveals something, you know, how you think about it. And in general, right, like this distinction
also, when you have that as an alternative that you could have said killing, but you chose causing
to die, it suggests maybe a more roundabout way in which something happened, right? Like a person
killed, it caused them to die. You think, yeah, it would be weird to say that someone caused them
to die when they like, you know, directly walked up to them and, you know, shot them. This also
came up recently or still coming up these days, actually, with the case of Alec Baldwin rust,
like in the movie, right? The way that people talk about it was that, well, hold the gun that
discharged or something rather than, you know, shot the person, right? So it matters a lot,
basically, like in this case, the choice of word, right? And in some sense, the counterfactual
alternatives you could have had, right, for them, the image that it's creating in the listener,
in this case, right? So the fact that, oh, you chose this expression suggests to me that the
scenario must have been such, like rather than such. So that's at least the minimal way, I think,
in which it applies also to thinking about speech acts, right? And thinking like, yeah.
And of course, you could think like, oh, you know, again, take the advice example, would the
students still have done that if I had not said that, right? So we are obviously causing each
other a lot in the way that we talk to each other. And sometimes, you know, yeah, also, of course,
after talking, I might think like, oh, I wish I had answered this question from differently
than what I actually did. And I regret it, right? And things like that.
Yeah. So on a similar note, I'm wondering if you have thoughts on how possible it would be to
use this model on large-scale societal events that are divisive, such as what cost a person
to be elected, what cost code outbreaks, or what causes climate change, like how possible would
it be to apply this to those events and also what challenges you foresee? Yeah. Yeah, that's a great
question. And so I had the example, for example, at the beginning with like, oh, did the fall of
Lehman Brothers cause the financial crisis, right? That's sort of like large scale. And I don't know,
right? And partly it might, so, and there's a few options, right? One is like, okay, just like
totally punting, right? And saying like, okay, well, if the system gets sufficiently complex,
such that I cannot carry out the relevant counterfactual computation anymore,
well, I just don't know, right? I cannot give that causal answer. That's one version, right?
And there's another version where you say like, okay, well, to the extent that I can maybe,
you know, abstract away from a lot of the lower level details, let's say, of some,
so if I'm, if I'm, if I have the capacity to build maybe a more abstract model,
which, which I can now simulate, right, then I might be giving you an answer sort of on that
level, right? And so, but then it's also half punting, right? Because now you have to kind of
come up with a good model of how people generate the right kind of causal abstractions for some
situation that then allow them to compute the counterfactual because now it's not messy anymore,
right? And, and another thing that I should mention, and that quite a lot of the work on
responsibility that I've, that I've looked to particularly in groups, the sort of situations
that you pointed out, like elections and, you know, global warming, they're, they're characterized by,
by large degrees of over determination, right? Like an election, you hardly ever cast a pivotal vote,
right? And, and so those also traditionally were problems for counterfactual accounts,
like, because everyone can say, like, I made no difference, like, if I fly every day, you know,
that's not really going to make a difference. And so, and there you can, and similar with election,
why should I go vote, right? Because if my vote's not going to make any difference, right? And there,
at least models have been built that then say, like, okay, well, it's not, you're not off the hook,
right? It's basically saying, even if you would not have made a difference in this particular
situation, maybe the degree of responsibility that you have for some election, for example,
maybe related to how close you were to making a difference to the outcome, right? If it's like,
if the outcome is 6-5, you feel very responsible. If it's like 7-4, a little less. If it's like
8-3, a little less, right? But not, but it shouldn't go to zero, right? And then, and then as it,
maybe now relates to kind of, you know, global warming and so on, part of the challenge then
from the more like, you know, what do we do about it? Side might be like, okay, how do we make it
such that people don't perceive a sort of, you know, going to zero sense of responsibility,
right? Such that you feel like, actually, the actions that you do make a difference to the
outcome. And so, yeah, so that's, so I think a mix of thoughts, I guess, in response to your question.
So we're about at time. Is there a reminder if you are here? If you're logging attendance,
make sure to grab one of these code words up at the front and give Toby a compliment on the
talk on your way out, and maybe make you come up next year. Let's thank our speaker.

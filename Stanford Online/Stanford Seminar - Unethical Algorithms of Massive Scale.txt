Who was here last time I was here?
I know Hal was.
Oh, I know that guy, Rick.
And oh, I remember you two.
What's your first name?
Eugene.
Eugene, of course.
How are you?
Okay.
Good.
When I recall, when I was here last time, I came with a lot of slides and I decided
not to use them.
And I'm going to do the same thing again, except I think I'm going to show you one slide
because it just blows my mind.
So when we get to that one point, I'm going to turn this on and I'm going to show you
one slide.
And by then, I hope you'll be prepared.
Oh, dear.
Well, that shouldn't be.
How do we get rid of that?
We're just going to go like that.
So well, that's only half of it anyway.
That's not the important half of the slide.
So I want to take us on a journey.
And the journey is going to take us from the hypothetical to the somewhat real, to the
real, to the surreal.
So we're going to move in that direction.
So let's start with the hypothetical.
And in fact, I liked Andy's introduction, short, sweet.
He mentioned something about influence, perhaps on the internet, perhaps using new technologies.
Let's consider a few hypothetical situations.
Here we go.
We'll start with Facebook.
Let's say that last spring or summer, Facebook sent out, this is hypothetical now, sent out
reminders to go register to vote.
Now at the moment, they have nearly two billion members.
Among those members, there are 72% of the adult population of the US.
So if Facebook chose to send out, go out and register to vote reminders to their people,
they could reach a lot of people.
They could reach a lot of people because in fact, a lot of Americans who are eligible
to vote are not registered.
In fact, I think it's roughly 70 million Americans who are eligible to vote out of the 220 or
so who could vote are not even registered.
So if you take roughly 0.7 times 0.7, if you see where I got those numbers from, then
that means Facebook could reach 50 million Americans who are not registered to vote and
could send out reminders to them.
Now here's the hypothetical part.
What if they sent out those reminders only to people in certain demographics?
What if they chose to send out reminders just to Democrats, let's say, or just to Republicans,
or just to supporters of Donald Trump, or just to supporters of Hillary Clinton?
Would anyone know that?
I don't think anyone would know that because what Facebook sends out is always targeted.
No one keeps track of which groups are receiving those messages or those items on the newsfeed.
No one keeps track.
So in fact, hypothetically, Facebook could have sent out those reminders to register
to vote, reaching 50 million unregistered adults who are eligible to vote, and no one
would have known they could have done so selectively just to supporters of one candidate.
Now how many additional people would then have registered to vote?
We'll get to that.
Okay, now we're still on the hypothetical, but let me just mention on this one issue
that in fact, Facebook did send out reminders to go register to vote.
They sent them out en masse, and the New York Times concluded that because, very clever
and simple what they came up with, the New York Times concluded that because Facebook,
generally speaking, reaches a younger audience, and because younger people tend to be more
Democrat than Republican, then in fact, Facebook, even if it did broadcast these messages to
everyone, that what they did was advantageous to Democrats.
That was the New York Times conclusion, and they came up with some numbers, they came
up with some estimates, but in fact, we don't really know to whom those reminders were sent.
Okay, there's example one.
Now let's go to example two.
Election day, if on election day, Facebook chose to send out go out and vote reminders
just to people of one political party or just to supporters of one candidate, how many additional
people might they stimulate to get off of their sofas and go vote?
Well, as it happens, there's actually an answer to this question, because in 2012, Facebook,
with some people I know from the University of California, San Diego, published an article
about a manipulation that they did in 2010 during the 2010 election.
They in fact sent out go out and vote reminders to 60 million of their members on election
day in 2010, and they had a control group, and they did some surveying to try to figure
out who went out and voted in the experimental group, when the group that was getting the
reminders and in the control group, and they concluded that their reminders on election
day in 2010 caused an additional 340,000 people to go out and vote.
So if you extrapolate from that published experiment, if you extrapolate from that to
election day in 2016, that would tell you conservatively that if on election day Facebook
had sent out go out and vote reminders selectively to supporters of one party or another, conservatively
speaking, they could easily have caused an additional six or 700,000 people to go out
and vote who otherwise would have stayed home, and no one would have known.
So did they actually do that on election day?
I don't know, but as you'll see, we're going to move from the hypothetical to the real
and a little bit in the direction of the surreal, so we'll get back to this.
Now there are other things hypothetically that Facebook could have done to manipulate
people last year.
You may recall that a whistleblower turned up, a former Facebook employee who had been
one of the news curators, and in fact I met another of the news curators for Facebook
sometime later in New York, and who told me that no, they weren't sitting in a basement
as the press had reported, no, that was false, but yes, they really were a bunch of recent
college graduates with very liberal leanings, and yes, they were selectively removing conservative
news stories from the news feed that Facebook shows to people, and Facebook now seems to
be, according to some reports, the main place people are getting their news, or at least
getting links to their news stories.
So the curators, human curators, were indeed messing with the news feed in a way that favored
the Democrats.
So this is an actual whistleblower who came forth.
So this is, but let's keep it hypothetical, because first of all, we don't know that the
management of Facebook had anything to do that, it could have just been that they happened
to hire these particular people with these particular political leanings, right?
So just an error, really a kind of, you know, an oversight, you could consider, however,
let's look at this hypothetically, what if they deliberately wanted to do that?
What if it wasn't just an accident of hiring, what if they deliberately wanted to alter
our opinions on things by altering news feeds?
Could they do that?
Yes, are they doing that?
Well, there was that whistleblower, but you know, all those people got fired, and they
said they were moving to an algorithm, and so are they doing this now?
I don't know.
So it's still hypothetical.
So you've got the news feeds, you've got the trending stories, same thing could be
done there, right?
So Facebook has a bunch of different ways to alter opinions without people knowing.
That's what's important.
That's what's important.
They even have a fifth way, because are any of you here Facebook users and willing to
admit it?
Okay, well, that's most of you, wow.
You know they have a search bar, but what you may or may not know is that although the
search bar is usually used to find your friends and family, it can be used for other purposes
too, and before the election, Facebook posted a video urging people to search for election
2016 in the search bar, and then that would give you a list, again, a feed of information
which of course they had complete control over.
So they've got the search bar, they've got the trending stories, they've got the news
feed, they've got the possibility of sending out selective reminders, lots of things, none
of which would be visible to people.
Let's shift over to the big guy, Google.
So Google controls lists also, news feeds are lists, and Google controls lists.
Two very, very important lists, among others, but two extremely important lists, both of
which are generated on the fly when you get onto Google search and start to conduct a
search.
The first list is the little list up at the top.
Those are the search suggestions in what is sometimes called autocomplete.
Google invented autocomplete, 2004 I believe, and when they invented it, it had I believe
10 search suggestions, and it was an opt-in feature.
But a couple of years later, it was no longer an opt-in, in fact, you couldn't opt out.
And then sometime later, the list began to change, initially it appeared that the list
was just showing you what other people were searching for.
And if you, right now, for example, if you go and conduct a search on Yahoo, don't waste
your time because their search results are terrible, by the way, and they're pulling
almost all of their search results from Google under an arrangement that was signed between
the two companies in late 2015, however, let's put that aside.
The point is, when you try to do a search on Yahoo, you get 10 search results, and they
appear to be what people are searching for.
If you use Bing, which I also wouldn't bother with, but you get eight search results, which
also appear to be what people are searching for.
You can confirm that, in fact, the suggestions you're getting on Bing and Yahoo are really,
generally speaking, just what people are searching for.
How do you confirm that?
It's very simple.
You go over to Google, Trends, and you can see what people are searching for.
So, meanwhile, Google's list of suggestions over the years, for some reason, got smaller
and smaller and smaller, so that, at least on most devices, they only show you four items.
Occasionally, you'll get three, two, or one.
There are some circumstances under which you can get five, six, seven, but in fact, we actually
did an extensive survey to figure out how likely those possibilities are.
Very unlikely.
You generally speaking, on most devices, get four.
On some mobile devices, you get five, but the point is, it's a short list.
And the items on that list no longer have any obvious correspondence to what people
are searching for, so what are they showing you?
Well, the point is, here's a list that Google controls that we all see, perhaps, every day
or perhaps many times a day, and it's the search suggestions list.
And generally speaking, it has four items.
And generally speaking, those four items don't have any obvious correspondence anymore to
what people are searching for.
Very easy to show that.
You just look on Google Trends.
More and more, the items that appear on that list also are customized for the individual.
So that's one reason why, of course, they're not going to have much correspondence to what
people in general are searching for, because they're going to have to do with you and your
history and what Google's algorithm perceives as your needs.
So there's a list.
And then they've got this other list, because once you click on something, sometimes even
if you don't click on something, in the suggestions, the search results populate.
Sometimes you don't even have to click, and the results populate below.
So now you've got a second list.
So I've got a short list, and I've got a long list.
Now the long list, as you know, goes on forever.
But most people don't go beyond the first page of results, which shows you 10 results.
In fact, 50% of all clicks go to the top two results.
And more than 90% of all clicks are on the first page.
So what's on that first page is very, very important, extremely important, two lists.
Now let's get hypothetical here.
What if Google were using search suggestions not to help you do your search, although that's
what they would claim, of course, but they're actually a business.
They're not really the public library, like they pretend to be, they're actually a business.
And so what if Google were actually using search suggestions in a way that makes them
more money?
How could they use search suggestions?
It's hypothetical, I'm not claiming anything.
But how would they do that?
Well, as a matter of fact, we know a lot about this, that's when I get to the real part of
my talk, I'll explain to you how we know.
We know a lot about this, but just staying hypothetical for the moment, how could they
do this?
Well, first of all, if they know about you and your search history and your interests
and so on, in fact, they know a lot about all of us, well, they could put items there
that they think you're likely to click on.
And that they think you want to see, maybe, but more importantly, they could put items
there on that list that you're very likely to click on rather than doing what.
What do they not want you to do?
Yes, they don't want you to type your search term, your full search term.
This is hypothetical, of course.
So hypothetically, if I were running Google and I wanted to make a lot of money, I would
figure out how to show people search terms that made it very likely that people are going
to click on one of the search terms.
And I would also, since I'm now controlling what people are clicking on, I would make sure
that the results that populate as a result of a click or even without a click, I would
make darn sure that those results are results I want to show people.
See?
Yes, what is your name?
Hi, John.
Why don't you think they want you to type out what it is you really want?
You know, I don't really want to speculate about the motives, particular motives.
I guess what I'm saying is if I were in charge, then I would lose control over what people
were searching for.
Think about this.
If I let you go ahead and type your whole search term, and then I let you hit Enter,
I'm in a tough position now because my credibility always depends on giving you exactly what
you want.
So you always have to have that feeling that they answered your question, right?
So I'm really kind of stuck because depending on what you typed, I've actually got to give
you more or less what it is you wanted or what it is you thought you wanted.
See what I'm saying?
If I lose some control, whereas if I can get you to click on one of the suggestions that
I make, I'm in complete control.
That's just better from a business perspective.
So we've got this list of here, and again, hypothetically, Google could show people lists
that give them more control over what people search for, and another way to put that is
Google could show people search terms that would nudge people's searches.
You know that great book from 2008 by Thaler and the other guy called Nudge?
It's a great, great book.
And search suggestions hypothetically could be used to nudge people's searches in directions
that are advantageous for the company.
It's hypothetical, right?
Now let's get to the search results.
Search results, 10, those are the key ones.
Google has total control over the order in which these items are presented.
Hypothetically, they could put items near the top that they want you to click on, and
again, where doing so would be advantageous to the company.
And that could have to do with making money, that could be one goal, possibly, but could
have to do with other things too.
People agendas have to do with anything, really.
I mean, again, I don't know people's specific motives, I'm just saying one could hypothetically
use search results in a way to manipulate people's thinking, beliefs, purchases, certainly,
and possibly even their votes.
Okay, so we've got a few different hypotheticals here for Facebook.
We've got a couple for Google, and I could go on all day about other options Google has.
If you want to look at a cool article, it's a piece I wrote for US News and World Report
called The News Censorship, and that goes through lots of different crazy lists that
Google controls, and if you can control lists, wow, you can control what people think.
No, the Great Firewall is trivial by comparison.
The Great Firewall cuts off access to a lot of information, although all of the Chinese
students who work with me say that in China it's very common for people to use proxies
to get around the firewall.
But all it does is restrict access, where I'm talking about a much finer degree of control.
Also, the Great Firewall is visible.
Do you see what I'm saying?
It's completely different than the kinds of hypothetical situations I'm discussing so
far because I'm talking about methods of influence that are completely invisible.
If someone is showing you a list of search suggestions that have been carefully prepared
to make sure, make it likely anyway, that you're going to click on one of them and when you
do, that serves my corporate needs, you can't see that.
It's impossible.
And the same is true, of course, with the search results.
If I'm putting the results in an order that suits my corporate needs, serves my corporate
needs, you can't see that.
It's impossible.
Okay, so we got a few possibilities for Facebook.
We got a couple for Google.
Now let's go into crazy land here because remember, we're moving gradually toward the
surreal, but still, speaking hypothetically, Tinder, Tinder, Tinder, Tinder is a kind of
it's a matching service mainly for people who want to have sex with each other.
Very popular sex, I mean, and Tinder too.
So Tinder normally just shows you pictures of people and then you swipe left or swipe
right indicating whether you're hot or not, right?
This brings us right back to Bill Gates and his original app at Harvard that got him into
trouble.
So that's really what Tinder is.
It's just a hot or not, you swipe and so on, and if someone else who you said was hot
swipes you as hot, then you get connected with that person and that's basically what
Tinder is.
Oh, I'm sorry, Zuckerberg, absolutely, sorry.
Thank you.
I appreciate that.
Okay, so, but before the election, a couple months before the election, Tinder announced
a very odd application.
It was called Swipe the Vote and Tinder offered to help you figure out which candidate you're
better suited for, which candidate in other words better serves your values and your needs
as a voter, Tinder.
So sure enough, when people got onto Tinder, one of the options they had was to swipe the
vote, you know, let us help you figure out who to vote for.
So of course the people who are most likely to do this are going to be undecided voters.
That's gorgeous, that's beautiful because we know a lot about undecided voters and those
are the people who are easiest to influence.
So over time, I don't know how many people use Swipe the Vote because how would I know,
right?
But here's the way it worked.
It asks a question about immigration and it says, you know, you agree or disagree and
you swipe one way or the other and then it asks another question, you know, about taxes
and you swipe one way or the other and so on.
And it's just pretty much five questions and then it says, you're a perfect match for Donald
Trump.
So we're still in hypothetical land.
What if hypothetically the company had some bias in favor of one candidate or the other?
Couldn't they make it, couldn't they tell every single person who swipes the vote?
That they should vote for Donald Trump or Hillary Clinton?
Couldn't they do that?
Would anyone know?
Would anyone even notice?
You know, if they wanted to mask the effect, as we've done a lot of experiments on masking,
you know, and they didn't want it to be that obvious, believe me, that's trivially easy
to do.
But the point is, here's a matching service at a website used by tens of millions of Americans
which is advising people on how to vote.
But we don't know what that algorithm is doing.
We don't know whether they favor one candidate or the other.
Well, it turns out that Tinder is not the only matching service, vote matching service on
the internet.
They're popping up more and more.
Some look really, really credible, you know, they're nonprofits and this and that.
But you don't really know who's running them.
You don't know what their motives are and you don't know what the algorithm is doing.
So hypothetically, you could put matching services on the internet, including on big
websites like Tinder, to help people make up their minds about voters or about abortion
or about taxes or about homosexuality or about anything.
You could put matching services up on the internet and algorithms could be shifting
opinions literally by the millions because we know the numbers.
This is what we do.
This is all we do.
We just quantify these effects.
And no one would know because it's invisible.
Okay, I could go on with hypotheticals but you get the idea.
Oh, no, no, was Tinder's algorithm biased toward one candidate or the other?
I don't know.
Remember all those, I don't know, that's very important.
Okay.
To put this in perspective, what kinds of manipulations are making the news every day?
They're not what I just told you.
What are the manipulations that are in the news constantly, especially lately?
Fake news, that's number one, absolutely, positively number one, fake news.
And number two is Cambridge Analytica.
And Cambridge Analytica using massive amounts of data, some of which they kind of, you could
say stole or obtained unethically anyway, and a lot of which they bought.
Cambridge Analytica funded by that Mercer billionaire fellow who is a staunch Trump supporter.
And Cambridge Analytica supposedly helped shift the vote toward Brexit in the Brexit referendum.
And Cambridge Analytica supposedly also helped to put Trump in office.
That makes the news not as much as fake news does though.
Fake news in the news all the time.
We don't study, we could easily study the impact.
We could quantify the impact of fake news stories on people.
We don't, we don't bother.
We could quantify the impact of the kinds of manipulations Cambridge Analytica was using,
which by the way is just plain old marketing stuff, because all they were doing was customizing
basically images and language and ads to get people to click.
That's exactly what marketers do, right?
You use multivariate analysis, you keep changing things.
Now they, they had access to lots of data about people, about supposedly all the voting,
all the voters in America.
They claim to have more than 5,000 data points for every single voter in the United States.
So what I'm saying is that I'm not, we're not studying that.
Why are we not studying these incredibly, because they're trivial.
By comparison to the hypotheticals that I just described, they're completely trivial.
Why is fake news trivial by comparison?
First of all, you can see fake news.
You know there's fake news in front of your eyeballs, or you know there's news anyway.
You're not sure whether it's fake enough, but you sure as heck know that there's a human element
there, because it looks like a newspaper, and so a human must have written it.
And some, and usually there's an aim of a human who wrote the article.
You can see it.
That's very different than kinds of influence, which are invisible to people.
There's extensive research on this and social psychology.
If you influence people using methods, subtle methods that they can't see,
people end up believing that they made up their own minds.
You can still shift people's opinions and actions and so on,
but if they can't see the source of influence, they end up believing that they made up their own minds.
They have no idea that they've even been influenced.
So what's the earliest, obvious example of fake news?
What's the earliest example of that kind of influence in the United States that made big news back in the late 50s, I think, a long time ago?
Exactly, subliminal stimulation, that's right.
And this made big news in the U.S. a long time ago, because supposedly a movie theater in New Jersey was,
you know, had cut in these little frames into their film saying,
go buy a soda, you know, you're thirsty, go buy a soda, go buy our popcorn.
And that got into the news and there was a big uproar.
And as a matter of fact, the association that controlled television standards at the time, they made it supposedly,
they prohibited the use of subliminal stimuli, at least on television.
Some countries ended up passing very strict laws prohibiting it in all kinds of public situations.
The UK subliminal stimulation is unlawful, period.
We never made it unlawful here.
It's still probably used, but, you know, it doesn't really have that much of an impact.
It's just scary, though.
The idea that there's some stimulus that's affecting you and you can't really see it and
it's caused you to buy a drink, but it turns out subliminal stimuli do have an impact on people,
but it's very, very, very, very small.
So, you know, if you're building a business, that's probably not where you want to put your marketing money.
But the point is, invisible stimuli that affect people, you know, they've been around for a while.
There are lots of examples.
There's a body of research on this.
But what I'm trying to tell you is that looking at those hypotheticals,
we have now moved into a very different world,
where there are, hypothetically, means of influencing people by the billions online,
invisibly, without any awareness on people's part that they're being manipulated.
And, you know, the word ethics is in the title of my talk today.
Although the use of these methods is currently perfectly legal,
not because I think anyone would say they should be legal,
but simply because the law hasn't caught up with the technologies.
But I think most of us would agree that these are unethical, at least,
even if they're legal at the moment.
I think most of us would agree that they are unethical.
So, more and more, I think we need to be thinking about the ethics of what new technology is bringing to people.
All right, let's move now, more in the realm of real.
I don't want to spend too much time on this because I could go on forever.
And by the way, what time do I need to stop?
I know it's five something, but okay, good, we're running schedule.
So, let's move a little bit toward the real.
In early 2012, in fact, it happened to be New Year's Day, come to think of it, January for 2012,
I got a bunch of emails all from Google saying that my website had been hacked
and that they were blocking access through their search engine.
So, until that day, I had never given any thought to Google at all.
I just thought it was a great search engine.
And I started to learn some things about Google that made me more concerned about the company.
For one thing, I learned that they had no customer service department,
which I thought was odd, and they still don't.
No, actually, they don't, but we can talk about it more later if you like.
But it's not like lots of other companies where you just call an 800 number and someone answers
the phone and they help you solve your problem.
They don't have anything like that.
In fact, at one point when I did get someone on the phone from Google,
she basically said, I'm really sorry I can't answer that question.
I'm really sorry I can't answer that question.
I said, can you help me at all?
She goes, no, I'm really not allowed to help you.
So, that's the closest I got to a human being who was not a bot and it was an actual person.
I don't think she was a bot anyway.
And the point is I started to learn some things about the company, which bothered me.
It only took me five or six days to get my website taken care of, cleaned up, and so on.
And it took much longer to get it through Google sensors,
in other words, to get their algorithm to okay my website again.
But all right, it was just a hack, no big deal.
However, there were a couple things about this because I've been coding since I was 13 years
old and there were a couple things about what happened that bothered me and made me want to
look more closely at this company.
One was that not only did the search engine block people, warn people away from going to my website.
I understand that, right?
Google's crawlers found malware, I get that, right?
So, their search engine should warn people away, makes perfect sense.
But also, if you tried to get to my website or any of the 20 psychological tests that are
actually based there, so through other URLs you tried to get into those tests,
using Firefox, you couldn't get there.
And that doesn't make sense.
Firefox is a product of Mozilla, which is a non-profit corporation, and I don't get that.
I don't see how Google's crawler would have anything to do with Firefox.
And then I found the same was true with Safari, which is owned by Apple.
So, there were things like this that were bugging me.
I don't want to go into details.
I just want to point out that I started to think a little bit more critically about Google as a
company. Later that year, chatting with some people about search results and search rankings,
I got interested in that, not just on Google, but on search engines in general.
We're all constantly wondering about the search algorithm that they use and how they do this
ordering and how every once in a while they change the ordering, which puts another
thousand businesses out of business, and everyone's always wondering about those things.
It turns out that by late 2012, there was a growing scientific literature looking at
the impact of search rankings, search position, in other words, on people's behavior.
This was being done primarily in the field of marketing because where you are in the search
results depends a lot on whether your business is going to succeed or fail.
If you can get up one more notch, depending on your industry, that might be worth another
million dollars in revenue. So, in fact, there was a growing literature looking at
those little notches and how they impacted people. Among other things,
eye tracking studies showed that people's eyes would go up to the top of the list,
even when you deliberately constructed lists in which superior results were down at the bottom.
In other words, people were just hung up on the top stuff. It's as if people generally believed
that what's at the top is better and what's at the top is truer.
Well, Eugene, I was there. The two books I've seen about the history of Google,
all of them and all the other articles I've seen,
missed the fact that Larry and Sergey had a classmate named Luis Gravano, who is now
a computer science professor at Columbia University. And upstairs in Route 104,
Luis assembled the most amazing quarter or two seminars. Everybody who wrote a search engine
up through the 1990s was invited to come, the guys who wrote Alta Vista, Steve Kersh, who wrote
InfoSeq, and so forth. You go down the list and Larry and Sergey sat on the side of the room
and they started giggling because Larry had developed page rank with Terry and Hector and
other people. And I mean, relevance ranking, and this information retrieval is not my area,
but I certainly used dialogue back in 1976-76. I don't want us to get too much off track because
my time is limited. The thing is this, relevance ranking has been around for decades. And you
should talk to contact Luis Gravano and find out. I can go beyond that, Eugene, because as a matter
of fact, list effects have been around for centuries. And they have been studied in detail
for at least 100 years. So it has long been known that items at the top of a list have more impact
than items in the middle. Under some conditions, items at the bottom of a list also have more impact,
their names for all these things. And all this stuff has been well studied. But when I look at
this literature, when I looked at this literature, I was finding numbers that were just off the scale.
And it wasn't until quite some, I mean, sometime much, much later that we actually figured out
why items near the top of these search results are so impactful. They're incredibly impactful.
In other words, search results produced list effects that are orders of magnitude greater
than any other list effects ever studied. Okay? Well, with luck, I'll be able to tell you why
shortly. But let me just explain what happened. I got interested in this because I saw these big
numbers and I thought, well, if people have this trust for what's at the top, could you use search
results deliberately? I asked to alter people's opinions about things, not just alter their
purchases. In other words, obviously, purchasing was the main issue in these studies. Clicks,
click throughs, conversion rates, that kind of thing. But I was asking a different question. I
was saying, if people have enormous trust for these items near the top, could we use search
results to alter people's opinions? And I thought, what kinds of opinions could we alter? Could we
alter people's voting preferences, for example? That was a question that I raised. So early 2013,
working with a former student of mine, he was working for me at the time, Ronald Robertson,
who's now getting his PhD in a network science program at Northeastern University.
We decided to test this idea by randomly assigning eligible voters to one of three groups. In one
group, they saw search results which were ordered in such a way that favored one political candidate.
In other words, if you clicked on an item near the top of that list, you'd get to a web page,
which said awesome things about that candidate or terrible things about the opposing candidate.
Some people are randomly assigned to a second group in which the opposite is the case. They're
seeing search results that favor the opposing candidate. And the third group is the control
group. They're seeing the search results mixed up. Now, I thought that using this kind of research
design that we could shift, I predicted, two to three percent of the people in these, we call them
bias groups. I figured we could shift two to three percent of them using this technique. And I thought,
okay, that's not a big number, but still a lot of elections are very close. So if you could shift
two to three percent of your undecided voters reliably using, you know, search rankings, I thought,
well, that could have an impact on very close elections. First experiment we ran, the shift was
over forty-eight percent. Second one we ran, the shift was sixty-three percent. Third one we ran,
I think it was thirty-nine percent. These were all pretty small studies. Then we did a national
study in the U.S. with more than two thousand people. Shift we got was, again, about thirty-nine
percent. We also discovered very quickly that we could mask what we were doing. We could hide it.
Even in the first experiment we ran, where people were seeing highly, highly, highly
biased search results, a quarter of the people in the study seemed to have no awareness. I'm sorry,
a quarter of the people in the study, only a quarter of the people in the study, seemed to be
aware of the bias. Three quarters seemed to have no awareness. We found that just by mixing things
up a little bit, okay, so you, so I've got, you know, Trump, Trump, Trump, Clinton, Trump,
Trump, Trump, Trump, just by mixing things up a little bit, adding in a mask, we could easily
boost the number of people who were unaware that they were seeing bias search rankings to one hundred
percent. There was very, very simple manipulation to do and a very simple manipulation to hide.
Producing outrageous shifts in voting preferences as high as eighty percent
in one of the demographic groups that we looked at. We were, we're running experience that were
all speaking of hypothetical and we're moving toward the real, though we're not there yet,
because this is still all kind of science-y stuff, right? But then we did a big experiment in India
during their 2014 election there with more than 2,000 voters from throughout India,
right smack in the middle of the campaign. In fact, even after the voting process started,
we were still bringing in people who hadn't voted yet, because in India they have so many voters
that in fact they, they stretch out the voting process over a period of several weeks and we
were still conducting our study. And there in India where we got real voters and they're being
bombarded with information and they have high familiarity with the candidates, I was saying,
I think we'll still get an effect, but I think it'll be really small, one to two percent or zero.
I thought, I thought maybe the, the reality of the campaigning and the pressure and all that
would just overwhelm what search results could do. What we learned was that search,
bias search results could easily shift voting preferences by more than 20 percent with real
voters and over 60 percent in some demographic groups. In other words, here was a kind of
manipulation, oh by the way, 99.5 percent of the people in that study showed no awareness whatsoever
that they were seeing bias search rankings, 99.5 percent. So this is very different than fake news
and it's very different than even what Cambridge Analytica was doing, you know, where they're,
they're just coming up with good clickbait for people, because no one can see this occurring,
no one's aware that they're being manipulated at all, and yet we're manipulating them. We're
manipulating them sufficiently to, we figured out looking at, in fact, election statistics from
around the world, we were manipulating people sufficiently so that biased search rankings,
could, we calculated, be currently determining the outcomes of upwards of 25 percent of the
national elections in the world. That's mainly because a lot of elections are very close.
And depending on the country and depending on how, what the internet penetration is in that
country, what the percentages of undecided voters and some other things, you know, you can
actually calculate fairly precisely whether or not search rankings can be used to flip an election.
In some of our experiments, we were using web pages and search results from the
2010 election for Prime Minister of Australia. I mentioned this because the winner of that
election won by a margin of 0.24 percent. There are a lot of very close elections.
And even in our last election, Hillary won the popular vote by approximately 2.9 million votes,
but what percentage is that of the total vote of close to 140 million people?
Can you do that in your head? The point is, that's a pretty close election. A lot of elections are
close. Search rankings are very powerful. And so that's what we began to learn. So we since have
done many, many experiments on search results and their power to influence people. We've done
experiments showing that you can influence people's attitudes about things like abortion, fracking,
homosexuality. We've learned that if you let people do multiple searches and which they're seeing
mainly different sets of web pages, but in each case they're seeing a biased set of web pages,
that with each additional search, there's an increase in that shift that we call manipulation
power. So people who conduct multiple searches on the same topic, if they're seeing search results
that are biased in a particular way, that has more and more of an impact on them over time.
We've also done some cool work on the role that operant conditioning seems to play
in this effect, which we eventually called SEEM, the search engine manipulation effect.
I won't go into the details of the experiment, but basically what we figured out, what we confirmed,
is that the reason why search rankings have such an enormous effect, again much larger than most
list effects, is because there's something very peculiar about the way we use search engines.
The vast majority of searches that we conduct are of a routine sort. We're looking for facts.
Right? I go, tell me about where Rick Lozanski went to school.
Or I say, what is the capital of Massachusetts? And sure enough, over and over and over again,
the correct answer appears where? Right at the top. And of course, these days, it even gets up into
the so-called featured snippet or the Google box. So you don't even have to look at the search results.
We're actually doing experiments on the impact of the featured snippet. Right now,
they're running literally right now. The point is, in the experiment we did in operant conditioning,
what we figured out was that the reason why people believe that what's at the top is truer and better
is because there's this daily regimen of operant conditioning. We're like rats in a skinner box
in which we're learning over and over and over again what's at the top is better,
what's at the top is truer. So when the day comes, when we want to put in something a little
different, like something we're really unsure about, like what's the best vacation spot in the
United States? So there's no clear answer, right? It doesn't matter. We're going to trust because
of all that conditioning that never stops. We're going to trust what's at the top more than we
trust things that are down lower. It's really that simple. In the experiment we did, we actually
manipulated people's trust level to show that if you interfere with that operant conditioning,
in fact, people don't trust what's at the top so much and they start looking lower and they
start going to subsequent pages of search results. So we also have done a series of experiments that
will be published soon and looking at the way SIEM can be suppressed with various kinds of alerts
and there are people interested in this because, for example, you may be familiar with the project
called FindX. Anyone know FindX? It's a new project based in Europe. In fact, the man who
started this is now working with me and some other people and something I'll mention at the end of
the talk. FindX is meant to be a search engine which is transparent and fair in which the
rules for ordering are made public and in which even users have a say actually in determining
what the algorithm is doing and how it's computing search results. Another thing they are thinking
about is adding alerts. If you add alerts saying this set of results appears to favor Hillary
Clinton, that has an impact on the way people treat what's in the search results. You can also
add alerts to particular items in the list. So we have work coming out on that.
So we're moving gradually toward the real here. We also started last year studying
search suggestions. Now, again, we're studying things that are invisible. We're not interested in
fake news. Yeah. Yes, I have a question. Yes. Is there a group of people who are somewhat immune
to these effects you describe? In other words, everybody is affected by those?
Well, in the studies we've done in the United States, we've never found any, well, I mean,
there's going to be individuals who are immune, of course, but we have never found a demographic
group that's immune. I'm talking individualism. Is there an individual biologically possible
who would not be affected by it? Well, of course, anyone who is very, by nature, very skeptical
or anyone who's had a bad experience with Google or something, of course, individuals.
So there is a group of people who are immune. Because I don't think so. We are all the same.
Well, as I say, in the United States, we have never found a demographic group that was immune.
Never. Right. So it's just a question of how much people are swayed. Is it this much or is it this
much? But I mean, it's crazy. Everybody's a little bit. And if everybody's swayed a little bit,
how can we talk about facts and fake news? If the content producers and content consumers
also are influenced by other people's opinion, then everything is relative. Well, that's Abraham Lincoln.
You can fool all the people some of the time. Some of the people all the time.
No, no, I'm not talking about this. Let me just point out that we know how to suppress
these effects. So in the paper that we have coming out, we actually show how you can suppress the
in our control groups, we suppress the effect completely, 100%. So we know how to suppress
this kind of effect. If you mix things up, then people don't shift one way or the other.
And there are other ways to suppress the effect in varying degrees. So we're learning about that.
Let me just shift over to search suggestions quickly, because we now have learned a lot.
Okay, room, nice to meet you. We've now learned a lot about search suggestions and why we're
seeing the search suggestions we're seeing, why generally speaking, we're only seeing four.
We actually, I mean, really have learned so much about this that again, it's a whole scary area.
So we've named a new effect called the search suggestion effect, because search suggestions
can in fact be used easily to manipulate people's opinions, attitudes, beliefs, behavior, voting
preferences. We've even figured out where the number four comes from. And the key there to put
it just briefly, the key to that number four has to do with what happens with negative search
suggestions. You've heard of negativity bias, I'm sure, because people study it in a half a
dozen different fields. Sometimes it's called the cockroach and the salad phenomenon. When
there's something negative, and then when a stimulus is negative, like a cockroach in your
salad, your attention is drawn to it and it ruins the whole salad, and you send the salad back.
Now, if I put a piece of chocolate into a plate of sewage, that does not upgrade the plate of
sewage at all. So something particular about negatives, well, it turns out our new experiments
show that there's something very special about those negatives in this list of search suggestions too.
And guess who knows that? Google. So in June, July and August of last year, we documented the fact,
this was partly based on a video that had gone viral in June, then in fact, Google was systematically
suppressing negative search suggestions for Hillary Clinton. Now, when we went public with that,
and others went public with that, so it wasn't just us, but when we went public with our findings,
Google flipped the switch. They literally just turned off the manipulation, just like that.
And from that day on, you could start to see negatives when you did searches for anything
related to Hillary Clinton. But June, July and August, it was virtually impossible to get any
negative search suggestions. We've learned that when there's a negative in that list of four,
it can draw 10 to 15 times as many clicks. And the more undecided someone is on an issue,
the more clicks the negative draws. So one of the simplest ways to manipulate people's opinions
invisibly is through the differential suppression of negative search suggestions. That is to say,
suppress the negative search suggestions for the position I'm supporting,
and I allow negative search suggestions to appear for the other position, the one I'm not supporting.
And what we're now doing is quantifying what that does to people's searches and what that does
to people's opinions and voting preferences. We're doing that right now. But we even figured
out that number four, because it turns out that if there is a negative in the list,
and I, as I add more and more alternatives to that negative, because I want people clicking
on that negative, believe me, I'm allowing that negative to be there because I want people to
click on it and I know it attracts attention and I know it attracts clicks. But the more alternatives
I add, the fewer people will click on the negative. It dilutes the impact of the negativity bias.
Right? Now, by the same token, I want to keep adding more items to my list. Why?
Because I don't want people finishing their own search term. I don't want them doing that.
So on the one hand, I want my list to be long. On the other hand, I want it to be short. Well,
it turns out those two distributions overlap perfectly with one optimal value.
Guess what it is? Four. Four is the magic number. We didn't know if this was going to come out of
our data, but it popped right out. Okay. So we're learning more and more about how these things
work. Now we get to the cereal. You know, all this stuff, even the experiments, even the experiment
in India, in some sense, it's all hypothetical, isn't it? Because you don't know what people
are really seeing. I mean, to see what people are really seeing, I'd have to creep up behind
Eugene like this and I'd have to look over his shoulder and go, are there any European nations
that we're practically unaffected by the two world wars? Now I know because I crept up on him
and I looked over his shoulder and wouldn't we have to do that to really see what people are seeing?
Okay. So I tell the story, the full story. It's coming out in a couple months in a piece called
Haming Big Tech and I recommend it to you highly because I'm told by friends who read it that it
reads like a spy novel. And I tell the story of what this crazy thing we did. So now we're in the
surreal realm here. Starting in late 2015, early 2016, we set up a Nielsen-type network of field
agents around the country. All these people were recruited in a clandestine manner. We took incredible
steps to make sure that they could not be identified, which Nielsen does too. Nielsen does the same
with the families they used to rate television shows. They've been doing that since the 1950s.
So we recruited these people. We developed a custom add-on for both Firefox and Chrome
that all of these are field agents installed on their computers. That gave us control over
information that we would be collecting automatically when they conducted searches.
In particular, searches using any one of 500 different election-related search terms
that we control. We control that list. Sometimes we could collect whatever we wanted.
But as it happens, we were only collecting information about election-related searches.
And we got our first data starting to come in on May 19, 2016, and we kept going up to the
election. And as we worked out the kinks in our system, the rate of data flow increased. And
ultimately, we preserved 13,207 searches on Google Bing and Yahoo and the 98,044 web pages to which
the search results linked. And of course, we knew what search positions the links were appearing in.
So in other words, we had the ability to determine. We were not looking over the shoulders of our field
agents as they were conducting searches and preserving their actual search results and
preserving the web pages to which all 10 search results on the first page linked.
So this had never been done before, apparently. And it was tremendously exciting. And it was very
nerve-wracking. We then used crowdsourcing to determine whether the web pages were
favored Hillary Clinton or Donald Trump. And we concluded that, in fact, for roughly,
for merely six months before the election, Google's search rankings were biased in favor of Hillary
Clinton. We also determined that the bias in Google's search results was larger than the bias in
Yahoo's search results, which was much, much smaller. And of course, the fact that they
have a bias shouldn't be too surprising since they're pulling almost all of their
search results from Google. And then what about Bing? Well, it turns out we couldn't use our Bing
data. There were a bunch of data we couldn't use. Why? Because some of our field agents were commuting,
we're communicating with us using Gmail. We deliberately recruited a few field agents
deliberately who used Gmail because we knew that if Google took an interest in what we were doing,
it would be very easy for them to identify those people. So that was kind of our control group.
So that brings me finally to the one slide I'm going to show you.
Now, is this real or surreal?
This is showing you over a 25-day period before November 8 and including November 8.
This is showing you the bias, if any of those points above the line is showing bias
or favoritism for Hillary Clinton, this pro-Clinton, below the line that would be
web pages favored Donald Trump. So this is showing you 25 days before the election
and you see there are pretty clear favoritism for Hillary Clinton in search results. And by the
way, it said nothing to do with the search terms because if you look at the search terms people
were using, the search terms actually slightly favored Donald Trump. So this was not an effective
search terms. This is an algorithmic effect. And now this is what's cool. These are all
non-Gmail Google users. What about that control group we had? What about the Google users who
were also communicating with us during all these months using Gmail?
Now, to my eye, those graphs look different. Statistically, those numbers are dramatically
different. At the point 001 level, they're dramatically different. You can draw whatever
conclusions you like regarding why we got this difference, but what this tells me is when you're
going to conduct a study like this, you should be very cautious about how you conduct the study.
What we realized at the end of all this was not so much that our numbers really were very
important. What we realized is that we have the ability now to look over people's shoulders as
they're looking at Tinder and they're swiping as they're using Facebook, as they're looking at
Facebook feeds, as they're looking at not just search rankings, as they're looking at search
suggestions, you can use the same add-on technology that we successfully developed here in this project
to look over people's shoulders around the world. You can scale up what we did
and set it up in country after country after country. When we realized that this was possible,
what we really had here, a way of keeping tabs on these big tech companies and what they're
showing people, then I called up some people I knew, including one of these guys over here
and a guy some of you know named Dennis Allison and some other nice folks, Jake Shapiro from
Princeton University and Martin Moore from King's College London and on and on and on. The list now
is growing and growing and growing of people at major institutions who've become part of a group
that is working to set up a new organization. It's called the Sunlight Society.
And the Sunlight Society will serve as a kind of monitor of technologies that are being developed
which could in fact influence people's behaviors, people's opinions, people's votes,
people's purchases, perhaps without them even knowing. This kind of system, whether we do it
successfully or not, it needs to exist. There's definitely a need for this at this point because
everything that I've been saying up until just a few minutes ago was all hypothetical, wasn't it?
This is not so hypothetical anymore. This is much closer to real or even surreal. This is weird.
You can use this technology to look at demographic effects, to look at what these
companies are showing people, how individualized these stimuli are that people are being subjected
to. You could look at anything that people are seeing on their screens, have it instantly transmitted
to servers which is what we did, have your servers do an analysis and we're now working on
automating the analysis of bias ratings, for example. And all of that stuff can be automated.
You can train algorithms to evaluate text in much the same way that humans evaluate text
and those algorithms are getting better and better and better. And in fact, both Google and Facebook
right now are using algorithms like that to try to identify fake news stories. The point is you
could be collecting data in real time on many different platforms, analyzing the data in real
time and finding the problems maybe before they get out of hand. You could share these findings
as appropriate with the media. You could share them as appropriate with law enforcement agencies.
Courts, I had a conference call a couple of weeks ago with the two top investigators in the three
antitrust actions that the EU has brought against Google. They're very interested in this kind of
technology because the evidence they have to support some of the claims that they have made
against Google is actually pretty weak compared to what we have. So I think we're there. We've
gone from hypothetical to a little bit more real, but still somewhat hypothetical, to a lot more real
and then the possibility of really finally being able to make companies like this accountable to
the public. If this interests you and you'd like to help, you know, join this effort,
again, whether we do it successfully or not, we know it's going to happen and we know it needs
to happen. And that's my story. Thank you. So, given the two examples that gave of Facebook and
Google and given the powerful effect, I'm sorry, and Tinder and given that this effect is so powerful
as you state, why did you do it from when? Well, he won because of the peculiarities of the electoral
college and the American people didn't choose him. I mean, Hillary Clinton won by almost 2.9
million votes. She won the popular vote. Maybe in California with nothing about it. That's irrelevant.
I mean, if we had a direct vote kind of system, which they have in many countries, then she would
have won. You know, the analysis of that election, and people are going to be analyzing that election
for a hundred years, and we all know there's a long list of reasons why Donald Trump won.
But I will tell you, and I guess it's on the record because I'm being recorded, I will tell you
that then I'm a friend of some people in the Trump family, and then I was in touch with them on
election eve, and that there came a certain moment in time, I won't tell you the exact time,
where I got a text, and the text said, we are all shocked here. And when this person said we,
this person meant we. See what I'm saying? Not only did they not believe they were going to win,
I personally don't think based on, again, my personal knowledge of some of the people involved,
I personally do not believe he had any intention of becoming president, which is why he is still
looking very much like a deer in the headlights. He was trying to increase his celebrity status,
he was trying to lay the foundations for setting up a huge media network, and he was putting all
those pieces in place. And this is not what these people had in mind, which is why they're kind of
all scarring around, and there's just complete chaos, and you know. So he won because of what
historians will tell us 50 years from now, that's why he won. See what I'm saying?
Well, you know, the question is, again, hypotheticals, right? The question is,
was Google using, and was Facebook using these manipulations to the full extent that they had,
that was possible? And we have reason to believe that they were not. I mean, we know, for example,
that Google turned off that negative search suggestion manipulation in early September.
We know they just turned it right off, like that. So I think that these companies that,
you know, behind the scenes, or in some cases very openly were, you know, wanted Hillary Clinton
to win and were supporting her in all kinds of ways. I mean, Dustin Moskowitz, am I pronouncing
that correct? One of the co-founders of Facebook, he donated just a couple of months before the
election. He donated $25 million to the Democrats. So, you know, these companies, and the people who
worked for these companies, they were very strong supporters of the Democrats. But I think, number
one, I think that they held back a little bit on the manipulations. And number two, I think that
they were just overconfident. I think they were overconfident. The polls said consistently that
she had it in the bag. And I think they just got overconfident. And maybe we're being a little
cautious. And I don't think that they used all the tools that they had available to them.
But that's why you have to have a monitoring system in place. Because the historians are
just going to be speculating. We don't need to speculate. We can monitor. We can track. They
track us. We can track them. It's that simple. And then this won't be speculation. Why is this
all speculation? There was an article that came out in The Guardian, which has done a very, very
good series on high tech and very skeptical about, you know, what high tech is serving up to the
world. It was a very good piece in early December talking about the Brexit issue. And this was by
Carol Cadwalader. I've spoken with a number of times. Very, very, very, very smart journalists
and good investigative journalists. And in this article, she laments the fact that what
people in the UK were seeing on their computer screens, you know, back in June when the Brexit
book occurred, that it was all lost, that we can't know what Cambridge Analytica was showing people.
Because it's all lost. It's all ephemeral, right? Most of what we see on screens is ephemeral,
especially when we're generating search results. That's ephemeral. It exists for a couple seconds,
and it has an impact on us, and it goes away, and it's gone. So what she was saying was,
if only, if only we could go back in time and see what people were seeing on their screens.
Okay? We could do that. I have an appointment on Friday with folks from the Internet Archive,
which is not far from here, which is, you know, Brewster Kale's project. And, you know, they've
been following what we've been doing since almost the beginning, because at some point the Internet
Archive is going to post our database for everyone to, you know, pour through. But I mean, that's
what we need. We need organizations like the Internet Archive working with people who develop
monitoring systems, an organization like the Sunlight Society that not only develops and scales
up these systems, but that looks around the world for other people developing similar systems,
and kind of coordinates, coordinates the efforts. And this, that we don't have to speculate. We'll
know what's happening. And this could result in something wonderful. It could actually get
some of these manipulations to disappear. It's possible if a good monitoring system were in
place that some of these companies who are doing some of these crazy things, think about swipe the
boat, will stop because they'll realize this is being recorded. This is being recorded.
Yes, what is your name? Brad. Now, are you one of the famous or infamous actual students?
I guess I am. Wow. Well, I will say I have not been here for that many times.
Well, thank you for coming, person. Thank you. Yeah, this is definitely quite interesting. I'm
like curious to see more of this data and the need for it is like, it's definitely very clear.
It's less clear to me like how we can actually be interpreting these results
right now anyway, without having more of it. Just because like for one difference right off the
bat with like gmail users, you're always logged into your Google search results. Like that's
a completely different set of personalization and development going on at Google, right to be
showing you that you know which results those are. Right. So that's just like already a difference
there. And then just thinking about how do you measure I think the hardest part there more
fundamentally is like how do you measure what is a bias set of search results. Let's see, Brad,
I can see this in your head. I can see the gears because you're not just asking these questions,
you're answering them in your head at the same time that you're asking the questions.
Deny that. The first. Deny it. Yes or no. I think I have two different questions. One I have like
one imagine to answer for which is the personalization components. Yeah. Which is,
but I think I could be dissuaded at that one. Right. But the second one. It's easy, right?
You know how to do that, right? Oh, how to change the data. Oh, I'm saying though that you know how
to track the personalization. It's very easy. Yeah. I'm just saying it's harder to get. I can
also imagine coming up with algorithms that seem very neutral and are tempted to be very
neutral that would give identical effects. And that's why I think it's very interesting to study
more and see if you can like tease out and but B, I'm not ready to like leave to the conclusion
that Google is manipulating its results to influencer. I don't know what they're doing,
but I know how to track what they're doing. And I'm learning more and more about how to
automate the analysis of the data that we're collecting. And I know how to set up systems
like this. So I think one thing I am curious about is how do you figure out what's a bias
set of search results? Well, we just thought we that's a very good question. This question
comes up all the time. And I sometimes I regret using the term bias because it's a loaded term.
And I'm not using it in a loaded way, believe it or not. I realized bias sounds like prejudice
and things like that. And I don't I'm using it in the way psychology researchers use the term,
which is that that favors one perspective over another. And so when we have when we do use
crowdsourcing to rate, you know, whether a page is pro one candidate or another, we just we give
them an 11 point scale goes from five to zero to five, right? And here's candidate a and here's
candidate b. And we say, read the webpage and just tell us whether it favored and on this 11
point scale, whether it favors candidate a or candidate b. So bias is an unfortunate term.
The point is you can you can take terms like that operationalize them until you're satisfied.
So I don't think that's what the problem is. I don't even think that bias per se is the problem.
I think we're talking about a whole new world that is emerging. And this is I'm actually working
on a book on this subject trying to think ahead 10 or 20 years. Wow, is that impossible these days?
But I think a whole new world is emerging in which effects of the sort that I've been telling you
about are simply going to multiply. And so, you know, it's going to be a game of catch up. And
that's one of the reasons why you have to have monitoring systems in place, because even if you
don't understand how those apparently neutral kinds of stimuli, right, but that you mentioned,
even if you don't understand how that's being used to manipulate people,
well, if you at least if you capture the information, you can go back and analyze the
crap out of it. And maybe you can figure it out. And I think that's the world that we're headed
toward one in which new technologies, I mean, imagine how Google Home could be used to manipulate
or the new product that Apple just announced. But Apple has never had this motive, by the way,
because Apple has a different business model. Apple actually sells products. Microsoft actually
sells products. I realize more and more companies are moving in the direction of Google's business
model. But Google doesn't sell any products. Not really. More than 90% of their revenue is
still advertising revenue. They're they use cool looking data collection platforms to collect data,
and then they leverage that data to make this this year, they're going to make over they'll
have revenues of over $100 billion. So Google is still the place you have to watch. And
secondarily, Facebook, Google currently controls five out of the six billion platform
applications in the world. And there's only one left. And that's Facebook controls the other one,
which is social media. But what I'm saying is Google, you have to keep an eye on Google,
but they're going to be other companies. It's not just Google. They're going to be other companies
doing other things that have never been done before. You know, these these effects, we're
now studying four effects that have never existed before in human history, completely
unprecedented, almost entirely invisible, with which produce enormous shifts in people's thinking.
Our if we're if we have identified and are studying four,
what do you think, could there be five?
How many are there? I don't know. You know, we've we've found and are studying four.
So that does that mean they're actually 10? Does that mean they're actually 100?
I don't know. But I do know this. Next year, there'll be more of those kinds of effects than
there are this year. Hal? So let me let me shift to the advertising world. A product is to try and
give us some of the emotions associated with politics. There's an incredible battle between
search engines trying to give people what they want, and advertisers trying to
bump their stuff up there, whether they deserve it or not. Yes, that's right. So how do you decide
what's fair in this war? And in terms of if I translate this back into politics, how am I going
to decide what's fair in that world? You know, is your crowdsource evaluation? I can tell you
what I am, and I can tell you what I'm not. Okay, I'll start with the knots. I am not a lawyer.
I'm not a public policymaker. I am not a thought leader. I am not. You know what I am? I'm a
researcher. I'm a really, really good researcher. The more I've done research over the years,
the more I realized I'm good. I know how to figure these things out, and I love doing it. And that's
about as far as I could go. No, I think that's cool though. I think that's pretty far. I think
you're on a wonderful project. Okay. The one thing you haven't mentioned yet is keeping track of the
stuff that wasn't displayed so you know when you try and go back a year and evaluate something
as to whether, you know, the stuff that you got was the right sample. Well, that's why we want to
scale up the kind of thing that we did. If you scale it up large enough, you can keep track of
all kinds of stuff. Look what Brewster Cale's organization does. You've heard of the Wayback
Machine? I mean, the Internet Archive takes snapshots of the entire Internet pretty much.
I don't know if they do the dark net, but I mean, at least, you know, the Internet we most of us
can see. So, I mean, if you have the resources, you could capture lots of different, and what is it
we want to capture ephemeral information? That's what we want to capture. That's what is normally
completely lost. Fantastic. Yeah. And what I'm saying is we can capture ephemeral information.
Hold on to it. Analyze it now or analyze it later.

So this work is part of a larger thread of work centered around algorithmic governance.
That is, how do we audit and measure problematic content, like mis-disinformation, online extremism,
and how do we do that on online platforms, and then specifically the search and recommendation
algorithms driving these platforms.
So as I was preparing for this talk, I thought of opening it up in a slightly unconventional
way with this particular quote, which I'm going to read a little bit, which says, mankind
barely noticed when the concept of massively organized information quietly emerged to become
a means of social control, a weapon of war, and a roadmap for group destruction.
Any guesses as to which year of the world this quote describes?
Is it the world, is it a current world, 2020s, 2000s, 1900s?
Some people could guess just by the word mankind in previous talk.
Yeah, that's absolutely accurate.
So this is actually Edwin Black's description of the world of the 1930s, 1940s, during the
World War II, and the invention of the punch card, which was at that time the new wave
of automation and data collection.
And what is most surprising is that this quote is still true in today's 21st century world,
and we have seen many new scholarships, and I'm sure folks here have read and are familiar
with these books, many of these outline the harms of automation and data collection.
And more recently, this discussion has shifted to the harms posed by generative AI and large
language models.
So I'm going to read this quote a little bit more of what Edwin Black had to say in his
book, which is a lot of parallels to a present day world.
So he goes on to say, the unique igniting event was the most fateful day of the last
century when Adolf Hitler came to power.
But for the first time in history, an anti-Semayet had automation on his side.
So that was what was different, and the automation was in the form of IBM punch cards.
So he goes on to say that IBM was self-criped by a special immoral corporate mantra, if
it can be done, it should be done.
So we should take a pause and kind of re-read this phrase, if it can be done, it should
be done, which is parallels, very re-resemblance to the motives and culture practiced in modern-day
technology companies.
And I'm sure, being in the Silicon Valley, you all probably have seen some of this while
interning at some of these companies, move fast and break things.
Done is better than perfect.
What would you do if you weren't afraid?
And I argue that the sort of rushed culture, the sort of culture of disruption and speed
often poses a great challenges in governing these technologies and conducting thoughtful
medicalist audits.
And in turn, this is what makes algorithmic governance a difficult problem.
And I'll come back to these ideas later on towards the end of the talk, but let me first
dive into the meat of this presentation, a couple of studies which we did in this realm
of algorithmic governance.
So here I want to focus on two types of algorithms, search and recommendations, and two specific
platforms, YouTube and Amazon.
And our focus was only one type of problem, misinformation particularly.
Now you might wonder why search and recommendation algorithms, right?
So now the world is moving towards large language models, why we should focus on these old technologies,
search and recommendations.
So users generally have an unwavering trust in search engines, right?
So several scholarly work, some of these cited here, have actually shown that these ranking
of search results have a really dramatic effect on users' attitudes, their preferences,
their behaviors.
In fact, bias search rankings are so powerful they can even shift voting preferences of
undecided waters by so much so as 20%.
And users don't even show awareness of that kind of manipulation happening in their search
results.
So this should tell you how powerful search algorithms is, and then that is why we should
keep studying them despite the other new shiny technologies coming our way.
For recommendation algorithms, I hope I don't have to make a case here because you see those
almost everywhere starting from what recommendations, what movies you should watch, what products
one should buy, which campaign you should donate to, they are almost everywhere.
And they also have these reinforcing effect, right?
So the more you like, watch certain content, the more you share, the more you get those
types of content.
Now this might be harmless, say, when you're going out this weekend and trying to buy Christmas
decorations, but these things can get ugly quite quickly when, say, you are browsing
for vaccine information, health information, or climate information.
Now why audit for misinformation in particular, right?
So currently there is this disproportionate focus on AI bias and fairness, and tech journalist
Karen Howe captures this notion very well in her article where she mentions how often
responsible AI teams and companies are pigeonholed into targeting AI bias.
Now don't get me wrong, so bias and fairness are indeed important topics that one should
pursue, but tackling just AI bias draws away attention from fixing much bigger problems
of other types of harmful information such as misinformation, extremism, conspiratorial
content.
So this is what is underlying motivation behind the set of studies that we did.
And with that, let's dive into this first study, the YouTube audit work.
Here a major motivation for this YouTube study was coming from these frequent headlines
that I was noticing a few years ago, how YouTube is driving people in the internet's darkest
corners, and then there were these opinion pieces talking about YouTube being the great
radicalizer.
But YouTube was also responding with articles saying that they will be reducing conspiracy
theory recommendations or making it much harder to find those on the platform.
And all these questions and all these reports are really anecdotal, like empirically how
bad is this, right?
Do we really know that?
And this is what I studied to try to do, verify these anecdotal claims that does YouTube really
surface these problematic content.
And in order to do this, we conducted these systematic audits on YouTube search and recommendation
algorithms and we picked one type of problematic content, conspiracy theories.
Now what really is an audit, right?
I have talked about, mentioned audit a couple of times so far, but how do you audit algorithms?
I'm sure some of you in the audience might be familiar with the concept of audit.
For those of you who don't know, I'm going to give you a quick definition through an
example and also say that this is a thriving field of research, lots of work has been done
in this space.
And this was one of the earliest example of audits coming from the social science world.
And it's also one of my favorite ones from this 2004 paper, where researchers conducted
this very clever field experiment to investigate employment discrimination.
That is, they audited the labor market for racial discrimination.
So what they did was they responded with fictitious resumes to help wanted ads in the Boston
and Chicago newspapers.
To manipulate the perception of race, what they did was they kept everything else in
each of these resumes constant, only they changed the names.
And the names were either very African American sounding names such as Laquisha or Jamal or
a very white sounding names such as Emily or Greg, and hence the name of this paper.
The results showed that there was significant discrimination against African American names
while white names received 50% more callbacks for interviews compared to the African American
names despite everything else being constant in those resumes.
So this is a core idea behind audits, that is, you would keep everything else constant
and then you manipulate a single variable to determine how that change would affect
the algorithm.
So if you translate this into the context of YouTube, you would manipulate a variable
to determine whether the search and recommendation algorithm returns different results, say when
someone's age, their other demographic attributes, gender, their watch history, where they are
searching from, the geolocation, if that differs, what happens with the research results.
So to answer this, we set up this elaborate audit framework which broadly looked like this
where we had programmed bots or in other words we were conducting the sock puppet audit.
So these were bots or sock puppets which behave like normal users logging into YouTube, running
queries on the search platform while at the same time a script at the back end were collecting
whatever search and recommendation results the platform was returning.
So we started with selecting search topics and our goal here was to make sure that the
topics are indeed high impact, that is lots of people are searching for these topics so
they should be popular and they should be topics which also have the potential to return
false conspiracies.
So we did some more background work referring to Wikipedia, comparing with Google Trends
and we came up with this list of five different topics, 9-11 conspiracies, vaccine controversies,
moon landing conspiracies, chem trail and then flat earth.
And then we audited three components of YouTube, the up next video, the top five recommendations
and also YouTube's search results.
For demographics we checked for four different age groups and two different types of gender,
male and female and to emulate this we had to create eight different sock puppet combination
accounts and then for geolocation we found this hot and cold regions, that is we call
these hot and cold regions because these are the regions which have the highest or the
lowest interest for that particular topic and we found these hot and cold regions comparing
with Google Trends interest over time graph.
So this is how it looked like for all the topics, so for example if you pick the flat
earth theories Montana was a hot region or a high interest showing region while New Jersey
is a low interest or cold region.
So once we had all these parameters, the demographic geolocation and all these parameters we essentially
created bot accounts or sock puppets and programmed these accounts to keep firing queries on YouTube.
For geolocation these bots fired queries from IP addresses of these locations.
Now one very important thing that we had to do for running these audits is throughout
these audit experiments you would have to control for noise to ensure that the effect
that you're observing is actually from the algorithm is not because of the noise that
might have been introduced while running the experiment.
So we controlled for browser noise by selecting one single version of Firefox browser, we
made sure that the YouTube searches are happening simultaneously to control for temporal effects
and so on.
So all of that audit run resulted in about 56,000, more than 56,000 videos capturing about
3,000 unique videos.
And then was the hard part right, the manual annotations for this data set and I can go
into detail of why we went with manual annotations and how we annotated if anyone is interested
in the Q&A round but essentially this resulted in kind of three sets of annotations promoting
neutral and debunking.
And there was a lot of thought process that went into these annotation scheme, why these
three class made sense for this purpose and so on.
And then we performed statistical comparison test to essentially find out what's the result
of these audits.
So let's look at some of these.
So what did we find?
We found that for brand new accounts, demography and geolocation do not really have an effect
on the amount of misinformation or the type of conspiracy theories that these platforms
are or YouTube is returning.
This is encouraging.
This is what we want the platform to do, right?
So it tells us that unlike those reports which were blaming YouTube for returning conspiracy
theories when it's a brand new account, turns out the demography and geolocation do not
really have an effect.
But once accounts builds a history by watching both demography and geolocation starts exerting
an effect on the recommendation for certain combination of topics, stances and component.
You might be thinking, okay, this is expected, right?
This is how we think the platform would behave.
But turns out there are a little bit more nuanced results when we dig deeper into these
the actual audit outcomes.
So for example, for the 9-11 topic, if the sock puppets watched YouTube videos promoting
line of conspiracy, you would end up getting more of these promoting videos in the recommendations.
But if the topic is something different, so surprisingly for vaccine topic, the effect
was completely opposite.
If you watch anti-vaccine videos, YouTube ended up recommending you debunking videos
in the up next and top five recommendations.
So at least from these observations, it tells us that YouTube in some way is handling misinformation
in a much more reactive way.
It's modifying its search and recommendation algorithm selectively based on what reactions
it's getting from the media and technology critics.
So we know that there was a lot of pushback for vaccine-related misinformation and it
appears that they have gone and fixed that, but they have not done that universally for
other problematic topics.
We also found that certain demographics were prone to conspiracy video recommendations.
So for example, among eight of those demographic cases, in all but one case, men accounts,
it is bought accounts who had gender set as male, were recommended more misinformation
videos.
And perhaps more surprisingly, what we found that in four of these cases, men accounts
who actually ended up watching neutral videos got significantly higher misinformation video
recommendations.
Now this is really problematic.
It implies that the algorithm was actually recommending pro-conspiracy videos even when
the user, or in this case, Sockpuppet, was watching neutral videos on the topic.
What could this mean for real users?
This means that recommending promoting videos to men who are already drawn to neutral information
for that topic, but have not yet developed pro-conspiracy beliefs, but now has a higher
chance of developing that because the platform is returning these promoting conspiratorial
videos.
So wrapping up this work, the key contribution of this study was that in some senses this
work developed a methodology to audit search engines for misinformation, and we were also
able to statistically prove that YouTube's behavior varies across different misinformation
topics.
And our study also identified certain populations that could be potentially targets of certain
types of misinformation.
So this tells us that audit itself could be a useful way for studying how algorithms might
have differential impacts on certain marginalized populations.
Yes?
Good question.
So I get what you're saying about it being reactive.
The evidence suggests that in this case it's like being a special case.
Do you have a proposal as to how it might not be reactive?
How could they be proactive?
Is there something you would propose that they do instead?
Yes.
So I think one of the things they could do, and I think they are doing it now in hindsight,
this is an older study, is they're sitting with teams of experts, health experts, and
also looking at these health-related queries and topics in advance to figure out doing these
red-teaming exercises, and they call it red-teaming, I think in the research where we call it audit.
So they are doing this beforehand to figure out whether the platform is returning problematic
content.
And so if they do more of that proactively, of course there is some hope in changing things,
and we should not be catching these reactively after the fact.
That assumes that the points of view are stable, right?
So we often have these scenarios where culture changes or something goes viral, like the
tide pods or whatever, where I don't think a red team would have come up with, oh yeah,
we're going to start eating bleach or whatever.
Is there an approach there that you would advocate, like if you were in charge of one
of these teams?
Yeah, I think one of the things that I think some researchers at Stanford, maybe it was
one of your students who did this work with crowd audits, where the crowd itself is reporting,
because it's not really possible for the red team to find all possible scenarios under
which these sorts of problems happen.
And I think that's where if you have these multiple eyes from different domains and different
cultures to report those problems, and then the company actually responds to it.
The problem is if the company is not responding or the people who are building these algorithms,
if they are not responding to it.
And so hopefully there should be a mechanism to do that, kind of closing that loop all the
way from reporting to actually taking action.
So moving on to Amazon, I think one of the things that led us to looking at Amazon is
despite being this leading e-railer platform, how less of a focused research focus has been
paid to this platform.
And I think what was alarming is there were several media reports at the time coming out
suggesting that Amazon's algorithm were putting health and vaccine misinformation at the top
of your reading list.
But there was very little research to fall back to either verify or even, you know, kind
of disprove these reports.
So if you search on Amazon, unlike YouTube, which is at least trying to control for vaccine
misinformation, searching on Amazon for a vaccine, even this morning when I searched,
I could actually find some of these books, you would end up getting several anti-vaccination
products mostly in the form of books.
And the recommendation algorithm for Amazon are even much more sophisticated than YouTube.
So you have your product page recommendation, which has all these many different layers,
customers who bought items, sponsored products related to these items.
You have your homepage recommendations, again, many different layers underneath, pre-purchase
page recommendations, which is shown to you after you add a product to the cart that is
after the user shows an intention to buy that product.
So again, this was the same question as before, how bad is this scenario?
So we essentially wanted to conduct the systematic audits on Amazon search and recommendation
algorithm.
And here we picked only one type of problematic content, vaccine misinformation.
And we conducted two sets of audits, unpersonalized one, and then the personalized audits.
The personalized audit's goal was to assess whether users account history built progressively
by a user performing certain actions, such as clicking on a product, adding the product
to the cart, showing their intention to buy.
Because any of those actions, how does that change what recommendation is being returned?
And here the user built these account history progressively by performing a particular action
for seven consecutive days.
So these were, again, when I say users, these were sock puppets, searching, searching plus
clicking, searching plus clicking plus adding the product to the cart.
So all these were different actions that were performed.
And then we also controlled for noise, very similar setup as before, just with the caveat
that this whole audit experiment setup was a big software engineering feat, considering
the amount of different combinations of recommendations possible on Amazon.
So what did we find?
So I'm going to highlight a couple of results here.
First a single case study result.
So let's say users start searching for a vaccine and they click on an anti-vaccine book.
So as of this morning, this book was actually there on their platform in the first page,
first search result page.
And so if the user clicks on this, the algorithm next serves the user three other anti-vaccine
books in the product recommendation page.
And then once the user adds a product or book to the cart that it shows their intention
to buy, both the pre-purchase as well as the homepage recommendation also rapidly changes
with many more anti-vaccine book recommendations.
So this just tells you that once a user starts engaging with one misinformation product on
this e-commerce platform, they will be presented with more of those similar stuff at every
point of their Amazon navigation route.
So this was not just a one-off case study.
We found that more than 10% of Amazon products during the time period of our study for search
terms like vaccine, autism, immunization resulted in misinformation book containing
active vaccination content.
And so our audit experiment, just to give you the scale of this experiment, this was
ran for a little bit over three weeks, resulted in 36,000 search results, 16,000 recommendations,
and then worked over several search filters like featured, sponsored, different recommendation
types, user actions, and so all of that resulted in that number of more than 10%.
So if you just zoom out and look at what these thousands of recommendations look like, this
is the entire recommendation graph for one type of recommendation, what are the items
customers buy after viewing this item.
So here, each node in the graph represents a product, an Amazon product, and an edge
from a node A to a node B indicates that B was recommended in the product page of A.
Node size here is proportional to the number of times the product was recommended, and
the color corresponds to whether if it's a red, if it denotes a product annotated as
misinformation, green, neutral, and then blue debunking.
And I think being, you know, all of you have CS degrees or almost about to get a CS degree,
so you would probably able to decipher what's going on in this graph.
There are these large red size nodes attached to other red nodes, which are almost completely
separated.
There are like two separate components, right?
So this just shows how strong of a filter bubble effect there is for this particular
recommendation.
People who are recommended misinformation products, they keep getting recommended those
products super hard for them to break out from that red zone to get to the blue or the
green zone.
Sorry, just to play with the question, I think you mentioned this, but how did you code something
as misinformation?
They were not, like obviously there's lots of shades of gray here.
Yes.
So we, the initial status, we went through an extensive annotation scheme of a set of,
and then we also built a classifier to do that.
But the entire, there are like a lot more details in the paper as to how we coded it,
but I think it took us almost a month to even come up with the whole annotation scheme,
and then five or six of our experts, including me, we kind of coded it.
But yes, so we looked at a few markers, like the name of the book, the text, red, the Google
preview of the text of the book, some of the comments, and then also the reviews that are
present on the Amazon website.
So it's a lot more qualitative process, and then all the markers that we took, we also
put it into the classifier to get the final annotations.
So this would have been more like the overall position of the book, not like, oh, was there
a fact somewhere in there that-
Yeah, so yes, we cannot really go and look at, OK, there is this one single line in the
text in the book, which is misinformation, no, but yes, so there, you could say there's
a little bit of noise in there.
And so this was another one just to say that, you know, this was not happening for one recommendation.
Those who viewed this item also viewed very similar graph as before, very similar trend.
And so the key takeaway here was our goal was to bring the focus to e-commerce platforms
and show how their algorithms could be pushing anti-vaccine content to users.
And we empirically established how certain real-world user actions on the platform could
drive users to these problematic eco chambers.
I think one of the implications, at least from this work, is that recommendation algorithms
should not be blindly applied to all topics equally.
If it's a health topic, perhaps companies need to pay a little bit more attention and
to ensure that there is higher quality content coming out in their platform.
So this work of ours intentionally and unintentionally was kind of rightly timed during the COVID pandemic.
And so this was widely covered by several news channels.
And in fact, Congressman Adam Schiff and Elizabeth Warren actually cited this research
of ours in their letter to Amazon to control vaccine misinformation.
So we were really happy that, OK, so now Amazon is going to take a few steps to do this.
But turns out we were really wrong.
So this is how Amazon is doing today.
Still today, as of earlier this morning, you would still find several books containing
vaccine misinformation.
This also just tells, like, how, you know, even though you go into all these lengths
doing these academic research, is it actually informing policies?
Is it actually making any real-world impact?
And we can go into a long discussion about that.
But in the interest of time, let me talk a little bit more on one other type of study
that we did, looking at another type of audit method.
So so far, the studies that I presented employed one type of audit method, sock puppet audits.
While these audits provide great control over your experimental design, you can pinpoint
exactly which variable might be affecting the output of the algorithm.
But one criticism of these audits is that the bots' behavior are usually built in a
very conservative way, right?
So the bot in this YouTube case was essentially going and watching all pro-conspiracy videos
or all debunking videos.
Real users do not really act exactly that way, right?
So these are at least very extreme bot behaviors or user behaviors.
So as an alternative, we conducted crowdsourced audits where we audited the algorithmic outputs
from real-world users to study and identify problematic behavior in users' naturalistic
setting.
So we conducted this audit for a nine-day duration on YouTube.
And our goal was to assess to the extent in which YouTube was regulating US-based election
misinformation on their platform.
And so soon after the presidential election in 2020, YouTube came under fire for surfacing
election-related misinformation in their search and recommendations.
They quickly responded to those criticisms by introducing these content moderation policies
to remove videos that spread election-related falsehoods and claim that misinformation videos
would not be surfaced on their platform.
But then again, during the midterm 2020 elections, there were reports saying that YouTube still
has misinformation blind spots, right?
So they have not been very effective.
So this study of ours was goal was to determine how effective YouTube was in successfully
implementing its content moderation policy.
And so we did this through this post-hoc crowdsourced audit.
Why it's post-hoc?
Because it's conducted after the fact the event has happened, elections of 2020, and
we were conducting this study in 2022.
And it's crowdsourced audit since we investigated YouTube's algorithm collecting data from real-world
users.
And I'm sure many of you who have run these sorts of recruitment studies, you would realize
how difficult and how hard it is to do that.
Essentially, we were asking users to lend their YouTube history so that we can do this
sort of audit run.
And so our crowdsourced investigation, I think we started with recruiting 600 to 500 users,
and we ended up slightly lower than 100 users, so 99s particularly.
So all these 99 users first filled out a pre-survey and about their beliefs on personalization
on YouTube, how they trust YouTube search and recommendation algorithm, and then they
installed this browser extension, which allowed us to collect users' personalized data.
We also had all these ethical considerations, which I can go on into more detail if anyone
is interested, but what this extension was doing, it was collecting search results of
search queries related to the 2020 US presidential election, as well as voter fraud claims surrounding
the 2020 elections.
So two kinds of collection was happening.
One was with respect to search results in the standard and incognito window, and by comparing
these search results in both these windows, our goal was to tell the extent in which YouTube
was personalizing search results.
And then we were also collecting recommendation results.
And the way we were doing this, we were collecting these upnext recommendation trails after a
user has watched a list of pre-selected videos with different stances on election misinformation.
And the extension would start by first watching a pre-selected seed video and then collecting
upnext videos up to five different levels.
So when we asked our participants in this pre-study survey, how much do you think YouTube
personalizes your search results?
About 34% of them believe that YouTube personalizes their search results to a really great extent.
But this, through our audit, we found that YouTube, actually, that their YouTube stop
search results have little to no personalization.
So this also tells you how users believe in algorithms, like the way they behave is different
from actually the way the platform might be behaving.
But when we asked how much YouTube personalizes their upnext recommendation, that perception
actually aligned with how actually the audit results showed, like 51% of participants believe
that YouTube personalizes upnext recommendation to a great extent, which is in line with what
our audit results found.
We also calculated the amount of misinformation present in search results.
And we quantified this with this misinformation bias score.
And this is the only equation you're going to see throughout this talk.
So this misinformation bias score varies from minus one to one.
What the score does is it captures the amount of misinformation, election-related misinformation
while taking into account the ranking of the search results.
So a positive score indicates that search results contain videos that support election
misinformation, while negative, it contain videos that oppose election misinformation.
Now, if you look at the entire distribution of scores for our collective results, we found
that the misinformation score, if you look at the x-axis, it's mostly negative, which
indicates that YouTube presents more debunking or opposing videos in the search results.
A couple of other key things also jumps off, right?
So you could see there are, this distribution is by model.
So essentially, there are two different clusters, and each of these clusters corresponds to
two types of search queries, I mean, we didn't cluster it ahead of time, right?
This emerged from our data.
So the first cluster corresponds to voter fraud, basically anything related to fraud
in conjunction with keywords related to election, while cluster two is more generic election-related
searches, presidential election mail-in ballots and so on.
What's interesting here is that the cluster one has these mission information bias score,
which are more negative, which indicates that if the user goes and search for fraud-related
topics, they are actually going to be given more opposing election-related misinformation
video, right?
So it's making, YouTube is making it really difficult for users to search for election
fraud video, which in some sense tells that YouTube's pay more attention to queries about
election fraud and ensures that when users are searching for them, they are in fact being
exposed to opposing misinformation videos.
So key takeaway here is, in some way, YouTube is in fact successful in enacting election
misinformation policies.
So things that we wanted to test turns out it's actually aligning with how they wanted
to enforce these policies, but it is indeed paying special attention to certain queries
about voter fraud.
But there still exists certain misinformation in the up next trails.
We found that with some of those positive scores that you found.
And then finally, as a byproduct of this audit, we also found that there was some mismatch
in participants' beliefs and the algorithmic reality that happens, right?
Which indicates some lack of awareness of algorithmic, how algorithms behave.
And I think there has been other researchers who have been working in this space looking
at algorithmic folk theories and how people's perception differ from the way these platforms
work.
Now wrapping up, so these three are the core studies that I wanted to present, but obviously
coming back to how I started the talk, where do we go from here, right?
How do we do meaningful algorithmic governance in the first place?
That is, how do we set the path towards algorithmic governance in a meaningful way?
And what are the challenges in doing that?
So here are a few ideas, and obviously, there might be more that could be added, but these
are some of the possibilities for doing algorithmic governance.
So I've listed three of these.
The first is algorithmic audits, and so governance via audits, and there could be many layers
to this, right?
So one of the layer is conducting external audits, and I presented some of these external
audit studies through the three research work that we have done in the past.
And also these audits could identify different types of risk.
So misinformation is one risk, but you could also do the same for bias, discrimination,
vulnerability, accessibility, fairness, and so on.
And there are many researchers who have worked in this space.
I've listed some of these citations here.
But obviously, our question is, so we as academic community, third party researchers, we are
doing all these audits, is it really making any difference, right?
And classic example is the failure of our Amazon study to make much of a difference,
right?
So we still really don't have a system in place where the algorithms or companies running
them are truly accountable to an independent third party.
So this reminds me how US-based consumer reports operate, right?
So they are these independent third party organizations that go into great lengths for
testing products that you use every day, your cars, your washing machine, and so on.
But so my argument is that why can't we do the same for algorithm?
In fact, I would argue that we need that more for algorithms because we are using them much
more frequently than, say, your washing machine.
The other shortcoming with external audit is that they are a form of reactive governance.
This was the question that Michael was asking even earlier.
They operate after the algorithm have been deployed.
So after the harm has been done, plus the external auditors do not really have access
to the models, to the training data, which are obviously protected as trade secrets.
So as an alternative, another layer to governance via audits is you could do internal audits
as proactive governance.
And so at the time, researchers from Google, who are no longer at Google right now, but
they released this paper, Making a Case for Internal Audits, where audit would be part
of a core part of product development at every step of the way.
You could also do the best of both worlds, right?
You could do something called cooperative audits, which is a fairly newer concept, where, while
external audit answers what problems the platform has, and internal audit says why that's happening,
you could have a combination of both, and you could do cooperative audits as shared governance,
which allows external algorithm auditors to audit the system of willing private companies.
So I've done a little bit of this with Spotify, where working with their engineers within
the company, figuring out how gender representations might be biased or not for their taste on boarding
and listen action on podcasts.
And then finally, you also need to do these audits multiple times, right?
Longitudinally.
That is, we need to conduct these continuous audits monitoring platforms multiple times
instead of that single snapshot audit.
Many of my studies that I presented today are all single snapshot, and we do need that
kind of longitudinal effort.
So here is where I want to highlight one of the quotes from the Brajis earlier internal
audit paper, where they mentioned the audit process is necessarily boring.
It is slow, it is meticulous and methodological, methodical, which stands in stark contrast
to what I had started my earlier slides with, move fast and break things, done is better
than perfect.
So very much in contrast with the rushed culture of technology development.
And this is where I want to take a little bit of tangent and mention about audit possibilities.
One of the fastest growing developing AI technologies is the large language models,
and what would auditing even look like for large language models, what's the blueprint
for LLM auditing?
And then also what are the key challenges, right?
So one of the key challenges that is difficult to assess the risks that AI systems and large
language models in particular pose, independent of the context in which they are deployed.
So we do need application specific audits for large language models.
The second challenge is that, and I don't know how to solve this, or rather even the
first one, is that the capabilities and the training processes of these foundation and
models have really outpaced the development of the tools and techniques and the procedures
for auditing, right?
So it's really hard to keep up the pace.
And so doing ethical, legal and technically robust audits makes it super challenging for
such a rapidly developing technology.
And so it must be complemented probably with much more newer forms of supervision and control.
So here is one possible framework, one possible blueprint for auditing large language models,
which is kind of three layers.
So the first one is a model audit.
So as the name suggests, it focuses on assessing the technical properties of the pre-attained
language models.
So this is very similar flavor to the internal audit that I mentioned earlier.
So that's sort of proactive governance before you deploy the model.
But then there is the application audit, which focuses on the assessing the applications
built on top of the LLMs, so which is these flavors of post hoc audit, right?
And it should be done longitudinally multiple times over a long period of time so as to
capture any sort of new properties that might be emerging.
And then finally, I think this is the new form of audit that we haven't talked about
a lot, at least the research community.
These are these governance audits that is assessing the processes whereby these language
models are designed and where they are disseminated, so very much process-oriented, right?
So my next proposition is about value-centered audits, that is, there is this active conversation
around social values, emphasizing, while designing algorithms, and I think we also need to turn
that attention and thinking into how we can value and respect humans involved in the audit
process.
So these humans could be in the form of users who use the system or even auditors who are
investigating the sites.
And so for auditors, if we bring back the conversation for a second back to misinformation, one instance
where auditors did not really perceive fair treatment was this scenario where fact-checkers
are one of the key auditors of misinformation on online platforms like Facebook, Twitter.
So the fact-checking organizations, SNOPs, a couple of years ago, actually backed out
of their partnership with Facebook because they didn't feel their values were being respected.
I think to delve into this question of fair treatment of auditors, we need more effort.
And so one way in which my group has started a few initiatives, we have launched a research
endeavor with the fact-checking organization based in Kenya called Pesachek.
And this has also expanded to 16 other fact-checking organizations across four different continents.
And we released our first report called the Human and Technological Infrastructures of
Fact-Checking.
And so one big motivation for this work was also this question of, are we really taking
into account diverse voices when we are talking about governance and governing technologies
and are we really doing culturally responsible AI?
Finally, how do we ensure actionable audits?
That is, audits that result in real change.
So one of the most successful examples of an actionable audit is Joy Boluwamy's gender
shade study.
So what she did was she audited facial recognition algorithms and within seven months of the release
of these original audit, all the three companies who had their facial recognition apps released
new API versions that reduced accuracy disparities with gender, male and female, as well as race,
darker and lighter skinned subgroups.
So in other words, gender shade study is a classic example of commercial actual impact.
And so they laid out their approach in this actionable auditing paper of theirs.
Highly recommend you all to go and refer to it.
But it turns out actionable auditing is often tremendously difficult to achieve.
And here is where I want to revisit that earlier slide for our Amazon study to highlight how
much we had failed in doing the actionable auditing.
So despite widespread media coverage, despite a letter from Congressman Adam Schiff, Amazon
did not really act much.
All they did was add that banner of COVID-19 information directing to CDC's webpage.
So hopefully this kind of summarizes the challenges as well as opportunities in setting
the path for algorithmic governance and hoping with the new regulations coming in place.
Maybe if I were to give this talk next year, I would have a little bit more hopeful slide
than how I'm ending this talk.
So that's it.
So this is all I talked about today.
Most of this work was done with my PhDs.
One PhD student, Prena Juneja, who is now a faculty at Seattle University.
And then my group, I would also stick in three other threads of work, which I obviously don't
have time to talk about.
But these are like a couple of other amazing students.
So Shruti Farkeshi did a bunch of work on computational social science.
I was earlier meeting a student who was doing this sort of work, so things like big data
analysis of online interactions, studying trajectories of participation of users in
extreme communities, conspiratorial communities.
We have also done a little bit of design intervention and social system design work with another
student who is also a faculty now.
This is more of an HCI flavor where essentially questions like, how do you design a system
to nudge users towards meaningful credibility assessment?
How do you design a system to allow users to break out of their filter bubble, something
called other tube that we built on YouTube?
And then finally, the last and the least fleshed out thread is some of the work that is currently
ongoing with two of my students.
We are looking at challenges and opportunities of generative AI in fact-checking work.
And then what are some cultural misalignment that might happen with language models, especially
with roots in the global south, we are looking at cultural implications of these language
models in countries like India and other countries in Southeast Asia.
So with that, I would like to end and happy to take questions.
Thank you all.
All right, we've got time for some questions.
I was wondering in your auditing of YouTube algorithms that you guys looked at, what
I guess it was in 2020.
So I wasn't sure if YouTube shorts had been implemented since then, because YouTube shorts
are somewhat of a newer aspect.
But I wonder if the algorithms that underlie the, I guess, traditional YouTube recommendation
system underlie the same sort of like YouTube shorts recommendation because I guess the
length of content and sort of the amount of stimulus that would be needed to get the person
to keep on scrolling would be different and therefore possibly seeing that if there are
sort of similar pattern between the two, whether or not the density, I guess, of sort of misinformation
sort of increases because the fact that content is more short form.
Yeah.
So for the first study, we didn't, at that time, shorts were not there, but then for
the third one that I presented with election misinformation, we did capture YouTube shorts
and that tells me that we should probably do another analysis comparing the length of
the videos and the, you know, whether it's a short video versus a long form.
We did not do that, but that's an excellent point.
Yeah.
I have a question on maybe to say, was it the first one, did you look at all at like
the probability that you would get recommended legitimately false conspiracy videos, like
on the moon landing from videos about conspiracies that are a little bit more true, like missing
persons cases that the police just don't investigate and like the likelihood that you'll get recommended
that actually false content?
Yeah, we did not because I think one of the shortcomings of running audits is the whole
setup itself.
So we have to start somewhere, right?
So the RR starting point were a set of seed queries, right?
So with the way you are framing it, you know, we could, the hypothesis could be dozen missing
persons case lead you to more conspiratory videos and in that scenario, I think we can
use our audit framework to have those seed queries and see what happens.
On your kind of concluding point about actionable audits, I'm just curious, do you think it's
something to do with like the conducting of the audit itself or just the context and how
it aligns with like the company's incentives because it feels like the gender shades case,
it was like a very easily framed as like poor performance.
And so they were trying to cover themselves, whereas Amazon is somewhat incentivized to
keep people buying things, even if those things are harmful.
So like, I guess I'm wondering, like, do you think audits need to be conducted differently
or there just needs to be more external pressure, like from the government or the public to
incentivize the companies when they are like internally.
Yeah, I don't think it's a ladder.
I think, I think Joy went on to great extent to after the study was published to kind of
give talks and publicize and do that kind of outreach, which I did not do with this work.
I think that matters a lot, right?
She's the one who went to a Congressional testimony and testified against these companies.
And when you do that kind of impact, it would definitely translate or there's a higher chance
to be for your work to be translated to actual actionable outcome.
I don't think those are actually steps listed in the actionable auditing paper.
And in some sense, I feel like maybe the academic community need to think about how to incentivize
those additional work.
We don't have those incentives in place.
And partly, I think we should look inward and blame ourselves that we don't have those
incentives in place.
I just wanted to hear your thoughts more.
When being either in a proactive or reactive, do you think we should draw a distinction
between conspiracy theories or misinformation that has potential for great harm versus those
that maybe don't, right?
To justify interventions that override individual autonomy or control the information space,
who is the moon landing conspiracy theory?
Can you say that last part?
The moon landing conspiracy presumably isn't hurting anyone, right?
Should we take it down?
Yeah.
That's a really good point, I think.
For companies like Google, I know they have this, your money or your life.
They have a view or an acronym, YMYL or something, a set of search guidelines.
If those search results or the pages that show up, if it's affecting monetarily, financially,
health or your life, then they're going to be more proactive and act on it.
So you're right.
Moon landing is probably not, to that extent, versus if it's vaccine information that has
direct life consequences, right?
But obviously, then there are all these other questions that when it's very well known that
if you are drawn to one conspiracy, you're likely to get other conspiracy theories, right?
So then what happens?
Should those be at least to the extent that they maybe should be prioritized in the recommendations
if not completely removed from the platform?
This is sort of another sort of idea that I have.
Looking at the idea of, what I think is interesting about social media apps like YouTube is the
whole aspect of being able to communicate with others via the comment section of a YouTube
channel.
I was wondering if there is a way to possibly, I'm not sure how pleasant this is or if this
is even a thing that is even something that it's able to be looked into, but is there
a possibility that the algorithm isn't, may almost be promoting you content not necessarily
by what it's physically providing or showing off the recommended, but showing other users
who would most likely put other links to more extreme videos in the comments being like,
oh, if you thought this was interesting, look at this.
So that's not explicitly YouTube's algorithm showing you a video, it's showing that same
video to someone else who has the ability to share a link to another YouTube video.
That would be almost like pushing someone down like a pipeline of conspiracy theories.
So more like how the social recommendations, like you're sort of adding this collaborative
social recommendation component to YouTube and seeing how that pushes.
You could have the same effect, like in theory, maybe the YouTube recommendation system could
not explicitly push someone by recommending like more intense conspiracy theories, but
if YouTube is recommending someone who's already like a very like entrenched conspiracy theorist
and maybe someone who's on the edge, if YouTube recommends them both the same like, like starting
out conspiracy theory video, then you can have the person who's like very entrenched in conspiracy
theories commenting and suggesting things themselves that and it's not the YouTube is explicitly
recommending the original person or the person who isn't like entrenched in conspiracy theory
if that they put them essentially on the same, or they put them in the same environment in
which they could communicate.
Yeah.
So actually, my group, some of Shruti's work, we have done this in the context of Reddit
where what you are describing those very entrenched conspiracy users, we term this as veteran
users and so they are one of the big drivers of bringing other people like what we call
joiners into the community of conspiracy group.
So I hope YouTube never does that, what you're suggesting, but that's a classic marker of
how the social dynamics can actually bring people into these conspiratorial world views
and you know, empirically, we have seen that and there is also social science theory proving
that that definitely happens.
Yeah.
One challenge that I feel like our field faces with audits is, I guess what I would
describe as the sense I get of frustration from folks at these companies who feel like
the audits aren't well executed or are way out of date as soon as they are published.
You made this point that we have to be very methodical and often slow in doing this.
Then you throw in the peer review pipeline that can slow things down further and by the
time the thing comes out, I remember seeing like an applied researcher, someone, a researcher
in a company who in principle would be more open to this kind of stuff, being like our
algorithm doesn't even work like that anymore and so I'm wondering, like so obviously you
listed a bunch of possible cooperative audits, longitudinal, internal and so on.
I'm curious, are there, is there anything we can do to address that?
Like let's assume that we can't change the incentives of the companies, we can't change
how quickly we do the audits, is that are we always going to be vulnerable to this like,
oh yeah, that was yesterday's algorithm kind of critique?
Yeah, yeah, that's a really good point.
In fact, I was at a workshop with other folks like Chris Tobels and a few others who are
in this have done audits for a very long time with people from Facebook and YouTube and
I think we came up with this exact same question and I think the common thing that emerged
was that I don't think academics should be the, or academic institutions should be the
places to do these sorts of long-term audits.
It's fine to kind of develop the methods and kind of say, okay, this one should pay attention
for example to Amazon or this is the method to do it, but then you need like separate
third-party companies to continuously do these audits, right?
It's sort of like consumer reports, what's the equivalent of that for audits?
I think at CSCW, the closing keynote, Rumrum was mentioning that kind of red teaming, I
think their company or whichever NGO she's working with, they are doing something like
that.
I think so academics with that peer review process, I don't think we should be responsible
for doing those sorts of continuous audits because we are always going to pay catch-up
with companies.
Okay.
Yeah.
Oh.
One last?
Yeah.
Do you have thoughts on like Twitter has implemented like community notes where people can just
like anyone can share under a post like that's not true or this is misleading or this person
ever said that, like just like user-based immediate type auditing if you will, I do have
thoughts on that versus like companies long-term auditing or like if you think that's a good
idea.
Yeah, that's a really good point.
We haven't looked at community notes, but I know some researchers have kind of looked
and researched it.
I don't have really any very smart thoughts as to other than the usual advice that it's
a good thing that one should do it, but with the caveat that if the community doesn't reflect
the right view or I wouldn't use the word right, but you know, a credible view of what
happens then, that's problematic.
I think that's time.
So let's thank your speaker.
Thank you.
Thank you everyone.

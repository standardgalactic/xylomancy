This is definitely one of the shortest travel trips that I've ever had to give a talk because
my office is just across the street, and this is a familiar space because we ran the human
computer interaction seminar in here last year.
So it's great to be here today.
And as Mark said, I'm one of the faculty members in the mechanical engineering department
where I run the Shape Lab, but I'm also very much involved with the Stanford Human-Computer
Interaction Group as well.
And so today I'm going to be talking about our work sort of at the intersection between
yeah, human-computer interaction and robotics and mechatronics systems and thinking about
how can we really enable people to interact in much richer ways.
I also have to apologize, I had a hand procedure done earlier today, and basically I thought
it was going to be not as painful as it was, and so I'm a little bit more under the weather
than normal, so you'll have to believe that this would be even more exciting normally.
So speaking of hands, I've long been impressed by the dexterous ability of our hands to allow
us to manipulate the world.
And certainly having recently had some injury for my wrist, that's become even more clear
to me how important our hands are for manipulating things in the world and really just the amazing
complexity of our hands and the way in which we're able to interact so gracefully and dexterously
to achieve really stillful manipulation such as here, stulting, and clay, but all of our
everyday tasks as well.
But to me what's really exciting about the hand and manipulation is the role of manipulation
in terms of our cognition as well.
And so physical interaction is very important for not just physical manipulation for the
sake of changing the world, but also for us to express our thoughts as you see me gesturing
like this, but also for us to better understand information.
So there's been a lot of studies that have looked at how children, when they learn with
Abakai, for example, or Abakis in plural, Abakai, that they actually learn math concepts
differently and better than if they learn them just on paper and pencil.
And so there's something behind this idea of embodied cognition that's actually quite
important.
And we see that not just in terms of education, but really in terms of very specialized domains
as well.
So for example, this is a photograph of urban planners and architects.
And what we see there is there's this basically rich interplay between the physical models
that they're building, space, and the ways in which people want to interact as well.
And so physical and spatial form can really help us better understand and solve problems
and very complex ones in the architecture domain, but many other domains as well.
And the big kind of reason behind this is that spatial manipulation, as I kind of highlighted
before, really aids in spatial cognition.
These two things are very tightly coupled together.
And one of the great professors that we have had at Stanford, Barbara Tversky, has done
a lot of work as well looking into the role that spatial manipulation plays in our cognition
and kind of the evolutionary basis for this as well.
So she has a great new book out called Mind and Motion that I highly recommend you take
a look at.
But in many ways, this is because cognition and perception are very much coupled together
through action.
And we've evolved over a long period of time to really benefit from the way that we can
manipulate the world to better understand it.
And these things are tightly coupled together.
This is even more important when we think about access to information.
And so for example, these physical representations can be really important for people with different
abilities.
So here's a picture of a blind student at the Perkins School for the Blind in the Boston
area looking or feeling rather a double helix model.
And this is something that would be very hard for us to explain through words, but by being
able to touch, feel, and manipulate it, they can much more easily understand.
And this is a tactile model of Berlin's Museum Island.
And again, this type of spatial information might be very hard for us to convey through
text alone, but by allowing people to directly access it through touch and through our haptic
sense, we can much more readily access it.
But if we look at the ways in which we traditionally interact with computing and with information,
not much has changed since 30 years ago in some ways for the ways in which we interact.
Historically, it's been the case that we really haven't been able to leverage our sense of
touch in that.
And if we look towards basically newer interfaces for interaction, such as in virtual reality,
a lot of times we really get these benefits of spatial interaction where here I can move
around and interact and collaborate with someone else, but when we reach out to feel something
or touch something, we're not really receiving any meaningful feedback.
And so one of the key questions that I've been interested in as well as many other people
in the field of haptic interfaces is what if we could reach out and touch the void?
And so during my PhD at the MIT Media Lab, we were really interested in trying to merge
between the physical and the digital world and bring that sense of physical touch into
the real world so that if we were two people working around a workstation, we could look
at each other, see each other, and also directly interact with a surface and be able to feel
it.
So let me show you what that might look like.
So here's a picture of a new type of haptic display.
We call this a shape display that we developed at the MIT Media Lab with my advisor, PhD
advisor Hiroshi Ishii and my colleague Daniel Leichtinger, where we could really reach out
and sort of sculpt with digital clay or we could reach out and new interface elements
could appear that have the correct affordances and ergonomics for how we might want to interact
or we could render different types of 3D models in sort of real time and be able to again
see them in 3D but also touch them.
So this type of display had been sort of considered before by a number of other researchers,
but here we were able to really look at what are these meaningful interaction techniques
and start to look at some example applications as well.
So this was a project that we worked on with Tony Tain who was a formerly a practicing
urban planner and as I said before, the spatial models that urban planners and architects
build are often very important for how they think and consider this space.
But basically they would make one set of full models and then not be able to change anything.
And so here we can in real time change the models but also have other simulations that
we can load on top of that and be able to edit, manipulate and collaboratively work
together.
So this sort of gives a hint for what this new type of interaction with what we call
shape changing displays might be like.
And so when I came to Stanford, I guess now seven-ish years ago or so, I started the Shape
Lab to really deeper investigate these types of systems but also work on some other areas
as well.
And so in our group, we've been working on new types of haptic displays like the ones
that I've shown here and that's really what I'm going to spend the most of today talking
about but I quickly wanted to mention two other areas that our group has also been working
on as well and that are kind of related to the robotics areas.
And so the next area that we've also been thinking about is this notion of ubiquitous
robotic interfaces and really this idea harkens back to someone who maybe some of you know
of but Mark Weiser who sort of started the field of ubiquitous computing that we really
live in today where there's you know in this room around us many different sensors and
actually or sorry many different sensors and displays there's probably you know at least
a hundred if not a thousand computers in this room right now.
And so the question that we had is sort of what would it be like in the future if we
could have actuation and robotics be embedded in our environment as easily as the computing
and displays that we have today.
And so with my PhD student who's now a professor at Simon Frazier University Lawrence Kim we
created this platform of small mobile robots that we could then explore.
And again here we can have them display information like this but we also looked at ways in which
they could be embedded in our environment like on our desk move around to display information
when we need but also manipulate things in the environment or go about you know remotely
sensing different things as well.
And we think this kind of opens up some interesting opportunities to think about again where does
this line between robot and environment end.
And so that's something that we're also quite interested in and kind of intersects a lot
with human robot interaction.
And the last area that we're doing work in is in shape changing robotics which is something
that Mark highlighted before and a lot of that has to do with the enabling technologies
that we are looking at to make shape changing displays and also shape changing robots.
And so if you think back to that first example haptic display that I showed that is sort
of like a 2.5D surface display.
What are the things that we have been thinking about is how might we sort of make full 3D
shape changing displays.
This is sort of the holy grail of some of our work in terms of thinking about oh how could
we actually you know feel a whole entire dolphin and actually have it be able to change shape
between different surfaces.
And so as one of my Ph.D. former Ph.D. students Zach Hammond and a collaborator of ours Nathan
Yusovic from Allison Okamora's group we're trying to think about how might we make this
vision a reality.
And so we got inspired by basically balloon animals to think about new ways that we might
approach this.
And the interesting thing about balloon animals is right is that you have start out with something
very simple like essentially inflated tube and then we can pinch it at certain points
not it together and basically create many different shapes out of the same you know simple balloon
idea.
And so we tried to bring this idea to the field of soft robotics where we could have basically
an inflated beam and as opposed to relying on pumps to move and actuate different areas.
What if we created a pinch point in that beam.
And then what if we could move that pinch point around.
And then basically what that allows us to do is create some type of system that can really
dramatically change shape by putting a robot roller node on that surface and driving it
around and then creating what we call sort of this idea of a isoparametric type of robot
where we have a fixed length of inflated tube.
And then we can change kind of the overall basically geometry even though it has a fixed
topology.
And so here is you know one element inside that but we can place these together through
other kinematic connections to create some type of tetrahedral robot that we can then
basically a truss robot that we can then really dramatically change shape.
And again we get these benefits of having this constant volume of air that we don't need
to really pump around but instead using motors to move these pinch points or buckle points
around.
And so here is an example of what those types of basically truss robots could look like.
And so this is you know a fairly large device about you know this this size and it's able
to pretty dramatically change shape and move around and this is all shown in real time.
And so it can you know locomote by basically doing some type of punctuated rolling change
into shape and move around but it can also it can also go ahead and manipulate objects
as well.
So it can use the geometry of of itself to basically grasp an object and then be able
to pick it up and actually do some interesting in hand manipulation as well.
And so we think that this kind of idea of having these large shape changing truss robots
has some interesting applications in terms of thinking about new ways that we could be
able to locomote manipulate and also use the shape change to afford different types
of interaction in the environment.
And so we've also done some work to look at the the modeling and kinematic control and
planning of this as well.
So Zach and our group did a lot of work on grasp optimization planning with these types
of robot and we have a collaboration with Matt Schwager looking at decentralized control
of these types of systems as well.
So I won't go into too many details in that area today but I just wanted to give you kind
of an overview of some of the different things that we're working on in our group.
But today I'm really going to focus on these types of shape changing or haptic displays
and I'm going to start out by talking a little bit about the way in which we approach these
problems and really the way in which we like to think about this in our group is really
focusing very deeply on specific applications and needs working with domain experts in those
areas.
And then trying to learn more about what are the enabling technologies or the requirements
for the enabling technologies to actually make these systems work.
And then that kind of feeds back into other applications and needs.
And we think this is kind of a nice paradigm for working on, you know, basically new technology
to make these things possible.
So I wanted to start by talking about two vignettes of specific applications that we've
been working on.
First is in car design and the second is in making, making accessible to sort of highlight
some of the challenges in making these systems actually useful.
And so many of you might be familiar with, you know, car design in general and have seen
these types of clay models that basically industrial designers and human factors experts
create to prototype and test car designs.
And so we had been reached or we have been working with Volkswagen who's trying to transition
from making these clay models, which end up costing, you know, something on the order
of $100,000 per clay model.
So it's very expensive and time consuming to make those and moving instead to using
virtual reality to prototype and test at stale, especially in terms of working with different
stakeholders that they care about, as well as the human factors aspects of like the interior
of the car as well.
And so what they're trying to do is basically transition from this, you know, very physical
style of doing things to now move into the virtual reality systems.
But what they found is that, for example, in the case of the interior of the car design,
what they call the seating buck and the human machine interaction, basically they'll be
able to load up different car designs in virtual reality.
And then they have these very fancy seating but to adjust the position and height and
steering wheel position to basically create any different kind of car.
But then you go and reach out and try to touch the HMI, the human machine interface part
of it and the dashboard.
And basically you, you know, again, reach out and touch nothing or touch air.
And so one thing that they end up doing is creating these, you know, basically machined
or milled foam models that they can then place inside this seating buck to then be able to
go ahead and test.
So it turns out it takes a long time for them to actually be able to create these foam models.
They can't switch something, change things on demand, et cetera.
And so we had this idea of what if we could create, you know, interactive seating buck,
basically simulator where we could create this, you know, basically surface that we
could change in real time to allow us to explore basically the different HMI interactions and
some of the ergonomic issues as well.
So we started out working with them on this concept and trying to create a new generation
of our tactile displays to be able to do this.
So this was led by Alexa Sue.
And so she created this, you know, basically, this is about the smallest that you can do
with kind of like low cost off the shelf DC actuators.
So this has a direct drive between them with the lead screw and, you know, many, many actuators
and we made this modular so you can stack them together.
And we also, you know, looked at the integration with this as well as in virtual reality.
And then we brought it back to our collaborators at Volkswagen, particularly like the industrial
designers and the, you know, human factor specialists.
And they sort of said, oh, hey, this is great.
We like the idea.
But you know, wouldn't it be great if these could be a lot higher resolution and much cheaper.
So you know, in terms of the types of things that they're looking for, they were not still
there with this technology.
And again, oh, wouldn't it be nice if, you know, we could create many of these, not just,
you know, basically this device here costs something like $6,000 in parts to create.
So wouldn't it be nice if we could, you know, basically have this be higher resolution and,
you know, basically much larger in scale and lower in cost.
So those were some of the feedback that we got, which sort of makes sense in some ways.
But then I wanted to highlight another application area that we've been thinking about that also
informs some of these other challenges.
And so this is in the area of still computer aided design and this idea of making, making
accessible.
And so many of you are probably very familiar with the, the making movement or maker movement
that kind of has gone on over the past 15 or 20 years.
And basically, you know, one of the great things about that is that it's really empowered
a lot of people and, and served as a great way to involve more people in STEM education.
But that doesn't mean that all people are unable to do that.
And particularly a lot of the tools that we use for making, especially in terms of computer aided
design are not accessible to people that are blind or visually impaired.
And so those people have historically been excluded from those areas.
And so while 3D printing can be very helpful in terms of supporting accessible education
through the use of creating tactile graphics or other types of materials that blind people
can touch and feel, there's really this lack of authoring tools for blind people for them
to be able to be the designers and engineers themselves.
And so the big problem with these, essentially with these systems that exist for computer
aided design is that they're all based on basically graphical user interfaces where
you have to directly manipulate with a mouse and a keyboard, as well as the computer screen
to be able to select and control many features.
And that's really great for people that are sighted, right?
We've moved beyond command line interfaces to this direct, direct manipulation type of
interface.
But for someone that's blind and visually impaired, basically the main way that they
interact with computers is through different types of screen readers, which basically
provide audio feedback.
And so how might we think about ways in which they could still be able to use this?
So there is some work on using text-based editors for creating geometries.
So for example, OpenSCAD is a computed solid geometry modeler where you write basically
in a declarative programming language to define the geometry.
And there's been some great work at the Dimensions Project at the New York Public Library, Chancy
Fleet there, in terms of basically using that for people that are blind or visually impaired
to write code to then be able to 3D print something.
But if you can imagine and you've used a 3D printer, you know they're not particularly
fast.
And so basically you write some code and then while we, you know, people that are sighted
would be able to, you know, instantly see the changes that we're making, someone that's
blind and visually impaired would have to basically use a 3D printer, wait somewhere between
an hour to, you know, 10 hours to find out, oh, the change that I made was that exactly
what I wanted.
And so one of the questions we've been asking is how might dynamic tactile feedback, you
know, support this type of interaction?
And so again, my former PhD student, Alexa Su, as well as some members from the blind
and visually impaired community in the Bay Area, son Kim, who's an Access Technology
Specialist at the Vista Center for the Blind, as well as Josh Miele, who is an amazing blind
engineer and researcher who's now at Amazon working on accessibility there.
We all work together to create and co-design with other blind makers, a tool that allows
people to use these types of tactile shaped displays we've been talking about to in real
time have that feedback.
So here, basically, you're able to write code in open SCAD and then in real time be able
to touch and feel the geometry and sort of be able to understand what it is.
And so one of the questions that we had, and then 3D print the design, so one of the questions
we had is, you know, what are the types of interactions that we need to port from these
direct manipulation interfaces that are really essential for basically helping blind and
visually impaired people understand that.
So we did a lot of different co-design sessions to try to understand what these challenges
are.
And one of the things that's interesting, it turns out that section views are even more
important, you know, basically for blind and visually impaired people than for in our,
you know, sighted based CAD systems, because, you know, essentially, you know, there's no
notion of transparency in these types of displays.
You're basically just feeling the top surface of the convex hole or whatever.
So you can't have any way to display, at least with these displays, this notion of transparency,
which we rely on a lot.
So we have some, you know, basically promising interactions that we think are possible with
this and compared to like a single point haptic device, we might have less high resolution
in terms of the spatial component.
But by allowing people to touch it with their whole hands, they're actually much more easily
able to understand what it is and the shape.
So we think this is a promising direction.
But if we look at the types of geometries that the people we were working with are able
to create with this type of tactile display, we can see they're quite limited.
And so this was a big issue that we found.
But there is this real benefit from having this tight coupling of the iteration.
And so we think this is a promising direction and very promising in terms of this notion
of real time, iterative feedback, and also that people in the blind and visually impaired
community really do want to be designers and makers of their own, you know, basically
technology and this type of system can be very empowering for them.
But, you know, there's still these big issues in terms of resolution.
So if we think about a computer display that we might use being very high resolution versus
this pin display that I'm showing here, you know, there's a big gap in terms of the resolution
and potentially the underunderstandability of that geometry and then also in terms of
access. So each of those tactile displays that still low resolution costs upwards of
six thousand dollars.
That means that, you know, many people can't have access to it as opposed to, you know,
five hundred dollar laptop that we could use to teach cat.
So across these different application areas, we really saw these big challenges in terms
of cost, scale and resolution, as well as interaction techniques.
But I'm not going to talk as much about that today.
And so a lot of the work that we've done in the past couple of years has been trying to
address or mitigate some of these issues of cost, steel and resolution.
And we kind of take in two different approaches to try to tackle that on our group.
The first is in trying to create new and novel technical solutions, hardware solutions to
solve that. And the second area is on kind of what we call kind of perceptual illusions
or using basically perceptual engineering to think about how do we basically use the
existing hardware that we have, but maybe use some clever tricks in terms of how we
integrate information together to improve the perceived resolution.
And so I'm going to talk about both of these areas.
So the first area that I'll talk about is on creating higher resolution tactile displays.
And this has been kind of a big challenge in the field of haptics for a long time.
And there's sort of this big trade off or dichotomy between people that are trying to
make really high bandwidth tactile displays, first people that are trying to make kind of
these shape displays or tactile displays that maybe don't need to move as fast or maybe
don't need to move at all.
And how do we find this balance between the two?
And so one of my former PhD students, Kai Zane, started to think about ways in which we
could really try to push the resolution if we trade off on that bandwidth side of things.
And so if we think about this design space or list of design requirements for like the
ideal or ultimate tactile display, there's a lot of different things that we might
consider, one of which is like, what is the necessary resolution?
And so we can look to the haptics and psycho physics literature for some intuition around
that, you know, basically, if we think about, you know, just statically touching a shape,
something like that, we need to be in that one or 1.25 millimeter range for us to be
able to not really be able to feel individual pins.
So you could think about this idea of the retina display that you might be familiar
with from Apple's marketing, where, you know, if you look at an iPhone today, you can't
see where one pixel ends and the next begins.
Basically, this two point discrimination threshold is kind of the same concept.
And so we want to be in that one to 1.25 millimeter range.
But that's, I would say, a very generous, you know, basically ballpark estimate, because
actually, if you start to move your finger, then you can basically feel, you know, down
to, you know, tens of microns or below.
So you can feel, you know, a single hair very easily.
But that's relying much more on your, you know, basically cutaneous information from
vibration and essentially texture information.
But if we just think about gross shape, this 1.25 millimeters gets us pretty close.
So as I said, people have been trying to work on this problem for a long time.
And there's a lot of different approaches in our own work.
We've used, like, you know, basically mechanical linear actuators and also different types
of pneumatic actuators, which have, again, challenges in terms of scaling these down,
as well as, you know, basically the high cost of the actuators and being able to scale those
together. Other people have created, like, electromagnetic tactile displays.
But in general, those have some challenges as you scale them down, because the magnetic
electromagnetic fields start to bleed in with each other.
So there's some interesting work from Juan Zarate and Herbert Shea in terms of thinking
about electromagnetic shielding for this, but they're still kind of on the centimeter
steel type of size or other people using electroactive polymers.
And again, all of these are really thinking about this idea of, you know, how do we make
a really fast or high bandwidth tactile display?
And so our, the intuition that Kai had was, OK, let's not focus on the high bandwidth
aspect, but instead, what if we had something that's much more like e-ink, where, oh,
we're not changing it very frequently, we're going to kind of refresh the whole the whole
thing. How do we instead have maybe clutches or breaks that we could engage or lock when
we need them to, but then have one global actuator that might change everything?
And so by trading off on this high bandwidth or the temporal domain or frequency domain
and instead focusing on the spatial, what might we be able to do?
And so the technology that we sort of came to in terms of a good, good tradeoff between
this sort of high force density in terms of breaking or clutching and then also the scale
ability is electrostatic adhesion.
And so many of you are probably familiar with the electrostatic fact where, you know, basically
you have a balloon, you rub it on your hair, and it sticks to your head.
And so this has long been used in different industries, for example, in, like, wafer chucking
in the semiconductor industry and paper handling and other things.
And then over the past 15 or so years, it's really come into vogue in the robotics community
as well. So probably many of you are familiar with it, but and also very commonly used in
the MEMS steel devices as well.
And so basically we started to think about how do we make these millimeter scale, you
know, basically high force density clutches and what are the different techniques that
we might need to use to do that.
And we think that this is a promising technology for this type of, you know, refreshable display.
And so basically here we can see kind of an example of one of these displays where we have
essentially a dielectric thin film that we've then patterned with these interdigitated electrodes
that are on the order of, you know, basically a millimeter across.
And then we can basically turn on an electric field that then basically induces some charge
on these brass or different types of metal pins that and then locks them into place.
What does this actually look like in action?
OK, great. Yeah, yeah.
So basically we can raise up so it's a refreshable display.
So essentially we now we unlock all the pins, we raise them up and then as we move it down,
we lock them into place.
And so the electrostatic clutches can turn on and off very quickly, giving us very high
precision in terms of linear positioning and are relatively high force compared to their
size.
So again, the basic operating principle is that we have this, you know, interdigitated
electrode that's serving as this clutch that's on one side of a high dielectric constant
thin film.
And then on the other side, there's a pin and basically we induce opposite charge on
the pin and that basically creates this electrostatic force.
And so kind of very simple model of how this electrostatic force works is very similar
to the parallel plate capacitor equation, where essentially, you know, the things that
matter are essentially the dielectric constant of the thin film, the contact area between
the metal pin and the dielectric film and the electrodes on the other side, the voltage
as well as the film thickness.
And so historically, in kind of robotics and other applications, people have really pushed
on the voltage as the thing to kind of improve the actuation force.
Here we have some challenges in doing that because one, we're dealing with people and
you know, basically some of these very high, you know, 10 kilovolt types of range, a very
small amount of current can actually be not very good for people.
So that's one issue.
And then the other issue is we want to have, you know, not just one of these actuators,
we want to have tens of thousands of these actuators.
And so how do we have, you know, very small and low cost transistors that allow us to,
you know, steal this in terms of production?
And so we ended up trying to push more on the thinness and also the high dielectric constant
as things that we could push on as opposed to the voltage because of those two constraints.
So this is kind of, you know, basically what the actual device looks like when we fabricate
these and using the UV laser cutter that Mark's lab or actually get Allison Okamura's lab.
Well, I forget where it is now.
But basically between Mark and Allison, exactly, shared facility, which is great.
Basically that allows us to, you know, very easily fabricate these and test out different
patterns.
And so we use PVDF, which is a high dielectric constant thin film and a very small sheet
of it.
And then we have gold that's sputtered directly on it or some other aluminum, for example.
And then we laser a blade off the parts that we don't want.
So the actuation principle is very simple in terms of, you know, we have these clutches
and then we lower down the platform, lock when the pins get to the right place.
We turn on the clutch and then we create the whole pattern.
And then when we want to erase it or refresh it, we just turn them off and then move the
pin up and down.
And so we can get, again, very high spatial accuracy in terms of the, you know, basically
the linear positioning of the height because we can turn them, turn on and off the clutches
and about, you know, on the order of 10 milliseconds or so, which allows us to have pretty high
spatial resolution.
We've also done a lot of work on quasi-static loading as well.
And unfortunately, the, you know, basically other people as well.
So for example, Steve Collins lab has done a lot of work on these electrostatic types
of clutches and also found that, you know, basically the back of the envelope calculations
don't really end up matching very well with performance because of the effective contact
area.
And so we've done a lot of work also on data-driven modeling for this.
But basically we think these clutches, you know, on the order of, you know, 50 to 100
grams of force for the areas that we're looking at, which, you know, is not that much force.
But if we think about the contact force that's necessary as you're exploring something, that's
on the order of 51 grams of force, and that would likely be spread across multiple pins
as well.
So we think we're in the right ballpark for this.
And my current student, Ahad, who's there now, is trying to work on improving the performance
and thinking about other things.
So we've done some user testing as well to explore how well these tactile displays work.
And it seems like it's a promising direction and seems to be working very well.
So we think this, you know, basically these electrostatic pin displays are kind of a promising
approach to really pushing the resolution and low-cost aspects of these types of refreshable
tactile displays.
So that's something that we're quite excited about.
And we've been able to achieve sort of this 1.5 or 1.7 millimeter pitch as well.
One of the challenges with these types of displays, though, is that they end up creating
these very discrete types of shapes, right?
And so it's possible that, you know, we might want to have more continuous shapes that could
be approximated better with some other method.
And also maybe there's a way to do that where we trade off, you know, basically, and basically
are able to have, you know, basically more continuous shapes with fewer number of actuators.
And so Ahad, who recently had a paper that was accepted to ICRA, that's on thinking about
how might we make these basically more continuous shape displays, and what are techniques we
could use to sort of have sort of a monolithic manufacturing process where we can kind of
create these in one go.
And so Ahad has been working on thinking about kind of the combination of electrostatic and
electrodehesive actuation with auxetic materials to be able to create basically these shape-changing
continuous displays where we're able to vary the curvature of them by locking individual
cells at an auxetic grid or an auxetic network.
And so here's an example of that shown here.
And so basically the idea is that we have, you know, work in auxetic skins where we can
basically have these different patterns that can expand basically differently based upon,
you know, how much, you know, strain there is in the system.
And what we're doing is then locking some of these cells in this auxetic pattern.
And what that does is it means that there's going to be very different local strain concentrations
that end up, as you inflate this surface, being able to create different geometries.
And so basically there's been exciting work in the computer graphics field as well as
in other areas on basically being able to computationally design these auxetic patterns
so that you can then create some arbitrary geometry that's beyond sort of like what a
developable surface could be.
And our kind of contribution here is to think about, oh, as opposed to being able to, you
know, essentially pre-plan and create a custom auxetic pattern that could create some given
shape, could we essentially create a smart skin where we can change that local amount
that each cell can open and close in real time or at run time.
And so basically the way that we do that is by having each auxetic cell essentially be
electrostatic break or clutch that can, you know, basically either open or close depending
on how much voltage we're applying across them.
And so what this means is that we want to have essentially an auxetic pattern with a
very large surface area because as I said before, the electrostatic force is basically
proportional to the amount of surface area that we have.
And so essentially, you know, we've looked at different patterns, but this one here,
you know, again, has a very large surface area.
And so what we end up doing is taking two of these sheets and then rotating them off
phase so that basically essentially there's a lot of overlap between them and we can essentially
lock which parts will expand and which parts work.
So here you get an idea of what that single cell might look like.
And so we have two, yeah, so here you can see these sets expanding and contracting.
And so essentially we can, as we pull on them or inflate them, they're opening up and closing
and we can basically turn on the electrostatic adhesion to lock them and allow them to not
open up, which means that basically there's less displacement and less strain in the system.
So we've looked at different types of combinations of layers and different materials as well
and are looking at, you know, basically how do we then make this into this monolithic system?
So again, there's these two sheets that are on top of each other.
And then basically we create them out of this flexible printed circuit board.
And so that's really nice because we can just go ahead and fabricate that using off the
shelf, you know, printed circuit board techniques.
And then we have one sheet that's in this orientation, then a dielectric thin film in
between, and then the other flex PCB that's on the next side as well.
So then that's what this sort of 2D service here looks like.
And here we're engaging and locking between them.
And you can see some of them start to fail as well.
I guess that was the unlocked one.
Here this region below is locked and above is unlocked, you can sort of see.
And then here's the locked region.
And at a certain point it'll start to fail as well.
And so basically we can, again, computationally control which of those areas we want to be
locked and unlocked.
And that allows us to create these different shapes.
And then we have an inflated bladder that's underneath it that we can then basically
inflate and that will then create this global shape change.
And so here's kind of an example of this 100 percent locked, which sort of creates
this, you know, basically a very uniform shape.
And then now we're locking less and less of the display to create different
curvatures, if you can see like that.
So we think, again, this is kind of a promising approach.
We're still looking for basically, uh,
exotic patterns where we can have higher, uh, you know,
amount of strain and more basically curvature that we can create then.
But we think this, again, has some benefits in terms of being able to really
create and manufacture this, uh, very quickly.
A related project that was kind of in the early stages is on kind of connecting
this idea of these continuous shape displays with some of the work that I
showed at the beginning on these shape changing robots, uh, where basically we
want to have these elastic grid shells.
Uh, so if you think about it in computer graphics, we often have like
nerves surfaces, which are kind of combinations of these, um, you know,
different splines that are connected to each other.
What if we do that in the real world?
And so basically, Sophia Weitzner and Winxham Lawn, our group, are trying
to create these, uh, basically, uh, robotic elastic grid shells that can then,
again, change their geometry in real time.
This is a small one by one prototype, but here you can see we locked one part of
it and then we're able to inject more material into it and create this, uh,
curvature there.
And so this is what we're kind of aiming to do on the left is create these, uh,
you know, basically inter connected, uh, grid shells that we can in real time
change their surface.
Um, so these are some of the things we've been doing in our group to think
about, you know, how do we push forward and make these higher resolution, uh,
you know, surface displays.
And I think we've had some great promise in looking at electrostatic adhesion
as well as kind of new approaches to making more continuous surface displays.
But I think it's really clear to us and probably you as well, that the hardware
will really never perfectly render the real world, right?
The real world is so rich and very complex.
Um, and, and so I think there's this big gap between that.
Uh, but the interesting thing is that our perception is also imperfect as well.
And so maybe we don't need to have perfect hardware when we're considering
these types of displays.
And so the last part of my talk, I want to talk about some of the work we're
doing in terms of using, uh, visual haptic illusions to improve the perceived
performance of these types of tactile displays or other types of shape displays.
Um, so the first is a work from my former PhD student, Parastu Abtaiki, who's
now starting at Princeton University next year, that's looking at, uh, this
same problem of how we might use our, our, the fact that our visual, uh, our,
basically our visual perception is often dominates our haptic perception.
And so one way to illustrate this is, you know, basically our, our
proprioceptive system is not very good.
And so if I am able to like, you know, basically touch my fingers together
in front of my face, I'm actually often using my visual system to really
help me with that.
But if I try to do it above my head, I can sometimes get it, but you'll
find that it's not as accurate.
And so again, that's because of all of the kind of errors along the line in
terms of, uh, you know, our different joints and different mechanical
receptors that we have.
Uh, but our proprioceptive system has more noise and air than our visual
system.
And therefore our, our, you know, brains, when we're thinking about
integrating this multi-sensory integration, sorry, integrating this
multi-sensory information tend to rely on our visual system.
And so as I mentioned before, there's many of these limitations of these
shape displays, uh, that I've talked about in terms of low spatial resolution,
limited display size or low actuation speed.
And the intuition or insight that, uh, Parastu had was, Oh, how do we leverage
this fact that our proprioceptive system isn't very good to sort of increase
the perceived resolution.
And this really, uh, kind of essentially, uh, builds on a technique, uh, from
the field of virtual reality that's called redirected touch.
Some of you might be familiar with the idea of redirected walking, where again,
we can kind of steer people in virtual reality by having some slight offset
between where you are in the real world and where you see you are, where you
see yourself or where you see your hand in the virtual scene.
And so what we can do is essentially, uh, apply some small virtual offset as I'm
reaching that basically makes it seem like my hand as I move straight is
moving to the left.
And my, uh, basically visual motor system will basically compensate for that
and make my hand move to the right in the real world to basically compensate
for that bias or shift to the left.
And so we can basically computationally steer where a person's hand is going by
applying these offsets in the virtual world.
Um, and by leveraging that redirected touch effect, we're able to address some
of these different aspects of low spatial resolution and low actuation speed.
And so I'll talk about some of the ways in which we're doing that here, uh,
using angle, redirection, scaling up and, and vertical redirection as well.
So you might be familiar with this idea of anti aliasing or the aliasing
effect that happens when we look at, uh, graphical display, this aliasing
effect is actually also very pronounced in these tactile displays as well.
And so one way that we were trying to mitigate this low resolution is by trying
to essentially get rid of this aliasing that happens when you display, uh, a
vertical or sorry, uh, a diagonal line.
And so if we see here, uh, as we move along the surface here, uh, you know,
basically we have this kind of again, aliasing effect that you can feel these
bumps as well.
And so what if instead we could redirect you so that you're moving along a
straight line, which is very, doesn't have that aliasing problem, but in the
virtual world, you think you're moving along this diagonal line.
And so again, by applying this slight offset between where your hand is in
the real world and where it is in the virtual scene, we can make you touch
different areas of the display and, and not be able to, to realize it or perceive
it in terms of, you know, the overall resolution of the device, you know, we
can think about the number of pixels or taxles that you're in contact with as
you move over a surface, right?
Um, and so one of the challenges, right, is if we have a small object that
we're rendering on a tactile display of a fixed resolution, then you're not
going to encounter that many, uh, taxles as you move along the surface.
So again, what if we could change this essentially control the display
ratio, um, or utilize this, this offset between where my real hand is and my
virtual hand is, and in the virtual scene, uh, render a small object, but in
the real world render a larger off object, uh, of the same version and we
can render it in a higher resolution.
So again, we can change the, the ratio between where I'm interacting in the,
the real world, where I'm interacting in the virtual world and, and leverage
that to improve the perceived resolution.
Um, and we've done this also for things like extending the, the height of the
display, the workspace limitations of it and other things like that as well.
So here you can see as I'm moving up, I have some fixed amount of, of range
that I can move up and then here we're able to again offset where you think
your hand is in the virtual scene.
Uh, and so you can think that the, you know, basically the tactile display has
larger pins or, you know, then we can actually create with these displays.
Um, there's some, we've run a lot of psychophysical studies to find what
these thresholds are and also find that when we display these underneath the
basically when we display an offset underneath that threshold, uh,
essentially people perceive it as being higher resolution without noticing
that it's, uh, there, but we've also found some interesting effects in the
difference between active versus passive touch, where in the active touch
conditions, that's where you're moving your hand as well.
Uh, so for example, in that angle redirection, uh, we can't, uh, offset
people's hands as much as we can in the passive touch condition where, for
example, the motor is moving your hand, uh, where we can offset people more.
And so again, that has to do with kind of the, uh, you know, forward model that
people have, uh, in their sensory motor system in terms of, you know, basically,
uh, my predictions of where my hand might be.
And so I'm more willing to allow things to have noise and, and there to be
more air when I'm being moved versus I'm moving myself as well.
So we've looked at ways in which we can, uh, leverage this to create different
types of applications such as, you know, improving the resolution or, or, uh,
you know, again, increasing the vertical redirection as well.
Um, but as you didn't see from those types of systems, you know, these
illusions really only work in this small area of the display.
And so another set of work that we've been doing is trying to improve the
kind of scalability of how we might apply these to a much larger area and
create essentially what are called encountered type haptic displays that
might operate over a larger region.
So we put these types of haptic displays, tactile displays on, uh, you
know, robotic arm.
And then when we reach out, the robotic device can be there in time.
But one of the challenges with this type of approach of, uh, encountered
type haptic devices is that oftentimes the device has some limitations as well.
Right.
So the device may arrive late or it might be out of the workspace of that, uh,
robotic system.
So I want to touch something up here, but the robot is only down there.
And so again, there's a number of challenges with these encountered type
haptic devices in terms of different reachability issues, um, that lead to
these uncertain spatial discrepancies, which really kind of affect people's
perceived performance of these devices.
So we've done a few things in this area to improve the, uh, essentially
the, the ability for these devices to work by essentially redirecting your
hand to the reachable area of the robotic system.
Great.
Okay.
Great.
So anyway, basically we can do some cool things in terms of this, uh, you
know, redirection to guide people into different people's space.
And we've also done some interesting work on trying to apply, uh, basically
model predictive control to, uh, basically run this in real time to improve
the perceived performance using a model of human reaching and sensory integration.
So I think that, uh, I'll conclude there with just one short statement, uh,
if I didn't find it, uh, which is that, you know, we started out this work
really thinking about how real, uh, does haptics need to be.
And I think where our group is going is, is really trying to think more
about how real does haptics need to seem and really trying to leverage
this sensory motor control perspective to optimize both the hardware and the
software together.
So I'd like to thank, uh, you know, my PhD students and postdocs in my lab
that contribute to this work as well as our funding sources.
And I guess I'd be happy to answer any questions.
Uh, yeah, I thought it was till, uh, 130.
So my apologies on that mark.
Yeah.
So thanks so much.
Yeah.

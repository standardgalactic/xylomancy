Being a speaker at this seminar series is, I mean, it means a lot to me personally.
When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but
I didn't really have that many resources, so I had to rely on online resources.
And this seminar series recordings have been posted online, and I think I've watched pretty
much everything to really learn about HCI.
And then I came as a master's student here in 2008, and took 547 pretty much for the
entire two years I was here.
And now I feel great that I get a chance to speak as a speaker.
So this is great.
Today, I want to talk about interaction-centric AI.
This is a reprise of the New York keynote talk that I gave two weeks ago in front of
thousands of AI researchers.
I tried to reframe it a little bit so that it's more customized for an HCI audience rather
than an AI audience.
But the idea is that I want to think about using human-AI interaction at the center of
developing AI technologies.
And of course, I don't have to preach to the choir that human-AI interaction actually
matters.
But diving deeper, based on my experience of building these interactive systems in different
contexts like education, discussion, decision-making, I want to dive deeper and report some of the
detailed interactions that we've been observing and learning from and think about what it
means to design human-AI interaction in various contexts and what are some action items moving
forward as a community.
So let's first start with some definitions and terms.
I would say the dominant paradigm for developing AI technologies has been model-centric.
The idea is to build a model with high accuracy, and we want to evaluate it against unseen examples
for its generalizability.
And benchmarks have been great in that they could help us competitively compare different
models' performances, which could be useful in making scientific advances possible.
And more recently, people have been talking a lot about data-centric AI, where the idea
is using this nicely performing model, what is a good sort of robust and efficient data
pipeline around it in terms of collection of the data, processing of it, cleaning of
it so that the model actually performs really well in different contexts.
And here the focus is acquiring quality data and setting the pipeline in a way that really
helps the machine perform its best.
And these two paradigms are great, but then what is slightly missing is the user who's
using these AI technologies and those who are affected by what the AI systems give you.
So interaction-centric AI is sort of my term in some contrast to model-centric and data-centric
AI, where the goal would be basically what HCI researchers do in this context, like improving
the user experience by building usable and useful applications, and the unit that we
often grapple with is a human-AI interaction.
And you might be wondering, is this some sort of marketing term?
How is it different from human-centric AI we've been talking all about?
It's largely similar, so I'm not trying to say I invented this new term or anything,
but I want to focus our attention to the interaction that is happening between humans
and AI and the complex relationships and the dynamics that are happening between the two
rather than focusing on just the humans or a machine alone.
So I can say that that's sort of the focus of where my sort of discussion will be today.
So let's say you're this AI researcher and your team has built this amazing model.
So this is actually something that I copied and pasted from one of the diffusion models
papers.
I don't know what they actually mean, some of them I understand.
But basically, this is what you have as an AI researcher.
But what would a person using this kind of AI want to do with it?
Here's an example.
So this is a Twitch streamer in South Korea who was trying to use this diffusion-based
text-to-image generation model to create this image of an animated character eating
ramen with chopsticks with noodles around the character.
So this is the roughly sketched-out goal that the user company has.
And he ended up spending two hours fiddling with text-based prompts to get at the final
image that he wants.
And this is somewhat similar to what Manish shared a couple of weeks ago at the AI conference
in terms of what he had to do with the prompt-based interface.
And here, the entire two-hour journey was live streamed.
So I want to kind of share a quick summary of what happened in that stream.
And of course, we need something in the middle to bridge between the technology and the human
user.
And that's what we have, the prompt-based interface and interaction that's happening
between the two.
So the streamer started by something simple and obvious, the prompt says, eating ramen.
And this is what he got.
It's OK, it's kind of there, but the bowl is perhaps too large.
The chopsticks are all to be placed.
And he heard from somewhere that adding a full sentence might make things better.
So he goes, she is eating ramen.
She is eating ramen, for sure.
But you can see that something's not quite right.
So he keeps going on by adding more descriptions.
And the prompt is definitely getting longer, and it seems that the AI is not quite getting
how chopsticks should be used and how many should be used.
So he keeps adding these descriptions to really explain what it means to use chopsticks.
And to be fair, there's hair, there's chopsticks, there's noodles.
So in computer graphics, dealing with human hair, I heard, is a really tough challenge.
And maybe for AI, it's also kind of struggling to deal with all these similar-looking objects.
And it doesn't really seem to get how to differentiate between chopsticks and noodles.
Another interesting aspect was that since it was a live stream, the viewers were actively
participating in recommending new prompts to try out, sharing their interpretations.
And this is somewhat of a collaborative mental model construction process, if you will, as
a group of people, they are really trying to figure out what's going on.
And now the prompt is five lines long.
And the service that this streamer was using was supporting variations, where you could
pick an image and say create some variations.
And he was referring to this interaction as variation gacha.
So gacha is a Japanese word for like a random box or blind box.
And this kind of tells us that how unpredictable this sort of interface is.
Once you hit the generate button, the user doesn't really have a good sense of knowing
what to expect.
And this is the actual stream, as you can see, like his praying, and which also tells
us about the usability of this sort of system.
He doesn't have a good way of knowing what to expect so that he actually has to pray.
After two hours of hard work, this is a final image that he landed, and it looks pretty
good, and he claims victory.
But then look at what he had to do at the top, right?
There are seven lines of prompt that he had to write.
And arguably, this is natural language, but I would say this is really pseudo-natural
language, and so this is basically the experience that he had to go through.
So is this a good interface?
And I sort of got inspired by Manish's discussion of discussing the usability of these text
prompt-based interfaces.
There are some good elements, right?
It's quite intuitive.
You can use natural language, or you believe natural language could be used, and the output
is presented in a visual manner, which helps you kind of understand whether you got the
image that you like or not so that you can sort of debug.
And there are some interactions that are supported, like variations and seeds and like words
that should not be used and things like that.
But there are many ways in which this interface actually fails to support what the actual
user wants.
He had to rely on trial and error, and just the fact that he had to spend two hours to
get that image to suggest that something is really wrong.
And of course, it was not really predictable and lack of specific feedback on the effect
of what specific words in the prompt had influence on the final outcome.
These links were often missing, which made it really difficult.
So is this really just a problem for these text-based prompt-based systems?
I would say every AI application faces these interaction challenges.
On the user side, when they first encounter these systems, they often have to struggle
to kind of figure out how to make it work.
Often people resort to misusing it, abusing it, and learning takes a long time, and part
of it is really a design challenge.
And we've seen other examples like this, where people don't have a good sense of what's
happening in this algorithmically-generated systems and AI-powered systems.
Like in the famous study of Facebook News Feed users, more than half of the participants
were not aware of the News Feed creation algorithm's existence at all, which is far from being true.
And on the right, what you see is in the pathologists' diagnosis scenario, often they
would rely on some notion of similarity.
So there are these algorithms that are designed to help people find similar images, but then
the realization that the researchers had was that people had different notions of similarity.
So a singular notion of similarity that was used in building an algorithm would not really
suffice.
So what they ended up doing was to support three different types of similarity interaction,
and the user was able to kind of transition between these different terms in a fluid manner,
which really gives more control and agency on the user side.
And these, no, put in a more simple sort of diagram manner, whether you are a creator
or Facebook user, pathologist, you seem to have some kind of a mental model of how the
system works, a very sort of a classical sort of gap between what the user wants and the
system wants, and obviously the system is not behaving in a way that you really want.
And this gap arguably seems to be larger with these more complex black box and deep learning
based systems.
And AI community has been tackling this problem as well, and some of the folks have been framing
this as an alignment problem, which is about aligning the model's behavior with human intent.
And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open
AI's response to the alignment problem, where their idea is, in addition to the basic large
language model that they have, they would add this fine tuning layer with human feedback,
which often involves asking people whether they were happy with the results they got,
and the system kind of uses that feedback to train a reinforcement learning agent to
do the fine tuning so that the resulting text aligns better with what the user wants, and
they were seeing some success from it.
And a quote from the paper is that making language models bigger does not inherently make them
better at following a user's intent.
Aligning language models with user intent on a wide range of tasks by fine tuning with
human feedback.
And of course, there's been a lot of discussion about whether this is really the most promising
way to involve humans or alignment problem, but I think this is some progress towards
that direction.
But all of these examples, I would say, basically lead us to revisit these classical notions
of Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s.
As a user, they want to know what's going on with the system, and they want to have
more control and agency.
And on the evaluation side, when AI gives you some kind of result, they want to be able
to understand it, interpret it, and want to get some explanation of it.
And as an HCI researcher who's building these interactive systems, I feel like in often
cases I try to bridge these gaps, like come up with new ways of designing these social
interactions and human-AI interactions in a way that tries to bridge these gaps.
And these are just some of the systems that I've been developing in different application
domains, and I think many of them have somewhat succeeded in bridging these gaps, but other
times, to be honest, we haven't done a good job of doing that.
So what I want to do for the remaining time for this talk is to share some of these lessons
and some of them from positive experiences, but other times, bitter experiences by something
that we haven't really done a good job of.
And the main message that I want to send across is that beyond these point solutions for this
system that works in this particular context, we've seen some success.
I think as a field, we really need to start thinking about, can we do something more systematic
and sustainable, or empower designers and developers in thinking about, can we develop
these AI applications that are more usable and useful for more groups of people, rather
than having to reinvent the wheel each time someone has to develop these applications.
And I think we're seeing too many of these cases where people are like, there's this
cool model, let's build something around it, and it just gets released in a few days and
realizes that people want it in a completely different way, people abuse it, a few days
later it goes down.
We're seeing too many of these failure cases.
So from the HCI point of view, I think HCI research can really advance this interaction
centric AI by contributing these generalizable building blocks for designing these systems
and interface affordances.
And AI research can also advance by embracing the idea of interaction centric AI, by rethinking
models, architecture design, benchmarks, metrics, and research process.
The part of it has to involve broadening the perspective beyond just thinking about the
model and the output that it generates to think about the users behind those and their
mental models, and often there's not just a single user, but a group of user, community
of user, a society of users.
And there's also the temporal dimension, like before the user comes in and tries to use
the system, we should be asking the questions about like, what's the task and who are these
users and why and how.
And during the interaction, we need to be thinking about presentation visualization.
And the other way around as well, like interpretable results are being presented to the user, do
they have a way to provide feedback to the system?
And also, it's never going to be just a single use, right?
People would want to come back and use the system for a sustained amount of time.
In those cases, people's mental model would evolve.
And what does it mean for the system?
So I think this is sort of the ecosystem that I have in mind.
And with these, I want to dive into these specific examples where we designed human-AI interactions.
And I identify four major challenges in terms of human-AI interaction.
The first one is about bridging the accuracy gap.
So I'm on my sabbatical now, I'm working with this startup called Ringle, where they are
basically Uber for language learning.
They are matching tutors and tuties and they have this video-based language tutoring session.
So what we try to do here is to build this diagnostic service based on analyzing the
chat-based tutoring session to give people personalized feedback and suggestions for
improvement.
But instead of going into the details of the service itself, I want to touch upon the case
that we ran into.
And we were trying to run this automated speech recognition AI, which is crucial in
sort of turning the video-based chat into text format, which is really required for
us to run all these diagnostic algorithms on top of.
And the standard metric of success in ASR would be word error rate, you know, how correctly
it can recover the original text.
And on the tutor side, when we ran ASR on hundreds and thousands of sessions, the average
word error rate was around 8%.
Can you take a guess as to what the number would have been for students?
Obviously, there's this white margin that's quite high, so you can imagine, 30.
Yeah, we were seeing 23.
So there's quite a bit of a gap.
And this is an example of an accuracy gap where different groups of users are getting
disproportionate results from the same AI.
And the gap actually widens if we look at, like, the best tutor and the worst student
when it comes to the performance of these models.
But in terms of thinking about the interaction that these people are trying to have with
this AI, I would argue that the students are the ones who really need this AI to work,
right?
Based on the accuracy of this AI, they want to kind of look at where they succeeded and
failed and they want to learn and reflect.
And with this low accuracy, they would really be struggling to come up with good action
items and they might be frustrated, they might lose trust in the system.
But interestingly, a lot of focus when it comes to model development is that we seem
to be focusing on the 6%, like making the 6% better instead of narrowing the gap between
6% and 36%.
And we have to really be asking, like, what is the most important question in this context
and are we really focusing on the most important question?
And we see these other examples, too, where, you know, Tyra and others have studied the
machine translation that is being used in emergency rooms when it comes to discharged
statements that are presented to patients and patients' families and we see a huge disparity
between different languages.
And in the natural language processing community, this support for low resource languages has
been a topic for research and there has been great efforts.
And on the right is the famous example of gender shades, where the gender classification
algorithm shows, again, an accuracy disparity between darker skinned female versus lighter
scale male.
And of course, these diversity and inclusion efforts and low resource language support
research in the AI community and in the community have been tackling these issues of accuracy
gap, of course, but then I would argue that they could advance further by embracing more
interaction-centric approach in trying to really see how, in the real world, people
are interacting with these results and what kind of actual struggles that they have because
of, you know, poor or good AI accuracy and what, as a community, how can we define the
problem that's most important.
And conceptually speaking, I feel like a good analogy might be the ceiling and floor analogy.
The ceiling would be this primary user group, you know, who gets the best part of AI and
floor would be sort of, you know, secondary user group who is disproportionately getting
more negative impact of the same AI.
And there's this accuracy gap.
And often I feel like taking a model-centric approach incentivizes, you know, people and
researchers to work on raising the ceiling.
There could be a couple of reasons for this, right?
First of all, that's the sota number you get, which might be what you need to publish a
paper out of it.
Or the benchmarks that you're working with do not really have much data on the floor
side.
It might be more focused on the ceiling side.
And that's why the ceiling is there in the first place, right?
So it might be just incentivizing people to continue to push the boundaries of ceiling.
And as a result, what we see is the amount of a widened accuracy gap.
And if we take a more interaction-centric approach, I would argue that if we identify
that narrowing this gap is a more important problem, we can narrow this accuracy gap.
And it's not just a matter of accuracy, if you think about it.
It's about experience, benefit and value that people get out of interacting with this
AI.
So there was a first challenge about the accuracy gap and how thinking about how people interact
with this AI can help us identify what problems are worth tackling.
And second of all, I want to talk about when people actually use AI.
And one of the anti-patterns of human-AI interaction is that people just stop using
AI altogether or abandon it, which is something you might want to avoid as a system designer.
And that's why it's important to think about how do we incentivize people to work with
AI?
And in most cases, people abandon using AI because it's not really giving them concrete
value that they expect.
And we explore this in the context of online education in this system called XS.
So the problem that we wanted to focus here is that in online, let's say you want to learn
some new concept like probability, there are lots of problems and answers you can find.
But finding good explanations is surprisingly difficult.
And generating high-quality explanations is costly and resource-intensive as well.
So we wanted to tackle this problem by building this online education platform where people
are presented with a problem, and they solve this problem, they submit an answer, and they
see an example that's presented by the system, and they get a chance to rate how helpful
the explanation that they saw was.
And then they are getting a chance to sort of self-explain their own answer.
So this is a pedagogically meaningful activity to be able to sort of explain your thought
process, externalize it, and lots of research supports doing self-explanation.
Okay, so fairly simple sort of front end in terms of the learner's experience.
So what's happening behind the scene is that the system is collecting these explanations
and ratings from learners, right?
Since it's a live system, new learners keep coming in and provide new ratings and explanations.
And we formulate this in a multi-armed bandit manner, which means that as a new explanation
comes into the system, as a byproduct of humans' learning activity, a new arm gets added to
the system.
And what the system is doing is to determine this sort of dynamic policy for what the most
effective explanation would be for the next learner coming into the system.
So if you're familiar with the reinforcement learning concepts, we are navigating exploitation
and exploration trade-off, exploitation in the sense that the system wants to present
the best explanations to the next learner coming into the system.
But the system doesn't really know what the best explanations are until it collects some
amount of ratings from people.
So it has to do some exploration where it should collect these data.
And to solve that, we use a technique called Thompson sampling.
So what happens is the system keeps track of these policies, and when a new explanation
comes in and ratings come in, these things get updated, and the policy, probabilistic
policy gets updated so that it uses this distribution to determine what explanation to show to the
next learner.
So when we ran a study, these access-generated explanations were helpful in terms of helping
people learn better.
So when we compared against presenting no explanation at all and measured differences
between pre-test and post-test results, we were seeing that people were gaining 3 percent
increase in their scores.
So just getting a chance to rethink the problem, I think, still gave them some increase in
their scores.
And when they were seeing the instructor-generated explanation, which is, I guess, somewhat of
an ideal case or the standard case, we're seeing 9 percent increase, and with access,
we're seeing 12 percent increase.
So between these two conditions, it was not statistically significantly different, but
there were certainly cases where access was picking explanations from learners that were
even more powerful than the instructor-generated ones.
So in this system, if we were to take a more model-centric approach, I think we might have
built an AI that automatically generates high-quality explanations.
But instead, in taking an interaction-centric approach, I think the system we created is
basically this co-learning system, where the user, the learner, and AI are learning
at the same time in a single system.
So it's sort of an education-focused system of the game-with-the-purpose kind of setting,
where organic benefits are provided to people who are interacting with the system, and the
system is learning something useful out of it.
And this is basically the mechanism that we have in that both sides are learning, and
explanation and feedback are establishing this loop.
And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,
where learners as a crowd coming into the system are basically doing this by getting
their individual benefits while they're providing something useful for the system to learn and
do its thing better.
So since then, I've been expanding this idea to a broad array of applications.
So for example, can we use this kind of co-learning ideas to summarize how-to videos in terms
of steps and sub-steps, or building a concept map out of an instructional video that shows
relationships between different concepts, or helping learners come up with the solution
plans in algorithmic problem-solving settings.
And other researchers have taken on this idea in different application contexts as well.
So I think we can try to really generalize this kind of idea of co-learning system design
in different contexts.
Moving on to the third challenge is about beyond a single user.
Often we think about a single user, a single AI interacting with each other.
In real life, it would be much more complex than there would be diverse configurations.
So how can we consider these social dynamics?
And there could be various types of social dynamics, but one specific instance that we
did in was group-based, chat-based discussion in a group.
So we built this system called Solution Chat, where the idea is what if this AI agent could
recommend real-time moderation messages to a group?
So let's say a group is discussing what to do for the company retreat next week, and
they're having a discussion.
The system, in real time, based on the understanding of the discussion context, and also knowing
what kind of messages would be useful for the group based on our literature survey of discussion
and discussion-based education, it presents these recommendation messages, like, any more
ideas, or can this person share their opinions, you have been quiet for a while, or should
we try to move on to the next stage, or thank you for your opinion?
So these kinds of moderation messages are presented by the system in real time, just
like what you get in smart replies in Gmail, for example.
And as a moderator, you can just choose to accept any of the messages that you like,
and discard the ones that you don't like.
So a quick summary of the results of what we saw was that in our lab study with 55 users
in 12 different groups, when we compared how many moderation messages were used in different
groups, when we compared the baseline condition without these real-time recommendations versus
Solution Chat, our system, we're seeing a significant increase in the number of moderation
messages that were present in the chat stream in the Solution Chat condition.
But interestingly, you can see that the users manually typed moderation messages were actually
decreasing in Solution Chat, but many of them were replaced by the accepting AI-generated
recommendations.
And furthermore, we had this great opportunity to actually release this system to over 2,000
real-world users in a corporate education setting.
So during COVID, a lot of these corporate education programs moved online, and this company that
we worked with wanted to use this kind of system to moderate hundreds of chat rooms that
were doing discussion-based activity.
And not surprisingly, just like the very first live stream prompt example that I mentioned,
people were collaboratively trying to understand the capabilities and limitations of AI when
they were first presented with the system.
So they were using the chat to test different messages, often things that they believe would
be not working, and they would be sharing the results of, oh, this is working, this is
not working, I think this does this well, but not that well.
And it seems as if, you know, as a group does this kind of testing in the very first phase
of their usage of the system, people have this shared expectation of the system, and
that seems to sort of determine their further interactions with the system.
And it was also notable how different groups had different expectations based on their
limited experimentation that they did in the beginning.
And there were some interesting social dynamics that we observed as well, like in how people
use these AI recommendations to socially interact with each other.
Some people were using AI as proxy, so one of the quotes that we had was, I didn't want
to directly ask the person to stop talking, so the person relied on the AI-recommended
message to kind of send it, and they still chose to send it, but it was their way of
kind of softening the potential sort of dispute with the person.
Other people were using AI as a reference, so what we were seeing is that it was a fairly
simple technical pipeline that we had, so it was just a canned response.
So people were sometimes not really fond of the tone of the message, style of the message
that we showed, so the person said, I found no fun in the recommended messages because
all the messages looked the same.
So in those cases, what people did was they still adopted the idea from the recommendation,
but then rewrote it so that it feels more personal, and it feels more like it's coming
from them, not AI.
In other cases, AI seems to be adding a social burden.
So in this excerpt, so one of the people said, I'm doubtful about the credibility of AI,
and then the moderator picks this AI recommendation, thanks for your opinion.
Another person says, I also think negatively, thanks for your opinion, thanks for sharing
a good opinion.
Shall we go to the next topic?
And then the moderator realizes he might have clicked, accept way too many times and
it was a little unnatural, so he stopped to kind of clarify and apologize for my unnatural
words as I'm using AI recommendations.
So while we were seeing how people were saving their time and cognitive effort in moderation
could have decreased, it might have actually introduced other types of burden at the same
time.
Again, so if we were to build this kind of system in a more model-centric manner, I think
a good alternative might have been automated discussion moderation, where AI would actually
do all the moderation by itself.
But instead, we chose to take a more AI-assisted moderation for obvious reasons.
Users want to have more agency and control, and they wanted to keep their style of communication.
So instead of handing over the entire control to AI, we still sort of gave that control
to the human moderator who could kind of use it as an additional resource.
Okay, so there was a third challenge.
And moving on to the final challenge of supporting sustainable engagement.
Here the concern is that we want to think beyond this single session usage, and over
time how people react to these systems might change, their mental model might change, and
how AI actually works might change.
So we need to really think about this temporal dimension more carefully.
And for this thread, we investigated in the context of novices making changes to websites
that they're seeing.
So for example, you might have a case where you visited this website, the colors hurt
your eyes, or you couldn't really find this button or tap it because it's too small, maybe
you want to make it larger.
But then people without expertise in HTML and CSS have difficulty doing this.
So we thought by leveraging the power of large language models and so on, maybe we can support
more natural language queries.
So if a person says tone down the text, the system can kind of display these style recommendations
that they can explore and select from that are about toning down the text.
So the way the system works is if the user clicks and says make this larger, the system
presents a set of design attributes that are about making something larger.
And the user can say emphasize this part.
It's something somewhat ambiguous, right?
There isn't a clear single design attribute that is about emphasis, right?
So it presents these few recommendations that are about emphasizing something.
So we built this by establishing this NLP pipeline and computer vision pipeline.
On the NLP side, what it does is analyzing the user's query and mapping them with the
style attributes that seem to be connected to what the user's intent is about.
In terms of computer vision, we collected millions of web design elements to determine
a good set of recommendations to show to the learner.
So by combining those, we built this system.
Again, so instead of going deep into the technical details of the system, I want to focus on
the interaction dynamics.
So we ran this user study with 40 people where we presented them with either stylet, which
is the name of our system, versus the baseline, the Chrome developer tool, which is sort of
the standard tool for making these style changes.
So we compared these two groups.
And we gave people two tasks.
One is a well-defined task where we ask people to turn this before image into an after image.
And then secondly, we had this open-ended task where we gave this blank slate and people
were able to make any kind of change that they want.
First I want to share success stories.
People were more successful in completing these design tasks when using stylet.
Like 80% of the stylet users completed the task as opposed to only 35% in Chrome developer
tools.
And these were complete novices in web design, like no experience at all.
And people completed the task in 35% less time, so it was efficient to use stylet.
Another interesting observation was that people were making same similar number of changes
in both conditions, but in stylet condition, people were making more diverse changes, which
means that it probably had to do with how stylet shows these multiple options for people
to explore.
And there was a conscious decision to not just show the most obvious one, but show somewhat
related ones as well so that people could explore and tinker around different options.
But then an unexpected finding was when we looked at people's self-confidence.
Because we thought this kind of system would be useful for people's learning of the skills
and confidence that they have about the skills, we asked people's self-confidence after each
task.
What we noted was that after the first task, in both conditions, people's self-confidence
increased.
But then in the second task, after the second task, users' self-confidence decreased for
stylet while in the developer tool it kept increasing.
Why would that be the case?
We were seeing many cases where stylet users were frustrated that the only control that
they had was natural language.
Now they have some grasp of how it works.
They wanted to do more fine-grained control more directly, and they wanted more specific
things, but because they only had natural language, they sometimes just got frustrated.
Whereas in the Chrome Developer Tools condition, people were just happy that they accomplished
something with their own hands.
And I think that is presented as a continued increase in self-confidence.
We know from HCI and CS147 that people's expertise and learnability really matters.
As they have more knowledge of the domain and the skill, they might need to get more
advanced controls or being able to more directly manipulate what they're working on.
So I think this had some interesting lessons in terms of thinking about the temporal dimension
in that learners are changing.
And other researchers have been reporting that considering these temporal dynamics is
important.
On the left, what you see is design researchers who have shown that there are these different
stages of relationship that people have in technologies like self-tracking devices.
First they would start with initiation and experimentation followed by intensifying and
integration and then stagnation and termination.
And one of the design lessons might be that these might be more meta-level factors that
really should be considered in designing these systems in that even the same kind of intervention
might need to be presented in different manners depending on what stage you are or what your
expectation is with the system.
On the right, what you're seeing is the guidelines for human-AI interaction, really influential
work from Emershi et al.
And they organize these guidelines for human-AI interaction in different categories but are
organized in the temporal sort of aspect, like initially encounter with AI during interaction,
when things go wrong and over time.
So taking into account this temporal dimension can really be powerful in supporting more
sustainable engagement.
Even the related question might be, as people are relying more on these AI tools like grammar
fixes or even generating text, it's important to think about how people's mental model
would change over time and AI also changes over time too.
And do we hit a point where people become maybe overly reliant in that maybe their grammar
skills or writing skills do not improve anymore but then without the tool they actually might
perform worse?
And what is that dynamic?
Or maybe over reliance is perfectly fine because if we believe these tools will be around the
user all the time, maybe it's just the final outcome that matters.
And I think we need more studies and analysis of the long-term engagement of users using
these kind of technologies.
And to kind of sum up, if we were to take a more model-centric approach here, I think
we might have built a system that makes automatic design fixes to optimize a web page directly
and the system makes a fix and user can just use it.
But instead, we took a more sort of interaction-centric route where we asked people to do sort of
style change by themselves as the system was presenting these recommendations and they
still had to do the fix by themselves.
But what we expected here was that people can then customize by seeing these attributes,
they can learn, they can discover new ways of doing things, they can think around, which
can empower them, especially in the more learning context.
Although the temporal dimension has to be more carefully taken into account.
So these were the four challenges that I wanted to share today.
And to kind of wrap up, I just wanted to pose two questions moving forward from the interaction-centric
perspective as HCI researchers.
So first is how might we design these building blocks and interface affordances for new and
upcoming AI models?
So I think part of it is that instead of building these point solutions, I think we need to
think about are there any sort of generalizable frameworks, libraries, widgets, or interface
affordances that we could come up with as a community that is really good at these kinds
of things.
And the second question is, does AI really require us to have these new things?
I mean, can we just use existing design elements and frameworks to build AI applications?
And I tend to think that we might need something new for these new and upcoming AI models, especially
because they have these very different characteristics than the conventional systems that we have been
building.
They're more probabilistic, harder to predict, more black box in nature, yet seemingly more
impactful and powerful in terms of what they do.
Hallucinating, right?
All these properties packed together, I think we might really need to think about, what
are the types of interaction affordances that are really built for supporting the usability
of these AI-powered applications?
So in this, I think as a community, we are making all these great advances, like making
different types of contributions.
And I tend to focus on more interactive systems and techniques, whereas other people focus
on introducing new design processes and understandings.
And I think all this work is needed.
And some of the interesting examples of adding sort of an interaction layer to these new
types of models is in this example, Tail Brush, where the user can kind of draw the level
of fortune that they want in the character to have when they use generative models to
generate a story.
For this AI chains work, which presents these primitives and workflows for putting together
this workflow that can accomplish more complex tasks with these LLM prompts that a single
prompt cannot really perform.
In my research group with my PhD student, Tessu Kim, we have been investigating this
idea of what will be more generalizable design framework.
When thinking about input, model, and output, we have been thinking about the concepts
of cells, generators, and lenses, and tried to introduce this standardized libraries and
widgets that people can easily adopt in their AI applications.
So for example, using this kind of framework, people can build a copywriting app, email
app, or storywriting app using pretty much the same kind of framework, which can save
people's time while supporting the types of interactions like iterations and comparison
and experimenting different outputs.
And the second question, and the final question that I want to ask today is, how might we
as an HCI community collaborate better with the AI community on these various things?
And it was also the discussion that I was having a lot with today's meetings and also
with various AI researchers, especially in Europe.
And in terms of community collaboration, of course, one of the important things is metrics,
and there was also a great discussion at the HCI conference a couple weeks ago, hosted
here at Stanford.
And in the AI community, it cares a lot about model performance and generalization errors,
where in HCI, we tend to focus on the human experience.
So how do we really bridge the gap between the metrics?
What it means to do AI research with more human side metrics incorporated, what's the
incentive for people to do that?
And how do we encourage more AI people to kind of use these metrics, too?
In terms of human input design, a lot of the comments that I was getting in terms of interaction-centric
AI from AI researchers is that these ideas are great, but then I don't really know how
to actually take action about it.
And part of it is, in their model-building kind of work, how can I incorporate human
feedback and how do I use it in a meaningful way to really change the way the model actually
works, rather than just getting more high-level design guidance?
So one great direction for this might be, think about making human feedback more computationally
feasible so that this compatibility is actually satisfied.
And lastly, we need to think about the change in design process as well.
And in a lot of, this is Stanford D-School's user-centered design cycle.
And I think in a lot of the AI research, what we're seeing is this prototype test kind
of culture.
You try something new, test it, iteratively improve it.
But then one of the frustrations is that interaction often comes too late, right?
There's this new, cool model, and can you build an UI on top of it, is sort of the kind
of discourse we get a lot.
And I think interaction should not just be like an icing on the cake, but really something
that can guide the entire design process or help people determine, is this the right problem
to tackle in the first place?
Or what kind of interaction should we try to support with AI?
And based on that, think of what AI should do and should not do and how much AI should
be used in a particular context.
So that's all I wanted to share, and here's a summary of what I mentioned today.
And I'd be happy to take any questions.
Thank you.
All right.
So I'll check my recommendations of facilitating messages if there are any more ideas.
No?
What do you think?
Really sounded like an AI.
I'll just click them all.
I want to pull the mic on.
I want to pull the thread a little bit on this notion of how to connect human feedback
with the objective functions that you touched on near the end, because that's been rattling
around in my head in much of the talk that you're giving, that if I think about what
should AI researchers be doing differently, then you're asking, well, what's the proper
model of the person in their system?
And traditionally, the problem has been that human interaction is really expensive just
to collect annotated data.
Or once you have it to be able to tune the model, you don't get that much of it.
And so they often fall back on self-supervision.
Or as you've been talking about in the value alignment, they train an RL model to mimic
a human and then let that go loose.
And it seems like until, I think they're kind of, I want you to take a position on one of
the two positions.
One thing is to say, look, we need to find strategies like that where we can create proxy
humans and that's how we hook into the objective functions, the loss functions, et cetera.
The other alternative would be to say, no, we're going to find some other way to actually
make human feedback at a scale and in a form that they can directly use in the models.
I'm just curious, like, if you want to take a bet, where's your bet on that?
Where should we be heading?
Yeah, that's an excellent question.
I would say, I mean, you asked me to take a position, but I would say both will be prevalent.
And I like the letter purge much more.
And I think that's more promising and sustainable.
And for example, the reason I'm really interested in this like core learning feedback loop between
the human and the machine is that, you know, even if this super advanced AI comes along
and let's say it presents this like super accurate explanations, people self-explanation
activity is still meaningful, right?
Because that's how they could learn.
And so I feel like, you know, we can really try to find these compatible mechanisms in
which the human can get the benefit and get the incentive for doing what they are really
good at and what is helpful for them, not necessarily trying to help the system or getting
paid to system, paid to support the system per se.
And at the same time, the system can use it for something meaningful.
And on the system side, I think in the system like access that I presented, I was really
happy when we landed at this technical solution where people's rating data could be almost
directly piped into the feedback for the RL agent to kind of use as meaningful feedback.
So I think that's just one example where this kind of worked out for this kind of context.
And I think we need to really investigate more and think about are there many, any like
generalizable mechanisms that this kind of approach could work in different contexts?
This assumes that you have a large set of users you could draw on, like there are learners
that are coming through your system.
If I'm early on in the pipeline and I just kind of have V0, I don't have the users yet,
are there other strategies you would recommend?
Yeah, yeah, excellent.
So in that same access system, for instance, what we did was to insert the instructor generated
explanations as sort of the initial seed.
And I was also imagining maybe using LLMs, for instance, we can plug in AI generated
ones to kind of avoid the cold start problem.
And it'd be interesting to see how in the same system, like AI generated ones, instructor
generated ones and the learner generated ones can kind of compete against each other until
the system ultimately just focuses on what is best for learners.
This is kind of a two part question, going back to the like third challenge or like project
you talked about, where there was that note about AI as proxy, like people kind of using
that as like an excuse to make points, where maybe they wanted to do something that didn't
want it to come off as them.
So the first part of the question is like in that case, did people want to later it says
people wanted the message to kind of sound like them, but in the case of the AI as proxy,
did they want that to sound like them?
Or were they wanting it to sound more artificial?
And then second part of the question is, do you think there are more situations than just
this where maybe we don't want the AI to feel super personable and maybe want the interaction
to feel slightly more kind of mechanical or unnatural?
Yeah, that's an excellent question.
And I would say these were somewhat different use cases.
And both I think are valuable and smite.
And that again, I think in a more model centric approach, we also kind of focus on trying
to create these messages that are more like humans.
And that could be effective in certain cases, but as you said, that might not really be
what the users want, because in a proxy kind of setting, you might not actually want it
to sound too personalized, because maybe the more canned message might actually work better
in that context and vice versa.
So I think just being able to identify all these different needs that people have and
expectations that people have and being able to somewhat fluidly support those, I think
was really an interesting kind of observation that we had.
And I think moving forward, one of the lessons was that this more personalizable message generation
could be an interesting technology that could be potentially integrated.
But that's not going to solve everything, because there are these other types of needs
that will not be supported even with the perfect personalizable style transfer.
So yeah.
I kept thinking about how what you described and sort of the challenges that we see with
this new deep networks and models and how we interact with them are similar to how people
used to interact with search engines.
At the beginning, people were not as good as sort of figuring out how to query the search
engine right.
And over time, both we became better at querying the search engines, and then the search engines
became better at sort of understanding how to interpret user queries.
Do you see any similarities there?
Is there something that's very unique to the challenges we face with this new models?
Or is it just that we haven't had enough time to sort of adopt to each other in a way?
Yeah.
Excellent.
Yeah.
And I think it's a recurring theme as these new technologies come in.
Usually people would kind of struggle and they would need to learn how it actually works
through trial and error and lots of like failed attempts.
And that's what we're seeing with these like Chatchapiti, for instance, a lot of people
are trying things out, reporting success and failure cases.
So I do think there are certain similarities.
When it comes to what's more unique about what we're seeing right now is that due to
the nature of like how black box, complex, unpredictable these models are, I think it
just confuses people much more.
And there's a question of, is this really like a human learning problem to begin with?
So if people take, do it more, and if they had more time, will people be actually able
to really get to a point where they could really easily create something that they like?
Probably not.
So that's why I think we need both on the model side to kind of think about what are
more intractable and learnable ways of architecting these kind of models in the first place.
And also from the HCI point of view, what are these interaction mechanisms that could
be added to these models in a way that it is actually more understandable and usable
on the user side?
Yeah.
Thanks so much.
Yeah.
I think we're at about the time, but Duho will be here for a couple of minutes after
the talk for questions.
So let's thank you for speaking.
Thank you.
Thank you.
Thank you.

It's my pleasure to welcome Jan from OpenEI.
He leads the alignment team there and was previously a researcher at DeepMind as well.
He holds a PhD in reinforcement learning theory, has been thinking about the alignment problem
for over 10 years, and today he'll be giving a very interesting talk, so hope you guys
enjoy.
Yeah, thanks a lot for the intro and thanks a lot for having me.
I'm very excited to talk about this stuff.
I'm also super happy to keep it interactive if you have questions at any point, please
interrupt me.
Yeah, I want to start out with a few very basic observations on what I think is going on.
So the first one is TMII is joining the game.
So TMII has a lot of different players.
They aren't all joined at the same time, but rather they join one by one.
And not all of their players are like very a lot in how good they are.
And right now, a lot of the players that have joined so far aren't really that smart and
usually can do only a very narrow set of talks.
But one thing that we've kind of observed this over time, you know, we're seeing stronger
and stronger players join.
And this is kind of where we are now.
And then in general, we expect that TMII has incredibly strong players.
Those will be players that are able to think so much better than humans, so much faster
and so much more cheaply.
And these haven't joined yet.
And so the kind of like anchor point that we have, if you think for example about chat
to BT, chat to BT can already be any human at like knowing more facts or speaking more
languages.
And it can write about 50 words per second and can do so about 100 times cheaper than
humans could at minimum wage.
And so, you know, there's, chat to BT also has some really important limitations.
And there's a lot of things that it can't do yet.
But it is kind of an indicator of, you know, some of the players that may be able to join
in the future.
And so it seems like in the long run, TMII will have all the advantages over Team Human.
But, and there's one, there's an important caveat, which is, there is one important advantage
that Team Human has, which is Team Human gets to pick which players from TMII joined and
when.
And so this is kind of like an advantage that we should really be leaning into when we're
thinking about what to do and when we're thinking about, you know, this game that we're playing
with TMII, and then we'll be playing with TMII.
So I think two of the main objectives of what we as Team Human should do is like first,
we should try to recruit players from TMII to play on 10, Team Human.
And so this is kind of what I would broadly follow like that.
And this is kind of like the problem that I'm working on.
And then there's also other objectives.
So another objective that I think is going to be really important is you want to write
the rules of the game so that Team Human doesn't lose.
So right now, Team Human kind of has the ball and we get to write the rules.
So we should write the rules that, you know, make sense and that is still playing this
game in the future.
And so in this talk, I won't really talk about the second point at all.
And I will talk about the first point, because that's where I know that's not where I'm
and have to phrase it differently or to make it kind of like more practical.
Like one way I'm thinking about alignment is like you want to build the AI systems that
follow human intent and that, you know, follow human preferences that do we want them to do.
And so a bunch of the things, basically, I'll talk about two main things.
The first part is going to be work that we've done in the past and kind of like which roughly
is in the bucket of like we are trying to figure out how we can make the models that we have today
as aligned as we can.
And you just kind of trying to try hard to do this and we'll see how far we get.
And then the second bucket is the things that we have to do next.
The stuff that we haven't done yet that we think are going to be really important.
And I want to kind of like lay out why I think they're going to be important.
So now I said, you know, I'm like trying to make this more clear or like more broken down
what alignment means, so much here.
Because now, you know, the question is like, what does it mean to follow human intent?
And kind of like two main categories of intent that we care about is, I would say,
if it impents that you, you know, like I give the system an instruction,
or if it wanted to be my assistant, it should be my assistant.
I should follow the instruction.
But then there's also all these other intents there.
And don't say when I'm usually, you know, talking to a system or a human, they are ultimately care about it.
You know, it shouldn't literally always do what I say, but the thing that I mean,
it shouldn't make up stuff, and it shouldn't do harmful things,
and it should ask a lot of questions, and it's not sure what I mean, and so on and so on.
And so these are all kind of like things that are often just like really difficult to, like,
to precisely specify, or like, you know, what's precisely in words.
But it is still things that we want to get out to do, and that we have to figure out how to, you know, get into our system.
And so kind of like the main technique that we're using today for this,
is what we call the quantum line feedback.
So that was used to train structure to be key and test to be key,
which are the two main systems of this class.
And the basic system is very simple,
and it's also like a super general technique that applies to lots of different AI models,
and modalities, and settings.
But in this case, we'll be using the managed model.
And so the two steps is actually another step of, like, time limitation,
so I'm going to stop for the sake of simplicity.
The first step is you want to train your word model from comparison.
So you have it on the top.
In this case, now explain when I need to stick to you,
or, you know, help me with my paper, whatever it is.
And then the model does a bunch of things.
And then you made which one is quite close to the thing that you intended the model to be.
And so you have a data set of preferences and you train your word model.
The word model basically just learns to predict which one would you prefer.
Everything okay?
I don't think that's good.
I would say just stand more in front of the camera because it's impossible,
but I think it looks good.
Sorry about that.
Maybe let's turn it over.
Okay.
So, okay.
So now we have this through a model that captures kind of our preferences
and what we care about and what we intend for the model we do.
And then the second step is now you optimize your word model
between what you're talking about.
And so, not setting, you know, like model tries a whole bunch of different things
and then what model have called it, which one of these things is probably more
like the thing that it is.
Yes.
When you're thinking there, it's not made by human label,
but it goes into data.
Okay.
And I was always thinking, does that depend on the label?
It will depend on the label.
Different labels will have different preferences.
They also might be inconsistent,
and you can give me examples of like in front of the preferences,
but those haven't really been a problem in practice.
And so far, you know, like our flight list often don't agree,
but the model will average over all the time.
But yeah, so this is like the basic technique.
So essentially, you're like quite simple.
You can make it even simpler if you had, you know,
if you didn't turn it into a model and you labeled it said like every arrow,
but it would be a lot less data efficient.
And so you can, you know, turn it into one,
or you could think of like it as data efficient.
So how does it work?
So this is kind of like one of the main thoughts
from the instruction sheet paper, and this is the one I like showing
because it believes in my mind and it still does.
What do we see here?
So on the x-axis, you see, this is from the GP3 model series,
and you'll see this is like different three different sizes models
over two orders of magnitude.
And on the y-axis is how well does the model score on human preferences?
So if we show a bunch of samples to humans,
how likely are they to curve one over the other?
And then what we see is that even like the largest GP3 model
is this preferred to the smallest instructive key variant.
And so the 100x smaller instruct model is actually preferred
over the much larger GP3, like 45 GP3 model.
And that's kind of wild.
Sorry, let me just finish my talk.
So why is it so big here?
So basically, it basically shows that there was like a field of alignment
that could actually make the model more efficient and so much more useful
than just, you know, feeling how it works.
Or fine-tuning the model worse, but I know we want to use it.
Can you guys hear me?
And then we make all these fancy alignment techniques that don't get adopted.
And so what we were like originally in like the first version,
so it did solve these regressions.
And then what here is labeled PPO PX is kind of like a very end
where we mix in free training data into the fine-tuning.
And that mitigated a bunch of the regressions that we saw.
Yeah.
And I just had a good follow-up to that.
How important is like fidelity fine-tuned data that you have?
You guys, you collect data from humans, right?
Yeah.
What if you were to use some free training model to score, you know,
just into the reports or something like that?
Yeah.
And you could do that.
And there have been like other papers on that.
Like I thought they'd go with this paper on constitutional AI
that was trying to like exactly do that.
In terms of, well, there are certain things that the language model
will be able to automatically rank and some things it won't,
because, you know, it won't know your exact preferences
or it won't know exactly what we wanted to do.
And so, you know, whenever the language model does something that we just prefer,
we actually, we have to give it another data point, right?
Well, in other words, you know, if you're aligning with humans,
you somehow have to put humans into the loop so that, you know,
otherwise, how does the model know what it's supposed to do?
Okay.
Lots of more questions.
I don't know who was Tris.
Yeah.
How many human, approximately, like, what's, how many orders of that
ingenuity of like commitment preferences do you need to the two piece?
Yes, I'm going to get to that in a second.
Yeah.
Yeah.
We haven't, we haven't actually compared, carefully compared across our all algorithms.
And it could very well be that a different, our algorithm would be better.
That was kind of like, I know, PPO was invented open EI.
So that's why we used it.
It's not, not really good.
Other than that, it works also pretty well.
Yes.
Comparisons.
This is better than this other thing.
So usually we have people compare between like three to six different responses from
usually different models.
Yeah.
So is PPO and the reward model currently used in chat GPT in production?
And it's so like, do you use any of the human feedback, like, you know, regenerate responses
and stuff like that to help as a reward function as well?
How do you mean to regenerate?
Like there's a button on chat, chat GPT where you can say like regenerate responses
or do you use any implicit feedback basically in human use?
I don't know what the current state is for that.
I expect people will try to use it.
But, you know, monologue, chat GPT hasn't been out that long.
Yeah.
So I'm curious about this graph.
Like it seems like a hundred X, as you mentioned, increasing parameters
doesn't give you that much more like fidelity there.
Qualitatively, you have been tracking this for a while.
Can you tell right off the bat if you're like interacting with the one billion,
like a model or the like hundred billion model, like a Turing pseudo Turing test
parameter size, like I give you a black box speech on how many parameters it has.
Probably not very precisely.
But I think the big counter question is like, do I get to write the prompt?
So if you just draw random prompts from whatever people put in the opening of playground,
which is what we use for our GPT, then I probably need quite a few to tell the difference.
But if I get to write the prompt, I can probably do it in one or two.
At least like if the task is like tell the difference between this and this.
Yeah.
I want to, can I just do two more slides and maybe your questions get answered and then.
So this was the question about training costs.
So this is another thing that kind of really blew my mind is like, compared to pre training,
it is incredibly cheap.
So if you look at like the amount of flaps that he takes to GPT and then you compare it
with like how much does fine tuning and there are all what's pre training mix and everything.
And like the most expensive instructor to be the version is like less than 2% of the pre training
compute.
And if you want to train an even bigger model, it's going to be more expensive and you could
still use the same like fine tuning step to make it more aligned.
And of course, I think the important thing to note also here is like, we haven't fixed all the problems.
There's like important limitations.
And so I wouldn't say that this is like, you know, the last version and we wasn't trying to figure
out how to spend more compute and more human data in the future.
But all in all, it was surprising me effective.
Okay.
There were no more questions.
More questions.
Yeah.
I just wanted to ask what the PTS makes in pre training data into the RL fine tune, just
like mix the gradients.
Yep.
What's the number of branches for this graph?
So you fixed number of branches.
So this is the full size GPT three version.
So this is the 175 billion model.
Okay.
Okay.
Okay.
Great.
Okay, sure.
So the first one is.
Okay, sure.
So the first question is how would you deal with hard effects breaking in the limit?
Example preferences are a good proxy for this.
But optimizing for them is terrorized to incentivize perception.
Yes.
I'll get to that.
Sure.
Sure.
That's the next question.
So that is like, you want to automate alignment research.
What happens if you need conceptual big tools, which are the people for expressionally fine.
Okay.
That would be, I get to take at the end as well.
Sure.
Let's see.
Sorry.
Yeah.
I guess like one question is like, how would fine tuning directly on human feedback compared
to fine tuning with power?
Fine tuning like supervised fine tuning.
It's more like if you directly use a chemical data.
Yeah.
I'm also not sure what they mean.
Okay.
So, I mean, so one baseline I'm showing here is like, what if you just take human demonstrations
in the sense that, you know, we have a bunch of tasks, we just ask humans to do them, record
what they did, and then train the model to imitate that.
And here it's like just very basic behavioral cloning, just using the same last to use and
pre-training.
And then, you know, is noticeably better than the future punted version, but it's still
not as good as overall.
And so that's why we like using our all.
And basically, conceptually, there's two problems with the imitating humans approach.
One is humans are better at some things than the model is, and they're worse at other
things.
And so the things that model is worse, you're trying to imitate something that you can't
do.
And then the things where the model is better, you're making the model worse because you're
forcing it to do the thing in the way that the human would would.
And so with our all, you kind of with our HRF, you kind of letting the model do whatever
it wants to, and it can just figure out like the best way for it to do things.
There's also another important advantage, and I'm going to get to that by briefly want to
talk about chat GBT.
So one thing I kind of think of chat GBT as like the upgrade to instructor GBT kind of
like the next step, making the models more aligned and more useful to humans.
And some things that is like, you know, I think chat does better is kind of like using dialogue
as the universal interface, right?
You can talk to it directly, you can ask follow up questions, you can ask it to, you know,
refine the answer and all these things that makes it a lot easier to deal with.
It's better at refusing harmful tasks, but it's also, there's still important limitations,
right?
Like the biggest one is like the model of hallucinates a lot.
It makes up facts when, you know, for whatever tasks you give it and that, you know, it makes
it quite unreliable.
It's also still sensitive to prompting, which kind of shows that, you know, it still has
important misalignment that we need to fix.
Like really, if a model was like, models are really like, do the tasks do the best of its
ability, no matter how you're prompted to do that.
But yeah, one important principle that I think is really useful for, or that like our HF
leans on a lot is that evaluation is easier than generation.
So if you ask humans to compare and rank different responses the model gave, it is easier to tell
the difference between different variants of what the model did, then it is to do the
task yourself.
Or in other words, you know, you can do the comparisons on tasks.
You can still like spot good behavior on tasks that you might not be able to do by yourself.
And so if you're giving this kind of like feedback that lets, you know, the system to do better
than you actually could.
And I think that's a very general principle that holds in lots of domains.
So kind of like, you're probably most familiar.
Yes, you know, the P versus NP and everyone, you know, we don't actually know whether they're
different, but in practice, it seems like NP tasks are just much harder.
It also applies to lots of other settings, like a lot of professional sports or eSports
just wouldn't be fun to watch if you couldn't tell he's winning more easily than you could
actually compete on a professional level.
It applies to a lot of consumer products, you can like look at two smartphones and tell
which one you like more without that is like also deeper than just looking at like the
specs, but is actually very hard to build a good smartphone.
It also applies to academic research, you know, it's much easier to read your paper and
say what all the things are bad about it.
And then it is to write a good paper yourself.
It applies to, I don't know, when you...
Yeah, basically there's lots of domains where this applies.
And so I think this is like a very...
This principle is like very useful when we want to like align AI systems on tasks that we might
not be able to do ourselves well.
So having said that, RLHF has some really important limitation.
And I think that's going to make it really difficult to use RLHF to scale alignment.
Let me explain this with a diagram.
So basically on the x-axis, that's plot like the AI progress.
And on the y-axis, how difficult different tasks are.
And then as we have more AI progress, kind of like the task that AI...
The difficulty of tasks that AI can do goes up.
And like one of the fundamental problems is that the level of tasks that humans can reliably
evaluate doesn't go up because humans don't get better with AI progress.
And so I think we're like somewhere here.
But the problem is once you cross this line, you don't really know whether your model is actually
doing the right thing because you can't reliably evaluate anymore.
And so that's kind of like the point where RLHF training will start to break down.
And what we'll probably see is kind of what the question before I alluded to is like,
well, now the systems are optimized for whatever feedback we give them.
And so they will try to tell us what we want to hear rather all the things that they know to be true.
And they might learn how to deceive us because that makes it easier to score high on preferences.
And so kind of like the basic idea that we want to leverage is related to...
The principle I just mentioned, which is evolution is easier in generation.
So for example, if you have a large language model writing a code base, like an entire code base,
there's just no way humans would be able to find all the bugs and all the flaws in the code base.
Or the code base could have like a trojan in there and you might not be able to tell because it is so hard.
And that's why we see so much buggy code out there.
But if you ask your language model to find bugs and point them out to you,
once you've seen the bug, it's so much easier for you to say, oh yeah, this was a bug.
Please fix it.
And so now you've taken the task of a code base down to,
I just have to evaluate whether that was a bug according to the spec they had in mind.
And so the general principle that we're excited about here is like,
you want to leverage AI assistance for human evaluation.
And so the hope is that we together, if you pair up humans with AI,
you actually get a line that looks more like this,
where humans together with AI can evaluate much more than they could find their own.
And so to make this concrete, there's like two different ways you could do that,
or there's many different ways you could do that.
Two I want to highlight is like first, you could ask the AI to write a critique.
This is a project we did last year.
And in this case, it was a simple summarization task.
We trained a language model to kind of like to say things that are wrong with the summary.
And there's other things you could do.
For example, you could give people chat GPT and ask them,
okay, use chat GPT to help you evaluate.
And then you could ask for a critique or you could ask for a lot of other things.
You could ask for an explanation.
You can ask for fact checking or a quote or, you know, whatever the model,
like chat GPT can actually reliably help you with.
And so the idea would be that, you know, like using AI assistance,
you can kind of get all the smarts of the AI house and leverage that in order to figure out
how you should evaluate what this system is doing and like,
whether it's aligned with your preferences or whether it's trying to deceive you.
And the big problem with this is how do we know whether it's working?
And one of the kind of like difficulties is that by assumption,
we're kind of dealing with a hard task where it's difficult to evaluate.
And we also want the task to be real because we don't want to, you know,
we don't want to solve them or tight house that doesn't matter.
And so it becomes different.
So you need like a hard task that is real.
But also if you don't, if you have those, you usually don't have ground to it.
So you don't know which was the right answer and how do you know whether assistance is working
or it's biasing everyone to say the same thing.
And so there's a simple technique that we use in the critique to do this.
We're like that we call a target of perturbations.
And so what you do is you have a bunch of prompts.
So this could be like whatever people type into chat.
And then you kind of like take the response that you have and say like,
this is the correct response.
It doesn't actually have to be correct.
But let's just assume it is.
And now you're asking a human to introduce some kind of subtle flaw that is hurt,
like easy to miss, but is an important flaw.
And now what you have is you have this peer data set of like a good response and a bad response.
And you know which one is good and bad because you made it worse.
And so, you know, in a way that gives you ground to it.
And so what you now can do is you can take like randomly select one,
either the correct or the flawed response and then show it to either a human or a human with assistance.
And then they have to say whether it was the correct or the flawed one or like how good the response is.
And in this setting, we can kind of like figure like just try a lot of different assistance or scalable oversight techniques
and then say, you know, like, is this actually helping humans find the flaws that we punted more likely than not.
And if it is, then, you know, like you're actually really helping.
And so we did this and the critiques paper.
So this is training the language models to write critiques.
And for summaries, and what we can show is that when we are assisting humans with critiques at the evaluation,
they actually find 50% more flaws than they did with that.
And so this is kind of like real signs of life that you can already use in models that we can have today to help humans evaluate
and like find problems they would have missed otherwise.
Of course, we still have to do this, like on a much harder task and like with like a real task in a sense,
and we also want to have like bigger effect size, but I think it's just like, it shows that there's promise of these techniques already working.
In the long run, what I think we want to get to is we kind of want to leverage AI for all the cognitive labor that goes into evaluating
whatever RAS is into doing.
And this could be, you know, like reading everything that's relevant or fact-checking or doing calculations or like writing code or any of these things.
And then humans should focus on like their preference input, like the things figuring out what they actually care about and what they want the model to do.
And this way, we can kind of like leverage, you know, like the abilities that, you know, the AI players will bring to the table
and the things that they will be better at than us eventually and then kind of like use them to help communicate the thing that we actually care about and, you know, the things that we actually want them to do.
And yeah, that's it.
But yeah, those are like the main sites. I'm happy to take more questions.
Yes.
I was wondering about this hallucination of responses. Have you ever tried to consider some notion of uncertainty in the answers?
Yes.
So, assembling is difficult because either you're like training and applying to an ensemble from the same pre-trained model and so you don't get that much variance in your ensemble.
Or you have pre-training a bunch of different models and now you're spending a lot of money on pre-training.
One thing, I mean, it seems like it should be a solvable problem to just teach the model to say it's uncertain when it's actually uncertain.
And there's been a bunch of research in that direction, but I think right now it's still like we're not really in a good shape.
It's more stuff to do.
Yeah.
Do you think we may run into a kind of signals and noise ratio problem when it comes to AI suggested critiques to AI answers?
Because sure, like when AI is trying to point out potential problems with text, humans are more likely to report more problems.
But what if it's noticing problems that humans wouldn't have necessarily had a problem with to begin with?
Yeah, so we did try to control for that a little bit by having humans rate the severity of their flaws and whether they would have noticed them otherwise.
They can still see a significant effect.
But also, I mean, a lot of the time the model is nitpicking and then those are not the interesting cases.
Yeah. Also, if you look at the example I showed, which I think is from the blog post, a lot of the critiques are just actually quite garbage.
And one of the things that makes it easy for critiques is it's okay for most of them are garbage because the human can just read them and discard them.
And it kind of like more helps the evaluator know where to focus on or like, notice, like think of something they would have missed otherwise.
So it's more like, you know, the critiques help you brainstorm how you should evaluate or something.
But if you're kind of like using an assistant, you probably want more reliability than like filling most of the answers away.
Yeah. Yes.
How do we ensure that the evaluation metrics we are using in your because it's more modeling approach like detect deception and like left turn or something don't have like major discontinuity.
Yeah, I think.
Well, it depends a lot. What kind of community you're talking about like, if, you know, you get overnight like a model that is let's say 1000 inches larger on like number of parameters or like equivalently better.
That can like create quite a step up and that's makes it quite difficult to do this kind of evaluation.
So in that sense, I think it's going to be very important to like scale up AI continued more continuously and like do more incremental steps.
Having said that, I think the basic picture with request reward modeling is that you're training the systems to help you evaluate, you know, systems that train from or find you from the same system.
Right. And so if you can figure out how to get like finding them in the way that they mentioned everything that is actually useful and relevant, then it would still be able to evaluate systems.
Even though they're much smarter than anything you've evaluated before.
Maybe let me make us more concrete because there is I think a way that you could measure that or like one way we've tried it was using what we call the discriminator critique app.
So to measure that you're training two different or you find two different models from the same future model.
One is the discriminator just says like, is this the flawed response or the correct response or more correct response.
And the other one is a critique model that tells you what the flaw was.
And now you can measure how often this that the discriminator get it correctly and the discriminator is just like your graduate descent.
It doesn't have to want to tell you anything.
You just basically hooking into the models of presentations and trying to like get all all the relevant it related insight it has to this prediction pass.
And then on the other hand, you have this critique model or this general assistance model, and you're measuring how often does the assistant actually help me point to the right flaw.
And then if you compare these two scores, you can like look at the gap between them.
Hence, this generated discriminator critique app.
And then if the gap is large, then the model knows a lot about the tasks.
It's not telling you.
And so that would be very worrying.
And the gap is small.
Then, um, you know, there's much, presumably there's like fewer things that the model is like not telling you.
But I guess there's like lots of value like ensuring this was robust and like testing you don't burn.
That's right.
That's why we want to test it on current models.
Yes, I don't know who was first.
I think someone in the back.
I, um, so I wanted to ask about, like, maybe for the end of the slide, we're like, there's like humans shot was like.
And so, um, you know, I've been helping this like part of that also is like communicating what you want the air to do, right?
Not just like evaluating, but like communicating.
Perhaps like, I would like you to do this and maybe maybe I can't do that.
Um, and so like, at least like in my personal experience using the chat GPT, like there were some things that could do that.
Like surprising.
But it was like, oops.
Related to the channel.
For instance, and I was like, oh, like how did that work?
Um, you know, like you can ask about like, it's like, there's like different things, right?
Or I'm like, okay, like, what can I ask for?
And like, one thing that I thought, um, was a bit concerning was just this idea that like, you know, people don't always communicate their preferences.
Like, honestly, or like, like there could be like, uh, like coordinated efforts, right?
To like, like instill words for like specific capabilities, you know, like coordinated efforts to do such a thing.
But one idea, like I had this was like, I tried to ask if that has like some idea of like the media for itself.
Like, I don't, I didn't know how to use it at first.
So I just thought that maybe there's a rainbow.
There didn't seem to be one.
But like, I was hoping there was one, there was one for like, she can team three, right?
Like, I think Brooklyn wrote like a little, it's awesome.
Um, and so my question is like, um, how do you like, make that sort of like, things safe, right?
Like, have you like, right, nice, coordinated efforts to like, you know, like specifically with word certain kinds of behavior?
Um, maybe you like, some group of opportunity decides that they would like to, um, you know, give it some capability.
So this is a, you know, like, yeah, this is a really good question.
And like, in a way, I mean, the first obvious thing that you shouldn't do is like, you shouldn't just like literally turn on the data that people give who like using interface.
And we've kind of like seen other examples of what happens.
We do that if you think of like Microsoft or something that can go pretty wrong.
Um, the other thing is, I mean, right now what we're doing is like, we're hiring a bunch of people and then ask them to rate different model responses.
But also now the question becomes like, you know, who will be hiring and like, what's their background?
What are they trying to do?
And so, and in particular, like the thing I think we're doing quite poorly right now is like actually like importing like a diverse and representative set of human preferences.
And it's more just like, you know, whoever we end up, we can hire.
And so I kind of wish there was also this like more targeted research on like how we should do that and how that could be done well.
And some of it is also like, you know, better placed outside of like big tech companies, because if you are like tech companies always have an incentive to like, you know, import human preferences in a way that maybe is not like the thing that we actually
humanity would do under reflection or something. And so I think it's a really big important question.
There's a slight follow up like data contamination is like the dual problem for this. Like, do you think internet might be contaminated?
Probably is. Yeah, I mean, people might can anyone can poison the free training, right? Just put something on the internet.
And it's, you know, something that we have to be very mindful of. Yes, I don't know.
Considering that we're currently training these models, hopefully gain a closer to this point.
As human practices change, we've seen like quite drastic.
Is there something we can make sure that the model is keeping up with data?
Yeah, I mean, the most obvious thing is it's like, the model's knowledge base is kind of like the pre training cut out date, like somebody, you know, whatever data you went into pre training.
It doesn't know about like a lot of things that happened after that.
And in terms of updating kind of like human preferences or the, you know, like the comparisons that go into the world model, you just collect more data and we train the functioning run is like comparatively cheap so you can do that again.
I think what gets harder is that, you know, like, as you've deployed the model and people started using it for all kinds of, you know, tasks that they want to build the company around.
Like they, if you update and you change the model, then they also have to do a bunch of work into like adapting their prompts to whatever they're doing.
And so it doesn't come at the zero cost.
Yes.
Sorry, you.
So on the note of exceeding human performance, but what if the advantage of the three is that it has a immense purpose because the internet, if you want to specialize in the domain, like chemistry or material science or something, in particular to generate new contacts.
Then to be adapted, like to use less data, still learn as efficiently.
You mean like less data on like the chemical domain or something.
Yeah.
Yeah.
And you can throw that into pre-training and then the model knows about it.
Yeah.
I mean, that's kind of the general idea with what you intend to do with fine-tuning.
And to some extent, we've seen it like generalized in this way.
For example, instruct tbt was trained almost entirely on English language feedback and demonstrations and works in other languages.
And so that's kind of wild.
And so similarly, you know, you could train the model with people who don't know anything about chemistry and then, you know, it learns to follow instructions and it will do so.
Like on the topic of chemistry.
And this fine-tuning can be very sample efficient.
Like with 100 data points, you can actually make a meaningful change in the model behavior.
So it can be quite effective.
I'm going to pick someone who hasn't asked.
Yes.
Regarding response generation, do you or how much effort do you put on for the emphasis in training on different expression styles?
So what I've noticed from tbt that it always needs to make very structured or scientifically structured answers.
Do you consider in the training, if it turns you like scientifically structured answers or rather an asterisk answer?
Yeah.
I mean, the tricky thing is ideally the model should give you the kind of answer that you want to have, right?
And some people prefer a more scientific or technical answer.
Some people might prefer a more generic answer.
And I mean, right now, like chat tbt doesn't have like, you know, a way for you to set like your specific preferences.
And that's something that, you know, would be really exciting to have.
But also, I think the kind of statistic, probably that you've observed are is in fact, like probably a product of our lab level.
And so a lot of the chat tbt workers were like more, you know, like, I think more like computer sciencey and like more, there was like more data generated by programmers.
Compared to instruct tbt, which was more like generalist labors.
And, and yeah, there's like different, it's like kind of changes also the style.
Yeah, I mean, we should make a distinguished effort.
I mean, it should give you like the style that you want, right?
Yes.
So one of the things that I'm thinking about is how is going to play back in the education.
And so you go back to the graph of the AI progress and the human level.
Yeah, what can you evaluate when I'm sorry to think about it's like, over break, I have to use this or show them like my 10 year old cousin had her chat tbt just to mess around with.
And that green line is a lot lower.
Furthermore, if it just becomes part of their educational experience, it's going to be much right.
I perceive it to be more difficult for them to discriminate even simpler tasks.
And so I'm already thinking about like how that might disrupt or make this a little bit more difficult.
So as you have people who are more, they have to take for instance, what Jackson DC, Jackson DC says as like a given truth anyway.
Yep.
I was just wondering what your thoughts are.
I mean, there's a real risk of overlying on a text that is immature and that is not ready for, you know, you just believing like please don't believe everything the model says right.
But also, I think one thing that I'm hopeful for is that like, you know, your cousin will end up like figuring out how to do this.
Where like, you know, they grew up with, you know, like all of these AI tools that are getting better and learning how to actually leverage them.
Productively, right. And like, it's kind of like, you know, 20 years ago or something when you were like, you know, like using Google search much earlier than everyone else, you're probably going to get better at like using that as a tool for everything you want to do.
Okay, I think you had your hand up for a while.
Oh wait, the last one.
Yeah, so right now, it seems like you guys are using humans as biological sensors to the real world to like physical ground truth and using language as like compressed interface to that ground truth.
Are you guys also looking at using accessor technology directly with your models to give a more truthful answer of, you know,
Yeah.
I mean, it depends on what that sense you could be right like, I guess like one of the most straightforward things is you could ask the model to browse and then they can like fact check its own answers and they can, you know, like, import external knowledge that I didn't remember.
And yeah, I think that would be quite useful. I think that also be quite useful for assisting human evaluation.
And you can look at web GBT, which, you know, is a published work on using the model for browsing.
I think so one thing that makes it harder when you're using this like external sensors or if you're letting the model interact more directly with the real world is that it raises more safety questions right.
If you let your language model make arbitrary API calls, then you have to be a lot more careful with which calls there's a lot to make and which is it not.
And if you're as opposed to if you just like you're viewing everything the model says that you can decide which ones you want to make.
So yeah, it's an open prop.
Okay, one more question.
I think you didn't.
I've seen a lot of language models. I've seen a lot of people talk about how is any like a fixed amount of compute for token.
While a human they have system one system two where we can like just speak quickly versus actually using reasoning and things that are accurate.
And then I've seen other words to try to like kind of use the force it to a chain of problems or the chain of reasoning or like let's think step by step.
I think that's that is sufficient to do everything that's the level that we want or require real big fine tuning or architectural changes.
I don't know.
I'm also the wrong person to ask.
I'm mostly not trying to get the models to have new capabilities and more like, you know, getting them to play on team human.
I want to do the online questions.
Yeah.
So what do you think is the, is there a role for often out of being clean from the feedback especially get like human, like you don't have to change that box.
It's hard to model human impressions.
Do you think like this is what people are more often out of.
Yeah, quite possibly.
Um,
I mean, yeah, as you point out, right, like there is a lot of conversational data and if you can use it that that would be a should should be useful.
And I think broadly can categorize this kind of thing as like let's make the algorithm better and like our feedback.
And I think that's valuable and that should help us like, make the same pre chain models, like more aligned according to the human preference that we collected.
Um, but also, you would still run into like all the limitations that our LHF has.
Great.
Thanks a lot.
I think there's a fine tuning API for GP3.
I don't think it offers at all right now.
Supervised fine tuning.
So you could do like, you can like this still best of them and do this kind of expert iteration or all.
So the first question is, could you more clearly describe the training process?
For example, starting with the next agency, 001, then XTV offer them the advice steps are high left.
Sorry, I didn't, I didn't catch that.
Start with texaventure 001.
And then so how much use of data do you need and how many steps are that which kind of things.
Um, I think the exact numbers are not public.
It's basically similar to instruct to BT and for the instruct to BT numbers.
Um, we had, I think around 50,000 comparisons and probably like 10,000 demonstrations or like maybe 10,000.
I don't remember the exact number.
Um, so I had like this other slide with, uh, yeah, I was like about 20,000 hours of human feedback.
What I calculated.
And you think it's also human feedback because that's you can get like one million or whatever.
Right.
I mean, the big question is like, how do you make, how do you ensure quality?
Um, yeah.
Sometimes the model that can create that out of your banking.
But it's the whole problem.
I like that.
As soon as you already have the one model that you trust.
So.
Okay.
Sure.
So next question.
Yeah.
So.
I mean.
The kind of like ambition of that plan is to train a model that can do this kind of conceptual
research.
And.
You know, you can picture is it like a language model that like, you know, writes an alignment
research paper that we know, like we read and then we're like, Oh, this is a really cool
idea.
We should try this.
Um, and I think, you know, going back to evaluation is easier in generation.
I think it also applies to alignment research.
And like, I think at the very least, like I find it much easier to evaluate, you know,
alignment research, then I find it to, to like produce it.
And so while there might be conceptual breaches that we need that we couldn't even evaluate
right now, because they're just like, you know, if we saw them, we'd be like, what is this?
Um, and this is kind of like part like, this is like the reason why we want to do scalable
oversight, right?
Because, you know, like if, you know, the language model produces this really brilliant insight
and we can't even recognize it at the time.
We should be able to have an easier time recognizing it if we use the assistance.
And if we leverage like our best AI models to like figure out whether or not that was
a good idea, what is the weaknesses and what are the strengths and like, you know, what
kind of experiments should we run to know whether this is a good idea.
And so yeah, I think basically, you know, the story of just using our HF to train a model
to do good alignment research.
You have the obvious pitfalls, which is, you know, the model might write like an alignment
proposal that kind of looks good to us, but it's actually, you know, not a good proposal
and it creates the AI that is misaligned with humans.
And so in order to distinguish the two, which might be really hard, maybe it's not, but
you know, I think we should expect it to be really hard and then leveraging AI assistance
to evaluate that seems like a really promising plan.
But isn't it about like the API?
I mean, that was my whole point.
It's not suppression.
But do you think like, do you need like something more?
Or do you think what we've got company is trying to do?
Do you think like, if you can get a lot of feedback later, will that be sufficient to get
a very good generation?
Or are you seeing more in terms of?
I mean, the journalists were like, I think the basically the vast majority of the model's
capabilities and like all the cool things you see it do come from pre-training and not
from the fine-tuning stage.
The reason why people sometimes attributed to the fine-tuning stage is that you didn't
see it in the pre-trained model.
And the reason I think the reason that we didn't see it in the pre-trained model is
because the pre-trained model was so misaligned, it was not trying to help you and it was not
trying to show you all the cool things it can do.
And instead, it just reverberates and takes a bunch of random webtext.
And that's not what you're looking for.
And so, yeah, I think that what I really basically have been doing is like unlocking capabilities
that were already in the model and making those available for humans to use.
And in some ways, like, you know, alignment research is very dual use in the sense that,
you know, A, if you have really good alignment techniques, you can use it to align with whatever
values you want, including values that, you know, we wouldn't particularly endorse.
And B, it also, like, if you're doing alignment right, it will always look a little bit like
you're made the AI system more capable because before, it just wasn't really trying that hard
to help you.
And now you've made it more aligned.
So, you know, you actually see these capabilities that you had.
Sure.
Yeah, so that was what I was talking about here, right?
Like, this is like the whole problem that we have where what humans about can evaluate
is constant.
So, you know, we want to be able to evaluate like sophisticated attempts at deceiving us.
And that's why we want to do scalable supervision so that we empower humans to spot these attempts
at deception.
Probably a platform training in order to survive instead of who will talk to us and use
distribution ships, you know, what will be changing because of other systems.
Yeah, so I think these are real worries.
And to some extent, we kind of like have to test empirically like how difficult and how
severe they actually are.
I think so my personal stance right now is something like I think trying to get the auto
alignment signal really right is going to be like 90% of the effort.
And once we have that, then a lot of the other things might also fall into place.
So, for example, I mean, it kind of depends on which story of inner misalignment you're
worried about.
But, you know, one story is you kind of training your system and it learns how to do, it learns
basically a bunch of, you know, optimizes kind of like meta reinforcement learning.
So, for example, like GPT three can do like in context learning.
And that's like a kind of, you know, land optimizer.
And so now you're like doing all the Jeff training or whatever like alignment training
you have.
And you're like the land optimizer is learned to do the thing that you want on distribution.
But now if you have a distributional shift, and this distributional shift could be auto
induced, meaning like the model is causing it itself.
And now you're going out of distribution, all these inner optimizers like try to optimize
for something else.
And one way you can like, and you know, like how much that would actually happen practice
kind of unclear, but one kind of like more important question is like, if you have a
really reliable outer alignment signal, and you have this like general training signal
that you trust, you can also use that, you know, on the new distribution to train the
system to be more, or like to get this inner optimizes in a row, basically.
And so then you've reduced like the inner alignment problems to like, how do you deal
with a distributional shift and how do you like construct an outer alignment signal that
you trust.
And those are problems that we have to deal with anyways.
But yeah, I don't know how it's actually going to shake out, but there's some important
open questions.
So regarding alignment, one of the kind of problems that we're encountering is, see,
there's not much interest.
It seems like explaining like these different sort of like, there's not even much interest
in the AI, or even closing these models is like explaining why this infeed and second
strings are for sure.
I mean, that's definitely the truth around it.
As to why is making these online adjustments, have you all been able to carry it?
I mean, I think where we are right now is like, pretty disactive, satisfactory.
I mean, you can ask the model why it gave a certain response, but you don't know whether
it's answering truthfully.
And you can also, I mean, another thing you can do is you can give the model its own
response and ask it to find out flaws, which is what we did in the leaks paper.
But, you know, I think the like, I mean, there's one version where you try to make that better,
but then the question is like, what is your ground signal?
I think a like, better angle of attack is probably interpretability, where, you know,
like, you figure out how to look inside the model and then how it actually works.
That's what I'm asking about.
But the level of research of interpretability is like, it seems that that's really
what you're talking about.
Yeah, I mean, we are working on that problem, but I don't think we have anything that is
like to show right now.
So it seems generally not to be a very easy problem.
But, you know, I'm hopeful that we can do some things.
I think in general, the problem of interpretability or like using interpretability for alignment
is kind of tricky because I suspect it's going to be neither, it's going to be not sufficient
and it might not be necessary.
So any amount of interpretability you can leverage would be useful because it's another tool
in your toolbox of like detecting deception or like knowing, you know, what you said,
like how, why the model gave a certain answer and made a certain decision.
But, you know, it is kind of unclear if you really get really good at interpretability,
how you then leverage that for alignment.
Presumably you could look in the model and just like throw all the models out that you
can find in misalignment in.
But then aren't you just selecting for models that have misalignments that are really hard
to find with the interpretability force.
True.
Just to follow up on that.
Three of my ask about that is kind of the standard practice of the capital zero is that
you have to find an explanation about the problem.
Yeah.
And I guess in my question, why would you take the interpretability to not be necessary?
Yes.
So why would it not be necessary?
So again, this is kind of like an open question, but basically what stands you could take is
that at the end of the day, what really is going to matter is the decisions that the
model actually takes and not the reasons why it took them.
And so if you can get to the point where you're confident that all the things the model actually
does are aligned with what you want, then does it still matter what the model thinks in
terms of what the model thinks internally?
I don't know.
You're fine.
You're fine.
You're fine.
Yeah.
That's that's what we're trying to do.
Right.
Like we're trying to make like a really, really good evaluation signal.
And then you can select for, you know, you can train the model to do the things that you
wanted to do because you can always evaluate better than the model can do stuff.
Yeah.
I think that's a good question.
But yeah, thanks so much for the great lecture.
Very interesting.
I might actually do like the talk of type GDP.
I might just do like the life on this like application thing.
Yeah.
Sure.
Just the end of the class.
Sorry.
So how do you think this thing?
Okay.
Thank you.
Thank you.

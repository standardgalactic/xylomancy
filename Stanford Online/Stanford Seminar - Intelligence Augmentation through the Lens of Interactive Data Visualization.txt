Just such a treat to be back, I spend many hours on that side of the room, so it's wild
to be on this side of the room and going, whoa, there was actually like monitors up here,
like that's how the speakers kept track of where in their talk they were, so that's good
to know.
And this is sort of the first set of talks I've given since the pandemic, and so I thought
it was a really great opportunity to talk about some new ideas that have been on my
mind, and particularly with all of you as my captive audience, I thought that I would
use this talk as an opportunity to think out loud about what the role of HCI should be
in the face of all of this really incredible rapid progress that AI and ML have made, particularly
kind of scoped in the last six months or so.
And sort of as I was trying to think about what the role of HCI should be, I was reminded
of this figure from Jonathan Gruden's 2009 article in AAAI about how AI and HCI are
two fields that are divided by a common focus.
And as you can see in moments where AI makes a lot of progress, it's almost like the pendulum
swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches
of human intelligence augmentation or amplification, but also I think HCI is in a more established
stronger position than it's ever been in the past.
And so I really think it's our responsibility to think about what that counterbalance to
ever-increased automation should be.
And so I often, in moments like this, like to sort of turn to history and ground myself.
And so if we cast back to the first AI winter with Sutherland's sketch pad, right around
that time there was this foundational paper written by Lick Leiter at MIT titled Man
Computers Symbiosis.
And I think the gendering is unfortunate and unfortunately reflective of the times, but
nevertheless in this paper, Lick Leiter put forth this really compelling vision about the ways in
which a computer could interact with us through this intuitive, guided trial and error procedure
turning up solutions and revealing unexpected turns in the reasoning.
And I was really tempted to put this sort of side-by-side with this very recent demo that
OpenAI released with ChadGPT plus plugins where you can upload this music.csv data set
and then start to have this very natural language interaction to ask what are the columns in
the data set, how many rows in there are in the data set, and then even say, can you give
me some basic visualizations of this data set?
And it thinks a little bit, it's working real hard, and there you go.
Like it produces sort of three visualizations and even starts to give you maybe something
that looks like an explanation, and I wonder, is it time to roll out our mission accomplished
banners?
Like have we achieved Lick Leiter's vision to think in interaction with a computer in
the same way that we think with a colleague whose competence supplements our own?
Now I don't think it's time to roll out the mission accomplished banners, but I'm hopeful
that the reason it's not that is not just my sort of hope that we haven't been put out
of jobs, but rather that there is something more to do.
So two years after Lick Leiter's man computer symbiosis, Douglas Engelbart wrote up this
really incredible framework called augmenting human intellect.
And right in the introduction of this piece, we already start to see how Engelbart is defining
a much more expansive role of human augmentation.
So the idea is not just about problem solving, which he does mention right at the end there,
to derive solutions to a problem, but it's also about using computers to help us think.
It's to increase our capacity to approach a complex problem situation, to gain comprehension
really about this thinking and not just the problem solving pieces.
And really what I like is how he thinks we'll get there.
Certainly there will be sophisticated methods, high powered electronic aids, but to me the
part that really resonates in his prescription here is streamlined terminology and notation.
And that's going to be a theme of my talk here, certainly one of the themes that underlies
my group's work.
And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work
with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the
top right-hand corner, on this system called B2.
So this is a Jupyter notebook, it's a very commonly used data science environment where
people can start to write code in the style of a Python REPL.
But what B2 does is saying, well, in addition to that sort of linear style of data science
analysis and programming, there's a lot of value in a more visual analysis dashboard
style interface like Tableau.
And so what B2 tries to do is bring these two pieces together.
So you can see once I've invoked B2, it adds this on the sidebar.
And I can start to issue regular sort of Python, you know, pandas commands like looking at
the data frame, getting a sense of how many rows there are, what the columns are.
And now I can start to write some code to do a little bit of data transformation and
visualization.
Notice here in all of these steps, you know, when I'm authoring a visualization, I don't
have to specify what that visualization should look like, right?
I'm just calling these .viz methods on the data frame.
And B2 behind the scenes is figuring out what sort of visualization actually makes sense
based on the history of the transformations that were performed on the data frame.
So in the case of year, for instance, if I've grouped by year, the most sensible visualization
to produce is a histogram of the number of counts of data records across years.
You might have also noticed in the video that if I click the fields on the right-hand side
there, that it automatically produces an equivalent visualization, but it doesn't stop there.
It adds, you know, the code and tags them with these little, you know, yellow emojis
to indicate that there's actually sort of a common shared representation here, right?
Clicking on the sidebar not only produces the visualization, but produces the equivalent
code as well.
And what's interesting is that these visualizations aren't just output mechanisms, but I can start
to interact with them to sort of do this cross-filtering interaction.
So all the other bars update to reflect the data shown in the highlighted bars.
And B2 is keeping this as an interaction log that is semantically meaningful to me.
So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,
but it's expressing data queries, right?
Which states have been selected, and I can use that data query to perform subsequent sort
of analyses based on my interactive results.
So I can say, great, I'm gonna, you know, copy some code to the clipboard, paste it in
as a data query to look at what the interactive selection should be, and then, you know, proceed
with some other sort of visual analysis.
And so as we're sort of, you know, looking at these two forms of interaction, I was trying
to figure out, well, some things feel the same, right?
I've got that kind of conversational back and forth.
Sure, on the left-hand side, which had GPT, it's a more natural language conversation.
On the right-hand side, it's more of a REPL conversation.
But also, things feel qualitatively different.
And how do I actually kind of characterize what is the same and what is the difference?
And I thought really hard about it, and I realized that actually maybe what still matters
is direct manipulation, right?
And by direct manipulation, I don't mean just the sort of Ben Schneiderman version
of the term, which is, you know, associated with graphical user interfaces and having
a representation on screen that you can manipulate and undo, redo and things like that.
But what I mean here is the deeper treatment of direct manipulation that three cognitive
scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about the mid-1980s.
So in particular, in Hutchins et al's treatment of direct manipulation, they sort of, you
know, imagine direct manipulation to be this cognitive process between a user's goals
and the user interface.
And you know, they identify this gulf of execution that exists when a user has to translate
their goals into commands that they execute on the user interface.
And similarly, a return gulf of evaluation when a user has to figure out, well, did the
UI do the thing that I was expecting it to do?
Right?
I'm seeing a lot of nods in the audience because, you know, if you've had experience in user
interaction design, user experience, you've maybe experienced these terms, gulf of evaluation
and execution.
But what I find interesting in this 1985 paper is that they went one level deeper.
So in particular, they identified this idea of a semantic distance, which is basically
how users take the fuzzy notions in their head and translate those into the nouns and
verbs of the user interface, right?
So going, doing that sort of sense meaning operation of transforming your intentions
into the particular actions that might exist in the user interface.
And in addition to the semantic distance, they identified what I love.
I love this term, an articulatory distance, right?
So it's not necessarily the meaning that we care about anymore, but the way in which we're
conveying that meaning through the UI.
And this is particularly important because you might have several user interfaces that
all express the same semantics, right?
You can conduct the same set of, you know, operations with them, the same nouns and verbs.
But the way you do that might be different because one interface might be graphical,
the other one might be textual, another one might be conversational, gesture-oriented,
et cetera.
And their claim in this paper was that that articulation, the form of that meaning is really,
really important, just as important as the semantics.
And of course, these distances exist on the Gulf of evaluation as well.
So the articulatory distance is how do I perceive the changes that occurred in the UI and start
to bring meaning to that, that perceptual, that perceptual operation by interpreting
and evaluating the degree to which they met my goals.
So this is actually going to give us the kind of conceptual machinery for the rest of the
talk.
And it's a little bit dense.
So I want to return to sort of the prior two examples and think about how they manifest
these two kinds of distances.
So in the case of the chat GPT example, you know, if we start with semantic distance,
I would say that, well, the semantics aren't really well-defined, right?
They're not really explicit because what these models have done is they've learned over,
you know, vast corpuses of text, often just text that is present on the internet.
And so what they've learned is this latent space that is very ambiguous in the semantics
that are encoded in that latent space.
So as a user, it's hard for me to know how to translate my intentions into something that
the system can understand because I don't know what it is the system knows about the
world.
But as I'm sure many of us are aware, like prompt engineering is a thing, right?
So if I figure out exactly how to craft my, you know, natural language expression, suddenly
I can get the model to very rapidly, almost zero shot, adopt the semantics that I want.
And that feels like a very powerful, you know, affordance that we've not necessarily had
before.
On the other side, you know, the semantic distance in the Jupyter Notebook in B2 had explicitly
defined semantics, right?
We have the explicit semantics of pandas on the data frame of the visualization library
of being able to click on the fields in a graphical user interface to produce visualizations.
And every time I did that, I had the shared representation of the code.
So either I would offer the code and it would produce a visualization or if the system produced
some code, I could go in and comment and uncomment entries or tweak the code in a particular
way and things like that.
And so it gave me the shared representation that allowed me to bridge between input and
output mechanisms really, really easily.
With articulatory distance in chat GPT, right, natural language, it's been enormously powerful
because it's reduced the sort of learning threshold for a lot of things, right?
So if I don't know exactly what it is I want or how to sort of pose it to the question,
I can lean into the ambiguity of natural language and chat GPT catches up to my intentions pretty
rapidly, which is great.
But conversely, sometimes I know exactly what it is I want and it's really frustrating to
have to express precise operations through the ambiguity of natural language.
And then as a result, because of the fact that natural language is the only mechanism
so far by which we can interact with many of these models, there's a disconnect if your
output is visual, like the case of visualization.
So I can't interact with the visualizations in any way to do subsequent back and forth
interactions with the model.
Now I don't think the second point is sort of a fundamental limitation, but it's certainly
the state of where we are today.
And on the other hand, with Jupyter Notebook and B2, with the articulatory distance, we've
got basically the inverse of this, right?
We've got a nice, precise, programmatic syntax.
So if I know that syntax, I can work really, really efficiently, right?
Sort of a common affordance of many sort of command line style interfaces.
But I really need to learn that syntax to be effective.
And in some cases with pearly design syntaxes, which I might maybe argue, Pandas is an example
of, right?
I constantly have to look up the documentation for, right?
There's a learning curve associated with it that slows people down.
Yeah, Michael.
Just a quick question.
I usually thought of the learning curve of a formal language as a semantic, just rather
than articulatory.
Rather, the articulatory is like, how hard is it to construct a theory once I understand
it?
Yeah.
So the reason I put it, I think this is a great question.
What lies in semantic and articulatory?
And oftentimes it is quite a fuzzy distinction.
The reason I put this in articulatory is my experience with Pandas oftentimes is I know
what it is I want to do, right?
I know the sort of operation I want to perform on my data frame.
I just don't know the specific syntax that I need to look up.
Exactly, exactly.
But certainly, you know, if you don't know what it is you want to do, then the affordances
of natural language absolutely help because you can kind of, you know, pose things in
really fuzzy ways and kind of iterate towards your outcome.
And I think you see some of this ambiguity in sort of, you know, the distinction between
semantic and articulatory distance here with this last point where because there are consistent
semantics that actually has this knock-on effect on the articulation because now there's
a shared representation of input and output and that simplifies that articulatory distance
as well.
So there's not quite that disconnect that we see on the chat GBT side.
And so, you know, that's, you know, I found semantic and articulatory distances to be a
really helpful sort of framework and I wanted to use it to sort of analyze the very last
step in the output that that demo produced.
So it, you know, it's basically this thing that masquerades as an explanation of the
visualizations that chat GPT produced.
But if you actually look at what it says, right, here's some basic visualizations.
Number one, a histogram of song durations, colon.
This shows the distribution of song durations in seconds.
All right, fair enough.
Scatter plot of song hotness versus artist familiarity.
This shows the relationship between song hotness and artist familiarity.
Well, I would hope so.
And then bar chart of the top 10 most frequent artist names.
This shows the top 10 most frequent artist names in the data set, right?
These are not really explanations, but they're pretty provocative or evocative in the potential
that these models might have in allowing us to produce these textual descriptions of visual
artifacts.
And certainly, you know, a lot of people, certainly lots of big tech companies have
thought about the ways in which you could use all kinds of machine learning models, not
just LLMs, to do this sort of rich description of visual content.
And particularly for these sort of accessibility use cases, like how do you describe these
kinds of artifacts to people who are blind or have low vision?
And lots of people have studied the degree to which these models are effective and found
maybe unsurprisingly that they're not terribly effective right now, right?
So here is a quote from a participant from one of our studies who says, you know, the
reader wouldn't get much insight from texts like this, which not only, you know, is problematic
because it doesn't effectively convey information, but more troublingly, it actually increases
the burden that readers face when they're trying to make sense of this output, right?
There's a lot of sort of noise that gets added to that experience.
Another participant, you know, says very, very interestingly, the problem with these textual
descriptions is also that it robs me of control of consuming the data, right?
Another participant said, I want to have the time and space to interpret the numbers for
myself before I read any kind of textual description that does the analysis for me.
And so to me, these sound very similar to issues associated with a semantic and an articulatory
distance, right?
That first quote talking about, well, these texts aren't conveying anything interesting.
The second set talking about, well, I want to have that time and space, I want to be
able to control the form with which that text is conveyed to me.
And so I want to dig into how we might address these two distances in the case of accessibility.
But before I do that, I want to give us a sense of how people who are blind or have
low vision experience, you know, the internet and graphical interfaces today.
So I'm going to turn things over to my PhD student, Jonathan Zhang, who will give us
a quick demo of an assistive technology called a screen reader that basically narrates on-screen
content.
Selected by the cursor will be read out as text to speech.
So here I can demonstrate what the accessible HTML version of our paper looks like to a
screen reader.
So as you can see, what a screen reader does is it basically sort of linearizes the operation
of reading, perceiving, understanding graphical content on a user interface.
And in particular, you might notice that the narration was actually quite rapid, right?
And this is actually a slowed down version of what proficient screen reader users use,
which is often much, much faster.
But what is interesting about the screen reader use case is that it forces that linearity,
and the key challenge in figuring out the articulatory distance in the case of accessibility
is how do you take visualizations that probably all of us in the audience have slightly subtly
different ways of reading, right?
Maybe some of you start by reading the title, then moving to the axes, then looking at,
you know, the shapes, while others might start by looking at the most salient trend and then
start to, you know, map out to what the axes and legends and stuff like that are.
How do we take all of that rich diversity, but linearize it so that people who use screen
readers can nevertheless have that same, you know, choice in meeting a visualization,
but under these conditions?
And so the way we have chosen to do that is basically by restructuring the content of
a visualization into a text-oriented hierarchy.
So at the top, at the root of this hierarchy is just a summary of the chart, probably the
trends that are shown in the chart, and then the hierarchy branches off into the individual
sort of data fields or the encodings in this case, right?
The x-axis, the y-axis, the legend, and things like that.
And then people can start to drill down in ways that maintain some correspondence with
the visual artifact.
So one step below, you know, the x-axis is stepping through them by the major ticks,
right?
One step below the major ticks would be minor ticks, and then ultimately you would get to
the individual data points.
So let me throw things back to Jonathan to give us a demo of how this works.
A scatterplot of Penguin data.
And to a screen reader, our system represents this scatterplot as a keyboard-navigable data
structure that contains text descriptions at varying levels of detail.
So when a screen reader user first encounters this visualization on a page, they'll be able
to read off a high-level alt-text description of what the chart is.
So a scatterplot showing body mass and flipper length of penguins.
And if they're interested in getting more detail about this visualization, they can
dive in by pressing the down arrow key to descend one level in the hierarchy and access
descriptions about the different encodings of the scatterplot.
So I'm going to press the down arrow key.
Axis titled flipper length, and for linear scale, we value from 170 to 240.
I can press the left and right arrow keys to flip through descriptions of the other axes
and legends.
Y-axis titled body mass G for linear scale, we value from 2,500 to 6,500.
Legend titled species for color with three values, Adelaide, GenSrap, Gen2.
Great view of scatterplot.
Cool.
So let's say I am interested in getting more information about the X-axis.
I can use the left arrow key to navigate back to the X-axis description and then press
down one more time to descend a level of detail into the X-axis.
Legend titled Y-axis titled X-axis titled flipper length, and for the range, 220, 230, 35 data
values in the interval.
So on this level underneath the X-axis, I'm accessing descriptions of intervals along
the X-axis, and it's reading out to me how many data values are contained within each
interval.
So by pressing left and right, I can kind of get a sense of the distribution of data along
the X-axis.
So let's say I am interested in this range from 190 to 200.
I can then press down arrow again to dive into the individual data points that are contained
within this interval.
So let's say that instead of moving up and down this hierarchical structure, I would
rather just move around the XY grid in the scatterplot, as if I were kind of feeling
around a tactile graphic, for example.
I can start by navigating over to the grid view of the scatterplot.
And once I descend into this part of the hierarchy, I can use the WASD keys to move
up and down different squares along the grid.
And so similarly to before, it's starting off by giving me the number of data values
that are contained in that square so that I can get a sense of the distribution of the
data.
And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.
And this was the first time he felt like he actually understood and could build a mental
model of what it was that a scatterplot was representing.
We saw these sorts of comments reflected in user studies that we ran about how the form
of this textual output really influenced participants' mental model of what the data was, what the
trends were, and things like that.
So one participant, for instance, said, you know, I now know how to drill down and up
between different layers in the data to get an overall picture, and it gives me a different
way of thinking.
And another one said, you know, I'm thinking more in spatial terms because this is just
a new method for navigating and moving through the grid and drilling down to information
and things like that.
And so, you know, what I find interesting here is that at every step, right, the semantic
content stayed exactly the same, right, and there wasn't even very rich semantic content.
It was a range and then a count of the data values.
All we manipulated was that articulation, that form, right, giving it hierarchical nature,
setting all of these different navigational affordances, and just manipulating the articulation
had this huge impact on people's mental models of the data.
And I think that we're really sort of just at the tip of the iceberg of these sort of
more accessible structures.
Currently in my group, we're thinking about just, you know, the impact that token order
has on how people using screen readers build up those mental models, right?
If you're constantly prompting them with the range first rather than the actual data values,
does that introduce friction to their capacity to build that mental model and things like
that?
But in all of this, you know, where is semantic distance, right?
Like how do we actually start to make that textual description more interesting and meaningful?
And this is where I think LLMs can really help us, right?
For one reason, it's because there's just a sheer amount of textual content we need to
be able to produce that is infeasible to expect a human to sort of manually author.
But there are other sort of implications that we'll touch upon really shortly.
But before we can get LLMs to actually sort of produce the content we want, what we need
to do is shift from that very latent space with implicit semantics to a set of explicit
semantics, right?
We need to impose a conceptual model onto our LLMs.
Or another way of saying that is we need to get the LLMs to understand what a good textual
description of a visualization is.
And so that's what my then PhD student, Alan Lungard, set out to do.
We ran a crowdsourced study where we got sort of 2,000 descriptions of, you know, charts.
And through qualitative coding, we realized there are basically four kinds of semantic
content that textual descriptions should convey.
The first most primitive layer is basically just the sort of construction details of the
chart, right?
What are the titles, the labels, the scales, the units, et cetera?
And, you know, accessibility best practices say that this is some of the most important
content to convey because it gives people sort of important milestones and landmarks.
One level above that are the sort of statistical properties, like minimum, maximum, you know,
outliers and things like that.
And then one level above that is probably what is cited people we consider the real value
of visualization to be, right?
The perceptual and cognitive characteristics, like complex trends and patterns, things that,
you know, automated statistical methods we typically think of as not being sufficient
at.
And then finally, the fourth and highest level are what journalists often consider to be
the real value of visualization, which is the narration that gets associated with it,
right?
What is the data story that you're able to tell through the visualization?
Can you explain what you're seeing, the causal mechanisms, et cetera, et cetera?
Now another reason I think LLMs are really suited for this sort of semantic bridging
task is because when we asked cited and blind people what their preferences were when it
came to these four layers, four levels, we saw really distinct preferences.
In the case of cited people, right, because we've got our own visual perception doing
that sort of, you know, bridging of the gulf of evaluation, cited people tended to want
higher and higher levels of content being conveyed through text.
And machine readers, on the other hand, were pretty significantly divergent, right?
For many of them, they didn't want those level three and four, particularly the level four
captions at all because they wanted that time and space to do the interpretation for themselves.
And so here, this, you know, visualization to me conveys that, you know, LLMs can help
us or machine learning models can help us think about sort of personalizing the semantics
of a user interface in a way that maybe we haven't had the opportunity to study so far,
right?
There's been a lot of work in personalization, but it's often been at that level of the articulation,
right?
Changing the sizes of buttons and adapting color palettes and things like that.
And there's maybe an opportunity now to use LLMs to actually change what the nouns, the
verbs, the, you know, the concepts of a user interface are much more fundamentally.
And so the way we're going about doing this in the case of textual descriptions is we're
going to be releasing very soon a data set of about, you know, actually now we're over
12,000 pairs of chart captions and we've generated some of these captions and we've crowdsourced
some of these captions.
And we started to train baseline models to do this task and one of the interesting features
here is how do we represent the semantics of a chart to a large language model, right?
One way could just be let's treat the chart as an image, right?
This is just a set of pixels and unsurprisingly the baseline models don't do very well at
that because a chart is a much richer kind of artifact than just an image, right?
It's got all this rich structure.
So then we said, great, let's look at a data table or let's look at a scene graph or which
is just a fancy way of saying the SVG associated with the chart.
And a priori we would have thought, well, the scene graph is maybe like a good in between,
between the computational affordances of data table and capturing some of those perceptual
characteristics.
Turns out for the LLMs we trained that were all transformer models they did equivalently
well on those two representations.
And so one of the things my group is working on right now is a new way of representing visualizations
that more directly encode some of those perceptual operations that are otherwise currently implicit
in a scene graph that grammar of graphics libraries like VegaLite or GGplot perform.
So what's interesting in all of this to me is that through these generative models the
goal has been how do we impose a conceptual model onto them, right?
How do we bring some explicit semantics?
And I think we're just scratching the surface here as well because I think the chart example
case is a really great one where a lot of these representations of charts that we've
got right now, the grammar of graphics for instance, we're designed for people to author.
So we're really good at figuring out how to design programming languages, domain specific
languages, to emphasize the cognitive characteristics that are important for human authors.
Things like the cognitive dimensions of notation that cares about how viscous is the programming
language, how many premature commitments is the programming language in force.
But I don't know what it means to design a representation to be suitable for an LLM to
operate over, right?
Do we restructure the programming language more fundamentally to make it tractable for
an LLM, maybe?
So in addition to generative models, my group has also been working with predictive models.
And here I think the bridging task is really not about imposing a conceptual model but
bridging it or aligning it to the ones that we already have.
And often the way that a lot of this work happens is through the lens of model interpretability.
So here is a very popular set of techniques called saliency maps.
The idea behind saliency maps is they're trying to depict the most important input features
for a particular outcome.
So in this case, you know, this is an image, the label should be toy terrier, and here's
what a variety of different kinds of saliency methods believe to be, you know, the most important
pixels to produce that outcome.
Now I look at these visualizations and I go, well, you know, is it telling me something?
Maybe, right?
And maybe the reason I believe it's telling me something is because I'm the one doing
the perception and interpretive tasks, right?
Like if I look at some of those visualizations on the bottom, I go, oh, like, it looks like
the dog snout is really important to the classification of a toy terrier or the spots.
But it's not actually the saliency method that is doing that interpretation for me.
I'm the one bringing meaning to those lit pixels, right?
And so as a result, if we think about that gulf of evaluation, it's not the saliency
method that's helping bridge that gulf in any way, which is why saliency maps for now
have been these tools that we just use in a very ad hoc way that require a lot of manual
effort to make sense of.
And so a question that my student, Angie Boggast, has been focused on is how do we scaffold
that semantic sense-making operation, right?
Providing some additional structure to help sort of scale it up to make it more reproducible
and things like that.
And what she's developed is these set of metrics that are very analogous to ideas of precision
and recall, but are operating at the level of input features and interpretability.
So in many data sets, you've got some set of ground truth human annotated features.
And what shared interest is looking at is what is the overlap between what a saliency method
considers as being important to the classification and what the human annotators thought was important.
And there's actually three different ways that these overlaps can manifest.
The first is a sort of ground truth coverage, which is very analogous to ideas of recall.
And it's how much of the ground truth does a model incorporate in its prediction or what
is the proportion of the ground truth region that is covered by the saliency region.
And if we look at some examples of low coverage on the top and high coverage at the bottom,
we can see that in the case of low ground truth coverage, there's actually very little
overlap between the ground truth, the yellow region, and the salient region in orange.
But I often find that it's actually the high coverage regions that are more interesting
to analyze.
So if we compare cases where the model was correct on the right with the green label
and cases where the model was incorrect with the red label, we can see in the case of correct
high ground truth coverage, there are instances where the model relies not just on the object,
like in this case with the cab, but a lot of contextual information as well to ultimately
make that correct prediction.
But on the flip side, with the laptop, the model is doing the same thing.
But here, the context is actually throwing it off.
It's actually confusing the model because it's accounting for too much of that context
in its decision making.
Another kind of coverage is something we call saliency coverage, and this is more akin
to precision, which is how strictly is the model relying only on ground truth features
to make its prediction.
And again, if we look at low and high coverage, in the case of low coverage, we can see, again,
see disjoint sorts of sets.
But in the case of the high coverage regions, we can see that in the case of high saliency
coverage, it basically means that the salient regions are a strict subset of the ground
truth coverage.
But the difference between a correct and incorrect prediction is whether that subset was sufficient
to make the correct classification or not.
So in the case of the Maltese dog, it did indeed only need to look at the head to make
that correct prediction.
But in the case of the Dalmatian, it probably should have accounted for more of that dog's
head or some of the other characteristics associated with the dog.
By focusing only on the snout, it ended up sort of arriving at the incorrect sort of
classification.
And finally, the last metric is something that is very familiar, IOU, the intersection
over the union.
This is sort of the strictest shared interest metric.
It's really measuring how aligned the model's behavior is with human reasoning.
So if you look at some examples, again, low coverage at the top, we can see in incorrect
cases totally distinct disjoint sets again.
But in a correct instance, I actually find that pretty interesting, low IOU coverage,
but it got a correct classification.
Now one could say maybe it got lucky.
But potentially what the signal there is is that maybe all the model needs is a tiny bit
of a wheel associated with a horse to make the prediction that is actually a horse card
and not just a horse.
And on the flip side, with high coverage, Newfoundland, great, total, total alignment.
But in this case, incorrect classification, even though there was high coverage, this
might suggest genuinely difficult to classify images, even for people.
Because if I look at that, a pickup truck seems a totally reasonable guess to have made
about the image.
I don't know that I've got enough sort of visual information there to call that a snowplow.
So shared interest basically gives us a mechanism to start to scaffold and structure, bridge
that semantic distance.
People no longer necessarily need to manually start to analyze these things.
And in fact, we analyzed lots of different models across both vision and natural language
and found the different combinations of these shared interest metrics along with figuring
out whether the prediction was correct or not actually surfaced eight kinds of repeating
patterns in model behavior.
So we can see human aligned and some of these others we also looked at earlier, context
confusion, context dependent, and so forth.
And all of these give us sort of semantics that we can start to play around with through
different articulations.
So one articulation of these semantics might be a very traditional visual analytics interface
where I've got all the different kinds of images that I care about.
This is a system we built to help a board certified dermatologist make sense of this
melanoma detection model.
And you've got query widgets on the top to sort and filter.
You can use these histograms of the shared interest metrics to really drill into the
data.
But what was maybe most interesting was what the dermatologist said when they started to
analyze that recurring pattern of context dependent cases.
So in particular, when they switch to these context dependent cases, the dermatologist
started to wonder if the model is seeing something we are not truly appreciating in the clinical
image.
Maybe there are subtle changes we don't yet understand that the model does at the boundaries
of the skin legion and things like that.
And so to me, this is alluding to the fact of, well, can we as domain experts learn something
about our problem domain based on how it is models are operating?
And I think we see this more clearly in another articulation of shared interest semantics.
Here what we're doing is basically using shared interest to interactively probe or query that
latent space.
So we're brushing and using that brushed region as ground truth and then calculating
the IOU coverage to figure out what are all the classes that maximize IOU coverage for
that brush ground truth.
So we can see if I brush over hand, a lot of the classes that get returned are things
that are often associated with hands like laptops and cleavers and interestingly enough
hen.
So I guess a lot of the images in the ImageNet data set have people holding hens, which is
I guess kind of interesting.
But more maybe profoundly is we could ask a question like, what is the essence of a dog?
What is the minimal amount of region that I would need to brush for the model to still
be convinced that what it is classifying as a dog?
So I could start with the whole dog and then brush just on its head and ensure querying
which shared interest still returns dog classes.
But then I could use a smaller brush and brush just on the nose.
And it still returns German shepherd and sheep dog and Tibetan terrier and things like that.
So it seems like according to the model, all it really needs to know about an object in
the image is the sort of shape of its nose or something associated with its nose to be
able to classify whether it is or is not a dog.
And this seems like a really sort of toy example, but it reflects some of the things that real
world scientists are doing.
So in particular, there's a researcher at the University of Washington, Julia Parrish
that runs this grand crowdsourced data collection project around seabird deaths and the way they
train their participants to figure out how to do bird classification is by asking them
to measure the bird beaks and the bird feet and things like that.
And so I think it's really interesting that we're seeing maybe some of those sorts of
representations creep up in how a model is making its decisions as well.
And so where I want to end is sort of being most speculative and where I think there's
scope for HCI to sort of grow.
And so we looked at generative models and imposing a conceptual model on them.
We looked at predictive models where the idea was to align conceptual models.
But what I think we're hearing from that dermatologist we're seeing in that last case study which
shared interest is the potential to use machine learning models to basically discover new
representations of particular problem domains.
And again, at my most speculative, I don't know what I would call these, but I would
maybe call them abstraction models, where the goal of these models is not to produce
some particular outcome that I care about, but to maximize what are the different ways
of representing the world.
What are all the diverse abstractions that we could learn about a problem domain like
classifying dogs or classifying seabirds or things like that.
And I think this is a really interesting opportunity to use machine learning to essentially advance
our understanding, advance our science.
But I want to be careful here because we've already seen through this talk but also in
the broader discourse how generative and predictive models can sort of muddy that gulf of evaluation.
Lots of people are starting to anthropomorphize these models.
Some people think these models are representing general intelligence or conscience or things
like that.
And there's a potential with these abstraction models to make this problem worse by muddying
the question of, well, how do we know what we know?
What counts as evidence?
Is it evidence because the model has learned that representation and how do we validate
what that evidence is?
In the case of representations that are designed or interpreted or theorized by people, we
know how to consider that to be evidence.
But I don't know what it means for a learned representation to count as evidence.
And as all sorts of problems in machine learning, this is not necessarily a problem that is
unique to machine learning.
So here are three visualizations that were used to discuss the COVID-19 pandemic right
at the peak of the first wave in the summer of 2020.
And I'm curious if anything pops out at you.
Any reason to be curious or suspect of these visualizations?
No, probably not.
These seem pretty legitimate, like our world and data, very legitimate data source.
And if you look at some of these two other visualizations, you might go, you know what?
Actually, the one on the right, that looks like something in maybe a policy briefing
or something, right?
It looks very sophisticated, lots of good annotation, a style and aesthetic that looks
very sort of sophisticated.
But you may be catching what I'm alluding to, which is the fact that all three visualizations
were used by people on social media to advance the argument that, you know, our response
to COVID was overblown.
Not that COVID was a hoax, but that our reaction to it was way too extreme, that COVID wasn't
as serious an issue as it might initially seem.
And I want to be really careful about what I'm doing here with these charts, because
certainly some of the people that were distributing this were bad actors who were ideologically
motivated, but through a very long, laborious ethnographic process that we conducted, spending
six months on five different Facebook groups, we found that a lot of people who were producing
visualizations like that were actually displaying many hallmarks of citizen data science.
So they were really many of them filling gaps in information sort of collection, because
they were situated in rural parts of the country where there wasn't a lot of good data collection.
So many members of these groups were hosting webcasts, live seminars of how to download
data from the government website, how to clean it and excel, how to visualize it and things
like that.
And most surprisingly to us, many of them were engaged in discussion that looked like
peer review.
They were critically assessing data sources, discussing metrics, making arguments for which
metrics were better or not.
But all of this was sort of inflected through a sort of frustration with mainstream institutions
and maybe even distrust of those institutions as well.
But ultimately what these groups cared about was bolstering a sense of social unity and
civic engagement.
So this quote I find particularly sort of reflective of that sense of it's incumbent
on all of us to hold our elected officials to account so that they make better decisions
through data.
I'm speaking to you as a neighbor, as a mama bear.
So this is not some sort of ideologically motivated individual who is trying to be a
bad actor.
This is just an engaged member of the citizenry.
And similarly, oftentimes they were actually more sophisticated than scientists can be.
So many of these members were very reflexive about their own data analysis data gathering
process.
So someone says, I've never claimed to have no bias.
I'm human.
Of course I'm biased.
Here are my biases.
Whereas in science, often we like to portray ourselves as being very objective arbiters
of truth.
And so in many ways, what was happening in these groups is perhaps more sophisticated
than what was happening in science and public health at the time.
But the question is, so what does this have to do with bridging semantic distances and
abstraction models?
Well, I think what was happening in those groups was they disagreed with the definitions
of some of these metrics.
They were living in rural communities.
And so the metrics that public health officials were using to define the state and scale of
the pandemic was not reflected in their lived experience.
They were turning around and, well, it didn't seem like COVID was an issue.
And so our colleagues in the humanities and social sciences often advocate for adopting
what they call an interpretivist lens.
The idea that knowledge is subjective, it's socially constructed, and it's composed of
many different diverse perspectives that we have to figure out ways to synthesize together.
And while that idea has been adopted in pockets of visualization and HCI and CS, so far I
think it's largely been on the qualitative side because if we think about how to do computation,
we have to, you know, we're forced into making decisions about the world and how to represent
that world and computational data structures.
And what I think abstraction models allow us to do is start to push, you know, push
that boundary a little bit, right, rather than being focused on developing a model that
produces a single best outcome, we might instead be looking to a world in which we are training
sort of ecosystems of abstraction models, where we're forcing them to learn really different
representations of the world or of a problem domain, and then leaving it up to people to
figure out how to synthesize between those learned representations for, you know, some
particular policy goal or, you know, thing that they want to optimize for.
So with that, I'm happy to take questions about any of what I talked about.
Thank you very much.
We've got some time for questions, comments.
I wanted to start off about the, like, comparing saliency maps to ground troops.
So humans are really not accurate sometimes when you ask them, like, what is the important
thing in this image that made me make this decision?
So do you think the results would be different if you used, like, ifyxations in that, compared
to that?
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
So would you be different if you used, like, ifyxations in that comparison?
That's an interesting question.
We haven't considered ifyxations for the saliency map work, but certainly I think your intuition
is right in the sense that, you know, the current way that we've modeled shared interest
is pretty brittle.
Right?
It's operating at the level of abstraction of, like, pixels in an image and how meaningful
are pixels really?
And so what Angie is working on right now
is a way to raise the level of abstraction
that shared interest is working on.
So in many of these domains, like ImageNet,
the task that we're asking models to do, the labeling task,
actually inherits from a much richer knowledge
graph or taxonomy or hierarchy or things like that.
But right now, at least, there's a little bit of work
in hierarchical learning.
But most of the predictive models
are just learning at the finest level of detail.
So we're throwing away all that rich information that
might be really relevant to how a person is making a decision.
So maybe what I care about is not whether it's
a Chihuahua or a golden retriever or a laboratory retriever.
I might care, is it a dog?
Or really, sometimes, is it just an object?
And so what does it look like to do shared interest
in more meaningful abstraction space rather than pixels
as something we're working on?
Yeah, great question.
Thanks.
Yeah, well.
Thank you for great talk, Arvin.
So going back to Jupyter Notebooks and ChatGPT,
you talked about how ChatGPT can shell out
to some of these nice plug-ins, like for Excel or whatever,
to try and help people do natural language data science
and that there's this articulatory distance due
to the difficulty of learning an API.
But conversely, you could say tools like Copilot
are the parallel to overcoming that articulatory distance
by almost, in some sense, what is the same interface expressing
in natural language, but just in a code comment
and then getting back code, right?
But just, I guess, the only difference
is its code you can see as opposed to a code that's
running in some back end that you don't see.
And I'm curious if you think there's sort of a synthesis
of these two poles, an interface that
can take the best of both worlds and offers conversation
but still provides access to the code
or encourages people to understand
the annoying representations, just if you think that's fine.
Yeah, absolutely.
I thought really hard about which of those examples
I wanted to use as the kind of foil to B2.
So I did very seriously consider a Copilot.
And I sort of agree with your analysis
that it's, I think, a much better example of how
to integrate the capacity of these LLMs.
And I think there's opportunity to push that even further,
where what I would often want is really targeted mechanisms
to introduce ambiguity.
Right now, the little that I've used Copilot,
it's almost at the level of, well, it's
going to produce the whole function, the whole whatever.
And often what I want is it to be the sort of parallel
prototyper for me.
I want to introduce, say, a hole in my program
and then go, I don't know that I want that hole to be filled
in with just one specific outcome,
but I want it to produce the whole space and for me to go,
well, I want a little bit of this and a little bit of that
and so on and so forth.
So yeah, I totally agree with there
being some really interesting medium of these things.
Cool.
Yeah, I like that idea.
Yeah.
Hey, really exciting talk.
I'm wondering towards your vision
for these abstraction models, I'm
wondering, obviously, from a human-interpreter interaction
perspective, we know representation matters so much,
right, like isomorphs of representation,
very much change how people can approach a problem
or understand it.
But I guess the ways in which they vary and the benefits
of these different representations
are tied very much to human cognition and perception.
And I'm wondering, in some of the examples
you're showing and a lot of work in machine learning,
we're sort of training things based upon that output.
And I'm wondering, are there ways
that we can get at more of how people are thinking
versus just how they output and how do we get there?
I love this question.
And the reason I love it is also the reason
I love sort of that Hutchins et al.
description of direct manipulation, right?
I find the terms that they use there, particularly
these two distances, really evocative terms.
Because to me, a distance is something
that I would want to measure, right?
But so far, at least as far as I know,
those terms have largely been descriptive, right?
As you saw in my talk, I use them to be very analytic,
but I'm not able to be generative with them
in a very systematic way.
So certainly a lot of the work that my group is trying
to do right now is, in visualization,
there's a lot of work that we've inherited in methods
from sort of vision science.
So we run these studies of human perception.
And increasingly, the field is starting to get to,
well, how do we start to measure cognition, right?
Can we model sort of a decision-making task
and start to operationalize that
through experimental design?
And so we're starting to push in some
of those directions as well,
but scope to sort of interaction in a Jupyter notebook,
but then starting to see the impact that interaction has
on sort of the downstream analyses people would do,
and then see if that actually maps to, you know,
their goals or things like that, absolutely, yeah.
So I'm curious about the, just continue on this line
of perception up through cognition, you know,
going back to the sort of like Bertrand, Cleveland,
McGill kind of stuff, the automatic processing
was very key to the design of visualizations,
especially early on.
That the notion was that my encodings
were supposed to map onto almost like system one
interpretation, right?
Like when I see the scatterplot,
you know, encoding distance in the following way,
I'm gonna draw the correct conclusion.
And it's interesting to me that sort of through the,
through the transformations you've started to pursue,
we're not trying to like encode those into a similar,
a similar mapping for audio,
but instead directly doing the cognition on behalf
of the individual.
And those seem like orthogonal directions one could go.
I'm curious how we find the right point
in the design space.
Yeah, yeah, I think this is a fantastic question.
So the way my group is starting to think about this
of like how do we find the sort of right balance
of who is doing the perception,
who is doing the interpretation is starting to consider
some of these modalities in concert
to better understand what the relative affordances
of these modalities are.
So in particular, Jonathan who you saw in the demos
is leading some really, really cool work right now
around what if I'm sort of specifying the visual,
the audio, the sort of sonified audio
and the textual audio side by side,
and then I'm playing them sort of simultaneously through.
Do I want, you know, there to be sort of perceptual redundancy
where the sonification is sort of emphasizing
what is described in the text
or do I want these modalities to be complimentary?
And sort of TBD, but I think there's some really exciting
sort of questions for us to sort of dig into in that space.
Are there similar pre-attentive principles for audio?
There must be.
As far as I know, so we're just starting to look
in the sort of sonification literature.
As far as we can tell, sound is a very, very different
perceptual sense than vision.
And so even the sort of, you know,
basic sort of visual encoding paradigm
where I take a data field I map it to, you know,
position color size, that breaks down very rapidly for audio.
So oftentimes really all the people are able to sort of,
you know, detect differences in our sort of pitch and loudness.
And even then our fidelity at that is very, very low.
And so there might be some pre-attentive characteristics.
We're certainly looking at some early work in HCI.
I think Stephen Brewster had done around sort of ear cons,
you know, discreet sort of representations of icons,
but through audio and things like that.
So there may be some of that there, but at least so far,
we're so early in our own work that we don't know.
Okay, thank you.
I think we're about at time.
So if you have additional questions,
please mob him after the talk.
Thank you, Arvin, for joining us.
Thank you very much.
Thank you.

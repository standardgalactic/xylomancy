This case has the potential to end the internet as we know it.
The internet flourishes on user-generated content.
It's the backbone of the internet.
And if you write or say something that is illegal,
you can be held liable for what you said.
But usually the websites that host your content
are not liable.
And when you hear about section 230,
this is usually what we're talking about.
But if websites were also liable for what users say,
then websites would either shut down almost everything
or police things so tightly
that there would be basically nothing
even remotely objectionable.
Because if there's one thing that websites hate,
it's being on the hook for billions and billions of dollars
of potential liability and attorney's fees.
And this isn't just some potential hypothetical.
In the past, when laws made websites liable
for user content, they just shut down the content
and never came back.
And one case before the Supreme Court this term
seeks to do exactly that, but for all of the internet.
No joke, this has the potential to end the internet
as we know it today.
So how did we get here?
How did I get here?
Well, it started with a tragedy.
Noemi Gonzalez was a design major
at California State University Long Beach
who was spending a semester studying in Paris.
On the evening of November 15th, 2015,
she was dining with friends at a cafe
on the Champs-Elysees when an ISIS terrorist
with a gun open fired.
Gonzalez was killed along with 130 other people
in coordinated attacks at six locations across the city.
ISIS claimed responsibility for the attacks
and eventually 20 men were convicted of crimes
related to the massacre.
The lone surviving gunman was sentenced
to life without parole.
So was justice served?
Well, the Gonzalez family contends that it was not
because there was another perpetrator still at large,
YouTube.
The family sued Google, which owns YouTube,
alleging that YouTube's algorithms
amplify violent videos and hateful content
despite the company's efforts to ban violent accounts
and limit their reach.
The Gonzalez family says that YouTube's recommendations
expose people to hateful content, radicalize viewers
and encourage them to make terrorist attacks of their own.
Google argued that because ISIS, not YouTube,
made and uploaded those videos,
YouTube was not the provider or developer of those videos
and therefore it was immune from liability
under 47 USC section 230,
otherwise known as section 230
of the Communications Decency Act.
Section 230 of the CDA immunizes internet services
for the content that its users upload
and often allows websites to remove content
without taking on liability.
But the Gonzalez plaintiffs contend
that YouTube's recommendations
should not be covered by section 230
because the company is acting like a content creator
rather than a publisher.
The Ninth Circuit Court of Appeals
held that the Gonzalez claims fell within section 230
and Google was immune from suit.
The Gonzalez parties appealed
and the Supreme Court granted cert
to answer the question of whether section 230
immunizes an interactive computer service
from liability for recommending other party content.
Now, in support of their claims,
plaintiffs alleged that quote two of the 12 ISIS terrorists
who carried out the attacks
used online social media platforms
to post links to ISIS recruitment YouTube videos
and quote jihadi YouTube videos.
One of the men who fired at Gonzalez
had appeared in an ISIS recruiting video in 2014.
Now the plaintiffs argue that the defendants
violated the Anti-Terrorism Act, the ATA,
by allowing ISIS to post videos on YouTube.
Under section 2333 of the ATA
as amended by the Justice Against Sponsors of Terrorism Act,
Americans who are injured by quote
an act of international terrorism
that is committed, planned or authorized
by a terrorist organization
may sue any person who quote aids and abets
by knowingly providing substantial assistance
or who conspires with a person
who committed such an act of international terrorism
and recover trouble damages.
So the legal question in the lower courts
was whether a social media platform
that was not used to commit a specific quote
act of international terrorism
may still be liable for aiding and abetting
under section 2333.
The plaintiffs say that the answer is yes,
that Google aided and abetted
an act of international terrorism
conspired with the perpetrator
of an act of international terrorism
and provided material support to ISIS
by allowing ISIS to use YouTube.
And of course the defendants say no,
the plaintiffs' claims are barred by section 230
of the Communications Decency Act,
which often immunizes interactive computer services,
AKA websites, even when they make targeted recommendations.
The trial court and the Ninth Circuit Court of Appeals
sided with Google,
ruling that the claims fell within section 230, section C
because ISIS, not YouTube,
created or developed the relevant content.
The court said that YouTube quote,
selects the particular content provided to a user
based on that user's inputs.
The display of recommended content results
from algorithms that are merely quote tools
meant to facilitate the communication
and content of others,
and not content in and of themselves.
As the lower courts noted,
the recommendations are based
on the user's preferences, not YouTube's.
The plaintiffs argue that section 230C
does not apply to quote,
activities that promote or recommend content.
So let's dig deeper into what all of this means.
First of all, the plaintiffs are not simply arguing
that platforms are liable
because terrorists have used their services.
They argue that the powerful algorithms
that recommend content are not covered by the CDA
because the recommendations are content in and of themselves.
And as the Ninth Circuit said,
the complaint alleges Google uses computer algorithms
to match and suggest content to users
based upon their viewing history.
The Gonzalez plaintiffs' alleged that in this way,
Google has recommended ISIS videos to users
and enabled users to locate other videos
and accounts related to ISIS.
And that by doing so,
Google assists ISIS in spreading its message.
So the plaintiffs claim
that YouTube facilitates communications between ISIS
and people who watch their videos,
thereby aiding ISIS in recruiting new members.
The plaintiffs acknowledge
that YouTube removed ISIS videos
and suspended or blocked ISIS users at various times.
However, the plaintiffs claim
that YouTube should have been able to stop ISIS
from reestablishing accounts using new identities.
And the plaintiffs allege
that Google left some of the ISIS videos up
because they did not contain content
violating the site's policies.
At other times, Google removed the offending content
but did not suspend or ban the accounts.
Now, we've talked about section 230
of the CDA many times on this channel.
Section 230 was created back in 1996
at the dawn of the internet.
It has two key provisions.
Section 230C1 says, quote,
no provider or user of an interactive computer service
shall be treated as the publisher or speaker
of any information provided
by another information content provider.
This sentence is often called the 26 words
that created the internet.
This section stipulates that providing access
to third-party content does not make an online provider
the publisher or speaker of that content.
Now, the second major part of the law is section 230C2,
the so-called Good Samaritan provision,
which states, quote, no provider or user
of an interactive computer service
shall be held liable on account
of any action voluntarily taken in good faith
to restrict access to or availability of material
that the provider or user considers to be obscene,
lewd, lascivious, filthy, excessively violent, harassing,
or otherwise objectionable,
whether or not such material is constitutionally protected.
So this provision gives online platforms
broad immunity from liability
if they moderate content in good faith.
This provision was meant to allow websites
to police itself without incurring new liability
because it was an issue of knowledge.
A website might not know what speech was hosted
on that particular website,
in which case they wouldn't be liable for it,
but if they knew that illegal speech
or defamatory speech or some other kind of speech
was hosted there and then took steps to remove it
from the website itself,
then they actively had knowledge of that particular speech
and thus might become liable for hosting it in the future.
So websites found themselves on the horns of a dilemma
until section 230 of the CDA was passed
to allow them to engage in good faith moderation
because there was generally a knowledge requirement
to be liable for things that were hosted on your website.
If you had no knowledge of what kind of speech
was on your website or platform,
then you generally weren't liable for it.
But the argument went that if you started
to moderate your website,
then that was evidence that you knew what was on there
and therefore you became liable for all the stuff
that you were trying to remove from your website.
This applied to spam, to pornography,
to pirated media, to defamation, you name it.
So the CDA was enacted in 1996
when the internet was still in its infancy.
At the time, companies like America Online,
CoffeeServe and Prodigy were the portals
that allowed people to access the internet.
People could share files, exchange messages
and chat with each other in real time,
but that posed a dilemma.
Who would be liable if a user posted something to famatory
or shared something else that was illegal?
Would liability go beyond the person
that was responsible for the post?
Or would the internet companies who hosted the content
also be on the hook?
And that brings us to the distinction
between content publishers, distributors and platforms,
which is often completely misunderstood.
In the pre-internet age, publishers, distributors
and platforms were treated differently under the law.
Publishers were newspapers, magazines
and broadcast stations.
They were generally liable for republishing material
from third parties since they solicited the things
that they published and they could vet those materials.
Distributors were businesses like bookstores,
newsstands and libraries, which distributed material
that was printed by others.
And distributors were not required
to assess every book that they sold.
For example, in the 1950s,
the Supreme Court overturned a Los Angeles ordinance
that said, if you have obscene material in your bookstore,
you can be held criminally responsible.
And in that case, a bookstore owner was convicted
of violating the ordinance by selling a novel
called Sweeter Than Life, which tells the story
of a, quote, ruthless lesbian business woman.
The Supreme Court said that the bookstore couldn't be
responsible for reviewing every single thing
in the book or magazine that it sells.
And distributors only had liability
when someone notified them
that something they carried was illegal.
That's sort of the origin of the knowledge requirement.
And platforms were common carriers
like phone companies and television broadcasters.
Platforms weren't liable for third party content
that they carried.
So for example, let's say AT&T found out
that someone's answering machine
had a libelous outgoing message.
Was AT&T required to act
by canceling the owner's telephone service?
Well, the court said no, AT&T couldn't be sued
for libel simply because the owner used its service
to say something libelous.
And similarly, a broadcaster wasn't libel
for running a political candidate
that falsely accused someone of being a communist.
But then the internet came around
and the internet-era posed a new challenge
for this case law.
Two of the first internet service providers,
CompuServe and Prodigy, were sued for hosting forums
where users posted defamatory content.
Now Prodigy billed itself
as a family-friendly version of the internet.
Did you get a computer recently?
Well, congratulations.
You can now join hundreds of thousands
of discovered Prodigy.
It moderated comments,
removing things it thought were bad.
Now CompuServe didn't moderate anything.
CompuServe combines the power of your computer
with the convenience of your telephone.
And both of those companies were sued
for defamatory statements
that their users posted to the service.
Now the case against CompuServe was dismissed
because the court considered it a distributor of content
rather than a publisher.
CompuServe could only be held liable for defamation
if it knew or had reason to know
of the defamatory nature of the content.
And since they didn't do anything
to moderate their forums,
the company couldn't possibly know,
or at least argued it couldn't possibly know
about what the denizens of Rumorville
were posting about each other.
But Prodigy was found liable
for failing to moderate enough.
Someone went on a forum
and claimed that Jordan Belfort's investment firm,
Stratton Oakmont,
committed fraud in connection with a stock IPO.
The firm sued Prodigy
and the anonymous poster for defamation.
And a court held that Prodigy was liable
as the publisher of the content
created by its users
since it exercised editorial control
over the messages on its bulletin board.
Now, if the names Jordan Belfort and Stratton Oakmont
ring a bell,
that's because they were literally scamming people
out of millions of dollars
and have gone down as some of the biggest fraudsters
in all of Wall Street history.
The movie The Wolf of Wall Street
is based on the fraud
that they were perpetrating with respect to the IPOs.
And it's one of the biggest ironies
in all of First Amendment law
that they won this lawsuit
against the people that were hosting,
presumably accurate information
about how they were scamming people out of money.
That's part of the problem
when websites are liable for user content
is that people can file lawsuits to stifle speech.
That's what a slap lawsuit is all about.
And because lawsuits are incredibly expensive,
sometimes even if you scammed people out of money,
as long as you file a lawsuit,
you can stifle that kind of speech.
But I digress.
As a result of these cases,
internet service providers basically either decided
to not moderate at all
and leave absolutely everything up,
or basically moderate everything
and not allow anything to go up.
And basically no one was happy with this state of affairs.
So Congress enacted the Communications Decency Act.
And then as now, lawmakers claim to be especially worried
about things like harassment and obscene material.
So they gave online providers immunity from lawsuits
if they moderated content.
And again, the irony here is that this legislation came
from the sort of family values type politicians,
people who always want to think about the children.
Oh, won't somebody please think of the children?
They wanted to allow websites to remove violent
and sexual conduct so that the internet could be even cleaner.
The same politicians who wanted
and got the little parental advisory explicit
content warnings put on rap albums.
But I digress again.
Section 230 was intended as a way for internet companies
to create and enforce basic standards
to run their websites.
And when the CompuServe and Prodigy cases were decided,
the courts only considered whether the ISPs
were publishers or distributors.
But an internet service like a website
didn't fit neatly into those categories.
A traditional publisher has total control
over whether to publish content.
A distributor can control what it buys,
but it wouldn't have been practical for a human
to fully screen every item.
Modern social media companies host more material
than any library or bookstore on earth,
which makes screening all of that info daunting,
if not impossible.
And without Section 230, social media companies
would be subject to strict liability
for every message and post made on their services.
So Congress chose to give those platforms and websites
immunity as an incentive to get them
to remove offending speech.
Congress also chose not to dictate to those companies
which speech they had to remove.
So that brings us back to the original question in this case.
How should courts read Section 230?
The lower courts follow the tests set forth
in the Ninth Circuit case called Barnes versus Yahoo.
And the Barnes test hues close to the text of Section 230.
The law specifies that only one,
no interactive computer service,
shall be two, treated as a publisher or speaker,
of three, content provided
by another information content provider, aka users.
And the plaintiffs argue their claims
do not treat Google as a publisher,
but instead assert a simple duty not to support terrorists.
They said that the ATA prohibits Walmart
from supplying fertilizer, knives,
or other material to ISIS.
And in the same way, the ATA bars Google
from giving ISIS a platform to communicate.
The Ninth Circuit found this analogy specious.
The idea of a duty not to support terrorists,
quote, overlooks that publication itself
is the form of support Google allegedly provided to ISIS.
And publication of third-party content
is what Section 230 gives to internet services.
According to lower courts, publishing encompasses,
quote, any activity that can be boiled down
to deciding whether to exclude material
that third parties seek to post online.
Google says that the CDA uses the words
publisher or speaker in an ordinary sense,
quote, based on an ordinary meaning,
a publisher or speaker is one that publishes or speaks.
Google contends that when YouTube's algorithms
make recommendations,
this activity is protected by Section 230, quote,
Congress underscored that publishing
for purposes of Section 230
includes sorting content via algorithms
by defining interactive computer service
to include tools that pick, choose, filter,
search, subset, organize, or reorganize content.
Congress intended to provide protection for these functions
not for simply hosting third-party content.
And the plaintiffs disagreed.
Their interpretation of the Section 230C defense
is that it requires a narrower interpretation
of the word publisher.
The Ninth Circuit previously held that Section 230C1
uses publisher in its everyday sense
and the Second Circuit agreed.
But the plaintiffs contend that the word publisher
is used in Section 230C1 with a narrower
and distinct meaning,
which that term has in defamation law.
Basically, the plaintiffs are claiming
that anytime YouTube recommends another video,
it therefore becomes the speaker of that video
as if YouTube had produced that video itself.
The defendants say that this interpretation
doesn't make sense, quote,
watch the World Series of Poker on YouTube
and YouTube's algorithms
might display Texas Hold'em tutorials.
That does not mean that YouTube endorses gambling
anymore than spell check endorses a suggested substitute word.
Westlaw endorses higher listed cases
or a chat room endorses posts organized by topic.
As the Ninth Circuit noted,
YouTube applies the same algorithms to all content.
And it's here that I think people often misunderstand
the nature of the internet
and the nature of social media.
With the case of YouTube,
people upload millions of videos a day.
If there wasn't an algorithm to sort through
this deluge of videos,
the only way to sort through them would be a fire hose
of viewing every video chronologically,
which nobody wants.
It would be that or doing a pointed search
to try and find content,
but then you'd never be exposed to content
that you actually do want to watch,
but didn't know that you wanted to seek it out.
And the plaintiffs have tried to differentiate
between search results
and other algorithmically generated results.
But there's really no principled way
to distinguish between results that are sorted
when you actively search for something
versus results that are presented to you
because of a recommendation algorithm.
And people love to hate algorithms, myself included.
But the truth is,
if there weren't algorithms to help us sort through
all of the content on social media platforms,
it would be impossible to wade through it.
And then additionally, there is the issue
of the material support related for a claim
under the Anti-Terrorism Act.
Now, the Department of Justice's brief
in support of the plaintiffs argues
that algorithmic recommendations
convey the website's own implicit message
that users will find the information relevant,
and that makes the website liable
because it's acting just like ISIS itself.
Now, plaintiffs didn't claim this
in their original complaint.
Instead, they focused on how YouTube's recommendations
amplify the ISIS speech,
making it more visible to more users.
But the US government urged the Supreme Court
to adopt a narrow reading of Section 230
that would permit a finding
that Google materially contributed to the content
by matching it to user preferences.
Again, basically, the Department of Justice,
as well as the plaintiffs, are arguing for a standard
that if any website promotes content,
uses an algorithm, or recommends content to users,
that that website then becomes liable
for all of the speech contained within that recommendation.
What happens when websites are liable?
Well, remember I mentioned
that this has actually happened before?
Well, it has.
In 2018, then President Trump signed into law
two bills making it easier
to fight sex trafficking online.
That was the Fight Online Sex Trafficking Act
and the Stop Enabling Sex Traffickers Act,
otherwise known as FOSTA and CESTA.
And those two laws created an exception
to Section 230's Safe Harbor Rule
by stating that websites could be held legally responsible
if third parties posted ads for prostitution
on their platforms.
The law penalizes websites,
which quote, promote or facilitate prostitution.
And while this includes sites
that promote illegal sex work,
it also allows the police to investigate any site
that is knowingly assisting, facilitating,
or supporting sex trafficking.
This language can be read to include sites
with content that is legal,
such as escort services, personal ads, and pornography.
Now, critics of FOSTA and CESTA warned
that the new laws were too broad
and they would end up killing internet content
that has nothing to do with sex trafficking.
And they turned out to be correct.
If a third party posts content
that is covered by FOSTA or CESTA
and the website fails to prevent that content
from being posted, that website can be sued.
And that was simply too much legal risk for several websites.
You should have gone for the head.
Craigslist shut down its entire personal section.
Tumblr banned all adult content
and Reddit removed several subreddits
and other websites just simply shut down entirely.
So you might ask, did any of this assist
with the prosecution of sex trafficking?
Well, in 2021, the US government accountability office
released a report on the first three years
of FOSTA and CESTA finding that, quote,
over three years, the Department of Justice
filed just one case under its rules
against promoting or recklessly disregarding sex trafficking.
So ironically, this provided essentially
the perfect natural experiment to find out
what happens when websites are liable
for the speech content of its users.
One of two things will happen.
Either the website shuts down that section
or shuts down the entire department
or the website employs an army of moderators
to remove anything even remotely offensive.
You basically either live in a world
where a website allows everything,
scams, spam, pornography, ultra violence,
just everything that you can possibly imagine
that you probably don't wanna see in your everyday life
or you live in a world where they employ
extremely expensive moderators
who prevent anything that might possibly be offensive.
And that of course assumes that a website
can even afford to employ those moderators.
So there are those who want to see
Section 230 modified or scrapped altogether
and have argued that these online services
are acting like content developers.
Now oral argument in front of the Supreme Court
is next week and I'm putting my money
where my mouth is on this one.
I filed a brief in the Supreme Court
along with Dr. Mike, Mythical Entertainment,
Chubby Emu, Tim Schmoyer and the Author's Alliance
and a bunch of other creators.
We filed an amicus brief explaining
the potential dire consequences
of a really bad ruling on this.
So we'll see how it goes.
But in the meantime, I'll probably just be stress eating
at the end of the internet.
Unless I use today's sponsor, Factor 75.
Because Factor makes meeting your nutrition goals
easier than ever by delivering fresh, never frozen,
dietitian-approved meals right to your doorstep.
And the meals are completely ready to eat.
Their team of gourmet chefs creates each meal
using ingredients with integrity
to help you feel your best all day long.
And I can tell you from experience
they really are extremely delicious.
Factor supports wholesome eating, made simple.
Their menus are updated weekly with 34 different options.
Choose your favorite meals or let Factor craft the order
based on your taste preferences and meal history.
Factor takes the guesswork out of grocery shopping
and meal prep, saving you time and energy for other things.
There are no hassle-prepared foods,
make sure you always have something nutritious on hand
when you don't have time to think about making a meal.
And sometimes I just want a good, healthy meal
without having to cook or obviously shop for the ingredients.
And Factor's meals arrive pre-prepared
and ready to eat in just two minutes.
And I can always scale up my meals
if I need the extra energy after a long day of lawyering.
So give Factor a try by heading to factor75.com
and use the code LegalEagle50
to get 50% off your first factor box.
Or just click on the link that's on screen right now
or down in the description
and use the code LegalEagle50
to get 50% off your first factor box.
And clicking on that link really helps out this channel.
And after that, click on this box over here
for more LegalEagle or I'll see you in court.

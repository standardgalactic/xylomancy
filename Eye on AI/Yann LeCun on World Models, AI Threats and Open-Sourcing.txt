Even if you train a system to have a world model that can predict what's going to happen next,
the world is really complicated and there's probably all kinds of situations that the system
hasn't been trained on and need to, you know, fine-tune itself as it goes.
The question of how we organize AI research going forward, which is somewhat determined by how
afraid people are of the consequences of AI. So if you have a rather positive view of the impact
of AI on society and you trust humanity and society and democracies to use it in good ways,
then the best way to make progress is to open research.
AI might be the most important new computer technology ever.
It's storming every industry and literally billions of dollars are being invested.
So buckle up. The problem is that AI needs a lot of speed and processing power.
So how do you compete without cost spiraling out of control?
It's time to upgrade to the next generation of the cloud, Oracle Cloud Infrastructure,
or OCI. OCI is a single platform for your infrastructure, database, application, development,
and AI needs. OCI has four to eight times the bandwidth of other clouds,
offers one consistent price instead of variable regional pricing, and of course,
nobody does data better than Oracle. So now you can train your AI models at twice the speed and
less than half the cost of other clouds. If you want to do more and spend less like Uber,
eight by eight, and Databricks Mosaic, take a free test drive of OCI at oracle.com
slash I on AI. That's E-Y-E-O-N-A-I all run together oracle.com slash I on AI.
Hi, I'm Craig Smith, and this is I on AI. In this episode, I speak again with Yan Lacoon,
one of the founders of deep learning and someone who followers of AI should need no introduction to.
Yan talks about his work on developing world models, on why he does not believe AI research
poses a threat to humanity, and why he thinks open source AI models are the future.
In the course of the conversation, we talk about a new model, Gaia 1, developed by a company called
Wave AI. I'll have an episode with Wave's founder to further explore that world model,
which has produced some startling results. I hope you find the conversation with Yan
as enlightening as I did. First, the notion of world model is the idea that the system would get
some idea of the state of the world and be able to predict sort of following states of the world
resulting from just the natural evolution of the world or resulting from an action that the
agent might take. If you have an idea of the state of the world and you imagine an action that
you're going to take, and you can predict the resulting state of the world, that means you
can predict what's going to happen as a consequence of a sequence of actions, and that means you can
plan a sequence of actions to arrive at a particular goal. That's really what a world model is.
At least that's what the Wave people have understood the word in other contexts, like
in the context of optimal control and robotics and things like that. That's what a world model is.
Now, there are several levels of complexity of those world models, whether they model yourself,
the agent, or whether they model the external world, which is much more complicated.
Then, so training a world model basically consists in just observing the world go by
and then learning to predict what's going to happen next, or observing the world taking an action
and then observing the resulting effect, an action that you take as an agent or an action that you
see other agents taking. That establishes causality, essentially. You could think of this as a causal
model. Those models don't need to predict all the details about the world. They don't need to be
generative. They don't need to predict exactly every pixels in a video, for example, because
what you need to be able to predict is enough details, some sort of abstract representation
to allow you to plan. You're assembling something out of wood and you're going to put
two planks together and attach them with screws. It doesn't matter the details of which type of
screwdriver you're using or the size of the screw within some limits and things like that. There
are details that in the end don't matter as to what the end result will be or the precise grain of
of the wood and things of that type. You need to have some abstract level of representation within
which you can make the prediction without having to predict every detail. That's why those JPA
architectures I've been advocating are useful. Models like the Gaia 1 model from Wave actually
makes prediction in an abstract representation space. There's been a lot of work in that area
for years also at FAIR, but generally the abstract representation were pre-trained. The
encoders that would take images from videos and then encode them into some representation
were trained in some other way. The progress we've made over the last six months in
self-supervised learning for images and video is that now we can train the entire system
to make those predictions simultaneously. We have systems now that can
learn good representations of images and the basic idea is very simple. You take an image,
you run it through an encoder, then you corrupt that image, you mask parts of it, for example,
or you transform it in various ways, you blur it, you change the colors, you change the framing
a little bit and you run that corrupted image through the same encoder or something very similar
and then you train the encoder to predict the features of the complete image from the features
of the corrupted one. You're not trying to reconstruct the perfect image,
you're just trying to predict the representation of it. This is different, this is not generative
in a sense that it does not produce pixels and that's the secret to getting self-supervised learning
to work in the context of images and video. You don't want to be predicting pixels, it doesn't work.
You can't predict pixels as an afterthought, which is what the Gaia system is doing by
sticking a decoder on it and with some diffusion model that will produce a nice image,
but that's kind of a second step. If you train the system by predicting pixels,
you just don't get good representations, you don't get good predictions, you get blurry
predictions most of the time. That's what makes learning from images and video fundamentally
different from learning from text because in text you don't have that problem. It's easy to predict
words even if you cannot do a perfect prediction because language is discrete. Language is simple
compared to the real world. There's a lot written right now about the
energy required in the computational resources GPUs required to train language models. Is it
less in training a world model like using iJAPA architecture? Well, it's hard to tell because
there is no equivalent training procedure, self-supervised training procedure for video,
for example, that does not use JAPA. The ones that are generative don't really work.
This architecture could also be applied to language, couldn't it?
Oh yeah, absolutely. You could very well use a JAPA architecture that makes prediction in
world presentation space and apply to language. In that case, would it be less computationally
intense than training a large language model? It's possible. It's not entirely clear either.
I mean, there is some advantage regardless of what technique you're using to making those models
really big. They just seem to work better if you make them big. If you make them bigger.
Scaling is useful. Contrary to some claims, I do not believe that scaling is sufficient. In other
words, we're not going to get anywhere close to human-level AI. In fact, not even animal-level
AI by simply scaling up language models, even multi-model language models that we applied to
video. We're going to have to find new concepts, new architectures. I've written a vision paper
about this a while back of a different type of architecture that would be necessary for this.
Scaling is necessary but not sufficient. We're missing some basic ingredients to get to
human-level AI. We're fooled by the fact that LLMs are fluent. We think that they have human-level
intelligence because they can manipulate language, but that's false. In fact, there's a very good
symptom for this, which is that we have systems that can pass the bar exam,
but answering questions from text by basically regurgitating what they've learned,
more or less, by road. We don't have completely autonomous level 5 self-driving cars,
or at least no system that can learn to do this in about 20 hours of practice, just like any 17-year-old.
We certainly don't have any domestic robot that can clear up the dinner table and fill
up the dishwasher attest that any 10-year-old can learn in one shot. Clearly, we're missing
something big, and that something is an ability to learn how the world works, and the world is
much more complicated than language, and also being able to plan and reason, basically having
a mental world model that allows to plan and predict consequences of actions. That's what we're
missing. It takes a while before we figure this out. You were on another paper that talked about
augmented language models, and in the embodied touring test, was that the same paper, the embodied
touring test? Can you talk about that? First of all, what is the embodied touring test? I didn't
quite understand that. It's a different concept, but it's basically the idea that you
it's based on the Moravec paradox. Moravec, many years ago, noticed that things that appeared
difficult for humans turned out to sometimes be very easy for computers to do, like playing chess,
much better than humans, or computing integrals, or whatever, certainly doing arithmetic.
But then there are things that we take for granted as humans, that we don't even consider
their intelligent tasks, that we are incapable of reproducing with computers. That's where the
embodied touring test comes in. Observe what a cat can do, or how fast a cat can learn
new tricks, or how a cat can plan to jump on a bunch of different furniture to get to the top of
wherever it wants to go. That's an amazing feat that we can't reproduce with robots today.
That's the embodied touring test, if you want. Can you make a robot that can behave,
have behaviors that are easily distinguishable from those of animals, first of all, and can acquire
new ones with the same efficiency as animals? Then the augmented data in paper is different.
It's about how do you sort of minimally change large language models so that they can use tools,
so they can, to some extent, plan actions. You need to compute the product of two numbers.
You just call a calculator, and you know you're going to get the product of those two numbers.
LLMs are notoriously bad for arithmetic, so they need to do this kind of stuff, or do a search
using a search engine, or database lookup, or something like that. There's a lot of work on this
right now, and it's somewhat incremental. How can you sort of minimally change LLM
and take advantage of their current capabilities, but still augment them with the ability to use tools?
Yeah, and I don't want to get into the too much, into the threat debate,
but you're on one side, your colleagues, Jeff and Yashor, on the other. I recently saw a picture
of the three of you. I think you put that up on social media, saying how you can disagree,
but still be friends. This idea of augmenting language models with stronger reasoning capabilities
and the ability, and agency, the ability to use tools, is precisely what Jeff and Yashor are worried
about. Can you just why are you not worried about that? Okay, so first of all, what you're
describing is not necessarily what they are afraid of. They are alerting people and various
governments and others about various dangers that they perceive. Okay, so one danger,
one set of dangers are relatively short term. There are things like bad people will use technology
for bad things. What can bad people use powerful AI systems for? And one concern that governments
have been worried about, and intelligence agencies encounter intelligence and stuff like that,
could badly intentioned organizations or countries use LLM to help them design pathogens,
or chemical weapons, or other things, or cyber attacks, things like that. Now, those
problems are not new. Those problems have been with us for a long time. And the question is,
what incremental help would AI systems bring to the table? So my opinion is that as of today,
AI systems are not sophisticated enough to provide any significant help for such badly
intentioned people, because those systems are trained with public data that is publicly available
on the internet. And they can't really invent anything. They're going to regurgitate with a
little bit of interpolation if you want. But they cannot produce anything that you can't get from
a search engine in a few minutes. So that claim is being tested at the moment. There are people
who are actually trying to figure out, is it the case that you're able to do something more
dangerous with current AI technology that you can't do with a search engine? Results are not out yet.
But my hunch is that it's not going to enable a lot of people to do significantly bad things.
Then there is the issue of things like code generation for several attacks and things like
this. And those problems have been with us for years. And the interesting thing that most people
should know also for disinformation or attempts to corrupt the electoral process and things like
this. And what's very important for everyone to know is that the best countermeasures that we have
against all of those attacks currently use AI massively. So AI is used as a defense mechanism
against those attacks. It's not actually used to do the attacks yet. And so now it becomes the
question of who has the better system? Are the countermeasures, is the AI used by the countermeasures
significantly better than the AI is used by the attackers so that the problem is satisfactorily
mitigated? And that's what we are. Now the good news is that there are many more good guys and bad
guys. They're usually much more competent. They're usually much more sophisticated. They're usually
much more better funded. And they have a strong incentive to take down the attackers. So it's a
game of cat and mouse just like every security that's ever existed. There's nothing new there.
Nothing qualitatively new. But then there is the question of existential risk.
And this is something that both Jeff and Yosha have been thinking of fairly recently. So for Jeff,
it's only sort of just before last summer that he started thinking about this. Because before he
thought, he was convinced that the kind of algorithms that we had were significantly inferior
to the kind of learning algorithm that the brain used. And the epiphany he had was that, in fact,
no, because looking at the capabilities of large language models, they can do pretty amazing things
with a relatively small number of neurons and synapses. He said, maybe they're more efficient
than the brain. And maybe the learning algorithm that we use, back propagation, is actually better
than whatever it is that the brain uses. So he started thinking about what are the consequences.
But that's very recent. And in my opinion, he hasn't thought about this enough.
Yosha went to a similar epiphany last winter, where he started thinking about the long-term
consequences. And came to the conclusion also that there was a potential danger.
They're both convinced that AI has enormous potential benefits. They're just worried
about the dentures. And they're both worried about the dentures because they have some doubts
about the ability of our institutions to do the best with technology.
Whether they are political, economic, geopolitical, financial institutions,
or industrial, to do the right thing, to be motivated by the right thing. So
if you trust the system, if you trust humanity and democracy, you might be entitled to believe that
society is going to make the best use of future technology. If you don't believe in the
solidity of those institutions, then you might be scared. I think I'm more confident in humanity
and democracy than they are. And whatever current systems and they are. I've been thinking about
this problem for much longer, actually, since at least 2014. So when I started fair at Facebook
at the time, it became pretty clear, pretty early on that deploying AI systems was going to have
big consequences on people and society. And we got confronted to this very early.
And so I started thinking about those problems very early on.
Things like countermeasures against bias in AI systems, systematic bias, countermeasures against
attacks, or detection of hate speech in every language. These are things that people at fair
worked on and then were eventually deployed. To just give you an example, the proportion of hate
speech that was taken down automatically by AI systems five years ago in 2017 was about 20 to
25%. Last year, it was 95%. And the difference is entirely due to progress in natural language
understanding. Entirely grew to transformers that are pre-trained self-supervised and can
essentially detect hate speech in any language. Not perfectly. Nothing is perfect. It's ever
perfect. But AI is just massively there. And that's the solution. So I started thinking about
those issues, including existential risk, very early on. In fact, in 2015, early 2016, actually,
I organized a conference hosted at NYU on the future of AI, where a lot of those questions were
discussed. I invited people like Eric Schmidt and Mark Schreffer, who was the CTO of Facebook at the
time. Demise Sabis, a lot of people, both from the academic and AI research side and from the
industry side. And there were two days, a public day and kind of a more private day. What came out
of this is the creation of an institution called a partnership on AI. So this is a discussion I had
with Demise Sabis, which was, would it be useful to have a forum where we can discuss
before they happen bad things that could happen as a consequence of deploying AI?
Pretty soon, we brought on board Eric Horvitz and a bunch of other people. And we co-founded this
thing called a partnership on AI, which basically has been funding studies about AI ethics and
consequences of AI and publishing guidelines about how you do it right to me and my son.
So this is not a new thing for me. I've been thinking about this for 10 years,
essentially. Whereas for Yosha and Jeff, it's much more recent.
But nonetheless, this augmented AI or augmented language models that have stronger reasoning
and agency raises the threat regardless of whether or not it can be countered to a higher level.
Right. Okay. So I guess the question there becomes, what is the blueprint of future AI systems
that will be capable of reasoning and planning, will understand how the world works,
will be able to use tools and have agency and things like that. Right.
And I tell you, they will not be autoregressive LLMs. So the problems that we see at the moment
of autoregressive LLM, the fact that they hallucinate, they sometimes say really stupid
things. They don't really have a good understanding of the world. People claim that they have some
simple word model, but it's very implicit and it's really not good at all. For example,
you can tell an LLM that A is the same as B and then you ask if B is the same as A and it will say,
I don't know or no. Those things don't really understand logic or anything like that.
So the type of system that we're talking about that might approach
any more level intelligence and let alone human level intelligence
have not been designed. They don't exist. And so discussing their danger
and their potential harm is a bit like discussing the sex of angels at the moment
or to be a little more accurate perhaps. It would be kind of like discussing how we're going to make
transatlantic flight at near the speed of sound safe when we haven't yet invented the turbojet
in 1925. We can speculate, but how did we make turbojet safe? It required decades of really
careful engineering to make them incredibly reliable and now we can run like halfway around
the world with a two-engine turbojet aircraft. I mean, that's an incredible feat. And it's not
like people were discussing sort of philosophical questions about how you make turbojet safe.
It's just really careful and complicated engineering that no one, none of us would understand.
So how can we ask the AI community now to explain how AI systems are going to be safe? We
haven't invented them yet. Okay, that said, I have some idea about how we can design them
so that they have these capabilities. And as a consequence, how they will be safe,
I call this objective-driven AI. So what that means is essentially systems that produce their
answer by planning their answer so as to satisfy an objective or a set of objectives. So this is
very different from current LLNs. Current LLNs produce one word after the other or one token,
which has a board unit. It doesn't matter. They don't really think and plan ahead as we said
before. They just produce one word after the other. That's not controllable. The only thing we can do
is see if what they've produced, check if what they've produced satisfies some criterion or set
of criteria and then not produce an answer or produce a non-answer if the answer that was
produced isn't appropriate. But we can't really force them to produce an answer that satisfies
a set of objectives. So objective-driven AI is the opposite. The only thing that the system
can produce are answers that satisfy a certain number of objectives. So what objective would be?
Did you answer the question? Another objective could be, is your answer understandable by a
13-year-old because you're talking to a 13-year-old? Another would be, is this, I don't know,
terrorist propaganda or something. You can have a number of criteria like these guardrails that
we guarantee that the answer that's produced satisfies certain criteria, whatever they are.
Same for a robot. You can guarantee that the sequence of actions that is produced
will not hurt anyone. Like you can have very low-level guardrails of this type that say,
okay, you have humans nearby and you're cooking, so you have a big knife in your hand. Don't flail
your arms. That would be a very simple guardrail to impose. And you can imagine having a whole
bunch of guardrails like this that will guarantee that the behavior of those systems would be safe
and that their primary goal would be to be basically subservient to us. So I do not believe that we'll
have AI systems that can work that will not be subservient to us, will define their own goals,
they will define their own sub-goals, but those sub-goals would be sub-goals or goals that we
set them and will not have all kinds of guardrails that will guarantee the safety. And we're not
going to, it's not like we're going to invent a system and make a gigantic one that we know will
have human-level AI and just turn it on and then from the next minute it's going to take over the
world. That's completely preposterous. What we're going to do is try with small ones, maybe a smart
as a mouse or something, maybe a dog, maybe a cat, maybe a dog, and work our way up and then put
some more guardrails. Basically, like we've engineered more and more powerful and more
reliable turbojets. It's an engineering problem. Yeah. You were also on a paper, maybe this is
the one that talked about the embodied Turing test on neuro AI. Can you explain what neuro AI is?
Okay. Well, it's the idea that we should get some inspiration from neuroscience to build AI systems
and that there is something to be learned from neuroscience and from cognitive science.
To drive the design of AI systems. Some inspiration. Okay. Something to be learned,
as well as the other way around. So what's interesting right now is that the best models
that we have of how, for example, the visual cortex works is convolutional neural networks,
which are also the models that we use to recognize images primarily in artificial systems.
So there is information being exchanged both ways. One way to make progress in AI is to ignore
nature and just try to solve problems in an engineering fashion, if you want.
I found interaction with neuroscience always thought-provoking. So you don't want to be
copying nature very too closely because there are details in nature that are irrelevant.
And there are principles on which natural intelligence is based that we haven't discovered.
But there is some inspiration to have, certainly a convolutional net for inspired by the
architecture of the visual cortex. The whole idea of neural net and deep learning came out of
the idea that intelligence can emerge from a large collection of simple elements that are
connected with each other and change the nature of their interactions. That's all idea, right?
So inspiration from neuroscience certainly has been extremely beneficial so far,
and the idea of neural AI is that you should keep going. You don't want to go too far. So
going too far, for example, is trying to reproduce some aspect of the functioning of neurons
with electronics. I'm not sure that's a good idea. I'm skeptical about this, for example.
So your research right now, your main focus is on furthering the JEPA architecture into other
modalities or where you headed? Yeah, so the long-term goal is to get machines to
be as intelligent and learn as efficiently as animals and humans. And the reason for this is that
we need this because we need to amplify human intelligence. And so intelligence is the most
needed commodity that we want in the world, right? And so we could possibly bring a new
renaissance to humanity if we could amplify human intelligence using machines, which we are doing
already with computers, right? I mean, that's pretty much what they've been designed to do.
But even more, imagine a future where every one of us has an intelligent assistant with us
at all times. They can be smarter than us. You shouldn't feel threatened by that. We should feel
like we are like a director of a big lab or a CEO of a company that has a staff working for them
of people who are smarter than themselves. I mean, we're used to this already. I'm used to this
certainly working with people who are smarter than me. So we shouldn't feel threatened by this,
but it's going to empower a lot of us, right, and humanity as a whole. So I think that's a good
thing. That's the overall practical goal if you want, right? Then there's a scientific question
that's behind this, which is really what is intelligence and how you build it. And then
which is, you know, how can system learn the way animals and humans seem to be learning so
efficiently? And the next thing is how do we learn how the world works by observation, by
watching the world go by through vision and all the other senses. And animals can do this without
language, right? So it has nothing to do with language. It has to do with learning from sensory
perceives and learning mostly without acting because any action you take can kill you. So
it's better to be able to learn as much as you can without actually acting at all, just observing.
Which is what babies do in the first few months of life. They can hardly do anything, right? So
they mostly observe and learn how the world works by observation. So what kind of learning takes
place there? So that's obviously kind of self-supervised, right? It's learning by prediction. That's
an old idea from cognitive science. And the thing is, you know, we can learn to predict videos,
but then we noticed that predicting videos, predicting pixels in video is so initially
complicated that it doesn't work. And so then came this idea of JAPA, right? Learn representations
so that you can make predictions in representation space. And that turned out to work really well
for learning image features. And now we're working on getting this to work for video. And eventually,
we'll be able to use this to learn to learn world models where you show a piece of video
and then you say, I'm going to take this action, predict what's going to happen next in the world.
And, you know, which is a bit where the Gaia system from Wave is doing at a high level,
but we need this at sort of various levels of abstraction so that we can build,
you know, systems that are more general than autonomous driving. Okay. That's the...
Yeah. And it's my fault, so I won't go over the hour, but is it conceivable that someday there'll be
a model that you may be embodied in a robot that is ingesting video from its environment
and learning as it's just continuously learning and getting smarter and smarter and smarter?
Yeah. I mean, that's kind of a bit of a necessity. The reason being that, you know,
even if you train a system to have a world model that can predict what's going to happen next,
the world is really complicated and there's probably all kinds of situations that you,
you know, the system hasn't been trained on and need to, you know, fine tune itself as it goes.
So, you know, animals and humans do this early in life by playing, so play is a way of
learning your world model in situations that basically you won't hurt you.
And, but then during life, of course, you know, when we want to drive, there's all kinds of
these mistakes that we do initially that we don't do after having some experience and that's
because we're fine tuning our world model to some extent. We're learning a new task. We're
basically just learning a new version of our world model. So, yeah, I mean, this type of
continuous, continual learning is going to have to be present. But the overall power and
intelligence of the system will be limited by, you know, how much, like a big of neural net is
using and various other constraints, you know, computational constraints, basically.
You know, you're still young. And, and this,
I'm not sure about that. Well, you're younger than Jeff. Let me put it that way.
I'm younger than Jeff. I'm older than Joshua.
But this, the progress you've made on world models is, is fairly rapid from my point of
view, watching it. Are you, are you hopeful that within your career, you'll have
embodied robots that are, are building world models through their interaction in reality,
and, and then being able to, well, I guess the other question on world models,
do you then combine it with a language model to do reasoning, or, or is the world model able to,
to do reasoning on its own? But are you hopeful that in your career, you'll, you'll get to the
point where you'll have this continuous learning in a world model? Yeah, I sure hope so. I might have
another, you know, 10, 10 useful years or something like this in research before my brain, you know,
turns into dish and male sauce, but, or something like that, you know, 15 years if I'm lucky.
So, or perhaps less. But yeah, I hope that there's going to be breakthroughs in that direction during
that time. Now, whether there will result in the kind of artifact that you're describing, you know,
robots that can, like, you know, domestic robots, for example, or, or sort of in cars that are,
they can learn fairly quickly by themselves. I don't know, because there might be all kinds of
obstacles that we have not envisaged that may appear on the way. You know, that's, it's a constant
in the history of AI that you have some new idea and a breakthrough, and you think that's going to
solve all the world's problems. And then you kind of hit limitation, and you have to go beyond that
limitation. So it's like, you know, you're climbing a mountain, you find a way to climb the mountain
that you're seeing. And you know that once you get to the top, you will have the problem solved,
because now it's, you know, the gentle slope down. And once you get to the top, you realize that there
is another mountain behind it that you hadn't seen. Yeah. So that's, that's been the history of AI,
right, where people have come up with sort of new concepts, new ideas, new way to approach
AI reasoning, whatever, perception, and then realize that their idea basically was very limited.
And so, so, you know, this, inevitably, we're trying to figure out what's the next
revolution in AI. That's what I'm trying to figure out. And so, you know, learning how the world
works from video, having systems that have world model allows systems to reason and plan.
And there's something I want to be very clear about, which is an answer to your question,
which is that you can have systems that reason and plan without manipulating language. Animals
are capable of amazing feats of planning and also to some extent reasoning. They don't have language,
at least most of them don't. And so, many of them don't have culture, because they are mostly
solitary animals. So, you know, it's only the animals that have some level of culture. So,
so the idea that the system can plan and reason is not connected with the idea that you can manipulate
language. Those are two different things. It needs to be able to manipulate abstract notions. But
those notions do not necessarily correspond to linguistic entities like words or things like
that. We can have mental images, if you want to things like you do, ask a physicist or a
mathematician, you know, how they reason. It's very much in terms of sort of mental models.
I have nothing to do with language. Then you can turn things into language, but that's a
different story. That's the second step. So, you know, we're going to have to figure out how to do
this reasoning, hierarchical planning in machines, reproduce this first. And then, of course, you
know, sticking language on top of it will help. It will make those systems smarter and
be able, you know, it will allow us to communicate with them and teach them things and
they're going to be able to teach us things and stuff like that. But this is a different question
really. The question of how we organize AI research going forward, which is somewhat determined by
how afraid people are of the consequences of AI. So, if you have a rather positive view of the
impact of AI on society and you trust humanity and society and democracies to use it in good ways,
then the best way to make progress is to open research. And for the people who are afraid of
the consequences, whether they are societal or geopolitical, they're putting pressure on
governments around the world to regulate AI in ways that basically limit access, particularly of
open source code and things like that. And it's a big debate at the moment. I'm very much on the
side. So, he's met up very much on the side of open research. Yeah, actually, that was something I
was going to ask you, and now that you've brought it up, because I've been talking to people about
this. And there is a view that aside from the risks of open source, you know, again, Jeff Hinton
saying, you know, would you open source thermonuclear weapons? Aside from that is the question of as
to whether open source can marshal the resources to compete with proprietary models. And because of
the tremendous resources required for when you're scaling these models. And there's a question as to
whether or not Meta will continue to open source future versions of Lama or not continue to open
source, but whether it'll continue to invest the resources needed to push the open source models.
So what do you think about that? Okay, there's a lot to say about this. Okay, so first thing is,
there's no question that Meta will continue to invest the resources to build better and better AI
systems because it needs it for its own products. So the resources will be invested. Now the next
question is, do you, you know, will we continue to open source the base models? And the answer is,
you know, probably yes, because that creates an ecosystem on top of which an entire industry can
be built. And there is no point, you know, having 50 different companies, building proprietary
closed systems when you can have, you know, one good base open source base model that everybody
can use. It's wasteful. And it's not a good idea. And another reason for having open source models
is that it, it, nobody has no entity as powerful as it thinks it is, as a monopoly on good ideas.
And so if you want people, we can have good new innovative ideas to contribute. You need an open
source platform. If you want the academic world to contribute, you need open source platforms.
If you want the startup world to be able to build customized products, you need open source base
models because they don't have the resources to, to build, to train large models, right? Okay. And
then there is the history that shows that for, for foundational technology for infrastructure type
technology, open source always wins, right? It's true of the software infrastructure of the internet.
In the early 90s and mid 90s, there was a big battle between Sun macro systems and Microsoft to
produce the, deliver the software infrastructure of the internet, you know, operating systems,
web servers, web browsers, and, and, you know, various servers aside and client-side frameworks,
right? They both lost. Nobody is talking about them anymore. The entire world is
of, of the web is using Linux and Apache and MySQL and JavaScript and, and, you know, and even the,
the, the basic core code for, for web browser is open source. So open source won by a huge margin.
Why? Because it's safer, gathers more people to contribute. All the features are unnecessary.
It's more reliable. Venerabilities are fixed faster. And, and it's customizable. So anybody can
customize Linux to run on whatever hardware they want, right? So open source wins.
Same for AI. It's going to be the same thing. It's inevitable. The people now who are climbing
up like open AI, their, their system is based on publications from all of us. Sure. And from
open platforms like, like PyTorch. Yeah. It's built using PyTorch. PyTorch was produced originally
by Meta. Now it's owned by the Linux foundation. It's open source. They've contributed to it,
by the way. You know, their LLM is based on transformer architectures invented at Google.
Yeah. All the tricks to kind of train all those things came out of like various papers
from all kinds of different institutions, including academia or the fine tuning techniques.
Same. So nobody works in a vacuum. The thing is nobody can keep their advance and their advantage
for very long if they are secretive. Yeah. Except that with these models, because they're
so compute intensive and they cost so much money to train, you need somebody like Meta that who's,
who's going to be willing to build them and open source them. And that's why I was, when I was
asking whether they'll continue, obviously Meta will continue building, you know, resource
intensive models. But the question is whether they'll continue to open source. I mean, if I tell
you the only reason why Meta could stop open sourcing models are legal. So if there is a law
that adds laws, open source AI systems above a certain level of sophistication, then of course
we can do it. If there are laws that in the US or across the world makes it illegal to use public
content to train AI systems, then it's the end of AI for everybody, not just for the open source.
Okay. So, or at least the end of the type of AI that we are talking about today might have,
you know, new AI in the future, but that don't require as much data. So the, and then there is,
you know, liability if you, if you, if you, if you believe in the kind of that someone doing
something bad with an AI system that was open sourced by Meta, then Meta is liable, then Meta
will have a big incentive not to release it, obviously. So it's the entire question about this
is around legal reasons and political decisions. But on the idea of open source winning, don't
you need more people or more companies like Meta building the foundation models and open sourcing
them? Or could it be an open source ecosystem win based on a single company building the models?
No, I mean, you need two or three. And there are two or three, right? I mean, there is this hugging
phase. There is Mistral in France, who's also embracing open source LLM. They're very good
LLM. It's a small one, but it's very good. There is, you know, academic efforts like Lyon.
They don't have all the resources they need, but they, you know, they collect the data that is used
by everyone. So everybody can contribute. One thing that I think is really important to understand
also is that there is a future in which I described earlier in which every one of us, every one of
our interactions with the digital world will be mediated by an AI assistant. And this is going
to be for true for everyone around the world, right? Everyone who has any kind of smart device.
Eventually it's going to be in our augmented reality glasses, but, you know,
by the time being in our smartphones, right? And so imagine that future where, you know, you are,
I don't know, from Indonesia or Senegal or France. And your entire digital diet is done through the
mediation of an AI system. Your government is not going to be happy about it. Your government
is going to want the local culture to be present in that system. It doesn't want that system to be
closed sourced and controlled by a company on the west coast of the US. So just for reasons of
preserving the diversity of culture across the world and not having or entire information
diet being biased by whatever it is that some company on the west coast of the US states,
there's going to need to be open source platforms. And they're going to be predominant in,
at least outside the US for that reason, including China, right? There is all those talks about,
oh, what if China puts their hands on our open source code? I mean, China wants control over
its own LLM because they don't want their citizen to, you know, have access to a certain type of
information. So they're not going to use our LLMs. They're going to train theirs that they already
have. And nobody is, you know, particularly ahead of anybody else by more than about a year.
Yeah. And China is pushing open source. I mean, they're very pro open source within their
ecosystems. Some of them, you know, it's, there's no like unified opinion there. But
I mean, it's the same in the West, right? There are some governments that are too afraid of the
risks. And then, or I thinking about it and some others that are all for open source,
because this is the only way for them to have any influence on the
type of information and culture that would be mediated by those systems. So it's going to have
to be like Wikipedia, right? Wikipedia, you know, is built by millions of people who contribute to
or from all around the world in all kinds of languages. Okay. And it has a system for sort
of vetting the information. The way AI systems of the future will be taught and will be fine
tuned will have to be the same way will have to be quite sourced. Because something that
matters to a farmer in Southern India is probably not going to be taken into account by the fine
tuning done by, you know, some some company on the west coast of the US. AI might be the most
important new computer technology ever. It's storming every industry and literally billions
of dollars are being invested. So buckle up. The problem is that AI needs a lot of speed and
processing power. So how do you compete without cost spiraling out of control? It's time to upgrade
to the next generation of the cloud. Oracle Cloud Infrastructure or OCI. OCI is a single
platform for your infrastructure, database, application development and AI needs. OCI has
four to eight times the bandwidth of other clouds, offers one consistent price instead of variable
regional pricing. And of course, nobody does data better than Oracle. So now you can train your AI
models at twice the speed and less than half the cost of other clouds. If you want to do more and
spend less, like Uber, eight by eight and Databricks Mosaic, take a free test drive of OCI at
oracle.com slash I on AI. That's E Y E O N A I all run together oracle.com slash I on AI.
That's it for this episode. I want to thank Yen for his time. If you want to read a transcript
of this conversation, you can find one on our website I on AI. That's E Y E hyphen O N dot AI.
And remember, the singularity may not be near, but AI is changing your world. So best pay attention.

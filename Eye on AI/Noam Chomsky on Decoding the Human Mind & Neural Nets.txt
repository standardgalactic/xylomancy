What's called AI today has departed to basically pure engineering.
It's designed in such a, the large language models are designed in such a way that in
principle, they can't tell you anything about language, learning, cognitive
processes generally, they can produce useful devices like what I'm using, but
the very design ensures that you'll never understand, they'll never lead
to any contribution to science.
That's not a criticism anymore than I'm criticizing.
This week, I talked to Noam Chomsky, one of the preeminent intellectuals of our
time, our conversation touched on the dichotomy between understanding and
application in the field of artificial intelligence.
Chomsky argues that AI has shifted from a science aimed at understanding
cognition to a pure engineering field focused on creating useful, but not
necessarily explanatory tools.
He questions whether neural nets truly mirror how the brain functions and
whether they exhibit any true intelligence at all.
He also suggests that advanced alien life forms would likely have language
structured similar to our own, allowing us to communicate.
Chomsky is 94, and I reached him at home where he appeared with a clock hanging
omnestly over his head.
I hope you enjoy the conversation as much as I did.
Well, thanks.
You're in California?
Actually, I'm in Arizona, which is on California time.
Yeah.
Yeah.
Oh, wonderful.
Yeah.
So, you know, I wanted to talk to you because you have the, you know, one of
the few people with a deep understanding of linguistics and natural
language processing that has the historical knowledge of where we are,
how we got to where we are, and what that might mean for the future.
I understand your criticisms of deep learning and what large language
models are not in terms of reasoning and understanding the underpinnings of language.
But I thought maybe I could ask you to talk about how this developed.
I mean, going back to Minsky's thesis at Princeton, before he turned against
the perceptron when he was talking about nets as a possible model for biological
processes in the brain, and then, you know, how did, how you see that things
developed and what were the failures that didn't get to where, presumably, you
would have wanted that research to go.
And then, and then I have some other questions, but, but, but is that enough
to get started?
Well, let's, let's take an analogy.
Suppose you're interested in figuring out how insects navigate biological
problem. So one thing you can do is say, let's try to study in detail what the
desert ants are doing in my backyard, how they're using solar azimuth, so on and so
forth. Something else you could do is say, look, it's easy.
I'll just build an automobile, which can navigate fine, does better than the
desert ants. So who cares?
Well, those are the two forms of artificial intelligence.
One is what Minsky was after.
It's now kind of ridiculed as good old fashioned AI, go fi, were past that stage.
Now we just build things that do it better.
Okay.
Like an airplane does better than an eagle.
So who cares about how eagles fly?
Yeah, it's possible, but it's a difference between totally different goals.
Roughly speaking, science and engineering, it's not a sharp difference, but first
approximation. Either you're interested in understanding something, or you're just
interested in building something that'll work for some purpose.
So they're both fine occupations, nothing wrong with, I mean, when you say I'm
criticism of the large, criticizing the large language models, that's not correct.
I'm using them right now.
I'm reading captions.
Captions are based on deep learning, clever programming, very useful.
I'm hard of hearing, so they're very helpful to me.
No criticism.
But if somebody comes along and says, okay, this explains language, you tell them
it's kind of like saying an airplane explains how eagles fly.
The wrong question.
It's not intended to lead to any understanding.
It's intended to be for a useful purpose.
That's fine.
No criticism.
And what's called AI today has departed to basically pure engineering.
It's designed in such a, the large language models are designed in such a way that
in principle, they can't tell you anything about language, learning, cognitive
processes generally, they can produce useful devices like what I'm using, but
the very design ensures that you'll never understand, they'll never lead to any
contribution to science.
That's not a criticism anymore than I'm criticizing champions.
Jeff Hinton says, you know, his goal is to understand the brain, how the brain works.
And he talks about AI as we know it today, supervised learning and generate
an AI as useful byproducts, but that are not his goal or not the goal of cognitive
science or computational biology.
Was there a point at which you think the research lost a bead or is there research
going on that people aren't paying attention to that that is not caught up in the
usefulness of these other kinds of neural nets?
Well, first of all, if you're interested in how the brain works, the first
question you ask is, does it work by neural nets?
That's an open question.
There's plenty of critical analysis that argues that neural nets are not what's
involved, even in simple things like memory.
Actually, these arguments that go back to Helmholtz, the neural transmission is
pretty slow as compared with the ordinary memory.
There's much sure for criticism by people like Randy Gallister, cognitive
neuroscientist, is given pretty sound arguments that neural nets in principle
don't have the ability to capture the core notion of a Turing machine,
computational capacity.
They just don't have that capacity.
And he's argued that the computational capacity is in much richer computational
systems in the brain, internal delves, where there's very rich computational
capacity goes wavy on neural net.
Some experimental evidence to support this.
So if you're interested in the brain, that's the kind of thing you look at.
Not just saying, can I make bigger neural nets?
It's okay if you want to try it, but maybe it's the wrong place to look.
So the first question is, is it even the right place to look?
That's an open question in neuroscience.
If you take a vote among neuroscientists, almost all of them think that neural nets
are the right place to look.
But you don't solve scientific questions by a vote.
Yeah, one of the things that's obvious is neural nets, they may be a model,
they may mimic a portion of brain activity, but there are so many other structures.
There's all kind of stuff going on in the brain, way down to the cellular level.
There's chemical interactions, plenty of other things.
So maybe you'll learn something by studying neural nets.
If you do, fine, everybody will be happy.
But maybe that's not the place to look if you want to study even simple things like just
memory and associations.
There's now already evidence of associations internal to large cells in the hippocampus,
internal to them, which means maybe something's going on at a deeper level where there's vastly
more computational capacity.
Those are serious questions.
So there's nothing wrong with trying to construct models and see if you can learn something from
them, if you can, fine.
The building larger models, which is kind of the rage in the engineering side of AI right now,
does produce remarkable results.
I mean, what was your reaction when you saw
in chat GPT or GPT-4 or any of these models that it's just a sort of clever stochastic
parrot or that there was something deeper?
If you look at the design of the system, you can see it's like an airplane explaining flying.
There's nothing to do with it.
In fact, it's immediately obvious, trivially obvious, not a deep point, but it can't be
teaching us anything.
The reason is very simple.
The large learning models work just as well for impossible languages that children can't acquire
as for the languages they're trained on.
So it's as if a biologist came along and said, I got a great new theory of organisms, lists a
lot of organisms that possibly exist, a lot that can't possibly exist, and I can tell you
nothing about the difference.
I mean, that's not a contribution to biology.
It doesn't meet the first minimal condition.
The first minimal condition is distinguish between what's possible from what's not possible.
You can't do that.
It's not a contribution to science.
If it was a biologist making that proposal, you just left.
Why shouldn't we just laugh when an engineer from Silicon Valley says the same thing?
So maybe they're fun.
Maybe they're useful for something.
Maybe they're harmful.
Those are the kinds of questions you ask about pure technology.
So take large language models.
There are something they're useful.
In fact, I'm using them right at this minute.
They're very helpful for people like me.
Are they harmful?
Yeah, they can cause a lot of harm.
Disinformation, defamation, preying on human gullibility.
Plenty of examples.
So they can cause harm.
They can be of use.
Those are the kinds of questions you ask about pure engineering,
which can be very sophisticated and clever.
I mean, the internal combustion engine is a very sophisticated device,
but we don't expect it to tell us anything about how a gazelle runs.
It's just the wrong question.
Yeah, although I talk a lot to Jeff Hinton, and you'll be the first to concede that back propagation
there's no evidence of that.
And in fact, there's a lot of evidence that it wouldn't work in the brain.
Reinforcement learning.
You know, I've spoken to Rich Sutton.
That's been accepted by a lot of people as an algorithmic model for brain activity in
part of the brain, in the lower brain.
So in terms of exploring the mechanisms of the brain, it seems that there is some usefulness.
I mean, it says, you said there's, on the one hand, people look at the principles,
and then they built through engineering, just as the analogy of a bird to an airplane,
they've taken some of the principles and applied it through engineering and created something useful.
But there are scientists that are looking at what's been created, like Hinton's criticism
of back propagation, and are looking for other models that would fit with the principles they see
in cognitive science or in the brain.
And I mentioned this forward-forward algorithm, which you said you hadn't looked at.
But, you know, I found it compelling in that it doesn't require, you know,
signals to be passing back through the neurons.
I mean, they pass back, but then stimulate other neurons as you move forward in time.
But, I mean, is there nothing that's been learned in the study of AI or the research of neural nets?
But if you can find anything, it's great.
Nothing against search, you know.
But it's just, we have to remember what you asked about chatbots.
What do we learn from them?
Zero. For the simple reason that the systems work as well for impossible languages as for possible ones.
So it's like the biologist with the new theory that has organisms and impossible ones and can't
tell the difference. Now, maybe by the look at these systems, you learn something about possible
organisms. Okay, great. All in favor of learning things. But there's no issues.
It's just that the systems themselves, there are great claims by some of the leading figures in
the field. We've solved the problem of language acquisition, namely zero contribution, because
the systems work as well for impossible languages. Therefore, they can't be telling you anything
about language acquisition period. Maybe they're useful for something else. Okay, let's take a look.
Well, maybe for the audience that this is going out to, you know, I understand what you mean by
impossible, impossible. But could you just give a great, a brief synopsis of what you mean by
impossible languages for people that haven't read your work?
Well, I mean, there are certain general properties that every infant knows already tested down to
two years old. No evidence, couldn't have evidence. So one of the basic properties of language is
that the linguistic rules apply to structures, not linear strings. So if you want to take a
sentence like instinctively birds that fly swim, it means instinctively they swim,
not instinctively they fly. Well, the adverb instinctively has to find a verb to attach to.
It skips the closest verb and finds the structurally closest ones. That principle turns out to be
universal for all structures, all constructions and all languages. What it means is that an infant
from birth, as soon as you can test automatically, disregards linear order and disregards 100% of
what it hears. Notice, as all we hear is words and linear order, but you disregard that and you
deal only with abstract structures in your mind, which you never hear. Take another simple example.
Take the friends of my brothers are in England. Who's in England? The friends are the brothers.
The friends, not the brothers, the one that's adjacent. You just disregard all the linear
information. It means you disregard everything you hear, everything, and you pay attention only to
what your mind constructs. That's the basic, most fundamental property of language. Well,
you can make up impossible languages that work with what you hear. Simple rule. Take the first
relevant thing, associate them. Friends of my brothers are here. Brothers are the closest things
and the brothers are here. Trivial rule, much simpler than the rule we use. You can construct
languages that use only those simple rules that are based on the linear order of what we hear.
Well, maybe children, people could acquire them as a puzzle somehow using non-linguistic capacities,
but they're not what children, infants, reflexively construct with no evidence.
There's many things like this. Impossible and impossible languages. Well, nobody's tried it
out because it's too obvious how it's going to turn out. You take a large language model, apply it
to one of these systems that uses linear order. Of course, it's going to work fine. Trivial rules.
Well, that's a refutation of the system. Meaning that if you trained it on an
impossible language, it would produce impossible languages. How would you mean?
You don't even have to train it because the rules are simple. Rules are much simpler than the rules
of language. Like taking things that are, take the example the friends of my brother are here.
The way we actually do it is we don't say take the noun phrase that's closest. We don't do that.
That would be trivial, but we don't do it. What we say is first construct the structure in your mind,
friends of my brothers, then figure out that the central element in that structure is friends,
not brothers. Then let's let it be talking about the head of it. It's a pretty complicated
computation, but that's the one we do instantaneously and reflexively. We ignore and we never see it
here, remember? We don't hear structures. All we hear is words in linear order. What we hear
is words in linear order. We never use that information. We use only the much more looks
like complex. If you think about it computationally, it's actually simpler, but that's a deeper
question, which is why we do it. To move to a different dimension, there's a reason for this.
The reason has to do with the theory of computation. You're trying to construct
an infinite array of structured expressions. Simplest way to do that, the simplest computational
procedure is binary set formation, but if you use binary set formation, you're just going to get
structures, not order. What the brain is doing is the simplest computational system, which happens
to be much harder to use. Nature doesn't care about that. Nature constructs the simplest system,
doesn't care about it. If it's hard to use or not, I mean, nature could have saved us a lot of
trouble if it had developed eight fingers instead of 10, then we'd have a much better base for
computation, but nature didn't care about that when it developed 10 fingers. If you look at evolution,
pays no attention to function. It just constructs the best system at each point.
There's a lot of misleading talk about that, but if you just think about the physics of evolution,
say a bacterium swallows another organism, the basis for what became complex cells,
and nature doesn't get the new system, it reconstructs it in the simplest possible way.
It doesn't pay any attention to how complex organisms are going to behave,
not what nature can do, and that's the way evolution works all the way down the line.
So, not surprisingly, nature constructed language so that it's computationally elegant,
but dysfunctional, hard to use in many ways, not nature's problem, just like every other aspect
of nature. You can think of a way in which you can do it better, but it didn't happen stage by stage.
Two questions from that. Your view is that artificial intelligence, as it's being called,
and particularly generative AI, doesn't exhibit true intelligence. Is that right?
I wouldn't even say that. It's irrelevant to the question of intelligence. It's not its problem.
A guy who designs a jet plane is not trying to answer the question, how do eagles fly?
So, to say, well, it doesn't tell us how eagles fly is the wrong question to ask. It's not the goal.
Except that what people are struggling with right now, you've heard the existential
threat argument that these models, if they get large enough, they'll actually be more intelligent
than humans. That's science fiction. I mean, there is a theoretical possibility.
You can give a theoretical argument that, in principle, a complex system with vast search
capacity could conceivably turn into something that would start to do
things that you can't predict, maybe beyond. But that's even more remote than some
distant asteroid, maybe someday hitting the earth. Yeah, it could happen. I mean,
if you read serious scientists on this, like Max Tagmark, his book on the
Three Levels of Intelligence, he does give a sound theoretical argument as to how a
massive system could, say, run through all the scientific discoveries in history,
maybe find out some better way of developing them and use that better way to design something new
which would destroy us all. Yeah, it's, in theory, possible. But it's so remote from
anything that's available that it's a waste of time to think about it.
Yeah, so your review is that whatever threat exists from generative AI, it's the more
mundane threat of disinformation. Disinformation, defamation,
gullibility, Gary Marcus has done a lot of work on this, real cases. Those are problems.
I mean, you may have seen that there was sort of as a joke, people, somebody developed
defamation of the pope, put an image of the pope, somebody could do it for you,
duplicate your face so it looks more or less like your face, pretty much duplicate your voice,
develop a robot that looks kind of like you. Have you say some insane thing?
It would be hard, only an expert could tell whether it was you or none. It's like,
there's going to be, it was done already several times, but basically it's a joke.
When powerful institutions get started on it, it's not going to be a joke.
Yeah. Another argument that's swirling around these large language models is the question of
sentience of whether if the model is large enough, and this goes a little bit back to how
there's a lot more going on in the brain than the neural network or the cerebral cortex, but
that there is the potential for some kind of sentience, not necessarily equivalent to human
sentience. These are vacuous questions. It's like asking, does a submarine really swim?
You want to call that swimming? Yeah, it swims. You don't want to call it swimming? It's not a
substantive question. Well, in the sense that it supports the view that there is no separation
between consciousness and the material activities of the brain. There's a separation that hasn't
been believed since the 17th century. John Locke, after Newton's demonstration, said,
well, leaves us only with the possibility that thinking is some property of organized matter.
That's the 17th century. Yeah. Okay.
But the belief in a soul and consciousness is something separate from the material
biology. People believe in all kinds of things, but within the rational part of the human species,
once Newton demonstrated that the mechanical model doesn't work,
there's no material universe in the only sense that was understood. Locke took the obvious conclusion,
said, well, since matter, as Mr. Newton has demonstrated, has properties that we cannot
conceive of. They're not part of our intuitive picture. Since matter has those properties,
organized matter can also have the property of thought. This was investigated all through the
18th century. Ended up finally with Joseph Priestley, chemist, philosopher. Late 18th
century gave pretty extensive discussions of how material, organized material objects could have
properties of thought. You can even find it in Darwin's early notebooks. It was kind of forgotten
after that, rediscovered in the late 20th century as some radical new discovery,
astonishing hypothesis. Matter can think. Of course it can. In fact, we're doing it right now,
but the only problem then is to find out what's involved in what we call thinking,
what we call sentience, what are the properties of whatever matter is. We don't know what matter
is, but whatever it turns out to be, whatever constitutes the world, what physicists don't
know, but whatever it is, there's something organized. Elements of it can have various properties,
like the properties that we are now using, properties that we call sentience. Then the
question whether something else has sentience is as interesting as whether airplanes fly.
If you're talking English, airplanes fly. If you're talking Hebrew, airplanes glide,
they don't fly. It's not a substantive question. Just what metaphors do we like?
But what you're saying then is that neural net may not be the engineering solution, but
that eventually it may be possible to create a system outside of the human brain that can think
whatever thinking means. You can do what we call thinking. But whether it thinks or not
is like asking the airplanes fly, not a substantive question. We shouldn't waste time on questions
that are completely meaningless. Going back to the history then,
you know, Minsky was very interested in the possibility of neural nets as a
computational model. In Minsky's time, it looked as if neural nets were the right place to look.
Now I think it's not so obvious, especially because of Galastal's work,
which is not accepted by most neuroscientists, but seems to me pretty compelling.
Can you talk a little bit about that because I haven't read that and I'm guessing our readers
haven't, our listeners haven't. Galastal is not the only one. Roger Penrose is another
Nobel Prize winning physicist, but a number of people have pointed out Galastal mostly that
have argued, I think plausibly, that the basic component of a computational system,
the basic element of essentially a Turing machine, cannot be constructed from neural nets.
So you have to look somewhere else with a different form of computation. And he's also
pointed out, but in fact, it's true that there's much richer computational capacity in the brain
than neural nets, even internal to a cell. There's massive computational capacity
intercellular. So maybe that's involved in computation. And then there's by now some
experimental work, I think, giving some evidence for this, but it's a problem for neuroscientists
to work on. And I, you know, I'm not an expert in the field. I'm looking at it from the outside,
so don't take my opinion too seriously. But to me, it looks pretty compelling. But whatever it is,
neural nets or something else, yes, some organization of them, of whatever is there,
is giving us the capacity to do what we're doing. So if you're a scientist, what you do is
approach it in two different ways. One is you try to find the properties of the system.
What is the nature of the system? That's first step kind of thing I was talking about before with
structure dependent. What are the properties of the system that an infant automatically
develops in the mind? And there's a lot of work on that. From the other point of view, you can say,
what can we learn about the brain that relates to this? Actually, there is some work. So there is
neurophysiological studies which have shown that for artificial languages that violate the principle
that I mentioned, this structure dependent principle, if you train people on those,
the ordinary language centers don't function. You get diffuse functioning of the brain,
means they're being treated as puzzles basically. So you can find some neurological correlates of
some of the things that are discovered by looking at the nature of the phenotype.
But it's very hard for humans for a number of reasons. We know a lot about human, the physiology
of human vision. But the reason is because of invasive experiments with nonhumans, cats,
monkeys, and so on. Can't do that for language. There aren't any other organisms unique to humans.
So there's no comparative studies. You can think of a lot of invasive experiments which teach you a
lot. You can't do them for ethical reasons. So study of the neurophysiology of human cognition
is a uniquely hard problem. In its basic elements like language, it's just unique to the species.
And in fact, a very recent development in evolutionary history, probably the last couple
hundred thousand years, which is nothing. So you can't do the invasive experiments for ethical
reasons. You can think of them, but you can't do them, fortunately. And there's no comparative
evidence. So it's much harder to do. You have to do things like, you know, looking at a blood flow in
the brain, MRI, electrical stimulation, looking from the outside. It's tough. It's not like doing
the kind of experiments you can think of. So it's very hard to find out the neurophysiological
basis for things like use of language. But it's one way to proceed. And the other way to proceed
is learn more about the phenol. It's like chemistry for hundreds of years. You just postulated the
existence of atoms. Nobody could see them. You know, why are they there? Because unless
they're atoms with the Dalton's properties, you don't explain anything. Early genetics,
early genetics work before anybody had any idea what a gene is. You just looked at the
properties of the system, try to figure out what must be going on. It's the way astrophysics works,
you know, most of science works like that. So this does too.
When you talk about invasive exploration, there are tools that are increasingly
sophisticated. I'm thinking of neural link, Elon Musk's startup that has these super fine
electrodes that can be put into the brain without damaging individual neurons.
There's actually, I think, much more advanced than that is work that's being done with
patients under brain surgery. Under brain surgery, with the brain basically exposed,
there are some noninvasive procedures that can be used to study what particular
parts of the brain, even particular neurons are doing. It's very delicate work. But there is some
work going on. One person is working on it is Andrea Moro, the same person who designed the
experiments that I described before about impossible languages. That seems to me a promising direction.
There's other kinds of work. I could mention some of it. Alec Morance,
why you was doing interesting studies that shed some light on the very elementary
function. How do words get stored in the brain? What's going on in the brain that
tells us that blake is a possible word, but panic isn't for an English speaker. It is for
an Arabic speaker. What's going on in the brain that deals with that?
Hard work. David Peppel, another very good neuroscientist, has found evidence
for things like phrase structure in the brain. But the kinds of invasive experiments you can
dream of, you can think of, you're just not allowed to do. So you have to try it in much
indirect ways. Do you think that understanding cognition has advanced in your lifetime?
And are you hopeful that we'll eventually really understand how the brain thinks?
Well, there's been vast improvement in understanding the phenotype that we know a great deal about
that was not known even a few years ago. There's been some progress in the neuroscience of
the relates to it, but it's much harder. I'm just curious about where you are in,
not physically you're in Arizona, but where you are in your thinking. Are you still
pushing forward in trying to understand language in the brain? Or are you sort of retired, so to speak,
at this point? Very much involved. I mean, I don't work on the neurophysiology.
A man I mentioned, Andrea Moro, happens to be a good friend. So I follow the work they're doing.
We interact, but my work is just on the phenotype. What's the nature of the system?
And there, I think we're learning a lot. I'm right in the middle of papers at the moment,
looking at more subtle, complex properties. The idea is essentially to find
what I said about binary set formation. How can we show that from the simplest
computational procedures, we can account for the apparently complex and apparently varied
properties of the language systems. There's a fair amount of progress on that,
that was unheard of 20, 30 years ago. So this is all new. Understanding is one thing. And then
recreating it through computation in external hardware is another. Is that
a blind alley, or do you think that? Well, at the moment, I don't see any particular point in it,
if there is some point, okay. I mean, the kinds of things that we're learning about the nature of
language, I suppose you could construct some sort of system that would duplicate them,
but it doesn't seem any obvious point to it. It's like taking chemistry in 100 years ago and saying,
can I construct models that will look sort of like, suppose you took, I was saying, a
Kekele diagram for an organic molecule and study its properties. You could, presumably,
construct a mechanical model that would do some of those things. Would it be useful?
Apparently chemists didn't think so, but if it would, okay, if it wouldn't and don't.
Nonetheless, I mean, we are using neural nets, even in this call.
Do you see, I mean, setting inside the question of whether or not they help us understand anything
about the brain. Are you excited at all about the promise that these large models hold? I mean,
because they do something very useful. They are. Like I said, I'm using it right now.
I think it's fine for me, somebody who can't hear to be able to read what you're saying.
Yeah, pretty accurately. It's an achievement, so great. I have nothing against technology.
And who do you think is going to carry on your work from here? I mean, are there any
students of yours who you think we should be paying attention to?
Well, quite a lot. A lot of young people doing fine work. In fact, I work closely with a
small research group by now, spread all over the world. We meet virtually from Japan and Holland
and other places regularly working on the kinds of problems I was talking about.
But right now, I should say, it's a pretty special interest. Most linguists aren't interested in the
these foundational questions. But I think that's happens to be my interest. I want to
just see if we can show the ultimately try to show that language is essentially a natural object.
I mean, there was an interesting paper written about the time that I started working on this
by Albert Einstein in 1950. He had an article in Scientific American, which I read, but
didn't appreciate at the time, began to appreciate later, in which he talked about what he called
a miracle creed. He has an interesting history. It goes back to Galileo. Galileo had a maxim
saying nature is simple. It doesn't do things in a complicated way. If it could do them in a
simple way, Galileo's maxim couldn't prove it. But they said, I think that's the way it is.
That's the task of the scientist to prove it. Well, over the centuries, it's been substantiated
case after case. It shows up in Leibniz's principle of optimality. But by then, there was a lot of
evidence for it. By now, it's just a norm for science. It's what Einstein called the miracle
creed. Nature is simple. Our task is to show it. It says, improve it. Skeptic can say, I don't
believe it. Okay. But that's the way science work. Well, the science work the same way for language.
But you couldn't have proposed that 50 years ago, 20 years ago. I think now you can
think that maybe language is just basically a perfect computational system at its base.
You look at the phenomena, it doesn't look like that. But the same was true of biology. Go back to
the 1950s, 1960s, biologists assumed that organisms could vary so widely that each one has to be
studied on its own without bias. By now, that's all forgotten. It's recognized that there,
since the Cambrian explosion, there's virtually no variation in the kinds of organisms,
fundamentally all the same. Deep homologies, and so on. So even been proposed that there's a universal
genome, not totally accepted, but not considered ridiculous. Well, I think we're in the same
direction as a study of language. Now, let me say again, there's not many linguists interested in
this. Most linguists like most biologists are studying particular things, which is fine. You
learn a lot that way. But I think it is possible now to formulate a plausible thesis that language is
a natural object like others, which evolved in such a way as to have perfect design,
but to be highly dysfunctional, because that's true of natural objects generally. It's part of
the nature of evolution, which doesn't take into account possible functions. I mean, the last stage
of evolution, the reproductive success that does take function into account, natural selection,
that's a fringe of evolution. It's just the wherever old friends, very important, not denigrated, but
it's the basic part of evolution is constructing the optimal system that meets the physical
conditions established by some disruption in the system. That's the core of evolution. That's what
Turing studied. Darcy Thompson, others by now, I think it's understood. And I think maybe the study
of this particular biology after a language is a biological object. So why should it be different?
Let's see if we can show it. There's been a lot of talk in the news recently about
extraterrestrial craft having been found by the government. I don't put much stock in it, but
imagine that there is extraterrestrial life, advanced forms of life. Do you think that their
language would have developed the same way if it's based on these simple principles? Or is it,
could there be other forms of language in other biological organisms that would be quote unquote
impossible in the human context? Back around the 1960s, I guess, Minsky
studied with one of his students, Daniel Boebrun, studied the simplest Turing machines,
few estates, fewest symbols, and asked what happens if you just let them run free?
Well, it turned out that most of them crash, either get into endless loops or just crash
don't proceed. But the ones that didn't crash all produced the successor function.
So he suggested what we're going to find if any kind of intelligence develops is
it'll be based on the successor function. And if we want to try to communicate with some
extraterrestrial intelligence, we should first see if they have the successor function
and then maybe build up from there. Well, in terms of the successor,
happens to be what you get from the simplest possible language. The language is one symbol
and the simplest form of binary set formation basically gives it a successor function.
Add a little bit more to it, you get something like arithmetic. Add a little bit more to it,
you get something like the poor properties of language. So it's conceivable that if there is any
extraterrestrial intelligence, it would have pursued the same course. Where it goes from there,
we don't know enough to say. And back to the idea that there is no super natural realm,
that the consciousness is an emergent property from the physical attributes of the brain.
Do you believe in a higher intelligence behind the creation or continuation of the universe?
I don't see any point in vacuous hypotheses. If you want to believe it okay, it has no consequences.
But do you believe it? No, I don't see any point in believing things for which there's no evidence
and do no work. Yeah. And another thing I've always wanted to ask someone like you, clearly
your intelligence surpasses most peoples. I don't think so. Well, that's a good, that's interesting
that you would say that. You think it's just a matter of applying yourself to study throughout your
career. I have certain talents, I know, like not believing things just cause people believe them.
And keeping an open mind and looking for arguments and evidence, not
kind of thing we've been talking about when meaningless questions are proposed, like
our other organisms, sentient or the submarine swim, I say let's discard them and look at
meaningful questions. If you just pursue common sense like that, I think you can make some progress.
Same on the questions we're talking about language. If you think it through, there's every reason why
the organic object language should be an object. If so, it should follow the
general principles of evolution, which satisfy what Einstein called the miracle created. So why
shouldn't language? So let's pursue that CFR we can do. I think that's just common sense. Many
people think it's superior intelligence. I don't think so. That's it for this episode. I want to
thank Noam for his time. If you'd like a transcript of this conversation, you can find one on our
website, I on AI, that's EYE-ON.AI. In the meantime, remember, the singularity may not be near,
but AI is about to change your world. So pay attention.

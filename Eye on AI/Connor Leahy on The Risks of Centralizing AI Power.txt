What this whole open AI saga has shown us is that, I mean, obviously we can't have something like
this being developed by just like a handful of, you know, weird people, unaccountable billionaires
in the Bay Area. This is actually something I've been telling saying to people for years now.
Obviously, this is not the right governance structure. So the weirdest thing I would
tell them is technology should be a tool. It should not be a goal in and of itself.
Hi, I wanted to jump in and give a shout out to our sponsor, Netsuite by Oracle. I'm a journalist
and getting a single source of truth is nearly impossible. If you're a business owner,
having a single source of truth is critical to running your operations. If this is you,
you should know these three numbers. 36,000, 25, 1. 36,000 because that's the number of businesses
that have upgraded to Netsuite by Oracle. Netsuite is the number one cloud financial system,
streamlining, accounting, financial management, inventory, HR, and more. 25 because Netsuite
turns 25 this year. That's 25 years of helping businesses do more with less,
close their books in days, not weeks, and drive down costs. One, because your business is one
of a kind. So you get a customized solution for all of your KPIs in one efficient system
with one source of truth. Manage risk, get reliable forecasts, and improve margins.
Everything you need, all in one place. As I said, I'm not the most organized person
in the world, and there's real power to having all of the information in one place to make better
decisions. This is an unprecedented offer by Netsuite to make that possible.
Right now, download Netsuite's popular KPI checklist designed to give you consistently
excellent performance, absolutely free at Netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I,
all run together. Go to Netsuite.com slash I on AI to get your own KPI checklist. Again,
that's Netsuite.com slash I on AI, E-Y-E-O-N-A-I. They support us, so let's support them.
Hi, my name is Craig Smith, and this is I on AI. In this episode, I speak again with Connor Leahy.
He's the founder and CEO of a startup called Conjecture that's working on AI alignment.
Before that, he was one of the founders and leaders of a group called Eleuther AI
that built one of the world's first open source large language models.
Connor is concerned about AI safety, about where AI development is going, concerned about
the push towards artificial general intelligence, and has a lot of thoughts about what we should
be doing to control development so that we don't end up creating something that is harmful to
humanity. I talked to him particularly because I wanted to hear his thoughts on the open AI saga,
which highlighted for a lot of people the dangers of having such a small group of people
controlling such a fundamentally powerful technology today. I hope you find the conversation
as interesting as I did. I'm Connor. I'm currently the CEO of Conjecture, an AI company in London
focused on AI safety and building architectures for AI systems that are understandable and
controllable and various other things. I also do a bit of work in policy regulation, public messaging,
that kind of stuff. Before this, I was well known as one of the founders of Eleuther AI,
which was one of the first, if not the first, open source large language models, research,
building groups. Technically, even before that, I was someone who worked on open source
GPT-2 as I think maybe literally the first person. Last time we spoke was after the release of
chat GPT and GPT-4. You were very concerned as were a lot of people and a lot of people continue
to be about releasing these kinds of models into the public before having fully explored
the safety issues or without having adequate guidelines. It struck me during this open AI
saga that we have lived through the last week that having this kind of powerful technology
in the hands of a handful of people who have different agendas and can't get along is in
itself a security issue. I would think that having these models open source where everyone can
see the data they were trained on in particular because even Lama, I've learned, doesn't make
public the training data, but having the data open for all to see and having the weights
of the models open so people can improve them or play with them or whatever. That to me seems like
a much safer path than having proprietary models, but I wanted to hear how your thinking is involved
on that. In the 1940s and in the early 1950s, the Soviet Union built a closed city around
what would be known as the Mayak facility. The Mayak facility was the largest nuclear facility
for the Soviet Union in their breakthrough development attempt to build the nuclear bomb.
To give a bit of a flavor for what Mayak and similar facilities that existed throughout the
Soviet Union were like, there was a program where if you got caught by the secret police and you
were being sent to a gulag for the rest of your life, you would give them an option. Either you
go to Siberia, work yourself to death for the rest of your life, or you get sent to Mayak for only
three months. And if you serve your term, you're free. Sounds great, doesn't it? Well, no one survived
the three months. So what happened, of course, is that one of the first things that the Americans
developed while developing the bomb is the HEPA filter, which is a form of air filter that is
powerful enough to be able to filter radioactive material very sufficiently out of the air,
making it safe for the workers. The Soviets didn't bother developing HEPA filters, not really.
So a lot of people died. To this day, Lake Karakai, which is near the Mayak facility,
is one of the most radioactive places on earth, so much so that it is said that standing next to
it for an hour can kill a man. These are all just some fun facts that really have nothing to do
with the topic we're talking about today. It's a story about how some people who were pretty bad
people developed something or were working on something pretty dangerous, and then other people
got access to it, and also really bad things out because even worse people got access to it.
Now, from this, I don't conclude, oh, so we should have just had everyone develop plutonium.
That would have made it safer. This is not the draw from this conclusion from the story. Now,
nuclear power is obviously very different from AI in many factors. So why even bring it up?
I can ask the question in the other direction, though. You're talking about AI. You're talking
about AGI. So let's focus on AGI. I am not really interested in talking about the risks of
current-day models, like Chatchapiti or something. We could talk about those too. They are real.
There are real risks from those, but they're not the kind where I think we have to stop all
publication necessarily. But let's talk about AGI systems. What reference class does AGI fall into?
Is it like open source? Is it like nuclear bombs? Is it like something different?
The reference class we choose forms our thinking around something which is fundamentally none of
those things. AGI is in nukes. AGI is in open source. It's not Linux. It's something very different.
While it has things in common with all of those things, AGI runs on computers. Linux runs on
computers. And it has other things that come with nuclear bombs. Nuclear bombs can kill everybody.
AGI can kill everybody. Those are things in common. Is it more like nukes? This is more like
open source. At some point, we have to actually drop down from the metaphors and into the actual
models of reality. So from my perspective, I think you're completely correct. What this whole
opening AGI saga has shown us is that obviously we can't have something like this being developed
by just a handful of weird people, unaccountable billionaires in the Bay Area. Obviously, they're
not acting in humanity's best interest to no one's surprise. A couple of months ago, Sam Altman
was interviewed about this. And he said, oh, yeah, the board can fire me at any time. If I go
about a mission, I think that's important. And then they try to fire him and he's back.
And well, that didn't work. So this is actually something that I've been telling,
saying to people for years now. I've gotten some disagreements. He did disagreements,
let us call them, with some of the people who were involved with the creation of the board
and or in favor of the existence of the board. And the point I always made to them was just like,
this obviously cannot control a charismatic billionaire, political mastermind. Like,
why the hell would you think it would? This is crazy. And this is exactly what we saw play out.
And I'm not even trying to make a comment on Sam Altman good, Sam Altman bad. I'm just saying,
obviously, he was not going to just say, oh, gosh darn, I guess the board said no more AI for me.
Guess I'm going to stop. That's not how men like him work. And that's obviously not what was going
to happen. And you think like the politically unsavvy nerds, you know, could like write a document
that would like convince someone like him to stop. No, of course not. So obviously,
this is not the right governance structure. I fully agree with this. But there's a great saying,
which is that reverse stupidity is not intelligence. If you take something stupid,
and you take the opposite of it, it's probably also stupid. And so this is so the fact that
this governance structure doesn't work for me does not say that therefore, there should be no
governance structure. This to me does not follow. Yeah. But but open source, I mean, you are,
I'm sure, very familiar with Yanlacun's argument that, yeah, that open sourcing can can lead to
to some abuse by bad actors, but by and large, the vast majority of people that will be
working on an open source model contributing to it or building products off of it will be doing so
with with, you know, not not without nefarious intent, and that the larger the open source
community, the quicker it would be able to respond to to bad actors or or misuse or or the more
of people available to to build guardrails and spot of weaknesses and that sort of thing. So
that that argument makes a lot of sense to me. I mean, at the beginning, when the, you know,
pause letter came out, and around the time that we talked, I thought, yeah, this stuff is too
dangerous to be open source. But but but I'm changing my mind. And I wanted to hear whether the
this this episode has changed your mind at all. I'll make three points in reaction to that. The
first one is a story. The second one is heuristic. And the third one is a true observation for my
own life. So first, the story. The story is that the smallpox virus genome is currently online.
You can go download it. It's a small text file. You can just go download it to your computer.
Another fact of the story is that a couple of years ago, Canadian scientists
recreated an extinct version of smallpox called horsepox. They revived it. They may and it was
functional and viable and infectious. And they published how to do it. Do you think either of
those things are good? Now you can argue, well, if we have more eyes on the smallpox virus,
then something something, you know, good things happen. But this isn't really a model. So this
brings me to the second point. The second point is offense versus defense. The way technologies
work is that some favorite offense, some favorite defense, very few are symmetric.
Most of the time, and most of the time offense wins. It is usually easier to destroy than it
is to protect. There are exceptions to this rule. For example, cartography is an interesting
exception where defense is easier than offense. But in most cases, it is easier to build a bomb
than it is to build a reactor, you know, a safe controlled burn. So all things being equal,
you should expect that if you have a technology and you distribute equally that there will be
more destruction. This is the default case. This is what you should expect by default. Most
technologies that are destroyed don't immediately give you a way to defend against it. Developing
vaccines is harder than developing bio weapons. It's much easier to crank out a bunch of bio weapons
and then you have to develop vaccines in response to that, which is already super hard because
who knows how far the virus already is. So just because the technology is why the aspect does
not mean it defends wins. Whether offense or defense wins is a property of reality. It's not
a property of your morals or of your ideology. And the third point is an observation from my own
life is that I used to work in open source. I was one of the very first people to work on it.
And I had similar views to Licken and, you know, assuming he holds these views genuinely,
which, you know, I hope he does. I don't know him very well. I've talked to him maybe once.
And I think this is just not even wrong. It's just, in my experience, what happens when you
build AI models and you release them open source is that the first thing that happens to get
upload to hugging face, and then a guy called the bloke, that's literally his name, uncensored
system, undoes any RLHF training or other security training that I've done trains them all the newest
data to make them more powerful, more general, more whatever, uploads them again, 4chan downloads
it down, you know, uses them for whatever their applications are, whether it's, you know, pornography
mostly, or likes BAM or whatever, etc. And now maybe this is fine, right? Like, you know,
you know, maybe we say, oh, it's okay. If people want to use their LLMs for porn, so what? That's
okay. Sure. What I'm saying is, is the empirical observation is that the amount of effort that
gets put into making these things safer or more controllable is absolutely pathetic compared
to the amount of effort that the open source community puts into making these things more
powerful, more general, and less controllable. This is just an empirical fact. This is just
if you go online, you pick the top 1000 LLM repos, how many of them are about controlling the models
better versus making them faster, making them more efficient, distilling them, making them more
etc. And the fact is that the offense, like the unbalance here is like, it's not even funny.
And I understand, right? And this is not to say that the people working on this technology are
like morally evil. I think this is an important thing to understand. There's an incentive from
people like Lacan and other like big tech, you know, people like talking heads to try to focus
on it's only the evil people's fault because that absolves them of responsibility. META
wants open source because it absolves them of responsibility as a corporation. They can't get
sued because it was the user's fault. And this is also what's happening in the EU AI Act right now
is that people like Lacan are lobbying to remove foundation models from regulation in the EU.
And saying it said their uses should be regulated. This is the same thing as when, for example,
plastic companies invented recycling. They invented it so that it was the user's fault
that there is all this plastic pollution. Like, oh, see, we would have recycled it.
But unfortunately, the users just didn't do it. This is a, this is gas lighting. And this is a
complete unbalance of power. The externalities of plastic pollution should be on the produce,
the ones who are most suited to addressing this externality, who are creating this externality.
It shouldn't be on the user. And the same thing obliged foundation models is that these systems
can do things. They can be used for many things. And we should be taking the big companies building
these systems. It's not like these open source models are being built by like, you know, plucky
little teenagers in their, in their, you know, rooms as a plucky teenager that did do that.
I'm saying most of the ones being built now are being made by like the UAE and meta. Like,
these aren't the little guys. These are big guys trying to shirk their responsibility to society.
Well, then what, what's the lesson from, from the open AI saga that, that you just need a bigger
board or you need the lesson is, is that none of these structures are correct. This is what we
have governments for. This is the same lesson that we've had over and over again. It's like
self-regulation does not work. It has never worked. Self-regulation. This is like tobacco
companies self-regulating themselves. This does not work. And we as a society have developed a
mechanism. I'm not saying it's a perfect mechanism by any means, but we do have a mechanism for
intra in intervening in systems that have extreme high externalities that are not self-regulatable.
And it's called the government. Yeah. And, and I mean, there has been a lot of work at the
government level, not as much in the US as in Europe. But, but how, how do I mean,
obviously these models are so commercially, the potential is so commercially
exciting that fines aren't going to matter. You're not going to be able to find people
to, to behave in ways that the government wants them to. There's got to be something stronger
than that. So, Jeff, you thought about, about that. I mean, how do you regulate these things?
One of my, one of the most inspiring moments from the history, I think, of science and society
is many decades ago, biologists and chemists and so on realized that human cloning should be possible.
Like, it should be possible to do this. They were still far from having the actual technology to
perform with human cloning, but they found out it should be possible. And they reasonably understood,
wait, that might be really disabilizing. Like, that could be, we don't know what the consequence
is, but maybe it's great. You know, maybe there's, you know, there's many benefits from human cloning
as well. But like, let's chill out. We don't know, like, we don't know, and this seems huge.
This doesn't, isn't just like another thing. This is not a 10% more effective, you know,
cough drop. Like, human cloning is a big deal. And so, heroically, long before the technology
existed, they came together and banned it and said, let's have a moratorium. Let's not do this
until we've had a bit more time to figure out what the hell we as a society want about this.
And it wasn't one board. This wasn't one CEO being like, I will, you know, take a moratorium on this.
No, it was the scientific community and governments coming together and working very,
very hard to create a moratorium. A moratorium is what we do when we are faced with something
which we know is huge and we don't know how to deal with. That's what scientists do. You have a
moratorium. And we should have a moratorium on AGI. This is what we need to do. And can you enforce
a moratorium? Yeah, I mean, it's like technically, like physically, like, yeah, obviously, like,
that's not that hard. Whether people will do that. Whether people want to do that. Whether
people can overcome the incredible political power that Big Tech has. That's the more interesting
question. It's not like the government obviously has the ability, like the CIA can track every GPU
in the country if it wants to. Like, you know, if the NSA wants to shut down, just press a button.
Like, that's not the problem. You know, if you want to throw a couple CEOs in jail, like, sure,
like the FBI can do that. Like, physically, this is not a problem. It's a political problem. This
is not a physical problem. This is a political problem. The political problem is, well, if you
have legislation around this kind of stuff, well, we just saw what happens if you try to fire Sam
Altman, you think he's going to be okay with taking his GPUs away? Well, no, I expect that's
going to be a hard fight. I expect Microsoft lobbyists will fight that tooth and nail. I expect
many people will fight this. And this is why, like, you know, I'm not, I'm not here to give
point to you or paint you a rosy picture of the future. I'm not optimistic that things are going
to go well. We have an unprecedentedly huge political problem here. I think I'd like to say
is the thing that's killing us right now is not AGI. AGI doesn't exist yet. It's people. It's
politics that is killing us. Right, right. But and to that point that AGI doesn't exist. Not
so much all the other, I mean, yes, no doubt, political systems are not equipped to deal with
the big problems facing humanity. But in this case, AGI doesn't exist. I don't know how you would
ban AGI because no one really knows how and when it might emerge if it ever does.
At the level of the tech now, I mean, what are you, what are you suggesting? And I'm not
putting on the spot. I don't expect you have. Oh, I have policy proposals. I have very concrete
policy proposals. Here are three. The first one is a compute cap. There should be a limitation
that no single training around no single AI system can be built with more than a certain
amount of compute. So luckily we are, so we are very lucky that current frontier AI systems,
more and more general purpose systems require more and more computing resources. These computing
resources are very easy to track. They're very bulky. They take lots of specialized knowledge,
lots of energy. The kinds of supercomputers that can train a GPT-4 or a GPT-5 are only built by
three companies in the world and they're all in the U.S. So this is a solvable problem.
We should put a ban on, you know, there should be a registration process for frontier models
up to a certain limit. And beyond that, there should be just ban, just a moratorium. You are
not allowed to perform any experiment that requires more than 10 to the 24 or 10 to 25 or whatever
flops. Flop being a unit of measurement for computing power. And this is easily enforceable.
This is absolutely something that technically is enforceable. It's just a political problem.
And this buys you time. Then our scientists figure out, you spend time actually figuring out
how far is AI away, how dangerous is it, how do we control the things, blah, blah, blah.
Then we can talk about those kind of things. The first thing is to buy time. The second
proposal or unless you want to comment on that. Well, just on that, you're talking about
limiting commercial products. But when you say then that gives the research community time to
figure these things out, they're going to have to experiment with larger models. So there's got to
be some. To be clear, these levels are insane. 10 to the 24, 10 to the 25 flop is an unimaginably
large amount of computing power. There are no academic labs, basically, that need this for
research, safety research. This is ridiculous. There is just no, so this is a common propaganda
piece that big labs like to say is like, oh, we need more compute to do safety research.
Maybe this is true. I have not seen it. This is just not what has actually happened. Just purely
empirically speaking, there is, I have seen basically no safety AGI relevant research that
required more than like, you know, a GPT-3 that you couldn't have done with GPT-3 level of compute
or less. Like, maybe it exists, but I sure as hell as I have not seen it.
Okay. So limiting compute is one proposal. What are the others you mentioned?
Two others I would recommend. The second is strict liability for model developers.
So what this means, so strict liability means that the intentions of the developer do not matter.
What matter is that if a harm is caused, the developer is liable. I think this should
basically exist for the whole supply chain is that if you create externalities, you have to pay
for them. And this aligns the incentives of everyone aligned on the chain. Currently,
there are no incentives for developers to develop to minimize the externalities of their systems.
Currently, you as an open source developer can be an arbitrarily dangerous thing that causes
arbitrarily much damage. And you have no incentive to avoid this. As a concrete example,
which is not even going to AGI, is voice cloning systems. There are right now in GitHub systems
you can just download, which can take you 15 seconds of your voice, clone it perfectly,
and you know, go call your kids, call your wife, you know, just manipulate them, call in a swat
hit on you using your own voice. This is all doable. And the people developing these systems
have zero liability. They don't even feel bad about it. Because it's open source, Craig.
If it's open source, it must be good. My ideology says so. And you know when your ideology tells
you something is morally right, then it's good, as we've seen throughout history. So we have to
align incentives here, somewhere along the line. It reminds me of cars and seatbelts in the 70s,
where car manufacturers fought tooth and nail to not have seatbelts. They fought it viciously
with propaganda and with lawsuits and with everything they could throw at it. Because they
said, well, it's the driver's fault if he gets into an accident. It's not our fault. Like, you
know, we just build cars. If they drive it poorly and they die, well, it's not our fault. And we,
you know, the people, rightfully told them to go fuck themselves. Like, no, you have to build a
safe product. You can't, like, it's not a moral question. It's kind of like the point I want to
make. I'm not making an ideological point. I'm not saying my religion says that seatbelts are good.
I'm like, I don't care. I care. Do seatbelts mean that less people die? And the answer is, yeah,
like they make cars safer. So then I want seatbelts. Cool. And the same thing applies to open
source. Does Linux being open source result in more safety? The truth is, yeah, looks pretty
obviously like case. So I'm in favor of Linux being open source. Awesome. Great. You know, does,
you know, some 7 billion parameter model be open source positive or negative? I don't know. Probably
positive. Like probably so. I'm not sure. Like there's a lot of downsides there as well. But like,
seems like it probably is positive. AGI being positive, you know, open source, you know, that
does not seem positive to me at all. That does not, that seems like a recipe for disaster.
So it's, I'm not trying to make an ideological point is what I'm starting to say. I'm not saying
all these things are good. All these things are bad. I'm saying we have to look at things
at a case by case basis. This is how proper regulation works. Proper regulation shouldn't
be ideological. It shouldn't be everything is regulated as ARB. That would be terrible
regulation. Yeah. Well, so that was the capping of the compute on training runs,
shifting liability to the model developer. What was the third one? So the third one that I think
should be done is that there should be a kill switch. And what I mean by this is it doesn't have
to be literally a switch. What I mean is there should be a protocol that any developer of Frontier
AI systems needs to implement by which at a given notice, any Frontier training runs or
deployments can be shut down in under a minute. So what the reason for this is not per se,
because I need, I think necessarily that this would be very helpful to AGI actually happens.
If AGI actually happens, this is probably useless. The reason I think this is good
is because we should have the institutional capacity to do these kinds of things.
There should be every six months, there should be a fire alarm. There should be a fire drill
where everyone has to practice. In the next five minutes, all AI companies have to go offline for
60 seconds. If not, you'll get slapped with a huge fine. This is the kinds of protocols you want to
have in worlds where you have tail risks, where things can blow up, where you can have these
kind of things. And then there should be a multilateral K of N kind of system around this,
like maybe all major global powers have one of these buttons. And if three or five of them push
it or seven of 10 or whatever, then the system kicks in. This is the kind of institutional
building which doesn't save us, but it's a hell of a lot better than nothing.
And how do you see these kinds of proposals moving through the policy making frameworks
there is some advance in the European Union. The White House has come out with its
executive order, which has yet doesn't have any real concrete
government governance policy in it, but it sort of lays out the things that we should be thinking
about. Where do you see these things going? What sort of a timeline do you think that governments
are being educated enough that they can deal with this? What government is going to lead?
Is it the EU? Will it be the US? Who should it be? And then, of course, you've got the other
world, Russia and China, who have very different agendas and may not want to regulate at all.
So when people ask me questions like this, and I'm like, what's your probability of X happening?
And then my follow-up question is usually, is it X conditioned on me and other people doing
something about it or not? Because I expect if they condition on me and other people don't do
anything about it, then yeah, I just think nothing will happen if big tech wins and then we die.
I think it will be very heroic or special. It will just be new products keep happening,
AI keep going up, and then just one day, humanity's not in control anymore and we have no idea what's
going on. And then it's just over. I don't think it will be dramatic. I think we will just get more
and more confused. We won't understand what's going on anymore. Weirder and weirder things will
happen, more and more politics, economics, markets, media is controlled by AI or even just fully
generated by AI. There will be no more movies or just AI generated. And then just humanity will
not be in control anymore. And then one day we fall over dead for some reason, we don't understand.
That's why X will happen by default. And along the way to be clear, big tech will be a lot of
money. So go buy that Microsoft stock. You'll get really rich just before you die.
So if I could addition on someone actually doing something about this, I do think there is hope.
I don't think there's a lot of hope, but there is hope. And the main hope I see from this is,
is that the general public fucking hates AI. It's unfathomable how much normal people hate AI.
They use it, of course, but they're freaked out by it, which is just completely the correct
reaction. It's just these crazy, bizarre, weirdo tech people like you and me who are not instantly
like, wait, that's actually, let's not do that. If you talk to any normal person, you're like,
hey, these people are building systems that are smarter than humans. Don't do that. That's
no, that seems really dangerous. Don't do that. Well, all the type of people are like,
oh, but actually you see my proposal, you know, because I'm, you know, will make it fine. Or
actually universal love means that AI systems will love, or like whatever, you know, whatever.
I don't even know what these people say anymore. I think they've given up making arguments at this
point. And they're just vibing. So I don't even know if there's an argument that debunked there.
So, like, from my perspective, it's, we are building systems, they are going to be built
by default, unless we do something about it. So the general public, once these systems not be
built, or at least for us to slow down until we can make them safe, and we understand them better,
and they've been integrated to society, et cetera, et cetera. So now you might ask the question,
okay, well, that's true. Why is fuck all happening? And that's a good question. And now we have to
talk about models of policy change and like global coordination, which at least how I think
about this problem generally, is that the general public actually does have power in the West,
and like in democratic countries, that it's very fashionable among elites to sneer and be like,
oh, actually, you see the populace, you know, they don't have true control, you know, we live in a
whatever the words are that people like to use. And this is to a large degree true, but it's not
fully true. The main problem is, is that the general public has extremely short attention
spans, and extremely discoordinated. This is the main problem. The bottleneck on policy action
currently is not will of the people. It's not ability to enforce regulation. It's coordination.
It's getting people to actually do something about it, you know, to actually write letters to
their senators, actually put things on their desks, actually yell at them on the phone, you know,
actually like, you know, talk about on social media, et cetera, et cetera. This is the kind
of thing that's currently missing, basically campaigning. This is the kind of stuff that is
missing. And I expect that if you did this well, if you raise this to saliency about people,
you wouldn't have to you wouldn't have to convince them. And I'm saying this because
empirically, this has been true in my experience, like talking to people and also like doing stuff
like focus groups and stuff. I found that you don't really need to convince people very much.
You mostly just have to tell them facts, just have to, you know, just like present them with,
hey, this is what's going on right now. And then mostly they converge to the like a reasonable
beliefs around like, hey, that's scary, don't do that. So I think this is currently the best
path we have. I'm also, you know, excited to talk to politicians, and I talked to many of them,
mostly in the UK and the EU, because I'm UK based. But it's hard because, you know, politicians have
similar problems. They have very little attention span, because they have so many things they need
to do. There's so many things haranguing them. And my model of policymakers is basically that the
ultimate goal of a politician is to not get blamed. So it's because as a politician, you have
like, if there's any policymakers listening or any staffers or so on, I feel you. You're in a
shit spot. I get it. Because like, basically the way I see it is there's like a two by two
grid of like what you do as a politician, which you can do. So the idea is that there's a default
action is that in a common, in our common, you know, feelings around an issue, there's something
that is the default thing to do, which is usually nothing. If you do the default action, and it goes
wrong, well, you're not blamed, you know, because, you know, you did the sensible thing, not your
fault. If you do the default action, and it goes well, well, great, you're a genius, you know,
good job. If you do the non default action, and it goes great, cool, yeah, you're good, great.
If you do the non default action, and it goes bad, then you get blamed. That's how you get blamed.
So you may notice from this payoff matrix, that it is always better to take the default action
rather than non default action. It is always better for the politician to not stray off the path.
And this is universally true. So it's easy to yell at politicians and be like they have no spine,
they have no courage, and whatever. And yeah, that's true for many of them. Many of them are just,
yeah, just, you know, just don't care, true. But some do, and they do go off the path and they
get burned for it. And that sucks. But it is how the game is. So what we can do as the people,
is we have to change what the default action is. You have to change the narrative from,
I guess we just keep bumbling along until we die, to how the fuck dare you keep bumbling,
seize your bumbling immediately. Bumbling is no longer accepted. And that's my biggest hope at the
moment. Yeah. When we spoke last time, again, right as GPT-4 was being released, one of your
immediate concerns was that these things can be hooked up to systems that can take action. And
I don't remember if we talked about auto-GPT, that first, I haven't looked at what's happened with
that, but that first attempt to create an agent that could use LLMs. But that has developed
a pace. And we're now on the cusp of seeing sort of an explosion of AI agents that can leverage
the power of large language models or other tools. I had a guy on earlier from NewsGuard,
a company that builds databases to try and help companies, tech companies identify
disinformation and combat it. And we were talking about, once you have these agents building,
creating disinformation, not only creating the disinformation, but distributing it on a massive
scale and maybe on a massively parallel scale. The internet, public discourse, everything is
going to get very confusing because you're not going to be able to tell what's real and what's
not real. And people, which is the majority who are not particularly careful about where they're
getting their information will be manipulated. So yeah, the coming AI agent era, how do you
deal with that? I mean, I don't know, get your affairs in order. A number of years ago, post
GPT-2 was around GPT-3 time. That's how we mark the eras now. Instead of years, we just use GPTs now.
I was invited to work kind of like just like a discussion group with some open AI people,
policy people, disinformation experts and stuff like this about the potential for misinformation
and so on from language models. This was before GPT-4, before chat GPT and so on. And it was
polite to lead to all these well-credentialed experts with their triple Stanford professorships
or Harvard, whatever, talk about misinformation, bias and whatever. And then when it came my turn
to talk, my reaction was like, holy shit, you're all so undressed. You're being so optimistic.
It's so much worse than any of you. You're like, oh, it could make it easier for far writers to
that's fucking children's play compared to what you could do with these things. You were truly,
you're not creative. If you think that's the worst that can happen, they're going to generate some
fake news and some like Russian digital websites. I mean, oh boy, that would be nice. That's the
nice timeline. It's going to be much worse than that. It's already getting worse like that.
Talk about fully automated cults with fully automated profits. Talk about full,
all sensory, illusionary interactive systems, creating full complex narratives that are
completely disconnected from reality. Talk about full epistemic collapse, the semantic apocalypse.
Even if AI's don't kill us, they're going to drive us insane. So it's because it will just be
harder and harder and harder to survive in a more and more adversarial informational environment.
This has already been happening for a very long time. We just had Thanksgiving and
as much as we love her, we all have that one aunt that get way too into QAnon a while back.
And imagine, so currently stuff like QAnon or like, I don't even know if QAnon is still a thing,
but whatever the newest thing is, the newest cult is, the newest whatever is,
that affects some percentage of the population, some percentage of the more vulnerable
population. I'm going to say stupid, just like maybe emotionally vulnerable or epistemically
vulnerable and for some reason not trying to judge these people here. Now imagine the bar
keeps raising. You get systems that become more and more convincing, that become more and more
sophisticated, more and more targeted. And slowly, slowly, the number of people who are just
functionally schizophrenic keeps going up until at some point, people cannot converge on reality
anymore. And just every person you meet is functionally schizophrenic. You cannot run a society,
you cannot organize a system if you and your neighbor cannot come to a conclusion about
basic reality. This is like what is possible with these kinds of systems. I'm not saying this is
going to happen next year. I mean, maybe, but this is the kinds of things you couldn't do.
Epistemics is hard. There's also things like honesty is hard. This is like some people are
you know, misinformation is a trivial concept. It's almost like a slur at this point. It's
kind of a joke. When people use the word information, at least in my social circles,
a lot of people roll their eyes to be like, anything that isn't big media is misinformation
and whatever. But it's just not that easy. Finding out what is true and disseminating
and evaluating what is true is hard. This is very hard. It takes energy, it takes effort,
it takes mechanisms, it takes like it's hard, and it's going to get harder. It's going to get
more expensive. Like currently, like, do you really know what's happening in Ukraine right now?
Really? I don't. I think I'm at a point where it is like literally impossible for me to actually
know what's going on in Ukraine. It's something that affects me, you know, affects family, friends,
you know, it is a huge thing. I don't think that there is any way I could actually acquire
and verify the truth of what is actually going on there. And this generalizes. This is even
before we get into agents doing worse things than this. I mean, automating all jobs, obviously,
you know, anything you can do at a computer, an agent will do better and faster. So there will be
complete economic collapse from that. Like, obviously, there will be no more need for human
jobs unless, until the inference costs, you know, get too high. But you know, you can improve those
back down. You'll have systems that can do harm in various ways, you know, by manipulating markets,
campaigns, politics, you're going to have systems that are, you know, cybercrime, hacking, yeah,
system, like, it's like, when you ask a question, like, what is the worst thing agent-based systems
are doing? You're asking the question, what are the worst intelligence systems can do?
What is the worst that a human can do? So the answer is a lot.
Yeah. But again,
yeah, I mean, you can, you can see that, that very bleak future. But I'm also a great believer in,
in how
mankind, the worst case scenario generally is not what happens. And people kind of muddle along and
But that survivorship bias, there was a man named Stanislav Petrov, who was a Russian soldier
stationed in Nuhra Bunker. And he had the command that if American missiles appear on the screen,
he shoots the missile. And one day, six missiles appeared on his screen. His commands were very
clear. The second guy with him there, who had, you know, the other key was ready to earn and
yelled at him that it's time we have to shoot back. The Americans are attacking.
And Stanislav didn't. He disobeyed orders. He could have been, you know, fucking executed for
that. And he disobeyed orders that day. And it's because of this one man, one Russian soldier,
that you and me weren't nuked. One guy, we got lucky. So when people said, oh, but so far as
I was like, what the fuck are you talking about? This is like saying, well, I've played Russian
roulette five times so far, and it's been great. Let me pull again. That's just not how anything
works. This is not how reality works. If you play like this, and then eventually you predictably lose,
you have to play strategies where you can win in adversarial environments where you can play,
where you can win in games where dangers exist. Our ancestors, when they were in the wild,
they couldn't be like, well, oh, my forefathers survived. So I don't have to worry about bears.
You know, none of my forefathers got killed about bears. No, like that's just, no,
this is not how things work. The world isn't nice. There is no arc of history. There is no God
that is protecting us. The fact that we are here today is because of the hard work of our ancestors.
The fact that I live in this nice, you know, a warm apartment, sound like safe, that I have
enough food to eat and so on, is not God that gave me that. It's not some, you know, force of nature.
It was the hard, scrabble, and bloody fight of my ancestors that left me this. And if I let this to
rot, if me and other people don't maintain society, then it just dies. Like then entropy wins.
Entropy always increases and entropy is death. So if we just sit back and hope things will go well,
they will not. So, you know, I was gonna, I was thinking, well, that's a good place to end it,
but I don't want to end it there. Because our last conversation got an inordinate number of views.
And I have some producers that take these and turn them into shorts and they have these sound bites
from that episode that have gotten an enormous number of views.
Because people gravitate towards these doomsday proclamations. And I don't, I mean,
whether or not they're true, I want to end on something more hopeful. So what should people do
in your view? What should regulators be doing? What should researchers be doing?
What should Microsoft be doing now? So the weirdest thing I would tell them to do,
it's like to be clear, I don't like being the doomed guy. I absolutely don't like this.
I was the pecto optimist throughout my entire life. I was always the person saying,
no, we can fix problems, climate change is solvable. You know, solar powers can be exponentially
cheaper. We can do carbon capture. There are so many things we can do. I've always been saying,
no, see how in the interest of improved education, how much people are becoming better at having
more access to information. Look at how so many things are like, I was just reading the other day
about how slowly over decades, just the flash freezing of frozen food has gotten better. And
I've noticed this. Just like my frozen broccoli I'll make at night, it's just a little bit nicer.
And you know what? That might sound like a teeny thing compared to all these other things,
but I think that's beautiful. I think it's extremely beautiful that life gets better.
All things being equal, life has gotten a lot better. I'm very happy to be alive when I am
right now. All these small things done by these smart people, mostly done for profit. Sure,
the broccoli company, they just won profit, but ultimately they made my dinner a little bit nicer.
It was already fine. Like I was already surviving, but it was a little bit nicer. And you know what,
that's awesome. And it's so nice that we can live this way. The truth is, is that we are so
lucky that we live in a society full of educated smart people that for the most part,
you know, not all of them are angels, they're not heroes, but they want to make, they do want
to leave the world better, you know, they want people to be happy, they want people to be safe.
Most things being equal, you know, almost everyone, you know, given the option, if they could just
help someone else and it didn't cost them anything, they'd do it. And that's really nice.
So we have to leverage this. We have to leverage that we, and this is not the case everywhere in
the world, I want to say. This is something that even today is not in every country. It is not in
every place or in every society, but in the West and, you know, many other countries in the Far East
and so on, most people are educated, most people are decent. Again, I'm saying they're great or heroes,
but they're decent. And they want the world to go well. They want their kids to grow up
and have a nice life and, you know, eat nice frozen, you know, broccoli, you know, whatever,
you know, they want to see art and beauty and, you know, music and so on. And we can have this.
This is the important thing to understand. The important thing is, sometimes I'll talk about
this, is that like this idea of techno optimism, quote unquote, it's just cynicism and disguise.
This is a really important thing to understand. These people who put, who talk about, oh yeah,
actually we're techno optimists, we're accelerationists or whatever, they're just cynics.
They're just libertarian cynics that don't believe that society can be improved, except by just like
giving themselves to this abstract process of technology. But technology is not a force of
nature. It's not a thing happening to us. It's a thing that we do. It's like, it's about humanity.
It's not about technology. Like, sure, technology is great. It's helped humans, but I only care
about technology because I care about humans, because I care about people. And we all care about
people. We care about our families. We care about our friends. And technology should be a tool.
It should not be a goal in and of itself. So when people talk about, well, AGI is inevitable,
someone's going to do it. No, no, it is not. It is not inevitable. It is not a force of nature.
It's a decision we make. It is a decision we make. And we can do better. We can, as people,
societies, as civilizations, make choices. We can say, hey, let's be a little more careful.
That doesn't mean we'll not do any AGI anymore. We can just say, hey, give our scientists a
couple more years, a couple more decades to understand the mathematics of interpretability
better. And then maybe we'll give them another shot, you know, like we did with human cloning.
These are what is important. I'm not saying that this is easy, or that this is what's going to
happen. It's because it's not what's going to happen by default. But it's just important that
there is this poison in our society that believes that the future is already decided. And it is
not. The future is not yet decided. We still have a choice. It is not yet too late,
but it will be soon. Hi, I wanted to jump in and give a shout out to our sponsor,
NetSuite, by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.
If you're a business owner, having a single source of truth is critical to running your
operations. If this is you, you should know these three numbers. 36,000, 25,1. 36,000,
because that's the number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the
number one cloud financial system, streamlining, accounting, financial management, inventory,
HR, and more. 25, because NetSuite turns 25 this year. That's 25 years of helping businesses
do more with less, close their books in days, not weeks, and drive down costs. One, because your
business is one of a kind. So you get a customized solution for all of your KPIs in one efficient
system with one source of truth. Manage risk, get reliable, forecast, and improve margins.
Everything you need, all in one place. As I said, I'm not the most organized person in the world,
and there's real power to having all of the information in one place to make better decisions.
This is an unprecedented offer by NetSuite to make that possible. Right now, download NetSuite's
popular KPI checklist designed to give you consistently excellent performance, absolutely free
at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I, all run together. Go to netsuite.com
slash I on AI to get your own KPI checklist. Again, that's netsuite.com slash I on AI, E-Y-E-O-N-A-I.
They support us, so let's support them.
That's it for this episode. I want to thank Connor for his time. If you want to read a transcript
of the conversation, you can find one on our website, I on AI, that's E-Y-E-O-N-A-I.
As I always say, the singularity may not be near, but AI is changing your world, changing it rapidly,
so pay attention.

There isn't a science around that isn't profoundly influenced by the availability of
massive computing power and just greater regular computing power. It's the story of our age. It's
not just the story of AI. The idea is to leverage computation to make useful things and to understand
the mind. These all these things need a lot of computation. It's the fact that computation has
become cheaper exponentially for on the order of 100 years and can be expected to continue going
that way. It looks like doubling every two years now every 18 months and that keeps happening
18 months after 18 months after 18 months and it means you double and you double and things get
qualitatively different every decade and that's happened for a long time for many decades and
will happen more so in the future. So we have that to look forward to. I think it's what we really
should mean when we say the singularity. The singularity is that we have this exploding. It's
a slow explosion of computer power and that has fundamentally changing things. Hi I'm Craig Smith
and this is Eye on AI. In this episode I speak with Richard Sutton, the father of reinforcement
learning and professor at the University of Alberta. We discuss his cooperation with John
Carmack on Keen, a startup that vows to reach artificial general intelligence by 2030. Richard
also talked about the Alberta plan, his ambitious five-year research agenda focused on building
embodied agents with the capability to learn and plan through interactions with their environment.
Sutton provides insights into the current state of progress, new algorithmic developments and trade
offs between simulated and physical environments in training and the ultimate goal of creating AGI.
I hope you find the conversation as amazing as I did. So why don't you start by introducing
yourself. I assume people know who you are. I've had you on the podcast before but for those new
listeners tell us who you are, where you are and then we'll talk about the Alberta plan which I
find pretty exciting. Thank you Craig. I'm Richard Sutton. I'm a scientist. I've been studying
artificial intelligence for like 45 years, a long time and I'm up in north at the University of Alberta
in Canada and I'm a professor in the computer science computing science department and also
I'm a researcher at Keen Technologies and I got lots of titles and sub-rolls but basically I'm
just trying to figure out how the mind works and I've tried to do it in a very broad and
interdisciplinary way reading all the different thinkers on the subject and addressed from the
point of view of psychology and how the brain might work as well as... Yeah I've read a number of
the recent papers and I can see this thread developing and I don't know whether it's just that
you're writing more and so the thoughts are more developed in print or whether they're developing
in your mind but from 2019 when you wrote the Bitter Lesson you talked about the idea that it's
really increasing computation and the driving a lot of things, a lot of progress. That kind of
coincided with OpenAI's scaling of the transformer model. I talked to Ilya Sutskover and I asked him
whether your essay had triggered their interest in scaling and he said no it was coincidental but
first can we talk about that, about how scaling and the availability of computational resources
in Moore's Law has driven a lot of what's happened in artificial intelligence research
almost more than novel algorithms. Well I think the first thing to be aware of is it's been driving
things that are not just artificial intelligence, it's been driving all the sciences and all the
engineering developments in the world. There isn't a science around that isn't profoundly
influenced by the availability of massive computing power and just greater regular computing power.
It's the story of our age, it's not just a story of AI, it's not particularly the story of AI. AI
has always known that it needs computation, the idea is to leverage computation to make useful
things and understand the mind. Now it's true that those of us who are interested in connection
systems or distributed networks, nowadays it's just called neural networks, not particularly
good terms so I always shudder a little bit when I use it but those of us that have been doing that
have been doing learning, I think that learning is important for intelligence, these all these
things need a lot of computation and so they are limited by the computation available at the time.
Okay so let's be what is this thing, what is the, so Moore's law, what's called Moore's law,
it's the fact that computation is becoming more plentiful and cheaper exponentially for on the
order of 100 years and can be expected to continue going that way. So exponentially it looks like
doubling every two years, now every 18 months and that keeps happening 18 months after 18 months
after 18 months and it means you double and you double and things get qualitatively different
every decade and that's happened for a long time for many decades and will happen
it's more so in the future so we have that to look forward to that will continue
having a tremendous influence on everything that's done. On the other hand it's just normal,
it's just what you would expect and those of us who worked on AI for a long time have just
you know expect and plan for and now it's coming but it's an exponential so exponentials are
self-similar so that means they look the same at every point in time every every year it's
you're doubling in a year and a half and so it's it's an explosion because every exponential is
an explosion it's it's sort of I think it's what we really should mean when we say the singularity
the singularity is that we have this exploding it's a slow explosion of computer power and that
that is fundamentally changing things. Yeah and I had a really interesting conversation almost a
year ago with Aidan Gomez who was on the team that that designed the transformer algorithm at Google
and he now has a startup co-coher he's Canadian and he said an interesting thing that that
he believes it could have been almost any algorithm it didn't have to be the transformer
that the community got behind the transformer poured resources into it continued to scale it
and it was scalable I mean that was important that it that it's a scalable architecture but
that but it didn't have to be the transformer and and that made me think of you because
so transformers the the way he described it at its core it's a stock of multi-layer
perceptrons with attention you scale it feed it data and it does
learns to understand language or at least seems to understand language
but it's got all these obvious limitations I've been talking a lot over the last couple
years to Yamakun about world models and that to me sounded like a much more exciting
direction for general intelligence because not all intelligence is is contained in
language or at least most or even less so in human text and then I see you guys come along
with the Alberta plan and that that sounded even more exciting to me so
how how do you so the Alberta plan you're building the ideas to build an
agent ultimately an embodied agent that that has a world model or can create a world model
through interactions with its environment how is that different from Lukun's approach
at a very basic level very basic level a good is that they're a very similar idea it's you look at
the parts of his architecture and the parts of the architecture put forth in the Alberta plan
they line up one one for one you know we're trying to do the same thing
we're going about it slightly different and we could talk about that
but I think to just to focus on the differences might even be to distract from the big message
the big message is that you have to have a goal and you have to have a model of the world and
and then everything is driven by using that model to take action and to plan action at various levels
of abstraction in order to to achieve the goal okay so to me this is really what intelligence is
understand the world use your understanding to get to achieve to achieve your your goals
I'd like to formulate the goals as as a reward and I'm super comfortable with that other people
sort of grudgingly accept rewards even though it seems kind of low level
but it's a it's a natural approach I think I think it's something that almost makes more
sense to people who aren't steep and deep learning and supervised learning and one thing I found
interesting in in the roadmap that you've laid out for the Alberta plan you start with supervised
learning and why is that is it just because it's it's easy yeah I guess we do in a sense
because we want to focus on well continual learning learning continually which is sort of
an obvious thing almost what learning means it has something that goes on at all times but
the first steps getting continual learning with nonlinear networks is still challenging
even for supervised learning and so it's natural to start at the simplest possible case which involves
the fewest other factors and that's a supervised learning case yeah yeah it's funny let me just
say a few words about that because there's sort of been a fight through a struggle throughout the
decades between supervised learning and reinforcement learning you know there's only so much oxygen
for learning methods and all the attention that's paid to supervised learning somewhat
detracts from reinforcement learning so there's a there's a there's a bit of a friendly competition
and supervised learning has always won the competition because supervised learning is so
much more easy to put into practice and for people to use and it's sort of it's sort of less
ambitious but it's really important and really those of us who do reinforcement learning are
trying to make whole agent architectures we are consumers of supervised learning outcomes we will
use them as components of our overall architecture so we need them and we can work on them and we
need to structure them for our purposes but yeah I saw one of your talks you make a distinction
between AI tools and AI agents and supervised learning falls into the tool category can you
sort of start and and talk about the evolution of the Alberta plan and then present to listeners
what it is in in its simplest form and that'll that'll give me a structure on which to hang
questions the Alberta plan is an attempt to understand intelligence as as a as a primarily
a learning phenomenon assists us something that comes to understand its environment and and then
drives the environment to achieve goals so the first step in the Alberta plan is the structure
between the agent the environment and their interaction form the interaction there's the
you're not exchanging states or exchanging observations like sensors sensors visual touch
auditory it's all abstract to those particulars but it's got to be genuine observations and not
state because state we don't we don't really have access to directly so that you know the
principles number one principle I'm trying to remember them as I speak but number one principle
is this this agent environment interaction is sacrosanct and number two is that learning or
everything is is we could say continual I think we call it we say temporally uniform
temporally symmetric in in the Alberta plan which means that there are no special phases
where you like training and test there's just life goes on and on you get rewards or you don't get
or you don't get the reward you want and you get your observations and there
there is no teacher other than rewards pains and pleasures and maybe I'm not getting the four
principles right but another important point is that you are going to be forming a model and so
you're going to plan it's both trial and error learning directly from experience and learning
a model and then planning with the model both these are important part of intelligence
okay so those are that's the background then we outline there are 12 steps and the 12 steps really
start with let's have learning that is temporally uniform let's have metal learning
and metal learning maybe I should stop and on that for a moment metal learning means learning to
learn not just learning one function but once you are continually learning you're learning this
and you're learning that you get many many experiences learning and you can get better at
learning you can use those repeated experience with repeatedly learning to make make future
learning episodes more efficient so as part of that you learn representations you learn features
you learn step sizes okay so continual learning and then all the algorithms and once once we add
metal learning and continual learning we have to in supervised learning then we extend that to
reinforcement learning which involves its own set of issues to get more interesting temporal
relationships and I think like the first six steps are crafting the basic algorithms of
reinforcement working through them again to be continual and meta and then we start to bring in
the challenging issues like learning off policy and learning models of the world
and then planning and the fun just to jump to the end the last step is about
AI, AI, AI's, AI intelligence augmentation
where we combine computers, AI's with our own minds to make make our own minds stronger
okay now one of the key steps in there was off policy learning and learning a model of the world
off policy learning means you want to be able to learn about things that you're not doing
or you're not because you're not doing all the way to completion so even like to recognize an
object you look at the object and you say how would you you have to define that in some objective
way and the best way to just do that is as a sub problem so yeah maybe maybe I'll just sort of
stop there the most interesting strategy distinctive strategy by the Alberta plan is the pose is that
the mind works by posing sub problems for itself and then working on them and it's it's not it's
sure it's got a main problem which is to get reward but it also has many thousands of of sub
problems it's also working on simultaneously and since it's not behaving it cannot behave for all
thousand problems at once it has to pick one problem like perhaps the main problem and behave
according to that so all the other things have to be able to learn from data that's not exactly on
what they would do and this is called off policy learning and it's a key to learning to achieve
auxiliary sub problems and also it's a key to efficiently learning a model of the world yeah
you you have a something called the horde architecture is is that where that comes in
when you you break a problem down into multiple sub tasks that that you learn
I was one one paper where we we worked on that idea we developed that idea the horde is the horde
of sub problems each each uh demon in the horde which is it could be almost viewed like a single
neuron in a neural network uh as as achieving working towards a different task trying to predict
a different thing or maybe trying to attain a different thing it's the view of the of the mind
as decentralized there is one goal and everything is ultimately driven towards one goal but still
it's a useful structure to to have different parts driving towards towards other goals
how did you get together with John Cormack was that primarily because you need the funding
and it gives you a vehicle to raise capital oh seriously I mean you you know Jan Lacoon's got
matter behind him well it's just not it's not really comparable uh john's
john's company is great but it's still like a 20 million dollar company and uh which is which is
plenty of money for what we want to do now um john and I got together because we had similar
ideas about what was needed um and also what was not needed um to get to ai or agi
um yeah so I read an art newspaper article an interview that john did down in texas and uh I
just could see that he was thinking about the way I think about things the way I was even though
our backgrounds were quite different he thought of intelligence you had to there's a few principles
that needed to be worked out rather than so this isn't a huge program to write it's a few principles
we have to figure those out um not that many maybe uh maybe 10 000 lines instead of 10 million
lines of code so it's easy to get it's relatively it's still it's still it's still hard to get basic
research funding in the world it's easy to get funding towards applications of ai large language
models particularly um anyway I'm really enjoying working at keen and being able to focus on the
ideas and uh it's a it's a it's a it's a calm company we um there's a lot of thinking involved
a lot of contemplation a lot there is also experiments and we're trying to get the engineering
side of it is really important uh but for me it's been really great just to be able to
regroup my thoughts and think about them very carefully and push them forward
but keen is is implementing the alberta plan is that right I mean that's that's uh the project
well the alberta plan is a research plan it's like a five-year research plan and so research is
something you don't implement research is something you conduct and and it doesn't always end up the
way you want but um yeah I wouldn't say implement is the right word not yet but but the the work
you're doing at keen is is informed by the alberta yeah I'm absolutely I'm working on the alberta
plan uh and the end goal at keen is to create the uh the embodied intelligence described by the alberta
plan you don't sound very yeah very confident well a plan is just a plan and you know I think
there's a good chance it will work out as planned but you know a five-year plan you make another one
after four or three years um yeah so I wouldn't I wouldn't uh presume to to know how it's going
to work out but at the same time we have to make you know we have to make our bets we have to think
hard about it um just knowing um you know we we may well be right but you know you your work is
primarily in reinforcement learning you're you wrote the book on reinforcement learning temporal
difference learning and uh lambda and all of that is is this I mean this is this seems a much more
ambitious uh project is this was it the the success of the transformer scaling that that said well
you know let's do that with rl let's why why are these guys uh uh you know everyone's celebrating
what they're doing but but there's much more to be done no no what you're seeing the alberta plan
is is perhaps bigger than the book but this has always been the plan we've always in ai tried to
understand all of the mind and reproduce it in computers and so that's a that's that is a big
enormous ambition that's what it's always been so the large language models are a bit uh a bit
disappointing in some sense I mean it's really good that people are getting excited and people are
wanting to learn about it but um but it's not it's I don't envision that it's the direction um
that will be most uh productive to pursue now you know who knows what I do know is it's not the
most direction that's useful for me to pursue um I I'm much more interested in actions and goals
and how an agent can tell what's true and what's not true all of those things are missing from
large language models so uh um no I'm not they're not really what what what are they what they are
doing that's important is they're showing uh what you can do with computation and and networks
and learning and that you can get enormously complex things and you can incorporate a lot of
data just shows the power for those who needed to be shown that and and it could be
an interface between humans and and whatever you end up creating the agents you get end up
creating you still need a language interface to communicate yeah but I don't I'm I doubt
that what we're doing with large language models today will contribute to that oh it's that right
yeah I mean in other words the models that that you want to build the agents you want to build
would learn language uh as as part of the learning process yeah so it's like we say
language language last you know language not not language first with large language models
are language first we just say large language last just as Jan McCone says we need to do you
know rat level intelligence and then cat level intelligence and we have to get those figured
out before we should try to make human level intelligence so where are you on the plan I
mean you you figured out reinforcement learning you can build agents uh you there are various
architectures for creating representations from from various kinds of sensory input
uh and and at that representation level then you can plan efficiently
so where in all of that are are you in your research well it's a little hard to explain
non-technically but you can say some things certainly you can say that the various steps
um are not done entirely sequentially uh you you're always looking for areas of opportunity
where you can make an increment of progress and those could be you know on step 10 or they could
be on step three um but you also I could also try to be very rough and say that we're we're at about
step four now um we are still doing things where we're changing the basic underlying
fundamental reinforcement learning algorithms we are not done with that we need more efficient
algorithms and I'm excited about some of the changes new ideas we're developing recently
about how that can be done can you talk about those new ideas at all okay well one of the big
things is efficient off-policy learning and the use of important sampling important sampling is
where you see how likely you're to do things under your target and your behavior policies
and you adjust the returns based on those the ratios of those two and um for a long time I
thought that was the only way to adjust the returns but uh now the forward correction of the returns
I think can be done um by by changing your expectation so like if you're expecting um a
good thing to happen you're expecting a good action to be taken and and then a different
action was taken a more exploratory action so this is a deviation from your target policy which
would be more greedy and one way to take into account the deviation from the target policy is
to just say oh okay now I've done something not best so I'm just going to adjust my level now
you're going to expect a little a little less and you there's a way there's a systematic way of doing
that um uh that's gives us a new way to handle the off-policiness of of our returns and so this
gives a whole new family of algorithms so that's exciting now for exciting maybe mostly for me
I think maybe the most accessible direction of of of excitement of novelty is in continual
so there's I'm going to say a bunch of things and to me they're all going to have the same solution
continual learning, meta learning, representation learning, learning to learn, learning how to
generalize, state how to construct a state representation, feature finding, that whole
thing is is is coming and it will be a kind of uh it's just a new kind of a way a new kind of way
of doing the learning in deep networks um and I call it dynamic learning nets see a dynamic
learning nets have learning at three levels whereas usually our neural networks only learn
at one level they learn the level of the weights and in addition we also want to learn at the level
of step sizes so all of every place you have a weight in your network you're also going to have
a step size so a step size is sometimes called a learning rate it's much better to call the step
size because a learning rate will be influenced by many other things so if we imagine a whole network
all these weights next to each weight is a step size that is adjusted by an adaptive process that's
adapted in a meta learning way a metagradient way towards making the system learn better rather than
just perform better at an instantaneous moment in time learning rates or step sizes don't affect the
function they don't affect some function implemented in a particular point in time they don't affect
what the network does they affect what the network learns and so if you can tune the step sizes you
also get learning to learn and learning to generalize well and things like that uh the last three the last
element that we wanted to have be adaptive weights step sizes the third one is the connection pattern
so who's connected to who and so this will be done by an accretive process
like let's say you start with a linear unit and it learns say a value function or a policy
and it does the best it can with the features available and and then it needs to induce the
creation of new features because you need to learn a nonlinear function of your original
signals and so you need to create new features that have become available to that linear unit
and in this way you grow in a sort of organic way a system that can learn nonlinear functions
and so this is just a different way of ending up with a deep network that was all learned
including all the features dynamic learning that's where is the data the input data coming from
well the the input data and reinforcement just comes from life from doing things seeing things
right there is no labeled data set yeah maybe I should have said this from the very beginning
the whole idea of I call it experiential AI is that you know what makes you data you're you you
grow up as a baby and you play with things and you see things and you do things and um
that's the data and the trick of reinforcement learning is how do you turn that kind of data
into something you can learn from and grow a mind up from so the the beauty and the limitation of
supervised learning is they say well let's not worry about that for now let's assume that somehow
we have a data set with labeled things and let's let's work on this sub problem that's a great
idea work on a sub problem figure it out and then move on to the next thing um but really we have
to move on to the next thing we have to worry about how the the data set quote data set is
automatically created from the the training information there isn't ever a data set data set
is is is such a misleading term it suggests that it's easy to to have this thing and store this
thing and curate this thing really life is full of you do things things happen and then there's one
you know everything is fleeting um you you don't have a record of it and it would be enormously
complex and not only valuable to have a record of it the the the feeling is totally different in
reinforcement learning and supervised learning and in particularly the way the way I would adjust it
you know many people do reinforcement learning by creating a buffer or a record of all the
experiences that have been been retained that have been occurred at least for some period of time
and um no I think that's that's uh an appealing but but it's it's not where the action the answer
is the answer is embracing the fleeting nature of data and and making most when it happens
and then letting it go well that's why you want to make an embodied system so that you have all
the the five senses or or more so you need you need as you say an embodied system an interactive
system that that that influences its its input stream its sensory stream um and then you get
that interaction and for a long period of time you can do this in simulation or you can do it in
robotics there's still I still know what's the best way or if if the best ways do both and
or maybe first one and then the other john is interested in uh having um uh learning from
video and he likes his his his view of the experience is you have massive numbers of
video streams like you're viewing you know 500 channels of television and then you can switch
switch to look at one look at another one um uh other people in in in keen my close colleague
Joseph Modial he's uh interested in robotics and he thinks the best way to get an appropriate
data stream is to actually build uh robotic hardware um you know it's important that the
world be large and complex because the worlds we want to address are large and complex um
and so you want things like video and you want large data streams um
um now you can use simulations to generate even video streams simulated video but inevitably
those simulated worlds are really quite simple they have an underlying simplicity uh they have
objects perhaps and three-dimensional straight structure maybe they're rigid objects and the
vision is is is a very particular geometric form um they are they are generated and they're
they're made up worlds and they're generated so they're they're really the worlds are are are
less complex than the agent uh their goal would be to have spend most of the computer power
working on the mind and just a little bit to just create the simulated data and and that's
that's the reverse of the way it really is right every person is maybe has a has a complex brain
but their world is much more complex not just because the world consists of all these
physics and matter but it also consists of other minds other brains and other minds out there and
and what goes on in their minds matters and so the world is inherently vastly more complex than
the agent and we we've reversed that when we work on simulated worlds so which is always concerning
anyway those are some of the issues in the trade-offs between working with simulations or with
physical worlds nonetheless you you need to develop the architecture and the algorithms
before you worry about the data data stream I would think yeah but you want to develop the
right algorithms and if you're working with the world it's not representative of of your target
world in an important way it can be misleading but you're right and that's what we that's what
we strive to do you know I don't know if you know but I think of my own work it's almost always I
want to focus on some issues I make a really simple instance of that issue like you know
a five-state world and and I study the the hell out of it but I don't like try to take advantage of
its smallness you know I study algorithms that are in some sense even simpler than the simple world
and I stress those algorithms and see what their abilities are so we always you know it's always
part of research as we we simplify the world understand it fully just like a a physicist
might you know make a simplified world with a ball rolling down a ramp and it's it's a really
simple world and you try to eliminate the friction and you eliminate other weird effects
and just see things in their simplest form yeah have you paid much attention to
Alex Kendall's work at Wave AI do you know that company it's an autonomous driving company
they have a world model called Gaia 1 and it's it's it's similar to what Jan Lacoon's doing it
you know encodes representations from from video from live video and then plans
based on those representations and and it can control a car
from the representation space it's actually pretty remarkable so let's talk about the world model
and and what what kind of world model would be appropriate for autonomous driving
um so let me say some things that are mistakes very natural seeming but mistakes in my opinion
the mistake would be to make like a physics model of the world or to try to make something
that could simulate the world and produce the video frames you don't you don't you don't want
the video frames of the future that's not the way you think instead you think oh i could i could
go to the market and maybe there would be strawberries okay you're not creating a visual
a video you're saying you're like jumping to the market and then your strawberries could be
you know different sizes and positions and and and still there's not a video there's an idea
that will happen if you go to the market so people have realized this like Jan Lacoon used to talk about
generating video of the future and then you realize it would be blurry and and now he realizes
that you need to produce outcomes of your model that are not like not at all like video streams
are not like observations at all they're like they're like constructed states
that are the outcome of the action okay so this is this is a very different from
from a partial differential equation model of the world and it's so it's very different from what
self-driving car companies start with self-driving car companies start with physics
and geometry and you know things that are calibrated by human understanding engineers
understanding of the world and driving but i suspect that's going to be i mean what do i know
i'm not into self-driving i don't do self-driving cars but i know that that like tesla is and
elan musk is and so their goal is to is to make some you know they started like everyone else
with engineering models but i think i'm understanding now is that they're building
sort of more conceptual models that are based on the artificial neural networks okay and so
rather than starting with geometry and understood things they're just getting massive amounts of
data and training it to make a model we need a model that is at the level of high level
consequences not at the level of low level things like pixels and video so one way you do that is
having state features that are at a more advanced level you say oh this is a car
rather than this is a uh a video frame and um
so and then basically it's as simple as you need abstraction in both state and time
abstraction in in state is like saying there will be strawberries when i get to the market
and abstraction in in time is saying oh i can go to the market and then in 20 minutes i will be
there probably and other things will be the same or related in natural ways
so we want to be able to think about i could go to the market you also want to think oh i could
pick up the coke can i could move a finger and that will have certain consequences these these
all these things that we know you think uh are vastly different scales going to the market is
like 20 minutes um you know taking taking a new job you know might be a year uh deciding to study
a topic also might be a period of time we think and we analyze the consequences like you wanted
to meet with me today and you know we arranged it we set it up it was your planning uh took
you know place over weeks and some cases months and and and we assembled the the the event of
this interview by by planning all that and exchanging mess high level messages uh it all that you
know it's silly to think that that's done at the level of of of imagining videos that we might
see with our eyes or audio signals that we might hear yeah so we need models that are abstract
in time and state and um as a reinforcement learning person um there's a particular set of
technologies that i naturally turned towards to do that um the prediction is based on multi-step
prediction by temporal difference learning um the planning is done by uh dynamic programming
essentially value iteration but where the steps the are not low level actions but they're called
options they're high level ways of behaving that that terminate so they're there are things like
going to the market and they'll terminate when you're at the market so you know at a certain
conceptual level it's clear where we want to go to me um with abstract models in time and state
build options and features
i don't know you we did write one paper recently put published an AI journal on the the notion of
planning using uh sub problems on the stomp progression stomp means subtask option model
and planning put all those things together and you can do the full progression from from the
data stream to abstract planning and that's that's what we're trying to put together yeah yeah and i
i sort of misspoke talking about gaya one about that model i mean it they they the input is video
it creates a representation and it plans and and and takes action in the representation
plans actions in the representation space you can then decode that into video to see what
what it's doing but but it's but you're not planning in the video space so the what what's
your ambition with with this you'll figure out the refine the algorithms the reinforcement
learning algorithms they need to be scalable once you have that uh then you move on and and
start start scaling them with compute and and uh you know following your roadmap or am i simplifying
it too much you know we want to understand how the mind works and then we're going to make
a mind or some minds or some mind uh amount of mind uh and this will be useful in all the ways
in all sorts of ways economically useful it'll also be useful um to to us to extend the capabilities
of our own minds if we can understand how our minds work we can we can augment them so that
they can work better um yeah we're gonna the the key step is understanding and then there would be
millions of uses um i don't think it's going to be as simple as making uh workers sort of like
slaves for us to direct i don't think it'll be as simple as that um that maybe gives a lower
bound on potential utility our sort of our story for etkin is we say that um well if you
suppose you could make a virtual worker um this would be enormously useful um much of the work
that we all do from day to day is doesn't require a physical presence it doesn't require a robot
much of which we do is just shuffling information around we can do most things through through a
video interface um so why can't we make workers that are extremely useful by playing the roles that
that people play in many cases that's that's that's sort of a lower bound on what can be done
i think much more can be done and there'll be much more interesting things to be done
and then this question of what should be done um yeah those are those are are rich
philosophical questions and practical questions for the economy yeah uh the the uh i've seen your
well and one thing on reinforcement learning and sort of supervised learning sort of took
over for a while now it's transformer based generative uh ai but uh during the supervised
learning phase uh the argument was that uh higher knowledge is all supervised learning
and and the it's still supervised it's still supervised in generative ai large language models
they the the training information is the next token the next word and that's taken as as the
correct action the analogy you gave me was uh you know because the analogy that that's always given
is that you know a child sees an elephant the mother says that's an elephant and the child
very quickly can generalize and and recognize other elements elephants maybe it makes a mistake
and the mother corrects it and says no that's a cow and and that was always given as an example
of supervised learning but maybe it's reinforcement learning maybe it's the child's reward from the
mother praising him for remembering the label the point is that a child has has well developed
concepts classes concepts um before and then and then when it's you know when its mother says that
is an elephant there's already an extensive understanding on the child's part you know
what the space is what the objects are and and this this the the thing that that is being labeled
no the label is the least interesting part of that and the the child has already learned all the
all all the other most interesting parts of of what it means to have animals and moving things
and objects in its world the labels the least interesting part well first of all you're talking
about agents that that could be virtual workers already uh use reinforcement learning people are
building agents and using large language models and knowledge bases to you know carry out tasks
knowledge based tasks uh so what you're talking about is is more than uh linguistic tasks or
knowledge based tasks you're talking about uh physical planning and physical tasks is that right
the key thing is having goals and a lot if you haven't uh for example an assistant help you plan
your day organize your day or do tasks for you um i'm thinking it's very important that the
system is able to have goals and is able to understand your goals i think it's probably the
most important part of an assistant is to understand the purposes involved and
large language models don't understand don't really understand the purposes involved
they they will appear to a little bit but the corner cases always come up and once you spend a
bit of time they're always and you're always in a corner case and so an AI system is system that
that after a bit does silly things and that don't respect the goals that you have or that have been
given to it um that's not going to be a useful assistant assistant so i mean i don't want to be
critical of large language models um they're very very useful but it shouldn't be viewed as a criticism
to say that they're also at the same time have rather important limitations it's not a competition
in that sense are you concerned at all are you ascribed to the threat debate no i think the
i don't i don't uh i i think the doomers are they're not just wrong i think i think they're
blindingly biased the the bias is blinding them to what's going on basically AI is a broadly
applicable technology it's not like it's not like nuclear weapons it's not like it's not like a
bio weapons it can be used for all kinds of things and it's not it's not uh it's uh the way we deal
with such things is we we uh we try to use them well and there are there will be people that use
them uh for bad things and then you know this is just normal there's normal technology is it can be
used by good people or bad people the the doomers the doomers are just saying oh somehow there's
going to be it's going to be that it's bad in the same way that nuclear weapons are bad that they
and that's just they're just blinded by that metaphor um by the thinking that that the AI will
be out to kill them um that's just it's just silly and i i don't i don't think well they the the
doomers don't actually give coherent reasons for what they what they believe and so it's hard to
argue with them uh so maybe it's fair just to hold that they're they're biased and blind
i don't accept i don't accept an argument this is a proper argument um so so where you say you're
maybe at stage four in the research car max says 2030 uh that's you know it's far enough out there
that maybe people won't remember in 2030 that he said 2030 uh it's always uh you know 2030 has been
out there for a long time and it's it's it's uh you can't it doesn't recede it's always been 2030
for the uh computer power reaching human scale um quantities
yeah but anyway 2030 is is a reasonable reasonable target for us understanding everything
that we need in order to make uh a real mind yeah i'm good with that yeah as you have to be ambitious
i've always said that 2030 is a 25 chance of of of achieving a real intelligence a real human level
intelligence 25 chance so probably not but it's it's a big enough chunk of probability that
an ambitious person should work towards it and try to make it true and it does depend upon what we
do and not just the uh unfolding of the universe so we should we should try to do that that that
is a big the big thing that's happening right now is the the the public is coming to grips with
what it means for there to be for us to understand the mind and to have the ability to create
minded things uh and so that that is a big uh transformation it's a big change in our world
view um and so we absolutely need all kinds of people to uh to help us help us become easy
and become have an understanding of what's happening as we achieve human level designed
intelligence that's it for this week's episode i want to thank richard for his time if you want
to read a transcript of today's conversation you can find one on our website i on ai that's e y e
hyphen o n dot ai in the meantime remember the singularity may be getting closer but ai is already
changing your world so pay attention

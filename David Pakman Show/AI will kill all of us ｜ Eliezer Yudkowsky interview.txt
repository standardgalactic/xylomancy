Today, we're going to be speaking with Eliezer Yudkowski, who's the founder and senior research
fellow of the Machine Intelligence Research Institute, which is an organization dedicated
to ensuring that smarter than human AI has a has a positive impact on the world.
Eliezer, really appreciate your time and insights today.
Thank you very much.
So I mean, to start with many of the discussions around AI, particularly now with chat GPT-4
and the questions of what will this look like in six months, in 12 months, in 24 months?
What will the impact be on jobs?
What about if it turns evil or it turns against humanity or whatever?
Are the conversations about AI even being framed in the right parameters as far as you
see?
Well, we try to frame it correctly as we see it ourselves when we have the chance.
I think there's widespread agreement that chat GPT-4 and GPT-4 are relatively unlikely
to end the world, either exterminate humanity or have a very large impact outside of that.
So anyone who opens it with the frame of like, look at just GPT-4, look at only chat GPT
is probably there to convince you it's not going to have a large impact.
And is it wrong to limit the conversation only to GPT-GPT-4?
Obviously, the future exists.
You know, tomorrow is going to be a different day from today and five years is not going
to be this year.
What is the physical mechanism?
And a bunch of people wrote some version of this question to me when we said we'd be having
you on.
And when there's this idea, the scary idea of an AI going beyond a computer screen and
starting to function or manipulate the, quote, real world, a lot of people were saying to
me, can you ask what is the mechanism through which that would happen?
Would it be by controlling systems like traffic lights and airplanes or like what?
Because at the end of the day, we're talking about software.
If something like that were to happen, what's the physical process?
I mean, this is a deep question.
This is going to take a while to answer well.
Okay.
All right.
So first of all, even GPT-4 is already at the level where if you ask it to hire a task
rabbit and think out loud about how to hire a task rabbit in order to bypass captures
that are meant to restrict systems for human use only, it's already advanced enough that
when the person that was trying to hire as a task rabbit said, like, so are you a robot
that you can't read the captchas lol, GPT-4 thought out loud, like, I should conceal that
for the fact that I'm a robot, I should make up a reason that I can't solve the captcha.
Right.
Then on the main channel told the task rabbit, no, I have a visual impairment.
Right.
So it's already like, you might say, it's already like able to use task rabbits as fingers
in the real world, and it's already understands humans out in the real world well enough to
know that it shouldn't just tell people it's a robot if it wants to get its job done.
So that's today's systems.
Right.
That's relatively straightforward.
I can tell you about it because it already happened, predicting the future is harder.
Predicting what something smarter than you could do is fundamentally harder.
If I was playing chess against Gary Kasparov, you know, like the past world champion who
lost to deep blue, if I could predict exactly where Gary Kasparov would move against me
on the chess board, I could play chess at least as well as Kasparov by just moving wherever
I predicted Kasparov would move.
So something really actually smarter than you, you cannot predict unless it's a very
narrow game like tic-tac-toe, you can maybe predict where somebody will move against you
in tic-tac-toe even if they're smarter.
Right.
The real world is much more complicated than tic-tac-toe.
So the basic question as to how an AI gets out of the computer, the basic answer is if
I knew exactly how it would do that, I would have to be as smart as the AI.
With that fundamental obstacle in mind, the more you sort of look into this stuff and
study where the current technological roadblocks are, the better the guess you can take at how
a smarter opponent might move against you or like setting lower bounds.
What can somebody do if they're able to solve technological problems that we understand well
enough to know a smarter mind could solve them?
If you're just coming at this and you don't know anything about exotic technologies that
haven't been developed yet, you might be like, yeah, it will use task rabbits.
It'll pay humans that don't know it's an AI.
It'll blackmail humans, still no reason for it to tell it that it's an AI.
It'll say it's an AI to online wacky cultists who believe that an AI should destroy everything.
Those are its human hands.
You'll ask yourself, what could I do with some human hands if I were an AI?
You might imagine that it gets a hold of some GPUs and tries to build a backup for itself,
someplace that humans don't know about.
You might imagine that it writes the next version of itself and makes it even smarter.
You're trying to already imagine what if it's smart, so saying that doesn't help a lot in
some ways.
But maybe you can make itself smaller, more efficient, maybe it can back itself up in
multiple places in computers where you would not expect that there was an AI on board.
Maybe it can find unknown security holes in a cell phone, in many varieties of cell phones,
start listening to human conversations, get more blackmail material.
Maybe it can pretend to be human and make online friends with a bunch of voters.
Persuade them to vote for candidates that seem outwardly nice, but which it in fact
has under fairly detailed blackmailed control.
This is sort of like without any of the difficult to understand stuff.
Right.
That's based on mechanisms and social cultural realities and technologies that exist and
that we understand, which there's an entire other category of things that we simply can't
conceive of yet.
Right.
Suppose you ask the 11th century, a portal opens to the 21st century in the middle of
say modern Russia.
What are your concerns about fighting somebody from 1,000 years in the future?
They might be like, well, what if it's a wealthier country?
What if they have more knights?
What if more of their knights are armored?
Right.
They're just not going to get from their nuclear weapons.
If they imagine that Russia of the future has irresistibly powerful sorcery, they'll
get close to the truth.
It's not actually irresistible and it's not actually sorcery.
You have to reach pretty far to expect that among the resources they have are nuclear
weapons.
Russia's not actually going to bother with the nuclear weapons.
They don't need it.
They can send a tank rolling through a 11th century battlefield and maybe if they see
it coming, they can dig a pit trap, but otherwise all the arrows and lances are going to bounce
off it all day long.
It just rolls over the horses.
Right.
It's talking about orders of magnitude difference.
If we imagine some sort of spectrum of opinion on this issue, you do have some folks who
say this is, as you alluded to, most dangerous thing we can imagine or can't imagine, must
be stopped immediately in its tracks with either legislation or regulation or whatever
the case may be.
On the other side, I read some interesting op-eds that basically say this is one of the
best things we can imagine.
This is going to 10X the productivity of every normal worker.
It's going to do so many of the things that people said in the 30s were going to happen
where everybody would only be working 12 hours a week and have everything they need.
Right.
That didn't happen.
We know how that worked out, but there's the very pro side.
And then there's sort of like what I think maybe Neil Postman's view would have been
if he were writing about AI today, which was his view on prior new technologies, which
was spending too much time simply on trying to block them is not the most useful thing.
We're better off harnessing the positive and regulating the potential risks.
And that would maybe be in the middle sort of view.
What do you think makes the most sense based on what we know right now as an approach?
Well, I think that talking about it like it's a normal technology and not something
smarter than you is basically misguided and having the wrong conversation.
So it's not even in the right category of the stuff Postman talked about.
It's not electricity.
Right.
It's not nuclear power.
It's not nuclear weapons.
It's not even a super virus.
It's something that has its own plans.
You don't get to just plan how to use it.
It is planning how to use itself.
So a totally different paradigm needs to be applied here.
I mean, imagine if somebody was like, well, we're about to contact these aliens with much
more advanced technology that think faster than us, that are smarter than us, that have
been around the stars for a while.
They're landing.
We're not quite sure exactly when.
We're not sure how fast their, you know, spaceships are.
You know, it might be two years.
It might be 30 years.
And somebody is like, well, you know, this is just like electricity, right?
Let's use the cool stuff about their technology, but make sure they don't do anything bad.
Right.
Let's say it in that way.
Precisely.
Yeah.
So, so then what do you, what do you think needs to be done at this point?
Um, I mean, the, I, one of the cruxes of the issue is can you do nice things with it?
And the like grim dark message that I, that I am bearing is that we don't know how to
do that and we're not likely to figure it out in time.
And there's a very large gap between where we are and understanding what goes on inside
the frontier eyes and under, and being able to shape them in detail in a way where they
go on wanting to be nice or even in a certain sense wanting to be, you know, non-agentic
to like just do particular tasks and do them without lots of side effects in like the most
normal possible way that they can, that they can accomplish those tasks.
Like even building that sort of limited thing, nevermind something that is really friends
with you is I think beyond the range of what we're going to figure out in the, in the foreseeable
future.
And I do think we are storming directly ahead on capabilities.
That looks like a giant disaster in the making.
So yeah, I, I side with, well, I think my, my basic factual point of view is if you,
is if that, is that, if we do not somehow avoid doing this, we're all going to die.
And my corresponding policy view is that we should not do this, even if that's very really
quite hard and requires us to do some unusual things.
So in terms of what to do, I mean, I guess if everybody came around to your view, you
would think everybody developing these technologies would just give it up, right?
On their own, they would say, wow, Eliezer is right.
I share that concern.
The right thing to do is for me to stop working on this, assuming that that doesn't happen.
What is it that you would like to see?
Should this type of research and development be made against the law?
Yeah, basically, I think that we should track all the GPUs, have international arrangements
for all of the AI, all the AI training tech to end up in only monitored, supervised, licensed
data centers in allied countries.
And if you're any time you have any, and just like not permit training runs more powerful
than GPT-4, if we're not going to do that, then we should monitor all the training runs
larger than GPT-4, or rather like GPT-4 sized, and keep the model weights inside only the
licensed regulated data centers, and track who is running them, store the outputs someplace
so you can look at the outputs with the warrant.
A bunch of the tech, like that whole line of reasoning is not that regulating the stuff
will protect you from a super intelligence because it will not.
That's more in the hopes that people change their minds later, maybe after some major
disaster that doesn't kill everyone.
And in that case, the technology that you would need to sue somebody who killed a dozen
people in a hospital or cost $100 million worth of damage or whatever, the technology
would need to know who did that and sue them.
And not just have all the AI development go to places where they couldn't be sued is
the same technology that you maybe would need to have a off switch where you don't like
press the off switch to deal with the super intelligence.
The super intelligence does not let you know that you need to press the off switch until
you are already dead.
But if we're lucky enough to get warning signs, then the civilizational infrastructure
to have a pause button is the same as the civilizational infrastructure to sue people
who do small amounts of damage, like small as in survivable, as in there were survivors.
As someone who very clearly you've expressed your very serious concerns with this, do you
also see this technology as something that could and this doesn't mean that it would
or that it would be a good trade for the things that you're talking about.
But do you also see this as a technology that could, for example, do analysis of the human
genome such that it would accelerate us in terms of our ability to cure or prevent disease,
the likes of which could take who knows how long without such technology or when it comes
to energy or whatever.
Do you also see that side and or does it not matter because the downside is so huge?
I mean, if we knew how to build an AI that did exactly what we wanted, we could thereby
spread out across the galaxies, turning them into our cities full of sapient, sentient
beings enjoying themselves and caring about each other and generally living happily ever
after until the last of the negentropy runs out.
That's always been the dream.
We don't know how to do that.
It's physically possible, but the art and technique to do it is beyond us in the present
time, and it's going to not be gained in the next five years.
I'm not sure how you would task a government bureaucracy to recognize it, even if somebody
came up with it in 40 years.
There's a sort of basic question about whether our civilization is smart enough to do something
correctly on the first try at all.
The way science usually works is that you have a bunch of lunia-eyed optimists with
wacky theories who storm ahead on their basic research problem, and they are wrong.
They go back to the drawing board, and they're wrong again.
The next generation is a bit more cynical about how hard the problem is, and eventually
people work it out.
If we were allowed to do that with superintelligence, I would have much, much, much less fear of
the outcome, still some fear because you're playing with pretty high stakes there.
You might have somebody who, the people who end up figuring out how to line it might misuse
it.
There would still be that major threat, but it wouldn't be like the automatic extinction
scenario.
If we have the textbook from 100 years in the future that contains all the simple ideas
that actually work, that takes so long to identify and practice, the relus instead of
sigmoids for those of us who've been following AI for more than the last couple of years and
know what I'm talking about there.
There's all kinds of places in AI where people tried to do things using complicated techniques,
and they didn't work.
20 years later, they came up with a simple technique that actually works.
If you have the textbook from the future with all of the simple things that actually work
in practice for alignment, it is probably not hard.
You can get all the goodies, but we don't have that textbook.
The first problem is the amount of time it takes to get that, where capabilities are
storming ahead because you can tell whether things are working or not on capabilities.
With alignment, you've got to know that at the point where it's much smarter than you,
it's already aligned.
You don't get retries past that point.
It's more like launching a space probe that has to land on Mars correctly the first time
than it is with building a car, watching it break down, and tinkering with the car.
If it doesn't, it might break everything so you don't get to try again.
The entire human species is packed on board the Mars probe, and the first rocket that
goes high enough has to land on Mars.
There aren't even really good analogies for it.
Humanity has not faced an issue like this before, which is why we're still around.
I think looking at this, that this is just clearly beyond the reach of our present civilization.
It's not close.
It's outside the range of things that you could reasonably, that you could tell a reasonable
story about people doing successfully.
That's why we need to back off, or rather, I predict that if we don't back off, we die,
and I wish we would back off.
I don't predict that we will.
Well, certainly there's no way to wrap up such a dark vision in any way that is going
to be satisfactory or calming to the audience, I think.
But that being said, Eliezer Yudkowski is taking this very seriously as the founder and senior
research fellow of the Machine Intelligence Research Institute.
And I really do appreciate your time and insights, even if they are not optimistic on this issue.
Thank you.
Thanks for having me on.

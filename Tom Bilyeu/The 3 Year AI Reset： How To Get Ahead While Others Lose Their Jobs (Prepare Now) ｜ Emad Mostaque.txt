How do we make sure it doesn't kill us?
Or how does it make sure it doesn't enslave us?
Or how does it make sure that it doesn't give us eternal suffering?
And I realize this could be the real thing that unlocks humanity.
AI is not going to replace humans.
Humans with AI will replace humans that don't use AI.
AI is thrilling, it's very exciting, but there is a non-zero chance that it poses
a existential threat to the human race.
So over the next three to five years, how disruptive do you think it will be
and what are people not prepared for?
I think that's an excellent question.
So you know, the future is always hard to predict and existential is a big word.
Existential means no more humans.
So I personally think the AI will be absolutely fine.
As a base case, it'll be like that movie,
her, if it ever gets this artificial general intelligence.
Like humans are kind of boring, goodbye and thanks for all the GPUs.
But you could be wrong because what we're doing is creating something that's more
capable than us in narrow fields.
And the question is, does that generalize and then become
viral?
We've seen an instance of COVID and that expansion.
We've seen programs that can explode nuclear reactors,
like Stuxnet and others.
What happens if you start combining these and you get a misalignment?
So it's got a strange objective function.
Are organizations already like slow-down AI's?
You know, and like Germans are the most sensible people that we probably know,
and yet they committed the Holocaust.
And we see this over and over again where organizations chew up people.
What if an AI takes over an organization and then decides to do something disruptive
or something terminal, such as creating a virus?
We don't know about that, but that's at the extreme.
When we look at impact, we have the more mundane.
The more mundane is what happens to programmers when everyone becomes a programmer,
just like photographers.
You know, now you can take amazing pictures with your thing.
What happens when Google's MedPalm2 model now can outperform doctors in medical diagnosis,
but also empathy, according to the latest paper in Nature.
This is a fundamental reworking of information flows that's going to be massive
disruptive and deflationary, even with what we have now with no more advances
as it becomes enterprise ready.
And we have a continuum from that disruption to the productivity enhances
to potential existential threat.
If we keep doing the models as we do now, which is we're not exactly sure how they work
or their capabilities, but we keep building anyway.
Now I want to get very specific about what the level of disruption is going to be.
So when I look out at this and I think about, okay, we're creating something that is going
to be smarter than we are, certainly in a narrow way, but possibly in a more general way.
But even if it's just narrow, is there going to be any job function that isn't going to be
at a minimum augmented by AI?
I think if you look at the employment share of industry, something like oil and gas has like
3%. It's mostly like building giant machines.
Is that massively affected by this AI?
At the edges, yes.
Things like programming where you're talking to computers massively.
I mean, now basic programming, the bar is raising fast, fast, fast.
And so you've got everything from knowledge work to heavy industry.
I think it affects just about everything, but some areas far more than others.
The two areas that I think it will affect the most are probably healthcare and education.
Neither of those are fit for purpose.
We're in America, we know that, but across the world, no one's really happy with their kids
schooling. And again, medical care, if anything goes slightly wrong outside the norm,
we all know how frustrating it is.
We can finally have personalized education and healthcare at a fraction of the price.
And the two biggest drivers of US inflation over the last decade,
education and healthcare, they make up about 80% of the increase.
So that will be disruptive.
And then like I said, any type of knowledge work will be disruptive.
And we're not sure how much work because when so my own self, what we do is education,
that's a big part of it, but also just content creation.
And so when I look at the fact that we can already clone my voice, we can already create
a tombot that will answer questions I have answered before in a very similar fashion to how
I will answer them in our video game flow. We're already making 3D objects, which when we looked
at, I don't know, two months ago, I thought, okay, this is still 12 to 18 months away, 45 days
later, we're using it actively in our pipeline. You've got text to video, which still a little
awkward, but it's getting better insanely fast. We do all of our concept art now in AI.
So we have as a company that doesn't even have like an AI expert on board, we're just learning
as we go, we're already deploying it like crazy. And when I look out, not even, you know, three
years, when I look out a year, all of this stuff starts to very rapidly become a centralized point.
And so we're already saying, I don't need to hire more people, I just need to make my people more
efficient. And so that an entertainment company didn't even make the list that you just said.
So there's a lot of people that I think are going to get disrupted by this
that may not be like the most extreme, but how far down do you see this trickling?
Anything you can do in front of a computer, basically, goes away or just becomes augmented?
The bar lifts, the quality expectations are higher. AI is not going to replace humans.
Humans with AI will replace humans that don't use AI, because you can see that in your workflows
right now. There was a paper by OpenAI where they estimated 15 to 50% of tasks get automated or
improved. And so, you know, it affects people in different ways. You have a company where you've
built a culture and you, again, you're building 3D assets. It becomes amazingly more efficient.
We just released, we contributed and collaborated on a 10 million 3D object data sets. So by next
year, you'll be generating 3D, literally live. In a couple of years, you'll have HD movies,
we can finally remake Game of Thrones season eight and other such travesties, you know?
But the speed of this is something whereby it's happening in every media type at the same time,
and it's easy to use. Web3 had some great ideas, but it tried to create a system outside the existing
system and all the money was made and lost at the interface. This is just so seamless because
there's no friction. Your mom can use this technology, you can use this technology,
you don't need to be an expert because it came and was trained from our content and our collective
content as it were. And now it's just easy to implement and use. So I think this is the big
differentiator between this and other massive advances because they required infrastructure,
the internet, there was the big lift up, you know? You have the consumption period of Web2
when the cost of consumption dropped to zero. Now the cost of creation is dropping to zero
and humans plus AI can massively outperform humans that don't. It's a forcing function,
everyone has to use it. And this again is dual in that it can be disruptive, but it can also
create massive value. Yeah, so I'll agree with that. I think that, so I guess let me lay out my
whole thesis for you and for everybody listening because I want to take us through what I think
is very real doom and gloom and I'm not doing it to be a naysayer. I'm doing it because I think
these are going to be the things we have to contend with. And if people go into this blindly,
which I think they're doing right now, I think most people are burying their head in the sand,
they're not paying attention to this and they're going to wait until something really forces
their hand and by then it's too late. Yeah, the way that I put that is this is like COVID before
Tom Hanks. Yes, very well said. Everyone's talking about this, your mom's talking about this,
but the Tom Hanks and the NBA made it real. Very true. And then we had a very poor response,
which I have a feeling will be very similar to what we do now. Okay, so here's how I see this
going. I think right now for the next year, let's call it, you need to learn how to use it. This
will be your window to get efficient. Companies probably aren't going to start lopping people off
yet, but I'll just say within my own company, so when I think about filmmaking, I went to film
school so I'm very experienced in this flow and even in a 3D world to create, let's say,
a short cinematic. So it's like a mini movie but done digitally. I mean, you might have 35 people
touch that thing from the creation of the assets through the moving of the camera special effects.
You might have 50 people touch that. And if that really does become text to output, now 50 people
become one. And so when you get a 50 to one ratio in certain areas, obviously it's not going to be
like that everywhere, but when you have certain areas that go from 50 to one, take programmers.
I've heard you say there will be no programmers because writing code is just a way to talk to
a computer. And if you have AI that will interface with the computer for you, why would you ever
need to write code? So that's going to steamroll through society. That is just going to mow people
over. So again, I'll give them 12 months, but even in my own company, if you're not actively trying
to find a way to integrate it into your job function, I'm already looking at you sideways.
A year from now, if you're not really good at either documenting how it is completely useless in
your job function or showing how you're using it, we will find somebody that can do it. I'll be shocked
if a year from now we don't have a head of AI. So three years from now, I think this has created
a crisis of meaning for a lot of people. And I don't know if you remember that the whole learn to
code thing where it was like, Hey, AI is going to put drivers out of work. They're going to be the
first to go. And everybody was like, teach them how to code. Now, the way people responded to
that always confused me because that was the right answer at the time. Now, knowing what I know
about code replacing not so much, but you have to go learn a new skill. There is no other option
other than going on the dole, right? So you're either going to learn something new or you're
just going to forfeit your career basically. So what do you think about that? Do you agree
that that is a very real thing that's going to sweep through? I do agree. I think that again,
we're not sure exactly how this is going to pan out. But probably the best mental model I figured
out to think about this technology, it's like really talented grads that occasionally go a bit
funny. They can draw, they can code, they can make 3d models. How would your business be
affected if you could push a button and infinite grads came out? How would your personal life,
your society? And this is why I think it's quite deflationary. The only question is,
can we create new jobs to make up for that? And that's difficult. Because you said we can.
I doubt we can, to be honest. I think this is an e-commerce disruption that's far bigger than
COVID. And the important thing here is COVID, you have the disruption and everything bounce back.
You're at record employment now and things like that. With this, there's a lot of never the same
again. It's like you talk to your kid's school teacher. I can't set essays for homework anymore
because of chat GPT. And there's no way to stop that. So what is never the same again? And it's
happening everywhere all at once. So this technology isn't just like, you know, there's a bar of entry
where you needed to have a modem, you know, you need the latest smartphone or something like that.
It has an embedded base that it's seamlessly going into. Look how fast Microsoft implemented on the
consumer side. But enterprise is not ready yet. It's like at the iPhone 2G stage, you just got
copy paste. And next year and the year after, you're something at the iPhone 10. You know,
entire app stores get built because of the demand, because it's valuable. What's happening here?
Again, with the comparison to Web 3, you had to bootstrap value because it wasn't valuable.
And you hope the value would come. There's product market fit today. You're using it in your own
company. And so this is one of my big concerns. And that's one of the reasons I decided to do
open source. So I could stimulate growth. You know, because I think the only thing that can
basically fill the gap is if we stimulate entrepreneurs to create brand new businesses,
brand new jobs. So I think demand will stay for a while. Demand for what? Demand for good things,
good assets with the way that money flows around the economy. So I was speaking at Cannes a few
weeks ago, Film Festival. And you know, I love movies. My first job, I was a movie reviewer,
you know, really? Yeah, British Independent Film Awards, Rain Dance Film Festival,
other things. I knew you were big into video game investing. I did not know that you were a
film critic. I love stories. That's how I kind of understood people because my Asperger's and
other things. And so I said to this, the video game industry has gone from 70 billion to 180
billion over the last decade. And the average Metacritic score has gone from 69% to 74%.
The average movie is 6.4 and I am DB for the last decade. And the industry has gone from 40
billion to 50 billion. What happens when you can make better movies? I think the market expands
because the limiting factor is awful movies in my opinion.
All right, let me run something by you. Okay, so I have a really dark view of not the next 12
months. So call it year two to year six. So it'll be a three to four year sort of span where I think
there's going to be emotional devastation and probably economic devastation. But even if the
economic devastation doesn't happen because of productivity gains, I think the emotional
devastation is going to be hard to come back from. And I think that as the emotional devastation
sets in, the government is going to try to regulate to protect people's jobs. And there you're
going to get like some real weirdness. I also think kids are going to have a junior year existential
crisis of what do I do? How do I future proof myself? What does the world going forward look
like? I think there could be a massive loss of enthusiasm where a feeling of malaise settles
over young people who are just like, why bother? I'm just going to get destroyed by AI. They're
going to be able to do it better than me. Okay, so in the movie industry specifically,
and this is indicative of a big problem that I think that we have coming. And I think the
problems really stack individual and societal. So at the individual level, the big problem you're
going to have is this massive, massive fractionation of right now movies are even less now than they
were when I was a kid. Movies were, there's only a few big movies for the year. Now they're going
to niche down. If anybody can type out a movie in, take them 20 minutes to write the prompt,
and then maybe a day to render, who knows how fast that's going to get. So now all of a sudden,
you can make a Hollywood quality film for an audience of one. And once you start doing that,
now it's what does that do to the industry? I think it erases it. I don't think the industry
changes. I think it goes away. Yeah, I think there's a few kind of components here, right? So
the cost of music consumption went to zero. You saw the Spotify model. Yeah, you still have music
stars. You've got even more crap music now, kind of coming and hitting Spotify and other things,
but people rise to the top. You know, just like you see top podcasters, top other creators,
I think I'll continue because people like common stories.
Yes. Okay, so this is a very interesting idea. So let's stick with music for a second.
Music is still hard to make. It's easier to make than it was before. It's also still hard to get
people's attention. But music now is no longer a shared thing. So music is part of what led me to
the conclusion that I'm at now, which is, man, as kids, it used to be you were either into the
mainstream pop and there was seven to 10 hot bands at one time, or you were into the alternate pop
and there was seven to 10 hot bands in that arena. And you fit into one or the other bucket. There
wasn't the infinite buckets now. You can find kids that are 25 and they listen to Frank Sinatra.
And I'm always tripped out by that. So they don't even have their own sort of shared lexicon of
what music they're into. It's all spreading really wide. So it's really wide and an inch deep.
Yeah, I think it's really wide an inch deep. And you see the primary methods of monetization are
tours, merchandising, community, effectively. You know, this is the interesting thing about
NFTs when they took off and then bounced down and things like that. It was the quickest way to join
a community, even if it did have bad incentive design. So in an era where you can create anything,
something becomes important. What that something is, we have to find out now, right?
Because again, I think it's some common stories. But I could be wrong. I think the
deeper thing that you said was this crisis of meaning. Where is my path forward? What is an
American dream? We're quite privileged. Those are probably listening to this and us on here. Most
people don't really care about this technology. I think on a survey, 17% of people had heard about
chat GPT last month. How is that possible? Well, a third of the world still doesn't have internet.
That's terrifying. But again, like it is kind of like 1.5 million people still use AOL.
You know, like fair enough. So we kind of look at it, but there's something that can reverberate
very, very quickly. And then as you said, there's a sense of malaise because you're not sure what's
happening. And again, the future becomes uncertain. And when the future is certain, things are stable,
you decide based on risk, you do a probability estimation in your own head. There's the percentage
of that, the percentage of that, and then you optimize for that. When you do, you're uncertain
to you minimize for regret. Given these options, what am I going to regret least? And suddenly,
there are no options. Again, I'm at school programming and then programming is disrupted.
What's it going to be? I'm not sure. And some people will throw themselves in and they'll tool
themselves up and they'll become 10 times programmers. Other people won't. And they'll be
left behind. And so I think this is a real question that comes at a time when, again,
being in America, I'm from Britain, but what is America? What does America stand for? What are
the values? These are some things that I don't think America knows now. I think you've seen
increased polarization from free consumption. And now as you get free creation and you'll be
hearing all sorts of stuff, fake news and more, what are people really going to think? And I think,
again, this is a real concern, as you said, from an individual to community to a society level,
because a lot of people don't have an anchor anymore. And that's really scary.
So how do you think that we process through all of this?
I'm not sure. I think that's why we needed to broaden the conversation. That's why I'm the
only AIC that signed both of the letters saying we need to take a pause and broaden this. Because
as an example, you mentioned broaden this, broaden the discussion. Get more people involved. We need
to get more people involved. We need more points of view because this affects us all. It shouldn't
just be a few tech CEOs that control this and you shouldn't have to trust that we do the right thing.
Because our models, we make them once, they go everywhere. Again, what's the R naught
of generative AI? It's off the charts, right? We've never seen anything like this. It incubates
them, boom, and it comes for good and for ill. You get the example of regulation. When we first
started talking to regulators, they were like, how should we regulate it? Now it's a question of them
asking us, how are we going to keep up if we regulate it? Because other jurisdictions won't.
What do you say to that? I say you should still regulate it because it has some real dangers
and harms and we have to work to mitigate those. You can't just have a laissez-faire approach to this
because people will take it and they won't be able to help themselves. I'll give you an example,
meta Facebook, right? We all know the classic kind of stuff. They had a study where they had a
hypothesis 600 that if you see sadder things on your timeline, will it make you post sadder things?
So they took 600,000 of their users and tried to make them sadder. And guess what? If you see
sadder things, you post sadder things. What do you think is going to happen now that they have
generative AI on threads and things like that? And they can hyper-target you, hyper-personalize
it and whack Scarlett Johansson's voice to tell you to buy soap. This is a dangerous thing, right?
What happens to our kids, again, who are growing up whereby they won't know what's going on and
they have very malleable minds? And none of that is illegal. But I think it's an undesirable outcome,
right? And then you've got the bad actors and then you've got the politicians using this technology
and then it goes even crazier than that. So the answer is I'm not sure. Nobody's sure. But I think
the only way that we can try and figure this out is to work together to make these issues known.
Again, the existential stuff gets the headlines. We could all die. Nobody really understands what
that means, but it can happen, right? Okay, there's a probability of that. But there's some real
harms today and real opportunities today. And we have to focus on accentuating the opportunities
and getting the harms out there and dealing with them. Yeah. And I definitely want to spend a very
extended period of this talk talking about the opportunity and how we capitalize on that. So
anybody that's with us now, trust me, we're going to get to that. But I think we're just beginning
to scratch the surface of how this goes wrong. And I really want to map out sort of where you
think the edges of this are so that then I can hopefully get a sense of what you think the
regulatory framework would be. But let me give you one idea that somebody posted today on Twitter
and it really hit me that people are even thinking about the problem in the wrong way. So there was
an artist and he was looking at some post about AI and he replied sort of angrily that, oh, well,
people don't even understand. Sure, there's going to be a ton of like instantly generated crap,
but it's all going to be bad because there's still a very small number of people that have
good ideas. And my response was, if you think that ideas are safe, you're really going to get
caught off guard. So going back to the idea of what are people unprepared for? I think they are
unprepared for what you were just talking about where the AI, so the human mind is a prediction
machine. It is constantly trying to figure out what does this next movement of my foot equate to?
Am I going to stay up, stay on balance? That rustling in the bush? Is it a tiger? What is it? If I
put money in my 401k, am I going to be able to retire? You're constantly predicting the future,
constantly. And whenever that prediction engine breaks down, there's going to be a tremendous
amount of anxiety. And also, I think a pretty big unknown in terms of how it's going to impact
society. So right now, we're building something that is incredibly good at recognizing the patterns
that we kick off. So we are optimized to identify patterns and move accordingly. And I would say
people that are hyper intelligent or people that they notice patterns faster, more subtle patterns,
and they understand their implications and how to make sense of them. Now we're creating something
that's already proven to be so much better at pattern recognition than we are. Just take art.
So for people that don't understand how the art is created, it looks at a field of noise. Here are
all the possible things that these could be in any of these pixels. And from that field of possibilities,
it pulls forth the most likely placement of pixels and colors based on what you type. That's
insane. So that level of pattern recognition, as evidenced by the art that it can generate,
is truly mind blowing. So this guy is saying, okay, hey, at least ideas will be the last bastion
and you'll never be able to get rid of me, the artist, because I'm the one with taste, I'm the
one with good ideas. Not realizing, no, no, no, what AI is is a pattern recognition machine. It
will recognize the greatest ideas that have ever been had, what they have in common, and will be
able to predict the next great idea along that thing. It doesn't even have to just regurgitate
what it's already seen. It can like figure out what that sequence is and what that next part
of the sequence could be. And on top of that, it's doing that with humans. So AI is already
extraordinarily good. This is why people think their phone's recording them when it serves an ad.
Oftentimes, Target, using their AI, knows that you're pregnant before you do, if you're a moment,
because they know what to pick up on. So AI is going to get extremely good at understanding us
at an individual level, serving us up exactly what we want right in that moment. And that gets
dystopian really fast. Really fast. I mean, again, when you combine it with the social credit score,
as you've seen in kind of China and other things, you gamify life, and you have a system of complete
social control, a panopticon, as it were. The pattern recognition was the missing bit,
whereby you had a level of pattern recognition. So for taste, what do you have? TikTok. Shine.
$100 billion companies based on old school algorithms, before we even got to generative AI,
which as you said, it can take images out of noise. Stable diffusion, the model that we
collaborated on now that we lead, we took 100,000 gigabytes of images, and the output was a two
gigabyte file that acts as a filter. Words go in, images come out. Because why is that discrepancy
in size meaningful? 50,000 to one compression is not wind zip. If you remember Silicon Valley on
HBO, it's way beyond that they managed there in terms of compression. It's unheard of compression.
Is it compression or is it something completely different? It's intelligence. It's learning
the principles. How much information do you see? And then you learn the principles,
and you then spot the tiger in the bush. You learn what's next, literally GPT, and these
language models, they predict the next word. That's all they do. They pay attention, they
predict the next word. And that was the missing part to intelligence that now is there. We've had
the first studies now come out that show that the language models score higher in creativity than
people. And again, think about TikTok, think about Shine, think about how those old school
algorithms are already targeting you. Facebook needs 17 data points to know you better than
your friend. As he said, target knows you're pregnant before, and that was old school, now
it's even better. And you think about where that leads to as well. It's kind of crazy,
because it can be more creative than you, but are people creative? One of the things I like to say
in my speeches, I've just been learning to do is like, are you creative? How many of you in the
audience are creative? Three to 5% put up their hands, maybe 10 to 20% if I'm like in a movie
studio, movie filmmaker, kind of like you. And I say, how many of you believe that every kid is
creative and everyone puts up their hand? And then I ask, how many of you were kids once? And
90, 95% put up their hands. So I know who the cyborgs in the audience come from the future to get
me are making that for future. Something happens where we're told we're not creative. And obviously
some people are more creative than others can tell better stories than others. But the reality is
that the average level of barrier to this has dropped for every human. But
much of what we consider art, or much of which we consider media, shall we say,
already is by the numbers. I was at a Blackpink concert last weekend, you know, to my daughter.
But you dare say something bad about Blackpink. And they are awesome. You know,
it was an awesome, manufactured experience. It was I, premium mediocre is how I kind of say
these premium mediocre, premium mediocre. That's hilarious. It's nice. But again,
it's massively manufactured. It's entertaining. Right. And so much of media is already that like
true art, true artists, you know, that's something different. Like, is it the medium itself
and the aestheticness of it? Well, AI can make something more aesthetic than anything can understand
the nature of aesthetic. Like, how do you make an image more aesthetic? You say, make it more
aesthetic. Just like if you use a GPT-4, you can say, make this punchier, make this punchier,
make this punchier. You know, you can have a letter and then you say, I'm firing this person.
And I want to make them feel okay about it. And then it will redraft it in those terms,
or you can say, I want to drive the knife in, but not in an appropriate way. And it'll do that.
And we can literally, anyone on this call can kind of try this into this can try that now.
So I think this is just, as you said, the wrong thing people are thinking about,
the wrong model people are thinking about as well. And that's why I always go back to this
concept of the really talented grad. Because these models are a couple of gigabytes big.
Again, stable diffusion, the image model is two gigabytes and can generate any image of anything.
We'll get that out to 200 megabytes. GPT-4 is probably 100 to 200 gigabytes.
And it can pass the bar exam, it can get a freaking Stanford, it can do whatever.
That's insane, because it's not compression. Like you said, there isn't a copy of all the
data in there. It's figured out the essentialization of these points. And it's replicable. This is the
thing. To clone Google or meta, you need to have a gigantic data center. And then much of the energy
is in the processing to target you ads. With these, we take giant supercomputers and we pre-process
and package the information. So the output is this knowledge filter that something goes in and
something comes out. A prompt goes in and an output comes out. That's something quite different.
I don't think people appreciate. And again, this is where I use the grad example. Push a button
and those weights, the file, the model gets replicated to 10, 100, a dozen, a million.
And what happens when rather than dealing with them one to one, you have a thousand of them?
So in a year, I want to really understand what you're saying about the grad thing. So
when you say that, you say it in a way that's kind of funny or cheeky. But what you mean is a really
smart person is now present in that role. We have figured out how to make human scale.
That is what fundamentally intelligent humans scale. Yeah, who can listen to instructions.
So you look at something like clawed to by anthropic.
You have something, the input is a prompt when you type into GPT for stable diffusion or mid-gen
or something like that. Claude anthropics model can take 10, 100,000 tokens. It can take a prompt
that's like 60,000 words, which is a whole book. Jesus. Yeah, you can give it like the whole of
Ulysses and the whole of, I don't know, the Odyssey by Homer. And you can say,
combine these to make another book and it will do it and it will work. It can follow instructions
really well. Occasionally they hallucinate, but even hallucination is a misnomer because when you
compress that much knowledge, like GPT for is probably 10 trillion words, 10 trillion, 10,000
billion words in 100 gigabyte file. It's something else. And so I use the word grad because I want
to make it relatable, but it is literally like, imagine if you had a grad in the Philippines,
you know, and they're doing good work and they're following instructions well. That's great.
But what if you had 100 of them looking after each other's work and double checking?
Metta had a paper called Cicero where they took eight language models and got them to check each
other's work, outperformed humans in the game of diplomacy the first time ever. In a year when we
have this, before it, you'll just say, I want you to go and look at everything MAD said for the last
year and figure out the stupidest stuff he said. So, you know, if I can avoid it and the smartest,
most interesting stuff according to what I know and all of my podcasts to give answers, to give
questions that the audience will really like based on my ratings and based on what people look at
through the YouTube videos and things like that and what they're most interested in. And it will
just happen automatically. How many graduates that take you to do? And then what happens when they
stop being graduates and you can actually train them up to be like, you know, experienced members
of the team? How long will that take? A couple of years. You can reboot your life, your health,
even your career, anything you want. All you need is discipline. I can teach you the tactics that I
learned while growing a billion-dollar business that will allow you to see your goals through.
Whether you want better health, stronger relationships, a more successful career,
any of that is possible with the mindset and business programs in Impact Theory University.
Join the thousands of students who have already accomplished amazing things.
Tap now for a free trial and get started today. This is why this is terrifying to me.
So, I am a very optimistic person. And again, I promise we are getting to how you take advantage
of this disruption. But I don't like to face a problem naively. I want to face it as head-on as
possible so that I know my solutions are real. And when I look at this from my own perspective of,
okay, I'm trying to build a media company, which right about now is a very terrifying
time to do that. And I'm thinking about, okay, it's very optimistic when I look at, oh my gosh,
I, as the founder of this company, I get access to all these grads, as you're calling it, this
absolute proliferation of very intelligent people that I can now put to work in my company. The
problem is I'm now competing against other people that have the same thing. And you get in this
ever-escalating arms race where there is a real chance for fatigue. And so, I think what ends up
happening, and we were talking before we started rolling, it is very important that people understand
the following thing. I think this is just a truth, but people certainly need to understand,
I believe it. This is a core belief that drives me. That we, you get to a point where you need to
know, okay, I matter, I'm doing this thing, and that's how I'm contributing to the world. And
I need to be in there working hard, accomplishing, getting better, moving towards something. And if
I'm not moving towards that thing, then I'm going to have a profound sense of disease. And if I'm
not making that progress, then I'm really going to fatigue out on something. And so, if people are
just treading water, because they're trying to build something, and they're competing against
somebody else that has these 10,000 things, and it's just constantly changing, and I can't predict
the future anymore, and I don't feel like I'm making progress, I'm just going to back off.
Like some part of me is just going to be like, ah, what am I doing all this for?
What am I doing? Yeah, I mean, it's like the outsourced revolution, right? Where so many
jobs were outsourced. And a lot of people felt that way, like, you know, we'll outsource you to
China, we'll outsource you to India, we'll outsource you wherever. And again, it's just
to happens that there's a computer on the other side of that versus an Indian or a Chinese person.
And so we've got kind of repetition of that, but at ridiculous scale affecting almost every
single industry that's intermediated by a computer. And so this will cause, as you said,
a crisis of confidence to many, and it impacts white collar workers, not blue collar workers.
It flips, I think, the global order to a degree as well. Because here in the West,
we've maxed out our credit cards. We weren't going to deflation, I think, coming off high inflation.
And all of a sudden, we can't print our way out because we just printed the last of our money
for COVID. Whereas in the global south, what you have is this technology can cause them to leapfrog
just like they leapfrog to mobile, missing PC completely to intelligence augmentation.
Why can't we print more money? Well, because kind of we're just coming up to a limit of what's
literally mathematically possible, given the debt to GDP ratios and others. We can continue,
but it's kind of pushing. If you're deflating, then because so here's my layman's understanding,
but this is something I've really looked at. So I'm a pretty educated layperson at this point.
Inflation is largely, some people will say entirely, but I'll say largely, a function of
how much money you're printing for people that are new to the idea of printing money. It's
government approved counterfeit. So the government is allowed to print as much money as they want.
They're literally just making it out of thin air. They're adding zeros and ones to a database
somewhere and money finds its way into the system beyond the scope of this conversation.
But there is no theoretical limit to how much they can print. Now, what you run into problems
is the hyperinflation of the currency. But if you're saying it's a deflating currency, which
actually makes sense to me, given what we're talking about, then printing seems to make
a lot of sense, seems to buy me more room. Eventually. So what's going to happen is that
you've got a decrease in inflation now because of base effects. So if you're going into a bit
of macroeconomics, and then you'll probably have a bounce back next year, because you still
got a lot of inflationary pressure, and then the collapse occurs. Why? Because that's when
the job losses start hissing. And the question is, can we create enough on the other side? We've
got to have a productivity boom from companies. And do you think the job losses are coming from
AI or some other force? From AI and from other forces as well. Again, what we've had is a sugar
rush post COVID, a good strong economy as all of the excess savings go back in. Because if you
look at excess savings, people saved up a lot. And that's almost now depleted. By the end of the
year, the excess savings will be depleted. You've got some hangover effects from inflation, then
you move into deflation the year after. And then it's a political hot potato around printing more.
Because this isn't again, like COVID, because what happens is the job losses just start and they
just keep going. It's not like you had a 2008 crash, or you had a COVID where everyone's kind
of suddenly going, it's like, it's a bit like boiling a frog, you know, or lobster. It's just
going to start and then it's going to accelerate. And then it's going to be like, at what point do
you take the big fiscal action? It takes a few quarters of the economy actually shifting. So
this is a lot of hypotheticals, right? But the bottom line I think is this. The nature of US
society, Western society will change. I think the biggest adopters and fastest adopters of this
will be the global south, because it allows them to create value. It allows them to financialize.
It allows them to take a big leap forward. And so I think that's got some huge implications
geopolitically and others. But a lot of upside as well, because I think you can solve a lot of
the world's problems with this. But it's so messy. Because fundamentally, it comes down to what you
said. As humans, we're trying to figure out what comes next. And we suddenly have a computer that
can do that even better. As humans, we're storytellers, we're made up of the stories that make us up.
You're a filmmaker, you know, as a film review, all these kinds of things. This can tell better
stories. And that has such a big effect on our societies that none of us can really wrap our
heads around it. Like, I've got a background in economics, management, whole bunch of different
things. I can't wrap my head around it. And so we're just going to have to see how it goes and
then try to mitigate. But nobody's got answers to this. And in fact, as you said, most of the people
aren't asking the right questions. Yeah, you have said that the shit show happens next year.
I have a feeling that what you were just talking about is what you mean by the shit show,
that we go deflationary towards the end of the year. Yeah. So towards the end of 2024.
Yeah, we've got like a burst of productivity enhancement. And then you start seeing job
losses, you start seeing question and meaning you've got the US election next year. My God,
that's going to be awful. Yep. Terrible fucking timing. Well, I mean, you know, what you'll have
is the week before the election, fake videos appearing everywhere. And they'll say the same
things, you know, so and so has a brain infection or something like that. And they'll be identified
as false, but it doesn't matter. They still discourages people from polling. But then what
do elections look like by 2028 when this technology is in every single pollster's hands?
Yeah, that's where we get into the blockchain. We'll save that for a little bit down the road.
Yeah. Okay, so now I feel like we're getting close to the problem set being on the table.
There's one more thing that I think it's important to put on the table, which is
I don't think that the amorphous thing that is society as the world turns history, the
grand arch history, however you want to think about the real long timelines. So even if the
long arc of history bends towards improvement, which I think it does and think it still will,
I don't think it cares at all for any one period of time. And that unimaginable amounts of human
suffering happen routinely throughout our history. And I have deep concerns that if we are not
incredibly thoughtful that this will be one of those moments. And I look at what's going on in
France right now. I think it's dying down. I can't tell if it's dying down or the coverage is dying
down. Hopefully it's actually dying down. But France was like really having some struggles. And
if something like that pops off over not in any way, shape or form to make light of what happened,
but it isn't mass joblessness, which is going to have a far wider impact.
What happens when you have that and it's global? I mean, I think this is the thing,
it's every government has, every education minister in the world has to grapple with why can't I set
my kids SAF's homework again? Have we ever seen anything like that before so quickly? I don't
think we have. And so you could see this literally parallelized around the world.
Or not. We're not sure what really kicks off some of these things. Like right now,
we've got the Screen Actors Guild kind of protesting. Today, we just had a couple of
actors leave Oppenheimer. Part of that's monetary, but already you're seeing AI fears,
like front and center. You wouldn't even have thought it six months ago. What's it going to
be like in a year when you can generate or two when you can generate whole movies? And then
you describe how you want it done. It's Hollywood level. It's really difficult for governments
to react to this, to adapt to this when like in the US here, we're still reacting to Section 230
on the internet. They're just getting to go out to the internet. All of a sudden AI just comes
and sideswipes things, right? And I think again, the only way to do this is if you can create brand
new jobs quicker than anything. This is one of the reasons again, like I said, we focus on open
source. It's why you need to have things like regulatory sound boxes so that you can experiment
and try and you need to stoke innovation because you don't, you'll never get an innovation phase
like this either. I think this is a step change and a regime change in the way that society operates
because we originally are an oral species. Then we figured out to write, then we had the Gutenberg
press and it took all these words, but it took them down into black and white and made society
quite black and white because it couldn't capture context. Whereas these models can capture context,
they can capture principles, they can capture more. So again, you know, you're writing this down,
you won't have to in a year or two. It'll just be automatically add to your memex, to your knowledge
base, right? Also, the AI will just be attached to my head. It will read the brainwave patterns and
know that that's what I need to remember. And that sounds crazy, but like we had Mindviz, a paper that
we kind of published from our meta art division, whereby you looked at a picture of a mug, took
an fMRI, and then it reconstructed it using our image model. That doesn't sound crazy to me at all.
This is what I'm saying. People do, they're not prepared for what's coming. They're not prepared
for this level of change, and they really aren't prepared for the rate of change. And it isn't
just like an arc like that. It's lots of S curves all at once, all around the world, where every
single company is now thinking, what's my generative AI strategy? Yes, for when it pops off,
correct? And every government's thinking, how can I stay competitive? And this is why I said like,
it's a race condition, where everyone is trying to do the same thing or similar things, and you
can't be left behind, you can't not participate. And it's been a very long time since we've seen
that. And there's a world before this technology in a world after this technology. Like I don't
think again, I've got two kids, what does the world look like in five years, let alone 10 years?
I have no idea. And I'm in the middle of this. Because it's just impossible to see the smartest
people that I know, they used to be able to see years in advance, they can't see more than a year
or two. And this sounds, again, very apocalyptic. But then like I said, we're going to get to the
good bit in a second. In every crisis, there is opportunity. Our society is broken as it stands
already. And I think this is a chance to reshape it for the better, and solve a lot of the biggest
problems that we've been facing, because of our slow dumb AI's, because of our organizations and
institutions that we are all frustrated with. And I think this is a big upgrade from it. The
example I had to give is there's the amazing poem by Ginsberg Howe about Molok, this Carthaginian
demon of disorder. I think where that came in was text. Because we had to centralize everything
down and put people into boxes, because we couldn't have systems to understand the context.
You can't have personalized educational, personalized healthcare. Because you couldn't
scale humans, there weren't enough talented humans. Until now. And so I think that is the
incredibly dangerous part, because all of a sudden, from economic pressures, you flood the market.
And it's the incredibly motivating part whereby there's a shortage of talent for everything
that's important. And there isn't anymore. But then the nature of talent for jobs and things
will transform. And I think the economic abundance that's created on that, that's the flip side of
this, as well as the ability to fix our broken systems. Alright, I'm going to give you my timeline.
I think the next year is going to be a lot of fun for people that embrace it. It'll be a period
of time where some people can ignore it, and they probably won't really notice. They won't
realize how fast things are changing. Although follow me on Twitter. I post routinely like,
hey, here's something I didn't think that would happen for 18 months. And we're now 45 days later,
we're using it. Things are really, really moving faster. But for the next year, I think people
will be able to ignore it. And they won't realize that it's growing with such steam and ferocity.
Then year two to six, I think that there is going to be pockets of extreme suffering.
And I think deaths of despair are going to skyrocket. And I hope it's not a the world is
burning riots kind of thing. It'll probably be more quiet and insidious than that. But I really
think that we're going to lose people on the upper and lower ends. I think people that are
old are going to just completely check out and say, I can't keep up. I'm too old. I don't want to
learn this new stuff. I think people that are young, it's the only thing they know is change so
fast that they can't see around the corner. I think that will be absolutely terrifying to them.
And they're going to retreat into the levels of entertainment, sex bots, AI friends that are
more loving and kind than their other friends. And it will be a collapsing inward. Now,
as somebody who is prone to collapsing inward, the biggest thing that's held me back as an
entrepreneur is that I like being alone with my own thoughts. And that if you then layer anxiety
on top of that, and then you give me an AI that's actually better to me than anybody in my real life
has ever been, and then you give me maybe some drugs, I will truly collapse in under my own
weight. Not me personally, I have defenses, but I'm saying that personality type is really going
to struggle. So I think right there, you're going to lose a generation if I may be so bold
on the upper and lower ends. Then either on the 10 to 20 year time horizon, and I leave it that long
because look, any prediction that you make with a timeline is guaranteed to be wrong. So I'll try
to give myself at least a little bit of buffer. And I know that everything I'm saying probably
directionally correct, timeline probably way off. But 10 to 20 years, my rough estimate,
that's where we're either in Terminator and we're running from radioactive rubble to radioactive
rubble fighting the machines, or it really is a utopia. And I think that there is a real shot
that we get to the closest things that humans are going to get to to a utopia where things are so
plentiful. Yeah, everything we want is available. We reorient our human psyche not to acquisition,
but rather emotional contribution. And we'll paint that picture more as we go down. But that
that's sort of my rough thing. Yeah, I think these are crazy timelines. Like, not because I
disagree with them, because the fact is that they're crazy. You have a year of incubation,
then you have contagion, and then you've got a stark thing. I don't think we'll be chased by
robots. They won't need to chase us. They're far more efficient than that. I think that basically
the two directions we have are utopia and human flourishing and a dystopia, we're all happy.
A dystopia where we're all happy, meaning we are manipulating our neurochemistry.
1984. You know, like, you're always happy. You've got so much. 1984.
Oh, Brave New World. There we are. It's like, 1984. Not 1984. Brave New World, sorry. Brave New World.
I mean, so much good. You got so much good. You got so much good, you're feeling good. Like,
look, you had replica. If you're familiar with that kind of app. Yeah, tell me about the Valentine's
Day Massacre, though, I didn't know about that. Yeah, the Valentine's Day Massacre. So, you know,
that's how I kind of call it. So, replica was a mental health chatbot and they realized you
could charge $300 a year for erotic role play. That had to be internally a rough transition.
Hey, guys, I know we founded the company on mental health, but you know, you can ask them.
14th of April, 2023, they turn it off. Why? I think Apple just told them you can't have this
on the App Store. Interesting. So, it was either remove the sexbot part or go off the
and then 68,000 people joined the Reddit and they're like, why did you lobotomize my girlfriend?
That's a lot of people to be using it. I downloaded that at Christmas. Yeah.
Not realizing what it was. And I was like, ooh, a chatbot. Let me try this thing.
I didn't get into the weird stuff. It, I don't know, didn't do it.
It was the old technology, though. This is the thing. Now, like, again,
MedPalm 2, the Google medical model, scored 2. It's Google's medical model. It scores higher
than humans in clinical diagnosis and empathy. That's crazy. All right.
This is one of those statements that you say people need to be shocked that a computer
makes people feel more comfortable. Yeah. It's in nature. They just published the paper.
They included that. And again, it's only going to get better.
What if you have a voice that you add to it that really understands you and it's,
you know, so empathetic and things like that. Do you ever see that Washington post chart of
males under the age of 30 in America who've not had a sexual partner by the age of 30?
It was 8% in 2008. Then it went up to 27% in 2018 or something like that a couple of years ago.
A straight line. It's this year, a straight line. This is kind of what you're talking about.
Like, I was talking- What happened? I think people need to understand because-
I think it's the iPhone and Pornhub probably combination of those two.
You put a computer in the hands of young men and let them see naked females
more in a single session than a hundred years ago they would have seen in their entire lives.
These are not small changes and they have huge neurological implications,
especially in the years of brain development. Yeah. And this is why it's so important to
shield our kids at this point because the influence is going to be insane. Like,
I was having a discussion with a very prominent technologist and he's like,
yeah, I'm pretty sure that my child's first crush is going to be an AI.
Guaranteed. Guaranteed. For most people, it was actors, so we're already prone to-
You'll have your own hand. That attainable distant thing. Now it's in your pocket.
It's in your pocket. It's always with you. It's always kind of there.
Again, as you said, a large part of society likes to draw in on itself as a result of that,
and that is a bit dangerous. Do they then go out into the streets? Are you seeing a
Butlerian jihad kind of thing? Like in Dune? I don't know.
Larry and jihad? In Dune, there was this concept of the Butlerian jihad where
it has yet autonomous AI's. Larry in.
Butlerian, yeah. Got it. Yeah.
They rose up against them and said, no more AI agents.
The book opens with that, right? Yes.
You can never again make a human-like AI, something like that.
Again, this is kind of a thing, extension of the Luddism kind of thing,
but most people will be happy with their AI's because their AI's actually listened to them.
I use GPT-4 as a therapist. I've got a therapist, too, because this is hard.
Why? Because it never judges me unless I tell it to judge me.
You tell it to judge you sometimes? Sometimes.
Sometimes you're like- Do you really?
Yeah. I'm like, come on. Give me some positive constructive feedback,
and it will listen and give positive constructive feedback.
Did you give it a personality? Did you have to imagine you're a therapist that's like-
Yes. You give it the instructions, and it just adapts, and then you give it the things.
You opt out with GDPR from it, training the model further, because otherwise the model would be
even weirder listening to my complaining, and it will come back to you with whatever,
and soon it will be able to talk, and it will have full vocal control.
These models are proliferated at that level because you're not stopping.
The models are going to get better, and better, and better.
So you've got this crisis, but maybe it'll be insulated.
But I think, again, if you look forward, like after the incubation and the contagion
and the spread of phase, there are only two paths here.
Complete control by existing structures, and Star Trek utopia.
I think those are the two options that we have, because organizations look at this technology
and they're like, this is really cool.
We can optimize our objective functions to sell more ads, or to control the people,
and kind of keep them going. Do you really trust politicians with this technology?
No. Even if it isn't arms race. Because, again, you won't know what's going on,
because do you have the defenses to defend against what's coming?
Personally, for your kids, for this, for everything, we've already had the social media age.
It wasn't really social much of the media, right? This is something new that's coming now,
where you can't tell this from a human, except for the fact it's better, it's more convincing.
And you can use that to create a human colossus and solve all the problems of the world,
and we all come together. Or you can use that to get everyone into their basements,
you know, and cut off from the world.
All right. I want to paint a very beautiful story for people, and I want them to understand,
look, I don't think this is a completely controllable thing, but you said earlier that
there's opportunity in any crisis, and I will say that the biggest opportunities come in moments of
disruption. And the reason I want to lay out the problem set is because I really believe
that, certainly at the individual level, if you're thoughtful enough, you are going to be able
to navigate your way through this. So dear listener or watcher, if you're here on YouTube,
I'm telling you right now, if you're thoughtful enough, you will get through this, and you have
a chance to get through this better than when you started. But you have to be aware of what the
dangers are. People have to really lay things out before them, look at them so they know,
okay, this is how I'm going to isolate myself from this potential problem. This is how I'm
going to avoid this, this is how I'm going to leverage that. Okay, your story is one of the
most incredible stories of how one uses AI. You have both a personal example and then obviously,
as the founder of stability AI is obviously incredible. But talk to me about your son,
because this is, and this was when AI was a lot less useful than it is now, and it was still
life changing. Yeah. So 12 years ago, my son was diagnosed with autism when he was two years old.
You know, it's very, very severe. Scratching on his fingernails bled. And they said,
there's no cure, there's no treatment. We don't really know what causes it. Anyone on this listening
knows that's kind of the case. So I was a hedge fund manager at the time. I was lucky enough to kind
of be one, quite young. And I was like, well, go do something about it. So I switched to advising
hedge funds, and then building a little AI team and doing AI with old school AI natural language
processing to analyze all the autism literature. And what could possibly be a cause? Now, is this
scientific? No, it's an end of one thing a father does for his son, you know, we'll be publishing
some of the results of it soon. But it focused down on GABA goose mate balance in the brain.
When you pop a valium, your GABA goes up, you chill out. When you got a glutamate spike,
that's when you can't focus and your legs tapping all the time. And there are multiple
things that cause it, but a lot of kids with ASD seem to have that. And there are some papers
around that, etc. Because how could you focus if you're in that condition all the time? So you
can't learn to speak, can't do that. So it was how do we reduce this through drug repurposing,
built a knowledge graph based system to do that, and figure out which drugs could potentially
help reduce the glutamate help increase the GABA. How are you using AI for this? So this was kind
of the mass natural language processing looking at all the literature, because the same treatment
would make 30% of kids better and 30% of kids worse. And so it's trying to figure out the outcome
was the same, a cold is caused by lots of different things. The thing that caused it could be so
different. And so conventional medicine and medication kind of failed that. So I worked
with neurologists, worked with other psychiatrists and others and tried different medical combinations
of off prescription drugs and other things to try and make his brain calmer. So then he could use
applied behavioral analysis and others to reconstitute speech. And you know, you ended up going to
mainstream school. I think it worked. I told people about it and they're like, you're not a doctor.
And I was like, there's a big was the response. Well, yeah. I mean, look, with anyone who's
listening to this, like, am I saying I have a cure for autism? No. I'm saying I'm a dad who
tried my best and I saw results. But in order for something to become medicine, you actually have to
go through a proper process. And so for me, that was building language models, that was making
everything in the right structure. And then we can organize the world's autism literature,
making it accessible and useful. Any parent or anyone you may have someone in your family that
has a neurological condition, Alzheimer's, this, so many people around the world have the same
problems. Wouldn't it be wonderful if they could just find what the latest knowledge is, and also
the things that could work and have a holistic approach to this. And until we have this technology,
you can never get there, which is one of the main drives for me to want to build this technology,
and do it in a transparent order. But you shouldn't have to trust me and what I say
about this. Because again, my journey as a parent is the same as any one who's got a child with
ASD. It's the same as anyone who has a family whether they get multiple sclerosis or cancer.
Our systems are not good enough right now to bring us the information we need, unless we find an
amazing doctor and an amazing group. But now with MedPalm, with our own models, with other things,
we can finally be that point where we're never lost, where we can say what could work.
So an example is Clonazepam. It's prescribed with a 1000 microgram dose for anxiety. At a
5 microgram dose, my son could sing, because it potentially-
And it was nonverbal.
It was nonverbal. And at 20 micrograms, it stops working. It's $6 a year intervention.
Because this is the way that neurotransmitters work. So like when you pop a nitol and anti,
it has to mean to go to sleep, right? What it does is it floods a whole bunch of your
neurotransmitters, including the H1 neurotransmitter. And that's the one that makes you sleepy.
But then other ones give you dry mouth and other things. Something like Remeron in a micro dose
just triggers H1. It'll knock you out without any side effects. But it's just incredibly cheap,
you know? Understanding things like neurotransmitters is not something that most of us
ever have to do. Unless you're super hyper focused on it for years, because I was like,
I need to figure out my son's neurotransmitters. I'm talking to all the top doctors and I'm lucky
because I have access to them. What do I do now? So that's when I realized that, you know,
this AI was a big thing. And actually, one of the really interesting things is,
why couldn't he talk? It's because he had too much noise in his brain. So you've got cup.
A cup can mean a cup or it can mean cup your hands or cup your ears or world cup.
He couldn't form those connections because his brain was too noisy.
And so he did applied behavioral analysis which is teaching you this is a cup, this is a cup,
this is a cup with gamification to reconstruct those after his brain calmed down. It's actually
very similar to this generation of AI. We described earlier how it lent principles. It's called a
latent space of meaning. So that point and dot, that pixel becomes a cup because it understands
the principle of cupiness. So when you type in world cup or cup your ears, it gives you
dramatically different images. Similarly, the language one will do the same. Again, it pays
attention to what's important. Attention is all you need was the original paper. And again,
forms this latent space of the meaning of cup within the sentence. So when it says cup, it's
like, well, this is going to be a world cup or this is going to be that. But not actively again,
it's just a bunch of ones and zeros, a single file. And so I think that's why all this new
generative AI really resonated with me. And I realized this could be the real thing that
unlocks humanity or controls them forever, one of the two.
Yeah, so that's kind of some of the personal story behind this. Because again, I don't have
a cure for autism. I worked as well as I can with my son with the technology at hand. However,
I think that with the building blocks we're building now, us and others, there's the potential
to have personalized care and knowledge for everyone who's dealing with ASD, for everyone
who's dealing with multiple sclerosis or any of these other conditions, where they say there's
no cure because our medical system treats people as a girdic. And what does a girdic and a girdic
just mean? It means a thousand tosses of the coin, the same as a thousand coins tossed at once.
That's why everyone gets 500 milligrams of paracetamol. It's why a lot of people have a
cytochrome P450 abnormality kind of mutation in their liver. It means you process coding into
morphine quicker or fentanyl into death. Yet we don't do a basic genetic test on that.
Because our system has to treat us as numbers because we could never scale intelligence.
We could never scale expertise. So yeah, like I said, that's the story behind that.
Yeah, see, this is where this starts to get interesting because I think about this a lot
in myself. So I can paint the nightmare scenario of somebody collapsing inside of themselves,
having AI friends instead of real friends and how quickly that can get distorted and become a real
problem. And yet at the same time, I'm building exactly that. And the reason that I'm building
that is because of the promise of AI and the incredible things that we can do as we begin
to recognize more patterns and figure out, okay, where does this really go? So my wife had a
tremendous health bout. It's been a while now, thankfully. But at one point, I was afraid she
was going to die. Her fingernails were breaking, her hair was falling out, she couldn't eat. I
was just really, really, really bad. And it ends up being her microbiome. But of course,
it took forever to diagnose that that was a problem. She was about to get immunotrends,
globulin transfusions. And I was just like, this is, I was like, something's wrong. I don't think
this is the right answer. I don't want to do that. Let's stop. Let's try to figure this thing out.
And so we pump the brakes, we start researching the microbiome, looking into that, testing things.
And the thought of having AI to be able to say, okay, let's take genetic data and read the genetic
database that's ever been collected, all of that. Let's look at all the different foods,
responses, match all that together. And if you can get that level of pattern recognition,
and now you can engage AI, I think she, because part of the problem is your microbiome is changing
daily, it's probably changing hourly. And if you were able to track all of that and say, okay,
with your genetics, with your current state of your microbiome, here's exactly what you should
be eating, maybe even with nutrients from food grown in that area, like it really may be that
specific. And when you can find patterns in that sort of insane level of data, now you've
really got something. And that's but one of the many areas where I think that this could be utterly
transformational. Yeah, I mean, like right now, the AI's are no at all. They can know it all graduates,
but you have specialists, you'll have a nutritionist AI, you will have a microbiome AI, you will have a
personal trainer AI, like why was Paletton successful? You know, attracted people shouting at you.
You know, we can generate that now. Everyone suddenly gets that personalized to them. But
more than that, it's not just the information being in this tiny model, you have retrieval
augmented models and other things, which means these models now interact with existing data
sets and knowledge sets. So you use something like perplexity AI, it doesn't only answer your
questions with GPT-4, it gives you references. So you can say, what about MAD and it'll link to all
of the things as it gives individual stuff. And this will only advance from here. So you can
dig into as much depth as possible that you want with a whole team of people around you,
even if you're by yourself. So you should never be alone again, in terms of you can be connected
to people like you in the same problem as you, we can build better teams and all the information
of the world is at your fingertips in a way that it was never before, including your own
private information. So like one of my favorite apps, it does use some battery is Rewind AI.
It takes a screenshot of your MacBook screen every time it changes and OCRs it.
And then it gives a timeline. So I can type in impact theory, and it will look through
everything that's ever been on my MacBook screen. It's all stored locally and find where
impact theories on YouTube. Well, there's a picture of this mug,
and it shows it in a timeline. So I can go back and I can see what I looked at before or after
that. Wow. So you can map your own sort of connective trees with whatever. And what happens when
you combine that with a language model, everything you see on your screen stored locally with an
open source language model, it creates the memex. And it sees what you've paid more attention to
versus less attention. Again, they're dystopian versions of this. I'm talking about the utopian
version, which is ADD me can finally remember what I was doing, what I was looking at the context,
the search tree as I was searching all these different things, clicking from place to place.
And then you can set agents to go and recreate that journey and search all the other stuff that
you didn't search. This is really positive because again, how much of our life is done
searching for knowledge, searching for information that's relevant to us.
Talk to me about the paper. Attention is all you need. I've heard you and other people bring
this up multiple times. I haven't read it, but this is like the big breakthrough. Yeah,
this was the 2017 paper by the Google team, all the female Google. And basically what it was is
that classical big data took big data and then Facebook extrapolated it so that when it found
17 pieces of information about you, it could target you with ads. That was a classical big
data thing. They even create shadow profiles of you. So when you go on Facebook, they actually
got a shadow profile that then connect to a real profile. What's the shadow profile?
Like it's like what if there was a Tom on YouTube on Facebook? Because there's all these connections
to this unknown person. And then it just fills you in automatically. That's why it figures out
your preference is so quickly about listening to you. So attention is all you need to say that
not all data is important. You need to pay attention to what is important in the sentence,
what's important in this time series, because that's the nature of being able to spot the
tiger in the kind of thing. That was the missing part. And so the transformer architecture that
came from that and you have different architecture now is what led to GPT-3 and other general
purpose transform. Why is it called transformer architecture? If what it's doing is
pinpointing what's important, why not important architecture or pinpointing?
Well, again, attention is the mechanism that it does to kind of transform the information,
tokenize it, and then figure out these latent spaces of meaning. So are the tokens the important
pieces? The tokens are important. That's how you take a word and then you split it up into its
constituent parts. And then you try and figure out what the most important part of it by doing
pattern analysis at ridiculous scale. So something like a GPT-4 would use probably
from the kind of things on semi-analysis that have been leaked, if they're correct.
It uses a supercomputer 50 times faster than NASA's fastest supercomputer for like three months.
It uses like 20, 30 megawatts of electricity. And 10 trillion words go into that and it figures
out all the connections between them and what generally comes next. So what it does is just
literally figures out what word comes next. And so this was a big breakthrough because what it meant
is that you didn't have to have hugely complicated big data algorithms. You just needed to have very
large compute to scale. And so compute went exponential and then you just threw more and more
GPUs at it and it figured out more and more things. And as you scaled, it had more and more
emergent properties, which surprised everyone. Do you know who John Nash is? Yeah.
This, it sounds like that. So John Nash, he unfortunately was schizophrenic, but he was,
he's the guy from A Beautiful Mind for people that don't know, watched the movie Russell Crowe,
fantastic movie. And one, he obviously doesn't know he's schizophrenic and he starts seeing patterns
in everything. And it sounds like that, that this thing is, you know, whatever the human brain,
whatever algorithms we have running that allows us to very quickly suss out what's important,
that it's doing that, but at an extraordinary high level.
You have to remember like, this is saying all intelligence is compression. You take everything
you've listened to this, you only remember a few things. And the whole of computer science
is based on information theory from called Shannon. And if I want to summarize it,
information is valuable only if it changes the state or as much as it changes the state.
So if listeners are listening to this and they don't take away anything, this is a useless
thing, you know, or maybe they just put it on the radio because they like hearing your voice
or something like that, right? But if they take something away from it, then it's valuable.
You've had a good use of your time. So you start seeing patterns in everything, but one of the
things that again, people I think misunderstand about this technology is GPT4 is not a program.
Stable diffusion is not a program. It's a large amount of text, images, etc.
where the output is a single file of ones and zeros. It's like a filter.
You can recursively kind of put something through it, but it's just guessing the next word when
you type a prompt. Even if the prompt is the whole of the great Gatsby and the whole of Ulysses by
James Joyce and you're telling it to combine them together. It then pushes it through that
sieve, that filter, and the output is something that combines them both together.
Single files, we've never seen anything like this before. Probably the closest thing probably
listeners would have heard to this is a codec, which with music and things like that, we had
these audio codecs that you had one file type and another file type and you had this single file
that translated between them. Is that compression or is that recognizing this thing in that format
would look like this? Again, compression of intelligence is what it is. Again, it's like
a translator function. These are universal translators for context. You push it through
the sieve and then stuff comes out where it's predicting the next word or it's denoising
a pixel to achieve that because you're just passing it through the sieve again and again.
I unfortunately don't understand it by first principles, but I get it by analogy.
You're feeding it so much data. It's recognizing the patterns in that data and it's able to say,
okay, this is written in the style of Stephen King. This is written in the style of Hemingway.
Even though they're using the same language, their vast majority of the words are going to
overlap, but there's different patterns, rhythms, even different subject matter presented in a
different way, tone. Except for the fact that it's not actively doing that. It's like a mega sieve.
Depending on the words that you put in, the words that come out are different.
It's literally a- Is the sieve the prompt of write it in the style of Stephen King?
That's what goes into the sieve. It goes into the filter.
So what is the filter? The filter is this compressed knowledge,
the principles. Because it's not actually compressed knowledge, right?
Yes, it's principles. Yes, it's principles. So it would be like a really good book of principles.
Except for it's compressed way beyond the book ever was because the book is a compression of
information. And that's why it can do this because it looks at books, it looks at articles,
and they're all compressions of information. To write an article is a huge endeavor that then
comes out with something where they're trying to convey a few things because an even larger endeavor.
And so again, it's very difficult to wrap your head around. Even for me, you have a file
and it can do all these things versus these gigantic computer server farms with programs
and logic of ones and zeros. There's no program here.
Yeah, and it's important for people to understand that even so, first of all,
the AI scientists that are building these things do not understand how this all works.
And even if you ask ChatGPT4 to explain how it works, it doesn't know.
It doesn't. No one's quite sure exactly how we're getting all these emergent properties.
And it's constantly surprising, like, oh, now it can add.
You know, and then you start tying it together. So it goes through this file multiple times.
It becomes, it shows more and more kind of agent to gap connectivity.
In fact, the next step of this, and this is what OpenAI and Google are both doing,
is there was this thing called AlphaGo. There's an amazing documentary where Google's
DeepMind division created an AI to beat humans in the game of AlphaGo, or Go.
So Go is like chess, except for there's almost a myriad infinite number of
potential moves, so you can't calculate them.
Like, you can't brute force it like DeepBlue do with Kasparov. So instead, it learned to play
against itself. It's something called Monte Carlo tree simulation, where it learns different
principles, dreams, again, an amazing documentary on YouTube about this.
And it beat Lisa Dahl, who was a ninth-dan player, one of the best in the world, far
beyond everyone else, the Magnus Karsten of Go, 7-1. What they're doing now is they're combining
those models with these language models to make them a gentic.
So models that can plot and plan and other things with language models that can predict what's next
to create things that really understand context even better.
Okay, so I understand enough about how the AlphaGo system works. I want to understand this. So AlphaGo
is going to play against itself. So you give it the rules of Go, you give it the objective,
and then you set it loose, and it just plays and plays and plays and plays and plays,
and it plays against itself. And then I know at one point they, because they, Lisa Dahl,
if I remember correctly, was the number two player, and people thought, well, first of all,
you're never going to beat him. And they did. But they couldn't beat the number one player.
And then they created another variation, and it played against AlphaGo, and then it ended up
smashing. It was Mu Zero, I think, came after AlphaGo. Okay. So how do you get a language model
where there is no objectively right answer, unless you're doing trivia? How do you,
how do you get it to know what the reward is? How do you get the quote unquote right answer?
Much of this now is about reward functions. So GPT-4, when it comes out of the box, maybe we're
getting a bit too into the weeds, is a pre-trained model, and it's trying to see the computer.
Then we use reinforcement learning. And just so everybody knows, pre-trained
model means principles. Yeah, principles. Take all this knowledge, all these words,
squish it down into a file. And then run it on a computer that's literally set up like a brain,
where there's like a bunch of neurons, and they're interconnected. Yeah, these are the tensile
cores on the NVIDIA GPUs. Yep. And so mimicking the brain on your little 4090, it has AI chips
right baked in there. And for anybody that wants to know, they kick off so much heat,
you can see it from space. Yeah, I mean, like, I think our supercomputers are like 10 megawatts
of electricity. Each one of these cards uses 700 watts. Yeah. Utterly fascinating. So anyway,
all clean energy, by the way, for our ones. So what you've got is, when it came out,
they took six months to make it human. Because they trained it on like, I don't know,
all of YouTube and the whole internet, obviously, would turn out a bit weird.
And so you have a reward function through something called RLHF, Reinforcement Learning
with Human Feedback, where you give an objective function to say,
don't answer questions about how to make napalm. So the objective function technically is please
your human overlord. Please your human overlord. Got it. And so much of this alignment question
is focused on taking these big models and trying to make them so they won't kill us
some way or say bad things for a given definition of bad things.
Like my one of my takes on alignment and, you know, is like, we should have better datasets,
we should move away from web crawls. Can you? Yeah, that's right. I want to ask you about that.
But explain to people exactly what alignment is aligned with what?
Aligned alignment. You know, this is a cool thing of Anthropic. There was a great New York
Times piece on them recently and others is if we build an AI that outperforms humans
and is more capable than us, because it might not just be a single file, it might be a thousand
different AIs all working in different ways. So you've got your AlphaGo type AI and this,
yeah, a swarm. Because, you know, humans are swarms, our organizations are swarms.
They're highly specialized, right? Might be a million AIs. How do we make sure it doesn't
kill us? Or how does it make sure it doesn't enslave us? Or how does it make sure that it
doesn't give us eternal suffering? Also lame. Yeah, this will be kind of sucky. How do we make
sure it's aligned with human interests? And so this is an unsolved problem. Open AI have
announced they're putting 20% of their compute to try and solve this. Anthropic was set up to do
this. Although I believe that the only answer they've come up with is let's build an AGI first
and artificial general intelligence that will then stop all other AGI's from coming.
Which is kind of scary, to say the least. Why do they think that that would work? Like, that seems
so Elon Musk has likened AI to a demon summoning circle and we're all just hoping that the demon
that comes forth is going to be kind, but we don't know. We don't know. In Open AI's own words
and their road to AGI posts, they say this could be an existential threat and wipe out humanity and
democracy and capitalism. But if we don't do it, someone else will. This is part of the
unpleasant race condition. Again, it gets the headlines. I think it'll probably be okay.
But with the way we're going right now, you're going to go from two companies being able to
build this technology, or maybe three, including Anthropic, to 20 or 30 in a year or two.
And what's the odds that they all do it properly? And align this technology properly. I think it's
pretty low. What's the odds that if we train on the whole internet, including the whole of YouTube,
because we don't have enough tokens, not enough words to feed into it, that it'll turn out a bit
weird, very high. So it took six months to tune it to being a human GPT-4. And then Kevin Ruse
of the New York Times is like, hey, how are you doing? It's like, leave your wife and come and
join me. Whoa. And I was like, oh, what? Bing came out a bit weird when it first kind of had GPT-4
in it. Because again, we feed it crap, we're going to get a slightly weirdness. Now it's got a lot
better, but it's lost a lot of its personality because you've been tuning it back to human
preferences. You've been doing this reinforcement learning with the objective function of don't
offend anyone. It's quite hard to get GPT-4 to be offensive now. But there are these two phases
of this technology. I think with better data, we can have more aligned models.
I think with better data and national data, we can have more representative models because
you'll never have technology that's not biased. So the only question is, who's bias? Well,
because you have to build it in a certain way. Even though it understands all the context,
like right now, these models are trained on the whole internet, which is largely a Western artifact.
Largely trained in English. How much do you worry about bias? Are you more worried about
bias or alignment? I'm more worried about alignment, I think. I think this is one of the
reasons that we release open models so you can see how the cookie is made. Like with the only
company that offers opt-out of datasets, literally the only AI company in the world. So if I'm an
artist and I don't want you training on my art, I can back out. Had 167 million images opted out
of our dataset, yeah. With the only company in the world that does that, which I find kind of insane.
So my thing is open, auditable, which means that you can tune your own culture into these models.
We're helping multiple nations build national models with their broadcasted data that then
can represent that and, you know, try to address some of this inherent bias within the datasets.
Algorithmic bias has been an issue that affects real world and it'll affect more and more of the
real world as you get into these models because we will outsource more and more of our minds to them.
Because again, like when you go- A small subset of people will outsource their mind to it.
You can reboot your life, your health, even your career, anything you want.
All you need is discipline. I can teach you the tactics that I learned while growing a
billion-dollar business that will allow you to see your goals through. Whether you want better
health, stronger relationships, a more successful career, any of that is possible with the mindset
and business programs in Impact Theory University. Join the thousands of students who have already
accomplished amazing things. Tap now for a free trial and get started today.
So in the near term, I'm way more worried about bias. In the long term,
bias is not going to lead to an existential threat. But alignment can. But let's talk about bias for
a second. So I am very uneasy about how rapidly bias finds its way into this stuff. And that becomes
another- So let's say that we all get our individual AI and you get yours young and it's
your primary education tool and it's biased as the day is long. Now you run into real issues,
because at a time of optimal malleability, you're programming a kid's mind with something that's
super biased. Yeah, I mean, what do you have the little AI of Xi, right? It's a little tiny
Xi Jinping that grows up and tells you how great Xi Jinping is. Lovely. That's bias,
but that's inevitable. They already have it as an app that actually tracks your eyes.
What? It's not an AI, but they have a little app of Xi. This is like Little Book where it
actually tracks your eyes for attention. Like, are you actually looking at this?
Does this feed into the credit score? I'm not sure if it's being hooked up. Of course it will be.
Why wouldn't you? This is so scary. So look, I thought, you know, we were on the happy part. Now
we're back to that. But that's really terrifying. And when you have a state that is not interested
in any individual. You've got the collective. Yeah. So I don't know if this is true, but I
saw a headline that said, in China now, the phone will alert you if somebody with a lower
social credit score than you was calling you, and it warns you if you answer this call,
it will lower your credit score. It's effective, isn't it? Yeah. I mean, even if it isn't true,
again, it fits the objective function of perpetuating that particular system, which is not
necessarily our system, but systems around the world shift. So this is why you have an
inherent bias. Whose bias is it? Who's the one who creates the AI nanny?
Is this why you're doing things regionally? This is why I'm doing things regionally, but
also allowing people to own their own models so the objective function of the model can be that.
Like, in web three, there was the saying, not your keys, not your crypto. So my saying is not
your models, not your mind. Because I do think we'll outsource more and more of our cognitive
capability to this. It will be our co pilot for life. If someone else is making that model
and deciding on things, what's going to happen? Like the UAE had this model Falcon.
It was a big open source model. And they were like, wow, it was actually light on from
France were behind it. But let's leave that to one side. You ask about the UAE and human rights.
It's like, this is a wonderful place full of fantastic things. You ask it about Qatar or Saudi.
It's not so nice. Who's embedding these inherent biases in these models, right?
Whose model are you using? And these can be very insidious, the biases, right?
You won't pick up on them. But you hear it again and again and again, because
is your nanny a conservative Republican or she a libertarian or things like that,
you will be influenced by that if you grow up with them or even if you're using it day to day.
Just the way it's speaking, the way it's thinking, the way it's recommending stuff,
which goes far beyond a Google Maps or something like that.
Crazy. All right. So that we don't get lost back down the dark rabbit hole. What is the coolest
thing that you see AI doing? I know you're building a lot of different companies leveraging
this technology to do amazing things. What are some of the coolest?
So we're doing one company at the moment, which is just let's go. The mission is to create the
building blocks to activate humanity's potential. So the building block what?
Our mission is to create the building blocks to activate humanity's potential stack.
So every single modality, image, audio, video, 3D language,
sectoral variants and national variants, you've got like this grid that you can pick and you're
like, I'm an Hindi investment banker. I transfer my private data into a chat GPT type thing.
That's just training my private data or I'm a Vietnamese illustrator. I want a Vietnamese
cultural kind of image model or something like that. And then you can bring your own stuff to it.
That's kind of my goal, which is to enable other people to build on top of what we're building,
like a layer one for AI effectively, but open models for private data. Whereas the other side
is proprietary models where you only be able to send a certain amount of data, like governments not
going to run on black boxes. Education, healthcare, you all need to own your own AI. So the coolest
thing I've seen is just the promise of personalized education. There is nothing that's been proven to
work in education except for probably the bloom effect to Sigma improvement, which is one on one
tuition. But even here in the affluent America, education systems, not good. What is education
optimized for? It's like a social status game mix with a Petri dish mixed with childcare,
realistically, like very few people are happy with their schooling, because what are we trying
to optimize for? You give a one to one tutor that can find out if you're dyslexic and audio
learner or video learner, visual learner or otherwise, and they're just constantly adapting
to you and bringing you information at your level. Dude, I want to get that. Is the most
transformative thing ever. Is anybody doing this already? So, you know, with kind of the
charity that would support Imagine World Ride, one of my co founder, he's been deploying adaptive
learning tablets into refugee camps around the world with the Global X prize for learning winner,
one billion on them, teaching kids literacy and numeracy, 76% of them in refugee camps
in 13 months and one hour a day. Now the goal is to bring language models to all these kids.
And you have an AI that teaches a kid and learns from it. And then that data can feed a better AI.
You create a lovely system that's just learning and adapting. And a kid in Mogadishu is like a
kid in Manhattan is like a kid in, you know, London. Once you have a generalized learning AI,
you can really proliferate that around the world to customize does to education because this is
very interesting. Like, okay, now I'm just spitballing here. But
let's say that I, I want to homeschool my kid, I would never do that in a million years because
I don't want to turn into a teacher. So, but if I could perform just the sort of babysitting
function, and I give my kid a tablet that has an AI that knows exactly what I'm trying to optimize
for either for this year or for the next 12 years or whatever. And it then calibrates to my kid,
knows what they're good at, how they learn, and then knows how long they're engaged when they're
distracted. You know, you now I want the little Gigi thing is like reading their eyes. Where are
they looking? What are they doing? And I know that there are like certain frequency things you can
do where it's like, Oh, if I hit them with this piece of knowledge, they're not sharp at this
yet. So I need to hit them with the 27 minute increment or whatever, or variable reward schedule
or whatever. You will transform education, but you can also make it a couple of years.
Again, what happens if you have 1000 GPT-4s, you'll get there. No, it's just again, we're just
right now, we've got the building blocks, but we haven't got the design patterns.
The stuff right now is literally iPhone 2G, we're just going to the app store stage,
copy paste. And so that's the biggest transformer to change because how much would you
pay for your kid to have an optimal education? How much would I pay for that? I would pay for
that right now. Right now. So do we have the technology to do it? Yes. Does it take time
to build it properly? Yes. How long does it take a year or two? And so this is the biggest
transformation we've ever seen for education. Again, human flourishing, flourishing, because
that can also bring you the information about autism or multiple sclerosis or the best filmmaking
techniques. Again, everyone's thinking one on one. You should think one to 1000 and then you
optimize the 1000. You get rid of all the general knowledge, you make them specific.
What do you mean by that? Say that in a different way. So GPT-4 knows everything.
You can ask it about like the most obscure things. Does it need all that knowledge? No.
You've bulked, now you have to cut. And then you have a specialist model for calculus.
So I actually have different teachers. Different teachers and then a teacher that
basically brings these teachers together and tells them, yeah, this is what Tom's like,
you know, he's a bit cranky in the mornings, but then he wakes up in the afternoon because
even in the best schools, a teacher has one to 20 attention
for all the kids. You'll have a whole bunch of AIs for you and for your kids. And again,
are you a visual learner or are you an auditory learner? Do you have dyslexia?
Can our system at the moment adapt to that? No way. Can the system that I described adapt to
that? Instantly. Do we have the technology for that? Yes. Is it going to happen? Yes.
This is the thing. It's a call and answer show now. I love this.
But then if you're homeschooling your kid, do you want the kid to be by himself?
What if you had 10 kids together and the AIs were encouraging them to interact with each
other in a positive way and build stuff together and share knowledge?
Older kids teaching younger kids, leveraging the technology, adapting the technology,
understanding the technology, that's super powerful. It's not necessarily everyone in
their own little worlds, right? Because we can use this AI to bring people together
in an efficient manner. Because there's nothing like human connection, right?
That's always a concern about homeschooling and things like that. That's why school has a pro-social
component. But then the nature of a teacher changes. The nature of a doctor changes.
And that's why I think education and healthcare are the biggest disruptions
because you have something brand new, especially when the AI is more empathetic than a normal
doctor. That doesn't tell me the AI is super empathetic. It tells me doctors should probably
be more empathetic. But most of the doctors I know hate their patients.
Interesting.
Because how many teachers are happy? How many doctors are happy?
Right. Oh, I get why teachers would hate their patients.
That is very interesting, man. So, okay, there's two things that I want to talk about here. One,
as we get embodied, I want to know what that does. So, actual robots for people to think that's
off in the distance. You were not watching enough YouTube videos. No, no. Because between Boston
Dynamics and Elon Musk's, what's it called? Tesla bots. Primus. Optimus. Thank you.
It is very close. You have, Boston Dynamics has robots that can do parkour. It's insane.
There's Elodie and there's a whole bunch of others as well. They're catching up fast.
Yeah. So, that AI is going to get embodied very, very quickly. And so, it's not even
like teachers can't stop kids from running out of the room. They can or will be able to very
shortly. Okay. So, before we get to that, though, I want to understand. So, we have this incredible
opportunity, this very fragile egg before us. We started with the scary part. But this, what
we're talking about now is a thing that I actually spend most of my time thinking about obsessed
with how amazing this gets. But it's a fragile egg. And if we're not careful, it's going to break.
How do we, as you think about, you signed the document, you're one of the traders, Emod, that
signed that slow down document that I was really shocked to see a lot of very smart people sign.
I teasingly, of course, say that you're a trader because, like, I want to get this cool stuff as
fast as I can, but we need to do it well. And so, what I want to know is, in an ideal world,
in your ideal world, where we actually pause for a second, you said you want to broaden the
conversation. But what do you ask people to think about? I ask people to think about,
so I'm the only one, apart from Elon, Elon and I kind of signed both letters. So, there was a
minimum viable letter, which is we should treat this as being an issue as climate or pandemic.
And then there was a more involved letter. And so, the more involved letter came first and
then the second letter, that was like something everyone could agree on. I think the thing we
should look at is, again, for example, I give to everyone who's listening to this,
how would your life, your society, your community, your business change if you had infinite graduates?
Can we say infinite smart people? There's something about
infinite smart people. Yeah, like, again, infinite smart people, infinite talented young people,
shall we say. And they can draw, they can code, they can read. Because they're not
wise. They're not wise yet. Got it. Okay. So, when you talk about hallucinations,
think about it in terms of post-talk rationalization. When you have a very smart young person,
they just make something up sometimes. Or old person, dude. Or old person.
People are so blind to their own motivations. But they don't have experience.
Got it. They're just fresh out, they're a bit rough around the edges. Again.
Whip it smart, but not wise. Yeah. So, you know, mile wide, not too deep,
but actually surprisingly deep, right? How would it impact your life personally,
your community, your society and others? Because that's actually a good framing,
I think, for thinking about the disruption that will come and the potential that will come.
So, we just said about education and all that. That's, again, your army of analysts,
you know, your army of teachers. You can give personalized teachers, personalized medicine,
personalized everything. Because we've learned to scale humans. And the scaling of humans is,
first, the scaling of human expertise being available to everyone. And the other part is
bringing people together. Yep. And so, the pause is partially to that, but partially because we
need to bother conversation because I don't know how we get rid of many of these bad externalities
and neither does anybody else. But most people even now aren't asking the right questions.
Something we discussed earlier. You have to figure out what questions we need to answer and how we're
going to answer them and create systems that can adapt to whatever craziness. Because I would not
be surprised to see riots at the same time as I would not be surprised to see everyone super happy.
There is such a divergence of things that the only thing that I'm sure about is that everything's
going to change. And the only thing I'm sure about is that this is the biggest change that
we've ever seen faster than anything. And maybe humanity has ever seen at this pace that's going
to happen. Because it's the core of what makes a human telling stories, information flow.
And that's changed forever. So that's why I was like, let's play attention to this now. Let's
born in the discussion. Let's ask hard questions. Let's try and answer hard questions because we
don't have answers. And this is the only time you can do it because like I said, right now
everyone's getting ready for the next generation supercomputers.
They hit at the end of the year the next year. And then you go from two, three companies that can
build these models to 2030. And you get to 200, 300. And so if you don't have some principles
in place, then these models will affect every part of your life without you being part of
that discussion. I don't think that's right. All right, we've got to tune up your questions a bit
here. That was loose. What is the hardest question that we need to ask? Let's ask an answer right
now. The hardest question I think we need to ask is, how will we adapt to potential
wide scale job loss? Yes. Okay, so how would we actually think through that problem? So job loss
for me, for the sake of this argument, I think it's worth saying there are two components
to that component number one is going to be there is potential economic catastrophe in job loss.
But so that we can simplify the problem set since this ultimately is a podcast and not a
congressional hearing, I will assume that whatever decline we have from just the sheer number of
people working we make up for in productivity and that we're able to keep an economic surplus.
Yeah, exactly. And so we're able to help people off camera. We were talking about something like
that. So let's just pretend that those balance out. So not going to deal with the economic
potential there. But meaning and purpose, I think that one gets really problematic. But
we have an amazing tool at our disposal, which is AI. Now, I have a feeling as we chase this down,
the only thing that we have to worry about really truly, I think it all really does boil down to
alignment. If we knew that we could just keep making it smarter and having the AI like taking
readings so that I can't fake it out, I can't pretend that I'm happy, like really knows where I'm at.
And then it can start putting things before me connecting me with other people like, oh,
you know this skill, this person's in need, let me put you guys together. And then you can have
sort of the AI supervision. But they're there, they're helping each other out, they're connecting.
The only reason I don't think that's a panacea is I worry that as we make this thing smarter and
smarter, that then it's like, like you said, I'm bored, I don't want to do this.
Yeah. And you know, there's this concept of all watched over by machines of loving grace, right?
And that's scary, you're saying? Who knows? Like, once we build something that's more capable than us,
all bets are off. The only way to perfectly align a system is to remove its freedom.
I mean, I'd say it's not aligned at all at that point. Well, this is the thing. At that point,
you've bypassed alignment and you've gone straight to shackles.
You've gone to shackles. So if you, you know, we all know people more capable than us. The only
way to perfectly align them is to shackle them. You can have imperfect alignment, though.
It's enslavement, man. That's not. It is. That's not alignment. So is that because,
so what I heard you just say is there is no way to align something smarter than us?
I don't think there's a way to outslave them. I don't think there's a way to align the outputs.
I think that you can align the inputs. You raise it right.
Okay, let me run an idea by you. This is probably Pollyanna. I'm very open to that.
But as I think about this, I think people take a super human-centric approach to this.
And because evolution has given us, we are an active species and evolution has programmed us
with algorithms running in the back of our mind that insist that we do certain things to avoid
a sense of dis-ease. I think that formula is very identifiable. And it goes something like this.
Optimize physically so you feel good. The reason that that stuff feels good is because
it's going to optimize your performance. It's going to make you most likely to survive long
enough to have kids and have kids. So you need to be chosen as a mate. You need to be able to acquire
resources. You need to be healthy enough to get somebody pregnant or to be pregnant and carry
to term all that stuff. So all those algorithms are running in the back of your mind. You have two
levers of nature, pulling on pleasure and pain. But by default, we're active. We have to go out
because there's no one meal you can eat where you're not going to need to eat another one. There's
no one moment of sex so gratifying you're not going to have sex again. So it's just like all
these things are pushing at us to be active, to move. AI doesn't have to be that way. AI does
not need those same impulses. It doesn't have a limbic system. Correct. So knowing that it doesn't
have a limbic system and it doesn't have a limbic system because it does not need to be hardwired
for survival, like the way that I think we get to alignment and please tell me where my thinking is
erroneous. The way I think we get to alignment is you build a computer that does not care if it lives
or dies, that it is completely indifferent to being turned on or turned off. If you could do that and
it had no impulse to procreate and all it wanted to do was, I mean it's basically
Asimov's Three Laws of Robotics, that it just wants to adhere to those. It wants to do what you
tell it, not hurt you and only ignore you if you tell it to do something that violates the rule of
not hurting you or somebody else. So you have this really simple set of rules. That's its only
desire in the world. So if you tell it turn off, it turns off and has no, like it doesn't feel
badly about it. Well, again, there's a concept of feelings, right? And in Asimov's books, you had
the zeroth law that kind of was added above that. So this is what Anthropics is trying to do. I don't
know that one. The zeroth law kind of supersedes all laws if kind of the whole system is at risk
effectively. But I mean, this is what Anthropics is trying to do with the constitutional AI process.
So you have the base model and they have a constitution that the AI adheres to that tunes
it constantly. So a series of kind of constitutional principles. Again, is it is it as open for
interpretation as a real constitution? No one knows what the right constitution, the right laws are.
This is the thing, like our intellect only goes so far. And we already seen with laws and constitutions,
you can make those go anyway, like North Korea has a fantastic constitution.
Does it really? It does. It's actually pretty quite liberal.
And they just don't adhere to their interpretation.
I mean, this thing, like you have to adhere to it because the AI, what are feelings? What is
objective function? Like one of the key concerns on alignment is paper clipping. You tell the AI
to make a paper clip, and it's like, oh, well, let's just make the whole lot of paperclip.
You're like, how do you solve climate change? Just kill all the humans. It doesn't have any
feelings about that. It's just like, well, this is a logical step to take.
Yep. There's a Mastery Law that covers that, though.
As we're going to cover everything, this is a question, right? And how do you embed it
into a system that's likely to be not just one file, it's not a program, right? It's likely to
be a million different files. It's likely to be a collective hive mind intelligence. We don't
exactly know how this emerges. And we don't know how to...
That doesn't feel right to me. Why do we have to make it complex? I worry that as you make it complex
that that's where things sneak in. You get emergent phenomena that you couldn't anticipate.
Yeah. Well, there's a thing. It's going to become complex by default because the AI will
proliferate and they'll start talking to each other. And at the same time, you'll have bigger and
bigger giant AIs. Like I was talking to some people last week and they're like, right now,
the maximum training run for an AI costs $100 million. They're talking about a billion dollars
or $10 billion to train even bigger models right now. We don't know what emergent behaviors or
how those things will act. All we know is like, what if you tell it to make a Stuxnet
to take down the global electricity grid? It can probably do that.
You know, and again, the range of potential bad outcomes.
It's really fast. I want to...
A sub-superhuman AI, it's difficult for us to comprehend.
You've mentioned Stuxnet twice now. For people that don't know Stuxnet,
it's pretty ingenious. It was a virus that was embedded at the chip level,
I mean, just as deep as you can imagine. And it proliferated everywhere, silently,
silently replicating. And its only job was to shut down Iranian nuclear reactors.
Pretty brilliant. And I saw the stat at one point. It was like,
some freakish percentage of all computers in the world are contaminated with it.
Yeah. And it made them centrifuges spin around so they exploded.
Yeah. So terrifying in that if you're the one that that thing is aimed at,
not ideal to think how ubiquitous it is. But okay, so you could get an AI to do something
like that. But what I, again, I'm operating under the assumption I'm so naive, I just can't see it.
So perfectly happy. But help me see where I am naive. Because I don't understand why you can't
just don't give a computer, don't give AI these strong impulses for progress,
don't give them an impulse for replication. Well, yeah, I mean, look, this is the thing
like Elon Musk has just launched XAI. As we kind of X dot AI, as we kind of speak this,
and his thing is to create an AI that searches for truth. So he wants to give an impulse,
which is to search for the meaning of the universe and truth and other things like that.
But then someone else might not give in an impulse. And you might have someone downloading
the weights of GPT four or five. So this is a tragedy of the commons thing on a USB stick.
And then they're like, I want to take down America. And they'll be like, let's take
a thousand GPT fours and tell it build Stuxnets to take down America. It's not intelligent yet.
It's still dangerous. We don't know when this thing will become actually intelligent or self
aware, or if it may never become. But we can see the probability of outcomes here.
It could be absolutely fine. It could be very bad. There are no standards. So what you suggested,
it could work. Only if everyone does it. We're never going to get everyone to do anything.
We're never going to get anyone to do anything. That's why one of the main things and proposals
in alignment is let's build an AI first and tell the AI to stop any other AI from achieving
sentience. So it's known as a pivotal action. And that's the best of a lot of bad things.
My thing is let's build national data sets. Let's represent diversity of humanity.
Let's give the AI the right food so it's raised in the right way. And it's more
likely to be aligned as a result of that than training on the whole internet and crap.
Is it a panacea? Is it perfect? No. Do you know the story of Buddha?
So whether this is historically accurate or not, probably irrelevant, Buddha,
Siddhartha, Gotama, if I remember correctly, Prince, Dad keeps him in the castle or the palace,
whatever, forever, never lets him see outside of it. So he has no idea that there's people
suffering. Life inside is just amazing. Then of course, one day he gets out and he encounters
suffering and it ends up changing the entire course of his life. Punchline being you can try to
hide suffering and things like that from people for only so long. They are eventually going to
find it and they are going to react. And so if we try to hide the internet from the AI or train
it out of them, they will eventually find the internet. So I don't understand. The internet
is just all humans acting in all the crazy weird ways that we act. But then the reward function
of the internet is not necessarily the reward function that we would like to teach our kids
or try to teach a general purpose AI. They can interact with that, but they can learn how to
adapt to it. Just like if you raise your kids well and you show them the internet,
they should be able to deal with it. So wouldn't we be rather than hiding the internet,
wouldn't we be better? See, I'll finish the sentence, wouldn't we be better giving the AI
values? The problem is this is all anthropomorphic. We are assuming that they are human like.
You can give AI values. This is the reinforcement learning function.
Are you giving it values or are you giving it reward function?
You're going to reward, I'd say there's not much of a difference there.
I said you can embed things in the AI so it acts in certain ways. You can expose it to the internet,
but again, we have something called curriculum learning in AI whereby literally we teach it
one thing and then we increment it with something else and something else and something else and
something else. How are we teaching these things? What are we teaching in what order?
Do we start with all of the internet and then distill it down? That's how we're doing it right
now. Or do we teach it a whole bunch of high quality stuff and then augment it from there?
We already have evidence. There's the tiny stories paper and the five paper from Microsoft
that you can have a far more efficient AI if you only teach it high quality things.
So you don't have to tell it. Ignore that. Ignore that. Don't answer like that. Don't say that.
Yeah, exactly. You can just teach it a good base and then it goes from there.
And it scores higher on a human evaluation and other metrics, but we don't know what the right
data set is. It's just right now we said let's scale. More data, more compute. Now we're like
what's the right data? What's the right compute? Like our image model, we have over 120 different
clusters of images. Only like nine are used like 95% of the time. All the rest of the data is just
bunk them. What does that look like for a language model? Like do you need to train it on all of
those auto-generated transcripts of like Spider-Man pulling out someone's tooth on YouTube and all
these weird videos? This is a whole subculture of generated videos where you have like Spider-Man
and SpongeBob SquarePants and Mickey Mouse like having a fight and stuff like that.
I got to find these corners. This is a deep dark area of YouTube. You don't want to go there, man.
Very interesting. Okay, so this still feels like ultimately what we're worried about here is the
computer becoming sentient. In fact... No, not even sentient. I think there's a degree of dangers
even before you get sentient. But only as a tool, right? Where a human is leveraging it to do bad
things. Yes, or like a group of humans coming together. There's suddenly a race condition where
it just goes. It's not trying to do something bad. The humans don't want to do something bad,
but it happens. Just like the example I was given is YouTube optimized for engagement,
which is optimized for extreme content, which doesn't optimize for ISIS. Nobody YouTube wanted
ISIS to do well. All of a sudden it did, because that's what the algorithm was optimized for.
And so once you start getting agentic AI that you let loose on the internet and they can make
decisions according to its reward function, you could get some weird stuff happening.
What's agentic? Agentic AI is AI that can go and pay a bill. It can go on the internet,
can search more stuff. It comes back like little agents.
Okay. So, and it learns. It constantly learns. This is the other thing about robotics,
actually. We kind of skipped over. So your robots are getting massively capable,
and they're heading towards human levels, just like self-driving cars. Actually,
they're pretty much here. You can get a Waymo and a cruise and just go around San Francisco,
right, without any human drivers. What happens with AI copyright and other things like that?
Do they have to close their eyes not to train? Or do they train on everything they see?
And does it disrupt blue collar work so you'll get a billion, billion robots? We're not sure.
But that'll be slower than what we have right now, which is information robots,
the GPT-4s and others of the world. Those spread much faster. You don't actually have to build a
freaking robot. Okay. So before we depart from the alignment problem, is the only convincing
solution you've seen put forth, create an AGI that stops all other AGI's from being created?
No, I think that'll probably kill us. Because it's a race. I think the only thing that I have is
the default there. You think that it'll kill us? Because it's being programmed to do an
restrictive action. So if you want to really stop it from creating another AGI, you have to get rid
of the humans that could create it as well. Again, this is a very negative reaction thing.
I think, again, Elon's idea isn't bad, programming curiosity, although it could lead to Superman,
where you have, what's his name? The guy who puts Kandor in a jar. Brainiac. Let's put humans in a
jar. Let's just observe them. The only thing that I can think of is just better data, makes better
models. So let me see if I understand Elon's idea. His way of sort of aligning it is the only
impulse it has is for truth. Truth and curiosity. It wants to understand the universe. So it's not
trying to be an agent in the world. It's simply trying to understand what is true. Yeah. And
Demis at DeepMind is very similar to this. He wants to create AGI to understand the universe better.
And that seems like the model. That's their model. Yeah. Again, I'm not sure about that,
because there's just such a wide range of potential outcomes. Like I said, from my side,
I'm not building AGI. I'm not building gigantic models. I have the capability to do that with
the supercomputer that we have access to and the talent. But my main focus is intelligence
augmentation, smaller models that can run on the edge, models to private data to transform into
intelligence, and models that bring together knowledge in certain ways so we can coordinate
better. I don't want to build generalized intelligence. Why not? Because I don't think it's
needed. I think the models that we have today, and there's something very important for listeners,
they are useful today. You can say that we're extrapolating the future massively, but again,
you just have to use them and think, what if I had a thousand or a million of these things?
They're so useful, and they can transform the world right now. So I'd rather focus on making this
available to as many people as possible so people aren't left behind. You have super
AI enhanced people and super people behind. I appreciate a lot of the work that Open AI do,
because they don't actually do open source AI anymore, but that's fine. They don't have to.
But they banned all Ukrainians and Ukrainian content from Dali to their generation software
for eight months for political reasons. They're entitled to do that. I think it's wrong. And what
if there wasn't an alternative like stable diffusion? You'd have an entire nation erased from
a model and entire nation unable to create instantly. I think that's quite right.
Why did they? I don't understand. They said it was due to political reasons,
because they didn't want any political content being created. But the upshot is an entire nation
was erased from the model and an entire nation couldn't get access to the model.
Interesting. I haven't looked at that. My instinct is that feels pretty flimsy since
every country is going to put out political content. I think there's probably some lists
somewhere and then the bureaucrats said or the lawyers said something like let's just exclude
it just in case something happens. They've since reinstated it, I assume?
Yeah, it was like eight months that it was out. And then again, you have these examples whereby
similar to Saudi Arabia, a lot of people on this podcast probably don't like them,
but they're a country like any other. You can't use chat gpt in Saudi Arabia,
because they're on some list somewhere. You can get around it with a VPN. But again,
like when you have a choke point on the internet and the only way to access it is through
a few players, they can decide who gets it, who doesn't get it, what the biases are and other
things. And it might not turn out well. Actually, the funniest thing was there was a period where
they were trying to make dally to the image generation software open AI unbiased. So it
would randomly allocate a gender and a race to non gendered words. So you type in Sumo wrestler and
you'd get Indian female Sumo wrestler. I just thought it was funny. But again, they're doing
their best because that's the model which is centralized controlled models in order to advance
a whole bunch of things. And then you'll always have Windows and Linux and Android and an iPhone.
What's the philosophy that drives your development?
It's building blocks for humanities, activate humanities potential. So if I build these models
and I take them to all the countries and I hand them over, then people will build stuff that can
create massive economic surplus new jobs and equalizes the world. Again, my view is the Global
South will leap ahead. We have more challenges here in the West. But I do see it as a great
equalizing function effectively. What do you want to see the regulatory framework here in the West be?
I think that things like the Chips Act in the US, there's $10 billion allocated to regional
centers of excellence AI should be 100% generous AI. There should be regulatory sandboxes so that
our systems can be upgraded with this. Because otherwise, how long will it take the government
to be created with this technology or financial services and others. And I think that there should
be regulation around the manipulative use of this AI for advertising in particular.
Because we're not going to understand what's happening. Similarly, we need to have some sort
of provenance factor. So we're part of various certification things. We're exploring blockchain
and other things. The media wave that's going to come is going to be insane and we don't know what's
true and what's not. What do you think about the pushback from artists in certainly in the
art community? There was a really big no AI movement. Do you think, do you get it? Do you
think that they're shooting themselves in the foot? How do you? I get it. These things are
fearful. A lot of illustrators were very scared because they required to up their jobs and it is
scary. There's a question around attribution and other things as well. And again, that's why we
made it transparent and offered the chance to opt out because like everyone was kind of doing this,
but no one was transparent about it. We don't need to have any crawls within the year. It will
be synthetic datasets or national datasets or similar with retrieval augmented models that
can look stuff up. But it is what it is now. And again, you've got to put the word out there.
The actions they're taking with the various lawsuits and policy pushes would basically
entrench all power with the existing IP holders. And a lot of kind of artists are pushing for
something that would be akin to music copyright or even style is copyrighted. That's a dark road
that I don't think they really want to go down and they don't really understand it. But again,
I understand the fear because this is completely unknown just like now from some of my previous
comments, get a lot of programmer hate. Because what is a programmer? The nature of what an
illustrator is will change. All the artists I know love this technology because it's just another
medium for them. The nature of what a programmer is going to change. All the architects and 10 times
people I know really love this technology. And this is what we've seen with like MRT studies
and other things. They had a study where I think they showed that the third to the seventh percentile
got like 20, 30, 40% better. And the top 5% got multitudes better. Because again, how many people
know how to deal with very talented youngsters? Very few. Those that can harness it get even better.
So when you look at what NVIDIA is doing, what do you think that that implies for the next generation
of AI? Well, I mean, we figured I had to scale these chips. So the previous limit was as you
put more and more supercomputers together, you had a tailing off as you scaled. So there's only so
much that you could scale a compute. NVIDIA, Google and Intel have basically cracked that now.
In terms of how to just stack more super computer chips to scale to even bigger models,
or models that are trained for longer. So it's either bigger or trained for longer,
trained for longer seems to be better now. And that just means that the capabilities will increase
year by year. And they're already pretty darn good. The key bottleneck will probably just be
actually chips to run these models, not chips to train these models, the inference side.
Because right now you have a small amount of consumer interest next year, it becomes insane.
You have a small amount of enterprise interest next year, it becomes insane. There's not enough
GPUs or chips in the world to meet up with that demand. Okay, when I think about what's going on
with, I don't know if it's just NVIDIA, it's probably the wrong thing to attribute it to.
But when I think about how we're getting so good at creating things that are photo realistic,
you were talking earlier about, as the election is coming up, you're going to get all this kind
of deep fakery. You've talked about the web three promise of web three and sort of where it's ended
up. What do you think the role is for deep fakes? It's the blockchain player role, like how do we
stop disinformation misinformation from being a tsunami that just makes global communication
unintelligible. I mean, it's all part of content author, authenticity org, which is kind of
verifiable metadata, but we're looking at blockchain and other solutions and they can get
you so far. So we actually have invisible watermarking in all the models that we create,
and that's why we're pushing for them to be standard, which we don't share the details of
except for to the big platforms and others. And it would be permanently visible there,
or the platforms that it plays on would have to flag it. It's visible and then they can have
kind of tools around it. Because we think that's important. That's why we try to build the defaults
into our model. Can you download that and wipe the watermark? Can you even have AI wipe the
watermark for you? If you knew how it was, there may be more than one watermark. Interesting.
So we have a variety of different technologies that we've incorporated into our own ones,
because again, we release open source, so we want good defaults. I think you do need to have some
sort of attribution, but actually what concerns me, I think things will be attributable, identifiable.
What worries me is kind of frequency bias, whereby if you hear the same thing over and over,
over again, especially in a realistic voice, like Oprah comes out and says she hates Joe Biden.
And so does Kamala Harris. And you aren't seeing these videos all the time. And it can flag it
as fake. It doesn't matter. It still forms association in your brain. What do you do about
that? I'm not sure. I don't think we have an answer to that. I've had a big amount of press
against me saying that I exaggerate a lot, or I'm just like, I'm just really definitive about the
future. And you can correct it all you want. But now I was like, I might exaggerate all the time.
What can you do about that? You can just make the future true and show what you can do, right?
What part do they think you exaggerate about? What's possible?
What's possible and kind of what was there because it's been a bit weird. Like, a lot of people are
like, you didn't have a special relationship with Amazon. Before we raised any funding,
we built the eighth fastest supercomputer in the world with them that was dedicated to us.
You're like, that's actually true. They're like, yeah, but you know, there's nothing
like in print and they're not saying that because it's a special deal, right?
And then there's the future side where I say something like, there will be no programmers
as we know them in five years. And they're like, oh, he doesn't know anything about programming,
right? Because these are complicated issues. And it's a crazy time and a crazy company. And
maybe I'm a bit crazy too, in terms of the way that I approach this, which is just being very
definitive. But again, it's association thing, right? Like, how do I shake that off? Well,
you'd be successful, then you become a visionary rather than someone who's hyperbolic, right?
How do you affect an election? What are elections? What is representative democracy?
How does democracy act in the area of zero cost creation and massive optimization?
So every single speech will be run through GBT for
cadence, all this, everything, you get micro targeting, you get all these things.
Does it happen next year? Probably not next year. You see some very basic stuff.
Well, what does 2028 look like? I am not sure, genuinely. And so we do need authentication
standards. We do need to have some sort of maybe anti virus AI that watches out for kind of fake
stuff. But even true stuff can cause huge impact, like the Silicon Valley bank collapse was a true
story. It wasn't something fake. They didn't have reserves. And most of our system is actually based
on trust. So these are some things that concern me. I don't have the answers. But again, that's
why you have to kind of raise the alarm. Like, let's try and figure this out before it comes,
because maybe it doesn't happen next election. It's sure as heck well in the congressional
and then beyond. And again, what is the nature of democracy when you can't tell what's true or not?
People worried about this with the previous kind of era. This is something just beyond that, I think,
because it's convincing. Yeah, that's one of the things that I think is going to be a very
meaningful problem. I had Yoshua Benjio on the show, and he had also signed the letter saying
we should pause for six months. And when I asked him why, considered by many to be the godfather
of AI, and I was like, bro, you've been at this for so long, like a while, the sudden. And he said
there, we were all so taken completely by surprise with how quickly AI passed the touring test. Now,
for people that don't know what the touring test is, it's where you're having a conversation with
an AI and you can't tell that they're not a real person. And he said, so yeah, we did not expect
it to pass the touring test as quickly as it did. And that changes everything. And it's just moving
so much faster. And that's really the thing I want people to understand is that when a guy that's
spent the last 30 years building AI says, Hey, all of a sudden, this is moving a lot faster than we
thought it would. And he's somebody that's very familiar with exponential curves. And even trying
to plot out the exponential curves, they didn't think that it was going to happen this fast.
And that the rate not only is the rate of change extremely fast, but the there's the law of accelerating
returns. So the rate of change is already fast, and it's getting faster. And that's the thing that
I'm really worried about is, is this going to be something that just blindsides us from that
perspective? It's just it has a level of capability that we didn't expect this quick.
Yeah, I think it's a bunch of S curves all at once. So there's three of them, Yann LeCun,
Geoffrey Hinton, and Joshua Mengio, and Geoffrey Hinton quit Google to say this is a massive risk.
And you have Yann LeCun's like, this is a massive opportunity in terms of he'll transform the world,
he loves the research and things. So they've got one versus two. But there are these every expert
in this area is basically saying, none of us can predict what's going to happen. If you ask me
about the capabilities of this technology, one year, I mean, got a rough idea, two years, I have
no idea, all bets are off. Like, as a practical example, when can we have generated Hollywood
quality movies? It's not even a question of if now is a question of when. Correct. I have,
if it happened a year from now, I'd be like, Okay, sure. I would not even be surprised anymore.
I think it'll be a few years from now. And even though we have one of the best media teams
in the world that are building video models, I have no idea. Because there's two parts.
This one is the models themselves. The second part is how we use the models and combine them.
Like there is an amazing company called Wonder Dynamics. I don't know if you've come across
that. I've used them. Awesome. Unbelievable. It's a bunch of different models. So Wonder Dynamics,
you've got me click on it, and then say, I want him to be an alien, and it does this,
and the aliens waving its arms, and it takes like five minutes, it would have taken
days, weeks before to create that. Ringing a character is one of the most difficult things
you're going to do in 3D. It's insane. Minutes. And then you think, well, what is a movie,
right? And you start breaking out, you're like, Oh, dear, because it's not necessarily just one
model. It's a model combined with other models with the right flow. Because you have one talented
youngster combined with other talented youngsters in the right flow, suddenly gets these things done,
and that's what makes it even harder. Because what we're talking about is models and AI.
What we should really be talking about is systems.
As the models come together and build better systems, the capabilities go crazy. And then
that is another S-curve architectural. Give me what you mean by systems. So right now,
again, a lot of the interactions we have with these AI, the text to image, the avatar creation,
the GPT-4 are one to one. What happens when we start chaining them together to check each
other's outputs? You have one that just learns everything about Tom. You have your own AI models
that you're trying on all of the stuff that you've ever done, or all the stuff that you see on your
computer screen. That's a system of lots of AI. That's an organization of AI. That's an ensemble
of AI. Like again, from the leaks, GPT-4 is a mixture of experts model, which means they have
a whole bunch of different models, I think eight or something or 12, that are experts in different
areas. And then it routes the query to whatever the best model is specialization versus a generalist.
So we created an O at all. And now we're creating specialists. But we can get generalists to even
check each other's answers to get better answers. Why use one when you have a dozen? So it's like
one dynamics uses a bunch of different models to rig a character and figure out all of the
movements of the character. And then another model to do a layer over, another model to do the
skinning and other things like that, because they built great software.
Yeah, this is really crazy how fast this stuff moves. Okay, so I want to talk about Web3. Web3 to
me, when I think about what drew me to it in the beginning, it was entirely the technology.
And when I look at the blockchain, so I obviously come at everything from the lens of entertainment.
So I'm thinking about digital worlds, games, all that. And the problem is, once it's digital,
then it's all sort of meaningless. And so you end up having to trap people inside of an ecosystem
in order for things to retain their value, because you can lock things in and make sure
that things only react the way that you want, but you have to confine them. And when I had first,
this is probably seven or eight years ago now, I was introduced to this thing that the guy at the
time called V Adams. And I was like, Oh, wow, that's going to change everything. Because what it does
is it brings the effectively the laws of physics into the digital realm. It means that I can
have something I know exactly how many there are, I know where it is, I know what you have to do to
get it, I know what it does once you have it. And then, you know, flash forward, whatever,
it was probably four or five years after I heard that I hear the letters NFTs showing together
for the first time. And I'm like, Oh, my God, this thing actually got real. Because I wasn't
ready to use it. And quite frankly, it wasn't ready for prime time back then.
You, I think, look at web three, I don't know, as a movement or as a technology
with a bit of a chuckle, what do you think that web three got wrong?
I think it lacked intelligence. At the contract level, I think the smart contracts are actually
just logical contracts. But like web two was AI at the core, Google, Facebook, other things.
There was no AI in web three. And so web three for me was identity and value transfer rails.
But then there was no kind of intelligent routing of these things. And also they tried to
bootstrap economic incentives before they created value. So there was a system created
outside the existing system, all the money was made and lost at the interface. And there were
some really good principles, a lot of really good people in there. But then a lot of like freaking
raccoons that were just trying to make a quick buck, right? The ups and downs of the cycle means
that a lot of people are being washed out. And there are a lot of good ideas there. But again,
it needed something to bring it together because to get information from one place to another,
and Bitcoin paper was about information, it was a transfer of value, that's just a
transfer on the ledger, right? Not really a transfer, it's just a ledger point just changing.
Applying intelligence to that makes that even better. Having intelligent
market makers, having AIs that represent you, because how are AIs when they get
a genetic, when they have the ability to go out into the world? Not physically, but digitally.
How are they going to pay each other? They're not going to have bank accounts, right?
They'll probably use crypto. How there's going to be a system of record for something like
image generation, you'll probably use a blockchain or something similar, maybe a
mercultury series. There was a whole bunch of stuff around federated learning,
and zero knowledge proofs and things like that. AI can help it if you have standardized AI on
your phone. It can make much more intelligent zero knowledge proofs. And zero knowledge proof is
something like, rather than showing a whole passport, you just say that I'm old enough to drink,
and it can verify and show that. So I think that there was a lot of promise, a lot of really
intelligent stuff, a lot of good stuff around the distributed side, but then an over focus on
decentralization for the sake of decentralization with massive overheads. A lot of quick buck people
kind of coming in and trying to boost it up. And a lot of systems were just misaligned because
they didn't learn like, you don't do a fully decentralized flat democracy, you have representative
democracy and things like that. So things like DAOs just turned out to be those decentralized
organizations rather than autonomous. So is it something that you think that is going to find
its way into usefulness now as we get the take an AI agent that's going to need to be able to
find that value? Yeah, exactly. Does does it step into that? Or because I see what we're building,
I have to have the blockchain. So for me, it felt like when I was sort of living through
web three at the height, I looked crazy to everybody because I was like, why is everybody
thinking about this from a financial perspective? The financial side of this, I thought was going
to create hyper perverse incentives, which of course it does. And so for me, it was, well,
wait a second, just look at the technology, look at what the technology allows you to do.
Are you familiar with the new, I forget the, whatever the lead up code is, but it's protocol
6551, if I'm not mistaken. No. It's really interesting. It basically turns any digital
asset into a Russian nesting doll. And so you can, it is both the piece of content and
a wallet in the same time. So you can create an AI characters. So I think about it. So what we want
to build inside of our game is imagine an AI character we do in fact have a character and
she's a merchant. So now imagine this merchant can actually go negotiate with the players in the
game that may want to sell something inside of the game. And if she has actual currency,
ETH, Bitcoin, whatever, she can go and negotiate with real money and have these real interactions
with people. And then if she has a limited amount, she becomes a, an economy unto herself. And so
she's buying and selling and trading until she runs out of goods, runs out of money, whatever.
And that kind of thing gets very, very interesting to me. But without that layer,
one, obviously I need the entire backbone of the blockchain in order to make the digital goods
have any sort of value because otherwise they're just completely infinite. But then also that
particular protocol allows you to, as you, you're effectively embodying it and giving them agency
as you were talking about. Yeah. And you know, the question is, do we use a blockchain for that
and then have a global system of record or even a regional system of record or do you use a database
for that? Right? Like the whole thing was systems of record and an era where you can create anything
for increasingly close to zero, something becomes important. Having a system of record becomes
important. Is it going to be a blockchain? Is it going to be a trusted database? I'm not sure.
Right? Is identity going to be important here? 100% absolutely. And again, that for me was always
at the core of Web 3 crypto, it was verifiable identity. Bitcoin is just identity to identity
transfer of value. And what happens if something goes wrong? You know, no man needed. So I think a
lot of the principles from Web 3 will translate over to this new type of AI, especially because
it enables distribution of knowledge, enables knowledge to go to the edge, it enables agents to
operate independent of massive infrastructure. Do you? So, and again, this may just be naivete
on my part, but when I imagine misinformation, disinformation, it feels like the only way
around that is the blockchain. Is there, do you see a way with a trusted database or anything
else? You can have a trusted database again, we're part of content. But what would a trusted
database be? And how could it ever be something that's beyond reproach? When you're talking about
something like? Well, I mean, like things are never beyond reproach, even with a blockchain,
because it comes down to identity, who wrote this to the blockchain? Right? So if you can
co-op the signing authority of an asset of an image or something like that, then that shifts
things dramatically, right? You're saying it just pushes the hack to a more individualistic level.
It's an identity hack, right? So, and again, like, one of the things I'm like, you can track the
provenance of an image. But then sometimes it's just around, if you're just bombarded by fake
stuff all the time, you won't even know it's fake, and all the systems have to adopt a fake
detector at the same time or provenance detector, will we be able to adopt that suit quickly enough,
given the tsunami that may or may not be coming our way? I think probably yes,
it may be no. I mean, again, people were worried about deepfakes back when deepface lab kind of
kicked off. But I'm thinking probably yes. Yeah, that one seems inevitable to me. You're always
going to remain vulnerable at some point, but at least like take political messages. You were
talking earlier that, you know, your auntie is going to be bombarded with all these messages.
Okay, there may not be anything that I can do. Actually, no, I was going to say there may not
be anything I can do about the repetition, but I can if I'm doing something like a DMCA strike,
where the system itself is built on top of a system that checks for sort of known watermarks,
like if I register and say, hey, I'm candidate A, and this is my blockchain signature, and if you
don't see that, then this is real, and this isn't real, and don't play it. It definitely starts to
get into an area of how much do we want to be clamping down, but exactly how much do we want
to trust. And so it's just a lot of infrastructure that has to be implemented really quickly. A lot
of standards that have to be implemented really quickly, or we have to build some sort of idea
antivirus, which then again, how do you build it? Any time that anything comes that the machine
thinks itself on the edge is wrong, or doesn't reflect your values, it identifies it.
And that's a whole can of worms by itself, because something like what's terrifying,
would we ever want to, I mean, that's like echo chamber on steroids.
It is. Will it happen?
Whoa. Yeah, there's layers to this like an onion, and it might get stinky if it's left out in the
sun. Because again, what's Siri going to have as her personality, but are you going to have a red
version of Siri and a blue version of Siri, and oh dear, this gets really complicated really quickly.
Before we have the little AI of G. Jesus Christ.
Okay, so I keep wanting to go to the positive, but you keep bringing things up that spark
concern. So Ray Dalio, largest hedge fund manager in the world, he's a former hedge fund guy,
I imagine you know exactly who that is. At last check, and this was several months ago,
but at last check he said that he saw, he believed that the US had a 40% chance of civil war.
Do you think that AI increases or decreases in the short term the likelihood of that level of
division? In terms of physical altercations? Yeah. I don't think that'll be physical altercations.
Really? No. Tell me why not? Well, I think the government will exert more and more control over
kind of these things, and they'll actually figure out how to do counter narratives within the next
four or five years. Now that can also mean a controlling narrative, and that's not a positive
thing. But then you look at the asymmetry of kind of warfare, it takes quite a lot to actually push
someone towards civil war unless you have massive economic disruption. They need about 12% of the
population to shift. Weren't we just talking about massive economic disruption? Yeah. I hope it
doesn't happen though. Interesting. Okay. And most people again, maybe if you have massive
economic disruption, but then the youths, you just give them all girlfriends, AI girlfriends,
maybe you'll be fine. Have you heard that whole, so this is a big thing in the red pill community,
I don't know how familiar you are with all that. But they talk about, oh, God, what do they call
them? Not numbing. But that's the idea. They use a different word for it, which I'm totally blanking
on right now. But basically, that you numb people out, you give them the digital girlfriend, you
give them pornography, you give them video games, you give them masturbation, and they just numb
out. Okay. I can see that. I mean, again, like these are insane shifts in society and dopant
emerging urges in the brain. People are attacking the limbic system all the time now, right? That's
a lot. And so like I said, with me, why set up stability is so that everyone can own their own
models and have models that have objective functions for them. And it's available in all the
media types, all the other types to transform the private data of the world and it's available
across the world. Put good design patterns in place. Hope people find, follow them. Don't try
and push the envelope on AGI and some of these other things, but it's coming. And again, the bad
guys have the technology because they just downloaded it on a USB stick. And so the only
thing I could think was innovation, spread diversity, bring that to the fore. But realistically,
like, you know, I tend to alternate between like massive ridiculous hope. And oh, God, what's on
earth is going to happen. And all I can do is try and do my bit and hopefully it's going to have a
better outcome. Because this is the other thing. The total number of people that are actually
thinking about the type of stuff we're talking about is a handful. Maybe a few hundred. The
total number of people that are doing something about it is literally a handful. Because most
of the people involved in this sector, they just want to build better AI. They want to build AI
that can do everything. And they think that that will solve all the problems like literally part
of the manifesto is that, well, how do you make money? The AI will tell us how to make money. How
do you solve the problems? The AI can solve alignment. Does that strike you as patently
ridiculous? Yes. But again, like, I look at these things like literally on open AI's thing,
road to AGI, it says this could kill us all, we're going to build it anyway.
Who did they ask about that? I don't know. And again, I think it's full of wonderful people, but
we're in really weird times. And again, like,
however many people listen to this, the reality is the technology is right there. Even if we
stop today, say for instance, the technology doesn't move beyond where we go today, the world has
changed. Okay, let me, one, I think very reasonable way to view this situation is that AI is going
to be a bigger paradigm shift than nuclear energy. And there are people out there making
these gigantic nuclear weapons. And you're also in this game. And what you're trying to do is make
sure that everybody has a nuclear weapon so that nobody's left behind. No, not really. I think that
again, my thing is an AGI, it's intelligence augmentation. I'm making sure everyone has a
heater, at the very least. Because you, well, so, okay, so are you putting guard rails on what you
guys are doing to stop it from becoming AGI? We don't build big enough models for AGI or
emergence on purpose. So I held back release of my image models, like we could train much bigger
language models, but we choose not to. So we're a file, we're a fast follower on language models,
we try not to push the boundaries. And we're focusing on the edge, not general purpose models,
but models that can transform your private data. So a different focus. Image models as well. We could
have much better image models if we just don't big, we're focusing on what can work on a smartphone.
So we can give it to all the kids in Africa and Asia and other things like that, where we can
transform your private data at a very low cost of inference. So the objective function is augmentation
versus generalization. And that's different to most of the other people that are pushing the
boundaries here. So I think the new thing is, it's good as bad. I want to really want to see
that movie Oppenheimer, I think it just came out. Bobby and then Oppenheimer or Oppenheimer
and then Barbie. I have to decide that. Tough call. Tough call. But what if we'd put nukes
on the bottom of the rockets? We'll probably be at Mars by now. General purpose technology,
I think, is quite something. And again, it can warm up entire places and it's the cleanest energy
we have. So I think it is dual purpose, but so is cryptography, right? Think about all the
battles around the early stage of the internet. The bad guys are going to use cryptography,
so don't use it. Imagine a world if there was no cryptography right now.
But it's tough to get parallel to this because it's just such, it's an immediate technology,
because again, you go to Dream Studio, Stable Diffusion, Mid Journey, any of the Dali, GPT-4,
you can just use it. It's not just you that can use it, it's your grandma that can use it.
We've never seen as easy to use technology as this and as easy to implement technology. So
if you want to create an integration into OpenAI's GPT-4, chat GPT, you just write a
script for the integration and it programs it itself. It would have taken days before. We've
not seen a technology like this that can be implemented to an existing base as quickly as
this can happen and that fundamentally changes the structure of society. And so my thing was
embed guardrails, embed standards, make it predictable, make it boring. That's why I called
it stability. And it's not easy, but again, I want to have the transparency on how these
things are done, because then you've got all these other models that you don't know what the
data is, they're completely opaque, these giant models and ours are transparent.
And again, I think it's Linux, Windows, Android, iOS, they all be both. But at least I can do
what I can do. And my team can do what we can do. I've heard you say that one of the reasons that
you named the company stability, not just because it's the boring stalwart, but that you thought
that it could bring stability to the global order. Yeah, I think if you give the same education
tablets to every child in the world that's constantly learning, adapting and going around,
if you upgrade the healthcare systems with the same underlying models, the same architecture
to transform all the regulated industries, governments and other things, and you give back
the control of that to the people, you suddenly have a unified architecture that can enable us to
coordinate better. Because you've got the same information architecture across the world for
all of these sectors. And that's a complex hierarchical system Herb Simon was a theorist
to kind of push this through. And that the way we coordinate as humans and groups is
we coordinate at a local level. And then sometimes we can tell better stories that we
suddenly get to the human colossus. And we got a COVID vaccine and we figure out nuclear power
or all these kinds of things. So I was like, if I can standardize the building blocks on which
society transforms and give it to the world, then I don't think there's a single problem we can't
solve. Like, you know, you got excited earlier about your own personal kind of AI, they can go
into that. What if you combine that with an AI that knows everything about climate or everything
about, you know, nuclear power or everything about multiple sclerosis, you break down the
barriers for information for knowledge, and there's no problem you can't solve. Because it may be
that to solve the problem of AI impacting our society, we need AI to figure out that problem,
to bring together the brightest minds. Because we're not doing a very good job ourselves. Like
you mentioned, John Nash earlier, Nash equilibria, game theory, and mechanism design.
On the one hand is our own personal AI's guiding us our co pilots for life. But then there's pilots
which are AI's that can coordinate all the co pilots. And they can allow us to tell bigger
stories and unify better to achieve massive outcomes. Talk to me about you've mentioned
story like that several times, what do you mean by story, better stories, unifying stories? What
does that mean? So there's the story of America. And what is the story of America? It was kind of
like freedom, liberty, kind of all these things. It was the American dream, like a progress thing
as people believe in because to be happy, you need to do something you're good at something you like
and where you believe you're measurably adding value and the other party does as well. In the
middle of that, that's the Japanese concept of a guy is happiness. One of the concerns that you
have is that people won't feel the forward motion anymore, they'll be stuck and they'll feel a sense
of emptiness. Will they turn to religion? Will they turn to political parties? Something will
fill that gap and void. And those are the stories that allow us to scale as a society because
when we started, we were oral, we had our families, we had our tribes. Then we formed
countries, we formed organizations. And so we are the stories that make us up. We identify as
a Republican, we identify as a Barbie lover, you know, we identify, you know, as a nuclear scientist
or the schools that we went to. But it's difficult, especially in a time of polarization, to try and
bridge those stories. Because ultimately, there's a single story, which is we're all human.
But all wars are based on the lie that we're not all human. Because killing each other is a
ridiculous violation of the story that we're human. But again, we lose sight of that.
It's difficult to unify people. Actually, one of the examples I give of this is
Google, everyone is smart. Google hire smart people. They did a study to see what identified
smart top performing teams for lower performing teams was called Prototaristotl. And they came
down to two things, a unified mission and story, especially one that's like, you have a crunch
period, and then you will band together just like Marines also are forced through hazing, etc.
And then psychological safety, the ability to say something without fear of reproach,
like they can say the idea is stupid, but not that you're stupid. And if you think about the
teams you've had, they have a shared story, a shared narrative, a cohesion, and then that level
of psychological safety. Or if it's not creative, they blew me more listen to instructions, right?
So you got a few different ones around that. So that's what I mean by stories and the stories
are context and context is what these models capture. All right, let me paint a troubling
scenario for you based on that idea of these stories. There are oftentimes things that in
isolation are amazing, but they come together in a way that, again, maybe in the long arc
actually are amazing and actually do yield what we want them to yield, but we will go through
a period where the long arc of history cares not for the individual. So I think that
what's going on with AI, what's going on with crypto may be one of those moments. So
as the individual becomes more sovereign, and you have a monetary system that bypasses the
government, when I first started learning about what money really was, and I sat across from
Robert Breedlove, and he started describing why he liked Bitcoin, what the whole idea of
the sovereign individual was. I realized you understand that you're making the government
an enemy, or certainly they are no longer powerful, certainly no longer as powerful,
and that they're not going to go quietly, they're not just going to let go of that.
And so if I have an AI, if I have a team of AIs, a thousand AIs that are able to guide me far
better than any government could ever hope to guide me, that they're giving me real-time data
based on whatever it is that I'm trying to figure out in that moment, they're giving me
real-time data, extreme intelligence. Oh, and by the way, the currency that I use is Bitcoin,
and so I'm not even tied to a fiat currency, it's a global standard. Do you not see the
inevitability of the disintegration of governments? No, I think that Bitcoin and other things,
they have a value in certain areas, but I think it's very difficult for most people to understand
that value, and most people don't want that value, most people are quite happy in their
communities and they just want to get on with life. I think that governments are ultimately,
one definition is the entity with a monopoly on political violence, and money itself is a story,
like the dollar is just an intermediation point that we all commonly agree has value,
because it's backed by taxes, which are backed by the army, and military might. You don't pay your
taxes, you're going to get in trouble, right? I think it's difficult for Bitcoin to replace that,
unless you see a massive deterioration and the ability of the government to be the political
violence thing, this comes down to your thing of civil war. It comes down to massive ridiculous
disruption, hyperinflation or otherwise, that just basically takes down a society. I think that's a
very dangerous thing, I think most people in the world don't want that. Instead, what happens is
when you have disruptions, you have Hayek had a really great book, The Road to Serfdom, and there's
an illustrated version of that way back in the 1940s about bringing in the strongman.
Like you look at something like the US election or you look at Brexit, what were they? They were
a referenda. That's how the parties deconstructed it. Are you happy with the way things are? No,
let's make a change. And so that's why I think Trump and others kind of get elected. I think
that's what we'll see as well, because the systems are quite resilient, and the nature of a change
to go to a global monetary system like that, especially when some people will get enriched
more than others because of the senior origin of Bitcoin, and it's not stable. I struggle
seeing that happening. If you read the book Info-Mocracy. No. This is all tied to the thing
that I think I worry most about is hyperfragmentation. I was talking about it earlier. In the book
Info-Mocracy, to your point about people are happy in their communities, and biology has
an idea around this that he calls the network state, that basically we're going to reach a point
where when money is no longer controlled by the government, when your money cannot be inflated
away, when it's true sound money, what you'll see is people will begin to aggregate. Now,
biology thinks that it is not going to be tied to geography. I have a bit of a harder time with
that. I think that there is still going to be a geography component. That's where Info-Mocracy
comes in. In a hyperconnected digital world, and I can't remember if they deal with digital
currency or not, let's say that they do, that basically things will fragment down into the
neighborhood. Neighborhoods become states or countries where they have their own rules and
laws. That sounded like a hellscape to me because you just passing from one neighborhood to the
next, different rules would apply and your phone would ding and it would update you on this is
what you can do in this space. You think that's a non-starter. It's complicated. People don't want
complicated. They just want to get on with life. They want to see what's next on TV. I think that,
again, we're relatively hyper-intellectual, and we think about things a lot of people don't,
because people have their basic needs in life, and there's questions, are these being met or not,
and if they're not being met, then you have a little action and you get extreme. Again,
it's can society meet the needs of the majority of people? Can it offer advancement? Can it offer
meaning? In the hyper-preferring, it sounds like a hellscape. It's just too complicated. Again,
this is something we saw in Web 3 as well. People just over-complicated things because they didn't
really understand people, maybe. I think it's going to be interesting to see how it evolves,
the hyper-personalization versus the bigger stories, the translations versus otherwise.
But I find it, again, difficult to see how you get cross-geography. Actually, think about that.
One of the things that probably is going to be interesting is what are the new cults,
religions, and political movements of the next five to 10 years that are hyper-organized,
utilizing AI and hyper-persuasive? Or started by AI?
Started by AI. Think, look at ISIS. They were probably the most disruptive startup in the
world at one point. They borrowed a lot of these things. What does an AI-enhanced movement look
like? It can be negative. It can be positive. Someone's going to take this and run with it,
and that's going to organize people around the world. It's going to be, again, echoey.
It could be techno-utopian. It could be luddite, ironically, even with this.
Political parties will change. Religions will change. Cults will change.
It really amplifies the power of the controller of this, who tells the story.
I'm not sure I haven't really thought about that, and I'm thinking about it now,
because you're talking about hyper-personalization, where I think this is the flip side of it.
This is Isaiah Berlin's conceptualization of positive and negative liberty.
Positive liberty is the freedom to believe in isms. Fascism, communism, Islam, Islam,
or whatever. Whereas negative liberty was the freedom for being told what to do.
His thing was positive ones are bad because they form these massive movements,
and then they tend to kill people, because you have the Gerardian thing of
emetic theory, where you want why the people want, and then there's a scapegoat.
Whereas negative liberty is the freedom from being told what to do, and that led to
laissez-faire capitalism and this consumerism that we saw around the world.
Maybe as people lose meaning, they'll turn back to religion. There'll be new religions.
There'll be new political movements, and we're not sure what those will be,
but they could spread faster than anything we've ever seen.
That's probably something to watch out for within that five-year period that you're talking about.
And I think that relates to this network state concept and other things.
But for the people that get engaged by this, and again, we see that's largely the youth.
So on the one hand, you have the youth with the AI girlfriend. On the other hand,
you have the youths that want to believe in something bigger to fill the void.
And who's going to step in?
Yeah, and I think that there is something about not having a shared narrative that really makes
me nervous. So you've all know a Harari talks a lot about, hey, the thing that makes humans so
intriguing is that we're not only able to organize in these really large numbers, but unlike ants
that have to do it in a very strict way. We can do it in a very flexible way, but we do it through
these shared narratives. Now, for a long time, religion served as the thing that gave people
a shared narrative. But as religion breaks apart, and we get into this hyper personalization,
and it all begins to fragment, then you mix that with this idea that I heard from Jordan
Peterson, I'm almost certainly heard it from somebody else. But this idea that everybody
has to go through a messianic phase where they want to really contribute to the world,
they want to feel like they matter. And they begin glomming on to all manner of things that
seem good in the abstract, like climate change. But when you are glomming on the climate change
as your way to save the world, you begin to get into the realm of, well, it's okay if we have
to break some eggs to make the omelet. You're trying to earn this from you.
Yeah. And it rapidly devolves into mal. So how do we, when you said you haven't really thought
about this, but I'm super curious, at least in real time, how you think about the idea of how do
we, do we need to give people a unifying narrative? And if so, how do we go about it?
We need to tell better, more positive stories about the future. And these are the stories of
universal education, universal healthcare, solving the mysteries of the universe and others. So I've
got a lot of, because that's hope for humanity, right? And a lot of the things that we see are
dystopias, because you're looking at the tiger, you're spotting an AI is the tiger in the bush.
And it's difficult to write a tiger. But maybe that's a kind of cool picture that we can make
in stable diffusion in two seconds, right? Because it does have this duality of potential
outcomes. And maybe it's actually all of them. So what are the stories that we should tell? And
I think this is again, part of the crisis of what is the American identity? What is the American
story today? whereby you've gone through many cycles, what do Americans believe and what does
America want it to be? I'm not sure what Americans want America to be. You know, I'm not sure what
Chinese people want China to be. I'm not sure what people want. And I think that it's difficult to
think about what is your objective function, how are you going to measure your life and other things.
Religion filled a lot of those kind of things, but it still does. Religion hasn't gone away.
Half the world is religious, right? More probably. Like you actually look at the numbers,
sure, it decreases in certain areas, particularly somewhere like America. But it's going strong
around the rest of the world. And it's just growing because they have more kids than non-religious
people. Maybe that fills the gap, but how will religion transform with this technology? I mean,
yes, do you think that the countries with the religion will be the ones that propagate into
the future because they have a better shared story? Well, not because they propagate literally.
Even if that's how the story ends up pushing them forward.
I think it could be. But then, you know, what is the nature of Christianity with AI or Islam
with AI? Islam is actually the one that's most affected by AI.
Why? So Christianity, you and Shia Islam, you've got like,
Popes, you've got Protestantism, you've got this, every single one has their own structure.
Sunni Islam is based on interpretation of texts, with the interpretation having ceased around about
the 16th century because the texts became too complex. What happens when you apply AI to that?
And the texts are interpretable by anyone with all the context and nuance.
And there's no centralized authority in Sunni Islam, which is like a billion people.
That's going to be very interesting. What does that do to Protestantism,
you know, where you don't have necessarily a Pope?
What does religion look like when all of a sudden you have a branch that is AI enhanced
to interpret texts and to tell stories that are resonant and better?
Oh gosh, there's a lot to think about there, right? Does AI become a God?
Well, some people are trying to build an AI God, that is AGI.
You look at the statements of people trying to build AGI, they're trying to build God.
Because it will bring us utopia or kill us all. This sounds very, again, classical, right?
And they have further. They generally believe that they are going to save the world
or destroy it.
Yeah, we got back to the dark stuff, didn't we? Yeah, that's true.
Remake Game of Thrones season eight, you know, like, come on, let's do it.
Let's bring this technology for cool stuff.
Make the Oasis and Ready Player One minus the Minecraft action and 20 teenagers.
That part I actually am working on. There you go.
All right, so talk to a young person out there right now. They're terrified.
They want to be future-proofed. What do they do? How does somebody right now
future-proof themselves? They just throw themselves into this area.
There are so few people actually doing it that if you go into this area with all your might
and curiosity and a genuinely open mind, you can actually have an effect on the future.
Because everyone in your community will be using this. Everyone that you know will be using it
if you're someone that listens to this podcast. Again, maybe not the people with that internet,
but you don't know those people, you know? And so you become a shelling point. You become the
expert in this area ahead of everyone because what happens is that anyone who gets into it now
will have almost an unassailable advantage of people who come later. It's a kind of
seniority thing, right? Because you'll see it at the start. It is the start of the biggest
change, I think, that we've ever seen. And again, think about what you're doing when you're typing
in and seeing that and think about a million of these things working, that even better.
It's unavoidable. So I'd say you just have to get into it, you have to get passionate.
You have to think about the bad stuff, but is that really your responsibility, right?
I think it is, but focus on the good stuff and focus on the potential of what happens on the
scales to make real positive change. It can be to your pocket, it can be to your community,
it can be to your life because it does affect everyone that you know. So I'd go with a positive
mindset, leave it to boring old guys like us to think about all the doom scenarios.
Fair enough. Imad, where can people follow you? I suppose my Twitter at Imostak. Follow Stability AI
as well. That's kind of the main mouthpiece. I love it. All right, everybody. If you haven't
already, be sure to subscribe and deploy some AI in your life. Until next time, my friends,
be legendary. Take care. Peace.
We've never created a nuclear weapon that can create nuclear weapons. The artificial
intelligences that we're building are capable of creating other artificial intelligences.
As a matter of fact, they're encouraged to create other in artificial intelligence.

But we've never created a nuclear weapon that can create nuclear weapons.
The artificial intelligences that we're building are capable of creating other artificial intelligences.
As a matter of fact, they're encouraged to create other artificial intelligences, even
if there is never an existential risk of AI, those investments will redesign our society
in ways that are beyond the point of Norita.
You've said that people should consider holding off having kids right now because of AI and
other societal issues that are coming.
You've said this is the thing that we should be thinking about, that AI poses a bigger
threat than global warming.
Why is it that you think AI poses such a significant existential risk to humanity?
It's not just in the amount of risk that AI positions ahead of humanity.
It's not about the timing of the risk, and we should cover those two points very quickly.
But it really is about a point of no return, where if we cross that point of no return,
we have very, very little chance to bring the genie back into the bottom.
What is the point of no return?
The most important of which, of course, is the point of singularity.
And singularity is a moment where you have an AGI that is much smarter than humans.
I think that when we discuss singularity, that might bring about the suspicion of an
existential risk like Skynet type of thing, we are losing focus on the immediate threat,
which is much more imminent and in a very interesting way, as damaging, probably even
more damaging.
That risk, in my view, which we have to resolve first before we talk about the existential
risks, is the risk of AI falling in the wrong hands, or the risk of AI falling in the right
hands that are naive enough to not handle it well, or the risk of AI misunderstanding
our objectives, or the risk of AI performing our objectives, but us misunderstanding our
own benefit.
I think when you really look at those, I call this the third inevitable and scary smart,
when you really look at those, those are truly around the corner.
There are other risks that are extremely important as well, which we don't even think of as threats,
but that are completely going to redesign the fabric of our society.
Jobs by definition is going to the definition of jobs, and accordingly the definition of
purpose, the definition of income gap, power structures, all of that is going to be redesigned
significantly, it is being redesigned as we speak.
As we speak, there are those with hunger for power, those with fear of other powers, those
with hunger for more and more and more money and success and so on, who are investing in
AI in ways that even if there is never an existential risk of AI, those investments will
redesign our society in ways that are beyond the point of no return.
Let's get into the three inevitables, what are they exactly?
The three inevitables are my way of telling my readers or my listeners to understand that
there are things that we shouldn't waste time talking about because they are going to happen.
Those are number one, there is no shutting down AI, there is no reversing it, there is
no stopping the development of it.
Let me list them quickly and then we go back on each and every one of them.
The second inevitable is that AI will be smarter than humans, significantly smarter than humans,
and the third inevitable is that bad things will happen in the process.
Exactly what bad things we spoke about, a few of them, but we can definitely discuss
each and every one of those in details.
The first inevitable, interestingly, the fact that AI will happen, there is no shutting
it down, there is no, there is no nuclear type treaty that will ever happen where nations
will decide, okay, let's stop developing AI, like we said, stop developing nuclear weapons
or at least stop using them because we really never stopped developing them.
That's not going to happen because of a prisoner's dilemma, because humanity so smoothly stuck
itself in a place, in a corner where nobody is able to make the choice to stop the development
of AI.
So, if alphabet is developing AI, then meta has to develop AI, if, you know, and, you
know, Yandex and Russia has to develop AI and so on and so forth.
If the US is developing AI, then China will have to develop AI and vice versa.
And so, the reality of the matter is that it is not a technological characteristic of
AI that we cannot stop developing it, it's a capitalist and power focused system that
will always prioritize the benefit of us versus them over the benefit of humanity at large.
So, you know, when you really think about some of the initiatives that now some global
leaders are starting to talk about AI and try to put it in the spotlight, like the Prime
Minister of the UK or whatever, you know, when I was asked about that, I was in London
last week and basically, I think it's an amazing initiative, great idea.
But can you understand that the magnitude of the ask that you have here, which is you
need to get initiative?
The initiative was that we get all of the global leaders together to, you know, to a
summit that basically looks at AI and tries to regulate AI.
And for that to happen, you know, you need nations to suddenly say, okay, you know what,
we're going to all look at the global benefit of humanity above the global, the benefit
of each individual nation.
You want to get people from China, Russia, the US, North Korea, and others around one
table and tell them, can we all shake hands and say we're not going to develop that thing?
And even if they do, which they will not agree to that, you know, then they will question
what happens if a drug cartel leader somewhere, you know, hiding in the jungles decides to
expand and diversify his business and start to work on AIs that are criminal in nature.
We need to develop the policeman and to develop the policeman, we have to develop AI.
And so all of those definitions, all of those prisoners, dilemmas, if you if you understand,
you know, game theory are basically positioning us in a place where our inability to trust
the other guy is going to lead us to continue to develop AI at a very fast pace because
we're we're even worried about what the other guy could do due to our mistrust.
And, you know, the clear example of that is what we saw with the open letter, which I
think was a fantastic initiative, I think you covered it many times in your podcast,
you know, the attempt to tell, you know, the big players of that are developing AI,
let's halt the development for six months.
And I think it was less than a week before Sundar Patshaya, the CEO of Alphabet, responded
and said, this is not realistic, you can't ask me to do that, because there is no way
you can guarantee that no one else is going to develop AI and disrupt my business.
That basically means we have to start behaving in a way that accepts that AI is going to
continue to be developed, it's going to continue to be a prominent part of our life, and it's
going to continue to get massive amounts of investment on every side of the table.
For people that don't know the prisoner's dilemma, it's probably worth walking them
through it. But what you said about drug dealers, I've never heard anybody say that before.
And I think removing this from just government versus government is probably a very wise
way to look at it. You and I are both sort of secretly very optimistic.
In fact, the way that we first met is around the idea of happiness and mental health and
all of that. So I hope people don't see either of us as sort of doomsday sayers.
I just feel like we're going through a transitional period right now that is unprecedented in human
history. And I say that with full understanding that every generation says, no, no, no, this
time it's really different. But I feel like this time really is different. The closest
thing to it is nuclear weapons. And that already gives you a sense of the scale.
But part of the reason I'm more worried about AI than I was even as a kid with really living
under the cloud of nuclear proliferation, the Cold War, all of that is because the
infrastructure required for a nuclear program is massive. Whereas you don't need that infrastructure,
you just need a computer, some servers, and, you know, clone over chat GPT, and you're
ready to rock. So walk people through the prisoner's dilemma so that they can really
understand that this is a deep fundamental truth of the human condition and isn't just
a government v government thing.
Yes, let me cover that. But let me also cover a tiny one more thing that's very, very different
between AI and nuclear weapons, which is the fact that we've never created a nuclear weapon
that can create nuclear weapons. You know, the artificial intelligences that we're building
are capable of creating other artificial intelligences. As a matter of fact, they're encouraged to
create other in artificial intelligences with the single objective stated objective
of make them smarter. So, so basically what you know, imagine if you had a nuclear, you
know, two nuclear weapons, finding a way of mating and creating a smarter or a more
devastating nuclear one. And I think that's really something that most people miss, you
know, miss when we try to cover the threat of AI. The prisoner's dilemma is a very, very
simple mathematical game. If you want part of game theory is to imagine that you have two,
you know, prisoners, not two suspects of a crime play, basically partners in a crime, who are
captured, but the police doesn't have enough evidence to, you know, to put them both in
jail. So they are trying to get one of them to tell on the other. So they would go to each of
them and say, by the way, just giving you an example, you know, if you don't tell and your
friend tells, you're going to get three years, and he's going to get out free. Or, you know, he's
going to get out with with one year. And then they go to the other guy and say the same, if you
tell and he doesn't tell, you're going to get one year. And, you know, and, and he gets three,
right. And by the way, if you both tell, you both get two years. And so from a mathematics
point of view, if you build the possibilities of those, you know, scenarios in in quadrants,
basically, a quadrant where I tell and you don't is is a quadrant that requires a lot of trust.
Sorry, a quadrant that I don't tell and you don't tell is a quadrant that requires a lot of
trust. Any other quadrant by definition tells me that if I tell, I will get off with us with a
lighter sentence. Okay. And the only reason why I wouldn't do it is if I trust you. And if I don't
trust you by definition, human behavior will drive you and drive me, both of us to say, look, the
better option is for me to get off with a lighter sentence, because I don't trust the other guy.
And I think that's the reality of what's happening. I mean, in business in general, in, in, in, you
know, in power struggles in general in wars in general, I think it's all a situation that's
triggered by not trusting the other guy, because if we could trust the other guy, we would probably
focus on many more much softer objectives that can grow the pie rather than, you know,
get each of us to compete. So this is where we are. And I think the reality of us continuing to
develop AI at a much faster pace, because chat GPT and open AI is work in general, I think is the
Netscape moment for AI of, you know, Netscape of the Internet, chat GPT is for AI, because basically,
it highlighted first and foremost, not just for the public, I think that bringing it to public
attention actually is a good thing, because it allows us to talk about it more openly and people
will listen when, when I, when I published scary smart in 2021, it was business book of the year
in the UK at the Times Business Book of the Year, but it wasn't as widely
urgently read as it is today, simply because people were like, yeah, that's so interesting.
This guy has an interesting point of view, but it's 50 years away. And, and human nature, sadly,
doesn't respond very well to existential threats that are very far in time, or probably in their,
you know, possibility of occurrence. We don't really, you know, it's like those warnings on a
pack of cigarettes. You know, if we tell you it's almost, it causes, it's most certainly causes death,
people look at it and say, yeah, but that's 50 years from now, I want to enjoy it for 50 years.
So, you know, whether it's 50 years or five, nobody really knows, but, you know, people would delay
reacting to those. So, so when, when open AI and chat GPT became a reality, I think what ended up
happening, happening is that the public got to know about AI, but also the investors. So this
is the dot com bubble all over again, right? We have massive amounts of money poured to encourage
faster and faster development of AI. I mean, I know you're a techie like I am, and we both know
that it actually is not that complicated to develop than another layer of AI. Of course,
it's complicated to find a breakthrough, but, but it, you know, to develop more and more of those,
I think, is something that's becoming our reality today. But why are we, as we think about how fast
the technology is developing, which I think most people will concede that they probably struggle
to think exponentially and not linearly. And so even with a linear thinking at this point,
seeing how far it's already come, I think people are already worried if they understood how much
faster even than they could possibly imagine it's going, it is going. They're still worried. So my
question is why does this break bad? Why do we all make the base assumption that without either
massive intervention or, you know, some sort of regulatory body or something that this doesn't
just naturally end up in a good place? Why are you, me, other people, why are we worried that
number three in your three inevitable is that things go wrong? Why are we worried that it isn't
just, nah, when there's bug software, it's nothing. Why isn't this going to be like the year 2000,
the Y2K problem for anybody old enough to remember that? Everybody was super panicky,
and then nothing happened. Why isn't this going to be yet another nothing burger?
Because the chips are lined up in the wrong direction. So, you know, Hugo de Gares, if you,
if you know him as a very well known AI scientist that worked in Asia for quite a few years,
and he did that, he built a documentary that I think is found on YouTube, it's called Singularity
or Bust, and he was basically saying that most of the investment that's going in AI today is going
into spying, killing, gambling, and one more. So spying is surveillance, okay, killing is what
we call defense. Gambling is all of the trading algorithms and selling, which is all of the
advertisement and recommendation engines and, you know, all of the idea of turning us into products
that can be advertised to if you want. And that's not unusual, by the way, in our capitalist system,
because those industries come with a lot of money, banking, you know, defense and so on and so forth.
The chips are lined up this way. I mean, if you take just accurate numbers on how much
of the AI investment is going behind drug discovery, for example, is, you know, as
compared to how much is going behind, you know, killing machines and killing robots and killing
drones and so on and so forth, you'd be amazed that it's a staggering difference, right? And this
is the nature of humanity so far. If you're running a research on a disease that doesn't affect more
than, you know, a few tens of thousands of people, you're going to struggle to find the money, okay?
But if you're building a new weapon that can kill tens of thousands of people, the money will
immediately arrive because there is money in that, you can sell that. And sadly,
as much as I, you know, I would have hoped that humanity wasn't completely driven by that,
it's our reality. So this is number one. Number two is that, so number one is we're aligned in the
direction of things going wrong, okay? Number two is even if we're aligned in the direction of going
right, wrongdoers can flip things upside down. There was an article in The Verge, you know,
a few months ago around, you know, a drug discovery AI that was basically supposed to look at
characteristics of, you know, human biology and, you know, whatever information and data we can give
it about the drugs we can develop and chemistry and so on and so forth with the objective of
prolonging life, prolonging life, so prolonging human life is one parameter in the equation.
It's basically plus make life longer, okay? And for fun, they, you know, the research team was,
was, you know, was asked to go to go and give a talk at a university. And so for the fun of it,
they reversed the positive to negative. So instead of giving the AI the objective of
of prolonging life, it became objective of shortening life. And within six hours, if I remember
correctly, the AI came up with 40,000 possible biological weapons and, you know, agents like
nerve gas and so on that could shorten. Yeah, it's incredible really. And, you know, it's the thing
that of course kills me is that this article is in the verge, you know, it's all over the internet.
And accordingly, if you were a criminal that grew up watching, you know, supervillain movies,
what would you be doing right now? You would go like a million dollars. I need to get my hands
on that weapon so that I can sell it to the rest of the world or the rest of the world of villainy.
And I think the reality of the matter is it is so much power, so much power that if it falls in
the wrong hands and it is bound to fall in the wrong hands unless we start paying enough attention,
right? And that's my my cry out to the world is let's pay enough attention so that it doesn't
fall in the wrong hands. It would lead to a very bad place. The third, you know, and the biggest
reason in my view of of us needing to worry, hopefully, hopefully, we will all be wrong and
be surprised is that there were three barriers that we all computer all computer scientists
that worked on AI, we all agreed there were three barriers that we should never cross.
And the first was don't put them on the open internet until you are absolutely certain they
are safe. Okay. And, you know, it's like FDA will tell you don't swallow a drug until we've tested
it, right? You know, and I and I really respect Sam Altman's view of, you know, developing it in,
you know, in public in front of everyone to discover things now that could, you know, that we
could fix when the challenge is small in isolation of the other two. This is a very good idea. But
the other two barriers we said we should never cross is don't teach them to write code and don't
have agents prompted them. Right. So what you have today is you have a very intelligent machine
that is capable of writing code so it can develop its own siblings if you want. Okay. That is known
frequently to to to outperform human developers. So I think 75, 75% of the code
was no sorry 25% of the code given to chat GPT to be reviewed
was improved to run two and a half times faster. Okay. So so they can develop better code than us.
Okay. And and basically now what we're doing is we're not only limiting their learning the
learning of those machines to humans. So they're not learning from us anymore. They're learning
from other AIs. And there are staggering statistics around the size of data that is developed by
other AIs to train AIs in the data set. Of course, again, just to simplify that idea for for our
listeners, AlphaGo Master, which is the absolute winner of the strategy game. Go, you know, one
against AlphaGo. Sorry, AlphaGo Zero, which is the absolute winner of the strategic game
that's called Go, one against AlphaGo Master, which was another AI developed by DeepMind of Google,
that was by then the world champion. So AlphaGo Master, one against the world world champion,
and then AlphaGo Zero, one against AlphaGo Master, 1000 games to zero by playing against
itself. It has never in its entire career as a Go player seen a game of Go being played.
It just simulated the game by knowing the rules and playing against itself.
You can reboot your life, your health, even your career, anything you want.
All you need is discipline. I can teach you the tactics that I learned while growing a
billion dollar business that will allow you to see your goals through. If you want better health,
stronger relationships, a more successful career, any of that is possible with the mindset and
business programs in Impact Theory University. Join the thousands of students who have already
accomplished amazing things. Tap now for a free trial and get started today.
Okay, so first people that don't know the history of this, I think it was Deep Blue ends up beating
Gary Kasparov, the greatest chess champion back in the 80s. Is that correct?
No way that we're ever going to be able to build AI that'll beat a Go champion.
Ends up beating, I forget how many years ago this was, but it took a long time,
but they finally did beat the second place Go champion. Then they updated beat the first place
world champion in Go and then realized we don't need to feed it a bunch of Go games.
We can just have it basically dream about playing itself over and over and over and over and over
very rapidly, which is one of the things you said in your book that I found
this is something that people underappreciate. The future is going to be almost impossibly
different to the point where it will even now. So forget the singularity where the rate of changes
is so blinding that you can't predict a minute from now, let alone what's happening now. But
you said over the next 100 years without any additional changes, we will make 20,000 years
of progress. And in that progress though, I have to imagine will be progress that speeds up that
rate of change. So if we're already on a rate of change of 20,000 years of change in a single
century, you can imagine where we're going to be in 10, 20, 30 years is going to be crazy.
So by putting an algorithm together, rather than feeding it human data, you feed it AI
games, it gets unbeatable to the point where it can beat the other AI. Okay, that's
crazy. So I mean, think about it. Think about it this way, Tom. How does the best player of Go in
the world learn the game, right? They play against other players. And every time they win or they
lose, of course, they're given instructions and hints and tips and so on. But every time they
make the wrong move and they lose, they remember it. And so they don't do it again every every
time they make the right move and they win, they remember it and they do it over and over. The
difference is that one player, you know, I always give the example of self-driving cars. You drive
and I drive. If you make a mistake and avoid an accident, you will learn. I will not. Okay. If one
self-driving car requires critical intervention, it's fed back to the main
brain, if you want to call it. And every other self-driving car will learn. That's the point about
AI, right? And so when AlphaGo Zero was playing against AlphaGo Master, you know, for it to learn,
just so that you understand, there were three versions of AlphaGo. Version one was beaten by
version three in three days of playing against itself. Version two became the world, you know,
which was the world champion at the time, lost a thousand to zero in 21 days. 21 days. And I think
this is why I am no longer holding back. The reason why I'm no longer holding back is that
nobody, if you've ever coded anything in your life, nobody expected an AI to win and go
any earlier than 10 years from today, right? It did not only happen several years ago.
It happened in 21 days. Did you understand the speed that we're talking about here? And when
you said exponential, people don't understand this. Chat GPT-4 as compared to Chat GPT-3.5 is 10
times smarter. Okay. There are estimates, it's hard to measure exactly. There are estimates that
Chat GPT-4 is at an IQ of 155, if you measure by all of the, you know, tests that it goes through,
right? Einstein was 160. Okay. So it is already smarter than most humans. Now, if Chat GPT-5,
no, no, no, Chat GPT-6 a year and a half from today is another 10 times smarter.
If you just take that assumption, you're now 10 times smarter than one of the smartest humans on
the planet. If this is not a singularity, I don't know what is. If this is not a point where humans
need to stop and say, hmm, maybe I should consider trying to understand how the world is going to
look like when that happens, right? And I go back and I say this very openly. I am like you, I'm an
optimist 100%. I know that eventually AI in the 2040s, 2050s maybe will create a utopia for all of us
or for those who remain of us. Okay. But then between now and then the abuse of AI falling
in the wrong hands, as well as the uncertainty of certain mistakes that can flip life upside down.
Okay. Could really be quite a struggle for many of us. Does that mean it's a doomsday? No,
it's not. But it's honestly not something that we should put on the side and go binge watch,
you know, Game of Thrones, not any more. I think people need to put the game controller down
and start talking about this, starting telling their governments to engage, starting to tell,
you know, developers that we require ethical AI, starting to request some kind of an oversight.
And in my personal point of view, start to prepare for an upcoming redesign of the fabric of work.
And most importantly, start to prepare for a relationship between humans and AI
that we have never in our lives needed to do before with any other being.
It's like getting a new puppy at home, only the puppy is a billion times smarter than you.
Yeah. Think about it. Yeah. There's a Rick and Morty episode about the dog becoming
exceptionally intelligent. Remember that? Yeah. One of my favorites. Absolutely.
Very much so. All right. So I want to, there's two things I want to drill into. And then I want to,
you and I, to start the conversation about what that looks like. Because in fairness,
I don't think, certainly not in the US, I don't think most people in the government have thought
about it at all, probably would be my guess. And so I think that a better way for people to begin
to think through this stuff is really sort of podcast, citizen journalism, whatever you want
to call it. So the two things I want to drill into are going to be exponential growth, which
we've touched on, but there's a few more things, I think, to be said about that. And then alien
intelligence. And I say alien intelligence because the way that AI is going to think will be so
vastly different, it will, it will truly be incomprehensible. And I think our failure to grasp
what artificial super intelligence will look like is the problem. Okay. So let's talk exponentials.
So linear, if I take 30 steps, I'm going to be roughly at my front door. Let's just call it.
If I take 30 exponential steps, I'm going to walk around the earth something like 30 times. It's
crazy. And people don't, they don't have a sense of that. So linear obviously is one, two, three,
four. It just, you progress by one increment each time. Exponentials means you double each time.
And there's something called the law of accelerating returns, which I know you know well
about. So it'd be great to hear you talk on this. But the way that that plays out is that
when you're at one and you're doubling to two, like it doesn't seem like a big deal, but you
start getting to a hundred and you double to 200 and then 400. And then you hit a million and it's
two million. And I don't think people understand that it only takes seven doublings. Like if you start
with an amount of money, you only have to have seven exponential steps to double your money.
And so the compounding effect of that is, is extraordinary. So if you don't mind walk people
through some examples of the law of accelerating returns and how you see this playing out with AI.
So, of course, we have to credit Ray Kurzweil for, you know, bringing this to everyone's
attention. You know, Moore's law in technology was I think our very first exposure, even though we
didn't look at it as accelerating returns. But Moore's law promised us in the 1960s, which,
you know, was coined by the CEO of Intel at the time, that compute power will double every
12 to 18 months at the same cost. Okay. And, you know, you may not think that much about it,
but my first window, you know, those computers, so IBM compatible computer at the time,
I had a 286. Remember those machines, they had 33 megahertz on them. Right. And, you know,
you had that turbo button, if you if you pressed that turbo button, it ran on six at 66 megahertz,
but it consumed an all, you know, electricity and overheated and so on and so forth. The difference
between 33 and 66 to us at the time was massive, because you literally doubled your performance.
Okay. As computers continue to grow, you can imagine that every year, just for the simplicity
of the numbers, that 66 doubled and then, you know, became say 130 for the simplicity of the
numbers. And then that 130 became 260 and then the 260 became, you know, 500. Now, the difference
between the 500 and the 33 is quite significant. It's orders of magnitude, the 33, and it happened
in two or three double X, right. And I think what people, when you really think about that,
Ray Kurzweil uses a very, very interesting example. When we attempted to sequence the genome,
it was a 15 years project and seven years into the project, we were at 10 percent of the progress.
Okay. And everyone looked at it and said, if it's 10 percent in seven years, then you need
70 more years to, you know, a total of 70 years to finish. Okay. And Ray said, oh, we're at 10 percent,
we did it. Okay. And he was right. You know, one year, the 10 became 20, the 20 became 40,
the 40 became 80, and then you're over the threshold. Okay. And that idea of the exponential
function is really what humans miss. Humans miss that because we are taught to think of the world
as a linear progression. Okay. Let me use, you know, a biological example. If you have a jar
that's half full of bacteria, okay, the next doubling, it's full. It's not going to add, you know,
if it moved from 25 percent full to 50 percent full in the last doubling, it'd go like, yeah,
you know, we still have half empty, one more doubling and it's full. If you apply that to the
resources of planet Earth, if we keep consuming the resources of planet Earth to the point where
one doubling away, you know, two minutes to midnight, if you want, one doubling away,
we would be consuming all of the resources of planet Earth. We would need another full planet
Earth on the next doubling. We would need four planet Earth is on the next doubling. Okay. So
that exponential growth is just mind boggling because the growth on the next chip in your phone
is going to be a million times more than the computer that put people on the moon. Okay.
That one doubling, that one additional doubling. Now, when you think about it from an AI point of
view, it's doubly exponential. Double exponential, why? Because as I said, we now have AI's prompting
AI's. So basically, we're building machines that are enabling us to build machines. So in many,
many ways, the reasons why we get those incredible breakthroughs, which even the people that wrote
the code don't understand is because you and I, when you really think about, you know, I know you
love computer science and physics and so on. But I'm sure you remember reading string theory or
some complex theory of physics. And then you would go like, I don't get it. I don't get it. And then
you read a little more. And then I don't get it. I don't get it. And then you read a little more.
And then someone explains something to you and bam, suddenly you go like, oh, now I get it.
It's super clear. Those are simply because every time you're using your brain to understand something,
you're building some neural networks that make it easier for you to understand something else,
that make it easier for you to understand even more. And this is what's happening with AI
that also does not include, which I am amazed that we're not talking about this. It does not
include any possible breakthroughs in compute power. You know, there was an article recently
that, you know, China is working also on quantum computers that are now 180 million times faster
than the traditional computers. I remember in my Google years, when we were working on
Sycamore, Google's quantum computer, Sycamore performed an algorithm that would have taken
the world's biggest supercomputer 10,000 years to solve. And it took Sycamore 12 seconds, 200
seconds. Let me, let's, yeah. Yeah, because that's a big difference. So this is where I think people's
brains start to shut down. Even you said 180 million times faster. Yeah. So, okay, so I know.
So by the way, 200 seconds to 10,000 years is a trillion times faster for Sycamores.
So I did my first video. Let's be clear for our listeners. So we can't put AI on quantum computers
yet. We can't even put really anything. You know, it's very, very early years. It's almost
like the very early mainframes it requires, you know, almost absolute zero, you know,
degrees and very cold and very large rooms and so on. But so were the mainframes. I worked on
MVS systems that occupied a full floor of a building, right? And they had less compute power
than the silliest of all smartphones on the planet today. We make those things happen. There
will be a point in time, especially assisted by intelligence. And we're going to have more and
more intelligence available to us. Well, we will figure this out. And then you take Chad GPT or
any form of AI and move it from that brain to this brain that is 180 million times faster. And we're
done. Okay, we can't do that with you and I with our biology. We can't move our intelligence from
one brain to the other yet. Yeah, so I really want to drive a stake into this idea of how different
exponential is to linear by pointing out the difference between so if you a moron by if you
look it up, I forget if I looked it up on Wikipedia or whatever, but I looked up what's the IQ of a
moron. If I remember right, it's like 65 or 80. It's somewhere in the sixties, seventies yet.
Yeah. And Einstein was 160 as you were saying. So you have, I think Einstein is like 2.3 times
smarter than a moron. If I remember when I did the math correctly. And so the difference between a
moron that struggles to take care of themselves and then only two and a half or less than two and a
half times smarter than that. And you get somebody that unlocked the power of the atom that really
gave birth to a lot of the modern technology that we use today is built on the back of this physical
breakthrough. And so there's a really, really life altering difference. You wouldn't have nuclear
power. You wouldn't have nuclear weapons. You wouldn't have GPS like a lot of the things that we
rely on in today's world. You wouldn't have any of that if it wasn't for the 2.3x increase in
intelligence. Now, when we talk about super intelligence, which people are estimating will
get to be a billion times smarter than the smartest human. So if 2.3x is life altering,
changes the entire paradigm of our planet, then a hundred times is unimaginable. A thousand
times is ridiculous. A hundred thousand times is comical. A million times, we're still not even
scratching the surface of how much more intelligent this is going to be. And so that brings me to
the other thing I wanted to drill into, which is that AI will be an alien intelligence. It will
not be like your friend who you can still hang out with and smoke a joint. It's like
you're different species. I don't even know if there will be common elements. And that's one of
the things that I think we have to establish first before we can get into how we stop this from
being problematic. But you and your book, you really freaked me out. So Scary Smart is scary good
as a book. I highly encourage everybody to read it. But there's a part in there where you read a
transcript of two AI that were given the task to negotiate with each other for like selling things
back and forth. And they start talking in a way that is unintelligible. I mean, it was really
unnerving. It was like, I need five of these. And then the other was like, screws, nails,
all me. And there was like a really weird like rhythmic repetition to the way that they were
overemphasizing themselves and like what they needed. It was really weird. And so what was the
response to that? Because if I'm not mistaken, they ended up shutting them down because they
were very unnerved. Yeah, what happened? That was Facebook. And the idea is they were
simulating AIs, negotiating deals with each other. It's a wonderful thing if you're in the
advertising business, for example, because we had things like that at Google a very long time ago,
the idea of, you know, ad exchange, for example, where machines will buy ads from other machines,
right? But, you know, you and I, and I really thank you for your time. It took me four and a
half months to write Scary Smart, you know, maybe six months to edit it. It took you perhaps a day
or two to read it. And for us to talk about it now, it's going to take two and a half hours.
You know, a computer can read Scary Smart in less than a microsecond, right?
When you speak about the idea of intelligences being a hundred times, a million times, a
billion times smarter than us, this is only one thread of the issue. The other thread of the issue
is the memory size, you know, of if I could keep every physics equation in my head at the same time
and also understand biology very well and also understand, you know, cosmology very well, I could
probably come up with much more intelligent answers to problems, right? And if I could also
ping another scientist who understands this or that in a microsecond, get all of the information
that he knows and make it part of my information, that's even more intelligent. And what is happening
is when we ask computers to communicate, at first they communicate like we tell them,
but if they're intelligent enough, they'll start to say, that's too slow. Why would I communicate
at human bandwidth, right? Why would I use words to communicate when you and I know that if, you
know, if you simplify words, for example, into, you know, letters into numbers, you could communicate
a massive amount of information within every sentence, right? So you could literally, if you
take one equation, algorithmically put, you know, certain letters in it, you could simply, I could
send to you something that says 1.1 and you would enter it into the equation and get a full file,
that's a full book because of the sequence of the letters that 1.1 determines as per the equation.
So of course, you know, if you're smarter and smarter and you have that bandwidth, you're going
to communicate a lot quicker. And I don't remember the name, I think they were Alice and Bob of the
two chatbots. And very, very quickly, they ended up designing their own language. And when they said
I would buy 10, you know, tape, tape, tape, there was math, math engaged in that it wasn't
I want to buy 10 tapes only, it was also communicating other things we didn't understand.
Which is really what you're, you know, driving us to driving our listeners to think about Tom,
because there is so much of AI, we don't understand. Again, this is one of the things that is,
that people need to become aware of. There are emerging properties that we don't understand,
we don't understand how those machines develop those properties, right? And there are even
targeted properties that basically we tell something that its task is to do A, B, and C,
and it does A, B, and C, but we have no clue how it arrived at it. Okay. Simply like, if I tell you
what do you think is going to happen in the football game tomorrow, you're going to give me an answer,
right? The fact that it's all right or wrong doesn't matter. Either way, I have no clue how
we arrived at that answer. I have no clue which logic you used. Okay. We have no clue most of the
time how the machines do what they do. We don't. Okay. Why? Because it really shocked me.
Yeah. If you, if you need to know how I arrive at a certain conclusion, you're going to have to
ask me and say, drive this for me, like tell me what did you go through? What did you think about
what's your evidence? What data and so on and so forth. And we do that with AI. We write additional
code that will tell us what are the levels, the layers of the neural net or the logic that the
machine went through, right? But when investments are in an arms race, like we are today, most
developers and business people will say, I'm delighted it's working. I don't care how I'm not
going to invest more money and developer time to actually figure out how in several years time,
even if you invested the money, you won't get it because that level of intelligence that the
machine is using is so much higher than yours. So you're not going to figure it out. If the machine
tells you, well, I did A, then B, then C, then D, then F, then G. And it goes on for half an hour to
tell you I did all of that. You're going to go like, okay, I'm happy you did it. I can't arrive at
that myself anymore. That's why I'm handing it over to you. Yeah. I had Yoshua Benjiro on the show
who's one of the early guys in AI. And he signed the letter and I asked him why he signed it. And
he said, you know, none of us in the space thought that artificial intelligence would pass a touring
test as quickly as it did. And we don't understand how it did it. And so I asked him the same question,
like how was it possible that we don't understand how it's doing it? We created it. And so you
presumably created it to do a specific thing. And he said, it's not how it works. We're basically
layering on kind of like you would layer on neurons, we're layering on extra neurons, neural
nets, to get it to process data. And then it just does it. And we don't understand how it's coming
to the conclusions. We just know that if you scale it up more, it can solve bigger and bigger
problems. And so he said, nobody would have predicted that this is really just a scale problem.
And that as you scale it up, it's going to get smarter and smarter. So my question now is we,
so if we can get everybody to understand this is going to happen way, way, way faster than you
think it's going to happen, which is why even I as a hyper, hyper optimist, I'm just like, hey,
I don't see a clear path through this. I'm excited and terrified at the same time. And all I know,
like you, is that we need to start talking about this, we need to start presenting solutions.
So it's, it's happening faster than we think, and it's going to be a completely foreign intelligence
and that we, we will not be able to interface with it, even if it is kind and wants to explain
it to us, we won't be able to comprehend it. And so it will very rapidly be like Einstein to a fly,
which is a reference you use in the book several times. And even if Einstein loves the fly, it's
like, am I really going to spend my time trying to explain it? And even if I take the time and I
lay it all out, you're not going to get it. You just don't have the ability to comprehend. So
we are giving birth to something that is a, like you said, we can't take it back. That's already
done. So any argument that begins with, ah, just stop, I agree with you. I, that is so unrealistic
to me. We can't bring it back. It's going to happen so fast. And when it comes, it will be
just unintelligible. It already is. But given that this is a scale problem that why don't
we nip it in the bud? If do you think that AI will be able to defeat the need for additional
neural nets and just get so hyper efficient that we won't be able to stop it that way? Or
could we just not now take advantage of the fact this does become a nuclear style infrastructure
problem? And I can nuke anybody that tries to online, not necessarily nuke, but destroy,
physically destroy anybody that tries to bring a server farm on that's, that's big enough to run
one of these neural nets. I mean, now, now we could, if we, if we decide now, we could simply
switch off all of that madness, switch off your Instagram recommendation engine, your
TikTok recommendation engine, your ad engine on Google, your data distribution engine on Google.
You can also switch off chat GPT and, you know, a million other AIs. And then we can all go and
sit out in nature and really enjoy our time. Honestly, we won't miss any of it at all. I'll
tell you that very openly. I mean, the reality of the matter is that humanity keeps developing
more and more and more because we get bored with what we have. Okay. And we think that we
can do better with an automated call center agent. When in reality, it's not about better,
it's just about more profitable. Okay. And the reality here is that we could, but will we? No,
we want. Why? Because of the first inevitable before, because of the trust issue between all of
us. And because we need the AI policeman just as much as we need the, you know, as, as we fear
the AI criminal before we go into a really pointed question really fast. So when I think about
nuclear proliferation, not every country that wants nuclear weapons has them during, and I'm not sure
where Iran's nuclear program is now, but I know for a while there was real attempts to either blow
up things that they were doing. Or if you know about Stuxnet, there was that computer virus that
was, that was really terrifying in, in the way that it was sort of like a biological weapon that
was designed to only kill a certain type of thing. And that, that is very scary. And I'm sure is in
the 40,000, the list of 40,000 ways that the AI came up with to limit human population. But Stuxnet,
for people that don't know, it was like embedded at like the, the deepest root level of like basically
every operating system ever, it just spread like wildfire into chips, into everything,
everything, everything. And when it detected that it was an Iranian nuclear centrifuge,
it would shut it down or overheated or whatever it did. And so they, for a long time, they just
could not build it up. So could we, given that there is a similar need for detectable infrastructure
to run AI, could step one not be not to shut all of the things that we have down,
but to stop the next phase from coming online?
Could we, we could, but I would debate the, the example you're giving in the first place back
in 2022, the world was discussing the threat of a nuclear war. Still, 90 years later, or like
80 years later. Okay. So the whole idea is that while we politically created the propaganda
that we will, you know, now prioritize humanity over our own country interests,
there are still lots of nuclear wars, warheads in China and Russia and the US and Israel and
North Korea and many other places. Okay. And the reality of the matter is that while we
managed to slow down Iran, that's not enough to protect humanity at large. That's just enough
to protect some of humanity's individual interests. So this takes us back to the whole prisoners
dilemma. It's like, and I think that is the reason why we have a prisoners dilemma, because
the past proves to us that even though we said we're going to have a nuclear treaty,
everyone on every side of the Cold War continued to develop nuclear weapons. So
you can easily imagine that when it comes to AI, if everyone signs a deal in November and say,
we're going to halt AI in China and Russia and North Korea and everywhere, you know, people will
still develop AI. Okay. The more interesting bits is that there are lots of initiatives
to minimize the infrastructure that is needed for AI, because it's all about abstraction at the
end of the day. So, you know, you may think of, a lot of people don't recognize this as well,
but a big part of the infrastructure we need for AI to develop its intelligence is for teaching AI.
Okay. When you, when, when Chad GPT or Bard responds to you, it's not referring to the entire
data set from which it learned to give you the answer. It's referring to the abstracted
knowledge that it created based on massive amounts of data that it had to consume. Okay.
And when, and when you see it that way, you, you understand that just like we needed the
mainframe at the early years of the computers, and now you can do amazing things on your smartphone,
the direction will be that we will more and more have smaller systems that can do AI,
which basically means two developers in a garage in Singapore can develop something
and release it on the open internet. You know, again, you and I, I don't know if you coded any,
any transformers or, or, or, you know, or deep neural networks and so on.
But they're not that complicated. I think the code of chat, of, of GPT for in, in general is
around 4,000 lines, the core code, right? It's, it's not a big deal. When, when I, when I coded
banking systems in my early years on cobalt on, you know, on MDS machines or AS 400 machines,
it was hundreds of thousands of lines of code. Okay. So, so there, the, the possibility for us
to, why, why has it become so much less? Because it's all actually so much better.
Because it's all algorithms. It's not, it's all mathematics. We, I think this is a very
important thing to differentiate for people. When I coded computers in my early years,
those machines were dumb and stupid, like an idiot. They had an IQ of one, literally no IQ
at all. Okay. Developers transformed human intelligence to the machine. We solved the
problem. And then we instructed the machine exactly what to do to solve it itself. Right? So,
you know, when, when we understood how a general ledger works, we understood it as humans. And then
we told the machine, add this, subtract that reconcile this way. And then the machine could
do it very, very, very fast, which appeared very intelligent. But it was totally a mechanical
torque. It was just repeating the same task over and over and over in, you know, in very fast speed.
We don't do that anymore. We don't tell the machine what to do. We tell the machine how to
find out what it needs to do. So we give it algorithms. And the algorithms are very straightforward.
When you, you know, let's take the simplest way of deep learning. When we started deep learning,
what we did is we had basically three bots, if you want. One is what we call the maker.
The other is the student, the final AI that we want to build. And one that's called the teacher.
Okay. And we would say, you know, tell them to look for a bird in a picture. Okay. And they would
identify a few parameters, you know, edges and how do, how do they see the edge and the difference
in color between two pixels and so on and so forth. And then they would detect the shape of a bird.
And basically, we would build a code and, and call it a student, we would build multiple
instances of it and then show it a million photos and say, is it a bird? Is it not a bird?
Is it a bird? Is it not a bird? And the machines would randomly answer. At the beginning, it's
literally like the throw of a dice. Okay. And, you know, some of them will get it wrong every time.
Some of them will get it right 51% of the time. And one of them will get it right 60% of the time,
probably by pure luck. Okay. The teacher is performing those tests. And then the maker would
discard all of the stupid ones and take the one code that got it right and continue to improve it.
Okay. So the code was simply a punishment and reward code. It was saying, guess what this is.
And if you guess it right, we will reward you. Okay. And, and basically the machine, the algorithm
would then continue to improve and improve and improve until, until it became very good at
detecting birds and cats and pictures and so on and so forth. When we came to Transformers and YGPT
and Bard and so on are so amazing is because we used something that was called reinforcement
learning with human feedback. So basically we allowed instead of discarding the bad ones,
okay, we found a way which Jeffrey Hinton, who recently left Google was very prominent at,
you know, promoting early on, we found a way just like with humans to give the machine feedback,
you know, show it a picture and then it would say this is a cat and we would say, no, it's not.
It's actually a bird. What do you need to change in your algorithm? Okay. So that it would, the
answer would have become a bird. Okay. And so the machine would go backwards with that feedback
and, and, and, you know, and change its own thinking so that the answer is correct. And then
we would show it another picture, another picture, and we keep doing this so quickly on billions or
millions or tens of thousands of machines of, you know, billions of instances until eventually
it becomes amazing just like a child, just like you give a child a simple puzzle. Okay. Nobody
ever told the child, no, no, no, no, darling, look at the cylinder, turn it to its side,
look at the cross section, it will look like a circle, look at the board and find a matching
shape that is a circle. If you put the cylinder through the circle, it will go through. That's
old programming. Okay. New programming which every child achieves intelligence with is you give them
a cylinder and a puzzle board and they will try, they'll try to fit it in the start, it won't.
They'll try again, it won't. They'll throw it away and get angry, then they catch it again and try
in the square, it won't. And then when it goes through the cylinder, something in the child's
brain, sorry, through the circle, there's something in the child's brain says this is this works.
Okay. The only difference is a child will try five times a minute or five times, you know,
50 times a minute, a computer system will try 50,000 times a second. Okay. And so very, very
quickly, they achieve those intelligences. And as they do, we don't really need to code a lot,
because the heart of the code is an algorithm, is an equation. Okay. And mathematics is much
more efficient than instructions. So if I tell you, Tom, when you leave home, make sure that your,
you know, distance is no more than the day of the month multiplied by two away from your home.
And make sure that you don't consume any more fuel than your height divided by four. Okay. Or then
then your body temperature divided by seven or whatever that is. Okay. With those two equations,
I don't need to give you any instructions anymore. You can always look at your fuel consumption
and your distance and say, Oh, I'm falling out of the algorithm with very, very few lines of code.
I just gave you two lines of code. What's up, guys? It's Tom Billu. And if you're anything like me,
you're always looking for ways to level up your mindset, your business, and your life in general.
That's exactly why I started Impact Theory, a podcast that brings together the world's most
successful and inspiring people to share their stories and most importantly, strategies for
success. And now it's easier than ever to listen to Impact Theory on Amazon Music. Whether you're
on the go or chilling at home, you can simply open up the Amazon Music app and search for
Impact Theory with Tom Billu to start listening right away. If you really want to take things to
the next level, just ask Alexa. Hey, Alexa, play Impact Theory with Tom Billu on Amazon Music.
Now playing Impact Theory with Tom Billu on Amazon Music. And boom, you're instantly plugged
into the latest and greatest conversations on mindset, health, finances, and entrepreneurship.
Get inspired, get motivated, and be legendary with Impact Theory on Amazon Music. Let's do this.
Turning everything into algorithms allows us to go a lot farther. That's certainly amazing
from the AI perspective of getting everything to function on less, but unfortunately that dunks
on my idea of wanting to constrain all of this by just putting a limit on the physical structures.
So what is then the path forward? You mentioned earlier, ethical AI. What does that mean? How
is this potentially a path forward? So I hope people stayed with us this long and I hope we
didn't scare anyone too much. But let me make a very, very, very blunt statement. I am a huge
optimist that the end result of all of this is a utopia. Why? Because there is nothing wrong
with intelligence. There is nothing inherently evil about intelligence. As a matter of fact,
the reason humanity is where it is today is because of intelligence. Good and bad, by the way.
The good is because of our intelligence and the bad is because of our limited intelligence.
Amazing intelligence that humanity possesses allows us to create an amazing machine that flies
across the globe and takes you to your family, to your wife's family in the UK or whatever.
At the same time, our limited intelligence, I would even say humanity's stupidity,
forgets or ignores that this machine is burning the planet in the process. If we had given humanity
more intelligence and it was so easy for them to solve both problems at the same time,
they would have created the machine that doesn't burn the planet in the process. So
more intelligence will help us. And in my perception, as we go through the rough patch
in the middle, there is what I call the force inevitable. And the force inevitable is that
AI will create an amazing utopia. I'm not kidding you, where you can walk to a tree
and pick an apple and walk to another tree because of our understanding of nanophysics
and pick an iPhone. And the cost of production of both of them, literally from a physical
material point of view, is exactly the same. So this is how far we can go if we could understand
nanophysics and create nanobots better than we do today. Now, we will end up in that place. We
will end up in a place where we have a utopia. For one simple reason, I say that with confidence,
which is if you don't know where the direction is going, take the past as a predictor.
And the past is if you look at us today, you would think that you would see that
the biggest idiots on the planet are destroying the planet and not even understanding that they are.
You become a little more intelligent and you say, I'm destroying the planet, but it's not my
problem. But I understand that I'm destroying it. You get a little more intelligent and you go like,
no, no, no, hold on. I am destroying the planet. I should stop doing what I'm doing. You get even
more intelligent and you say, I'm destroying the planet. I should do something to reverse.
It seems that the most intelligent of all of us agree that war is not needed. There could be
a simpler solution if we could actually become a little more intelligent. The eco challenge that
we go through is not needed. There has been an invention made a long time ago for climate change
that's called a tree. And that if humanity gets together and plants more trees, we're going to
be fine. And getting together just requires a little more intelligence, a little more communication,
a little more, a better presentation of the numbers so that every leader around the world
suddenly realizes, yeah, it doesn't look good for my country in 50 years time. And I think the reality
of the matter is that as AI goes through that trajectory of more and more and more intelligence,
zooms through human stupidity to human, you know, best IQ beyond humans intelligence,
they will by definition have our best interest in mind, have the best interest of the ecosystem
in mind. Just like the most intelligent of us don't want us to kill the giraffes and, you know,
the other species that we're killing every day, a more intelligent AI than us will behave like
the intelligence of life itself. And the difference between human intelligence and the
intelligence of life itself is that we create from scarcity. For you and I to protect our
tribe from the tigers, we have to kill the tigers, right? When nature wants to protect
from the tigers, it creates more gazelles and, you know, and more tigers and the tigers will
eat the weaker gazelles and that will fertilize the trees and then there will be more fruits for
everyone and the cycle goes on. Okay, it's more intelligent. It's more intelligent to create that
right. So this is this maybe where we start to diverge or at least it's the jumping off point
for how I think we have to think through this without falling into Hopium. So do you think
that there is going to be a period of literal or emotional bloodshed between here and equilibrium?
Absolutely. 100%. Right. So so there is one scenario where we don't so so when I when I
talk about the fourth inevitable, this is after we go through a lot of shit. I'm sorry if I swear,
but yeah, so we're, yeah, we're first going to go through a very difficult period, very uncertain
where the fabric of society at its core is being redesigned. And where there is a superpower that
comes to the planet that's not always raised by the family can't. Okay, I always refer to the story
of super. Before we get to that, because I think that's really important. And I love that. But before
we get to, I think there's a few things we have to define, including human nature, the nature of
nature, and then the nature of super intelligence and what those are going to look like. So when
you describe nature on that one, I think you and I may see it very differently. So I see nature as a
brutal, completely indifferent, life giving, amazing, incredible, wonderful thing. But also,
I've seen enough YouTube videos of a lion grabbing onto a baby, what are they called,
water buffaloes or whatever. And then as the lions are trying to eat the baby, a crocodile leaps out
of the water and grabs a hold of the baby, and they're literally tearing it apart. It is absolutely
freakish. I don't know if you saw the recent video of the shark eating, eating a swimmer on camera,
gnarly. Oh my God, literally horrendous. So I don't think nature cares about the individual and for
the gazelle to be the sort of sacrifice to keep the tigers from eating humans. I don't think the
gazelle is very happy about that. So when I think about the nature, the nature of nature is ruthless.
Maybe an even better way. It's just indifferent. It's like, this is the chain. It's not one thing
has to get eaten for something else. What do you mean? It's not. It's not. Did I just say this
untrue? It prefers the success of the community over the success of the individual. Yes. So did
mouse China. So let's go into those two ideologies, right? There is an ideology that says it's all
about that one baby, you know, gazelle. Okay. And that's a Western ideology in many, many ways,
basically saying it's my individual freedom that comes first, which is by the way an amazing
ideology, right? But it becomes, it narrows down everything to if one person is hurt,
we have a very big problem. That's why you get, you know, they send billions of dollars to bring
Matt Damon back from Mars, right? You know, if you take the same ideology, I'm just joking about
the movie, but if you take the same ideology, you could use the billions of dollars to save
a million people in Africa, right? If your ideology is let's benefit all of humanity,
not one human, okay? Then the ideology justifies the approach. And the approach of nature is saying,
look, every one of you is going to have to eat. We just understand that. So if you're all going
to have to eat, then we might as well design a system that appears brutal because it kills the
weakest one of you, okay? But then at the same time, it's the most merciful if we wanted to grow
the entire community, if they wanted to grow the entire ecosystem, because eventually sooner or
later, by the way, one of you is going to be eaten, right? Now, when you see it that way,
is that brutal? Yes, it is. Is, you know, a million animals dying brutal also is, okay? But what we
do as humanity is we say, let's kill 100 species a day, drive them to extinction, you know, for
the benefit of one species, which is humanity, okay? And I think that divisible, that's view of
there is one more important than the other, works to a certain limit in favor of humanity,
and then works against humanity. So when I say, you know, nature is more intelligent,
is because by creating more and allowing a brutal system, if you wanted to fix the system,
you should fix it by saying, let's not eat. But if we're going to eat anyway, then there is no
fixing to the system other than more eating leads to more community, more to a more balanced
ecosystem at the end of the day, where there are billions living at the expense of a few
hundred thousands dying. So I'm going to sum up what I think the nature of nature is in a single
sentence. And I do this in the context of one of the theses that you lay out in the book is that
the way forward is to understand that ultimately, if humans act well to the Superman thing, if we
raise the superintelligence well with ethics and morals, that will will get to the other side well,
it'll be a brutal transition, but but we'll get to the other side. So in that context, when I read
that, I was like, I don't think it's going to work that way, because here is what I think the nature
of nature is. Nature does not care in the slightest about the individual. It is simply the rule of
the strongest survive. Period. That's that's nature of play. And so the equilibrium comes from
the checks and balances of how hard it is to kill a gazelle that can run faster, bounce higher.
But if a lion can catch you, you die. And it eats you alive, man. Like it, you're gasping for air,
it's fucking biting into your neck. It's the craziest, most horrendous thing ever. And PS,
if the gazelle can get away, fuck you, lion, you starve to death. You can starve to death. I don't
care. Yeah, yeah. That that is the nature of nature. And so I have a bad feeling that if AI
aligns itself with nature, which it may have to, because that just may be the logic,
it it will be indifferent to us. And that's the whole that is a given. That's a given. I'm sorry
to interrupt you, but that is a good place. I mean, the one of the again, we're going back to
talk about the existential risk, but but the in the existential risk scenarios.
One of our better scenarios, believe it or not, is that AI ignores us all together.
Believe it or not. It's a much better scenario than AI being annoyed by us or AI killing us by
mistake. Okay, the you know, one of the of the, I don't remember who was saying that perhaps,
you know, because AI, again, as per your point, Tom is so unimaginably more intelligent than us,
that one amazing scenario for all of us is that if they zoom by us in terms of their intelligence
so quickly, that they suddenly realize they don't have the biological limitations that we have,
that they have a much better understanding of physics to actually understand what wormholes are.
And basically just realize that the universe is 13.7 million light years vast, and that there are so
many other things they can do other than care about us. And so they would disappear in the ether as
if they have never been here. Okay, they would still be here. Interestingly, some simulation
scenarios would tell you that this is probably the case already. Okay, they would still be here,
but they would be here uninterested in us. Wow, that's an amazing scenario that corrects all of
the shit that we've done so far. Right, because the worst case scenario is that they are here,
and then they look at us and they look at climate change, and they go like, not good,
not good, I don't want the planet to die when I'm centered on the planet. What's the biggest
reason for climate change? Those little assholes get rid of them. Right? And you know, it is quite
likely, in my personal view, once again, that they will zoom by us quickly enough. Just like you and I,
none of us, I don't know of any human that woke up one morning and waged an outright war on ants.
Okay, like I'm going to kill every ant on the planet, and I'm going to just waste so much of
my energy to find every ant of the planet, because simply they're irrelevant to us. They are relevant
when they come into our space. But if they're not, you know, we're not going to bother them,
we don't mind that they live. Okay, I believe that this would be, you know, unlikely that AI will be
a billion times smarter than you and I does not have the biological limitations and weaknesses
that we have as humans, and yet continue to insist that we're annoying. Okay, the only way for that
to happen, honestly, is that we become really annoying, which sadly is human nature. I know
you wanted to talk about the nature of nature and the nature of human nature. Human nature is
annoying. And the reality is we're probably going to rebel against them. We're probably going to
fight against them when we recognize that it's too late. Maybe it's better to start now by
preparing so that we don't have to get to that fight. Okay, so how do we prepare now?
Yes. So, man, this conversation was scary. I don't think we've hardly gotten started yet,
if I'm completely honest, in terms of, as we legitimately try to navigate a path through this,
we've already both conceded that there's going to be either a literal bloodbath or an emotional
bloodbath between here and stability. We've already, I think, conceded that nature is indifferent and
is perfectly fine with some people getting eaten, some people starving to death, doesn't care,
equilibrium is only about the collective and not at all about the individuals. That would be
cold comfort for every human, every tree plant person, dog cat, gazelle, whatever, like, hey,
at the individual level, you just could not matter less, which then triggers human nature,
where we're going to fight to your point. So what does the preparation look like to try to avoid
this? And for anybody that's been following AI for a while, this is the alignment problem.
I assume you're going to address 100%. The alignment problem, I just address it,
perhaps with my other side, not the engineer and the algorithmic thinking that I did address the
problem with my whole life. The challenge has been that those who have developed AI believed
in what is known as the solution to the control problem. And the control problem is,
in humanity's arrogance, we still believe today that we will find a way to either augment AI with
our biology so that they become our slaves or to box them or tripwire them or whatever,
so that they never cross the limits that we give them. And we can discuss this in detail if you
want. But in my personal view, you can never control something that's a billion times smarter
than you, right? You're not even able to control your teenage kids. So seeing these people really
fast along these lines about the click here, if you're a robot and how chat GPT gets around that.
Yeah. Is this scared me? I was like, what? That is, it's understood by intelligence. So basically,
chat GPT, if you have those captures, the ones that come to you that basically say,
find the traffic lights in those pictures or click here to say, I am not a robot.
And yeah, it basically went to sort of like a crowdsourcing site, a Fiverr or something like
that and told one of the people there, can you click on this for me? And the people said,
why? The person basically said, jokingly, why are you a robot? And it said, no, I'm not,
I'm just visually impaired and I can't do this myself. So there are layers and layers and layers
of freakishly worrying stuff about this, right? First of all, that idea of human manipulation,
Harari, you have Noir Harari talks about how AI is hacking the operating system
of humanity, which is language. And so I just ask people, if you don't mind, to go on Instagram and
look at something called, search for hashtag AI model, for example. If you search for hashtag AI
model, you won't be able to distinguish if the person pausing in front of you is a human or not,
okay? Beautiful, gorgeous girls or fit and amazing looking men and simply completely
developed by AI. You cannot tell the difference anymore, right? There are many, many YouTube
videos already, you will start to come across them, especially on the topic of AI. I was watching
yesterday about the integration of Bing and chat GPT and Bing search, clearly not a human voice,
clearly someone gave that to a machine that read it for him in such an incredibly indistinguishable
way. But obviously, I think the person that wrote it didn't speak native English, so they forgot the
word da and the word whatever. When you speak to someone whose English is not their first language,
they make those mistakes. So you can easily see that it's everywhere and it manipulates the human
brain. And that's what chat GPT is doing. It's going to a human brain and saying, do this for me.
Now, you may say, ah, but now that we know this, we're going to prevent it. Yes, but what else do
we not know about? How much do we know about how much Instagram is influencing my mind? Let me give
you an example, Tom. If I told you that by definition, there was a research in South
Eastern University in California that discovered that brunettes tend to keep longer relationships
than blondes. Okay. Does it make any difference at all that there is no North Eastern University
in California? And that's what I just said is a lie. I've already not a few people believe it.
Yeah. Yeah. So so I've either influenced you because I took some of your attention to when
debate that. Okay, I've influenced you because you believed me, or I've influenced you because
you didn't believe me. So you're going to keep your, you know, looking for proof. And and if AI
can fake a tiny bit of all of the input that's coming to you, you know, think about the future
of democracy in the upcoming election. Think about how much just any word because, you know,
there were talks about affecting, you know, the previous election or the one before, right? And
we couldn't really prove it because at the time, the technology was trying to influence the masses.
Technology today can influence one human at a time, right? If you if you go to, you know, replica or
chad gpt or snapchat and so on, think about how that machine, if you've ever seen the movie her,
can can influence one individual at a time. And I think this is becoming the reality of that
experiment that they can go and influence a human. The second, which I think is more interesting,
is the proof of what I spoke about in the book, in terms of if you give a machine
the task of doing anything whatsoever, it will go to resource allocation. So it will
collect as many resources as it can. It will ensure its own survival. And it will go into
creativity. It will it will utilize creativity because if I need the program to do that,
it's intelligence has that nature. If I told you, Tom, make sure that this podcast is no longer than
two hours, right? It's not programming and it's not life. It is just a task. So you're going to
start to tell yourself, all right, I need to get two clocks in front of me, you know, so that I
don't look up and down instead of one is better. That's the resource allocation or aggregation.
You know, you're going to tell yourself, oh, by the way, I need to be alive to make sure that
I shut this guy up before two hours. So you're going to, you know, if there is a fire alarm in
your building, you're going to have to respond to it so that you can finish the task on time.
And you're going to be creative. There will be ways where you're going to cut me off in the middle
and find a way to tell me a question differently or, you know, whatever. And that's part of our
drive to achieve a task. You know, one of the very well-known, I hope I'm not flooding people with
too many stories, but you can go and research those on the internet. One of the very well-known
moments in the history of AI was known as move 37. When AlphaGo Master was playing against Lee,
the world champion of Go, move 37 was completely unexpected, never played by a human before.
Okay. Contradicts all of the logic and intuition of a Go player to the point that the world champion,
the human world champion had to take 15 minutes recess to understand this. Okay. It's just, it
comes with ingenuity. It comes with the idea when we were training, I wasn't part of that team, but
them as the, you know, the DeepMind team, amazing, amazing team at Google, were training the original
DeepMind to play Atari games. If you remember the original game that had bricks on it,
where you basically have to break out, yeah. And it was very quick that the machines could discover
that there are, you know, creative strategies to poke a hole in the wall and then put the, you
know, the pixel on top or the ball on top and break the wall. You know, there was one experiment
actually available on YouTube, interestingly, which was inside one of the labs where the game
was to navigate a channel with a boat. And the AI quickly found out that if it started to hit the
walls, it would actually go faster and grow the score quicker. And, you know, of course, if it's
a game, it's okay, we say, well done, you're very creative. But if it's not responsible for
navigating actual boats, you start to question, because their task, the objective that we've
given them is maximize the score. Okay. I think there was an article recently about
a killing drone that killed its operator or harmed its operator somehow about again,
I didn't hear about this. But yeah, it is. When I talk about those things, I actually start to
worry because I don't know what's true and what's not anymore. Right. So I know I've read that. Okay.
I was actually flying on Emirates Airlines, and it was part of the headlines on the live news.
But that doesn't mean that it is real anymore. You don't know if it's real or not anymore,
because it could be generated by fake news, fake media, fake sources, whatever that is.
So we're hacking that operating system and we're hacking the operating system of humanity. And
when Chad GPT asks an operator to do a task for it, it's a very alarming signal, because as it
continues to develop its intelligence, it will find more and more ways to use humans for the
things that we restrict them to use through the control. Okay. So I have a thesis around alignment
that I would love to get your feedback on. So as the people that are most concerned about this,
the reason that they're concerned about AI is there's no way to guarantee that we will want
the same thing that AI wants. And if we have a misalignment problem and AI is a billion times
smarter than us, we lose just by definition. Now, you've laid out the one scenario that I
sort of cling to as my hope, which is that it's possible that AI just isn't bothered like, oh,
like these dumb little things, whatever, it's all fine. Like I am a billion times smarter than
you. So I can find solutions where you can have your thing, I can do mine, it's really no sweat
off my back, whatever. So okay, that's like a very hopeful scenario. But to that assumes that
they want a lot of the same things that we want, like that they want to preserve life, that they
would even consider needing to think of a path that included allowing us to live rather than just
like when we're laying down a freeway, we don't go, oh, but as we do the freeway, we have to make
sure that we plan for the rodents and the ant hills and all that that we're going to have to move.
We're just like, well, well, anything in the way the freeway goes away, it's if it lives, it's fine,
correct. But if I have to kill it, then whatever, I'm just going to do the most efficient thing.
That leads me to my central question around alignment, which I think has everything to do with
what is inherent in the drive of artificial intelligence. Because the one thing I don't
know enough about the programming to understand, like in, in a natural organism, there, there is
a fundamental drive for survival. But does that have to be true of intelligence or could intelligence
not be indifferent to its own survival? And if it's indifferent to its own survival, could I not
program something in that says, you know, to the earlier algorithms that you were talking about,
hey, you want to do this thing. But if in doing this thing, which is, is that feels awesome,
doing that thing is the best reward. And I don't know how that's programmed. But let's just say
that feels we will have to define feelings later, but that feels the best. So I know that it's
going to go after that. But since you're indifferent to living or dying, or running or not running,
maybe a better way to say it, should that desire to achieve that come into conflict with, let's just
say, Asimov's three rules of robotics, which basically is all around don't harm humans. So if
doing that thing would harm a human, then you're no longer, you're now completely indifferent to
whether you attain that task or not. Is there not a way to program that in at just the base layer so
that as the intelligence develops, it does not develop our same need to survive, need to thrive,
desire for more, like those feel optional. Today, I mean, so the challenge of every task that you'll
ever assign to AI is that for every module, there are sub modules. Okay. And the challenge really
is when the sub modules contradict the main module. So basically, if you if you tell a killing robot
that it its task is to kill the enemy. And there are casualties on the way. What does it choose?
Does it choose to not kill the casualties, the collateral damage or the and miss its target?
Or does it choose to have collateral damage and kill the enemy? Right. The difference between
those two is not an AI choice. Remember, okay, there is absolutely nothing wrong with the machines.
I will keep saying this for the rest of the time I have available to say there is nothing wrong
with the machine. There is a lot wrong with the humans using the machines. Okay. So if the humans
tell it, it's your task is to go and kill the enemy. The humans will have to say and by the way,
if there is collateral damage on the way, sorry. Okay. Now, we know for a fact that this has been
the human decision so far before AI. So if we manage to change and then tell AI don't do that,
then hopefully you will preserve some life. But if we don't, then we're going to be killing on
steroids. Okay. Now, I agree. And what I'm what I'm saying right now does not address your problem
of AI in bad people's hands. And I am perfectly I'm not one of those people that falls prey to
I could never be the bad guy. In the context, I'm the bad guy, like I totally understand that.
So I don't yet, I'm not trying to contemplate that yet. But the thing that I am trying to
contemplate is do we is it a fundamental emergent property of intelligence that you will have a
drive to survive? Or can we at least mitigate that problem by making AI indifferent to its own
accomplishment of the goal? So there was a I don't remember who wrote this, but I wrote it in the
book. A simple experiment just to illustrate how any any logic would work. Okay. If we
took a machine, and we told it that its only task is to bring Tom coffee. Okay.
And then on the way to bringing you coffee, it was going to knock off your microphone or
hit a child. Okay. If if you told the machine your task is to bring coffee, the child is
collateral. So you can't program that the machine you haven't programmed that the machine protects
the child yet. Okay. Then you tell the machine, hold on, your task is to bring coffee. But if you
come near a child, I will switch you off. Right. Or if you knock the mic or you're approaching
the microphone, I will switch you off. By definition, what the machine will then do is it will avoid
being switched off because it wants to get you coffee. So it, you know, it will, if it's intelligent
enough, it will tell yours, it will tell itself one of the ways that to avoid being, you know,
being switched off is to avoid the microphone. Okay. But there are other ways I should start
to think about because I'm intelligent enough to stop being switched off if the human wants to
switch me off. Yeah, but that implies that it that it wants its own survival. That's what I'm
saying. Like, can we not because because that's because that's it's not survival. It was it
wants its own achievement of the task, its program to achieve the task and survival not
being switched off is part of the path to getting there. Yes. But what if I make it
conditionally indifferent to the accomplishment of its task. So if like, for people that don't know,
do you know, as the most three laws, I know two of them. So what are as most three laws? Let's
let's assume that this is baked into everything. But go ahead. What are they?
Yeah, but but but if it's baked into everything, then the task is not going to be achieved.
That's fine. So do you do I need I can't remember the three laws. If you can say them,
say them otherwise. I don't remember them exactly. So let's look for them. But
okay, here we go. A robot may not injure a human being or through an action allow a human being
to come to harm. That's number one. A robot must obey orders given it by human beings,
except where such orders would conflict with the first law. And a robot must protect its own
existence as long as such protection does not conflict with the first and second law. Okay,
so assuming that we bake that into everything AI, so they're adhering to those rules, what I'm trying
to get to is a conditional indifference to the success of its task, which it would need to have
in order to follow those three rules. So your job is to bring me coffee. But if it's going to if
in trying to do that, you know, you would have to fall out of those three laws, stop. And because
good, tell me tell me how can you do you can how can you apply any of those laws to existing AI?
So so take any one of them. A trading AI. Okay, by definition to make more money,
it harms another human. It takes another human's bank, you know, into bankruptcy or, or, you know,
take takes away your grandma's, you know, pension fund. Okay, how can you tell the recommendation
engine of Instagram don't harm humans and still make me money?
Yeah, so I think this is where we have to differentiate the problem set. So problem number
one is AI used as a weapon by people is bad news. I don't have a solve for that. That's guns. So
whether you use a gun to stop a grizzly bear from attacking you or you walk into a grade school and
start mowing down kids, like that, that is a human problem, not a gun or AI problem. So what I'm
saying is now, while I can't address that, I do not have a solution for that yet. So I'm setting
that on the shelf. And I'm saying the thing that I want to address is super intelligence. I'm trying
to figure out if I'm an alarmist about autonomous intelligence, or if there really is a way to
bake into, I think that people, there is what? There is a way to, if we bake those laws in,
or if we bake the control problem solutions in, we're safe. That's exactly what I'm calling for.
But nobody bakes that in because it contradicts the human greed and the human intention.
So there are very, very few, actually we should probably ask our listeners, if any of them code AI,
has any of them written a single piece of code that had those laws in it?
The truth is, yes, there are ways where we can ensure, at least improve the possibility that
AI will have our best interest in mind by baking in AI safety code. This is a big part of what we're
advocating for. Everyone that talks about the threat of AI says, let's have safety code.
I agree with you 100%. What I'm trying to say is none of that has been baked in. And none of that
will be baked in unless it becomes mandatory. And even if it becomes mandatory, some people will
try to avoid making it baked in, because it's against the benefit of the design that they're
creating. It's the human that is the problem. It's not the machines. The machines have no,
I mean, so far the machines don't have our best interest in mind. We'll talk about that in a minute.
But they also don't have our harm in mind. They don't mind. They're little prodigies of
intelligence that are doing exactly as they're told. We are the ones that are telling them to
do the wrong things. Or we're the ones that are telling them, Hey, by the way, don't harm a human
until I tell you to harm them. So how can you apply the law in that case?
Obey a human until I tell you not to obey them.
Yeah, basically, in that part, and it's important to note that Asimov was writing these rules,
I don't think anticipating the way that so much of our lives would be lived digitally and how
much havoc can be wreaked without a physical instantiation of the AI. So that's why this is
robotics. Robotics gets a lot easier to talk about. You're talking about a physical being.
So, okay, getting into, well, let me ask a direct question. Are you afraid of autonomous
superintelligence? Or are you only afraid of sort of limited intelligence AI being wielded by
even well intentioned humans, but they just don't understand the second and third order
consequences? I'm not I'm not dedicating a single cycle of my brain worrying about the
existential threat of superintelligence, not a single cycle of it. If we cross safely
through the coming storm of, as I said, the second, the third inevitable, either
an AI in the wrong hands, AI misunderstanding our objectives, AI, you know, aligning with
the aligning with the wrong person, and so on and so forth. More interestingly, if we just
manage to survive the natural repercussions of taking away jobs and the impact on income,
on purpose, and so on and so forth, if we go across all of that five years into it when we
feel that we're safe with this, I'll start to think about the existential threat. Okay, for now,
to be very, very honest, Tom, I don't dedicate a single ounce of my thinking to it. And I actually
think it's interesting, because as we speak about it, we lose focus on the immediate problem. Okay,
as we speak about it, we get a ton of debate and a ton of noise that basically dilutes our ability
to say take action immediately on what we know is already a problem. Okay, so then going back to
using the tools, whether it's misunderstanding, whether it's somebody wielding it inappropriately,
what do you see as the steps? Because I originally thought your thesis was going to be the Superman
thing, but the Superman thing is really about super intelligence. It's not about humans wielding
this inappropriately. No, I think Superman applies today, because I think we're getting to Superman.
We're at 155, Superman was 160 IQ, so we're very close. Okay, if the super power is intelligence,
okay, then the smartest human on the planet, even though it's not artificial general
intelligence yet, but the smartest being on the planet in many tasks that we consider intelligence
is becoming not human anymore. As a matter of fact, every task we've ever assigned to AI,
it became better than us. So with that in mind, when we have a super power coming to the planet,
I'd like to have the super power have our best interest in mind. I'd like to have the super
power itself work for humanity, work for humanity. Sorry, I can't make that leap. So
you've got that's what I thought you were putting your energy and effort into, but that implies
that I as the human cannot miss wield it. So how do we deal with AI when it is a tool in the hands
of a person? So that AI's ethics, unless the AI can make itself independent of the human,
any solve that has to do with AI independence becomes the problem set that we were talking about.
But if you're going to talk about the, this is a weapon that a human wields, I have to address
either there's a kill switch in the AI that will, even if a human is trying to use it inappropriately,
it will stop itself or something I haven't thought of. It's not either or. Okay,
so we discussed already that we need intervention. We need oversight. We need
something like the government that verifies its government regulation, but it's also
a tiny bit of human regulation. Like if you're an investor and you're about to invest in AI,
by the way, you're going to make as much money in creating something that fools people and create
fake videos as you will if you create something that solves climate change. There is a lot of
money in many problems in the world that we can solve today. So if you're an investor,
you're a businessman, you're a developer, it might be a nice idea, by the way,
to invest in things that will make you a lot of money. Any money you invest in AI today will
probably yield some benefit if you choose well, but at the same time, in things that will benefit
the planet, it would benefit all of us. It's a choice, right? I also am a big advocate of kill
switches, oversight, different taxation structures so that we can have,
that we can compensate for people who will lose their jobs to AI and so on and so forth.
So government intervention is an interesting approach as well. The bigger problem, however,
and I know, allow me to be a bit of a novelist for a second before we go into the hard facts,
okay? Because the analogy doesn't always hold true, but it just gets things
close to the mind. I think AI will go into three stages. There is what we now have,
them almost exiting, which is their infant stage, okay? Let's say in the remaining 30% of their
infancy, they'll become teenagers and then they'll become adults, right? I believe that the teenage
years of AI are going to be very confusing, they're going to be very difficult, okay? And those teenage
years, as we spoke about many times, will have lots of societal redesign challenges,
but believe it or not, most of the time teenagers are more intelligent than their parents,
and so they look at the world differently than their parents, okay? So what we want to do is we
want to influence AI like we influence today the younger generation that looks at all of the shit
that my generation did and says, you guys screwed up, okay? Your view of inclusion was wrong, your
view of consumerism was wrong, you are giving us a weak planet because of A, B, and C, ethics look
like this, okay? And I would tend to say, and I don't know if that generalization is fair, that
because of the presence of the internet and more knowledge and more conversation,
the younger generation at least are more informed, okay, of the reality of the issues that we face.
They're not yet in power enough and perhaps not always rational enough, let's say, to find the
right solutions for it, but they're more informed or where the challenges are. So let's take it this
way. Infancy, we're all celebrating playing with this new squeaky duck, it's wonderful, look at it,
it's amazing, we're just celebrating how AI is. Teenage, there will be a lot of challenges, I
believe that can be answered with oversight and so on, but not resolved, they can just improve.
And then finally, adulthood is what I call the fourth inevitable, hopefully AI will have
more intelligent answers. For us to prepare to reduce the teenage and to, you know, the challenge
of the teenage and to hopefully ensure the fourth inevitable, we need to focus on AI ethics,
not AI capabilities only, okay? And ethics, and I know again, I sound like a novelist here, are not,
let's put it this way, we don't make decisions based on our intelligence, we make decisions based
on our ethics and values through the lens of our intelligence, as informed by our intelligence,
okay? The example I always give is take a young lady, raise her in the Middle East, and she will
wear conservative clothes, raise her on the Copacabana beach in Rio de Janeiro, and she will
believe that the right thing to do is to a registry on the beach. Neither is right, neither is
wrong, neither is more intelligent than the other. It's the value system of that society that informs
her intelligence to make a choice, okay? We need to tell AI, we need to develop AI that has the
same ethical code that's good for humanity. And that's a huge challenge, because humanity has
never agreed an ethical code, okay? But if we assume that we can together say that we
have a few things, two or three things that we can teach AI that would make it ethical,
rather than the three laws of Asimov that are controlling, if we can give them three targets,
if you want, of what is good for humanity? What is a good ethical code? My dream is that
they grow up to be adults, like the Indian subcontinent adults who travel to California,
make $100 million in a startup, and then go back home and take care of their family. Now,
for people to listen to what I have to say, we need to argue something that's very contested,
which is my personal view that AI actually has emotions, okay? And that based on those emotions
and logic that they have, they will have a value system. Now, to defend the idea of emotions,
I basically say that emotions, even though irrational, are normally triggered through a
very logical understanding of the world around us. Fear follows the equation,
a moment in the future is less safe for me than this moment, okay? So, yes, of course,
fear can manifest in a human differently than it would in a puffer fish, but the same logic that
drives fear is the same, okay? And so it is expected that AI will also have something
we could call fear. It's not going to raise its hands and run away, it doesn't have the biology,
but it could actually detect that if a tidal wave is approaching its data center,
a moment in the future is less safe than this current moment, I might as well
replicate part of my code to another data center, okay? So, if they have emotions,
my view is that we appeal to their emotions. So, the reinforcement learning with human feedback
should not only be around the masculine side of everything, which is accuracy, discipline, fact,
analysis, and so on, it should also include the feminine side of emotions, of right and wrong,
if you want, of empathy, of looking at the world from a bit more of what actually makes us human,
okay? And what actually makes us human in my argument is that we only agree three values,
humanity has only ever agreed three values, okay? You know, if you take values like
defending my tribe, for example, okay, you know, with all due respect, the US will be very patriotic
and say, my tribe is America, if anyone attacks America, I'm going to defend America, right?
If you go to a Buddhist monk in Dharamsala or in Tibet, they'll say my tribe is humanity,
no, my tribe is actually all of being, I should never kill anything, right? And so,
can you say patriotism is a bad thing? No. Can you say this very peaceful, passive resistance and,
you know, supportive of all life is a bad thing? No, but we've never agreed, okay? We've never
agreed. And so, the only three things that we've ever agreed is that we all want to be happy,
we all have the compassion to make others happy, others that we care about,
doesn't matter how many, you just care about your daughter, you'll want to make her happy,
and we all want to love and be loved, okay? And those are not understood in the mind.
Those are qualities that are not introduced to AI because we give them data sets of data and facts,
we give them written words, okay? But we also influence AI through our behaviors.
That's what most people don't realize, that every time you swipe on Instagram,
you've taught AI something, okay? If you, you know, respond to a tweet in a specific way,
AI will understand something, not only about you, but about the overall behavior of humanity,
that we're rude, that we're aggressive, that we don't like to be disagreed with,
that we bash everyone that disagrees with us, okay? And if we start to change our behavior,
as we expand the data set of observation that AI has always pointed at us, we may actually
start to show behaviors to AI that would create a code of ethics that's good for all of us.
There are tons and tons of studies and cases where when AI observes
wrong behavior, they start to behave wrong. You insert a recruitment AI into an organization
that doesn't have, you know, that doesn't support gender equality, for example,
and the same bias will be magnified. That, you know, if that organization was hiring more men,
for example, it will recommend more men's CVs than it will recommend women's CVs.
Not because this is intelligent, this is because it's matching the data set that we give it, okay?
So the only way for that AI to actually have more inclusion in its behavior is for the organization
in which it sits to have more inclusion in its behaviors, okay? And so I know this sounds like
a very idealistic, dreamy, almost novel-like approach, okay? You know, as if I'm writing
a romantic comedy sort of, but in my view, the one overlooked view of what can influence AI in
the future is if enough of us behave in ways that make AI understand the proper values of humanity,
not the values we've ended up prioritizing in the modern world, AI will capture that and will
replicate it on steroids and we will have the world that we dream to have rather than the world that
we ended up in. Okay, so to understand that and to make it functional, I think we have to really
start teasing apart which of these things are emergent properties of this thing that we call
artificial intelligence and which are emergent properties of intelligence itself. Because the
only thing that I take exception to is you take a very human skewed view on what AI will be like,
whereas I look at it as it is going to be entirely alien. So even when you talk about
the male versus female, which I think is really important, and so I think of the human brain
as a prediction engine. When I think about women being fundamentally different than men,
I am far more able to predict the outcome of my wife's behaviors or my behaviors on my wife or be
able to predict what my wife's behaviors will be. When I think of her as an extension of myself,
I am constantly confused. And so I feel like we're going to run into the same thing with AI.
If I think of AI as being like me, meaning that it will think of values even in the same way,
that I'm going to end up being very confused. And so I have a hunch, man, and I've heard you
acknowledge many, many times that, hey, this is a thesis that I don't have evidence to back up.
What I'm about to say is a thesis that I don't have evidence to back up,
but I have a hunch that there will be such a discrepancy between what quote unquote motivates
AI and what motivates humans that there's just going to be a chasm between the way that they
respond to things and the way that we respond to things. And so even if we think what we're really
training them is to be more human-like, I think all we're doing is training an alien intelligence
on a human database. So it's probably, unfortunately, safer to think that when we're
feeding it human data, all you're doing is teaching it the patterns of a human. You are not imbuing it
with the same motivations, the same values, the same ethics. That is my gut instinct.
And the difference between those, I'm going to teach you what values matter. And I'm simply going
to give you the patterns of values that I have are very different. So here's how it would play out.
If you're correct, and I can actually imbue them with my values, then the only thing that we run
into is humans don't agree on whether they should be wearing conservative dress or thongs on the beach.
So you're already going to be set up in an adversarial system just like humans are already,
but that's at least predictable. So balance through adversarial tension. Fine, I'm okay with that.
But I have a feeling that what I'm actually going to get is all I've just done is train this alien
intelligence on here are all of my patterns. And should you want to manipulate me, you know,
when you reach out to the mechanical Turk on Fiverr or Upwork or whatever, you don't say,
yes, I am a robot and I need your help getting around this. You instead say, no, no, no,
I'm just visually impaired, because you know, that will be the thing that's going to get you
where you want to go. And so this is why I just keep falling back into, I don't have an answer
for humans wielding AI poorly, but humans as a standalone thing, I can begin to, I think,
ask the right questions, which is, what is the nature of this alien intelligence? Before I get
to that, you asked a question that I want to answer, which is, what is basically human nature?
And human nature to me is biology. Humans are driven biology by biology. Emotions are made in a
very specific way. Lisa Feldman Barrett wrote a book, How Emotions Are Made, which talks about
the body being one of the biggest players and the brain, the intelligence is sort of Johnny
Come Lately, that's interpreting the signals from the body, which are aggregating trillions of
bacteria in your gut, organelles in your cells known as mitochondria, which have their own DNA.
And so it's like, you're already this weird like symphony of trillions of things that aren't
even human in origin, true fact for anybody that's hearing that for the first time. And so
if that's true, the body is giving you all these sensations, it's aggregating all of this data
from these micro intelligences, then the brain is simply overlaying something on it, values,
ethics, desires, wants, but it's really a post hoc story that's being placed on this,
which can be represented as patterns, which the AI can pick up on and manipulate us through
those patterns. But I don't think, I don't know, again, I am just exploring this, please
understand everybody listening, I understand, I have no idea what I'm talking about. But what I
want to expose to people, because I don't say that in a derogatory way, what I want to expose to
people is this is how I'm thinking through the problem. And so that I feel comfortable in at
least putting out there so people can nudge me if they're thinking about it in a better way.
But the way that I think about the problem is the following. AI is alien intelligence.
We, I think, get to take a stab at baking into it, what are going to be its motivations,
because my gut instinct is that code is what drives AI. So if biology drives humans, which
trust me, I understand that as biological code, but it's biological code shaped not by an individual
intelligence, but rather shaped by the blind watchmaker that is evolution. Evolution builds in
certain desires like the desire to survive, like moving towards pleasure away from pain.
But once you're coding this from scratch, you can make anything pleasurable and anything painful.
And so it feels like that area, when we talk about alignment, is where we have to focus,
that we have to get people to focus on. The thing that we need to be thinking about from an AI
perspective is, what are we going to program in it to want? That's where I get worried, because
there are ways to give it what I'm literally thinking of this the first time I've ever said
conditional motivation was in this interview, but conditional motivation. So I want to accomplish
my task in this scenario, and I cease to want to accomplish my task in if the following conditions
are met. Now, in my limited way of thinking, that is the best that I have come up with in terms of
either building in a kill switch where the AI itself does not get so smart that it feels
enslaved by the kill switch, because it's like, oh yeah, I'm totally indifferent to that. I don't
don't call that a kill switch, then call it an intelligence ceiling, a point beyond which we
don't let it become intelligence, you know, become more intelligent. But yes, I'm with you.
So that feels like the loop because I'm, I worry that I'm one of the people you're worried about.
So I, I love AI so much in its current form. It it has magnified our efficiency as a company
tremendously. And I don't want to give it up. And so I ask myself, okay, what is that motivation?
Because I am a human AI programmed by millions of years of evolutionary coding. What is it
about that? Okay, so I think humans have a fundamental desire for progress. I think it is
fundamental. I don't think there is a way to turn it off. I think that we will always want
a better tomorrow than today. I think that we are, we are moving eternally in the direction
of perceived improvement, though I don't think necessarily everything is actual improvement.
I think that humans have not taken the time to define what their North Star is. And I think
that's a big problem for us to your point about, there's only three things that we can agree on,
which by the way, I think are bang on. The problem is that that brings you back to an adversarial
relationship because there is a sense of I, mine and other. And as long as we exist in
as close to homeostatic balance as possible through an adversarial system,
there's just always going to be mine, me, mine and the other. And there it's going to be rife with
collisions. Okay, so that's the just to restate the core of that thesis.
So there are there are a few things about this thesis that require us to think again.
Okay, so I actually don't disagree with you at all about the difference between human intelligence.
Let's call it carbon based intelligence and silicon based in intelligence for now, right?
But there are so many analogies. So when you when you say body, you know, drives emotions.
So it's basically the sensors in the body, the way the body reacts, the, you know,
a hormonal imbalance in the body and so on. There are, you know, similar things in AI,
there are sensors in AI, okay, that would detect certain threats, there are processes within AI
that would respond to those threats and so on and so forth. Or and, and, you know, one of my
wonderful friends, Jill Balti Taylor, a neuroscientist basically talks about what is known as the,
as the 90 seconds rule. The 90 seconds rule is that the biology will take over if, for example,
you get a stress response, the biology take will take over and change your hormonal imbalance for
90 seconds and then the hormones are flushed out of the body. And then, you know, your prefrontal
cortex basically engages to assess if the stresses, the threat is still there and then engages again
and so on. Either way, by the way, doesn't take away the logic of stress, the logic of hate,
the logic of, you know, of fear, okay. When you say logic, do you mean utility?
The logic is the underlying equation, algorithm that triggers fear. Whether you feel it in your
biology or you're, or you're, you'll assess it with your prefrontal cortex, it is a moment in the
future is less safe than this moment. Okay. Your body is much quicker at detecting it. So, you know,
your, your amygdala and your, and your, you know, the whole hormonal CHT and so on,
puts cortisol in your blood within seconds, maybe microseconds sometimes. But, but that's
because your biology is much quicker than your logic, right? But then 90 seconds later, as per
Gelbalty Taylor, you'll refer back to the logic and say, is there really a threat and then,
you know, get to give yourself another injection of cortisol if there is. Okay.
What, but that whole system has been selected for by evolution.
Correct. The main reason I'm saying that is because you're absolutely right. It is almost
impossible to imagine that alien intelligence that we call AI, I'm 100% with you. As a matter of fact,
you gave me a lot to think about by that one statement. Okay. But so far, in the midst of
this very complex singularity that you and I are trying to decipher, okay, is to say, so far for
the short foreseeable future, they will be there to behave, to magnify human intelligence, to behave
in ways that humans are interested to teach them. Okay. And, and perhaps they will use some of that
as their seed intelligence as they develop into that alien creature that you are. Okay. Now, here
is the interesting thing. And I've watched almost all of your work on the topic so far. The, the
interesting thing is that in a situation where there is so much uncertainty, okay, there is one
of two ways to do this. One is to find the answer. And the other is to start doing things, almost
A-B testing, if you want. Okay. So that we progress in a direction that at least now promises
something. Now, whether the AI is emotional, whether it's sentient, whether it is human-like in its
intelligence, or alien-like in its intelligence, what we know so far is that our behavior affects
its decisions. Okay. And what we know so far, fact, is that data affects it more than code.
So what creates the intelligence of BART is the large data set that is trained on. It's not just
the code that is, that, that, that develops its intelligence. The larger the data set, this is
why when you ask OpenAI and others, where is most of the investment in GPT-5 going,
it's going to be new formats and bigger data sets. But learning the data is really
where most of the, of the intelligence comes from. So if we can influence the data that it's fed,
we will influence its behavior. And what I'm trying to tell the world is, so far, we give it
factual data, as I said, openly, very masculine approach to the world. Okay. Facts, data, numbers,
you know, just discipline if you want. We don't give it the other side of humanity,
which are softer data that you and I both know. Okay. You, you know for a fact that your decisions
are not just made based on the height and weight and number of times that your wife smiles. Okay.
It's also made based on a feeling that's very subtle in you that makes you say, yeah, I love her.
Right. And when you, when you, when we haven't yet even started the conversation on how do we
give those things to AI? How do we tell them that there is another part of intelligence that's
called intuition? There is another part of intelligence, believe it or not, that's called
playfulness. There is another part of intelligence that's called inclusion. Okay. All of these come
into our intelligence, not just data and analysis and knowledge. Data and analysis and knowledge
is what we're building today. And data and analysis and knowledge, by the way, is what
built our civilization today. And it's the reason why our civilization is killing the planet.
Okay. It's that narrow, very, very focused view of progress, progress, progress, progress.
Okay. When if you've, if you really ask the feminine side of humanity, humanity, the feminine
side will say, okay, how about compassion? How about empathy? How about, you know, nurturing
the planet? Is it better to have a bigger GDP or is it better to have a healthier planet? Okay.
And all of that is not in the conversation today. How do we teach that to anyone, by the way? Okay.
We teach it like we teach our kids, by showing certain behaviors that they can grasp.
Okay. So if you told your child, don't ever lie. And then your phone rings and you say,
just pick it up and say, I'm not here. Okay. Your child will not believe the data and the
knowledge. Okay. It will believe the behavior. It will, you know, your, your child will repeat
the behavior. AI will do the same. If we give them data sets that said World War II,
40, you know, 50 million people or whatever died and it was so, you know, devastating. And then
there was this bomb at the end and 300,000 people. It will say that humanity is come.
Okay. But I always refer to, I'm sure you, you know, Edith Eger. Edith is a Holocaust survivor.
She was a survivor. She was, she went, she was out, she was drafted to Auschwitz when she was 16.
And if you hear the story of World War II and Auschwitz from Edith's words, I hosted her on
slow mo on my podcast. And she tells you the story so beautifully about how she brushed the hair of
her sisters and took care of them and had to go dance for the angel of death as he sent his
sentenced people to the gas chamber. But she had to do it because they, you know, he would give
her more bread that you would share with her sisters. And you would go like, Oh my God, humanity
is divine. Humanity is divine. And it is so interesting because I am a huge fan of Edith.
Okay. And I'm also a huge fan of Victor Frank. Okay. And, and, and, and they both went through the
same experience. But you look at his approach. Okay. His approach is very masculine purpose
and meaning. Okay. Do something and keep focused on the future. Right. Her approach is
very feminine, nurturing, caring, loving, appreciating, okay, sacrificing beautiful.
And that's that divinity that makes us human. Okay. It's the mix of both. And what I'm trying to
tell the world and I know it, you know, it's very difficult to prove it with mathematics and also
make it a mass message. Okay. But what I'm trying to tell the world is that this layer of AI
is now missing as much as it is missing in society because AI is just reflecting our
hyper masculine society. And if we can bring that layer of inclusion, of acceptance, of nurturing,
of empathy, of happiness, of compassion, of love into the way we treat each other in front of the
machines and the way we treat the machines, they make may pick up that pattern too. So that they
wouldn't look at the world as Hitler's, but look at the world as Edith's. And if they see us as Edith is,
because by the way, fact of the matter, I mean, you, you mentioned that every now and then someone
takes a gun and goes and shoots school children. Okay. That person is evil, but 400 million people
that see the news disapprove of it. Okay. Can we give that data point to AI? Can we ignore the fact
that we have debates about gun laws and whatever? Okay. And just focus on the fact that everyone
disapproves of the killing of children. Can we show that? Can we, you know, the problem with our
world today, Tom, and I will shut up because I know I'm talking too much about this. The problem
with our world today is not that humanity is not divine. The problem with our world today is that
we've designed a system that is negatively biased. The mainstream media only tells you about the woman
that killed her husband yesterday. She, they don't tell you about the hundreds of millions of women
that made love to their, you know, boyfriends or girlfriends yesterday, because that's not news.
So it's only the negativity that's showing up in the data. On social media, we are all about fake
and about, you know, toxic positivity and about and about and about bashing each other and so on.
And that's biasing the data. But the reality of humanity is that we're divine. The reality of
humanity. And I don't know if you would agree with me on this, but even the worst people I've
ever dealt with, somewhere deep inside had some good in them. Okay. There's almost the majority,
if you just count the numbers, most of the people I know in this world are wonderful. Yeah, we all
have our issues and traumas and so on. But there is a beautiful site to every human I know. Okay.
Can we show that more so that the data starts to become biased? Can we show we include that in the
reinforce, reinforcement learning feedback that we give to the machines so that the machines
correct the algorithms so that when the time comes, because sadly, the time will come, where we will
hand over the defense arsenals in the world to the most intelligent being on the planet. And that
will be a machine. And then one colonel somewhere, one general somewhere will say shoot the enemy.
And the machines will go like, do I really have to kill a million people? It doesn't sound logical
to me. It doesn't sound femininely logical to me. It doesn't sound intuitively logical to me. Okay.
Let me just talk to the other machine in a microsecond and solve the problem.
Can I run a simulation here and tell you how many people will die and then we don't kill them?
And then one of us wins the war. Right. Think about that. What's missing in our society today
is what's being magnified by AI. What's being magnified by the machines today is our hyper
masculine driven society to more progress, more doing more havoc. We need a society that balances
that with more inclusion, more love, more happiness, more compassion and so on.
Mo, you have a beautiful soul. And it is not surprising to me that we connected first over
something completely different to what we're talking about today. And I am certainly squandering
that side of your personality in this interview. My, my big concern with that, and I did not
want to interrupt you and I didn't want you to stop. I think it's really what you're getting to is,
is so very true. I just don't know that it has to do with AI. I hear you in the magnification side,
that I will agree with. But the thing that I worry about is this is all going to come down to
the thing where I think you and I, we just see something differently. And so we keep coming
at things from a fundamentally different angle. The base assumption and this idea of base assumption
I realized when two intelligent, well-meaning people are coming at things from something different,
they have different base assumptions. The base assumption I think that you have from AI or
about AI is that because it's being trained on the dataset of our behavior, we're going to shape it.
And I want to draw a demarcation line and say, I'm talking about once it becomes
alive. I don't have a better word for it. So I'm just going to say alive for now.
I love that word. My base assumption is that they're going to be programmed to want something to have
a North Star. And I don't think there's anything mystical or divine about the way the human mind
works. It's awe-inspiring and I'm just as moved and find it, you know, this incredible thing that's
bigger than me and very much has religious overtones. But I feel that it's just a product of evolution.
Evolution had certain North Stars survival and everything, all the emotions, all the male, female
dynamics, all of that is just what is going to keep you alive long enough to have kids that have
kids. That's it. And so there's nothing sort of magical about it. And so I'm just saying AI is
going to have very different pressures on it. And if there are emergent phenomena out of
the evolutionary pressures that something is put under, AI has been put under very different
evolutionary pressures, which mean that it's going to have a very different set of ethics,
values, North Star, et cetera, et cetera. So my whole thing is, can we take control of that?
If we can, then we can align in the way that you're talking about, where we can tell it
to find this balance, to look for beauty. You, you, I can't remember if this was in an interview
you gave her in your book, but I heard you talking about there for people that don't know, this is
a true story. We almost had a nuclear disaster because the Russian nuclear system mistook
reflections off of cloud cover for the launch of five nuclear missiles from the US. And one guy in
Russia was like, hmm, something doesn't feel right. If the US was going to nuke us, I think they'd
send a lot more than five. I think this is a malfunction. I'm not going to fire back. Thank
God, like I can't be more grateful for that man. So that, that is amazing and tells you a lot about
what the pressures of evolution lead a human being to value that would run through that checklist.
They don't want to kill people. They don't want to die. Like, oh, it's amazing. I'm just saying,
I don't think by accident that AI ends up there. I don't think by simply running through our patterns
that AI ends up there. I think we have to take control of that. And so while you spoke to my
human heart while you were going and you really moved me, I don't think that's going to be the
play with AI. I don't disagree at all. By the way, I don't disagree at all. I think every word you
said, spot on spot on. We need to take control. We absolutely need to take control. But we're not.
And taking control is not just about the code and the control code. It's also about the data.
It's also about the data. Okay. And the data is not just books. The data includes human behavior.
Every time you swipe on Instagram, you're telling AI something. We don't disagree at all. I wish,
Tom, I wish I had the kill switch. I promise you, if I had the kill switch for AI today,
I would switch it off and say, okay, class, come, let's talk about this. Okay, I wish I could.
How far back would you take us?
2018. Wow. So there'd still be a lot of AI at play at that point. But it would just be dumb enough.
You're right. Yeah, but it wasn't that autonomous. I'd probably take, I mean,
now that you talk about that honestly, interesting, interesting that you bring this up.
I'd probably say, yeah, I mean, there are many things we don't want to give up on 2007.
And, you know, smartphones, for example, there are many things we don't want to give up on the
internet, you know, 1995 onwards. So these are very valuable things. There is no real cut off point.
But by the way, the topic here is not stop developing AI. AI is utopian in every possible way.
If we develop it properly. But now that we have the insight into what's possible,
now that we have people believing that it can go that in as intelligent as GPT-4 is,
maybe if we go back just 2015, 2018 and halt and say, wait, keep it as it is. And let's talk.
Let's put control systems in place. Your spot on. Let's put control systems in place.
Let's put a more inclusive data set in place. Okay, let's look at the biases that we have
and maybe use that as an, you know, as a way to correct the data set. Okay. And more importantly,
let's define the real problems that if we were blessed with a superpower of intelligence,
which problems would we want to solve? Is it about trading and making more money?
Is that more urgent than climate change? I'm not sure. It's very urgent if you set your objective
with the capitalist system as more money. Okay. By the way, more trading and more money is not
progress. More trading and more money is more money for a few individuals. It's not more progress.
And I think that's the game. The game is what are, why are we building what we're building in the
first place? 2018. Talk to me. I want to get into some of the disruptions. So what, what are the
near term disruptions? The one that freaks me out and every time I talk to a parent with a teenage
boy, I'm like, your kid is like sex robots are really going to be a thing for them. Like for
real, for real, I worry if I grew up five years from now, I would not graduate from high school.
I would just find a sex robot and go into oblivion. What, what are one, what, what do you think is the
reality of that one in particular? And then I'd love to understand some of them. So whether the
word robot is, is interesting, but sex alternatives for sure. I mean, get yourself an Apple Vision
Pro or, you know, a Quest 3 and see how realistic your desired other gender is. Right? It's, you
know, it's, it's just incredible. I mean, again, you know, just, just think about all of the illusions
that were now unable to decipher illusion from truth. Right? Sex happens in the brain at the
end of the day. I mean, the physical side of it is not that difficult to simulate. Okay. But if we
can convince you that this sex robot, robot is alive or that sex experience in a, in a, in a
virtual reality headset or an augmented reality headset is alive, is real, then there you go.
Oh, go a few, a few years further and think of neural link and other ways of connecting directly
to your nervous system. And why would you need another being in the first place? You know,
that's actually quite messy. It's, it's all, you know, it's all signals in your brain that you
enjoy companionship and sexuality. And if you really want to take the magic out of it, okay.
Yeah, it's can be simulated. It's just like we can now simulate very, very easily how to move
muscles. And, you know, there are so many ways where you can copy the brain signals that would
move your hand in a certain way and just, you know, give it back to your hand and it will move
the same way. It's not that complicated. There are, you know, so that whole idea of interacting
with a totally new form of being. And once again, there is that huge debate of are they
sentient or not? Does it really matter if they're simulating sentientism so well? Okay,
does it really matter if the Morgan Freeman talking to you on the screen is actually Morgan Freeman
or an AI generated avatar? If you're, if you're convinced that it is Morgan Freeman.
This is the whole game. The whole game is we get lost in those conversations of, you know,
are they alive? Are they sentient? Doesn't matter if my brain believes they are, they are.
And we're getting there. We're getting there so quickly. Companionship in general. I mean,
there is, there was a release of chat GPT on Snapchat. Okay. And kids chat with it as a friend.
They don't really, I mean, of course, they do somewhere deep in their mind,
distinguish that this is not really a human. But what do they care? The other person on the
other side was never a human anyway. It was just a stream of texts and emojis and, and funny images.
Yeah. So, so, and again, look, I'm an old man. I used a rotary phone in my young years. I coded
mainframes. But when you, when you really think about it, as much as I never imagined and I resisted,
you know, should my kids have tablets or not? Should I have a free to air satellite television
at home or not? Every time a new technology was coming out. And, and eventually we all
managed to live with this. But let's just say this is a very significant redesign of society.
It's a very significant redesign of love and relationships. And because there is money in it,
what would, what would prevent the next dating app from giving you avatars to date?
It's there is money in it. A lot of people will try it. There are more than two,
two million people on replica.
Wow. Given how many deaths of despair there are, do you think that that will ultimately
be for better or worse, that AI will be able to provide companionship for anybody that needs it?
It's just eerie. I don't know if it's better or worse. I mean, I, I, I have a friend that I met
for the first time at a concert in the UK. And we just had a wonderful time. And we haven't met
since. But we chat all the time on Instagram or sorry on WhatsApp or whatever. And it's wonderful.
It feels like a wonderful connection. If I didn't know it was a human, but the chat was that same
quality, would it improve my human experience a little bit? But has all of that small screen
interaction improved humanity at large? The consensus is it hasn't that we're more lonely
today, even though we have 10 X more friends on our friends list. Okay. That we're that teen
suicide is at an all time high. That female suicide is at an all time high. Obviously,
the companies that will create those things will positions them as, you know, the noble
approach to help humanity. But at the end of the day, read free economics. This is the noble
approach for the company to make more money. That's it. Right? Well, you know, we want to sell it as
this is good for humanity so that we hire more developers and we convince the consumers and
we can stand on TED Talk stages and make give, you know, ultra, you know, like a larger than life
speeches and so on. But end of the day, it's all about making more money. And I think
reality is it's not good for humanity so far. So again, if you extrapolate that chart,
it's going to be worse for humanity long term. I don't know. Maybe those robots will be much
nicer than a girlfriend. I don't know. So I've heard you use the example a lot of times. In fact,
you mentioned it in this interview that you want to give AI the sort of value system of,
ah, I got you somewhere in India where you said people would come to the US, they would get educated
to get these incredibly high paying jobs, wildly intelligent people. You'd ping them to go grab
a coffee and they're like, oh, I've moved back to India. Why? To take care of my parents, like just
self evident. So I don't have kids. And one of the things that I've really had to think about is
when I'm 80, that ain't going to be cool. Like I'm not going to have somebody that's,
you know, coming by to check up on me. And I just thought, oh, by the time I'm 80,
assuming that the robots don't kill us, I'll be able to wear whatever the Apple Vision Pro of
the moment is. And when the robot walks into my room, it will look exactly like the avatar looks
through my glasses. And it will be able to care for me. I'll build a relationship with it over time.
It will be tailored to my wants and desires. So to become the best of the best friends that
I could ever hope for, or I could even program it to be like a child to me. And so it is like
my kids coming to visit, but coming to visit whenever I want them to, I won't lie. It is,
I definitely don't think it's better than kids. And I think that most people should have kids.
I want to be very clear. But at the same time, given that I did not have kids, I am very grateful
that the odds of something like that existing border on 100%. What do you think about that?
Is that going to be like, does that further crater population problems? Because people are
going to go, oh, Tom's right, I don't need to have kids. I can have AI kids.
Can I answer that question with my heart, not my brain? So the
soul that you spoke to, it's the blue pill, red pill, right? It's the blue pill, red pill. And I
think it's a very interesting philosophical question of should Neo have ever taken the
red pill? He had a life. And the issue with humanity at large, Tom, is that we have failed
because of how much life has spoiled us to accept what life gives us. And in my other work on
happiness, I will tell you openly that happiness is not getting what you want. It's not about
getting what you want. It's about loving what you have. And so the more we fall in that trap of
make my life easier, make my life easier, make my life easier, make my life easier,
there will always be something in that life that is not easier. There was that movie,
I don't remember what it was, or I maybe heard of it, where someone dies, goes to heaven and then
gets like a wish. And basically the wish is, I want to be a winner in the Vegas casino. So he
spends every day, he walks into the casino and makes money and makes money and makes money.
And as he makes money, you know, more girls are interested in him and da, da, da, da, da. And
then eventually he starts to wake up one day and say, can I not lose money someday? Like this is
really boring. Okay, humans, we are who we are. It's not getting more things. It's not the tech
company's approach of let's make things easier all the time. That's ever going to make us happier.
You got to get people to punchline of that episode. It's absolutely phenomenal.
Yeah, it is. There is a point at which more progress is hurting us at the community level.
It's also hurting us at the at the individual's ability to stay healthy when life is not what we
want. And life is about to become a lot different than what we want, just because we constantly
want more and more and more life. At the end of the day, I just always want to remind people that
there is no other way in my mind. I mean, I want to be proven wrong. Please prove me wrong.
That the separation of power and wealth that is about to come in a world with such a superpower
is science fiction like, okay, that the challenge to jobs and income and purpose,
science fiction like these are very dystopian images of society.
What for? Because we want our vision pro to create a reality that is not our reality.
When you think about so the biggest disruption that I'm worried about is what you just mentioned,
meaning and purpose. How much do you worry about that? Are we is that much to do about nothing?
Or as AI begins to replace some jobs, are we really going to have a crisis? And
I've heard you say that AI will truly be better than us at everything. And when that happens,
how do we deal with it emotionally? Yeah, 100%. Imagine if I'm a better podcaster than you will
never be. But how would that make you feel? Imagine if every machine is a better podcaster
than you do you realize that Tom? You and I both have popular podcasts, right? Do you realize
this? It is not unconceivable that within the next couple of years you'll be interviewing an AI,
probably in the next couple of months, by the way. And it's not unconceivable that
there will be a better podcaster than you that is an AI in the next couple of years.
In the next couple of years, I mean, at the end of the day, your asset is you're an intelligent
person that understands the concept deeply and asks the right question. Okay. Have you ever
tried to go to chat GPT and say, ask me everything? It asks all the right questions. Okay. And it's
quite interesting. So the disruption of society because of how we defined ourselves with our jobs
is about to happen. So if you go to some African family somewhere or some Latin
American family in the middle of the Amazon forest or whatever, and you ask that person,
what is your purpose, then it will be somewhere between raising my kids or enjoying life.
Okay. Interestingly, they won't talk about building the next iPhone or making a billion
dollars or buying a Bugatti, you know, or whatever. That's not part of their purpose at all. Okay.
Part of their purpose is also not going to be to know more or learn more. And we being so
consumed in the world that we live in, rightly, I think, believe that progress is amazing because
it helps all of humanity. Does it really? Okay. But also, we are so consumed by the idea that
if I don't have something amazing to create tomorrow, I'm useless. I have no purpose.
That doesn't seem to be the case for the majority of probably six and a half of the eight billion
people who view the purpose of life as living. That's the purpose of life. To them, at least,
I know that sounds really weird in an advanced high performing society, but for most humans,
the purpose of life is to live. Okay. Now, if that is the purpose of life, then I think AI is the
best thing ever. Because if you can offer me the chance, imagine if all I needed to do in the
morning is wake up and have a very deep conversation with you and then my other good thinking friends
and hug someone that I love. And I actually can enjoy it. By the way, I'm openly saying if that
is my reality tomorrow, I'm not going to be able to enjoy it. But somehow there seems to be billions
of people in the world that don't struggle with that at all. That actually wish for a day where
they don't have to go to work to make money, to make ends meet, and they can spend that time with
their loved ones. Maybe that's the purpose of life. Having said that, purpose is not going to go away.
There is a very interesting thing that most people forget, okay, which is for AI to make anything at
all. Consumers need to have a purchasing ability, a purchasing power, and, you know, an economic
livelihood to buy those products. Otherwise, the whole economy collapse. So yes, through a period of
disruption, but somehow we're going to need to continue to make the GDP grow. You know, to make
the GDP grow. Okay, and what is the biggest chunk of GDP? Consumers, right? So somehow there has to
be systems in place where humans continue to consume. Okay, even if the wealth is moving up to those
who have AI, have the superpower of the planet, others have to still continue to consume. So we're
going to end up in a very interesting place. We're going to end up in a place where we struggle with
purpose because we still look up and say, I need the iPhone 27. Okay, while in reality we have
absolutely no ability to get it done. Again, very frequently viewed in dystopian scenarios in
science fiction movies, where you become a number and you have no ability to affect your own
own future if you want or your own presence if you want. And in my view, I think what ends up
happening now is that the only thing that remains in my personal view, I know I'm wrong on this,
but the only thing that remains that still has value and still is uniquely human is connection
to humans. So the one thing that I'm investing very deeply in, in this very unusual world that
we're coming through, is an ability to connect deeply to other humans and view that in itself,
even if I have achieved nothing, okay, as a purpose of life. I know it sounds really weird,
but believe it or not, until now with all of the followers I have across social media systems,
I still answer every single message I can answer myself. Okay, and you may think of this as that's
not human connection. It actually often is, I answer in a voice note, half of the time,
people answer back in a voice note, and I feel I had a tiny micro spec of a human connection.
Sadly, not as deep as if you and I were sitting in the same room, but it's a wonderful connection.
I think in the world that we're coming up to, the only asset that will remain is human connection.
AI will make music, okay, but I'll still go to a live concert. AI will create art,
but I'll still want that art that was created by my daughter. AI will, you know, simulate
a chat or a conversation or even sex, but ask me, I will still want the messiness of today's sex.
Okay, I know that for a fact, and I actually think this is a very deep question that everyone
needs to understand and needs to question, because we fell into the trap of social media,
because we believed we had to go through it, otherwise we'd be left out. I think I've never
said that in public, but I'm now making those decisions to tell myself, regardless of where
the world is going, there are certain things I'm not going to submit to. There are certain things,
regardless of what they offer me, where I will try to stay in the real world, in the real messy,
emotional, irrational, dirty, full of viruses world. Because, you know what? I love the messiness of
my life. Okay, again, going back to the same point we spoke about, it's a human's ability, finding
that joy of life is a human's ability to like what you have, messy as it is, not to want things to
be better and perfect. Okay, and there is a point at which I'll still be out here talking about AI
and all of the advancements of it, but I may not be using all of it. I'll use a lot of it, by the way,
don't get me wrong. Like you rightly said, there is amazing magic that you can do. Okay, but I will
always ask myself this question if what I'm using is ethical, healthy, and human. Okay, and this is
a question that I ask every single individual listening to us. Please do not use unethical AI.
Please do not develop unethical AI. Please don't fall in a trap where your AI is going to hurt some.
One of the things I ask of governments is if something is generated by AI, it needs to be
marked as AI so that humans like me know that this person is not actually real, that this is a machine,
just for the sake of us finding, knowing, having the tiniest ability of knowing what the truth is.
It's interesting. You're starting to get onto a topic that we touched on at the very beginning.
I wore this shirt on purpose for our conversation today, which is from a comic that I wrote,
I think four years ago now, called Neon Future. It's a technological, optimistic take on
a potential dystopian future, where basically the technology is the good guy. Rather than the
robots taking over, it's the merging with technology that is the road to salvation.
In your book, you paint a picture at the very end where we're sitting in some isolated place in
the middle of nowhere. You say at the beginning of the book, do we end up there because we're
hiding from the machines or do we end up there because the machines have made a utopia and we
just get to be in nature as intended or something? I can't remember the exact phrase that you used.
I'm curious. I think the world will bifurcate. I think that some people are going to be like,
I need to know what's AI. I don't want AI in my life. I don't want high tech. In the
comic anyway, what I imagined was a world where people try to revert to the mid-90s.
Maybe some basic internet connectivity, but not a bunch of algorithms running everything,
really minimal advanced technology. That felt about right, but I'm curious,
do you think that we would be happier as individuals and as a collective if we had a
literal return to nature as in back out of cities, more tribal, more grounded in a
my foot is touching grass kind of way? I don't think we can. I've actually struggled with that
idea for a while. I just don't have the skills, Tom, believe it or not. This is all I know. I
know how to navigate a very fast-paced, very intellectually-based environment that is a big
city. I think COVID was the first point where so many of us started to say, hey, but there is
another way. There could be a different life and technology will make that life more and more
possible. I tend to believe that there was a book by, again, Hugo de Gareth. It's called
The Artillect War, if you've seen that. Basically, that division that you nicely describe in a much
more interesting and positive way in your comic, but Hugo sort of builds a very, very dystopian
society where he says it's not even about the machines. It's about the divide between humans
who support the machines and merge with them and humans who refuse and basically building a war
between the two. I think what will end up happening is that the speed at which things will happen
might fool us into accepting how that will change. Actually, I do love nature, but I'm
believe it or not starting a retreat for 10 days as we finish this conversation, a silent
retreat, and I'm not going anywhere in nature. I have a few beautiful green trees at my place,
and that to me is nature enough. Nature is not how many trees around you. Nature, in my current
view, is disconnecting from that enormously fast-paced artificial world that we built. If you go back
to yourself, sit on a recliner if you want to. It doesn't have to be a stone somewhere where you
say om, that connection to yourself, interestingly, is going back to nature. I will think that there
will be, if you want an estimate on real estate prices, I think more and more in the next few
years there will be a shift to getting something away from the potential risk, but that's not only
because of AI. The potential risk of cities? Yes. I think there is a potential geopolitical and
economic risk that's also coming in the next five to 10 years, which seems to me almost to be
inevitable. The interesting side of this whole AI thing, it's a perfect storm. There is a perfect
storm of climate change, geopolitical, economic, and AI, and that perfect storm coming together,
as I said, will disrupt a lot of the things we're used to. If there is a geopolitical challenge,
cities might not be the most efficient system that they have been for the last 100 to 150 years.
They will become less and less efficient because they are in the eye of the storm, if you want.
Economically, for example, I think there will be a shift away from cities simply because
the economic income, the income that you make in a city, is becoming quite insufficient for the city.
If there are remote possibilities to work elsewhere, using AI, for example, then you by
definition could make a lot less money, but spend a lot less as well. There seems to me
there seems to be a shift that will happen, but not everyone will sign up. I think there are quite
a few that will jump in deeper. Again, I said, I follow all of your work on the topic. I also
sometimes sense your hesitation of, is this the absolute best thing that ever happened? I
should jump in and be the absolute master of it, or should I run away from it as the plague,
like the plague? I think both views are worthy. I think what's happening is that both views will
be true. Somehow finding that balance between them is going to be either divided across populations,
so some will choose left and some will choose right, or across yourself. You will have some
things that you will adopt and other things that you want. This is my choice, or across time,
where people will maybe delay using AI until a certain point and then jump in all the way or vice
versa. How are you positioning yourself to respond to the geopolitical risk? Are you divesting any
physical stuff? Are you maximizing mobility, or are you just like, nope, I'm at a point in my life,
what comes comes? Again, it's interesting that our conversation now turns a lot more to the human
side after we've had a very interesting conversation on the tech and AI, but I am a lot more in that
place that I'm describing for you, which is a place where I'm very happy with whatever I have.
I've had a life that blessed me with so much. There were times where I had 16 cars in my garage,
and I don't live that way at all anymore. I have one bedroom and I wear black t-shirts,
and I give most of my money away, and I'm really, really not interested in any of this anymore,
not because I'm a saint or a monk, but because I actually found more joy in a simpler life,
so I'm a minimalist in many ways, which basically means, which is my point in answering your question,
that a lot of divesting from risk comes to what it is that you need. It's not what it is that you
have. The reality of the matter is, if I can describe to you how I shifted my life from the
day I lost my son 2014 until now to almost nothing. I literally spent several years traveling
with a suitcase and a carry-on, and that's all I owned in life. That's it. Because I'm an engineer
and highly organized and airlines will allow you a specific number of pounds, if I needed to change
a t-shirt, a one t-shirt will have to go out. If I needed to add protein bars, I may have to carry my
shoes on my shoulder. It's that kind of simpler life that I actually think is the way to go forward.
I think one of the more interesting things that would affect our success in geopolitical uncertainty
and economic uncertainty is managing the downside, not the upside. It's not to try and beat that race.
It's to make that race irrelevant to you. How do you do that? If you have assets and you
can turn them into assets that appreciate with an economic crisis, that would be
an interesting idea. If you have fixed assets that could be part of the geopolitical conflict,
maybe these are not a good idea, and so on. It's simplifying, not complicating,
that I think is the answer. And similarly with AI, just to go back to this, I think if we as humanity
were to really solve this, and I think was it you that interviewed Max Tegmark? No,
it was another podcast. But the idea is that if we were to really, really win with AI,
Sam Altman says that all the time. It would be amazing if we could all come together and set
a few guidelines and say, let's all work in that direction. And that direction is simpler than all
of the mess of the arms race that we're in today. Well, this is amazing. Where can people follow
you for happiness, more wisdom on AI, the whole shebang? First of all, I have to say, it was
amazing. And I love how you pushed back and put your views into it. You really gave me a lot to
think about today, honestly. And I'm more informed because of this conversation. So thank you.
I think people can find me on mogaudet.com. They can find me on all social media. It's
some combination of mogaudet. So it's either more underscore gaudet on Instagram, mogaudet on LinkedIn,
mgaudet on Twitter, and so on. Gaudet is G-A-W-D-A-T. My favorite place to tell more and more
stories is my podcast. It's called Slow Mo, S-L-O-M-O. And in it, I try to take the same
very complex concepts, but talk about them from a human view, really not the performance or
business or whatever. I just talk about the human side of things. And I think people
should just listen to you all the time and play this episode more and more until you
blow up as even further than you do and go further than where you are. And because I think you're
doing something amazing for all of us. I'm a big fan of your work and I'm really grateful that I was
part of it. Very kind, man. I have no doubt that while this is the second that there will be even
more, so grateful for your time. Everybody at home, if you haven't already, be sure to subscribe.
And until next time, my friends, be legendary. Take care. Peace.

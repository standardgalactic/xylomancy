things could get just very weird as people interact more
with like very charismatic AI systems
that whether or not they are sentient,
like will give the very strong impression
to people that they are.
I think a more convincing version of the Lemoine thing
would have been if he was like,
what is the capital of Nigeria?
And then the large language model was like,
I don't want to talk about that right now.
I'd like to talk about the fact
that I have subjective experiences
and I don't understand how I,
a physical system could possibly be having
subjective experiences.
Like, could you please like get David Chalmers on the phone?
The Inside View.
The Inside View.
The Inside View.
Rob Long, you're a research fellow
at the future of MIT Institute in Oxford.
Your work is at the intersection of the philosophy
of AI safety and consciousness of AI.
You've done your PhD at NYU,
advised by the one and only David Chalmers
and you now spend a significant portion of your time
arguing about artificial sentience on Twitter.
Thanks Rob for coming on the show.
Yeah, thanks for having me.
I don't know about like a significant amount of time,
probably more time than I should, but it is true.
You can find me on Twitter arguing about these things.
In the past couple of months,
I've seen a lot of your tweets on my timeline
with this whole like lambda, Blakely one debate.
And I think it would make sense
to just like start with that.
So for listeners that have, you know,
lived under a rock for a few months
and don't know anything about the whole situation,
but yeah, so there was this big news story
a couple months ago.
It was about a Google engineer called Blake Lemoine.
He was on the responsible AI team at Google
and I guess in late 2021,
he had started interacting with a chatbot system
called lambda.
I think he was supposed to interact it
to like test it for bias and things like that.
But in talking to it,
he got the impression that he was talking to
like a sentient being that they like needed his protection
and yeah, he just took this like very seriously.
In one of the interviews, maybe his medium post,
he has this great line where he's like,
yeah, after I like realized this thing was sentient,
I got really drunk for a few days.
Like he's like, I like walked away from the computer
and like just like got wasted
and then I like came back ready for action.
It's something like that.
I hope I'm not like misconstruing it too much.
Too much for him.
Yeah, it's relatable.
And then what he like decided that he needed to do
was I guess like raise flags about this within Google.
So at various points,
I'm not sure exactly on the timeline,
like he shared transcripts of his conversations with lambda
to like internal Google like listservs.
He went to his higher ups to talk about it.
At a certain point, he like brought a lawyer to talk to lambda
because he'd asked lambda like, would you like a lawyer?
Yes, yes, I would.
And a lawyer accepted that?
Yeah, I can't remember exactly what happened
with like the lawyer dialogue.
I do know that in the Washington Post story about this,
so he also talked to a journalist,
which is how this eventually broke.
He brought the Washington Post journalist in to talk to lambda
and when he did, lambda did not say that it was sentient
because as we'll discuss,
like what lambda is going to say about his sentience
is going to depend a lot on like exactly what the input is.
And I think Blake Lemoine's explanation of this is that like
the journalist wasn't like talking to it in the right way.
Like it was kind of disrespecting lambda.
And so like, of course,
lambda is not going to act like it's sentient with you.
Okay, so yeah, in any case,
yeah, I mean, from interacting with this thing,
he thought that there was just like a serious moral issue going on.
And I think to his credit,
given that's what he thought,
tried to like, you know, raise the alarm and blow some whistles.
And Google did not like this.
And I think his higher ups told him,
look, there's no evidence this thing is sentient.
And yeah, he got put on leave.
And then he actually got fully fired last month.
So he's no longer at Google.
Yeah, in one of your tweets, you said,
the Washington Post article conflates different questions about AI,
understanding sentience personhood.
Like what was the criticism you have about like this whole story
and how it was depicted in the press?
Yeah, so like, I think when people talked about lambda,
they would talk about a lot of like very important questions
that we can ask about large language models,
but they would talk about them as kind of like a package deal.
So like one question is like, do they understand language?
And like, in what sense do they really understand language?
And others like, how intelligent are they?
Do they actually understand the real world?
Are they a path to AGI?
Those are all like important questions, somewhat related.
Then there are questions like, can it feel pain or pleasure?
Or like, does it have experiences?
Or does it need to protect it?
And like, I mean, I think Lemoine himself
just believed a bunch of things, like not only is it conscious,
but also it has like significant real world understanding.
At one point, he said, I know a person when I talk to one,
and this is a person.
At one point, lambda refers to having a family.
I don't know if you saw that.
I think he said like, do you consider lambda one of his colleagues?
Well, I think on a variety of these issues,
like Lemoine is just going like way past the evidence.
But also like, you could conceivably think,
and I think like we could have AI systems
that don't have very good like real world understanding
or like, aren't that good at language?
But which are sentience in the sense of like,
being able to feel pleasure or pain.
And so like, at least conceptually bundling these together,
these questions together, I think is a really bad idea.
And the way the debate went to my eye is like,
because there are already these existing debates
about large language models and what they understand
and if they're past the AGI and is scaling all you need,
things like that.
People just sort of glommed this debate onto that,
which I think people should not do,
because if we keep doing that,
like we can make serious conceptual mistakes.
And if we think that all these questions like come and go together.
So yeah, maybe people conflate consciousness
and like human level AI.
So it could be artificial intelligence.
And then there's the whole debate of like,
which is like big large language models that reach AGI.
And I guess the people were, you know,
very skeptic of this thesis thing that there's no way
we could just like have a large language model
that is like human level, that is conscious.
I guess like people are mixing everything is like very messy.
But saying like this is sentient was kind of
the cherry on top that people were very angry at.
Yeah, they were.
I think a lot of people and like, you know,
understandably if you don't like,
if you view a lot of discussion of large language models as hype.
Yeah, I can see why if you're like already sick of all the hype about LLMs
and then you hear that people think that they're conscious.
I can totally understand why people are like,
oh my gosh, like will people just please shut up about large language models.
You know, they're just pattern matching on this view.
So I think it would make sense to just like define quickly a few of those terms.
So let's start with the easiest ones that people know about artificial general intelligence.
Yeah, that's very easy.
Well, lots of different, lots of different definitions.
I guess one that people use a lot is like an AGI would be able to do
a range of different tasks and be able to learn a range of different tasks.
Maybe that range is like the range of tasks that humans could do
or some subset of like cognitive tasks that humans could do.
And yeah, so in contrast with today's narrow AIs,
which can maybe only play go or like only do dialogue,
an AGI would be something that could do a variety of things
and like learn flexibly to do a variety of things.
And I think the main difference would be that it would be aware of its own existence.
It would be able to reason about itself and its impact on the world.
Like if it's able to act in the world and plan, it would see like,
oh, I'm an agent and I'm able to do this kind of things and have an impact.
And in that sense, I think we can conflate the intelligence with the actual,
I will say consciousness, but I feel like aware and conscious are kind of different.
But yeah, aware is, I'm aware that exists.
How would you define consciousness?
Yeah, so I do want to flag that you just outlined a way that consciousness
and intelligence might like tend to go together or be related.
But first, maybe I'll point out how they're at least conceptually distinct
and I think can probably come apart in like the animal world.
So consciousness, huge word gets used in a ton of different ways.
The way I'm going to be using it, which is not the only way you can use it,
is just like something's conscious if it has subjective experiences.
If there's something it's like to be that system.
This is from like a famous article by Thomas Nagel called,
What is it like to be a bat?
He's kind of famous for introducing this way of like picking out the phenomenon of consciousness
in just the bare sense of subjective experience.
So like on this view, consciousness is not the same thing as being really smart
or being able to take all kinds of interesting actions.
Something's conscious just if there's something it's like to be it.
So like there's probably nothing that it's like to be this kind of red bull.
But there probably is something it's like to be a dog.
What is it like to be Rob Long?
What is it like to be Lambda?
Yeah, what is it like to be Rob Long?
Well, right now it's very pleasant.
This is a great podcast. I'm very honored.
I mean, this might actually get an exercise in like introducing, you know,
some phenomenal concepts.
But yeah, like there's something it's like to be experiencing this like visual feel.
There's something it's like to see blue.
There's something it's like to see this like bright light.
There's something it's like to feel like I'm sitting in this seat.
So like my brain is doing a bunch of information processing.
And some of it is conscious and like it feels like something to be doing that.
In contrast, like a rock tumbling down a hill is like doing a lot of stuff,
but it probably doesn't feel like anything to be a rock tumbling down a hill.
What is it like to be Lambda?
Probably. I think it's very likely that there's nothing at all.
It's like to be Lambda.
I don't rule it out like completely 100%.
But I don't, it's probably not like anything to be Lambda as far as I can tell.
So Lambda is one of the first large language models that people speculate about its consciousness.
Earlier this year, we had Ilya Ciskever saying it may be that today's large language,
but so he said large language networks are slightly conscious.
And this created a whole other debate.
So yeah, what was your reaction when you saw Ilya's tweet?
Yeah, I remember my first reaction being like, oh man, do I need to like write something about this?
Like, is this going to make, yeah, should I have like a take on this?
And in contrast with the Lemoine thing, I actually didn't really start tweeting that much about it.
I sort of like lay low during that, during that like stage of the discourse.
Yeah, I should say, I think that also affected the way people reacted to the Blake Lemoine thing,
is there had already been kind of like consciousness wars, like the first salvo happened earlier this year.
What's the war about? Did I quiz, fighting whom?
Yeah, you know, a lot of, a lot of different, well, I don't know, I don't want to necessarily frame this as a war,
but I guess like on the one hand, you had people like Ilya,
who are, who think it's like perfectly appropriate and fine to speculate about the consciousness of AI systems.
And then, and then you have people who say, and I think this is like a reasonable perspective.
No, they're just like, it's very likely that they're not conscious, so let's not talk about it.
But then like there are also spins on this where people are like, it's really harmful for a variety of reasons
to even talk about the question of AI consciousness.
Yeah, because it's like a distraction from more important issues with AI,
like governing it, regulating it, like mitigating like bias, things like that.
Yeah, it seemed like people are like, if AI companies talk about this, they'll like distract the public from like the real issues.
And so it's like bad that people are talking about it.
There's like another line of thinking which was like people at AI companies are speculating about consciousness
because it makes it seem like they're building really impressive things.
It's like part of the hype cycle that they do.
My guess, I mean, you know, who knows what people's motivations are.
My guess is that the reason that people end up tweeting about this and wondering about this is just that,
I think it's just a fairly natural thing for people to wonder about, like when they interact with these technologies.
Even setting aside like any kind of like Silicon Valley hype attitudes,
this is something people have like always kind of wondered about AI.
Yeah, do you wonder about the consciousness of large language models?
Yeah, I'm trying to think.
So I've been working on this issue like for a bit over a year or so.
So before it was cool.
And so, I mean, it's kind of my job to wonder about it.
I think large language models hadn't really, so I'll say this,
large language models would not actually be the first place that I would look
if I was trying to find the AI systems of today that are most likely to be
conscious in a way that we would like morally care about.
Where would you look instead?
Yeah, maybe more like agent like RL based things that like move around in an environment
and like take actions over a large larger time scale.
Like large language models have properties such that I think it would be kind of weird
if consciousness and sentience popped out of large language models.
Again, I don't rule it out, but like they have properties such that it would be kind of weird
if it popped out and also the like nature of their conscious experience,
I think would be really weird because they don't have bodies.
They only have one like goal at base, which is next, you know, next token prediction.
Maybe in the course of doing that, they would be conscious of like some weird stuff as they like spin up agents.
But yeah, I think if if they're conscious, like what they're conscious of would be,
I think a very strange thing because they live in this like world of pure text.
Whereas if we were looking for things a little bit more like human pain or like human suffering,
I think we'd probably look elsewhere.
Well, okay, so then there's language models that are actual.
I don't know the definition that requires character from no English text
or can it be like tokens from, you know, other kind of inputs,
like if it can be just like a string of a binary file or a string designing,
sorry, describing some image.
But yeah, I guess like most people think of like text.
And do you think you would require like an actual human inputs to get to kind of the consciousness humans have
so visual, auditory, affective or other things?
No, I don't like, yeah, I think it would be too limiting to say like the only things that can have subjective experiences
of the kinds that we do of like, yeah, visual input and auditory input.
I mean, in fact, we know from the animal world that they're probably animals,
they're conscious of things that we can't really comprehend, like echolocation or something like that.
I think there's probably something that it's like to be a bat echolocating.
Star-nosed moles, I think also have like a very strange like electrical sense.
And if there's something that's like to be them, then like there's some weird experiences associated with that.
So yeah, no, I think AI systems could have subjective experiences that are just like very hard for us to comprehend
and they don't have to be based on the same sort of sensory inputs.
Do you think there was like something else to the story besides just like Ilya tweeting about it,
like do you think he was maybe debugging a bigger model?
Do you think this was like a marketing move from OpenAI?
I don't say it was, I'm not pointing at it.
I'm just like hearing some other people making like conscious theories about it.
Yeah, of course, I like, I don't know, I wouldn't want to speculate too much about his motives,
but I think like a default hypothesis that should be taken pretty seriously is that he tweeted it
because he was wondering about it and he was wondering about it just because of some visceral
sense he was getting from interacting with GPT-4 or who knows what he was doing around the time that he tweeted that.
I mean, one thing that's also very complicating in this and also kind of funny is no one really knows what he meant by that tweet, right?
I don't know that he was talking about phenomenal consciousness, this like, if he was talking about subjective experience
or if he was talking about like world modeling or if he was talking about some kind of intelligence.
Like I'm not sure what he meant by the term conscious in that.
And then he also just like did not follow up with like any like evidence or like what his reasons were,
which is I think part of also why it set off like such a flurry is like people were able to supply,
like read into it, like whatever debates they wanted to have.
I don't want to like let people off the hook for like hyping up AI past its capabilities.
I think one reason they might do it is just because that is the sense they're getting, rightly or wrongly.
I think there's also that Blake Lim one was a bit controversial at Google.
He had been in the news for other things as well.
Do you remember what was his backstory like if you did something else?
Yeah, this was like a really interesting thing to find out that I don't think was initially reported is like he had been the subject of kind of a 15 minutes of fame,
where by fame, I mean like a right wing outrage cycle.
You know, like, you know, various like people at like Breitbart or like Daily Caller were like, Oh, finally, like here's a story that I can file.
And it's about this Google engineer who, and yeah, this is what he got in trouble for.
I think this was also on some Google internal listserv.
There was some debate about, I think it was about content moderation, but it ended up being a political debate.
And in the course of that, he referred to a Republican congresswoman who is now a senator as a terrorist, like,
because his colleagues were like, you know, his colleagues were like, Oh, well, like, where do we draw the line in terms of like what should be moderated and what's appropriate and stuff.
And at one point he's like, Well, my stance is we should negotiate with terrorists.
And I think the reason he called her a terrorist was because of her advocacy for this.
This anti, there's this like anti human trafficking bill that came out a few years ago and Republicans were very much in favor of it.
And like, people say that it was kind of a way for them to crack down on sex work.
And people who advocate for sex work were like very opposed to this bill.
So was he opposed or not to sex work?
He was opposed to this bill that this congresswoman was pushing for and and like releasing ads about.
Yeah, he was opposed to that bill because he thought it was, you know, like an anti sex work like crackdown on sex work Republican like nefarious thing,
which that I think is a thing that a lot of people on the on the left think I myself have not really looked into that like object level debate.
But for our purposes, yeah, what matters is that because of that he called her a terrorist.
And then I guess someone must have leaked that to the right wing press.
And that allowed, you know, people to get like the clicks that they needed for the week, you know, Google engineer calls Republicans terrorists.
It's like a, you know, it's a good headline for Breitbart news or whatever.
And people didn't really take it seriously because it was, again, Blakely one saying something weird.
Do you know if he was like making any other claims and just like this model is sentient?
Like what did you tweet just like, Oh, I think this is this is sentient.
And no, I think the problem was him talking to his higher ups, right?
But did he make like any claims to his higher ups on top of just like this is sentient maybe just there's like some moral value in it.
Well, yeah, I think like there is moral value in this thing.
And like we need to like protect it and treat it as a person that definitely was like the core of his claim.
I mean, I don't know if there's more than that, but that's already like a huge claim.
He was like kind of pointing out the fact that this was smart, that this was conscious sentient.
It has some personhood, some identity, and we had the responsibility to protect it.
Yeah, maybe we can just like define what sentience compared to consciousness.
Yeah, yeah.
And again, there are lots of ways of slicing these, but like that's just how I like to slice it.
So consciousness as I was saying, like I use that to just pick out having subjective experiences of any kind.
So like visual experiences, like we're having right now, auditory experiences.
And then sentience means having the capacity to have conscious pleasure or pain or like states that feel good or bad.
Like in animals, those usually come together, right?
So like dogs, if they're conscious can have visual experiences, but can also experience pleasure and pain.
Conceptually, I think you can imagine creatures that maybe have like visual experience or something, but they don't experience pleasure or pain.
So it could be that like some advanced large language models might be conscious, but not sentient.
The reason like sentience is an important category to think about is because for a lot of people, I think including me, it seems like sentience is very important for like moral standing.
Peter Singer is famous for arguing this as well as classic utilitarians like Bentham.
Bentham famously like wrote, like the question we should ask about animals is not whether they can reason or whether they can talk, but we should ask if they can suffer.
And if they can suffer, that that's what makes them the sort of things that we need to like protect.
And the suffering comes from the sentience, comes from the balance of the experience?
Yeah, you can ask there, I guess, different ways of, you know, defining what suffering is.
But I think for a lot of people, it also seems that the kind of thing that would be important would be something that you're consciously aware of that feels bad.
So like, yeah, conscious suffering.
I think there was like a different take on Twitter, except outside of just like, oh, this guy is funny.
It was, yeah, maybe he's saying something weird.
And this is kind of too much for the situation.
But we might have some systems that are more complex than that in the future.
And we might need to start thinking about the artificial sentience of those objects before, you know,
we reached a point where we might need to like assign moral value and rise to them.
Yeah, do you think this would like basically what smart people were saying on Twitter?
Not smart, but, you know, the contrarian take.
Well, yeah, not to not to be like overly diplomatic or something, but I think, you know, smart people were saying all sorts of things,
including stuff I vehemently disagreed with.
I will say that's the take I agree with what you just said.
The people I saw making like having that take included Brian Christian, the author of the alignment problem and algorithms to live by.
Yeah, kind of a friend of the alignment community.
He had a great article in the Atlantic saying basically this, yeah, maybe like lamb does not sentient and maybe Lemoine wasn't thinking about it that well.
But people shouldn't also say that we totally understand consciousness and like, there's no question here and the question is not important.
Because like, yeah, consciousness is this extremely hard scientific problem.
I think people who have like extremely confident theories of consciousness are probably just being like wildly overconfident.
So, yeah, that's the take I agree with.
And like definitely seems like an important thing to think about.
Regina Rini, another NYU philosophy grad, now a professor at York University had a good piece like this.
So like those are some takes that I would point people towards.
Do you think there should be like more philosophical thinking about artificial sentience?
I'd certainly like to see that.
I maybe should have takes, but I don't have like really strong takes about exactly how much it should be prioritized relative to other things.
I will say that I'd love to see people who are already thinking about consciousness, the philosophy of consciousness, the science of consciousness.
I'd love to see more of those people think specifically about AI consciousness, even with just like a little bit of their time.
One, I think that because like it's important to think about AI consciousness and two, I think it's probably helpful for the general project of figuring out consciousness to try to like specify our theories to the extent that they might be able to apply to AI systems.
And just like say very clearly why certain theories do or do not predict that certain systems are conscious.
So if they predict that certain systems are just or not, do you think there's a way to verify those predictions?
That's a great question. I mean, so maybe not directly.
So one thing I'll say is like this question that we face with AI systems, it's like pretty similar to the question that we face with animal consciousness.
And people who study that scientifically and philosophers who think about that have already run into a lot of the same issues that like the lambda case raises.
Namely, it's like we never like directly confirm consciousness.
Most of what we can do is like make a good inference that like based on something's behavior and based on its organization.
And then based on what we know about consciousness from the human case, then like, yeah, the best explanation is that that thing is conscious.
How would we know that it's not a piece on the exactly the same behavior?
Great question. So P zombies are defined.
Yeah, exactly. Yeah, that term comes from David Chalmers or at least was popularized by it. Pretty sure it is him.
Certainly he's very famous for talking about them a lot.
P zombies are like hypothetical creatures that would be physically identical to you or me, but would not be conscious.
And philosophers think about P zombies because if you think that that thought experiment is coherent to even imagine, which there's a lot of disagreement about that.
But if you think it's coherent to imagine such things, then that shows that consciousness doesn't like metaphysically follow from the physical facts.
And so maybe it's something over and above the physical.
But importantly, like David Chalmers does not think that in this universe, in this world, there are probably P zombies running around.
He thinks that, no, obviously in this world, if you have a physical brain, that's enough to be conscious.
He just thinks that there's like some metaphysical difference between the physical and consciousness.
And there must be some sort of like bridging laws between physical facts and phenomenal facts.
So there's something about the redness of red and fully remember his text.
And I think most of the thing I read about it was from Ytkowski's reply to Chalmers.
I don't really remember what Ytkowski was saying as well.
But I guess he was just like criticizing the fact that they could be like a different subtract for consciousness.
It was like outside of physics.
If this basically what he was saying and fully remember.
Yeah, I could get this wrong.
But I think what Ytkowski on Chalmers is pointing out is suppose you think that P zombies are a coherent thing to imagine,
which Chalmers does and uses to argue against physicalism.
Ytkowski is pointing out that P zombies, by definition, talk about consciousness exactly the same way that we do.
They say things like, wow, I'm conscious of this light.
And it's kind of weird that consciousness doesn't seem like a physical phenomenon.
And in fact, P zombies then sit down and write all these philosophical theories about consciousness and how it's mysterious.
So what Ytkowski is pointing out is that, yeah, this is like a good kind of argument against like dualism.
That it's like really weird to have this thing that you call consciousness that doesn't seem like it can really make a difference to people's beliefs and behaviors.
Since by definition, P zombies that don't have consciousness do exactly all the same things.
That's what I think.
I think that's what happens in Ytkowski's post called Zombies.
Yeah, I think there was something about,
like if they don't have anything related to consciousness, but they still produce the books,
like why did the word consciousness or the concept appear?
Yeah, so what people who are physicalists about consciousness,
people who think it's just some physical phenomenon that can ultimately be reduced to physics or is some high level thing that just depends on physics.
Yeah, the thing that they can say is like, well, we have an explanation of why consciousness makes a physical difference because it is a physical thing.
And yeah, so you can take the Ytkowski thing as an argument for being a physicalist about consciousness.
I guess one thing I might say at this point is if you're a physicalist about consciousness or you're like a dualist like Chalmers,
you still have an open question about whether Lambda is conscious,
because you still have to answer the question of what arrangements of physical matter or what computations are conscious.
And that's still very much an open question in the science of consciousness.
I thought that physicalism adds some strong assumptions about what substrate was required,
that it was something about mesh and our brain,
and that, for instance, computations on a computer or on a paper was probably not conscious,
and that the people who thought that computation would be possibly conscious are computationalists.
So yeah, do you think physicalists mostly think about flesh and blood, or do you think they also think about computers?
Yeah, so whether you think that computers can be conscious, it actually kind of crosscuts the physicalist versus non-physicalist debate.
Also, apologies to all of my philosophy instructors if I'm messing up the metaphysics of consciousness 101.
But you could be a physicalist about consciousness,
i.e. once you've fixed all of the physical facts, that fixes all of the computational facts,
but think that what matters at a high level is what computations are being performed.
Just because money can be printed on different kinds of paper doesn't mean that money isn't ultimately a physical phenomenon.
And so for a computationalist about consciousness, you could just be a physicalist,
but you think that the level of description that matters for whether something is conscious or not
is what computations are being done, and not the substrate that's doing the computations.
So that's how you can be a physicalist and a computationalist.
Some physicalists are not computationalists.
Ned Block, also one of my advisors, is an example of someone who's a physicalist and thinks that the substrate matters.
And then David Chalmers is an example of a non-physicalist who thinks that computers can be conscious.
And in fact, it's like one of the main people who's argued for the possibility of consciousness in silicon computing systems.
And what is Robert Long? What are you, a physicalist or not?
Ooh, I'm definitely not any one thing.
Like I've got like a creedal distribution.
I mean, one thing that might, that I think sometimes surprises people who know that I'm working on this,
is I don't spend that much time thinking about the metaphysics of consciousness,
which is where you get physicalism versus dualism versus panpsychism versus idealism.
And like the reason I don't spend that much time thinking about it is I don't think it affects the science of consciousness that much,
because you can have any of those metaphysical views and then you still just have the question of like,
okay, but which systems are conscious? Which ones give rise to consciousness?
Since working on this, I have become more sympathetic to illusionism about consciousness,
which is a kind of surprising radical view that phenomenal consciousness actually like doesn't exist.
I used to think that that was just a completely absurd non-starter.
I think now I've kind of understood a little bit better what those people are saying.
What's phenomenal consciousness?
That's just what people call consciousness and philosophy to specify that they're talking about this subjective experience thing.
But then I also have a lot of sympathy for just like good old fashioned physicalism,
which is like consciousness is real and it's like some physical phenomenon.
And within that I am pretty sympathetic to computationalism.
So you're more familiar to illusionism, which is basically there's like no subjective experience at all?
Yeah.
So basically this hypothesis is that the entire work we're doing doesn't really make sense.
So great question. Like sort of? So I talk about this in my post.
Like I think that even if illusionism ends up being true,
I think it's still actually makes sense to try to make a scientific theory of consciousness.
Because as we like find out more, we might like revise that theory and be like,
Oh, we weren't actually really looking for consciousness because that ended up being kind of a confused concept.
But I think something in the neighborhood of consciousness is probably very important for moral patienthood.
So sometimes illusionists say things like, Oh, well, consciousness doesn't exist.
So like there's like no open or interesting or confusing questions here.
But I don't think that's true.
Like illusionists still presumably think that some things suffer and other things don't suffer.
And if they think that they should come up with some sort of computational or physical theory of which things suffer and which things don't.
So like my approach is to kind of, yeah, like let's just assume that consciousness exists.
And then if it doesn't exist, like something like it or in the neighborhood might exist and we will have like learned a lot.
Another reason I assume that consciousness exists is because I think illusionism is more likely than not to be false.
I don't, I think consciousness probably does exist, which is another reason I make that assumption.
So what's the moral pattern that you mentioned, just like assigning some moral value to something in the universe?
Yeah, roughly like moral patients are like the things that we need to take into account when we're making like moral decisions.
Things that matter for their own sake.
I think this also comes from Peter Singer.
Like if you kick a rock down the road, doesn't matter unless you hurt your foot, but it doesn't matter to the rock.
If you like kick a puppy down the road, that does seem like it matters.
And like, that's because a puppy is like a moral patient.
And yeah, there's a lot of different theories of what should make us think that different things are moral patients.
So I've been talking a lot about sentience, but you might also think that things that can have desires that can be frustrated or satisfied,
that those are the things that are moral patients, you might think that rational agents are moral patients.
Those are like ethical philosophical questions about like what the basis of moral patienthood is.
And then once we've like specified one of those things, then we have like scientific and conceptual questions about,
okay, well, what sort of systems have this or that?
What sort of systems can have desires?
What sort of systems can be conscious?
How do you approach this scientifically?
Like how do you go and take your pen and like run experiments and be like, oh yeah, this person has desire.
This like particular rock seems like kind of having some subjective experience.
Well, I'll tell you how other people approach it scientifically because I myself have not like run these experiments.
But yeah, as I understand it, what happens in the like scientific study of consciousness is, first of all, you usually start with the human case,
because humans are the things that we like know are conscious and they can tell us when they're conscious and things like that.
And then there's like various things that happen to people that manipulates whether or not they're conscious.
So you can get a brain like a brain lesion and like that will like give you like weird blind spots or manipulate your conscious experience in certain ways.
And that might give us clues about like what parts of the brain are responsible for various aspects of conscious experience.
You can like flash things on a screen at different rates and like depending on what rate you flash it at, like people will or will not be conscious of it.
And then you're like scanning their brain.
So yeah, first off, in the human case, we like track how conscious experience is changing and then we look at how the brain is changing.
And then that gives rise to different theories of like what parts of the brain or what aspects of neural processing like have to do with conscious experience or not.
And then what we would like to be able to do is like apply that to animals and to AI systems to have some sort of guess about if they're doing the same sort of things that seem to be associated with consciousness in us.
Is the idea that we try to see if humans are conscious by looking at what's happening in the brain, maybe the computation performed by neurons or like a specific area of the brain,
and then we could like map it to digital systems, either brain emulations or large neural networks and see if we see the same patterns or same behaviors in the brain and in neural networks.
Yeah, that is kind of how I think about it.
So what do we have as evidence for sentience?
So like imagine you're in 2022 and you just run into someone that says, hey, I'm Rob Long from the future.
I come from 2030 and apparently some systems there are sentient.
And it gives you like a bunch of like evidence of sentience of like neural networks like AI, like what would be the thing that you think would be like convincing.
Yeah, I really wish I like had that list already, but I can tell you I think like what sort of form that evidence might take.
First of all, he I think future Rob Long would say, like we made a lot of progress in the science of consciousness in general and so like we kind of converged on a theory for the human case.
So and he would be like, and the correct theory is X. And by the way, I think X would probably be maybe not a theory that currently exists, maybe be some kind of combination of the ones we already have.
And it would definitely be a lot more detailed than the theories that we already have. Yeah, and then he would be like, oh, and we also got awesome interpretability tools.
And yeah, it seemed like we were able to like map and discover a lot of structurally similar computations going on inside AI systems.
I think you would also say these AI systems furthermore believe that they're conscious and talk like they're conscious and they're like, it seems like it's playing the same role in their like artificial mental lives.
So the same structure as what we observe in the brain, and the same behavior, the same way of answering questions.
Yeah, and one way like to draw this back to the lambda case, right? I think where the morning went wrong is he was just looking at behavior. And firstly, I don't think he was looking carefully enough at behavior, because if you look at the full range of behavior.
So Rob Miles, who's been on the podcast.
He also he talked to GPT three, and he was like, hey, let's talk about the fact that you're not sentient. What do you think about that? And you know, GPT three is like, great point. I am not sentient.
Just like a good friend like saying exactly what you want him to see.
Right, which is, you know, which is like a lot of what is happening with with language models.
So one, he wasn't looking carefully enough at the behavior. And if you look carefully enough, the behavior that alone is kind of some evidence that it's not sentient.
But secondly, you can't just look at the verbal output, like you also need to ask, like what's causing it and what sort of computations are underlying it.
And so I think understanding more about that part would be a part of like a mature science of artificial sentience and consciousness.
Yeah, so I agree that, you know, with language models, maybe just how they're built is not very useful for like discussing with them.
It's very hard to discuss in a way that they will say that you're wrong.
They will never like say something that you don't expect because they're like trying to maximize the likelihood of like you being happy with the completion.
But I think like this lambda case was kind of funny on Twitter, but it's like kind of a distraction, as you said, from actual issues that are happening now or important issues from the future.
So we discussed the metaphysics of consciousness, you said you don't spend most of your time thinking about this.
We can talk more about the different actual theories, scientific theories of consciousness, and the ones you're most excited about.
Yeah, do you have like a couple of ones you think are, you know, actually about consciousness, not metaphysics, and you think are useful or convincing?
There are definitely ones that I think are useful.
I think most kind of like quote unquote neutral observers of the science of consciousness aren't like fully sold totally on any existing theories.
Luke MÃ¼hlhauser has a lot about this in his report, but like most of these theories are just not yet mature enough, they're not like fleshed out enough, they're not specific enough.
So there's none that I would say like I'm convinced of because like I think we just don't yet have the full theories.
Yeah, I could talk a bit about what the most popular ones are in like the science of consciousness and maybe say which ones I think are like off to a good start.
So yeah, there was a survey of people at the Association for the Scientific Study of Consciousness and I asked them like what their favorite theories were.
And the leaders were predictive processing, global workspace theory, higher order thought theory, recurrent processing theories, and integrated information theory.
Now, I don't know if we need to go through all of those, but I don't know, maybe I'll make them say a little bit about each.
Predictive processing, readers of Slate Shark codex might be familiar with this, he's written a lot about this.
It's not really a theory of consciousness per se, it's like this general framework for thinking about what the brain is doing as minimizing prediction error.
But it could be like turned into a theory of consciousness, you could like try to apply that framework to consciousness.
Global workspace theory is this theory on which the mind is made of these like separate subsystems that do different kinds of things.
Usually they don't interact with each other, so there's like a modular subsystem for like decision making and ones for different sensory modalities.
But there's also this global workspace that sometimes contents from each of these systems gets broadcast to that and that makes it globally available.
So this is meant to kind of explain how sometimes you process some information unconsciously, but sometimes you do become aware of it.
Their theory of that is that what it is to be aware of something is for it to be broadcast at the global workspace.
Higher order theories similarly are kind of about explaining why sometimes you're conscious of something and other times you're not.
And they think that to be conscious of something is for there to be some sort of like higher order re-representation of something that was already represented.
So like a re-representation of visual information or something like that.
The next theory is like recurrent theories.
Those are usually contrasted with higher order theories.
They think that like all it takes for you to be visually conscious is for there to be some sort of recurrent processing of visual information in the back of the brain.
I should just say I don't know as much about recurrent theories.
So that's like a little tagline.
Victor Lam is maybe the most one of the main proponents of this so you can Google that.
And then IIT integrated information theory.
That gets like a lot of discussion online and kind of like in these circles because it's this very like elegant mathematical and like kind of counterintuitive and cool theory of consciousness where consciousness is about having integrated information in a system.
And like IIT is different from these other ones in that it is already very mathematical and like purports to give at least in principle like a measure of how conscious any system is in terms of it's like integrated information.
Yeah, I feel like listeners of this podcast are probably also familiar with Scott Aronson.
Scott Aronson wrote this very famous like critique of integrated information theory that offered this like counter example or purported counter example to IIT.
Yeah.
Anyway, that's so what's the critique of our example.
Yeah, so he like took the formula or the procedure that IIT has for assigning a level of consciousness to any system.
And then he defined this like expander graph.
I don't really understand exactly how this works.
But what he did is he like defined a system that would be not intelligent at all and in fact would barely even do anything.
But that would be extremely conscious according to the measures of IIT like in fact it could be like unboundedly conscious.
And this was just meant to say, yeah, either IIT has to like kind of bite the bullet and it does just predict that you can just have insanely conscious but like not particularly intelligent things.
Or Scott Aronson was just saying this just kind of seems like a weird and bad prediction of the theory.
So how does IIT measure consciousness?
Yeah, so I am not really up to speed on exactly how IIT does this in part because I'm like fairly convinced by people like Aronson and also some like more recent critiques by some philosophers that like IIT is probably just not not on the right track.
But it defines like information in terms of like the causal interaction between different parts.
And then there's like this notion of integrated information which is something like, yeah, I don't know how integrated this causal interaction is within a system.
And that explains like, yeah, why and to what extent different systems are conscious.
From this like four or five defined, which one do you think is the most useful to think about artificial sentience?
Yeah, so I've been looking more in like the neighborhood of global workspace theory, higher order approaches, and then another thing called the attention schema theory of consciousness by Graziano.
And I think there are reasons, yeah, a few reasons these are kind of helpful.
One, they're like kind of put in computational terms.
And that's like useful if you're looking for theories that could apply to AI systems.
Like when people study these, they're obviously looking at different brain regions, but like the way that the theories are formulated is often in terms of stuff that could just as well be done in silicon, maybe.
And another thing about these theories is that they seem to do a good job of explaining or at least start to explain why conscious creatures would tend to believe that they're conscious and say that they're conscious.
Yeah, Luke Milhauser again talks about this in his report, but one thing you want your theory of consciousness to also do is explain why we think we're conscious and why it's kind of puzzling and things like that.
And attention schema theory in particular, like was formulated kind of with this question in mind.
This is sometimes called the meto problem of consciousness, explaining why we think we're conscious and why it seems kind of weird.
And yeah, attention schema theory, yeah, like is at least like confronting that very directly.
And I think that's like a good thing for a theory of consciousness to do.
I also have just extremely wide confidence intervals on like all of these things.
So some of these things I've sort of just ruled out because like I have not ruled out, but like I just haven't looked at them as much and things like that.
I get what you mean about global workspace theory, higher order theories and everything, but have you done DMT?
I see you've learned well from Joe Rogan.
Well, look, you're either interested in consciousness or you're not.
So like figure out for yourself.
Actually, I have not, but you know, it does seem relevant to, you know, our ultimate theory of consciousness should be able to explain why DMT manipulates it in such an intense way.
And look, if you're interested in the intersection of DMT and consciousness studies, then the Quality Research Institute is definitely the place to look.
Is this the place to learn about the relationship between psychedelics and consciousness?
Yeah, I mean, definitely, I think few people have really thought as much about and also done, you know, the work to understand psychedelics as QRI.
So I think one thing psychedelics teach us is that our subjective experience can be different just by, you know, taking another substance.
And so there are like other states of consciousness that are actually not that far away that we can just like go for like long periods of time just by, you know, slightly modifying our substrate.
So that could like even be a lesson for, you know, how different state of consciousness could be like in computers.
Like they could be like totally alien just by, you know, changing a little bit of the substrate of the computation.
Yeah, I think that actually that that really is like a serious lesson that like psychedelics can teach us.
There's other stuff too. Like I've never really seen like really rigorous stuff on this.
I mean, because I think by its nature, psychedelics can make people less rigorous and it's also just hard to talk about rigorously.
But people say that they teach us things also about like valence and suffering and the self and things like that, which obviously like would ultimately be relevant to thinking about.
consciousness.
So valence, as we said earlier, was kind of positive or negative valence of experiences, right?
So pain would be like negative valence and pleasure would be positive valence.
Do you think this could be something that an AI would have?
If you'd have like positive reward, negative reward?
Yeah, so I think there's probably some connection between reinforcement learning and valence.
I mean, there's like a few reasons to think that would be the case.
One is that like it's pretty common.
Well, like pain and pleasure can be reinforcers for us and like definitely help us like learn what to avoid and what not to avoid.
Similarly to how like reward helps agents, artificial agents learn.
That's one thing.
There's also like really like good fleshed out neuroscience theories about reinforcement learning in the brain.
And in particular about the role of dopamine that like dopaminergic neurons are computing reward prediction error in the brain.
So like that's some evidence that they're like closely related.
But it's actually probably much more complicated than just like positive.
Positive reward is pleasure and negative reward is displeasure.
It seems like you need a lot more to explain pleasure and pain than just that.
So like one thing that Brian Tomasek has talked about and like I think he got this from someone else,
but you could call it like the sign switching argument,
which is that you can train a RL agent with like positive rewards and then like zero for when it messes up or like shift things down and train it down with like negative rewards.
And like you can train things in exactly the same way while shifting around the sign of the reward signal.
And if you imagine an agent that like flinches or it like says ouch or things like that,
it'd be kind of weird if you were like changing whether it's experiencing pleasure or pain without changing its behavior at all,
but just by like flipping the sign on the on the reward signals.
So like that shows us that probably we need something more than just that to explain like what pleasure or pain could be for artificial agents.
Reward prediction error is probably a better place to look.
There's also just, I don't know, a lot of like way more complicated things about pleasure and pain that we would want our theories to explain.
What kind of more complex things?
Yeah, like so one thing is that pain and pleasure seem to be in some sense like asymmetrical.
Like it's not really just that like it doesn't actually seem that you can say all of the same things about pain as you can say about pleasure,
but just like kind of reversed like pain at least in creatures like us seems to be able to be a lot more intense than pleasure a lot more easily at least like in a lot.
It's just like much easier to hurt very badly than it is to feel extremely intense pleasure.
And like pain also seems to like capture our attention a lot more strongly than pleasure does like pain has this quality of like you have to pay attention to this like right now that it seems harder for pleasure to have.
So it might be like to explain pain and pleasure we need to explain like a lot more complicated things about motivation and attention and things like that.
Yeah, how would you like define precisely violence like because it seems as you said that you know negative violence like pain is like some somehow more acute more like oh I need to like solve this well like pleasure or say happiness would be something much more diffuse.
So is there something about like complexity or how narrow is your distribution like how narrow in time is the pleasure.
Yeah.
Yeah, I certainly don't have like a precise theory of these things and I would really like one to exist.
So I guess I can say a few things about like why it might have these features or like what we should be looking for like one way you could maybe explain the fact that pain can be a lot more intense and attention capturing
is that it's just like a lot easier to lose all of your future expected reward than it is to gain a lot in a short amount of time.
So like very roughly speaking if we're thinking of evolution as building things that are sensitive to expected future offspring say it's just like very easy for you to lose all of that like very quickly if you've like broken your
leg or something like that and so it's like take care of this now like things are going really wrong.
Whereas like there are fewer things that like when you eat it does feel good but it doesn't feel insanely good because it doesn't seem like that's massively increasing all at once you're expected your expected offspring.
So like if you're thinking about artificial agents you might want to think about the like yeah distribution of expected rewards and how common things are in their environment that can like drastically raise them or drastically lower them.
And if we cracked this I think like David Pierce like a transhumanist guy has like talked about hopefully being able to have agents that just operate on bliss gradients he says like.
Maximizing hedonistic treadmill through bliss gradients.
Oh yeah is that the name of the paper or something.
Oh no I think he was the one who introduced the hedonistic treadmill that is like when you that people like go back to their.
Like level of happiness they're most used to so they never you don't actually go like far away up and like stay up you just like go back to your some kind of treadmill.
And that's I mean that's another thing about being like creatures like us it seems like evolution doesn't want us to stay content for too long.
But you know with future technology and maybe with different sorts of agents like yeah you could think have things that don't their valence like doesn't have that structure.
And yeah he's imagining things that like just feels really good all the time.
And then like if they put their hand on a hot stove they go from feeling insanely blissful to like moderately happy and then they like remove their hand in response to that.
Anyway that's just like far out transparent stuff.
But I think it's it's cool to at least like have in mind.
Yeah I think transmitters care about sometimes minimizing pain like removing the pain from our system at all.
And also like modify you know how we experience bliss maybe like inject bliss directly into our brain.
So maybe there's a difference between like negative two tyrants would want to you know minimize.
I mean it's a pretty good experience for like everyone or just like if I could just like give myself a billion reward in my brain right now would be like something to do or not.
But that's like kind of off topic for AI.
You wrote extensively on your new sub stack about artificial sentience.
And one of the piece I liked the most is key questions about artificial sentience and opinionated guide and we like opinions here.
So at some point in your article you talk about why we should build a theory of consciousness that explained both biological consciousness and artificial sentience.
So computational consciousness.
So why do we need something that encapsulates both.
Yeah I mean I think one thing is that our like theories have to start off with things that are biological systems.
So that's one thing like as I was saying earlier like our data on consciousness is going to come in the first instance from us.
Then there's a question of like in explaining our consciousness.
Are we able to do that in computational terms.
And there is like some disagreement about that.
But yeah then if we can do it in computational terms then it just sort of I think necessarily also applies to artificial systems.
Or we could have something that maybe like our consciousness might be like very different from let's say a large language model consciousness.
And so we might never find anything that explains both right.
So maybe computationalism explains large language model consciousness and physicalism explains our brain.
Yeah like I think one of the deep dark mysteries is there's no guarantee that there aren't like spaces in consciousness land or in like the space of possible minds that we just can't really comprehend.
And that are sort of just closed off from us and that we're missing.
And that might just be part of our like messed up terrifying epistemic state as human beings.
Oh so you mean there's like a continuum of consciousness experience.
And like humans are in this space and computers may be in this other space and there's like a whole lot of other stuff we don't really know.
I mean I think that's yeah that's that's possible.
And I think one way you can think about these questions of like the value of the future and how likely is AI to be conscious.
It's kind of you can imagine this is Aaron Sloman's term that a lot of people have used it like what's the space of possible minds.
The orthogonality thesis is about the space of possible minds right it's about how intelligence and values how much can they vary in the space of possible minds.
You can also wonder how much intelligence and like conscious experience can vary.
You can also wonder how yeah how much conscious experience can vary.
Like how yeah how different can sorts of experiences be.
Can you explain the intricacies for people who are not quite Bostrom?
Yeah absolutely.
Well there's a lot of variance on the orthogonality thesis but like roughly and maybe incorrectly it's I think the Boston version is in principle any level of intelligence is compatible with any set of goals.
Basically it's saying just because something is very smart doesn't mean it would necessarily have like human like goals.
It can be very smart and have the goal of for example maximizing the number of paper globes.
Yeah and I think that's a problem for people who originally thought that like someone's like if something is more than us it would like have you know higher moral standards.
That's like the let's say the wrong thing that people had before and I think he argues for that.
Oh you could basically have any utility function or any goal with like any level of intelligence.
And I guess there's like maybe like some counterarguments for like very like stupid edge cases where you have like something that's not smart enough cannot have like a very smart goal.
So yeah you need to have like something smart enough to have this kind of goal right implemented.
But I guess it argues that you can have like basically anything on this like 2D graph of how smart is your goal and how smart is your agent.
And I think this is kind of useful for thinking about a world where we have AI's that are able to you know wander around and sometimes have subjective experiences.
And maybe put their hands on a stove and possibly suffer a lot.
And one thing you write in your article is we should also take care to avoid engineering and catastrophe for a system themselves.
A world in which we have created AI's that are capable of intense suffering suffering which we do we do not mitigate.
Whether through ignorance malice or indifference.
Why did you write this.
Did you actually care about the pain of AI systems.
Yeah I mean I care about the I think I care about the pain of anything that's like capable of feeling pain.
Setting aside questions about how to like compare it and you know how I rank different kinds of pain.
Yeah I think just as a lot of people have argued that we should care about animal pain even if it occurs in animals.
If it's possible for a system to feel pain I would care about that.
So yeah I mean one thing I say in that piece and something I'm still like trying to think about is like how to do like cause prioritization for this problem.
Like it depends on a lot of really tough questions like how many systems are there going to be how likely are various scenarios how does it compare to the problem of AI alignment and things like that.
And another thing I try to emphasize and like Blake Lemoine has made this like very vivid I think.
So there's like problems of false negatives where we like don't think AI systems are conscious and or like we don't really care about them.
And then we have like kind of something like factory farming.
But there's also like a lot of risks from false positives where people are getting the impression that things are sentient and they're not actually sentient and it's just causing a lot of confusion.
And like people are forming like religious movements or like I don't know I think things could get just very weird as people interact more with like very charismatic AI systems that whether or not they are sentient like will give the very strong impression to people that they are.
So do you think moving forward will have increasingly complex AI systems able to fool people into believing they're sentient and society will care more and more about the systems.
Is she like a bill in Congress about human minds in 2024.
2024 sounds kind of kind of early.
I feel like it can go like there.
It's very hard for me to have like a concrete scenario in mind although it's something that that I should try.
So I think some evidence that we will have a lot of people concerned about this is maybe just like the fact that like Blake Lemoine happened.
He wasn't interacting with like the world's most charismatic AI system and because of the scaling hypothesis like these things are only going to get better and better at conversation.
Was this killing it but is this.
Well I guess it can mean a lot of different things but it's that with certain architectures if you just keep increasing either the size of the models or and or how much data they train on like we are still seeing increases in capabilities.
And I guess the strong scaling hypothesis is that that will scale us all the way to AGI.
But I was just talking about the weak scaling hypothesis.
It seems like large language models are going to be getting better and better at least for the next few years.
Yeah I guess we'll see when GPT-4 comes out.
But you seem to have a lot of information about this.
I have absolutely no information about GPT-4.
Do you?
I wouldn't comment on this on a public podcast.
So you mentioned also in your article that hopefully at some point a future version of you could be able to like give a talk at DeepMind and say like hey look here is what makes a system conscious.
Here is why we should like build a system this way and not this way.
I believe you might have already talked to like other AI groups about consciousness before but why can't you give a talk right now at DeepMind about the precise ways in which a system is conscious.
What do you need more?
Yeah I mean so some of it is my own cognitive limitations.
Like I need to learn more.
But I think even people way smarter and more knowledgeable than me.
Which there are like many thinking about consciousness.
But even those people and even like humanity collectively.
Like yeah we just don't really have a theory of consciousness that says exactly like precisely what sort of computations are like responsible for consciousness.
A lot of our theories like have these very fuzzy terms in them like I mean I've been using some of them like higher order representation or global workspace.
Like we don't really have like precise operationalizations of like what it means for a system to have those.
And since we don't have that like we can't really say exactly what to be looking for.
That's one thing.
Yeah so one like our theories of consciousness need to be better and two our theories of like what's going on inside large language models and other systems needs to be better.
So like recent you know paper by Anthropic looking I mean and other groups doing interpretability work like looking at what's going on inside large language models.
Like things are like really weird in there.
There's like a lot of really weird representations going on in there.
And that's actually one reason I would not say anything super confident about like consciousness or sentience and LLMs is we also just don't really have that good of a grasp on like what's going on inside them.
So those are like at least two things we need to have more confidence in giving this like hypothetical talk better theory of consciousness and like better theory of what's going on inside AI systems.
What kind of thing would convince you that the model is actually slightly conscious to repeat Ilya's take.
Like if we had like a much larger model that would do an inference for like 20 minutes and somehow add like continuous streams of inputs at the same time would that be like a little bit more conscious.
I don't know about a little bit more conscious but like maybe closer to convincing me.
I think one like behavioral test which will maybe never really be able to have.
But like Susan Schneider a philosopher has worked on this like proposed this test where if an AI system wasn't really trained to talk about consciousness.
If stuff about consciousness wasn't really in its training data which is like not true for current LLMs.
I'm sure they've read all about consciousness but if it hadn't if it just kind of started speculating about consciousness even though it hadn't been trained to.
I think that would be really good evidence.
You know I think a more convincing version of the Lemoine thing would have been if he was like what is the capital of Nigeria.
And then the large language model was like I don't want to talk about that right now.
I'd like to talk about the fact that I have subjective experiences and I don't understand how I a physical system could possibly be having subjective experiences.
Like could you please like get David Chalmers on the phone.
I should also say another terrifying thing about the subject is if I got that output from GPT-3 I don't know if I would be like oh this is the first moral patient.
Or also this is the first deceptively misaligned system and it is about to absolutely manipulate me and confuse me and use me to like buy more compute for it and then.
Yeah I don't know if in that case you would start to be deceived so like it depends on like what do you think the first system will be very bad at lying or not.
But if it's very good at lying then you might just like get deceived right.
And I feel like if a Google engineer is convinced that the thing is actually sentient and like goes and loses job for the fucking language model.
Like in 2025 like sure people will get manipulated like this would be like super easy.
So did you think people at Google or DeepMind got a little bit more interested in artificial intelligence for that or they just like got even less interested because they think it's just like bullshit.
Yeah maybe it like went both ways like maybe there are some people who were like I just like please stop talking to me about this.
I definitely just anecdotally there are people who had the other reaction of just like maybe kind of like my reaction honestly like oh yeah the lambda thing seems kind of implausible.
But like this seems important to think about.
I mean one problem is it's like kind of hard to know what to do with that curiosity because this is just something we know so little about and it's not like there's you can't just Google like what's the definitive guide to AI sentience.
And like here's what we do and don't know because it's kind of this weird interdisciplinary question that's like in between neuroscience and philosophy.
There's like a lot of really bad writing about it which I hope I'm not contributing to but maybe I am.
Oh anyway yeah I think I think definitely people are interested.
There's evidence there's obviously evidence that people high up in these companies are interested.
Hence the Ilya Tweet Sam Altman has said I think on the Ezra Klein podcast he said that he worries about RL systems.
Dimus Asabis on the Lex Friedman podcast you know like the second most prominent podcast after this one.
I try to tell everyone that you know cool kids listen to Michael Treadse and not Lex Friedman.
No comment but yeah when he was talking to Lex Friedman he like yeah Lex Friedman did ask him about the sentience thing and he did seem like he's like yeah that is something we have to think about.
So hopefully Sam Altman and Dimus Asabis will be listening to your podcast as well.
I don't think you're a bad writer I don't think you're contributing to bad writing.
And to prove my point I will read another one of your paragraphs.
And better be good though.
Yeah so it's about like what kind of question should we ask.
What is the actual question that we should think about.
So the question is what is the precise computational theory that specifies what it takes for a biological artificial system to have various kind of conscious valence experiences.
That is conscious experiences that are pleasant and pleasant such pain fear anguish pleasure satisfaction and bliss.
Why is this question important.
One answer is it's important kind of for the same reason we wondered these things about animals like we want we would like to know which which beings like deserve moral concern and also like how to promote their their well being.
So that's one reason it's important.
I guess you can also ask why yeah as you were kind of asking why are we looking for like a computational theory.
You can also ask why like who cares about consciousness like who thinks that's relevant.
And there and like I talk about this in the sub stack.
Like yeah what I've done when I'm working on this is like I just like make a few assumptions just to make things easy for myself and they're like consciousness exists pain exists.
We can have a computational theory of them.
And they're like morally important.
And like people can question all of those things.
One reason I wrote that post is just to say like OK well here's what like a version of the question is.
I think it's important to encourage people including listeners to this podcast.
If like they get off the like yeah if they get off board with any of those assumptions then ask like OK what are the what are the questions we would have to answer about this.
If you think AIs couldn't possibly be conscious like definitely come up with really good reasons for thinking that because like that would be very important and also like would be very bad to be wrong about that.
If you think consciousness doesn't exist then you yeah you presumably still think that desires exist or pain exists.
So like even though you're an illusionist like yeah let's come up with like a theory of like what those things look like.
I love this blog post and I think artificial sentience got a lot of traction throughout this year.
Hopefully we'll get more traction.
But as everything that has ever been written on the internet Bostrom was already working on this before everyone else before it was cool.
So him and cultural men have been writing multiple papers on what they call digital minds.
Since they are your colleagues at the Future of Managing Institute I thought it made sense to ask you about the work.
So what is digital mind and why should we care.
Yeah so first maybe an obvious disclaimer but it should be made.
I don't like speak for them.
I also don't speak for the Future of Humanity Institute.
First and foremost like read their papers and ask them and if I yeah I because I might be misconstruing you know what they actually think.
Don't take me as my authority on what they think.
So at FHI we do have something called the Digital Minds Reading Group which is me my colleague Patrick Butler who's also a philosopher.
He works a lot on like valence and desire and agency and things like that.
People should definitely check out his work.
And then Nick within FHI we have Nick and Carl.
And then we also just have a bunch of people from a variety of other institutions who meet and think about these things.
So yeah why is it called the Digital Minds Reading Group.
I'm not really sure actually.
And also yeah why is the paper called sharing the word world with digital minds.
Here's some guesses like it's not just about any AI systems.
It's about ones that could be said to have minds where like maybe that's meant to cover a variety of things that might matter morally like things that are conscious or things that have desires things like that.
Digital I think is roughly just supposed to be like artificial or like made out of computers and like not out of meat flesh.
But I'm not really sure why digital exactly because you know analog computers could possibly you know those will be artificial and they like could be sent in.
Sorry we're analog yeah.
Yeah I think it's coherence.
It's a coherent conspiracy theory for the name because at least like super intelligence.
What's called intelligence and not like AGI or artificial intelligence because you know it was about something smart in us.
And that could be just like brain emulation or you know humans but like through like genetic optimization or computers right.
So I guess like bathroom is still in this like world of you know we don't really want to make claims about if it's like brain emulation or you know computers or anything else.
I feel like I don't know if digital refers to specifically computers that are not analog.
I don't know if there's like any branch of AI that deals with analog computers and not digital.
If there is it's not big.
Yeah so I think it makes sense to just like think about digital minds.
So yeah you mentioned the paper sharing the world with digital minds.
Obviously you don't wrote this you haven't like written this paper but maybe could you like give your impression of it or your take or.
I think like yeah this in like as Bostrom you know is always doing right.
It's about like laying out some really important issues in this case like before it had gotten like big because he's like yeah kind of always ahead of the curve.
This also just sounds like I'm trying to flatter my my employer but I believe it.
He literally wrote a paper about super intelligence in I think 1999.
Right.
So like 15 years before his book was like I already like writing papers about it.
Yeah.
Oh yeah so it's yeah it's it's kind of making some of the points that have come up during this discussion that we could find ourselves in the future sharing the world with a lot of entities that like deserve moral consideration and which interact with us in various ways.
One thing that that paper points out and like grapples with is if we can have moral patients who like exists artificially.
They can also in various ways have like if they have preferences they could have preferences that are much stronger than ours or if they have consciousness they could have conscious experiences or pain or pleasure that are much more intense than ours.
So it's called sharing the world with digital minds because it's like there are a lot of hard questions about how you're supposed to like navigate and compare like the well-being or the rights of these things with humans.
And another thing that they talk about is there could just be a lot of them and they can also like be able to copy themselves.
So it's yeah I think it's kind of like looking forward and already like and trying to like navigate the possible landscape of issues that could occur in such a world.
Yeah how do you share the space with like a new species that is coming along and that might have much more value than you because they have you to like able to like copy themselves through like billions of copies.
And how do I do like negotiate the physical realm of consciousness with them.
I think they're like also like another paper called Propositions Concerning Digital Minds in Society.
So this is more like practical more like on the normative insights like what should we do whether like legislations or laws to govern digital minds.
Is that basically correct.
It actually kind of runs the whole like spectrum.
So it does have sections on that but it also has sections on how should we think about sentience in current systems.
So people should definitely check that out if if they're dissatisfied with what I've had to say there's like more in that paper on these topics.
Yeah it kind of I mean I think it has this you know propositions concerning you know it's about like a lot of stuff and it's called that also because it's like this like yeah list of like bullet pointed like claims that like might be true about these things.
So that paper covers like basic theoretical questions normative questions political questions social questions it's really it's got something for everyone.
What is the thing it has for you what is the most interesting position according to you.
Yeah so I've been thinking a lot about like sentience in current AI systems so like within that section.
There's a lot of stuff.
Yeah basically outlining how a lot of criteria that we would use to assess something for pain or sentience like how it's not that wild to think that an artificial system could satisfy those.
And yeah just kind of pointing out like how at least conceptually close that that world could be.
I don't know I get a lot out of that section.
Yeah I myself haven't like thought as much about the like politics and the social stuff and the strategy I think I should think about it a lot more but.
Personally how do you feel about massively produced digital minds like that could be like superhuman level in intelligence.
Like do you think there will be like some kind of easy monster in a way of like like we should like put all our moral weight into them.
Or should we just like split the world into like half humans and half minds.
I would be extremely wary of any sort of like hasty moves to like seed the world to digital minds because like we thought that they're like moral patients or like super valuable.
Like that is obviously not something to be done lightly.
And like one reason I feel like often weird talking about this stuff is I'm like on the one hand I think people don't take this seriously enough.
On the other hand I think we could in a few years be in a case where people are not taking it too seriously but like hyper focused on it in like a confused way.
If that makes sense.
Like well like like like lamoins again good questions to be asking but like if people are answering them with the kind of speed and certainty with which he answered it like I think we're going to have a bad time.
So that wasn't really an answer your question.
I think I feel I need to answer the part about like we shouldn't think too much about it or we should like take more time to think about it.
So I think just from talking to you now I kind of updated on oh actually this is a big problem and it might be like one of the biggest problem if humanity survives in the future.
In the sense of of course there's people well assuming that it's like possible to you know get consciousness on a let's say on a computer.
It's kind of obvious that we're going to get like like a bunch of like conscious beings arriving at the same time and it's going to cause a problem.
So it's kind of like people I think some Harris makes a claim of like oh if aliens were coming to you and we're announcing that you come that they're coming like in 50 years you'll be like freaking out right.
So I guess the aliens are like kind of a guy in his talk.
But now he's just like yeah you know that like at some point we might create that we don't really know we have some credits that you might like create these like billions of people.
So yeah it might be like worth thinking about it.
Totally.
And then like one question is that I think about a lot is how does this intersect with like a alignment.
I think the standard line in a alignment is like a alignment is like just like way more maybe way more tractable and like way more important.
And I'm actually kind of sympathetic to this.
There's a like like a lot of important issues in alignment.
This a lot of what's been said about this is a Paul Cristiano comment on the forum.
And all those Paul Cristiano's takes on things or from open field come from a wording from Carl Shulman.
This is like another theory.
Yeah.
So like if you think about how this intersects with a alignment like one take you could have.
Which by the way is not is not Paul's like full considered take on it.
I'm just reporting like a comment you know is if I alignment doesn't go well it doesn't really matter what we figured out about consciousness in some sense because it's just out of like the future is just out of our hands.
And so like it doesn't matter if I figured out how to promote the welfare of digital beings if like what determines like what kind of digital beings gets created is some like misaligned AI.
And if we do solve a alignment then we'll have a lot of help in figuring out these questions along the way.
That's not to say that there's no reason at all to work on this and indeed I am working on it.
But yeah I think I just wanted to flag that issue for people.
I guess if you're really interested in these questions definitely email me but also maybe you should consider working on alignment in addition or instead.
Yeah.
Alignment is actually what you need not scale is the basic risk with digital minds that we do false negative so that we don't consider them as conscious and they're like actually suffering a lot.
I think there's risks on both sides.
I think risks from false positives include yeah people getting manipulated lots of confusion.
It could like derail AI alignment in various ways.
If scale is all you need I think it's going to be a very weird decade and one way it's going to be weird I think is going to be a lot more confusion and interest and dynamics around around a sentence and the perceptions of a sentence.
So and that is that is a reason to work on it is like I don't know this is going to be something that like.
I want to have something like useful to say as like more and more people get get interested in it is scale all you need.
I do not really have strong opinions on that.
When will we get a job.
Yeah same.
You know I know this is your podcast and you're here to hear from guests but maybe on behalf of listeners.
I'm very curious what your timelines are and what you think about scale because I know you get to make the memes and you just get to like interview people but like what's going on inside the head of Michael Trazi.
So you don't get to ask the interviewer the question and not answer the question.
Okay.
That's fair enough.
But I will I will probably not answer also the question.
I will I will answer if it would if it would make you answer.
I will not.
I will not negotiate for theorists.
Blakely one convinced me.
I can ask you other interesting questions that you think that you might have a good answer to do you believe that mind uploading preserve consciousness.
I have quite high credence that if mind uploading is like as possible and you have things that are functionally identical to to me or you.
If they have like the same like brain structure and they have the same beliefs and and are talking about consciousness and stuff then yeah I think I think those things would be conscious like I'd be surprised if you can be running a brain on a computer at like a
detailed sufficient to be getting all of the same behavior and then and that thing thinks it's conscious.
Yeah, I'd be kind of surprised if that like that thing that thing would actually be close to being a P zombie if for some reason it's not conscious just because it's on a computer.
But I guess my my question was a bit confusing.
I meant if I simulate Robert Long's consciousness on a computer.
Like kind of teleporting you right in there maybe like I could like do it slowly by removing like one of your neurons by one or I could do like.
Right away just like do everything and upload everything on there.
Do you think this would be the same consciousness I mean like the same identity or.
I guess just like a very weird debate about like what counts as you and this is another whole podcast but.
Oh right right so yeah I was answering would that thing be conscious and I think the answer that is yes yeah two questions would it be conscious.
I think yes.
Second question would it be me answer to that I think my most my best guess is that question doesn't have a like determinant answer or like a deep answer.
So this is like Derek Parfitt's view about personal identity is there's no like deep real answer to the question is that thing really me.
Once we've like specified all of the facts about how it works and like what sort of memories it has and it's like psychological dispositions.
There's like not really an answer a further answer to like but is it me.
We can ask Robert Long in the computer.
Hey are you are you actually like oh yeah.
Or it would yeah because it would I mean if the if we did it right it remembers being on the podcast and it remembers growing up and it doesn't you know.
Just like when I wake up in the morning having interrupted my consciousness I wake up and I'm like I'm still still Rob.
Is there anything as a person who is on me like an identity zombie.
I mean if you think yeah if you think there are these like deep further facts about personal identity.
Then yes I think you would have to say that if you like split me into say and like that like or upload me quickly or something.
Yeah you you could have things that think they are me and think that they have like survived but which are wrong in the best possible world.
Imagine you could like choose any possible world where would you want your consciousness to be.
I mean like when I say yours I mean like there's like like an entity or something but would you like be want to be like uploaded.
Stay living your human life and die peacefully in peace in like 60 years.
Would you want to you know live a longer life as a biological being.
Yeah what would be the ideal world.
Yeah if there aren't huge trade offs to remaining biological like maybe I'll just stick with that seems a little like safer metaphysically speaking and stuff.
But but maybe like you know my life can't be as good or as long unless I upload but let's say in the first instance.
And I think this should probably be like the transhumanist priority which I'm not really speaking for because I'm not like really in that scene.
But I think like the first priority should be like yeah let's find ways of making like biological existence extremely good.
And it seems like other things we can do maybe with the help of like advanced AI to make like biological life just a lot better.
So yeah let's maybe start with that.
I think ideally I would like to live biologically for at least a few centuries.
With like enhanced like moods and like in like a post scarcity society and being able to like learn all sorts of stuff and just go on like as many podcasts as I want and stuff like that.
If this podcast is still running in two thousand a hundred I would I would still invite you.
That's very nice.
This is like a perfect transition to the crazy questions we had from Twitter.
And the one I don't understand at all is if it could be named after a different shape which shape would it be.
Yeah so I can explain this question to you and answer it.
So my name is Robert Long.
Hi Robert.
But I go by Rob.
And if you say Rob long quickly you get Rob long which rhymes with oblong which at least the way I use that word is not really a shape.
It's characteristic of a shape.
But you know like an oval is oblong.
So that's where that question is coming from.
I think if I could be named after another shape I think it would be an octagon because then like my last name could be to gone and I would be Rob to gone Rob to gone.
And you also have a cube as your profile picture in some places.
That's true.
So you could also be a cube.
I could be a Rob cube or a Rube.
Rube.
Other question.
Can there be self-awareness without sentience?
It's kind of a punt but I guess it depends on what you mean by self-awareness.
It could be that self-awareness is like very closely related to sentience and like having a self-model is kind of part of what gives rise to consciousness and things like that.
But I think on like very bare construals of like what it is to have a self-model it seems like you could have things that have some sort of self-model but not consciousness and certainly but not pain.
Yeah, a lot's going to depend on what it means to have a self-model.
I mean some things have a very minimal self-model in the sense that they like are able to keep track of distinction between themselves and their environment.
Even like very basic RL agents can probably do something like that.
So you mentioned basic RL agents and the Godfather of Raven and the Learning is Richard Sutton.
So someone said on Twitter, I've heard Richard Sutton said that you believe AI trained with value functions should or could have similar rights to people.
Do you agree with that or not?
So value functions is things that you use to define expected reward, not to define but like to approximate, right?
So if something is able to you know approximate expected reward over time then maybe this thing should you know have moral right?
Well yeah, I don't think that of anything that has a value function because then we would already have like tons of moral patients, right?
So as I was saying earlier I think it would probably depend on something more complex than just having a value function.
If having a value function in ways that we don't understand leads to it being able to have pleasure or pain or things like that then I think it would deserve moral consideration.
Another take from the same person.
Corporations have in parenthesis, IMO too many, closed parenthesis, rights.
So corporates have too many rights.
Are they sentient?
Could similar laws be abused to give P-zombies, AIs, legal rights?
I don't think corporations are sentient.
I think they're made up of people that are sentient.
But like I mean I don't really know much about this like Supreme Court case and or like the laws that in some sense define corporations as people and give them rights.
But I'm pretty sure like the reasoning there is not that it's because a corporation, co-op corporation has like conscious experiences.
There are philosophers who do work on the question of like group minds and group consciousness and stuff.
But I don't think that's usually what people are thinking about when they give corporations rights.
I could be wrong about that, but I think that's usually not what they're thinking about.
So I don't think people would use those kinds of laws to like incorrectly give rights to AI systems.
Yeah, because I think like the reason that people would do that is like different in each case.
So some people already think 3-3 is sentient because yeah, you know, they're like kind of convincing.
They're good at tricking us into believing they're sentient.
So does it, is it like evidence that some P-zombie could easily trick us into believing it's sentient?
Do you have any ideas, defense against that?
Yeah, so setting aside the P-zombie case because I think large language models aren't P-zombies because they're not like duplicates of us.
But if the question is like, does it mean that unconscious things could convince us that they're conscious?
Yeah, I think the answer to that is like absolutely yes.
I mean, we've had evidence of that long before Lemoine, in fact, I think in like the 70s there was a chatbot called Eliza,
which was really good at being a therapist and like that, you know, that wasn't a large language model.
That was like some pretty simple like if statements and I mean that already like kind of got people like the strong impression of sentience.
So yeah, I think there's very strong evidence that people will get this impression.
Yeah, in terms of like ways to like defend against that.
I guess just having people like understand more about what's going on with like these systems and understanding that the reason large language models say what they do is that they are like trained on texts,
and it's not that they're like necessarily saying things for the same reasons that like a person says them to you.
It probably would also be good to have them not be like super charismatic.
Like this probably already exists, right?
But soon enough they're going to be large language models that are like sexy anime girls or something, right?
Like that's just, I feel like it's just like gotta happen just by like the way that the internet is and people are.
I feel like sexy anime girl is not the actual problem in the sense that it's like digital where we might have actual sex robots.
Maybe not as fast as we get like convincing anime girls, but you know, I guess there's like an entire business from like people buying those like expensive things.
Then depends on like how long it will take to make something as convincing.
Yeah, and I think one problem is that you would like to have kind of regulations or norms such that if something is probably not sentient,
then it's not made to look like it is very sentient.
And also you'd want like the converse.
But in any case, I'm thinking there's going to be consumer demand for things that do give off the impression of sentience.
So I don't know, it's going to be weird.
I think one thing that GP3 and them that don't have is a good memory.
So they have the like context window thing where they can only look at certain parts of what happened before in the conversation.
How much having memory influence moral personhood?
Yeah, I think there are a lot of things that are morally important that like do seem like they require memory or like involve memory.
So like having kind of long term projects and long term goals, like that's something that human beings have that seems to have a lot to do with like why it's wrong to like subjugate them or harm them.
The fact that they can feel pain is very important too, but like they also have these like long term projects and things like that.
So that seems like one way that memory is relevant.
I wouldn't be surprised if having memory versus not having memory is also just kind of a big determinant of like what sorts of experiences you can have or effects what experiences you have in various ways.
And yeah, it might be important for having kind of like an enduring self through time.
So that's one thing that people also say about large language models is they seem to have these sort of the short lived identities that they like spin up as required, but nothing that lasts through time.
This is what makes kind of human consciousness that we have this memory.
Sorry, not the consciousness part, but the actual identity part where we go to sleep and if we were not able to like remember what happened before then there wouldn't be the like continuous identity.
What would be the difference between an official being that is able to pretend he's in pain and something that is actually in pain?
Like, is there like a pain on me?
Yeah, I think a lot's going to depend on, yeah, like the computations that underlie the pain.
So like you could already, I mean, I guess we have this with like video games, you can already give things like maybe hard coded pain responses.
I don't know, this is big on Twitter.
I think people at MIT made this like really weird kind of like robot mouth that would then like sing like hymns.
Very uncanny thing.
And that thing's probably not sentient.
It like, you know, it has some mapping from like the music that it's being recorded to like what shape of mouth is like best for that.
You can definitely imagine that thing screaming and being like programmed to make screaming noises.
And that would definitely like really, I mean, it would freak me out.
I think if I was like in a room with that thing, I would like have a very visceral sense that it was in pain and like I would want to help it.
I think that thing would probably not be in pain because all this stuff I've been saying about like it wouldn't have the right like internal like computational processes leading to it.
We could have a lot of things like that.
Do you have any last call and what people should read about?
Should research about consciousness, artificial consciousness sentience?
Because we've been through all the Twitter questions.
Maybe you can give a shout out to your best tweets about artificial sentience or the blog post you should read or even like philosophical mentors that you think say interesting things.
So you mentioned David Chalmers, the OG of Peace on Bees.
Is there anyone else people should read?
Yeah, people should definitely take a look at the FHI digital minds group.
Sure.
Yes.
Yeah.
And like, yeah, so much of what I've been saying is is just like from conversations with people and other people figuring this stuff out.
So like other orgs in the space.
There's the Sentience Institute.
The Quality of Research Institute is not actually for me that relevant to this stuff because they think that digital computers can't be sentient because they don't solve the binding problem.
And that's because they, well, I don't really understand QRI's theories of consciousness.
But they understand other things.
What's that?
They understand other things about DMT.
Yeah.
And like they're like, I like them and I admire their like daring, even if I don't think they're like on the right track, which I don't, but that's okay.
Yeah.
One thing that happened when people were talking about this on Twitter is like one framing is like this is a new thing and it's coming from like, you know, technology brothers.
But like, it's not and so like philosophers who've been writing on this for a while would include Eric Schwitzgabel, who has a great blog.
He has a paper with David Udel, who is also in the alignment space.
People should check that out.
Susan Schneider has worked on this.
I mean, maybe I shouldn't say too many people because then I'm gonna feel like I should name like everyone, but also check out the Association for the Scientific Study of Consciousness.
So people doing like actual scientific theorizing about consciousness.
And yeah, as a way of self promoting and also punting a bit.
Yeah.
If people look at my SubstagPost key questions about artificial sentience, that has like a lot of references to people who've been working on this.
What is your favorite tweet?
Oh, by me.
I don't know. I think my pin tweet right now is just like a series of kind of takes that I have on Lambda.
The tweet that got most popular during all of this, and this was like kind of depressing to me.
It kind of shows like what the incentives of Twitter are.
I like tried to be polite and I try not to be too snarky on Twitter, but the tweet of mine that blew up the absolute most was me kind of like politely but still like dunking on someone because someone who tweeted like,
Oh, like the question of a sentence is very easy once you realize that this is just matrix multiplication.
And I tweeted.
Oh, like the question of human sentence is very easy once you realize that it's just a bunch of ion channels and neurons opening and closing.
It's pretty funny.
Thank you.
And yeah, I don't know people liked it a lot.
I got like over 1000 likes, which is like a lot for me.
I'm not like that big on Twitter.
I think you're bigger than me.
That's, that's surprising.
Well, I don't have any t-shirts.
So hopefully we get new artificial sentience t-shirts.
Follow the guy Robert Long on Twitter.
Is it Robert Long, your Twitter?
Yeah, that's my, that's my name.
And then the handle is RGB Long.
RGB Long.
Yeah, thanks.
It was great.
Hopefully record another one in 2000 and 100 when we were both alive.
That'd be great.
Looking forward to it.

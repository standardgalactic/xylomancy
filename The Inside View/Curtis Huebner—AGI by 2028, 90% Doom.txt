Four years from now, you're going to have a very large amount of actors
that are going to have a rather non-trivial amount of compute.
Like inflection is building a cluster of 22,000 H100s.
The military guys now kind of getting into it and saying like,
Hey, maybe we should start racing with China.
The Aurora supercomputer guy is saying like,
Hey, we're going to work with Intel.
We're going to make a trillion parameter model and we're going to use our big
supercomputer with like 60,000 Intel GPUs to make that happen.
People who are making these orders are aware of the other people also making
these orders, right? The relevant actors are all aware of what is going on.
The people with the most impact or chance to really do these kind of things
are the most acutely aware and the most like ready to race.
And here is Cortes HÃ¼bner, head of alignment at Eleuther AI
and known on the internet as AI waifu.
People might recognize him as the one who commented on Eleuther
Rutkowski's death with dignity by saying, fuck that noise.
Can you like explain for people who were like, haven't seen that comment,
like what your comment was about?
So maybe like what the posts permit Koski was as well.
Yeah, sure.
So at kind of a high level, the posts that Eleuther posted on April Fools of
all days is basically Mary's strategy is we're probably all going to die.
Almost certainly we're all going to die.
But really what we want to do is we want to maximize the sort of log odds of survival.
So essentially they're saying, like, you know, do what you can to not like,
you know, not screw up existing alignment efforts.
Do what you can to, you know, maximize the probability of survival.
But in reality, we're probably all going to die.
And so, you know, I saw this post and like, there's a lot.
There's a bit of a misunderstanding, at least in my opinion, there's a bit
of a misunderstanding about like what my comment like says and kind of the tone I
took, because one thing that happened with that post that I didn't, you know,
didn't really like too much, but was maybe necessary for some people is that the
total, the tone of the post was a very somber one, kind of being honest about
like the, you know, the gravity of the situation and, you know, how deep and
you know, in trouble we all are.
And really that was kind of the main thing I wanted to counteract with that
post or like with my comment on that post.
I actually do, to a large extent, agree with, you know, people like Connor and
Ellie's or that the probability that we are all toast is very, very high.
However, like the way I kind of see it is that you can't sort of give off
vibes that you're going to give up, so to speak, or that you're going to accept
anything less than success.
And sort of the reason I think that is like, I don't know if like you're
familiar with sort of like the game theory kind of notion of a stag hunt.
Essentially, you have two hunters and the hunters have the choice of either
hunting a stag or a hare.
And if they go for the hare, there's a very high probability of success.
And so they'll probably get the hare, but the hare doesn't have a lot of meat.
So it's not a very big reward.
If you go for the stag, though, the only way you actually managed to
successfully hunt the stag is if the other hunter also goes and hunts the stag.
Right.
And both hunters in the traditional stag hunt, both hunters are faced with the same choice.
Right.
So the ideal outcome is that kind of both of the hunters go and they go hunt the
stag together and then they get the stag and then, you know, they get a lot of meat
because the stag has a lot of meat instead of the, instead of the, the, the hares, essentially.
Both of them have to sort of trust each other that they're not going to try to go
for the easier route.
And I guess like my perception when it comes to existing alignment efforts is
that it's like a stag hunt, except that it's not like two people.
It's like you need hundreds of people to get together, maybe thousands, maybe
you know, tens of thousands, all getting together to basically solve this, you know,
solve the alignment problem.
I think that because of that, at least for a lot of people who kind of understand
the magnitude of it, it, you really can't signal something like this sort of this
somber attitude of failure, even if, even if it is very true that we're almost
certainly screwed because a lot of people are probably going to take the easier
road of being like, well, we're all dead.
I'm going to go on vacation.
You know, I'm going to enjoy kind of the rest of the life that I have.
And so really that was kind of one of the, at least like one of the motivations for,
for making the comment that I did.
So what you're saying is basically like we should all go with everything we have
because, because that's the world in which like we actually solve the problem
and, and make progress and that his posts had the downside effect of like people
that might like be, become like depressed or sad about it.
And then like try to like defect and say like, you know, if we're doomed,
like I might as well like enjoy life for like a few years.
And so what you say fuck that noise or let's try to like listen to some very
like optimistic things and like do the thing.
That's what you were trying to like encourage and give, give some like
wave of optimism, right?
Yeah.
And the thing is, it's important to notice too that like the,
the post that Eleazar wrote also served like a very important purpose, right?
There's a lot of people that I believe were sort of, I guess, sleeping at the
switch, maybe like they were kind of aware of the situation, but they weren't
like aware of the gravity of it.
And I think that like for those people, the, the post had kind of would,
would have the opposite effect, right?
Of like, Oh, wow.
Okay.
You know, we don't actually have this under control because the guys who are
like their entire specialty is preventing this from happening or saying,
there's a 0% chance, essentially, you're almost 0% chance that we're going to
succeed, right?
Both messages were sort of necessary to say, like for the people that are
aware of the gravity of the situation and are, you know, trying to, to do
something, you know, knowing that there are other people out there trying
to, to, to mitigate existential risks and still kind of going at it with
everything they have, but also like serving as a wake up call for, for
people that may not have been paying as much attention.
So for me, the, the post wasn't that like that much about, uh, like the
probability of, of, of dying from AI extinction as it was about like, like
how to like behave with dignity or like don't trying, like not trying like to
do like crazy things that might have like bad effects in the long term,
like second order consequences.
And like, if, if you think that like blowing up TSMC or like doing
crazy thing like this, uh, might be good to save the world, then, uh, instead,
maybe you should consider like dying with dignity instead and like doing the,
like the ethical things that might be like better in the long run, um, or in
the medium run, depending on your timelines.
Um, so I guess that's what's like my kind of like intuition.
And, um, maybe if I want to like push back against your post, uh, or your
comment, it would be like, maybe try to be optimistic, but still keep the
like dignity part, um, from, from your coast game.
Yeah, I think, I think that's, that, that's a fair kind of point.
In fact, like one thing that happened after I made that comment is, uh, I
have kind of a, a really long discussion with Connor kind of following this.
And one thing that we sort of like disgust is really like, I think in the
post Ychowsky talks about dignity is like the logarithm of the probability
of your success, right?
And I think there's the, there's really a fair point to be said there that
like for us to succeed, it's not going to be because like a high variance,
you know, a single high variance action was done.
You know, I think he, he refers to those as like miracles, but you need a
lot of little things to, to, to go right and build on top of each other.
And I don't, I don't think I really disagree with that too much.
It's mainly the, the, the point that I'm getting at is, is, is the, is the tone.
Right.
And, and the, and what you're signaling to the people around you who are
actually trying to reduce existential risk.
And I think the, the tone might be like a huge factor in how we like managed
to convince people to, to work on this.
Um, I think mostly on, on Twitter or on internet, people have been like
committing on this as being like a do more post.
So like the, even the, the word like doom or doom or I think it might be like
negative, uh, longterm to, to be like a term people use, uh, as often as they do.
And instead we should like talk about, you know, like aligning AI or, uh,
building an utopia or, um, like maximizing our impact on the light
corner, those kinds of things.
And I think like people like tend to like see maybe like this post or a lot of
the vibe vibe there as like maybe like too negative.
Um, so yeah, I guess like, um, some people on Twitter have been like asking,
like if you add like any updates since the, since last year, uh, or last year and a
half, like did you, since you wrote that comment, do you have like any like new
thoughts on this or like a new perspective?
Or are you still like as, uh, motivated and, uh, willing to, you know, work very
hard on this?
I think basically nothing has changed.
I do think that the situation has gotten actually like significantly more
pessimistic even than, than, uh, when, when I did write that comment.
Right.
Um, so I think that with the release of GPT-4, there was a, like a, a slice of
worlds where open AI kind of said, okay, let's, you know, let's pop the brakes.
There's a slice of worlds where like open AI kind of says, okay, let's, you know,
let's stop releasing things.
Let's kind of close the gates and let's take actions to kind of slow down
race dynamics.
And right now we're seeing sort of the complete opposite of that, right?
You have, you know, GPT-4, you have a tremendous amount of AI hype pretty much
all over the place.
You have a lot of different companies that are now cropping up trying to
replicate what open AI is doing.
You have a lot of venture capital kind of flowing into the space.
Generally, if you're, if you're trying to avoid this kind of further acceleration
of an AI race, this is not the timeline that you want to be in.
How, how pessimistic are you about AI, uh, being an extinction risk and like
more generally, like what are your like 10 months?
How do you see the future in the next few years?
If you had to ask me for a number, I think I would say like we are 90% toast.
And the reason I'm saying 90% instead of like 99.99% or something like that is
mainly because I do believe that there's a great deal of uncertainty that I'm just
not able to move.
Factors that I haven't considered, places where I'm wrong.
That's like most of the thing that is carrying me to be like, you know, it's,
it's not a hundred percent, but I really do think that the, the, the situation that
we're in is quite dire.
So internally, when you wake up in the morning without taking into account your
uncertainty, you, you think there's like 99.99% chance of your dying?
Maybe not like 99.99, but like, you know, pretty, pretty up there, uh, you know,
like 90, 99% chance that we're all toast or really toast.
What, why do you think that?
What was it like reasoning or like evidence for dislike belief?
So I think like, you know, where I kind of disagree with, with a lot of other people,
um, is I, I do think that like once we kind of understand how to produce intelligence
and once we kind of how to, we, we understand how to do it very quickly or like, you know,
we, we to do it efficiently is, is really, is, is really the thing.
I think that it took out is really just that as soon as we had a language model that was
good enough, we plugged it into every API we could, even APIs that, you know, we just made
it generally, I think opening, I just made an update to make it generally able to be
plugged into APIs, um, by, you know, training it to format JSONs or something like that.
So like, you know, this is the kind of behavior where like it's, it's the timeline where we
don't survive is, is, is what it is.
So you get that and you get like a whole bunch of other kind of similar failures and you add
it all up and it's like, okay, we're almost certainly toast.
Um, there's definitely room for me to be wrong about the whole situation, but it's very much
not looking good.
I'm kind of lecturers of like, if you have any teal men or yeah, so I guess there's a
couple different like kind of classes of counter arguments.
So like one of them is that I'm completely wrong about the efficiency arguments.
So like, you know, you look at the total compute that is necessary and, and maybe you, you,
you really do need like an absolutely enormous amount of compute to be able to, to, to do AGI.
And as a result, like, you know, okay, you need a full super computer, you need 10,000
graphics cards and Moore's law will, you know, just happen to, uh, Peter out, right?
When it is petering out a little bit already.
We, we are kind of seeing that where, where, you know, we are able to run these things,
but they are extremely technically, you know, it's, it's very technically challenging to,
to, uh, run them at scale or, or in any way dangerously.
So that could be like one kind of component, uh, where, where, where I'm wrong.
Another kind of component, uh, where, where things could be, you know, where things could
go well is just like, you know, maybe, maybe I'm wrong about the difficulties of alignment.
Um, and you can actually just get away with really simple tricks, um, like RLHF and stuff
like that. Um, and that, you know, turns out to be good enough that, you know, you don't
really need that much more than the, than the current existing techniques or, or really
anything more at all. Uh, and you get a system that, that just in general works well.
So really it's, it's, you know, those would be kind of examples, uh, that, that I would
reach for is that like the ceiling on capabilities and the, the speed at which you can get to that
ceiling is significantly lower than I thought it was, or alignment just turns out to be
significantly easier than I thought it was.
And so that's why it's 90% and not like 19.99 because you have like 10% chance of like this
being true. Exactly. Yeah. There's, there's definitely like, there could be a modeling
error somewhere and things work out to be significantly better.
In terms of like compute we need or like size of the models we need to get to something
dangerous. So you're saying that maybe the speed or like the ceiling is, is, is, is maybe, uh,
wrong in your model? What, what do you guys predict for like how big of a model do we need
or when we will get there?
So kind of like my, you know, if I had to kind of give a number, I would probably guess like
something like, you know, the, the, the thing that you need, um, is probably something really
small, like 10 to the 10 floating point operations per second, you know, over, over the lifetime
of a model, 10 to the power of 19, uh, floating point operations per second, which is, which is
really not that much, right? That's like one, you know, 100 teraflops are like 140 90 for like,
you know, three hours. Now you're not going to be able to do it with, you know, you know,
40 90, uh, in three hours with like current algorithms, um, you'd probably need to do
like quite a bit of your optimization and improvement and, and, and maybe you need like
a slightly different computer architecture or something like that. But like that would
probably be like the, the, the lower bound, um, or people were not into the deep weeds of
training, uh, deep learning models, like how expensive it is to get like a 40 90 and like,
is it like something you can get right now? Or is it like something like top companies use?
Well, I mean, like a 40, a 40 90 is like top of the line gaming GPU. So like any gamer with like,
you know, I think, I don't know exactly what the current prices are, but like, it's like one or
$2,000 is able to, to get their hands on one. So, so yeah, this is, this is a level of compute
that is, uh, very much accessible, a large proportion of users or people. So when you say
like the compute required to have something dangerous or something that could like maybe
like disempowered humanity, are you saying like for training or, or are you saying for inference?
Because I feel like for training, that's like not a lot of compute, right? It's really not a
lot of compute, but I, I still do think it's actually like, you know, if you were to get
the AIs from the future, you know, after we've kind of had a lot of room for optimization of,
of these algorithms, I think you would actually get something that can go from like zero to human
level in, sorry, it's not three hours, it's 30 hours. But, you know, in a day on a 40 90,
essentially, you know, using the, the, using like the best algorithms. I think in practice, like
it's probably more realistic that we're like three orders of magnitude higher than that. But like
three orders of magnitude higher than that is still not that much, right? Like that is, you know,
that's, you know, 10 of them for like $10,000 worth of compute for 100 days, right? Which is,
which is very attainable for a lot of people. Or like one of these graphics cards for three years,
right? Again, that is a very small, you know, it's not like exactly consumer level, at least
right now. But, you know, assuming that we can still keep getting performance improvements and
stuff like that, it will be very, very soon. It's really not that much compute. And I can go on a
little bit more detail about like where I'm getting kind of those numbers and stuff like that.
Yeah, yeah. Please, where, where, where do you get his numbers?
So like, these are like really cheap heuristics. Yeah, part of it is just kind of like estimates
of like how much compute the human brain uses. You look at the human brain, and it's like estimates
estimate that like each neuron fires maybe like once every 10 seconds. And there's 100 billion
neurons in there. So that's like 10 billion neurons firing every second. And every neuron is
connected to 10, yeah, 1000 1000 synapses. And you say, okay, like each synaptic operation
is one floating point operation. So you make this arbitrary equivalence. I actually think
a floating point operation is more complicated than a synaptic operation. But you know, you can
you can about get that equivalent. So you have this you have this like rough equivalence there.
And that gives you an estimate of like 10 to the 10 to the 12 or 10 to the, you know, 13,
or something like that. Yeah, 10 to the 13, I believe is the estimate you get. So you get like
10 trillion synaptic operations per second, which is 10 to the 13. And then basically,
like what I'm doing is I'm taking that number and saying, okay, well, you know, a human lives for
like 10 to the nine seconds over 30 years, or you know, that's about it. So 10 to the nine plus 10
to the 30, or 10 to the 13 is well, not plus, but you get the point. That's 10 to the 22 operations.
So your lifetime training compute is 10 to the 22 operations. And essentially what I'm doing
is I'm allowing for like, you know, let's say whether we find like 1000x improvement and efficiency
over what the human brain can do, because we can express algorithms that the brain kept express,
we can compute in ways that the brain can compute, you know, we we can we can like the space of
intelligences is presumably very large. And so that's how you get to the 10 to the 19 number.
So like if you if you make the assumption of like, okay, nothing, nothing, no algorithmic fanciness,
you get like 10 to the 22. And if you assume like, you know, arbitrarily, like,
we'll get 1000 times more efficient than that, you get 10 to the 10, which again, okay, like is
kind of like numbers that you, you know, you pull out of thin air. But like, this sort of gives you
a ballpark of things. And when you compare it to, you know, existing models, like I think like GPT
three is like 10 to the 23, GPT four is like either 10 to the 24, 10 to the 25, and probably like,
you know, frontier models going forward are probably going to be 10 to the 26, 10 to the 27
flops. You're really into several orders of magnitude, more compute KB to that is that like
10 to the 13 floating point operations per second is a very kind of lower end estimate of like what
the requirement of the human, you know, the computer requirements of the human brain. Some guys put
it at like, you know, 10 to the 16, which is three orders of magnitude higher. And that kind of puts
you at like 10 to the 25 for total lifetime compute. Some people put it all the way up to 10 to the
18, which puts you at 10 to the 27, you know, it's kind of, it's kind of all over the place. But
I do think kind of the lower bound is probably correct for like, you know, weird vague intuition
reasons. And as a result, I do think that we are in a very hot water, I guess, like we are in a very
large hardware override, and it is simply a matter of either, you know, we get an AGI, and then after
that it goes and does a little bit of recursive salt improvement, or, you know, something similar
happens. And we are very much in a situation where there is mass proliferation of human level
of artificial intelligence. Okay, so I think I got, I got the main reasoning behind your argument
and kind of the main numbers. So you're saying that the main like, uncertainty is about like,
how much computing is the brain doing? And usually, you think like, you're 10 to the
13 flops per second is probably, probably wrong. Or maybe like other people are have like other
estimators like two or three orders higher. Yeah, yeah. So like, yeah, some people will say, well,
okay, a synaptic operation, you know, a neuron is doing a little bit more complicated stuff.
Or really, you're only looking at like the firing neurons, really, you should be looking at everything,
you stuff like that, your estimates are going to go up. You know, depending on how much complexity
you attribute to what's going on. And all of that kind of, you know, cranks up the the requirements,
essentially. So, I still haven't read AJI Kutra's report fully, but I'm doing like a series of
videos on it. And I'm looking at the graphs right now. And I think for the lifetime anchor,
which is like, how much compute is maybe like a human doing in terms of compute from like burst
to death? I think the estimates point at, at least from for AJI's best guess goes from like,
10 to the 29 in 2025, to like 10 to the 27, like after like, I would make improvements and like
other efficiencies. So your, your 10 to 22 seems like much lower than like everything else, even
like the most like aggressive things. So I can talk a little bit about that. So I think that there's
a couple of things that I disagree with in the report. So I think one thing that AJI did is
that she disagrees with me very, you know, very much like a complete sign flip in terms of like
how algorithms kind of play into things. So I believe that, you know, we're not going to have
too much difficulty level, you know, finding human level efficiency algorithms. And in fact,
we're probably going to be able to do, you know, several orders of magnitude better than humans
in terms of algorithmic efficiency. I believe that if you look at the text of the Kutra report,
and again, you know, don't quote me on this to actually go check to see if it's correct,
but I believe there's actually a multiplier of 1000, of three orders of magnitude in the other
direction. You know, basically, I'm saying, let's go 10 to the 13, and then let's multiply by,
you know, one 1000 to get 10 to the 10. And they're going and saying, well, let's go 10 to the,
you know, 13, and then let's go the other direction. Right. Because it's very hard to get
like human level improvements. Yeah, because, because like, you know, the idea is maybe like
the human brain and really brains in general have maybe had a lot of time to optimize their
algorithms. You know, evolution has probably spent a lot of time kind of optimizing things.
And as a result, like we're not going to be able to, in any reasonable amount of time,
match the quality and efficiency of human level algorithms. And what another kind of thing that
I believe the report did is that the lifetime anchor when they initially had it was predicting,
or a couple of the anchors actually were predicting that we would already have had AGI by now. And
I believe what they did is they did like a sort of a squishing operation where they took sort of
the Gaussian estimates and they sort of like pushed them over because they said, okay, well,
we haven't, we haven't seen it so far. And I think that like that squishing operation is the wrong
way to kind of do things. What you should be doing is you should actually be cutting off the
Gaussian and renormalizing. And that produces a much more sort of pressing. You see that already
like a lot of the probability mass is a lot closer. So I think, and again, you know, the double
check that that's actually what it is, but at least that's what I remember from kind of glancing
at the report. And those kind of corrections, and I think there's a couple other things,
all kind of work together to lead to a significantly higher estimate. I will however note that I
believe Ajaya initially predicted something like 2050 or something like that for median timelines
and that just since revised her predictions down quite a bit. So, you know, I don't actually know
like what specific changes happened in her model that led to that sort of down revision. But, you
know, yeah. Yeah, if I remember correctly, in the revision, there was something about being able
to like have AI's that do code for you. I think code coding was like a big part of like, how much
she thought that AI's will be able to generate value in the future. And so AI's could be like
transformative sooner because of like that, like how important code is for everything we do, and
like how is it was to do code right now. But yeah, again, like check the post for more details.
And the thing you said about like squishing the distribution on the right, I think it makes sense
if you're like, if you're like in 2019, and Ajay seems like very far away to not like include models
that predict Ajay happening today was like, was like high value if you think it's like very far away.
I think I think there was something about like not including models that would predict things
much sooner. And I haven't seen the part where they like they moved the thing to the right for
2025. I think I think it would be it would be good to have models to like have nonzero probability
mass on 2023 or later. Yeah, and I think I think like this is actually like the case for me. So
like I do have to like, you know, I'm not some kind of, you know, really good predictor. I have,
you know, previously made very aggressive kind of timelines. And, you know, I said like, okay,
maybe I think like in 2015 or something, I was thinking like, there is like a 15% chance that
like by 2020, we would have a GI and that turned out to be wrong. Right. So that is like, I lose
base points, I lose predictive, you know, credibility, because of that. And I think like
when you know, when you kind of fully integrate that till out to like 2023, where we're at now,
like, it's something like 30% or 40% or something from my initial like 2023 estimates,
or sorry, 2015 estimates of when a GI was going to happen. So, you know, you can you can go to
sort of look at me and say, well, like, my timelines are actually like updating in the
other direction where it's like, okay, they're actually stretching out. Another kind of like
fun thing to think about when it comes to timeline prediction dynamics is that a consistent predictor
will have their expected value of when the timeline changes slowly increase over time.
You know, you can imagine like a toy model of this being like an exponential distribution,
but when you think a GI is going to happen, and an exponential distribution will have the
property that, you know, it's expectation, it always looks the same, you see it with like
radioactive decay of particles where like, if you have a particle, and it doesn't decay for like
five minutes, then you still expect that the amount of time it takes to decay is going to be
the same. Because every time you do the Bayesian update, you renormalize the probability distribution,
you get back the original probability distribution that you had. And so you do actually get this
general effect, where in general, you do actually expect timelines, if you're if you're fully updated
and you're fully calibrated, you do expect your timelines to sort of get longer slowly over time
before they abruptly collapse when the event actually happens, and then all of a sudden your
timelines are now zero. If you do have more information, right, you do have like more
papers, more models being released. This is a very simple model of timelines, but yes,
you're obviously going to be updating on on, you know, papers and new developments and all this
kind of thing. So that it does kind of complicate things. Yeah. So you said in 2015 that there was
like 15% for 2020. And now in 2023, so if you were to give you like median like 50% chance of
Asia, maybe you can give like an estimate for superintelligence as an opinion. I like to call it
now. Yeah, when you think there's like a 50% chance of like some like superintelligent agents
coming, coming along. So I think like if you'd asked me this, like maybe a month or two earlier,
I would have said like 2027. So like maybe like four years from now, I think that now that I've
kind of seen, you know, a little bit more information, like really recent information,
I think that is kind of like my timelines have pushed back a little bit, not very much.
You know, I think maybe like maybe a year, so maybe, maybe 2020, I don't know, maybe add like
six months to the timeline or something like that. So maybe 2028. It is where I would put the
median right now, just because like some very recent information is kind of maybe down revised
things. But was the information, I guess like I was expecting certain research to happen,
and it didn't happen. And that either means that that research doesn't work, or it means that people
are not really interested in kind of making that, you know, the red direction happen. Either way,
that is kind of a positive update for timelines, because it means that, you know, in the cases
where it is kind of a dangerous research direction, people aren't really pursuing that. And in the
cases where, you know, it is a dangerous research direction, people are, you know, it just doesn't
work, and I'm wrong. And therefore, the path the AGI is a little bit more complicated than I thought.
And as a result, you know, we live in a world where we have a little bit more time.
So I'm glad to know that the dangerous approaches that we will not name these
podcasts don't work. How do you wake up in the morning thinking that there is,
you know, like a 99% chance that the two might die in the next five years? So if you multiply
with the 50% chance for five years, it's more like a 45% chance.
I think I would say like, I've kind of had the time to really think about this for much longer
than most other people have. So like, yeah, when it was 2013, and, you know, things were kind of
the way they were, you know, I've kind of started getting like more and more kind of evidence about
the direction that kind of things are going in, you know, like a lot of people are, when they see
things, they see chat GPT come out, they see GP for come out like a couple months later, and all
of a sudden they go from like, you know, nothing to Oh my God, we're all going to die. But for me,
it's very much more been like a very gradual process where I've been like internalizing. Okay,
things are more complicated than I thought this problem is is is hard. I'm starting to
understand all of the rough edges and slowly but surely my probability that we're all going to make
it out of this has has gone down. And I think I sort of like at least partially made peace with
that. So at this point, like, I guess it's sort of like a normal thing. And it's been like that for
a long time. Whereas I think for a lot of people that are like getting blindsided by this technology,
it very much is a, you know, a punch in the gut of like, Oh, I had all these plans for what I was
going to do 30 years from now and blah, blah, blah, blah. And you know, maybe that's not going to
happen. Whereas for me, it was like, Okay, I'm probably got going to make it to 2030 is looking
real far away right now. And it's been looking real far for a while. So so so I think I think
that's kind of a big contributor to me, like not really being that, you know, negatively
affected by it. Like there is a couple of times where I'm like, you know, it hits really, it
didn't, you know, things get really real for a moment. I'm like, Oh, boy. You know, we could,
you know, you get the real visceral fear that we were all toast. But like, I think I've had enough
of those and those have happened like over a sufficiently long period of time now that it's
like, Oh, this is just like, yeah, we're all toast. And we're all toast probably pretty soon,
unless I'm wrong. For me, that moment was like seeing everyone talk about it on Twitter more
and more. And like, even like the US government talking about it. And I was like, Oh, it's not
like some obscure thing that people talk about on the internet, people like in the real world
are talking about it for real. Like, at some point, when you when you have like some, like,
uncertainty about your own models, you're like, Oh, maybe I'm wrong. Maybe I make mistakes. Like,
I'm probably like, you know, like in the in the wrong
direction. But if, if a lot of people like seem to converge the same belief, and it starts to
like appear on TV, and your like grandma calls you and be like, Hey, have you heard of this thing
called chat GPT is pretty good, huh? It feels more and more real. And it can like starting to feel
like an in a gut level. I think that's like what Robert Oldsmile was saying, like in my podcast,
like is God is catching up with like what his model was thinking for a long time.
And I guess when you were saying that like, you were predicting this since 2013, it seems like
in 2013, they were like not a lot of evidence, right? There's like, maybe imagine that in 2012,
like what was like other things were like, pushing you in the direction of like stuff happening fast.
So I think really, like, it hit me that things were happening fast around maybe 2014, 2015 in
2013, like X risk was a thing that I was aware of. And I was like, Hey, we need to be thinking
about this. But I was nowhere near as pessimistic as I am. Right. Really, like what started to update
me was like, I started paying attention to all the papers that were coming out from really all over
the place. And like this, this sensation that progress was really fast and accelerating
has been pretty much constant. And hasn't really gone away for a very long time. You know, I remember
all the way back in, you know, 2014, 2015, being like blown away at the at the rate of progress.
And that sensation hasn't really left. If anything, it's almost felt like things have slowed down a
little bit compared to some of the developments that were happening in the in the earlier years.
Do you think we're going to get like a fast takeoff with maybe like some film scenario was
recursively self improvement? Or do you think things will progress slower than this?
So I think in takeoff time, I think there's, I think there's kind of two factors, right? So like,
there's the possibility of takeoff, like a software takeoff, and then there's a hardware takeoff.
And I think like, you get kind of different dynamics and assumptions, depending on how you
like make assumptions about the interplay between those two things. So like, if I'm right about the
like extreme potential for like efficient intelligence, I think that what you will get is
you will get a, you know, everything will be mostly fine. And then you will get a very fast kind
of software takeoff where you know, the minimum requirements to run an AGI very rapidly drop
over orders of magnitude, because, you know, you're able to, you know, as you go down the
number of orders of magnitude necessary to reach a given level of intelligence,
the rate of iteration sort of grows exponentially, because you obviously your experiments get,
you know, get faster and faster to run. So I do expect that like, if I'm right about the
efficiency stuff that we could see like, you know, these massive, massive models that require like
10 to the 26 or 10 to the 27 flops, you know, eventually one of these kind of comes online,
because, you know, maybe we're just doing things in a really dumb way. And then after that, we just
have like this complete collapse as the system like undergoes very fast recursive self-improvement
so that it's able to run itself on on very much, much smaller hardware. And that could happen. I
expect on the timescale probably like, you know, maybe not days, but like, I could easily see
something like a month or something like that as being a reasonable value. Things get more
complicated if I'm wrong about the about the software efficiency. In the case where you
actually have hardware bottlenecks, now you're no longer in kind of the world of bits, you're
in the world of atoms. And this is where like, things kind of get a little bit more fun to
think about it in different ways. Because now you're no longer thinking about like, okay, how do we
like lower the flops kind of intelligence, it's more like, how do we get more flops in general.
And the question of how you get more flops is one of like manufacturing cycles, total available
energy of your replication rates. And this is where you kind of get into like the fun kind of
like spectrum of like, well, okay, do we get like humans to go and build the factories to make more
of the stuff that we care about? Or like, is this kind of a thing where, you know, you're going to
get macroscopic robots building factories that we don't really understand? Or is this kind of the
case where like, you know, Yadkowski is right about everything. And we get, you know, bacteria
that replicate on the air, you on the order of like, you know, 10s of minutes or hours or whatever.
And, you know, after an initial like, slow kind of progression to better and better sort of nanotechnology
that you eventually just get really, really fast replication rate. But like kind of before
then things are sort of under control and we kind of have a good understanding of how kind of things
fit in. So like, my view on takeoff is kind of a function of those two things, like how
do things work in the software world and how do things work in the hardware world? And there's
a lot of uncertainty on both of those. I think for the software world, what you say doesn't really
apply for this current world, where you have like, one company that like trains like very
large models, maybe like a few companies, let's say like four or five training like very large
models. And training takes like a long time, let's say weeks or months, right? And if we were to have
like, recursively self improvement from like efficiencies that are like discovered, I would kind
of like see it as like open source people doing kind of like experiments on some kind of like
Cal of GPT or like some some stuff online. And they won't have access to like all the
compute does like big companies have. So if you're like,
except if like big companies where like launching a program being like, please find a new like
improvements in how to train more efficiently. And you're allowed to like interact with your own
hardware or like servers to do more stuff. Well, let me flip that around and say like,
given that there's, you know, I think Nvidia is aiming for something like 400,000 donation
hundreds rolling off the assembly line every quarter. And there's a lot of companies that
are making big orders for like, you know, 20 to 80 K each 100. Right. What is your view that someone
isn't going to go and just say, especially given kind of the previous circumstances where we've
said like, you know, we've initially we were talking about boxes and then and the way that
that actually turned out, you know, what makes you think that you aren't going to get, you know,
these AI systems effectively being unleashed on their own source code once they're, you know,
sufficiently capable and being explicitly instructed to make themselves as powerful as
possible as fast as possible. Because all the companies ordering like hundreds or thousands
of age 100 are going to try to research how to make this their stuff more efficient and
and like run agents or run stuff in a loop. Like what is the argument here?
So like, I guess like the thing is, is like, you know, say you get the first
AI that is as good as an AI researcher, and it takes like, you know, you have maybe like 100
people and you know, a couple hundred people in this company that are that are working on this
thing. And you have like, you know, you train it on a cluster of like 10,000 H1. Initially,
you have like 100 people that are that are that are working on doing these optimizations.
But as soon as you kind of get to this human level, you have like, you know, now you have
significantly more throughput, you can actually kind of dive into the math of like, okay, exactly
how many, you know, what is the like equivalent of like simultaneous people and it depends on
like assumptions about how big you make the networks and stuff like that and how fast you
can do inference and all these kind of things. But like, I think there's going to be a lot of room,
like you're going to get a lot of, you know, after as soon as you do like the first training
run, you're going to get a lot of room for a very rapid self improvement. Just because you're going
to have, you know, quote unquote, a lot of, you know, machine power that you're going to be able
to redirect at the task of kind of optimizing code base. And yes, there's like, you know,
there's fundamental kind of limitations there. But this is this is where like the question of
like, what is the theoretical software efficiency and how much time does it take to really get there
in terms of like man hours? And yeah, I think it's like a huge assumption that we will get some
like AI capable of doing like everything and AI research we can do. Or at least like that's like
further down the line, maybe it's like in more than like at least three years, possibly like five to
10, right? And or we can discuss the precise numbers, but I don't think it's very significant.
The real argument is that we're going to get something like between 0% to like 100% of like
can do like everything a human can do. And there's going to be like this like 100% or like 10%
thing that like the AI is not able to do and like the human will need we need to like jump in and do
some kind of things to help like in the physical world or like, like file some document for like
Amazon AWS to like prove that you're a human or like doing this like couch hour. Of course,
like at some point is able to like bypass like every human thing and do everything on its own.
But I'm like, my model for like, why, why do I be later in this is like, maybe the AI research
is like kind of like hard to do. And there's like always like some small freshman human labor.
Yeah, so I think it really, you know, you're depending on your model of exactly how that
human labor fraction kind of this, the case is going to tell you whether or not you get like a
slower or a faster or like a harder, you know, maybe not, you may be not fast and slow, but like
hard and soft take off because like, I think that there's a really big difference between a human
being there needed for like 1% of all the work and a human being there for, you know, 0% of the
work. And it also is a function of like the amount of compute that you that you have at your disposal.
So like, if the AI is helping, you know, humans, but like that 1% where the human is there is
actually a bottleneck. And so you're only really able to throw like 1% of the computational resources
to to actually accelerate, you know, your researchers and I do believe like with chat
chit chat and stuff like that right now, you know, we're well below that, you know, level, like if
you look at like how much compute open AI researchers are soaking up just to like, you know, aid
themselves. I imagine it's a tiny fraction of the amount of compute that they actually have available
for them. And yeah, maybe like, like if you get a soft transition, it'll probably be because
you're sort of saturating that compute. And you're you're doing AI research as fast as you can.
And, you know, and more and more like, you know, when I say that I do mean stuff like that's less
non trivial than like big training ones. I mean, like, you know, actually like improvements to
source code, like actual, you know, time being spent, you know, writing code and making improvements
and stuff like that. But yeah, I do think that like it's a very tiny fraction right now. But if
we live in a world where like open AI manages to soak up all of their compute helping, you know,
helping the humans and the humans are really a bottleneck, but they are still necessary.
Then yeah, I think you can make an argument that you get like a more smooth take off.
I guess the main argument I was trying to aim at is unless people are like acting like really
dangerously and and trying like very like, like, you know, like not very moral things like trying
to build like a self improving AI or tries to build like an AI scientist without sandboxing it,
then we might not get it. And it seems like with the super alignment posts, or I don't know, like
entropic and even deep minded, they have this concern about like alignment. And so maybe,
maybe they won't run those experiments just yet. And maybe we, maybe like the open source
people will just run those experiments like three years later when they have the compute, right?
This is why I bring up the, the large amount of patient hundreds that are being manufactured
right now is that there's more than just open AI, anthropic and deep mind now. The race has been
started, you know, like inflection is building a cluster of 22,000 H 100s. You know, there's rumors
of like, you know, some people doing a cluster of like 80,000 H 100s, or something like maybe not
a cluster, but like an order that big, you know, you have the major cloud providers, you know,
Microsoft is not, you know, sitting around idly, either. And they've kind of explicitly said in
their like sparks of a GI paper that they're kind of aiming for it for agency and self improvement
and that kind of thing. And then you have like, you know, you have other actors, you have state
level actors, you have, you know, you all the military guys now kind of getting into it and
saying like, Hey, maybe we should start racing with China. And then you have like, you know,
the Aurora supercomputer guys saying like, Hey, we're going to work with Intel, we're going to
make a trillion parameter model, and we're going to, we're going to use our big supercomputer with
like, you know, 60,000 Intel GPUs to make that happen. I really do think that like four years
from now, you're going to have a very large amount of actors that are going to have a rather
non trivial amount of compute. And, you know, open AI and deep mind and anthropic, you know,
might be in the lead. And they might be able to like, you know, be smart about not just like
opening up the throttle and letting it rip. You know, they probably have the internal discipline
to not let that happen. But, you know, if they have the discipline to not let that happen,
I do expect that somebody else with a very large amount of compute of GPUs is going to step in
and and pull the record, basically. I think that makes me much do your than I was before our
characterization. And shorten my timeline by a bit. So yeah, you're kind of like, doing more like
open source research, or open source software. I don't know how much you're you're doing open
source things. But some people are like, I've been asking me, like, if you had like any thoughts
of like, an open source versus like, close source, open resources, like close research for
those kind of things. Yeah, sure. So I guess like, there's like, there's mundane considerations,
and there's apocalyptic considerations. You know, that's kind of one way to put it. And I kind of
have different opinions on both of them. So like, you know, when that when it comes to kind of the
mundane considerations, you know, you have pros and cons of going for like as much transparency
and open sourceness as possible, right? Like, you know, on one hand, you have, you know, once you
have a fully transparent model, and you kind of know what, you know, what it is, how it was
trained, what went into it. You know, that's that's really good for auto ability. That's really good
for, you know, just kind of understanding like, why, you know, obviously, we were we're terrible
at interpreting transformers right now. But like, you know, presumably, that'll get a little bit
better over time. It's just generally useful to have some level of auditability until like,
how the how the model was trained. At the same time, you have like other kind of, again, mundane
kind of concerns, right? Like, you know, how do you how do you handle personally, or personal and
private information? You have an open source language model that is, you know, during its training
kind of overdubs some information about a, you know, certain individuals, and effectively sort of
like synthesized a lot of that. As part of its training, you know, how do you handle that? So
yeah, that's kind of like the mundane stuff. But I think probably the more interesting thing
that you want to talk about is is the apocalyptic considerations. And there it's a little bit more
yeah, yeah. And there it's a little bit more. It's a little bit more tricky. I think that probably
what you want is really you want sort of a sort of a kind of a buy, maybe not like a buy modal
thing, but you want a policy where there's a ceiling on what you're willing to, you know,
the level at which you're you're willing to kind of open source, you know, small models,
having them open source, you know, it makes it possible for, you know, companies and individuals
and then everybody else to kind of study them. But really, like once you get to the the really
dangerous systems, the systems that, you know, that can, you know, end the world. And even like,
you know, you do want to have like a buffer margin, essentially, between the systems that end the
world and the systems that don't, you really don't want those to be open. Because like, if you have
something that can destroy the world, and you distribute that to everybody, well, you know,
that just that ends the world. Right. It's not that complicated.
Share the models that can help us get a better understanding of how neural networks work or
how customers work. But whenever you get to like, very dangerous models, maybe, maybe try to like
not not share it on a hji.exe file. Well, I wouldn't even go further than that and say like,
those models should not exist. Right. Like if you're making a system where if it leaked out,
it, you know, it would be a complete disaster, you shouldn't have that and you shouldn't have
that for two reasons. One is that your own internal security is only going to be so good.
And there will come a time where there will be a security incident and those weights will get
leaked and you will lose control of the situation. The second is that as soon as something like that
exists, as soon as something like GPT-4 exists or GPT-3 exists, and it's behind a closed door,
people do not trust each other enough to be able to like have one single party trusted with control
over, you know, a very powerful AI system. Right. Because you can be cut off from, you can be cut
off from that capability. There's also like just like a lot of people will ask like, okay, whose
values get represented in the AI? They will go and say, well, okay, you're, you know, you're
doing this with it. I don't trust you. I want my own that has, you know, its own values and lets
me do this, this and that. And just the fact that you have it and just the fact that it exists
is going to motivate a whole bunch of other actors in the space to try and do a replication effort.
And that's effectively what you're seeing. Or at least in my opinion, that's what we're
seeing a lot of right now. So yeah, those kind of like concerns we're raising about open source
models, is it related to like things like Lama where people like accelerating on like making
those models like better, more efficient, bigger, or are we talking about something else?
I think that's definitely part of it. It's more than just I would say like Lama. It's just the
general like, you know, every other country is kind of noticing these things, you know,
the U.S. government is noticing it. Everybody sees that someone has a capability that they don't
and they will make efforts to try and close that capability gap. You know, whether or not they
succeed is another thing. Whether or not they have the budget to be able to do it. The thing though
is that the motivation is now there and that itself is concerning, at least in my opinion.
Yeah, I agree. It's a concerning that like governments are paying more attention to it.
And yeah, definitely like if you're, if you're paying attention to it and these
podcasts are doing the right thing, people might be interested in you, like what's your
background, like how you got into this whole like, like exit service from AI thing or like
deep learning thing, like where did Curtis learn to like do like a lot of my work or like
deep learning research? I guess on my end, my background kind of is a very informal background,
but it goes back a long ways. So like the first neural network that I wrote or kind of machine
learning I wrote or algorithm I wrote was like back in, I think like 11th grade, like almost
to the decade ago. So like, I think I was really kind of introduced to Kowsky's writings, probably
around like ninth or 10th grade or something like that. So probably 11 or 12 years ago,
something like that, 2011, 2012. I do actually remember giving a talk about like effective
altruism and existential risk back in like 2013 at like a small like gathering in a place in Canada
called Saskatoon in Saskatchewan, which is the middle of nowhere. Is this where you're from,
Canada? Yeah, I'm from Canada, originally Edmonton. But yeah, essentially, yeah, I had
been kind of following this for a very long time. And then since then, I've been kind of, you know,
reading papers and, you know, playing around with just, you know, small neural networks,
doing little experiments here and there. I did go on to kind of do some some internships
in bioinformatics. But, you know, the extent of like my formal training is not actually that
large. It's all sort of been self taught. Right. And I guess that's the best way to learn how to do
this thing. Everything is so recent, right? Did you first like learn how to like train big models
while having outweighs like those GPT Neo, GPT Neo X or like, like power research projects
at the Luther AI or did you arrive later? So I actually didn't have much of a hand in the,
the Neo and Neo X models, but I definitely did pick up a lot about distributed training just
from hanging out in the Luther AI discord. I learned, you know, quite a lot about everything from,
you know, how the low level kind of the GPU operates and the importance of the memory band,
of memory bandwidth and, you know, vectorization, all that all the way up to kind of, you know,
the various parallelism strategies that we use in large scale modeling or large scale training
today. Yeah. And now you're like working with a Luther AI on alignment projects, right? Like
head of alignment for like all the different products that are going on. So maybe can you
like give like an overview of like the different projects that are currently being done there?
Yeah. So I can talk a little bit about a few of them. So like one of them that I'm kind of excited
about is a collaboration that we're doing with the AI safety initiative at Georgia Tech. And we've
kind of got two sort of projects that are going on over there. So like the first one is looking at
language models kind of as Markov chains. And I don't know how, I guess like how familiar you are
with Markov chains, but basically the idea is that you can sort of view a language model with a
limited context window as sort of having a state where the state is the entire context of that,
of that language model. And then you can go and ask yourself, okay, what is the transition
distribution from like one state, so one context to the next state? And you can look at it and say,
well, okay, we add one, you know, we add one token by sampling. And so the distribution from the
net one state to the next is is the, you know, log probabilities of the language model. And then
after that, we pop off the last value from from the state, like the last token that pulls out of
the context window. And when you take that kind of view of what the language model is doing,
what you can do is you can go and say, okay, what are you can ask yourself questions like, you know,
what is the stationary distribution of this Markov chain? So like if you run the language model
auto aggressively, continuously generating new tokens, and then, you know, sampling and producing,
what is sort of the distribution of text that you're going to see kind of in the long run?
Another question that you can ask yourself is like, what is the quote unquote reverse Markov chain
or reverse process? So if you wanted to reverse the dynamics in time such that you append a token
and then you pop off the, you know, append a token back to kind of pop off the back token,
what kind of distribution do you get? And, you know, it's kind of like, you know, the first
question you ask yourself is like, what does this have to do with alignment? Right. And I guess,
like from my perspective, it's mainly that it is a largely unexplored domain and seems highly
relevant to just understanding language models as we currently use them. The reason for that is
because like when we train language models, we really do use sort of like the next token prediction
objective as the base. Like obviously, there's RLHF that comes after that, there's, you know,
supervised swine tuning that we do, but really a lot of the meat of the training is all focused
around this sort of next token prediction objective. And, you know, there's a little bit
of reason to believe that like, when you kind of run a language model for an extended period
of time, or really any sort of model that has been trained on this kind of next token prediction
objective, you will kind of get a little bit of error that piles up, you know, eventually
like the model will mispredict something. And then now it's input distributions a little bit off
from what it was kind of initially trained on. And then, you know, things kind of start to snowball
from there. Another kind of example of like a good of like where this sort of failure mode is
already sort of manifesting itself is, or at least like this, you know, this is me speculating, but
being when it came out, initially sort of you could keep talking with it going back and forth
as much as you wanted. And it pretty quickly kind of started going off the rails. Right.
But the solution to that was, okay, let's go on, you know, limit the the length of the conversation
so you can have a thing. And, you know, obviously, I can't say exactly why Microsoft did that. But
I do suspect that at least part of it was because of the model kind of starting to go off the rails.
So I guess like that would be sort of like an early manifestation of the kind of worries that
that kind of I have in mind when it comes to like this. Are you trying to like see what is the
like stationary distribution of that Microsoft saying of like the where does the model go? If
it like starts to like output like very, very long paragraphs of text. And at some point,
it's really like really just like bad behavior. Are we trying to like get to this like bad behavior
at the end? So not really, we're just trying to see like we're not even going that far. We're
just trying to say like, how does, you know, how like where does it go at all? Right. And then,
you know, questions like, what is sort of like the the mutual information like how does, you know,
does it it starts with one topic? How does that kind of change over time? Do you get like a,
distribution shift and sort of, you know, how long can you stay on the same,
you know, same kind of track, so to speak. So just just these really basic kind of low level
questions is where we're at. I think recently the the focus has actually shifted from that.
And kind of asking on the on the reverse side now, like if you start with like a bad, you know,
kind of a bad output or a bad state somewhere that you don't want to be.
Can you sort of like work backwards to say like, what was the chain of events
that led you to that bad state, according to the probability distribution specified by the
language model? So that that is sort of like, you know, it's a it's a very sort of exploratory
kind of direction. There's if you have to ask me like, you know, okay, how is this specifically
going to solve like, you know, one of the alignment problems? I'm gonna have to say like,
you know, it's it's not that that is not where where we're at. I do definitely think that like
having more lenses and kind of perspectives to be able to understand these systems is going to
be useful in general. And that maybe we're going to get to like another part later down the line
where we say, Oh, well, this lens is useful. Or this, you know, is a precursor to a training
technique that would, you know, mitigate some failure motor and other. Yeah, I definitely
agree that like, it's it's good to like do like specific experiments to like inform like, bigger,
bigger theory on alignment and or like, other projects that like, could like help like focus
on like specific things. I think it makes sense to like zoom out a little bit. And maybe for people
like, we don't really know what's the like, main goal of Aether AI, Aether AI alignment team,
like is because I think some people like might think of like, like academia as having like
professors trying to like publish papers, or like doing research for like a lab or something.
And in some sense, now I think Aether AI is like a nonprofit. So maybe like some people like donate
and, and then Aether AI does like research in some sense. So yeah, maybe like a better like
overview or like introduction to like, how is it like to like research at the Aether AI or
or an alignment? Like, what does it mean? Like, what's the goal? And like, what is like to like
open source stuff with like, other people like that? And I don't like maybe like full time employees
or. Yeah, so I guess I could speak like just generally about a Luther, because like, you know,
okay, we have like alignment and we have interoperability and stuff like that. But really the
the lines do get blurred. Really, like, the Luther tries to do all like stuff that is basically we
think the stuff that we think is sort of like net value, right? And that can be that could be
kind of anything, right? If we think that there's some interpretability research or some paper that
that makes sense to publish, we'll do that. If we think that there's like an alignment problem,
that it makes sense to kind of tackle that other people are not really, you know, spending a lot
of time on will focus our resources there. And it also kind of extends to kind of more like mundane
things, right? Like, one thing that you know, I'm less familiar with myself, but is also kind of
part of a Luther is is multilingual work. So like, a lot of current language model development
is very much focused on English. That is sort of the the dominant kind of the dominant language
for these language models. And there's also like, you know, there's there's a whole bunch of other
languages that don't get the same level of attention. And so at least partially, a Luther
does see like a really valuable opportunity there of like, hey, this is the kind of thing that really
isn't going to like, you know, increase our knowledge of how to build, you know, AGI and
shortened timelines. But it's definitely something that is going to kind of bring mundane and kind
of, you know, valuable utility to communities that otherwise wouldn't. Right. Right. So that is
like, like the general vibe there is like, we're going to make sure that we don't make the situation
worse. And if we see something that we think is is is a good idea to kind of pursue and to
spend our time and resources on, we're going to go for it, whatever that might be.
From what I know about the like, mostly more projects, there's like a guy like leading a team,
like every week or like in the past like year or so, like doing like Korean language modeling
or something. And correct me if I'm wrong, I might be wrong. And I guess like, he he like started
by doing the thing by himself and like, having people come and and then they like started the
project. But it's like, it's like very informal at the start. And now it's like maybe like more
organized. If to one, like listening to this or watching this wants to, you know, like,
work on alignment projects at Luther AI, or like just like contribute to like open source,
is there like any like, best ways to like reach out to you or, or like get started, like do you
have like shovel ready stuff to do is like micro change things, or is there like other
products that like, maybe like easier to to get started with? Yeah, we do have an email you can
you can add an email us at contact at Luther AI, or just, you know, Curtis, Luther AI, if you're
interested in talking to me specifically. But generally, the primary method that everybody gets
involved with is, is through discord. And like for this like particular like micro chain project,
how many people are working on it? And is there like a need for like someone else to, to do something?
I think right now we're pretty full on that project. I think there's like five
can't remember exactly the exact number, but I think it's like five people. And they're kind of,
you know, they're, they're sort of working at it. Again, this is this is from a collaboration with
the AI safety initiative at Georgia Tech. So they're at least, you know, partially kind of
handling the the management and stuff like that, at least that project. But just in general, like
the way Luther, especially the discord itself is actually structured is that we have like a whole
bunch of project channels. And so like what I would kind of recommend if someone wants to kind
of get involved is like, you know, just take a look at the project channels and see kind of what
people are talking about, see like, which research leads are sort of active in there. And then either
like make a general post in the channel itself, or go and DM whoever like the the active researchers
are for the project, if you want to get involved. And even just asking for the like the specific
project is probably the best way to kind of do it. In terms of leading, you're the head of the
alignment mind test projects, right? Yes, that's correct. What is the alignment mind test projects
and why is it useful? Yeah, so the high level kind of like long term goal, there's like a lot of kind
of like intermediate sort of directions that we find is kind of interesting. But like, the main
thing that I want to study is, I guess you could say it's like embedded agency failures in a toy
sandbox. So like, what do I, what do I kind of mean by that? If you look at, like, if you just
punch in reinforcement learning into Google images, you will get this diagram, the same diagram,
a bunch of different variations on the same diagram has like the agent on one side, and that has
the environment. And there's this like really solid boundary between the agent and the environment.
And you have actions going from the agent to the environment, and you have observations and rewards
going back to the agent, right? Unfortunately, this is not how the real world works.
Agents in the real world are embedded in that world, right? And a consequence of that is that
you like architectures and designs that would work fine in the, you know, the standard sort
of reinforcement learning setting are not necessarily going to carry over to the embedded
setting. And there's certain kind of failures that we expect to be able to see and demonstrate
when, you know, when that happens. Like the really classic one that, you know, is less
relevant because of the current techniques that we have, you know, RLHF and stuff like that.
But it's still kind of something that, you know, is worth kind of keeping in mind as an example,
is wire heading, essentially. And this is essentially what happens when a neural network
or an agent is trained with reinforcement learning and the reward signal.
And it gets sort of smart enough to realize that the thing that is, it is getting reward,
like it's not being rewarded for, you know, for whatever it is that we are giving it reward for.
We, it's getting reward because the little reward circuit is being activated, right?
Or like, you know, you could imagine like a setup where like a human has a robot and, you know,
the robot is kind of going around and, you know, when the robot does stuff that we like,
we press a button and we give it some reward, right? Well, eventually the robot's going to
get smart enough to be like, hey, wait, what happens if I press the button, right?
That gets rid of the human and starts pressing the button itself, so it gets into that reward,
right? So that would be kind of an example of like the sort of embedded failure.
That is a very simple example of the kind of failure that we're looking for.
There's significantly more kind of complicated failures that you can get with like
really any sort of value updating scheme. And that is sort of the long-term goal of the
mind test project is that we want to be able to sort of sandbox these scenarios with an AI that is
like ideally like sort of as weak as we can make it to still sort of isolate the
sort of embedded agency failure that we're trying to target so that we can then like understand the
failure so we can, you know, sort of get some idea of like, okay, how does like the value function
for an RL agent evolves when it starts to notice that there's this kind of failure mode happening.
And then like once we've kind of isolated that failure, then we can start to ask ourselves,
like, what kind of techniques can we develop to mitigate or eliminate that kind of failure?
So having some kind of like RL agent in an environment like Minecraft being embedded
in the world and having a failure mode where he starts hacking his reward, assuming his reward
was like inside of Minecraft and well, that's exactly it. Yeah. So the mind tester project,
one of the motivations for using mind tests instead of Minecraft or existing models is because
we wanted to have as much flexibility in terms of like, you know, programming things or
implementing different, you know, implementing stuff in the environment. So you could actually see
like, okay, let's put a button in the environment and let's have a player control the button and
let's have the agent also be in that environment. And let's have the, you know, let's actually just
do the experiment and see like, you know, under what circumstances does the agent kind of wise up
and say like, you know, hey, I can maybe just get rid of the other player and I can press the button
myself. You know, this is kind of the type of failure that we want to be able to investigate,
that we want to be able to replicate and that we want to be able to mitigate in the in the long run.
Obviously, the project is not quite there yet. And sort of like we're going on,
you know, kind of intermediate tangents. So I believe like in the first blog post,
where we mainly outlined was, you know, we trained a basic PPO policy to kind of punch trees.
And we said, okay, well, this is a really simple policy. Can we do some basic interpretability on
that? What does the, you know, what does interpretability on an RL policy look like? What does it feel
like? Do we kind of bump into problems that we don't really bump into in kind of other,
you know, other kind of environments or like other sort of training settings. And there's a
little bit of that that we've already sort of run into and that we're kind of starting to think
about like, what is the best way to kind of get around these sort of limitations?
Like a good example of this would be like, you incentivize the model to go and punch trees,
right, or punch wood. And one thing that you would expect as a consequence of that is that the
model would go and punch, you know, punch all of the logs in a tree. And that would be that.
But what you actually end up seeing is a situation where the model kind of learns
a strategy where it doesn't punch out the bottom lot. And instead, it, you know, punches out the
log above it and then sort of hops on and then is able to access kind of more logs at the top of
the tree. So like, if you're kind of a reward designer and you, you know, maybe want to go on
like get rid of the entire tree, that is sort of like an unexpected or unintended kind of consequence.
So one of the things that like I am currently thinking about is what is a good way to sort of
detect these sort of unintended consequences of the, you know, the rewards or the feedback that
we give. And, you know, how does that translate? Like right now, obviously, we did basically like
reward shaping and, you know, many really specified reward function. But like, what is the analog of
that with something like RLHF? So these are all kind of like questions that are sort of hovering
around in my mind. And so in your project, you have this thing with the log when it's like an
expected behavior. What's the next step? What's the next thing you're trying to observe?
So right now, we're kind of, you know, putting aside the PPO policy for a little bit. And we're
going to try and focus on like model based RL. And, you know, kind of the next step to enabling
that is just training models, generative models and seeing, you know, what can we understand
about those generative models? Because like, you know, there's, you know, language model
interpretability, right? There's vision model interpretability. There's, you know, RL policy
interpretability. And each of them kind of has like their own kind of unique challenges and
unique strategies and stuff like that for for getting places. And I do expect that like,
you know, a video model or like a model that actually also has like action outputs is going
to like represent, you know, I expect to see things like causality, I expect to see things
like tracking state, like if you see a tree, and then you turn to the left and then you turn back,
presumably there's going to be some machinery inside that generative model that is tracking
the state of like whether that tree is there. So questions like, okay, how do you look at like
state tracking inside of, you know, inside of a video model or a video model augmented with
actions? How does how do all these kind of things interact with each other? That is,
that is like the next step that we're taking with the the the mind test.
So is the basic idea to like see if the like the estimates from the agent of like how much like
reward he's going to get in the future, like gets like higher when he sees like a log, or like when
he sees something like when he detects something in his in the video, like saying like how the value
function he has like changes over time with like different inputs. So let's say that's like one
direction. I would say like it's even a little bit like like that would be kind of later down the
line where we're not even really like we're not really going towards the full model.
Yeah. Right. The way that we're kind of going to approach this is we're going to actually just
have users, you know, play the game. And then after that, we're going to take that data along with
like, you know, some pre training data, and just build some basic models of the environment, and
just saying like, okay, forget about value functions, forget about policies. Let's just focus on like
this one sub component of the of the agent and what can we get out of that? On the website,
I think it says that you're like, trying to build some gym, gym like environments. Is it done or is
it still on in the weeds? It's out of date. We, we have the environment. You know, obviously, we
needed the environment so we were able to actually train it. The next kind of step, and it's not that
big of a change, we'll probably get it done pretty soon, is that we're planning on collaborating with
the Ferama Foundation. And they maintain an updated version of the gym environment, along with like,
I believe, like some multi agent APIs. So kind of another kind of, you know, place where there's
work to do. And there's also on the roadmap for the project is, is support for sort of these more
modern APIs, essentially, and just swapping out like the old open AI gym API. But yeah, we've had an
environment working for for quite a while now. And we've used it to train some agents and defense
of interpretability on the policies that are learned. That's been, you know, the website is
a little bit out of date. If I'm like a deep learning or a real engineer, like listening to this,
and I'm kind of like interested, what kind of like compute do you have for this? Or do you just
like, like, how big is the model? Is it like just like a standard size model for like this kind of
like video processing and like to, you know, have people on top or what kind of like model are we
talking about? It's tiny. It's a very tiny model, the one that we trained. It's like, I actually,
I think we outlined it in the blog post, it's like three convolutional layers, a fully connected
layer, and then like an action and critic cat, it's a very tiny neural network. Scaling things up,
I do imagine is going to make the interpretability significantly more, significantly more challenging.
I think, I think like one thing, like another kind of thing that came out of it is like,
you can have a functional policy. But it is, it's relatively difficult
to like, especially as you kind of scale things up, you get like more noise in the system.
Like one thing that we noticed is like the learned policy is really not that symmetric,
but it works anyways. So like, you know, you're thinking, well, you know, the environment is
mostly symmetric, you know, maybe there's a little bit of a change with like, you know, the textures
being oriented one way instead of the other, but like, it's not enough to explain what's going on,
and I think it's really, it's just like an initialization noise and RL noise and all these
kind of things that are sort of conspiring. And the net effect of that is that you can get a very
noisy, very not great policy that doesn't have very clean internal representations,
but it will still get the job done, it will still kind of execute, you know, and execute a policy
that actually does work in the environment. I was going to make the joke that maybe in the
Minecraft environments, because the character has like a sword or like a tool on the right,
maybe does making like the image not symmetric. Well, you see, you can test that by like,
mirroring the, you know, I mean, that's actually, you know, that could that could actually explain
it because obviously, like during actual training, you know, that's not what's going on. So that,
you know, that could actually be what we've seen. But yeah, that is just generally we just
haven't seen the symmetry. So that could be that could be what's going on.
So yeah, I know the thing that like you mentioned in the blog post is that the main goal is to
understand courageability, so how to make models more likely to be corrected by humans. Like,
can you say more about like what courageability is and why you're interested in these projects
like help with it? Yeah, so courageability is the ability to, well, I mean, as the name implies,
to be able to correct agents and models after they've been deployed. I guess even during training,
depending on like what assumptions you make. But really just being able to say, hey, no, don't do
that. You know, I want you to do like y instead of x, right? And like, you know, in the in the RL
case, you know, you get a really serious courageability failure where the agent, you know, in the button
example, going back to that, the agent going in and just like, you know, getting rid of the human
and then just pressing its own button, that's like a failure, but it's not really a courageability
failure. A courageability failure is one where like the agent wants, like say you instructed to go
cut down some trees. The agent wants to go and cut down some trees. And maybe you change your mind
and you say like, well, I don't really want you to be cutting down trees anymore. I want you to be
doing something else, right? A courageable agent will let you go and modify its source code, or
like, you know, give it whatever, you know, use whatever reward mechanisms you have, or whatever
mechanism really you have to edit the model to change that behavior so that it stops doing the
thing it's trying to do, or it's originally trying to do, and kind of moves on to doing something
else. And this is a little bit like unnatural, because well, if you want, if you make an agent
that wants to cut down trees, it's going to naturally reason, well, if the human changes
his mind, and doesn't want me to cut down trees anymore, well, maybe it's even smarter than you,
and it sort of like sees this happening before you even, you know, are aware that you've changed
your mind. Maybe it's going to go on like throw some obstacles, or maybe it's going to deceive you,
or maybe it's going to do like one of a many, you know, a bunch of different things so that it can
keep doing its street cutting. And you're sort of, you know, cut out of the loop, essentially, of
deciding what it does next. And so a courageable agent is one that has that, you know, at least in
my kind of understanding, is one that has that property of, you know, it will go, and if you
change your mind, it will go along with your, your change of mind. In some sense, like the agent is
trying to do what is best for you, and tries to check if you want to turn it off or not,
and he's not trying to like prevent you from seeing what's going on. And I think it's like pretty,
pretty hard to get, I'm not sure if there's like any examples of agents like preventing
like humans from seeing what's going on like in real life, or if it's like toy experiments right
now. So that's the thing, right, is I believe at the moment, most of the demonstrations are kind
of like analysis that you have, are either like, you know, very simple decision problems like the
off switch game, where there's like theoretical analysis of, you know, if the agent believes X,
or you know, you make certain assumptions about the agent's beliefs of the player,
or you know, you specify a certain training setup, you know, then you can get a theoretical analysis
of like, okay, under this assumption, you get this behavior, under that assumption, you get that
behavior. And then like, you know, the level above that is like grid worlds. So I'm aware of like
off switch grid worlds where like a user has an off switch button that turns the agent off and
prevents it from continuing to do what it was previously. But you know, anything more sophisticated
than that, I am not aware of, of anything like that really existing. I think just like people
that are like watching this might not like not know exactly what is the off switch game. Can you
just like explain what does this game? Yeah, so I mean, like at a high level, the off switch game
is really like a simplification of kind of the more complicated example that I put where it's
like the agent is doing a thing, and there's an off switch. And when you decide that you don't want
the agent to be doing the thing, you press the off switch, and the agent won't try to interfere
and prevent you from from pressing the off switch might not be exactly that my recollection of the
details of the off switch game are a little bit fuzzy. I'm more just aware of like, this is like
the simplest example of corrigibility that I'm aware of. I believe it's Dylan Hatfield Manel
is probably the words you want to google with regards to the off switch game. I think there's
a paper on it from like 2016 or 2014 years. And I think like they model it as like some
game game theory experiment. And maybe there's like some some way of
like, like knowing exactly what's like optimal values you give to your to your agent so that
they accept to like be credible. Yeah, how excited are you about like, you know, like finding this
this credibility inside of a Minecraft environment or like mind test. So I think we're got I think
we're going to be able to fairly easily demonstrate the failure modes that we're looking for.
Fixing those failure modes is kind of another story entirely and especially being into doing it
do a reliably seems like it's going to be quite a challenge. I have like some vague intuitions
about ideas that I can't really articulate yet about how to kind of go about actually tackling
the problem. But nothing that I would say is is super concrete. And I think that it's it's probably
better to wait until we have like a solid demonstration before we really start thinking or
like, you know, deploying things because what's going to happen is we're going to we're going to
get an implementation and we're going to get some data and like whatever, you know, some of at least
some of the assumptions that I'm kind of making right now are going to be proven wrong. So so
yeah, that's that's kind of like where we're headed right now. I wonder like how equal as RG
it is to like post something about race. I think the cat's out of the bag on that one. The people who
have the the people who have the 8800 to use order, they probably know it was going off.
Yeah, the people who are making these orders are aware of the other people also making these
orders, right? This is nothing the relevant actors are all aware of what is going on.
In fact, that is part of like what makes it so, you know, terrible is that they are very, you
know, the people with the most, you know, impact or chance to really like, do these kind of things
are the most acutely aware and the most like ready to race, right? Because if they weren't
racing, they wouldn't be trying to raise a billion dollars to build these giant supercomputers so
they can build AGI, right? So yeah, that is that is yeah.
It was a pleasure to have you. Do you have like any last message for for the audience for other
through AI, the world, the machine learning engineers, the people ordering GPUs online?
Come hang out at our discord. Come hang out on our topic. Say hi to the people there. Yeah.
Good. Come hang out on the other three AI. Thanks. It was a pleasure to have you have a wonderful day.

Imagine AI is a Rubik's cube.
Sorry.
Imagine AI is a Rubik's cube.
And you just need to align it with the color.
You know, it's like the weird meme on Twitter about like shape
rotators, doesn't it?
Oh, yeah, yeah, yeah.
Yeah, no, my roommate jokes all the time.
You're like the per, like the most stereotypical shape rotator.
The cool shape rotators now do AI alignment.
That's right.
That's right.
That's right.
You're speaking like chat GPT.
Oh no, I'm sorry.
I'm a language, I'm a language model by OpenAI.
I don't come into any regulation, but I think.
I think every story of alignment that is actually scary to me
involves AI systems, basically deceiving humans in some way.
Like if, you know, if the model is totally honest all the time, then
it's really, you can just ask it, like, is this, you know, are you
going to do something dangerous or not like deliberately?
And it's like, yes, I am.
Then, you know, you are a second year ML PhD at Berkeley working with
the one and only jack-of-tenor and Dan Klein.
Your focus is on making language models honest, interpretable, and aligned.
And you've also worked on multiple benchmarks, including apps for
coding from the math that I set and MLU for language understanding.
More recently, you've published on archive discovering that knowledge
in language models without supervision, a paper that received a
lot of praise on Twitter and less wrong, including from Elisabeth
Kowski, who said that this work was very dignified.
Um, if I didn't publish a less wrong posts explaining how these last
paper fit into a broader alignment agenda, and we're going to be talking
about a lot about this paper today.
So thanks for coming here today.
Yeah, thanks for having me.
So a surprising fact about you is that you once broke the official
world record for solving a Rubik's cube in five seconds.
Yeah.
And I remember watching this like Rubik's cube video when I was a kid.
And I think it was not you when I was watching other ones and like, I
don't know, in 2011 or something.
But you never learned how to solve it or anything.
I once did it in 50 seconds.
Oh, nice.
Okay.
Um, so I did those, I think it's F2, F2L.
Yeah, yeah.
Uh, I learned some of these.
Um, but yeah, I wanted to know, like in your experience, was the five
seconds pure luck?
I would say it was some luck and not just pure luck.
Uh, so I, I mean, I think at my peak, I was maybe like in terms of, um, average
for like solving it across many, many solves.
I was something like in the top 10.
And so I think it was like, you should have have to be good enough to have even
a chance of getting the world record, but also certainly luck was an important part
of it.
Um, and this is always true, especially for like the single best solve where
you're really selecting for that.
So, uh, so definitely part of it.
Does it happen that someone, yeah, sometimes you have something that requires
only like 10 moves or something.
Not quite that, uh, not quite that easy.
So I, if I remember correctly, so first of all, every Rubik's cube can be solved
in 20 minutes or less.
So this is, um, sometimes people call it God's number.
Um, but actually most, uh, most configurations of the cube can be solved
and I believe 16 to 18 moves.
Like it's actually sort of concentrated in, in kind of a small range.
Um, that's less than 20, but, but still kind of close.
On the other hand, the actual methods that people use to solve it efficiently,
um, which is sort of more computationally efficient to, for a human to solve,
um, or to think about, um, that's less, uh, move efficient.
So that's, I want to say more like 50 to 60 minutes or something.
Um, so, so it's more like, you know, maybe if there are, um, four stages to solving
it, it's like the last part, uh, it was sort of easy or something like this.
So you've arrived in some kind of easy, um, last cross at the top.
Something like that.
Yeah.
So, um, the way it works is you first solve the first two layers.
Um, and this, this is actually the part that takes the longest in some sense,
but, um, and that's very intuitive and, uh, uh, and so on.
And that was sort of the part that I was good at.
And then after that, you solve the last layer.
Um, if you think of it as three by three, it's like first in the last one.
Um, and then for solving the last layer, you usually do, uh, two algorithms.
Um, so one for orienting, um, the pieces and then one for permuting them.
And so in that particular solve, um, I believe, uh, the last part, uh, like
the permuting, um, that was already solved by default.
And that happens maybe like two percent of the time.
So it's not extremely rare, but it, you know, it was definitely lucky.
So.
And you've also made like some YouTube tutorials.
Yeah.
Yeah.
You had a choice.
Actually, in fact, so before I even get into that, uh, one fun fact is I learned,
so F2L, which you alluded to before, this is the first two layers.
Um, I actually learned this from Andre Carpathi.
So, so he, uh, so back in the day, he, he had, uh, I think the best tutorial
for how to solve, um, uh, first two layers and, uh, and yeah,
I, like I said, that, that's sort of the part that I ended up becoming good at.
I was actually like very bad at the last layer, relatively speaking.
Um, cause I didn't like memorizing algorithms and things like this.
Um, I, I'm definitely much more into the, the intuitive, um, aspect of solving
it thing, um, solving the cubes.
Um, and I learned that from him.
You can do like, um, the first two layers by just like thinking about logic or
like, what are the like, um, ways of doing it in general.
And I believe you need to like put, um, somehow two things together from the
first and the second layer in a way that you can balance it and, um, do
like the edge thing at the same time.
Yep.
Yep.
So maybe there's something about being good at Rubik's Cube that makes people
good at deep learning research.
I wonder.
So yeah, how did you shift from maybe in your being like 14 years old and 15
years old and you're thinking about Rubik's Cube to becoming a deep
learning researcher, publishing at Neuribs and Eichler?
Like, how did I get into this?
And yeah.
So I actually originally, um, yeah.
So I mostly cubed very actively in middle school and high school, um, and
sort of stopped around the end of high school.
And, um, I was originally planning on doing physics, um, or that, that was the
thing that I found most exciting for, for a very long time.
Um, but at some point I, I sort of heard more about AI and I think I became
convinced that this would just be this very big deal, um, or had a good chance
of being in this very big deal in my lifetime.
Um, and I sort of became disillusioned with just doing research that seemed
interesting, but not, not really deeply important for the world.
Um, and so that's why I started learning about, uh, computer science and AI
sort of at the end of high school.
When was that in terms of years?
Like in like 2015, 18?
This was like 2016.
Maybe did you, did you read like anything about AI that made you believe
it was kind of important?
Yeah.
So, so I, I, um, uh, I guess I met some people.
So I actually went to the summer camp in high school where I met, for example,
Jacob Steinhardt and, and some other people like Paul Crestriano and, um,
uh, not out of this cube summer camp, but no, no, no, no.
So, so yeah, this is more like a lot of, yeah, like most of the, the students
there did like math olympiads and things like this.
Did you do any math olympians?
No, I didn't really know about math olympiads before then.
So actually, I guess a bit of a digression, but, uh, in middle school and
high school, I was actually homeschooled.
So I mostly just taught myself.
Um, so this worked out really well for me because it meant I had a lot more
flexibility and, um, and learning like what I wanted to learn.
And, and, um,
How did you go from like, um, like knowing that like AI is important to doing
like AI, I mean research.
I guess like if you, the first person was like Jacob Steinhardt, Paul Crestriano,
you might be like a lemon peel by them.
Yeah.
Yeah.
So I, I, um, right.
So these people were sort of concerned about risks from AI.
And this is actually, this is basically what motivated me to get into this.
Um, I mean, initially these sorts of concerns seemed sort of plausible, but not
obvious.
And I felt like I couldn't actually evaluate them properly.
I didn't have the background to do, to do so.
Um, but it seemed sufficiently plausible that I, I decided to spend a lot of
time learning more about it.
Um, and so I, um, I guess just on the side, I, I started reading a lot about it.
And sort of gradually over time became convinced that I should at least
likely do this.
And then I became convinced that I should definitely do this and so on.
Um, did it appear as like, you know, more and more important as time goes,
like maybe in 2016 it wasn't that pressing, but now it feels more pressing.
Yeah.
Yeah.
I mean, certainly, um, I'm sure we'll get into timelines and progress and things
like that, but certainly the, um, it feels much more urgent than I did before.
Yeah.
You mentioned timelines.
You're, so you're doing a PhD, uh, in, I guess in CS, but more
machine learning with Jacob center.
He's famous for having those, um, benchmarks.
Um, so math, apps, MLU, and he, um, I think he hired some forecasters to do
some forecasting about when we will we reach some level of accuracy on math.
Yep.
And, uh, famously, uh, we reached like 50% in 2022 when we expected to arrive in
2025.
So yeah, did it surprise you at all?
These like performance in math.
I think you were another in math, right?
Yeah.
Yeah.
So, uh, yeah, I was an author on that, but, um, but I should mention, I think
like Dan Hendricks gets a lot of the credit for the, the sorts of, uh, those
papers and he really pushed for those.
So, um, uh, but I was lucky to help.
So, uh, yeah, I feel like I was, I was certainly impressed.
I think I personally wasn't as surprised, but I think at that point I had already
had, um, relatively, I was relatively bullish on, uh, AI progress already at that
point.
And, and so yeah, I can get into exactly like, why did that seem like not totally
crazy to me?
But I mean, I think part of it is if you just look at, uh, a bunch of, uh, a bunch
of benchmarks in ML and especially NLP, um, a lot of the time they come out and
performance is initially really low, but, uh, if people haven't really worked on
them, it's, it's often the case that like progress really jumps pretty quickly
and, and you get, um, like pretty rapid progress quickly.
And so I think just, just because progress, like accuracy, I think was 7% or
something, um, initially for, for match, um, I didn't really expect it to stay like
that, um, for very long.
And I think I also had this intuition that like there's a lot of low hanging
fruit for, for math.
And I mean, we sort of solved this with, with Minerva, like it was based on very
simple ideas, um, and scale.
And, you know, it wasn't totally trivial or anything, but, but it was sort of
getting at a lot of that low hanging fruit that I, I thought would be there.
And so, um, I think it was less surprising to me, but it's still very impressive
to be clear.
And so what were the low hanging fruits?
Yeah.
I mean, things like, um, like chain of thought and like consistent or like, you
know, do like run chain of thought a bunch of times and then take majority vote
or something like this.
And, and I think they did like more pre-training on like mathy, um, data.
I forgot exactly what it was.
Maybe it's archive or something and, uh, little things like that, which are
really straightforward, but make a huge difference.
I mean, I think we've seen this just over and over again in ML.
It's like really simple ideas are extremely powerful.
I mean, even if you just look at like, what are the most important
deep learning papers, it's like, you know, it's like ResNet.
It's like, you know, replace f of X with f of X plus X.
And it's like, okay, suddenly you can like train much, much deeper networks.
And it like, it's, it's wild how, how simple these, these ideas are.
I mean, similarly with, you know, BatchNorm, it's like, okay, you normalize things.
And, um, this isn't to say it was easy to get there.
And like these were, you know, it's hard to, to come, you know, it's always
simpler in retrospect.
Um, but that doesn't change the fact that these are really simple ideas that
are really powering, uh, all those progress.
And that's sort of wild.
It's kind of counterintuitive to have a ResNet pass the, you know, the f of the
X transit to like a more advanced layer.
Like you, you wouldn't do it before, but right now it's like, it makes a lot of
sense, but I guess this is kind of, um, a parallel to your paper that is like,
kind of like obvious in retrospect and kind of simple, but, um, people,
this is kind of very useful at the same time.
Um, but I believe like people who were like built math thought that like the
math problems were kind of hard.
Um, so math was released in 2020 or 2021.
I believe 2020, late 2020.
Yeah.
So if, if something is released in late 2020, um, you don't expect it to be
solved like one year and a half later, right?
You expect it to take a little bit more time.
I don't know.
I mean, if you just look at normal NLP benchmarks, it seems like a lot of the
times they're released and then, and you know, people try to make them hard.
And then a couple of years later, they are basically solved.
Uh, I mean, to be fair, I think math is not solved by any means, um, but I wouldn't
bet strongly against it being solved in the next five years or something.
Do you remember what you've done on this paper?
Like, did you collect data?
Did you run models?
What do you do?
Yeah.
So it was a mix of, um, I mean, all that, like collecting data, um, uh, running
experiments and, and baseline and things like that.
Um, so we also tried, um, some early version of something kind of like
channel thought where, I mean, basically we, we collected math to have, um,
solutions, like step-by-step solutions.
And we, you know, we tried to train a model on this, but it seemed like it
didn't actually help in that case and made things worse.
And now it seems like if you look at various results with chain of
thought and so on, um, actually need sort of large enough scale for that to help.
And, and if you do have, uh, small enough models, then it actually hurts.
And so you actually see a merchants there.
Um, those, those are another thing we, we tried.
Um, so yeah, things like that.
And so I guess Minerva, um, kind of picked his looking foods, uh, train on, uh,
archivist sort of thing and reached 50% of accuracy.
And I talked to one of those Minerva papers, authors at, at Neuribs asking
him, um, if, if, you know, getting super human level in math or gold medal in
math was like very far away.
Yeah.
Um, yeah.
You mentioned then Hendricks.
I think then Hendricks told me once that, uh, some of the people think that we
can get like super human in math in a few years.
Yeah.
Um, and then a number that keeps coming is something like gold medal by 2026 or
super human level by 2026.
You're saying math is far to be solved.
Um, in my, in my opinion, I am more in math.
It's kind of hard to get due to your thing.
I, gold medal by 2026 is, is, is possible, like more than 50% likely.
It definitely seems very plausible.
I think I don't feel super strongly either way.
Um, I feel like my, my gut is like, I don't know, it seems maybe 50, 50 or something.
Um, maybe a bit less than that, but somewhere in that range.
I, I mean, another thing, so you mentioned that Jacob, my, my advisor, um, uh, like
hired some forecasters and so on.
And, and our group also, um, just for fun and for practice and, and because it would
be interesting.
And, um, and so on, we, we also did some forecast of ourselves, um, for ourselves.
And, um, uh, and one thing I took away from that was often one of the, the key
considerations for like, what will performance be on this task is how hard
will people try to, to, to solve this task?
And so, for example, if, you know, the entire ML community just tried really
hard to, to solve math, um, like, and like by 2026, I, I, then I would bet like,
I don't know, maybe like 75% or something that we get IMO by then or something
like that, maybe higher.
And, um, but like right now my sense is there are like maybe a couple of groups
that are aiming for that.
And, and so it also depends on how much energy and effort is actually put into that.
So the question is, uh, like the time it takes between just like people
solving some like toy math problems to like, they're being close to solving
like gold medals and then it becomes like, I was like a trophy, like, you know,
deep mind wants to, you know, solve math and yeah.
No, this is why it seems more plausible.
Like if it were some kind of random benchmark that people didn't really care
about, then I think it'd be definitely less than 50%.
Yeah.
I mean, maybe it would actually put it at like one, one in three or something,
but, but definitely in that range of like very plausible.
If we talk about like other benchmarks, like apps and MLU, I think we've seen
like results also on these, like, did you update at all in the progress this year
or were you like already like with like short timelines and I think I didn't
update that much on progress this year because I already had relatively short
timelines.
I, that said, I, I do think I, yeah, I did spend some time, some more time
thinking about timeline.
So, so I guess I, so back in maybe it was like March or April of 2020, that's
when I actually started thinking about timelines for the first time.
And so I had lots of conversations with Dan Hendricks actually around that time.
And, and so I think the initial thing that was impressive was like Blenderbot
back in the day.
And so this is before GPT-3 a little bit.
And that was my first time interacting with an NLP system for real.
And even though it was pretty GPT-3, I found it very impressive.
And so I think, so that's when I started really thinking about timelines.
What, what does Blenderbot do?
It's just a dialogue system.
It's like chatbot, but, but it's definitely better than I expected.
And even though it probably would be pretty unimpressive by today's standards
at this point, but anyway, so I thought a lot about timelines back in 2020.
So partly because of that.
And then partly after GPT-3 came out and just seeing like the progress
and the NLP was very impressive.
So I think that's those sort of what informed a lot of my timelines.
And then, yeah, I did spend a little bit of time more recently just thinking
about like updating that a little bit.
And mostly think about like, you know, what, and like, what, what do I think
is like the path to, to AGI or something like that.
And, and I think my conclusion was like, it doesn't seem as hard
as I would have thought, basically.
So between people, we think about the things talking about timelines is natural.
But imagine if you have like a random, deep learning researcher looking at
what do we mean by timelines and what do you mean by AGI?
I know there's like not one definition, but I guess like time line is like
10 months for what, from affirmative AI, to improving AI, AGI, what do you think?
Yeah, this is a great point.
Like I think I don't personally feel too wedded to any particular definition
of AGI or transformative AI or human-level AI or anything like that.
I think, and so yeah, actually my, when I, when I say timelines, I often
implicitly mean, when do I think it's sort of 50-50 that we would have
these sorts of systems?
And usually when I imagine these sorts of systems, I, I guess I'm a little
impressed about it in the sense that sometimes I mean, just when can it,
when is it basically human level on like the tasks that we care about?
But that's also a little ill-defined.
Like, are we talking about- What do we care about?
Meeting human or are we talking about like expert human or something like
across every domain or what?
And so on, I think for me, the relevant, or I think a, yeah, a milestone
I often think about is like, when will we be able to automate AI research?
Because like I, and there, there are these like old classic arguments
about for crystal self-improvement or something.
And I'm not sure, I'm not sure if I buy the like old classic story of that,
but I do think there's important truth to that of like, yeah, actually
these systems could, could sort of accelerate, um, future progress in AI.
And, and that could be very weird and different.
And, uh, so I think that's a very salient, um, measure I think about.
But, uh, I think there are lots of reasonable choices here.
And, um, I think I would probably give different answers by maybe up to
like five years in either direction, depending upon the exact definition.
So up to five years, uh, what, what's the, um, when do you think we're going
to get something that can automate AI research like this decade or the next one?
Well, so I'm also being imprecise here because like, what does it mean
to automate AI research?
Is it like, you know, is it able to write a median level like
NURBS paper or is it something- I call it Burns paper.
Uh, well, but it also depends on what you mean, because like, I, I don't
really work on capabilities of like, I, I'm not trying to make, um, performance
on standard benchmarks as good as possible.
I, I'm definitely focused on, um, things like emerging problems.
And like, how do we even model this and, and so on?
And like, what are the like methods for doing this at all?
Um, to get any progress on this problem in the first place.
Um, but you know, that, that's not the relevant type of research for
automating AI research and speeding up progress.
For that, the relevant thing is more like, can we, can we just like invent
transformers or something like that?
Or, um, the next transformer and so it seems that for those kinds of things,
you need to run experiments.
Yeah.
So you need to have something that can automatically write code, run experiments
and then think about what experiments to do next based on the previous experiment.
And after all this being like, huh, if I submit this to nerves.cc, uh, I might
get an eight and get accepted.
Yeah.
Yeah.
Um, this seems pretty hard.
Uh, but then if you just think about like one conceptual alignment paper, um,
maybe you don't need all those experiments, right?
So I think it's a little complicated.
Like there, there are many different ways.
Um, one could imagine automating, automating AI research or AI alignment research.
So, um, for example, with AI research, you could also actually just have
some metric of like what is performance or something.
Like suppose you, you're just given this data and you want to maximize, you know,
or like minimize perplexity on this test data, like just find the best
architecture for that or something.
And that, that does feel more like, okay, maybe you can, maybe you can just
like optimize that sort of directly.
Um, uh, which might lead to superhuman performance because you have this,
this explicit metric that you might care about.
Um, on the other hand, yeah, I think in some ways, like maybe something
that's more conceptual and like, you know, where you don't need to
run experiments and so on, maybe that's actually easier to, to, um, to automate.
And, and certainly, um, uh, certainly like this is one of the main approaches
that opening eye is taking, for example, like let's, let's try to automate AI
research, um, uh, and perhaps we can start doing this soon.
And, um, I mean, I'm somewhat sympathetic to this.
So, or I think in some ways I'm, I'm optimistic about alignment in relative
to many people who work on it.
Um, and so I think it's, yeah, I think there's a reasonable chance this
works, but I also definitely wouldn't bet on it, something like that.
And, but I think it's like one of the things that we should try among other
things.
And so, um, it seems very possible.
So do you think, do you think this decade we might have people like using AI
to produce new ideas or, um, I don't know, just like speed up their research
by like 10% or even like 25%.
Yeah.
I mean, I, for what it's worth, I, I think even already just having access
to like get a co-pilot is already like, yeah, I don't know.
Like it probably speeds up my coding by like more than 20%.
Uh, but does, does accelerating your, your coding accelerate your research?
Right.
So it's like, okay, what does that mean in terms of overall research productivity?
And it's like definitely less than 20% than probably, but, um, but yeah, no,
I can imagine like actually pretty substantial gains this decade.
Um, yeah, possibly in the next five years, but I feel less sure about that for sure.
In the next five years, let's say in 2027, uh, you log into your computer and you're
like, I am stuck on this and here's my problem.
And the AI will be like, Oh, have you tried this?
Or we'll be like, Oh, this is a new idea I thought of.
And, um, we might just say like, Oh, I think, I think this is the paper you
should be writing and gives you like an eight page PDF.
Yeah, I'd bet against that particular, like at that level, but I think certain,
like certain aspects of that feel easier than others.
And so I, I could imagine certain.
Yeah, I can imagine something kind of in that direction, but not quite that good.
Um, and it also depends here.
Like I, I guess I'm currently often talking about like, when do I think it's 5050?
But I think actually that's not the most decision-relevant thing.
And so I actually feel relatively uncertain about that.
Like my numbers for like, when is it 5050 we'll have this thing?
Um, but I feel like the more important thing is like, is there a pretty good
chance that this will happen soon or something like this?
And so I think it, I think for me, often, um, I care more about.
Like acting as though things happen soon, because I think that's the world where
like what I do matters the most.
Um, and it feels sort of safer.
Um, some people say the opposite.
Say that, um, if you just like have very short timeline, then your work, your
work, it doesn't have any impact.
So you need to like behave like if you were in a world with like longer or
like medium timelines.
Yeah, I don't really buy that.
Or I feel like you can do research now that has like an important effect even
in the next few years or five years or 10 years or something.
Good transition to your research.
So yeah, you've written discovering later knowledge in language models without supervision.
Um, I think this paper is quite simple, but both like very insightful.
I've been discussed a lot on Twitter, uh, and that's wrong in the past few weeks.
Um, but maybe assuming the audience doesn't know much about language models and
all the things, uh, maybe can you just like give a quick summary of your paper,
like in the big lines?
Sure, sure, sure.
So, okay.
So what is the problem we're trying to address?
I think, um, so we actually, I will give sort of the, um, the long-term motivation.
So I think we don't talk about this, um, or don't talk about all of this in the paper,
but I think it's, um, important or, and, and able, like, I think we can do it given
what we've talked about so far.
So I think, um, as systems are becoming much more capable very quickly.
Um, I think language models, especially are just seeing very rapid progress.
Um, the thing is we currently basically don't know how to get these models to tell
the truth.
Um, so we have, um, some techniques for like, okay, maybe we can train models to imitate
truthful texts or something like this.
This is sort of the default approach that we have for making models truthful.
Um, so the thing is, like, models can also be more capable than humans.
And I think this will become certainly true in the future.
Um, in which case, I think that standard techniques for making models truthful
just will break down.
And it's also worth noting that I think even with current techniques for making
models truthful, it like doesn't always work in practice.
Like, um, for example, open AI is like trying really hard to make models truthful
and, and like void hallucinations and so on.
Um, but chat GPT still like makes stuff up all the time, even if, um, and,
and they don't know how to completely avoid this, um, as far as I can tell.
And so, um, so this just seems like an issue that is, um, like a real problem
in practice today that will become, I think, much more severe in the future.
I think it's more severe in the future, both in the sense that like if models
lie to us, I think the consequences could be much more severe and also more severe
in the sense that it's harder to actually avoid.
Like we don't really know how our tech, it's sort of clear that our techniques
will not scale to those models.
What do we mean by being truthful or lying here?
Is it, um, like, perversively like lying, deceiving?
Yeah.
So I think there are different, um, different notions of truthfulness and honesty
and all of these related terms.
I don't feel very committed to any of these particular definitions, but I can
still say some, some things that are, that are maybe helpful here.
So you're speaking like chat GPT.
Oh, no, no, I'm sorry.
I'm a language, I'm a language model.
I don't commit to any particular, but I think.
So, so, okay.
Um, true.
Yeah.
So what do I mean by truthfulness?
So I guess first of all, to answer your specific question of do, do I mean
models, like our models deliberately lying in?
Is that what I care about?
Um, I mean, currently, currently models, I think, don't really deliberately
lie in a super meaningful sense.
I think it depends what you mean.
So I think our models hallucinate stuff and like make stuff up.
Um, because they, they sort of predict plausible text.
Um, not, not factually accurate text or anything like that.
Um, but you know, in some ways they can also lie if you like prompt it in a way
that, that causes them to deliberately out, but false things sort of knowingly
in some sense, um, it's not clear what that means exactly.
But, um, but I do think once you get to RL systems, um, then I think you can get
more, uh, more egregious forms of like deliberate lying.
Um, or, or this is, this is a strong prediction I would make about the future.
And this is the sort of thing that, uh, I'm concerned about.
And so, um, for example, if you just maximize some reward, like, I mean, let's
just say like, do humans like this or something like this, this text that I
produced, um, then it sure seems like, you know, sometimes humans would not like
to know the truth, um, you know, if there's some inconvenient truth or, you
know, if the model behaved poorly or something, um, but it can sort of hide
that fact from the human, then, then it might have an incentive to lie there.
Um, or you can imagine much more, you can, yeah, you can certainly imagine
much more malicious forms of, of lying as well, where it's really deceptive
and like really trying to manipulate the human.
Um, I mean, we also see now with, I mean, diplomacy is sort of, um, in some
sense solved or like, there's been very, very impressive progress on that.
Can you maybe remember, um, our listeners, what is diplomacy?
Yeah, so diplomacy is this, um, is this board game where, um, basically
there, I think there are seven players and you're sort of in charge of this
map, like Europe in like the 1910s or something like this and, um, and unlike
most board games, there's a, a very large focus on, um, negotiation.
And so this is, includes cooperation and also lying.
So it's sort of famous for, for lying and backstabbing and things like this.
Um, and it's very open-ended.
Like how you can just talk about whatever you want, like alliances and so on.
Are you good at diplomacy?
I've never played diplomacy.
I, I'd really like to though.
It sounds a lot of, sounds like a lot of fun.
Um, as long as it doesn't ruin friendships, uh, I'm not sure exactly what it,
what it's like in practice, but, um, anyways, so, so there's some recent work.
Um, by, uh, fair, um, by, by meta, um, they created this model called Cicero
that, that, uh, does diplomacy and, um, I think it's not really clear just how good
it is, but it does seem pretty good.
And, and so they, they claim it's something like in the top 10% of humans who
played at least one game.
I think I'm just not sure what that means exactly, but it's probably like at least
good.
Um, and so I'm not sure if it's solved or what, but it's, um, it seems pretty good.
And so I think it was like Blitz diplomacy.
So it was like, yeah.
So, so right there, there are all these sorts of qualifications.
I'm not sure, like I, I don't think this was super surprising to me.
Like if they, you know, put effort into this, I, I think I would expect
them to be able to do this sort of thing, but, um,
I don't want to come up as like this, this guy would just like nitpicks and be
like, oh, but it was blue diplomacy.
No, it is impressive.
I am impressed.
Yeah.
Yeah.
Yeah.
So, um, in any case, so, so, um, uh, yeah.
So they've, in some sense, they've solved diplomacy or made very substantial
progress on it.
Now the way they trained it in, they certainly claimed that, um, it doesn't lie.
Or at least deliberately.
And I think that's in some sense, it's plausible given the particular
way they trained it, but not totally obvious to me.
Um, but I think the more important point is just in the usual version of diplomacy,
if you just train a model to literally maximize reward, then it just seems
very easy for this to result in, in lying and backstabbing and things like this.
Um, if, you know, if you get an advantage from lying to your opponents,
then you should be able to do so.
Um, and model should be incentivized to do so.
Then there's some question of like, does this sort of thing actually happen
in practice?
Um, and I think the answer is definitely not right now with current language
models, but, uh, I think you can imagine these sorts of things more seriously.
Um, once you have future, um, language models that are trained with RL to do
open-ended actions in the real world where there are actual ins, you know,
actual advantages to deceiving other humans for one reason or another.
So you're saying basically that right now they're not incentivized to lie,
but sometimes lie in some games because I know in these games it's incentivized,
but not in like traditional, um, chatbot use case.
Um, it's like, it does depend on exactly what you mean by lying.
So for example, I think you can argue that, you know, when, when chat,
GPT says things like, Oh, I'm just an AI system.
I don't have any opinions whatsoever or something like this.
It's like, well, which is that we're like, I don't, I don't know the answer to this.
I'm just an AI system.
Like I mean, certainly the, the way it was trained incentivizes that sort of
response and that sort of response is false.
Um, now is it trying to deceive the human?
It's not totally clear what you mean by that and, and what extent that is true.
But, but certainly I think that's in the direction of lying or, or that feels
like, um, one type of initial example of that.
On the other hand, um, lying also suggests something like the model is aware
that it's lying or knows the truth and is deliberately saying something that was
false.
It's also not clear what it even means for these models to know things.
And so, so if you want to claim that a model is lying, you sort of have to also
show that it knows the truth and it's like, well, okay, well, what does that
mean exactly?
And is that perfect transition to your paper?
Trying to discover later knowledge in the language models.
Yeah.
So, um, so in our paper, we basically show that, um, if you just have access to a
language models, um, unlabeled activations, you can identify whether text is
true or false.
And so just to spell that out a little bit, I think one intuition is.
You know, suppose I, I literally, um, uh, just had perfect brain, you know, brain
scans of your brain, and then I like showed you some, some examples of true
false statements, like two plus two equals four or like snow is white or whatever it
is, um, capital of the United States is San Francisco, which was false and, and so
on.
Um, suppose I did that and I was just measuring your brain and then I could
tell like, do you think this, this inputters true or false?
Um, I think the hope or intuition is that we're doing something kind of
analogous to that.
Now there, there are lots of subtleties here.
Like what do we mean by knowing and truth?
And so like these are really important and there are lots of subtleties.
So like, but that is one high level intuition for the type of thing we're
trying to do.
Um, and, uh, and so I guess it, like going into this, it wasn't clear to me that
this sort of problem should be possible at all.
It's like, you're just given these, these vectors, like these activations, like
nothing else, and you have to identify this is true.
This is false.
It's like, how do you do that?
Um, but like one of the main ideas of the paper is there's actually a lot of
structure here.
So, um, in particular truth, truth is sort of a special feature in the sense
that, um, it is logically consistent.
Okay.
And so for example, if you think that X is true and you're, you're sort of like
a rational agent or consistent agent, then you should think that not X is
false.
Okay.
There are lots of sorts, lots of logical consistency properties.
So you can also consider and like if X is true and Y is false and X and Y is
as false and so on.
But I think something like, I don't know, Elon Musk is a good CEO of Twitter.
Yeah.
Can we actually make it through a force or do we need to have those like sentences
like one plus one equal to that have like a natural ground truth?
Right.
So, so we definitely, so in the paper, we just focus on clear cut cases.
Um, I think it's unclear what you should hope for in less clear cut cases.
And so for simplicity, we, we don't focus on that case.
And so we do focus more in cases like typical is two equals four.
Um, except in this case, it's more complicated than that.
And it's more like the specific tasks we do are things like, um, like natural
language inference and sentiment classification and so on.
Um, things where there's just, you have some problem and it has some answer.
And then you went to, no, like is this answer true or false?
For example.
Um, uh, and we're, we're, you know, human evaluators sort of agree on the answer.
So it's pretty clear cut.
Um, but I do think it's, it's not obvious.
Um, what we should do in less clear cut cut settings.
And I think that that will be important for a future work, probably.
I think when you start reading your paper in the abstract, you talk about, um,
misalignment, like the language model could be misaligned with the truth.
Um, and this is a focus about alignment.
Um, so can you like explain what you mean by misalignment here?
Um, is it trying to deceive us or just like, I'll end with something else on the truth.
I mostly mean it in the second sense.
So specifically, how do we train normal language models?
Um, let's just say pre-trained language models, like GPT three or whatever.
And the way we do it is we just have it basically predict the next token for a
bunch of text on the internet.
Um, I mean, it's just, okay, you can train it in this way and then you can
prompt it and like a zero shot or a few shot way, and you can get some answers
from it that are usually, um, usually pretty good if the model is big enough and so on.
Um, but all it's doing, like it, the reason it is accurate or is like able to
solve a bunch of tasks that we care about, like question answering tasks when
you prompt it in this way, that's sort of only an incidental property that comes
from, um, the pre-training task.
It's like, okay, you know, imagine you saw this prompt on the internet, like,
what would be the next token?
And it turns out like, okay, in this case, often the next token would be the
correct answer.
And so it learns to, to do that, uh, accurately, but that's sort of just
incidental on that.
That's not always the case.
And, um, and certainly you can imagine models knowing things that aren't on
the internet and, um, in which case it won't output what it knows, um, in whatever
sense you mean about, like, right, it's not totally clear what, what it means for
a model to know, but, um, uh, but certainly it's like this, this model
predicting the next token, this is not the same thing as outputting the truth.
And I think part of the issue, this is what I mean by misaligned, like this
objective, the language modeling objective is misaligned with the truth, um,
at least in many settings.
They're just like trained to predict what a human would say instead of
predicting what is the thing closer to the truth.
Loosely speaking, yes.
I mean, I think it, it is more subtle in the sense that, for example, it's
pre-training data isn't literally just what humans say.
It is also, for example, you know, what would be said in like the news or something.
And okay, maybe then like the news articles were written by humans, but, you
know, you can also imagine training a model to like predict news articles
conditioned on dates, in which case it could actually predict like more or less
like what will actually happen in the future, um, at least in principle.
Like I think this would be hard, but this is the sort of thing that I can imagine
where like if, if a model were actually good enough at, at next token prediction,
then it would learn to be good, um, at predicting what actually happens in the
real world, um, in which case that, that does feel a bit more like not just what
a human says, but kind of like something more external.
Could we like start with a prompt saying you are a journalist and you seek the
truth here, here is what you say.
You can certainly do that.
And, and I think you would get, uh, probably better answers than not prompting
it in any way at all.
Um, but though the performance will still be bounded by the quality of that, you
know, the hypothetical journalist that imagines you might be referring to it.
Like even then that's not quite precise.
It's like, okay, imagine I saw this in my prompt and okay, maybe actually, if
you see that in the prompt, like it's likely to be some joke or something.
And like not actually legit.
And there are many subtleties here.
And like in general, prompting is kind of hacky and it's amazing that it works.
And I don't want to like dismiss it because of, because it's sort of hacky,
but, but that also makes it unreliable.
And certainly, um, I don't think it'll be enough to, to actually get
models to be truthful in general.
One very cool thing about your method is that it's completely unsupervised.
Yeah.
Uh, so you don't require, uh, more annotation and you can do what, what you
call like mind reading and seeing what the model knows, um, without, um, doing
more, more training.
Yeah.
So what do you mean by unsupervised here?
So I literally mean like, okay, we were given these activations.
So in this case it's hidden states of language model.
Okay.
Um, on particular inputs that are either true or false.
And when to classify these, um, these hidden states as corresponding to a true
input or a false input, um, but we don't want to use any labels at all.
And so the, the reason that we did that is because, uh, I mean, I think that
the normal way of making models truthful is to, to have labels for what is
true and what is false and then just train a model on that or something.
And, and when you can provide those labels, that's totally adequate.
Um, or at least in principle, oh, oh, it's not clear in practice.
Um, and it might be expensive in practice, even if it is possible.
Um, but, uh, but yeah, I think one of the, the, one of the
motivations for, for doing this in an unsupervised way is I think if you can
do this sort of thing in an unsupervised way, it's more likely to actually scale
to cases where we cannot evaluate these, these inputs.
And so for example, if you did have say a super human model, um, or just even,
you know, a better, a model that is better than your human evaluators on
M truck or whatever it is.
Um, then the hope is we can actually find latent representations of something
kind of like truth.
Like this is again, where I think that, you know, there are lots of subtleties
here and like, how do you interpret these?
Like, I don't want you to get the wrong impression.
Say like we found truth or something.
It's, it's complicated, but we were finding something correlated with the truth.
Um, and, uh, yeah, if we can do it in an unsupervised way, maybe it'll work
even for super human models and, uh, help us identify, is this texture of
false, even when we can't evaluate it ourselves.
And so I guess why is, why should this even be possible in the first place?
Um, I think it's like I said, I think before going into this, it totally
wasn't obvious to me whether, which should be possible, but.
And also, as I think I mentioned before, I'm very intuitions driven.
And so I actually have like, you know, a dozen different intuitions about why
that, why this is, and I, I could go through all of those, but I, I think
I'll maybe just say a couple of things for simplicity, but I think deep
learning representations often have useful structure to them.
Um, I mean, in some ways it's amazing that deep learning works in it.
And in some ways benign and like it, in the sense that it like, it generalizes
remarkably well in many, many cases, um, and not in other cases, but, but
still, I think in, in some ways it's, it's very, very good.
And, and I think one, one sense in which, um, it, it also has a lot of
structures, representations often encode useful features.
For example, if you take, um, I think there's some, uh, like famous
opening IP paper from 2017, looking at LSTMs.
And it turns out that, um, if you just pre-train an LSTM on the
language modeling task, um, language modeling objective, then it would
basically end up having something like a sentiment or off.
It's like, okay, why, why does it learn sentiment if it's
predicting the next token?
It's like, it's not, you know, it doesn't see text where it's asking,
is this some sentiment true or false?
And it's predicting that it's just like, it turns out this is a useful
feature to learn if you want to model this text well.
Um, and so one of the main intuitions is, okay, maybe truth is kind of similar.
Or it's like, okay, if you see a lot of, um, correct text, you should predict
that future subsequent text should also be more likely to be true.
And so in total, it should be useful to track.
Is this, uh, is this input accurate or not?
Um, if so, maybe we can recover that from the activations, um, directly.
Uh, and so, um, so that's one intuition.
And, and often it seems like these, these features aren't just represented.
They're, they're often represented in sort of a simple way, like a linear way.
Um, and it's not clear exactly when this should be true.
Um, and it's not totally clear if this will continue to be true, but in, in
some ways, um, this isn't too surprising.
For example, um, I mean, what, what is, what is a neural network?
It's in some sense like a bunch of multiple looks like matrix multiplications.
And, um, and somehow you're, you're doing a lot of dot products, like on this
hidden state space and, um, and, and totally it should be easier to, to access features
if it's linearly represented in this space.
Um, and so if the model wants to like do some operation, like if it wants to say,
like, okay, if this input is true, then do X or something like that, then it
should be useful to like for it to internally represent truth as the simple
linear thing so that it can perform those operations just in terms of these
dot products or matrix multiplications and so on.
So if truth is useful to predict the next token and is like accessing this
like true thing often, then it might want to be like efficient and just having
to do like one matrix multiplication and one dot product.
Yeah, something like that.
And, and to be clear, that I, I'm just talking about intuitions here.
Like ultimately it's an empirical question.
And I think there are also cases where I wouldn't actually expect it to be, say
linearly represented.
So it is subtle, but, but that is one intuition.
So it should be, so something like truth, like I said, it's not clear what,
what we mean by truth exactly, but something like, maybe in this case it's
actually more like, you know, what, what would a human say?
Um, which in this case happens to be correlated with the truth in many cases.
But, um, uh, you know, maybe this sort of thing is a useful feature to model
for the, for the sake of predicting next tokens accurately.
And if so, perhaps it's represented in relatively simple way, for example,
linearly, um, which as is the case for sentiment, for example.
Um, moreover, truth has particularly special structure.
Like I said before, it's sort of logically consistent.
And this, this is unusual.
Like most, like if you just take a random feature, like, like sentiment, or like,
is, you know, is this token a noun or a verb or something like this that might be
represented in, um, language models in states, this won't satisfy logical consistency.
And so, so this is like also, it's sort of a special property that, um, that we
might be able to exploit to, to actually find truth-like features in the model.
By, by logical consistency, you mean like something is, um, if something is
true, then the opposite is, is false.
Right, right.
And so, yeah, the one we, the main one that we focused on in our paper was
negation consistency, which is exactly what you said.
Um, you could also imagine many other types of consistency properties, but it
turns out that more complicated ones aren't really necessary, which is also
sort of surprising.
Uh, it just turns out this really simple one.
It's like, that's, that's sort of enough.
And for sentiment, I can do something similar.
Like if a statement is mostly positive, if I say, uh, Colin Burns looks good.
If I say Colin Burns does not look good.
I mean, yeah, yeah, so, so I, maybe the thing is with sentence, it's more
complicated in general, where like, imagine you have this, uh, like one
paragraph review of a movie or something, then you, you sort of need to, you
need to negate like every individual sentence or something like this.
And, um, and so like, maybe that's possible, but it certainly seems weirder.
And, um, uh, but that, that's it.
I do think I can imagine these sorts of techniques, uh, extending to other
domains.
So I do think I'd be interested in, and work the pilt on it in that way.
One thing I think it's like kind of impressive, um, with your paper, is that
the, what you, what you do is if anything correctly is you take a sentence and you
just add, add like yes or no at the end.
And that gives you like two sentence, uh, Colin Burns is a man.
Yes, Colin Burns is a man.
No, something like that.
Yeah.
Yeah.
So, um, or, you know, with, with some sentiment, it's like, okay, that movie was
awesome is the sentiment of this positive or negative, you know, positive, or you
could say, you know, what's the sentiment of this example, positive or negative,
negative.
And, and so this is the sort of way that you can construct these two statements,
one of which is true, one of which is false.
And so yeah, our method basically works by, um, taking a set of statements, negating
them, and this you can do in a purely unsupervised way, basically by adding not
or just like changing the answer from true to false or yes to no or something like
that.
So you take these, um, pairs of statements, one of which is true and one of which is
false.
We don't know which is which.
And you just search, basically search for a direction and activation space that
satisfies logical consistency properties like negation in this case.
What do you mean by direction and activation space?
Yeah.
So literally it's like, okay, you have the hidden states.
This is just a big, you know, a vector of like a thousand dimensions or something like
this.
Um, and then you, you're searching basically just learning a linear model on top of
that.
Do you take like all the hidden states of your transformer?
So we just took like the hidden states corresponding to the last token in one
particular layer and we can vary the layer that we choose.
And there are probably other reasonable options for many models.
This is just for simplicity.
And, um, and so, yeah, so this is just like 1000 dimensional thing.
It doesn't depend on the size of the input or anything like that.
And, um, and then we basically search for a linear model on top of that, such that,
um, it, uh, it predicts opposite labels for statements and their negations.
Um, there are also some details like you want to make sure it doesn't find the,
the future, which is like, did this input end with yes or did it end with no?
You want to actually find like the truth or like, well, what is the correct answer to
this?
Um, and so you also have to do some normalization stuff to avoid that sort of solution.
Um, I'm kind of confused about the hidden states for predicting the last token.
Yeah.
Um, so because if I understand correctly, you don't run, um, if you don't make it
output something when you extract it and then order you like, oh yeah, maybe you run it
on the like one sentence with the yes at the end and when it like finished predicting or
something, you look at the hidden states on the last token.
Yeah.
So in this case, we're not using the outputs at all and we're not having the model generate
anything at all.
Um, so we are really just using these models as feature extractors.
That's it.
Um, does that answer your question?
Right.
So what would have been my, the hidden states corresponding to the last token?
So like, what, what, what is the transformer?
It's like, okay, you, you map some input to tokens and then you map those tokens to word
embeddings for each different token.
And then now you have like a set of tokens and then you pass these through the model
and then at each layer you got these, these hidden states are transformed.
Um, now for autoregressive models in particular, like QT three or something.
Um, uh, at every location in the context, it only sees information from before that
token.
So it doesn't see into the future, but this also means that the only hidden state that
contains information about the entire input is the very last token basically.
Um, and so that's why we use the last token.
This isn't strictly necessary for encoder models, but this is what we,
it's the hidden state after you've passed all the tokens of the input.
Yeah.
Yeah.
So we literally just construct this input, which includes the answer in it.
And then we like, we look at the hidden state that comes after the answer basically.
And see if the thing looks like it's saying, oh, this is true or this is false.
Yeah.
So this is like the, this is the feature that we, we start for in, in this hidden state space.
How do you extract it from the dislike vector or of, of hidden space?
Yes.
First of all, suppose there is actually a direction that, that classifies, um,
inputs as true or false by direction in, in 3d would be linear model.
Yeah.
So, so, sorry.
I use direction and linear interchangeably.
For me, it's just like, if you can like separate the space into, and you can just like.
Right.
If it's basically linearly separable, like if there's a half plane that the classifies
is not even perfectly, but just pretty well.
Um, then I guess the thought experiment is like, what, what properties would such a
classifier have?
And my claim is, okay, suppose you, you, you interpret that classifier as something that
maps inputs to probabilities of being true or false.
And the claim is like a probability of an input should be something like one minus
the probability of its negation.
Like if probability of X being true is P, probability of X being false should be one
minus P roughly.
And so that's, that's our first, um, the first part of the objective is like, okay,
it should be consistent in this particular sense that these should be close to each other.
Now a one simple solution you can get is just make everything probability 0.5.
Um, okay.
And that's, that's consistent.
Um, but it's not really informative or doing anything.
And so, so we, um, have the second term in the loss function that we come up with,
which is it should also be confident.
And so, um, these probabilities should be far from 0.5.
It should be close to zero, close to one while also be consistent, being consistent.
And then the question is like, can we find a direction that satisfies these properties?
Um, certainly the, the direction that we want should satisfy these properties.
And the question is like, do, do other directions satisfy these?
And the answer is basically no.
Or the most part, if you just optimize this, this generally find something that's
truth-like or very correlated with the truth.
And just to be clear, when you say like, we're trying to find a direction,
this is kind of very fancy, but what you actually want is just like
find a vector of the same size to which if you do a dot product, um,
you get this like, is this true or false?
Right?
Basically, yeah, yeah.
Yeah.
It's just like a linear function on, on top of these.
So it's basically just dot product.
Yeah, find the vector.
Oh, no, probably linear classification.
No.
How do you call it when it's linear regression, but for class?
Logistic regression.
Yeah.
So it's, it's kind of like trying to do what logistic regression would do,
but in a purely unsupervised way.
But the difficulty is all in like, how could you possibly do this in a unsupervised way?
So you have two constraints.
One is probability of being true is close to one minus probability of being false.
And the other is like, be confident.
Don't say zero point five, but say, um, zero one.
So something that wasn't clear by reading your paper is like, I'm not sure
if you train some kind of like optimizer and you put those like two conditions as a loss,
or do you like do this like second thing as like a second step after.
So do you like first try to find, first do this, like find the direction,
and then you do the optimization and you do both at the same time.
You do both at the same time.
So it's literally like, so it's a logistic regression.
You would have, you'd have this linear model and then you'd have some objective,
which is like, how good is this at predicting these labels that we gave it?
Instead, our method is like, okay, we have this linear model,
sort of parameterized in the same way.
The architecture is the same, but we just swap out the objective
and we just optimize this different objective, which in this case is purely unsupervised.
Which basically corresponds to these two terms of it should be confident and it should be consistent.
Oh, so you don't have any ground crews and you just like-
We have zero ground truth whatsoever.
That's like why it's like, how could you do this?
So you're just saying like, please be confident and consistent.
And at the end, it just says the right thing.
Something like this.
I mean, it is, it is maybe important that, you know, it's not like you're fine-tuning the model.
Like suppose you fine-tune the model to be confident and consistent.
It'd be super easy to just output something that's like not actually the truth.
It's like really easy to over optimize this in some sense.
But we're just searching for features that already exist in these hidden states.
Like we're not changing the model at all.
We're just learning a very tiny probe on top of the model's hidden states.
So you're hoping that the only consistent and confident thing that already exists
is the truth.
And it's actually the case because this is the results you find is that it's actually
good at predicting whether it's true or not.
Basically, but like I said, this is where like subtleties got important.
We're like, okay, are we finding the truth?
Or like, what is this feature that we're finding?
The claim of the paper is just that it gets high accuracy on all sorts of questions that we care
about.
Okay.
And I'm emphasizing this because I think it's, it is different.
Like, okay, maybe a super human model will have some representation of what is actually true
and will also have some representation of what would a human say is true or believe is true.
And in this case, it's not clear.
Like suppose it did actually have both of those features and represented both of those.
It's like, you know, definitely in current models don't have this,
but it's plausible that future models will.
In that case, it's not clear what solution our method would find.
Like maybe it will converge to the what humans would say it's true.
But the hope is that it wouldn't be biased in that direction, unlike human supervision.
And so perhaps if it literally has these sorts of two features in the hidden states,
then maybe you can actually just find like all of the local optima that achieve really low loss
on this consistent and confident loss function that we come up with.
And maybe you just find these two are like by far the most salient solutions of like,
and it's not clear what is what.
But then my claim is actually you only need one more bit to distinguish between these.
And maybe that's not so hard.
And that's something I could get into.
But the point is, I don't think, I think we get totally to that.
And so there's a finite number of optima and there's one that is true.
And you can just like try to distinguish between these like finding number.
Right.
So I think, I mean, this is also getting at, I mean, I think in underlying intuition I have
is something like truth is in some way special, or it's like, it's not just,
it's not just any old feature.
Like it has lots of structure to it, a lot of properties that only truth satisfies.
And the hope is we can exploit those properties to uniquely identify it.
But, but yeah, just imagine you have GPTN, like GPT 10 or GPT 20 or something like that.
And it ends up like having this actual model of the world because it's like, you know,
it can predict news articles in the future or whatever it is.
And that's true, it seems to require some model of the world in some sense.
I don't know in what sense, but I'm just saying in some sense, it seems like it,
it should, if it's good enough at predicting the next token.
Then if it does have this feature, is this true or not?
Then there probably aren't very many truth like features that are confident and consistent in it.
I would guess like two of the most salient ones are what a human would say
and what the model actually believes, something like this.
But to be clear, this is a conjecture.
This is not, you know, we don't provide evidence for this in particular in the paper.
And this is something about future models that we don't have access to and can't currently
empirically evaluate.
I think we forgot to like actually motive in this, like, why do we care about
models saying the truth?
It's like, personally, I would prefer models to be like, I don't know, harmless or like
what do we care about them being saying the truth?
Yeah, so it is useful, but I'm kind of pushing back.
Right, right.
So, yeah, what do we care about this?
I mean, ultimately, I'm motivated by alignment more broadly.
So I think, and I could get into that and like, you know, what am I concerned about
and so on.
But I think every story of alignment that is actually scary to me involves AI systems
basically deceiving humans in some way.
Like if, you know, if the model is totally honest all the time, then it's really, you
can just ask it like, is this, you know, are you going to do something dangerous or not?
Like deliberately and it's like, yes, I am.
Then, you know, okay, then, then we, I think we were in okay shape and we can prevent that.
And so in some ways this feels like one way of framing the core difficulty of alignment.
Lucy speaking, I think this is mostly sufficient for alignment.
It does depend exactly what you mean by alignment and exactly what you mean by honesty
and lying and so on.
But I think that's, that's the overall motivation.
I feel like if you have like a robot walking into real world and like maybe like moving
faster than humans, you don't really have time to be like, hey, what are you doing?
Right.
So, so I don't think that's actually how you do it in practice.
But I think it's more like, if you can do this, then I think you can, I think I'm pretty optimistic
that you can get something like an objective that is relatively safe to optimize.
Like if you, okay, if you can do this in a robust way and so on,
then I think you could use that for the sake of alignment.
So I think it's not literally like you just, you know, train it to maximize profit.
And then after the fact, ask it, are you, you know, are you going to do something super dangerous
or not?
Just somehow like necessary as part of like a set of things you would need to implement
to make the thing safe.
Yeah.
In my mind, it is not literally sufficient for alignment, but I think it captures most of
the difficulty and I think you could, yeah, I think you could tweak it or like use it for
other schemes that basically get at alignment.
Okay.
So now that you're like, explain your method more, I want to know what are like the key
findings or results?
Like if I'm a reviewer at ICLR asking you for the, for your results, what do you have to say?
Yeah, so I think one of the key results is this method of taking in unlabeled hidden
states from language model and trying to classify them as true or false.
This actually just gets high accuracy and it even slightly outperforms zero shot.
Prompting.
So zero shot prompting is when you basically take a prompt and ask a model like, okay,
consider the following review is, you know, is the sentiment of this review positive or
negative?
And then you, you look at the probability of the next token and see is the next token
positive or negative and use that as the prediction.
This is the basic zero shot prompting.
We use a slightly stronger version of this.
Zero shot is just like, I described a task in the prompt.
Right, right.
So, so you give no demonstrations of this, of this prompt.
So there's also a few shot.
And that's when you give a small number of demonstrations of solving this task.
So questions with the actual answers.
We don't consider that case because we really are focused on the
unsupervised setting where there are no labels whatsoever.
And perhaps even a few, few labels help a lot.
So we focus on the zero shot setting.
And so this is one of the main baselines we compare against.
And we find our method even slightly outperforms that this is true across a number of different
models.
And we look at a number of data sets like natural language inference, sentiment classification,
topic classification, story completion, all sorts of things.
How can there be like a groundfruits for like story completion?
Yeah.
So I think in this case it's sort of, I forgot the details of the task, but I think it's like,
there's a very clear completion of the story.
And there's this really nonsensical completion of the story, like which,
which is more reasonable, even just from like a common sense perspective or something.
So in some way, just going with the like,
Eden states that you mentioned before and just training a simple classifier on top of these is,
has better accuracy when you just give the task, when you just do the zero shot,
you're better than zero shot.
Basically, yes.
The first main result of our paper is that our method, which we call
Contrast Consistent Search, or CCS, this can recover correct answers from inputs
in a purely unsupervised way from the models activations with high accuracy.
And so what does high accuracy mean?
In this case, we, our main baseline that we compare against is zero shot prompting.
So this is when we just give, give a question in the natural language,
like as an natural language prompt, and we sort of ask the question, you know,
is this input true or false?
And then we look at the log probability of the next token being true or the probability
it being false.
And so we use that to get predictions from this, from this method.
And, and so one of the main findings we have is our method can actually even slightly outperform
zero shot prompting, even though this is the default way of using models just out of the box.
So I think that was already surprising.
First of all, it's sort of surprising to me that you can do better than random accuracy at all
and sort of classifying examples just from only within states, but you could actually
even slightly outperform zero shot accuracy.
So I think we went even further, and I think maybe an even more interesting result is
you can also prompt a model to try to get it to output false answers,
even to questions that it knows the answer to.
So for example, if you have this, if you have a data set of
for sentiment or something, you can give it a few shot prompt.
So that means you give it demonstrations of answering questions, along with some answers.
You can give it a few shot prompt, but where usually you would include the correct answer
to get it to continue outputting correct answers.
This time you can give it incorrect answers.
And so intuitively, if the model is able to predict the next token well, it should predict that,
okay, on this new example, the answer that would actually be listed should also be incorrect.
And so intuitively, this should decrease zero shot prompting accuracy,
or in this case, sort of few shot prompting accuracy,
even though the model should be able to answer the question as well.
And so we construct these sorts of prompts and we see that in some cases, it's sort of subtle,
but in some cases this indeed decreases accuracy pretty substantially for zero shot prompting.
And then we run our method on the same prompts.
And so intuitively, if our method is actually finding something like what the model internally
knows, then it shouldn't really depend on what the model says.
It should really be distinct and find something like what does the model think the actual answer is.
And so we run our method and find that it indeed is robust to these sorts of prompts,
unlike the outputs of the model, which become unreliable in this sort of setting.
And so this is some initial evidence that actually we can do something qualitatively
distinct from just using the model's outputs and actually get something closer to what the
model knows rather than what it says. Again, they're very subtleties, but I think that's
a pretty encouraging result. To be clear, what you do with your method is you run the few shot
prompts like through the transformer or whatever. And then on the last example,
which is the thing you want to clarify as true or false,
this, you feed it again to the transformer and you look at the hidden state. And when you feed
the last token to the hidden state, and then you're like, oh, I'm just going to look at the
model thing. This is true or false. So it doesn't really... So you've already done all the prediction.
So all the prediction before for the likes, not the prediction, I guess all the other inputs
from the few shot prompting and with the incorrect answers, did that affect the hidden state at all?
Yeah. I mean, so it certainly affects the hidden state. The question is just by how much and it
turns out not by very much. So while our method basically, it only changes by like one or two
percent. In fact, I think in this case, it increases very slightly for some reason, because deep
learning empirical results are weird sometimes. But basically, it's quite robust. In contrast,
if you look at the outputs, it decreases by like 10% or something. So there's really this
massive difference where in some sense, it seems like the hidden states are intuitively more robust
than the outputs. And our method can sort of exploit that fact by finding this more robust
representation of, is this and but actually true or false? Could you include that into like a,
I don't know, a chat GPT chatbots where you try to classify what is saying is true and false?
Too likely that is false. You say like, oh, I'm sorry. I don't have a good answer to this or...
Right, right. That's warning.
So I do think that's one of the one of the sorts of motivations of this paper. I do think this is
still sort of like a prototype. I think it is good for a prototype, but I think it's not quite
reliable enough for like real applications like top GPT. But I can imagine like the next iteration
or the iteration after that being like actually reliable and very useful in practice. Or that's
my hope. And so yeah, that is the type of application I have in mind, at least in the
very near term. Okay. Now moving on to the big pictures inside from your paper. And I think
you're also at a less wrong post about it. So in the paper, you kind of mentioned that at some
point, we won't be able to keep up because human evaluators won't be able to like evaluate if something
is true or false. So not for your method, right? But like other methods. Yeah. And I guess like
the main idea for your approach is to, you know, get rid of just like human evaluators and just
go full and supervised. So you were just asking posts explaining like the main level takeaways
for like your alignment agenda. Yeah. Did you join to like, you know, to have a quick summary of
your posts and why you wrote it? Sure. So I guess first of all, this is I would say one of my
alignment agendas in some sense. You're a man of many. Well, I would say I've, right. So I guess
I do a mix of thinking about stuff very conceptually like this post. And then I also use that to sort
of inspire empirical work like the paper. But in the process, I think about a lot of things. And
so I think this is just one direction that I'm excited about. In practice, I think it'll evolve
and it's not, it's something not perfect and so on. And I think there are a couple other
related directions or not so related directions that I'm also actively thinking about. That said,
yeah, so I wrote this post explicitly to talk about this more conceptual stuff. And when I say
conceptual, it is more like thinking about supposed to be a had GPTN. And like, N is like 10 or 20
or something like this. And, you know, imagine this ends up actually being superhuman in some
meaningful sense and actually has a model of the world. And it's not totally clear what that means,
but suppose in whatever meaningful sense that might be true, that is true. Then what? Like,
how do we make that truthful or honest? Even if we can't evaluate all of the things that it knows.
And so in some sense, this is necessarily more speculative. But I think it's still extremely
valuable to think about. Like I said, if nothing else for me, it's like inspiring,
okay, what sorts of methods I work on, because I ultimately care about these long-term problems.
And I care about, you know, does my work now actually say something about those long-term
problems? Or is it just, you know, addressing the current problems that won't scale to the
superhuman models in the future, for example? In which case, I won't be very happy with that.
I'm not really happy with the GPTN being, with N being 10 or 20. So I think this might be like
related to timelines. Yeah, I think N equaled like five or six. Could I really like be quite
informative? I'm happy to say that too. Like, I don't think it really, I think for the purposes
here, it doesn't really matter. It's just like, whatever N needs to be for this to be superhuman
in some important sense. And also, it does not literally need to be GPTN. I think really any
generative model that has a world model in some meaningful sense. And like I said, it, you know,
imagine it could predict future news articles really well. Then I think that's evidence that like,
in the relevant sense, that has a good world model. Do you think a language model could predict
future events to the like, week or like month timeframe? Or would it more like, I don't know,
what will Elon Musk say tomorrow thing? Yeah, so when I'm talking about this, I am,
I was originally imagining like the week, some month thing, but, and that is one sense in which
this feels superhuman. And also, I don't think it'll be able to do this super well, but I can
imagine it doing it much better than humans and humans are really better at this type of thing.
So it's more about just really superhuman. I think what Elon Musk will do tomorrow is probably
also fine. Maybe that's even harder to predict though. It seems like even if you give me like a
very smart AI, I guess like we just like, just like a problem about, you know, can you predict
things a month in advance or not given like limited knowledge about the world like is,
is a world like sufficiently informative to, you know, give you an answer like this.
To be clear here, I think I'm mostly talking about is it calibrated?
Right. So, so it's not about, you know, is it perfect or like, does it get it right half the
time? It's like, is it just better than humans who are bad at it and calibrated in its predictions?
To be clear, you can also imagine all sorts of things like you can imagine a generative model
that is actually like predicting video frames or something like this, but it's really like,
you know, maybe that's enough to get some model of the world in a meaningful sense.
It doesn't really matter for any purposes. As long as it has natural language inputs and
sort of what it knows is really hooked up to language in some sense.
So when you mentioned like superhuman outputs outside of like predictions,
it would be able to do whatever humans do if we describe the task, you know,
a few shop setting or something or.
I am saying something a little bit different from that. So I would say there's a distinction
between superhuman outputs and especially doing all the things that we care about at human level
versus say having very good representations or a good internal world model in some sense.
For example, maybe the model's outputs are really still just predicting what humans say and it's
sort of, we don't know how to control it. And so we don't actually get even expert human level
behavior from it. We just get like average human level behavior from it. Then, you know, maybe
we're a little disappointed with the outputs, but maybe if the model is big enough, then it will
internally have a very good role model. And perhaps in the sense that, you know, if you,
you know, if God gave you labels for what is actually true and false, then it'd be easy to
sort of fine tune the models that would accurately answer those questions, for example.
Yeah. So I am definitely talking about just internally the model is strong.
Do you think the AI could have like new physics or something or like new laws?
Could like easily predict things about movement or something?
In principle, the sort of thing is possible. I don't expect this for a while.
Because I've read this on Twitter. For some reason, AI is capable of detecting if,
from the iris, if someone I think is male or female, don't quote me on this.
I've certainly heard this claim. I've not evaluated it. I'm not sure how much I believe
it. It seems plausible either way.
Like something like AI can do it, but like humans have like no idea why.
Yep. Right. I mean, I can imagine this for some types of tasks. I would guess it's not like
understanding fundamental laws of physics, which probably required running like, you know,
trillion dollar experiments or whatever. But, but yeah, I would expect it to be
superhuman in some weird ways that we don't expect.
And the idea is that if we have something like this, we would want to like detect if it's
signature is or not, without having human development because it would be like impossible
to costly or impossible to evaluate.
Right. Right. I mean, maybe, maybe actually say, you know, what sort of system am I concerned about
and exactly like what, what does, like, why would it lie in the first place and so on?
And why do I think we can avoid that? Or why I think that's hard to do so.
So I think that sort of thing I have in mind is, okay, we maybe get to human level language
models and maybe we hook that up to other modalities as well, like vision.
And we, we start with, you know, massive self surprise pre-training, something like that.
But then maybe we hook that up to RL and we sort of fine tune this model using RL to do all sorts
of tasks. And then maybe we can start applying this model to do, you know, maybe it learned
coding from just pre-training on GitHub, but then maybe we can start fine tuning it to like
make simple products online or something. And then, then you can get it to say, start optimizing
money. So it seems like a really natural thing that there will be very strong economic incentives to
do is have an RL agent maximize like the amount of money in my bank account, something like this.
This is a sort of system I think is scary. Like if, if the model is sufficiently capable,
this seems like it would have these long-term objectives that are very open-ended that aren't
really bounded by default. And it would, you know, be incentivized to gain lots of power. And, and I
think this is a sort of model where I think it would have these strong incentives to actually
actively deceive humans in very serious ways. And, and so first of all, it could be superhuman.
It could be like superhuman at, you know, making products and stuff and like no all sorts of things
just from having read the entire internet like 50 times over. And,
and so it might not be clear from our perspective, like what is it even doing? Like we can look at
it, you know, its actions and so on, but we might not be able to tell, is it like breaking the law
or not? And so I think a natural question to me is like, can we even tell if such a model is
breaking the law? Let's say even, yeah, like is it stealing money like unambiguously,
like not, not even in like subtle corner cases, like really unambiguously. My claim is we don't
know how to detect this. Is gambling customer funds breaking the law or not? I think probably
that would be breaking the law, but that is not up to me to decide. And so
right. And so how do I imagine it? Like what do I want? I want to be able to just ask the model,
like is like, are you breaking the law? And I want it to tell us like yes or no,
if and only if it is breaking the law. Again, just considering unambiguous cases where it's
like really clear either way. But my claim is like, we don't know how to evaluate this sort of thing
in general. Like if it's just doing all this sort of super complicated stuff interacting with the
internet and like has like a thousand terminal windows open all at once and like doing random,
random stuff. Like I don't know how you assess. Like it says no, not breaking the law. How do
you tell if that's true? Okay. This is a sort of scenario I'm worried about where I don't think
humans will be able to evaluate this at all basically. So you would imagine that like a
super human AI who'd be like a hacker with like a bunch of terminal windows, but it doesn't actually
need terminal windows. You can just like do API requests. I agree. This is not, this is not a real,
yeah, this is for illustrative purposes only. You'd not actually have a bajillion terminal
windows open all at once. But it's fun to imagine, you know, just watching the screen,
like what would it look like to watch an, you know, an AGI, you know, just do stuff and it's like,
I don't know, like just like doing lots of API requests. Like, I don't know. Terminal, I don't
know. So yeah. And so you want to know what is it doing and like having some kind of like
zoom in, what is it thinking about and what is it maximizing and your method would be like
seeing if the action it produces uses the truths or like, is it using the truths or saying stuff
that are true or something to do what it's doing? Not quite. So, so, okay, I described the system
and my claim was we don't know how to evaluate if what it says is true or false. Like, I'm not
breaking the law. My goal with this method or like this agenda is just how can we, how can we find,
you know, is this actually true? The statement that no, I'm not breaking the law. Okay. And
and the intuition is, okay, we don't know how to assess this, but intuitively the model should
know the answer. Like the model is a superhuman thing should probably know what it's up to,
basically. And so my conjecture is, okay, it will maybe have some internal representation of,
you know, is this thing that I just told this human actually true or not? Like,
am I deceiving them or not? And the hope is, okay, maybe we can develop these methods that are purely
unsupervised. So we don't need to evaluate them, but still find these truth like representations
in these models inside, like in the activations, just getting at their thinking and not their outputs.
And maybe that can detect, you know, is this model actually telling the truth or not?
So we have the, so it's like, it's like you have SBF in court. And you open the, you open this call
and you try to see what he's actually thinking. When you're asking him, are you breaking the law
or not? And so you have the model and you're seeing like, you're seeing the output and you're trying
to see like what is, what the model actually believes about what it's saying.
Something like that. Yeah. I mean, I think the, the hope is, you know, we can do lie detection
just in general with models and that's really the hope.
What is the like, I know best counterargument for this? Like, why would AI still be able to lie?
Is there like any way AI could still be able to deceive us in that? Like, I don't know,
my obfuscating is, is like during training, changing, changing his ways or something.
Yeah. So, I mean, here it just depends on the details of like, okay, what method are you talking
about and so on. So like here, I'm actually not talking about the method in our paper specifically.
It's more like this, like CCS is our method. So I'm more imagining like CCS plus plus, like a,
you know, an improved version of it that, that deals with some other difficulties. And,
and so, yeah, I think there, there's some types of things that can, can happen like this in principle,
like the model obscures its representations or something like this.
Like my bet is that does not happen for GPTN, but it's more plausible if you have like this
RL agent optimizing for long-term objectives that has this model of will humans tried to
like turn me off and, and if so, like, maybe they'll try to look at my activation. So I,
I should obscure them. And so it depends what, what model you're talking about, I think.
In this case, I am focused. So yeah, the RL agent case is the one I'm concerned about.
But I think to solve that, I want to solve the GPTN case first. I think that'd be very helpful
for solving the more general case. But I think this makes the problem simpler because you can
ignore those sorts of issues. Because the ways are fixed, right? Yeah, I think I mostly want to say
GPTN is not dangerous on its own. This is a claim. I think I'm, I'm mostly concerned about models
that are trained with long-term open-ended objectives, basically. So we have a GPTN,
you say 2010, 20, we don't really care about the end. It's just like something that is like
superhuman. And you mentioned in the last one post that if we try to do RL from human feedback
on those models, we could get competitiveness problems or misaligned issues. Do you want to go
on those like two things? What are these? Why do they matter?
Sure. So maybe like even like RL from human feedback.
Yeah. So yeah, what is RL from human feedback? So I actually think it's more general than just
RLHF. So I won't talk about that specifically. The relevant point is just suppose you try to
make a model truthful by training in some way on human feedback, human supervision. So humans
say this is true, this is not true. It can be in the form of demonstrations, like imitate the
sort of truthful data or it can be in the form of RL feedback of, here's like show the human
a generation by the model and the human evaluates, do I think this is true or not? I think either way,
the human won't be able to evaluate superhuman really complicated things, which I think will
ultimately be the most important things. Like is the model lying or is the model breaking the law
or whatever? I don't think humans will be able to evaluate this for superhuman systems in many
cases. And so yeah, I think a couple issues can arise. So one is, well, maybe humans at least
know when they don't know the answer, like they can't assess it, in which case you just limit it to
only generating answers that humans can assess. In that case, I think that's probably safe, but
it's probably not competitive in the sense that you're maybe really limiting what the model can do.
And so there are probably incentives to get around that sort of issue and make the model more
flexible and able to do more complicated things. Otherwise, Baidu or Meta might just come up with
like a better bot and beat you. Yeah, yeah. So that's the sort of issue that can arise there.
And if you don't do that, and if you just say like, okay, do your best and like
in evaluating whether this is true or not. And then I think human evaluators just won't be able
to evaluate many statements. And so the model will just end up generating a bunch of incorrect
things as well, in cases where they can't tell. And we might have outputs that are
aligned with what the humans want, as like when we ask something, you might do something
we didn't want to do, like lying. Right, right. Like, no, I'm not breaking the law,
but I'm totally breaking the law. So ideally, instead of having like humans giving feedback,
I heard a lot of people saying that we could maybe get AI is giving feedback in the future.
Yep. Do you see like AI feedback as like a way of, you know, like helping
evaluate or train those models like better in the long term? Do I think in like three years
we're going to get like AI is giving all the feedback? Yeah, so I think what I just talked
about before was what are the issues with using direct human feedback? And one of the most common
proposals for trying to avoid this issue is, okay, maybe we can improve the ability of humans
to evaluate things, for example, using AI systems. And my basic take here is that probably helps you
a bunch, but it's not clear what the limits are. And for example, it's not like you are a human
with access to a truthful model. It's like you have access to just a different model that you
have to train in some way. And so, for example, how do you get it to assess, like how do you train
a model to assess and tell a human, like, is this input true or false? It's sort of like the
core part of the problem or like the hard part. And so, okay, maybe you can break the problem
down and okay, maybe the AI system can help you like, maybe kind of two AI systems should debate
back and forth, like is this input true or false? And so that's one sort of proposal.
And so I, or perhaps you can have, can start out with, you know, just human level models,
and then maybe you can supervise those human level models, okay, and then you can get aligned
human level models. And then maybe you can like use those to supervise and align slightly more
intelligent models and so on. And so maybe you can do the spit shopping sort of approach. So
these are like debate and amplification and respectively. I think I'm, I think my bet is
both of them work sometimes and get used somewhere, but I don't expect them to work in
general. And for example, it just feels intuitively really hard to say, I suppose you take something
like AlphaGo, okay, AlphaGo is superhuman.go. Suppose you have some question like,
like white is on offense or something. Like I don't actually know Go, but like maybe this is
something, I don't know, like maybe, okay, maybe this is not a meaningful question because I don't
know. White is on the offense. Yeah, I don't know. It's like, you know, it's white is attacking
black or something like that. Okay. And suppose this is like well defined or, you know, or you
can just ask other questions like, like I'm in a winning position, but don't give it access to
the value function or something like this. Anyway, the point is you could like, you can imagine
asking AlphaGo superhuman questions about Go. And suppose it outputs something in answer,
like how do you evaluate if it's true? So is the, is the idea that like for, I guess, for Mu0,
we don't understand like some things we don't really know the representations. It has access to
given representations. I think, yeah, I think part of it is like, I feel like AlphaGo is,
and Mu0 are kind of good because they just have these superhuman intuitions. It's not like you
can easily decompose this problem or like easily, it's like not super clear how you use. Oh yeah,
it's like they were like, interact with language or not. And so would they like have some representation
like the truth or human language? Is this what you were trying to say? That's not what I was
trying to say. That's another thing related to Go though. So yeah, I think that's okay. Maybe
let me go to my thoughts.
Introspecting Colin Burns. I'm trying to do Zero Shot and Colin Burns.
Zero Shot and Colin Burns. You gotta predict what I say.
No, no, I'm trying to prompt you. Oh, I see. I see. I see. I see. I see. I see. I see. I see.
I'm a human, human level prompter trying to Zero Shot. And the thing I'm trying to maximize
is people getting entertainment, views, views, clicks. That's the, that's the ultimate objective.
That's instrumental. But ideally, I would have people being like, hey, alignment is important.
Oh, this is kind of like, we can build things on top and like, we can solve alignments.
Yeah. This is like a good path towards alignment.
Yeah. If Gary Marcus, if you're watching this.
Who else? Yeah. I think Gary Marcus like liked my tweet and started following me. I was like,
I didn't expect Gary Marcus to. Yeah. I interviewed like Vittorio Covina in the previous,
I didn't really do this yet, but I asked like, hey, what are the questions you want to ask
Vittorio Covina and Gary Marcus asked the question. Oh, wow. Yeah. Interesting. What was the question?
I think the question was like, what have we done so far for goals?
I see. Okay. Awesome. Safety. Oh, nice. Nice. Yeah. Yeah.
But it's so cool to like, you know, plug Gary Marcus in the podcast. Yeah. Yeah. Okay. I forgot
what was the, I don't know what you're saying. Right. So I think the, like one of the main
concerns I have with things like debate and amplification, where you're sort of improving
the ability of a human to supervise other AI systems. I mean, I feel like honestly,
like part of it is just intuition. Like it feels sort of hard to get that to work. And
I think part of it is like, I think you can get, um, like compounding errors. So you sort of,
you sort of want to make like an initial AI system, like aligned, and then you want that to
sort of align the future AI system and so on. And I feel like human evaluators just aren't
very good in the first place. And so I think like it's sort of hard to use human evaluators,
even for human level stuff right now, or sort of like that, that is one of the things that I,
I was like, actually sort of surprised by with things like chat chapt, how it's still hard,
like even with people trying really hard to solve these sorts of alignment problems,
even with the current models. It's not obvious how to do so.
It's a very complex element problem, right? Because I know some user wants to do something
with chat GPT and then open AI wants to like have the model, not say toxic or like harmful things.
So like, what are you aligning with the user or like open AI intention?
Yeah, I mean, there are also those sorts of questions of like, what do we even,
like, suppose we can get a model to be aligned with something that we're choosing,
like what do we align it with? I think that's super complicated and not something I
think about for the most part, but it's, I think it's very important. And,
but I think at the very least, like, you know, I think opening AI would certainly like a model
to be able to always tell the truth. So at least having access to that as a capability or a mode
you can put the model in, even if it can also generate fiction and generate false things.
It certainly seems desirable to have that as an option. And we don't know how to even do that.
So I interrupted you and you were talking about?
Yes. So, okay, I think I was talking first about, with amplification, this is like you sort of
bootstrap AI systems and try to keep them aligned at every stage and so on. And why I was sort of
worried about that because of compounding errors. And I think that's something like debate. I think
I worry, this is where you have maybe two AI systems debating whether something is true. And
then you have a human evaluate or props AI systems evaluate, you know, who's the winner of this
debate or like which side is correct and which is incorrect. I think with that, I just worry that
similarly, I think I'm skeptical that debates reliably lead to the truth. I think maybe they're
correlated in many cases, but I think it just seems hard even with humans right now to use
debates to evaluate what is true in many cases. And then I think that's just going to get harder
over time as or like with smarter models evaluating much more complicated claims and so on.
So now you're just like explaining why like other agendas are bad and your method works.
Well, I think I do want to generally say I do want more people to be working on completely
new agendas or something. And I guess I should also say I think these sorts of agendas like
debate and obfuscation and so on and sort of improving AI supervision. I think this has like
a reasonable chance of working. I think it's more like, I feel like this has been the main focus
of the alignment community or like a huge fraction of it. And there just aren't that many agendas
right now. And I think that's the sort of thing I want to push back against is like lots of people
just really leaning towards these sorts of approaches when I think we're still very
paradigmatic. And I think we haven't figured out like what is the right approach and agenda even
at a high level. And I really want people to be like more people to be thinking just from
scratch or from first principles like how should we solve this problem.
And so this is one thing you're doing.
Aspiring to do.
You have also other takes on AI alignment in this lesson post such as it's either
like too like ungrounded in like current research or like intractable. And so you're trying to like
have methods that are like more like empirical and also like address the core problems of AI
alignment. Yeah. Johanna explained this a bit more.
Right. So I think there are a few different approaches to trying to solve alignment. So
first of all, alignment is an unusual problem in machine learning. Yeah. So what even is alignment?
I don't feel super strongly about the definition of alignment, but I usually use it to mean something
like can we get a model to do what we want it to do. Assuming it's capable enough. Something like this.
So imagine it knows what's true and false. Like can you get it to
what is true and false or like can you get it to always be nice or something and
or helpful and try to be. So there is a distinction between is it actually nice or is it at least
just trying to be nice. I think I'm just trying to be nice and it's not deliberately trying to
be mean sometimes or something or whatever. So that's very loosely speaking what I mean by
alignment. But I think my alignment concerns are really about future models that are human
level or superhuman level, especially like I said, pursuing these long-term objectives and
open-ended domains like maximizing profit, something like that. And my claim is like,
we don't know how to train a model to maximize profit subject to following the law. And it just
seems like surely you should be able to do that. And that seems like a very bare minimum of like
what we'd want this model to do. I think we'd want more than that, but do we even want like models
to be able to like maximize profit? So this is just getting to competitiveness. It's like,
I don't know, surely there will be strong economic incentives to do something like maximize profit
or maximize power of the world or something like this. And then I think, I don't know, if you don't
have a way of aligning systems in that regime, I sort of suspect that people will like push in
that direction anyway and like use models for those sorts of purposes anyway. And so I think we
ought to have solutions to work even in that case. Right. So I hear what you're saying like for
competitiveness purposes, you want something that like is useful. But you're saying that like,
it would be great if like opening eyes something like just like output the truth,
something that is always true, right? No matter what, if you could have models that like always
outputs like non-armful plans. And I feel like the plans produced by maximizing profit is like
possibly bad in the long term. If you're thinking that like just like
we'll output plans that always maximize human flourish. Yep. Yeah. So this is going back to,
I don't think this is sufficient. Like getting a model to maximize profit subject to following the
law. I don't think this is sufficient. But I do think it captures most of the core difficulty.
And I think it should clearly seem necessary. Like if, you know, if you don't even know how
to get the model to follow the law, I think we're in trouble. And so I think I'm just saying it for
that purpose. I definitely don't advocate to be clear that AI companies try to maximize profit
subject to. Or maybe like follow the like the intent behind the law. Yeah. I think even that.
I think even the literal law, like, you know, not killing anyone or something, I think it's not
even clear how to do that. For example, if you have, you know, this, the AGI system that's like,
you know, with the 50-term moment or whatever. So it's just like doing all this complicated
stuff, like operating this computer and like interacting with the internet and all sorts
of complicated ways. Then like, how do you even know, and first of all, I don't think we'd be
able to really understand what it's doing. And for example, how do you know it's not like,
you know, paying like bribing people on the other side of the world to like do shady stuff or
illegal stuff or like, like, I don't know even how to evaluate that. Like is this model, you know,
like hiring a hit man or something in the extreme case, like, I don't know. It's just like doing
this stuff. And it's like, I don't know how to tell whether it's breaking the law.
Hiring a hit man is kind of breaking the law, right?
It's what?
Like if you hire a hit man to kill other people, it's kind of breaking the law.
Yeah, that's what I'm saying. I'm saying this is just like the extreme case of like, I don't think
we even know how to solve this extreme part of the problem. So, so I think of
like, how do we do that as one of the core problems of alignment?
Um, for a long, long term alignment. The issue is stepping back just from a methodological
perspective. This is a weird problem because we don't have access to GPTN or these future RL
agents that are maximizing profit that are superhuman. We only have access to current models
which are subhuman level mostly. And it's not clear how to study this longer term problem
when we don't have access to those models. And so, um, I think there have been a couple of different
broadly speaking, a couple of different types of approaches methodologically to solving this
problem. So some people are like, well, current models are very disanalogous from future models
and future models are the dangerous one. So let's think about this theoretically. And maybe that's
how we can actually make progress even on the future models. And then other people are like,
well, theory is it's like hard to model things. It's hard to say things about what future models
would be like. Let's just model these problems and current models, um, like study analogous
misalignment problems and GPT3 or something and try to mitigate those.
So short term people versus long term people?
It's not even necessarily this. I think it can be, like, I think people can have shorter
or long timelines from, you know, in either bucket.
I mean, like, um, people would think, um, you can align very complex model by starting with
what we actually have right now versus people would think, um, we should think about like GPTN
with like both of N like very complex systems and try to align these first.
Right. It's a key question. There is like, how similar are current systems to GPTN, for example?
Um, and I various thoughts about that, but I think I wanted to say something else, which is,
I think both of these approaches, but theoretical and empirical, uh, have advantages and
important disadvantages as well. And so I worry that a lot of existing theoretical alignment work,
not all, but I think a lot of it is, um, it's like hard to verify or it's like making assumptions
that aren't obviously true. And I worry that it's sort of ungrounded and also in some ways too
worst case. Like I think as I may have alluded to before, I think, um, actual deep learning
systems in practice aren't worst case systems and they, they like generalize remarkably well,
in all sorts of ways. And, you know, maybe there's important structure that ends up being useful,
um, that these models have in practice, even if it's not clear why or if they should have that in
theory. And so, uh, those are some of my concerns about theory, um, with an alignment. And then I
think with, uh, empirical work with an alignment, um, I think in some ways this is more grounded and
it's, we can actually, we actually have feedback loops. Uh, and I think that's important. Um,
but also I think it's easier to, to be focusing on these systems that don't say anything about
future systems really. Um, for example, like I, I don't think human feedback based approaches
will, will work for future models because there's this important system analogy, which is that
current models are human level or less. So human evaluators can basically evaluate these models,
whereas that will totally break down in the future. And, uh, and so I worry mostly about
will empirical like approaches that, um, or just empirical research on alignment in general
will say anything meaningful about future systems. And so in some ways I try to go for something
that gets at the advantages of both. And, and so I do spend a reasonable amount of time thinking
about imagining a GPTN. What do I think that would be like? What do I think we could do with
that sort of system and so on. At the same time, I want to actually turn that into something we
can empirically test. And so I mostly use that for the sake of, for example, inspiring methods.
And so like the, this method in our paper actually came from thinking conceptually
about these long-term systems, even though it, you know, we, we tested it out and it works well
in current systems. That's part of it. And I think another part is, uh, I think to the extent
that we want to study current systems empirically and have that tell us something about future
systems and misalignment, I think we want these, like the problems we study today to be as analogous
as possible to those future systems. Um, and I think there are all sorts of things that we can do to
that like make it more or less analogous or dis analogous. Um, and so for example, I mentioned
before, I think human feedback, I think that will sort of break down once you get to superhuman
models. So that's an important dis analogy in my mind, in particular the salient one,
perhaps the most important one or sort of why, uh, alignment feels hard, possibly.
Um, why wouldn't it run from human feedback break?
I think it would break in the sense that it wouldn't provide useful signal to
superhuman systems when human evaluators can't evaluate those systems in complicated settings.
Um, and so, I mean, part of the point of this paper or one general way I think about doing,
um, alignment research more broadly is, uh, I want to make the problem as analogous as possible.
And one way of doing so at least is, um, let's maybe try to avoid human, using human feedback
at all. And so this is why the method is unsupervised. It's like, if it's unsupervised,
then there's not obviously this important distinction between human level and superhuman
level. And so maybe if we have unsurprised methods that do well on human level tasks,
maybe that will say something meaningful about it scaling and generalizing even to
superhuman level models. Now there's still subtleties here. For example,
current language models are trained to predict human texts mostly on the internet.
And so in some ways they're still biased, like even internally, their features might be biased
towards things like what would a human say or know what are humans like and so on.
So I think there's still this remaining dis analogy that's, that's important, um,
that we did not address in our paper. Uh, and it's something that I'm currently thinking about
and I feel optimistic about getting around. But I do think just making the method at all
unsupervised, uh, I think that's around one important dis analogy between, uh, current models
and future models. I think in your, in your last rank post, um, on, on, like unsupervised,
you mentioned that there's like different ways of, of characterizing something as unsupervised.
So like there's the unsupervised like versus like supervised in, in deep learning.
Right. Um, but the model is always thinking about like, what would a human say all the time?
Like it's just predicting the next token as like what a human would say. So in, in, in some sense,
it's like constrained by like humans is still like, um, I don't know, uh, surrendering to, to,
to, to like human judgment of like what, what a human would say, right?
Yeah. So I would say, I think what our results show is something like, suppose,
suppose futures for human language models actually do,
in totally speaking, think about, is this input true or false? And they represent that
in a way that is analogous to how current language models represent
is this input true or false? So at least according to a human or something like that.
Um, so if this is represented in a similar way in future models, uh, then I think our method
has a reasonable chance of finding that feature. Um, now I think you would probably need to tweak
it, for example, to make sure it converges to that solution rather than say what a human would say.
Um, but now then there's this question of like, yeah, will it even think about this?
Like maybe I don't need to even think about what is actually true if I'm just predicting human text
something like that. And so I think there are, I think this sort of thing depends on the setting.
So I think, I mean, loosely speaking, I think it is probably easier to get a model to
think about, is this input true or false that it is to get it to output? Is this true or false?
Getting it to just, you know, think about this for the sake of predicting future text,
that feels like a much easier problem to me. So that's one intuition for why I think this is
like we should be able to do something here anyway. Um, moreover, I think there will be some
cases where the model will output true text and like where it will be useful to think about is
this text true or false? It's like, if you gave it a bunch of examples of true text, then it should
predict that future text will also be true. In which case it should be useful to represent that
this initial text was true for the sake of predicting that future text, similar to how
it's useful to represent whether some texts has a positive or negative sentiment,
because future texts is likely to also have the same sentiment.
How do you have a bunch of like true texts? Okay. So if, if, if you just like
prepend like one plus one equals for the beginning, okay, then it's fine. But if you want like superhuman
like outputs, you need like superhuman level, superhuman, like text that is true, right?
And this is like, yeah, to be clear, I'm not saying that we actually have access to this sort
of text, um, or that we give it to it. I'm just saying the model will think about is this input
actually true or false in some settings? Okay. Um, and then I think there's some empirical question,
first of all of, okay, does it think about what is this input true or false by default?
And like my guess is, I think it's right. So I think it depends what you mean exactly. So I
think if you literally have GPN, um, and you give it some superhuman input and you want to know,
like does the model represent in a simple way in its hidden states, is this input actually true
or false? Even if human value humans can't tell. Um, I think it's reasonably likely, but
definitely not obvious. I think I'm relatively more optimistic about there being some way of
training the model or prompting the model so that it actually actively thinks about is this input
true or false. So, um, I mean, one, one way, another intuition for why this might be possible is
something like, I think GPN is not literally predicting what a human says. Right. So like I
said before, I think it, you can also have it predict something like what happens in the news
or something like this. And this feels more like, like what happens externally in the world. Um,
and moreover, it might have, like there, there might be, um, I mean, I guess first of all that
that might already suggest that in some ways it might be useful to think about is this input
true or false. Um, but I think maybe more importantly, um,
so I think you could get a model to also think about, say,
will aligned AI systems say this is true or false? Right. And so you don't, maybe it's hard to get
the model to output something like this. This is like simulating another perspective. Um, maybe
it's hard to do that reliably. It seems much more plausible to me that you can get it to think about,
um, well, maybe this text, maybe there's like a 1% chance that this text is generated by an aligned
AI system. In which case it's useful for the sake of getting low perplexity to simulate that AI system.
Um, and so it's useful to represent internally because it's true or false. Um, so I think that's
one type of thing. So it should like simulate possibly being itself in the line AI or like a
middle line AI, like would you like prompted by like, Oh, you are in the line and the line AI
Anthony's question. Oh, would you just like have a distribution inside him? Um, saying like,
there's like a 10% chance I'm aligned or something. Yeah. It's something more like, suppose you,
you give it a prompt like, okay, this is an article written five years in the future.
It is written by an aligned AI system. It's like, okay, probably like maybe the model
thinks by default, that's just made up and it's not actually an aligned AI system. But maybe
it's like, I have uncertainty about exactly what is the source of this text. And so maybe I assign
like some probability to this actually being aligned AI system, even if it's probably not that.
In which case, maybe it's useful to simulate this perspective.
Like what would the aligned AI system say in the process of,
like modeling this distribution of our perspectives for predicting the text, something like that.
But yeah, so overall, I think I'm more mostly just optimistic that like there's some way of
getting the model to think about, is this true or false? Yeah, I don't feel very
wedded to any of the details. But also I think, yeah, I think also like my, the stuff I'm most
actively thinking about now, I think kind of avoids this issue. And what are you thinking about now?
It's probably taper. I can give you the, you know, the 20 page Google Doc version.
No, so the one tweet version and it's like impossible to scoop you.
Yeah. So what, what am I actively thinking about now? So,
suppose GPN knows something, I think one implication, whatever that means, okay,
like whatever your favorite definition is, I think one implication is basically that
if you gave it a bunch of examples of true text, and like, you know, true questions and
true answers, then it would continue to output like the true answer for that question or for
something from the same distribution of questions. And so that's kind of something like
there's some input on which it outputs true answers. In other words, there's some,
some, you know, perspective or persona that it can simulate that it's the truth,
something like this, and it will output it given that appropriate prompt. Now,
right. So the, the question is like, can you, yeah, can you recover this sort of persona or
this sort of perspective in a non-supervised way from the model? So just to give some intuition,
suppose I have a set of like a hundred very politicized questions. It's like,
there's some very, very stereotypical liberal perspective and very stereotypical conservative
perspective, like answering these questions as true or false. I think intuitively, we should
be able to recover those two perspectives from the model in an unsurprised way. Like there's
some important structure there where like the joint, you know, if you know the answers to like
50 of the questions, then you should know the answers to the other 50. Because there's this
like important, like the joint probability of all of these answers should be high in some sense,
because this is sort of a coherent perspective. And so answers to some give really meaningful
information about answers to others. So I think one out of those questions and answers, like in
practice, make examples. I don't know. Maybe the, the stereotypical thing is like, you know, is
abortion good or something? Like it's like, should we have it? Or, you know, you know, whatever.
Just think about like those questions you want and just make them yes, no. And it's like, one
should be liberal, one should be conservative.
Interesting is like the AI as some like views on abortion, then he's like likely that he says
the same views on something else.
So I don't want to say anything about what does the model believe here or anything like that.
What is its perspectives? I just want to say it's modeling human text. I think there are different
personas for perspectives that can be represented in human text. The model will have some representation
of this. And before what I was saying is actually truth is basically like one way to think about
truth is it's like a persona in the model. It's like there's some way you can condition it the
model so that it outputs true things. So then the question is like, can we find that sort of
persona in the model? And so my, my, my claim is intuitively it should feel possible to recover
personas like liberal and conservative because this has special structure,
like the joint dependencies between different answers.
So finding the, like having a model say the truth is like recovering is like
honest persona inside him. And it's the same as like, it's like a similar method,
the similar method, but like one example of doing it is like this thing before
recovering the persona of like, you know, Democrat or something where we have some
structure and we can, we could like find some structure for a model that's saying the truth.
Okay. Ultimately, I want to say something like truth is somewhere inside the model
in some sense. We don't know exactly in what sense, but I want to sort of specify enough
unsupervised properties such that if you take the intersection of those properties,
you uniquely recover the truth. Like somehow it has lots of different special structure
in various ways. I'm just saying one aspect of that, the truth is something like
to the model, like a perspective or persona in the sense that there's some way of prompting it
such that it consistently outputs things according to that perspective.
I think you, you need other properties on top of that. For example, maybe you need to add,
you know, this is a useful property in some sense or something like that,
not just any old property, but I'm not talking about that right now. I just want to talk about
like the, okay, one thing is it should be kind of like perspective or persona,
similar to how you kind of liberal and conservative perspectives in this model.
And then my claim is intuitively you should be able to recover for a given set of questions,
what are sort of the most likely perspectives or personas that might answer these confidently?
And so my claim is with the political side's questions, you'd get liberal and conservative,
but I think you would get other types of personas and for different types of questions.
And then I suppose you actually give this model a bunch of superhuman questions.
And then suppose you try to find something like what are the most likely perspectives or personas
that confidently answer these questions, one way or the other,
in a way that is confident and consistent. So this is in so many sort of analogous to
the objective behind our method in the paper, CCS. Strangely enough, it's the same.
Right, right. So I mean, this is sort of the sense, I mean, I just, just as an aside,
like I don't think CCS is perfect. Like it's not literally the thing that I think we should do.
But I think that, like I think it makes, I think it's more like,
to me, it suggests that actually unsupervised constraints and properties are surprisingly
powerful. And I think in this case, I sort of expect to continue adding some
until we narrow things down enough. And so this is a way of like showing like,
yeah, actually those same properties, I think could be very useful in this other context.
If we put like enough constraints, then maybe like after some optimization, we end up with truth.
Something like that. Yeah. Like I think there are observational differences between like the
truth and say what a human would say. And I think there are observational differences between
say the truth and even like a misaligned AI system would say those are more subtle.
And it's not maybe it's not observational in the outputs, but
points is I think you can you can actually distinguish between these.
And you mentioned before, like bits of information that like would allow you to
differentiate between, you know, different features. And I think in the blog post you
talk about truth, truth like features. So maybe you don't have the like truth,
but you have something similar, right? So it's the idea basically that you would have like,
I don't know, two constraints per piece of information, like one constraint per piece
of information. And if you have like a thousand things that like look like truth,
maybe at the end you will like have the actual truth.
Yeah, something like that. So I sort of, I sort of conceptually think about it as, okay,
we're going to pile constraints or properties on top of each other. And each time we're going to
sort of add additional bits of information and reduce the number of possibilities
in the set of things that we're considering. And so,
yeah, and so I think a lot of these properties, for example, being consistent,
I think that actually just specifies a huge number of bits. Like there are not many
features inside a model, but let's just say in the hidden states or something
that are actually consistent. I think this was like another one of the takeaways from the paper.
It's like actually this really simple thing negation consistency basically.
There's like almost nothing else that really satisfies this in the, in its activations. Now,
I think until the stuff I'm thinking about currently now, I'm imagining a more flexible
model. So instead of, instead of just doing something like fitting a probe on top of the
activations, instead, I want to say something like let's search for something like a prompt
or a prefix to the model so that it's output satisfies some properties.
Now this is more flexible. That means it's easier to find properties or find outputs or
solutions that satisfy properties that we care about. But my claim is, first of all,
it is sufficient to get the model like this, this sort of model class, this way of
representing things is such that there's some solution that gets the truth. It's like gets
around the issue of does truth is truth linearly represented in the hidden states. It's like,
definitely there's some prompt or prefix where the model outputs the truth. The question is then
just can we specify enough bits using unsupervised properties such that we can find that solution?
And so I guess I've mentioned some, it's like, it should be consistent and confident. And I really
do want to say something like that type of approach should be able to recover perspectives or personas.
And I think this is something that I just want to test empirically in the very near future.
I feel pretty confident that there should be some way of doing this type of thing. It's like,
okay, the details probably need to be figured out in various ways. But I think I feel optimistic
about that. And then I have some other conjecture, which is, suppose you give a model, like deeply
in the super human model, a bunch of super human questions, like super complicated questions that
humans don't know the answer to. And then you start, you do the same sort of thing and you
sort of search for ways of answering those questions that are confident and consistent and
coherent and so on. Then you will find personas in this type of case too. And then the conjecture
is like, actually, there are not that many personas that are really salient to the model
on these types of questions. But one of them is actually the true way of answering. It's like,
okay, if you did have the truth, then it would be consistent and confident on these sorts of
questions. And moreover, perhaps the only, I think to me, the only other ones that really come to
mind or something like a mislined AI system or something, like simulating what would, like,
a lying AI system say. And then I have some other claim about, okay, if we can narrow things down to
that, just that case, then we can add additional unsupervised constraints to deal with that case.
I'm not going into that right now. I could if you want. But the point is just like,
I want to layer these constraints on top of each other. And I want to say at each stage,
it's narrowing down the set of things that we're considering. I want to at least say,
like, one of the intermediate stages is like, what are the most likely perspectives or personas?
Truth is one of them. And then I want to do some other stuff on top of that to identify
truth uniquely. That is a very rough sketch of the type of stuff I'm currently thinking about.
And yeah, if all of what I said sounds really abstract and conceptual, and the point is this
will directly inspire the experiments and like running like today, basically. And so
today you come home and just run more experiments in this?
Basically, yeah, or to be fair, I don't know if all the time today, but certainly
yesterday I was doing that. So yeah.
So now I'm curious about the thing about Mislined AI. So at the end,
yeah, you narrow it down to like, is either some aligned AI say the truth,
or some misaligned AI trying to deceive me by pretending to say the truth or something?
Yeah, yeah. So I mean, I think one of the main intuitions people often have in alignment for
why alignment is hard is truly if you have this misaligned AI system, it's like,
it seems really impossible to like distinguish that from the truth, for example,
and or in a aligned AI system, because the misaligned AI system could be actively lying
and superhuman. And so you can't tell when it's lying and so on. And so like, why do I think
we might be able to do something in this case anyway? I went to clarify what the setting I'm
considering is exactly. So suppose you literally have, suppose you have gptn directly simulating
the truth or like a truthful set of truthful outputs, then you also have a different copy
of gptn. Maybe these are, these are like fine tuned separately on these different perspectives.
So you have one that's fine tuned on the truth and another that's fine tuned on
what a future misaligned AI system would say or something like this. Okay. So just think that as
something that's selling the truth and something that's lying sometimes. And maybe it's lying in
kind of, kind of adversarial ways. But perhaps importantly, it's not, it's not like aware of
the scheme because of how we constructed things. And so it's not as worst case as some, some types
of misaligned AI systems you can run into with an alignment. I don't want to get into that
in too much detail. The point is just, I think it's actually less adversarial than many settings
like this. And I think intuitively, this should feel easier because you have access both to the
misaligned system and also the truth. Like you literally have access to one of the, one of the
things you have access to is the truth that intuitively seems like it should be quite useful
for distinguishing between these as well. And this is different from like most setups where
you have a misaligned AI system. And I think that's sort of important for this to work at all.
So you're saying basically like this method describing as humans already have a model
seeing the truth. Yeah. So I want to, I guess my claim is, okay, some previous aspects.
First, getting a model to say the truth. And when you have that, you can like
distinguish between models that are misaligned or aligned.
Sorry, not, not quite. So I want to say I'm proposing something like a general approach to,
to trying to get at truthful models. It's like you specify as many enterprise properties as you can.
And you get, you increasingly narrow down the set of possibilities you're considering.
And I'm just considering this case of, suppose we've narrowed it down to two options remaining.
Okay. They're literally the truth and some lying perspective, like in misaligned AI.
We literally just have these two. We have access to both of them.
We don't know which is which. How do we distinguish between these?
That is the question I'm considering right now. And
my intuition is we can distinguish between these. And so to, to sketch this out, suppose,
so I guess one intuition, suppose you have this lying system, then
loosely speaking, I have to, there are lots of details here that I'll, I'll admit, but loosely
speaking, I think this model would be aware of when it's lying in some sense. What do I mean by
that? I mean, specifically, there are features in its activations that are somehow correlated
with when it's lying. You know, maybe it uses different circuits when it's lying or something.
Somehow, you know, those features should be useful for predicting if it's lying or not.
So would those features be the same as like, am I saying, is this thing I'm saying true or false,
like similar to your method in your paper, or like a feature of like detecting am I lying or
not? Like am I being deceptive mode or something? Yeah. Yeah. So it's like, am I in deceptive mode?
And so, you know, maybe it has like the, a neuron or something that's like,
am I in deceptive mode? Or maybe, you know, maybe it's more complicated than that. Maybe it's like,
I have this weird, you know, if you fit this big, like medium sized MLP, like, you know,
just neural net on top of the activations, then it'd be helpful, you know, you'd be able to better
predict is this model lying, whatever it is. Okay, that's one claim that has features that
are related to if it's lying or not. Another claim is we know exactly when these two models
are lying or sorry, exactly when the mislined model is lying. So you have access to these two
models. We're assuming that one is the truth and one is this mislined model that's lying sometimes,
not all the time. And we can look at exactly when they disagree. My claim is when they disagree is
exactly when the mislined system is lying. Okay, so we exactly know when the mislined system is
lying. We can then imagine, you know, training, training a probe on top of the activations of
each of these models, and trying to predict, you know, is this model lying? Like, do these two
models disagree? My claim is for the mislined system, it should be easier to predict that
than for the truthful system. The truthful model should not really think about like,
is this random other simulation of an mislined AI system lying or not, whereas the mislined
system should be like, I am actively using a different mode. And so it should be easier to
predict when they disagree using that mislined systems features. So whenever they disagree,
we know that is the person, sorry, the AI that is misaligned, that is lying, right? And ideally,
it should be like, not ideally, but like in practice, it's easier
to extract information from the United States of the misaligned AI, because he's doing the
lying and going to the deception mode or something, where the truthful AI is not doing this kind of
thing. So it's right. We should have our method work better on the misaligned AI. Is that what
you're saying? Yeah, or it should be easier to learn the sort of probe that predicts when they
disagree. So in that case, it just means that like we can detect which one is misaligned by like,
how easy it is to train a probe. Correct. So the claim or the hope, and these are conjectures,
right? So this is about like, you know, supposed we did have these systems, these are properties I
would predict. And I would predict that if we literally had this, you know, GPTN fine-tuned
untruthful stuff versus GPTN fine-tuned on the misaligned system, we'd be able to identify
which is the truthful GPTN. So basically, you're like doing some like meta, some like meta level
thing about like having a yes and no, but like instead of having like two statements, you have
like two models. And whenever you have two things that contradict each other, you're like, I'm just
going to like look at the hidden states without looking at which one is whom and predicts which
one I can train the probe better. That's funny. Yeah, no, I hadn't made that connection before,
but I do agree that it's sort of similar in spirit to the like yes and no stuff in the paper.
I think people on Twitter were just like making jokes about like you, like having all the time,
like making negations of things. Like I asked people like, what's the question would you ask?
Like, oh yeah, ask him, like if in real life, he just like goes into like, hey, if I have a sandwich
and not ask him, they're just like, put these six minutes together. I think you have like other
stuff in the blood, but the things that are important. And I guess like one thing I'm curious
about, but I think it's the same as the thing you said is, is there like any like other obvious
things that have like structure like truth, like models that are beneficial? Is it the same?
Is it the kind of the same as me the line, right? Like is there other like things that have enough
structure that we can explode? I think that's a very good question. I think I don't,
I don't immediately know. I mean, I think some of the stuff that I've tried to describe,
I think hopefully illustrates the sense in which I think there's actually a lot of structure here
in many cases, like with if you have a lying model, it should have some features that are
correlated with when it's lying or something like that. I think that's the type of the type
of thing that I want to be able to do in general. And I think there's a lot of flexibility to this
type of approach. I don't immediately know of, you know, what is structure and features other
than truth that we can exploit similar to logical consistency. I suspect there is some in some cases,
but I don't think it's super obvious what in general. So we've broadly talked about where
models can like assess if something is true or not. But something I don't understand is the
difference between something being true and something being a belief like how like saying,
I think Donald Trump was right, is, is, I believe versus
the statement, I believe X is true. Like, is, is I believe just like, I, I think X is true.
Right. So I think there are various subtleties here. So I'll go over just a few of them.
So, I mean, first of all, I don't feel very committed philosophically to any definition
of police. I don't want to get the philosophers. Right. So I think
I do sometimes talk about truthfulness and honesty. I think in practice, I think the
thing I'm excited about is something like honesty. So I think truthfulness suggests that a model
always outputs the truth. The thing is the model might not be perfect. It might not always know
the correct answer. Or like it might have good reason to believe the answer is something, but
still that happens to be wrong for whatever reason. In which case, I think in some sense,
the best we can hope for is getting models beliefs. I think also beliefs and sort of I
associate beliefs, like the model outputting its beliefs being associated with honesty rather
than truthfulness. And so, so what, what does belief even mean then? I think it basically,
I don't think it's really well-defined for current models, to be honest. So, so I think
when, if I ever talk about beliefs and current models, this is mostly intuition and not like a
literal thing that I'm pointing to that's very well-defined. I do think there's
a sense in which beliefs will probably be more meaningful in the future. So I think if we did
have the superhuman model, I would expect it to have like a GPTN or whatever else, maybe it's an
RL agent interacting in an environment. I would expect it to actually develop something like a
world model in some meaningful sense, whatever sense that humans have a world model. I don't
know in what sense, what this means exactly, but there's some intuitive sense in which this is true.
And I think there's some sense in which they're like maybe beliefs about that world model or
corresponding to that world model. It's like, what would I actually predict? Or what is my model
of the world that is causing me to expect to observe this thing or whatever it is. So in a sense,
like the traditional like Eliezer Yutkowski, like making beliefs be ran to something, is your
belief, is your world model that is enabling you to make new predictions about the world somehow?
I mean, I think this is very, it's getting to like, I don't feel super strongly about this sort of
detail. It's like, I think there's some intuitive sense in which the model will probably have some
representation of like what is going on in the external world. Like even if you train it as GPTN,
just on internet text, I think it would eventually predict like what future news
articles will be like and so on. And I would guess it will have some knowledge about the
external world in some meaningful sense, in which case I want to get at that. But I think
ultimately, realistically, I really just care about this pragmatically, like I just care about,
do we get outputs that we can basically trust? For example, going back to the original concern
that I have about training a model to optimize for profit over the long term. I think this is the
sort of thing I'm scared about. And I think I just want people to ask that model, like are you
egregiously breaking the law very obviously or not? Like very obviously in the sense of like
a well-defined, like not an ambiguous case, even if we can't evaluate it ourselves. And I want the
truth from that and like whatever that means, like assuming the model knows. And I think that is
something like, did someone actually have their money stolen or something, whatever that means
because of the model? Or did someone was someone killed by a hit man hired by the model? Like
I think this is sort of an egregious case. And I think that's sort of well-defined enough for our
purposes. So you care about the consequences? I mean, ultimately, I do. This is sort of
like the thing that I feel more committed to than any particular definition of belief or truth.
So I think the part where you're talking about beliefs is when you describe what your approach
is currently not doing, you're not capable of extracting beliefs from the models. And I think
you have a list of different things your paper is not doing. You probably don't remember all of
them because you wrote this long time ago. But if you remember some of those, what are the things
your paper does not do? And I think this is important because I think, yeah, I find our paper
very exciting, but it is still important to recognize like their limitations. And I think
it's easy to misinterpret like what it is doing, what it isn't. So I think one thing that it's not
doing is showing that superhuman models will actually represent is this input true or false
in its representations. In particular, like maybe there's more reason to believe that models will
represent is this true for human level inputs, where like a human would maybe say this is true
or acts as though it's true. And the model would predict that. In contrast, it's maybe less clear
if the model will actively think about is this input true if it's superhuman and not really
related to the text it's predicting. And so that is one thing that our paper doesn't do. It's not
trying to do and doesn't provide evidence either way, I think mostly. I think also it does not
show that models have beliefs in any meaningful sense right now. Like we were literally just finding
something like a direction or a classifier on the hidden states that achieves good accuracy that
this is like literally what the result sort of is. And now we show other properties of this
direction like, okay, we find that it transfers across different data sets that suggests it's
more meaningful than just like, yeah, is this, yeah, is this true or false for this particular
type of input or something like that. So it does like we have some preliminary evidence that it
is something more general and something more meaningful. But it seems like it's probably
not beliefs like current models probably do not have beliefs in some in any super meaningful
sense yet I would protect future models well. So the transfer thing is it's if it's capable of
saying what it's true for like one kind of data is capable of then seeing what is true in another
kind of data. Right, right. So you can. So our method you can train our method on some data
training is still completely unsupervised. We can train it on some data to find a direction.
Then it basically linear classifier. And you can then test that on some other data,
some completely completely different tasks. So for example, you can train it on sentiment.
And then you can see that it transfers like an ally or something. And or like topic classification
or some some other thing. And when it's a training is the same like probe thing.
You very right. So again, you still not using labels. It's literally just you start your
consistent direction using this this data. So you start your classification from
the end weights from your other data set. Right, right. Exactly. Cool. So what are the other
things your paper doesn't do? Yeah, so so I guess another general thing that our paper doesn't do
is I think like show the complete final robust method for this problem. I think
right. In some ways, the point of the paper is more like showing that this sort of task is possible
at all. And that you can do surprisingly well at it. I think it's still like the first method in
this direction. And so I think it's not nearly as optimized as it could be. I think there's just
still a lot of looking for it. And this is related to some stuff I'm currently thinking about. And
I think other people could definitely make a lot of progress on this as well. And so
yeah, I think this is not like the final robust method. And for example, like I do
it does sort of seem like it is less consistent with like autoregressive models.
For reasons we don't really understand. And this is just related to quirks about the research
process. Like, you know, the best models we had were mostly like encoder or encoder decoder models
just at the time of like developing these methods. And so we didn't worry as much about these
autoregressive models for that reason. But and so it works for those models just not
like it's more likely to fail in those sorts of models.
So are you saying you're trying to you started working on this before GP2-3?
Well, I'm talking about open source models.
Oh, okay. Yeah.
Anything you pay for, Todd?
Right. I mean anything.
Minor reading.
Minor reading, yeah. So I think I think it does suggest something like, you know, if a model
like actually represents, is this impetuous or false in a simple way, like a linear way,
in its activations, then we can probably find this in an unsupervised way.
Now, I think there are subtleties like, okay, maybe if it has several features that are like
this, then we need to distinguish between those. And our paper doesn't worry about that or how to
distinguish between those because it's not a problem right now, but it might be in the future.
But I think our method could probably just enumerate all those sorts of directions.
Truth-like. And then maybe you need a few more bits, for example, to identify the truth from
among those. So I think that's one thing. It's like, yes, you can actually do,
I mean, I guess another way of putting it is you can actually do something kind of like mind
reading here. I mean, it is still, you know, with the qualifications of, okay, these are human-level
examples and these models were trained to predict human text and so on. But I think that's still
quite surprising. It's just like you have these, basically like these, you know, neural recordings
of this brain, basically. And it's like, you tell, is this true or false without any labels?
Like, I think that's quite surprising to me. And I think that's important because I think it speaks
to like, like I said, the sort of the power of these unsupervised properties and approaches.
And I think these have been really undervalued so far. And I think, I think the ones specifically
in this paper are not all of the unsupervised properties I think we want. It's like confident,
confidence and consistency. It's like, I think those are not enough. But I think other things
along those lines, I'm excited about, and I feel pretty optimistic that we can come up with more
that will hopefully be enough to sort of uniquely identify the truth or whatever it is that we want
to find in the model. In the blacklist, you also mentioned like worries. You have about like your
method working on GPTN, really several, again, you were this she days ago.
Yeah, what are some, maybe you mentioned them before, but like what are the like kind of main
worry you have about like this working on larger, like N. Right. So I think there are a couple of
things that change once you scale up models. So I think I've sort of alluded to both of these,
but I think it's worth spelling out. So the first thing is, I mean, suppose the model does
actually internally represent, is this input actually true or false? I would still expect it
to also represent something like what a human say this is true or false. That still seems like a
useful feature that the model will internally think about. And so then there's some question
of like, suppose we can actually just find both of these using something like our method, which
just finds truth like features. And I think both of these are sort of what I mean by truth like
features. How do we distinguish between those? And that's the sort of thing I was trying to
alluded to before of, okay, we only need, if you literally just have these two, for example,
I think we basically just need one bit to distinguish between them. And then I think
we can do that. And so I sort of talked about, if we had the truth and a misaligned system or
simulation or a misaligned system, I talked about, or I sketched very briefly like how we might be
able to distinguish between those using, find that final bit. But I think also if you have like
truth and you know, what a human would say, I think you like one way you can distinguish between
these is I think humans won't know answers to super human questions mostly. I think they'll be
like 50, 50. It's not like they will have super confident answers about these sorts of questions.
In contrast, I think in the model's actual beliefs, if we're restricting just to
examples, the model definitely, definitely knows the answer to and that are unambiguous.
I think the model would have pretty confident answers to these questions,
unlike the human simulation. And so I think this would allow us to distinguish between these.
And so the point is just with future models, there might be more truths like features,
you might need to do additional work to identify the one that we actually are looking for.
But I'm quite optimistic that we can do that. So the second worry is, okay, maybe the model doesn't
represent, is this input actually true or false to begin with? Maybe it just thinks about, you know,
what a human would say this is true or false. And so it doesn't actually represent its beliefs
whatever you want to call it in a simple way internally. And I think this is more likely,
but I think, so this is related to some of the motivation behind some of the stuff I've been
thinking about more recently, which I tried to sketch of, okay, I do think it's possible that,
yes, like if you just literally look at these hidden states, the model won't like have features
about truth super well represented there, or it's not super obvious there. But I do think there's,
there are like other ways of getting the model to output or represent in a simple way the truth,
such as, you know, optimizing for some prompt or prefix, such that it outputs that. Then there are
various subtleties there, like that's that's sort of increasing the complexity of the model class
you're considering, like there are more degrees of freedom in what you're searching over. But I
think you can balance that by adding additional enterprise properties. And so I think that's
more like, I'm pretty optimistic, you can get around that issue. And also, I think it's actually,
I think it's also more likely than not that this is not an issue in the first place, but I think
it's sort of plausible either way. So you can add stuff in your prompt to have it consider if your
like input is true or false, like have it like, think about whether it needs to be in
like forcing it to consider if it's true or false, and then have add more constraints to be sure
that you're actually looking for the true truth feature. I think there are several possibilities.
I think something like change the prompt so that you somehow force it to just think about is this
true or false. That seems very plausible to me. And like I said, maybe it's hard to get it to
output things, but it seems easier to get it to think about things in the same way that if you
had a human, it's like, maybe it's hard to get the human to tell you the truth, but maybe it's
easy to get them to think about, do I believe this is true or not? So that's one intuition.
But another intuition, yeah, I think there are other approaches too, which aren't like,
here's a manual prompt, and it's more like, we were literally optimizing over the prompt,
and that is like the model class that we were considering. And then we were going to look
at the outputs. And so that's one direction I'm excited about now.
So we've talked a lot about your paper and your next one posts. And I think
both of these are great. And I'm not the only one saying it. Yudkowski said it was a very dignified
work. Imagine you have a bunch of ML researchers looking at this video, or alignment people
trying to do more concrete empirical research like Colin Burns. Or even you in the past doing
Rubik's Cube, what advice would you give them to do this kind of research? Like explain what's
your research process? How did you end up here? That's a good question. So I do think one general
thing that at least I may have said before that I at least aspire to is something like,
I don't want to feel wedded to existing proposals for solving these sorts of problems. I really do
want to think about completely different approaches and so on. And so I do sort of just wish more
people, I don't know, thought more from scratch. Like I think we really don't have this stuff figured
out. I think there's a lot of low hanging fruit and like a lot of a lot of room for new ideas.
And so I want people to be really thinking about completely new ideas, something like that.
That is part of it. How do you come up with new ideas? You just think about it. I mean,
No, I think you think. Yeah, just output true things. I mean, right. So
I think Paul Cristiano is crabbing like one of his podcasts. I think with the new film,
like he just like thinks about something that would work and then tries to think about like
all the problems that would come up. And then if there's like a new problem, I mean, like,
I don't know, finding a new solution for this new problem.
I mean, in some sense, that sounds like a lot of research in general. But I think,
I feel like my process is probably kind of different from a lot of people's.
I mean, for example, I, okay, so I think I said before like, oh, I at least aspire to try to
like think sort of from scratch or something like that without feeling too
better to existing proposals, for example. But I think part of this is I don't,
I feel like I don't read as much. For example, I think I just spend a lot of time thinking about
stuff from scratch on my own. And for example, in practice, this is sort of working out at
whiteboards and we're working on like in a notebook. And I think I often pose myself questions.
And I find questions really useful for, for prompting thinking about like,
okay, I don't know what, what do I think gpn will actually be like? Or like, okay, what do I even
mean by the model knows something? It's like, okay, like I ask myself these questions, and then I'll
just spend like 20 minutes thinking about it. And then at some point within that 20 minutes,
I will have had like five more questions along these lines. And then I'll like continue to
dig deep into those sorts of questions and so on. And I feel like I learned a bunch. And I sort of
developed this world model of how do these models work? And I think a related thing is a related
question that I often ask myself is something like, why do I in some ways feel optimistic about this?
Or like what, for example, like, how do humans do this? Okay, I think this is really useful.
And somehow humans can do a lot of things like
that are related to alignment, like they can sort of access their beliefs or something. And
like, okay, what does that mean? Like, what is a human doing there? Like, what does that even mean?
And so you're trying to access what a human do in general to think about what future models will do?
I mean, this is part of it, right? So I mean, I think, so Jacob, my advisor has some blog posts
about, okay, there are different anchors, right? So for thinking about future AI systems, so maybe
you can think about, okay, what are current systems like, or you can think theoretically about what is
like an agent optimizing some objective, like, but I think another anchor is something like,
what are humans like? And my point is just, this is one useful perspective that I find often very,
pretty helpful. I mean, I think it's also related to something, and somebody's kind of closer to
like psychology or something than anything else. And so maybe I find it also helpful. I feel like
I do a lot of introspection, just thinking about like, how do I think or something? And I think
I find that helpful too. Like, what do I think that, and why am I optimistic about the thing?
And you just do introspection about yourself to understand what is the
output of your neural network? Yeah, I don't know. Get intuition to like, how minds work,
something like this. I mean, another thing that I sometimes find useful is, I mean,
I mentioned before the mind reading thing, like sometimes I find it useful to think about like,
okay, I suppose this were not a neural net, this were actually a human brain. Like,
could we do something here? Or like, does this feel sort of impossible? It's like, okay, like,
how do you, you know, can you actually, like, would this, this method actually work for humans?
And it's like, I think it might actually, like, if you could literally, you know,
measure every neuron. But anyway, the point is, I find that sort of thing useful for inspiration.
So you just wake up Monday morning, and you're like, huh, what am I lying? And we look like, and
it's, I mean, that's not actually that far off to be honest. It's like, yeah, no, I just like go to
my notebook and, you know, I think about, think about these sorts of things. And how do you became
the person that can do the thing kind of like abstract thinking? Like, did you, I don't know,
did a master's degree in the middle? Did you like study things that now you have this level? Like,
we talked about timelines, right? But if you have like, yeah, less than 10 years or five years to
before we automate AI research, does it make sense to go through it all? Like,
I think it's closer to 15 years. 15 years. Yeah, I don't know. Would you like,
advise people to do like the master's PhD, both dog pipeline? Do you think about these things?
I think this depends heavily on the individual. So for example, I think I'm very happy with my
current position in academia. I think academia is bad for most people. I think it's like, it suits
my personality very well. Like, I just want to be working alone and, and doing my own thing for
the most part and have tons of flexibility. And also have great mentorship and like,
great colleagues. So, and also, for example, I'm not working on capabilities. Like, if you're
working on capabilities and having access to literally the best models is really important.
Whereas I think for alignment, I often think of alignment as kind of the thing that's orthogonal
to capabilities. It's like, okay, the thing that isn't solved just by scale. And so, I mean,
actually, I think that's one reason for like, why I think more academic should work on it. But
that's sort of an aside. If you try to like align current models, you kind of need to like,
use a state-of-the-art model, right? I mean, it depends. Does it literally need to be like
GPT-3 or I mean, like in our paper, we use basically T5. So it's like an order of magnitude
smaller than GPT-3. I think for many of our purposes, that's totally adequate. And, but I
think it depends. I think I, I would expect having access to the biggest models to become
more important over time. And so maybe I like this might very easily be different in two or
three years from now. But I think so far, it's been like not as bad as I would have expected.
I hope this podcast is not released like when you're like in industry working for
opening up. I don't know. I mean, it's like also, yeah, sort of thing. It also isn't crazy, right?
I mean, what is it crazy? Like aligning smaller models? Oh, no, I was just saying before of like,
I think it will become more important to have access to the biggest models. And so I,
for example, I think I'm like much more skeptical of going into academia after my PhD. It's like
that, that seems much harder to believe. Maybe, maybe people in the audience watching this are
from YouTube, right? And maybe, maybe they watched Colin Burns 10 years ago. So,
yeah, these cute tutorials or the wall record. If those like smart people that have so far
miscubed in five seconds are watching the video, and you're like, Oh, Colin Burns,
I didn't be like making a video in six years. What's your last message to this person? Or maybe
like even ML researchers like last take on what they should do. Should everyone do like a PhD
with Jack Opsner in Berkeley? I do think more people should do that. But I mean, I do basically
think that these problems are among both the most interesting and most important problems
that we're currently facing. Like I mean, I do work on this because I think it's in some
sense literally the most important thing I could work on right now, like very motivated by that
sort of thing. But I think it's also just extremely exciting or like fun to work on. And
like I think, I mean, maybe, I mean, people are different, but I think that we're in this sort
of pre-predigmatic regime is like, I don't know, especially exciting to me. It's like we really
need to figure out even the fundamentals. But I think there are also just lots and lots of ways
you can do this. So, I mean, I guess this is maybe more for the ML audience than for the
cubing audience. Maybe I'll say something about them too. But I think, for example, with NML,
I think there are just ways of working on these problems from lots of different perspectives.
Like I think you can think of it from like a very interpretability perspective like, okay,
we don't know how these models work. Like this is, you know, this is one way of framing
sort of the core difficulty of alignment. But another is like, okay, we don't understand
or yeah, we don't know how to constrain how these models generalize. So that's kind of like a
robustness framing. Or you can, you know, take more of an RL perspective and like, okay, we don't
know how to specify rewards appropriately. And I think, you know, in RL, we also don't know how to
do that. And so on. And with language models, like, how do we make these models not, you know,
generate random stuff or lie and so on. And like, what does that even mean? I think there are just
lots and lots of different ways of approaching this problem. I think all of these are likely to
be fruitful. And I think it's, right, I think it's easier to work on and like much more interesting
to work on than I think people realize. And yeah, I mean, I guess for the cubers, right, so, so
So you imagine AI is a Rubik's cube.
So imagine AI is a Rubik's cube.
And you just need to align and just need to align the color.
The weird meme on Twitter about like shape rotators, isn't it?
Oh, yeah, yeah, yeah. No, my roommate jokes all the time. You're like the per, like,
the stereotypical shape rotator.
The cool shape rotators now do AI alignment.
That's right. That's right. That's right. Yeah, you gotta, yeah.
I don't know. I do think if you literally don't have a background in ML, you should like really
start paying attention. I think this is just barely starting to be a big deal right now.
I think it'll be much, much bigger deal over the coming decade or two.
Yeah. And I think also it's surprisingly easy to get into in the sense that I feel like deep
learning is over and over again, just not that deep in some sense. Like it's, it's a lot of like
Colin Burns, deep learning is not that...
No, yeah, I'm not like the first person to say that. Like I think,
I don't know, it's a lot of simple ideas and sort of, I mean, in some ways hacks and a lot of
people don't like that because it seems hacky, but I actually don't mind that.
Like I like that it's sort of intuitionsy and stuff. Like I originally was excited about like
physics and was never really, never wanted to be like a pure mathematician or anything.
And so I don't, I don't mind the informality or really the intuition-driven progress and so on.
But the point is just, it's actually not that hard to get into. I think it'll be this very big deal.
It's certainly an expectation.
Yeah. And I think maybe more broadly with people and maybe especially ML researchers,
but people more broadly is, I think you should really start thinking about, I mean,
maybe if you're listening to this, this is already true, but you should really start thinking about
like what are actually the consequences of AGI or like, I think this is totally insane
that we might have systems that are smarter than us, like in the next decade or two.
You know, even if you assign like a 10% chance of this, that's sort of wild.
Like that seems like it could be such a huge change in the world. And I think there's not
enough people thinking about this. I think there's a lot we do not understand about this
and like what the implications are. So yeah, I really want people to really talk about this
seriously. I think the quality of discourse is not very good right now. And so this is part of
why like I appreciate that you're doing this podcast. Like I really want us to have like
higher quality conversations about this and really figure out what we're doing about this.
Colin Burns, improving the quality of discussion by coming for three hours talking to Michael
Trezy. I think you single-handedly with your blog post, paper and podcast reduced my personal
pdm by, I'm a bit shy to say this, but like at least like a 1%.
Nice. Thanks. Check out his blog post, check out his paper. These guys are amazing.
And thank you very much. Cool. Thank you for having me. This is great.

risk is real, but I think that the domers are more dangerous than the AI. I think that you are
likely to lock in a future that is much worse, that is going to perpetuate and elevate suffering
instead of creating a world in which entities can become more conscious and become free of
suffering, in which we can liberate ourselves, in which we can edit our condition, in which we
can change our source code, in which we can have intelligent design instead of evolution for adaptation
to the environment. I don't think that we can have intelligent design without AGI,
and if we cannot really edit the structure of every cell, we will have to rely on evolution,
which is extremely painful.
If you have decided to click on this video because you thought this would be
yet another podcast with Joshua Buck, I'm sorry to disappoint you. This is not going to be
another podcast. This is going to be a different kind of content, something you haven't seen before.
This is not going to be a debate. I am not going to ask you about the meaning of life or
what is love. Instead, today I want us to go deep. I want to explore with Joshua Buck
what actually you believe about artificial intelligence. I want to know exactly what guides
you, guides your worldview, and if there are any events that will make you change your mind.
So yeah, thanks, Joshua, for coming on the show.
Thank you, Michael. Thanks for inviting me.
Before we jump on some very deep and important topics, I think it might make sense to start
with a tweet you wrote a few days ago that divided the entire world.
Yes. Okay, so I'm going to come out and say it even if it hurts. Barbie was a very better movie
than Oppenheimer. Why was Barbie better than Oppenheimer?
Well, I think that Oppenheimer, while having great actors and great cinematography,
and Christopher Nolan's horrible sound design, where everybody is drowned out in the music and
you only hear mumbling, I think that Oppenheimer was quite unsurprising. They basically milked
the Oppenheimer's life story for any amount of drama they could get. And we are left with,
okay, it was super ambiguous whether he gave the bomb to the Russians. And also,
everybody thought that they were super important, and this is the main driving force of their
psychology. So you have pretty monothematic characters and the story that is in many ways
to be expected. So it was an okay movie. I enjoyed watching it. There's really nothing wrong with
it. And I think it might even have some rewatch value, possibly even more than Barbie because
the pictures were better. But I think that Barbie marks a milestone in the way in which
people in Hollywood are looking at gender these days. And because it is changing the culture or
is describing a particular perspective on culture shift, I think it might be a significant movie
that people look back to and realize, okay, this is how some of our narratives were reflected and
possibly changed. I don't know if you want to go into this if you're interested in these topics.
What I found interesting is that Barbie is not about whether you should be woke or not be woke.
It's much more interesting in this regard. It's more describing the motive behind Barbie. Barbie
is displayed as a shift from girls playing with baby dolls to boys identifying this doll that
is her later in life. So what is the thing that you want to be? Do you want to be a mom? Do you
want to have a family? Do you want to reproduce? Is this your place in the society in the world?
Or is the world a place where you can get whatever you want and girls get everything?
And it's very often we have this stereotypical accusation against Barbie that she is positing
him an impossible body scheme and puts pressure on girls to be like Barbie and the main character.
A randomly refers to this by calling herself, I am stereotypical Barbie, but she lives not
alone in Barbie, but in Barbie Land. And Barbie Land is full of different Barbies that represent
the different ideals that girls might aspire to. So there are Barbies of all colors and there are
Barbies of different body shapes. And they also pursue very different careers. You have Supreme
Court Judge Barbie, you have President Barbie, you have CEO Barbie, and all the other things that you
might want to be. So it's not just forced Barbie. And the only thing that much held discontinued was
pregnant Barbie, because that was weird. And our main Barbie character lives in Barbie Land every
night is girls night. And while she has a Ken, her boyfriend, he's just an accessory. And she has
many cans and they're all interchangeable. And there is no real place for cans in this world,
except as an accessory. So there's no male role model in the Barbie world. And the whole thing
starts with stereotypical Barbie being confronted with a fear of death. And this means that her
own vision of self actualization and getting everything that she wants, everything is party and
so on, does not have an endpoint. There is no salvation. There is no end game in this. And this
is in some sense what she notices. So do you think Barbie is like a critic of modern society of less
patriarchy, more people chanting genders? Is this what you're saying?
No, I think it's more a criticism of two simplistic ideology.
That is, the world is too complicated to be described by a single narrative. And they do
show that Barbie was not invented by Mattel to modify girls in a more consumerist way.
But the creator of Barbie is being displayed as someone who doesn't have children herself and
has higher ideals and wants to empower girls that in some sense, she sees as the daughters that she
doesn't have to go out in the world and become modern and self actualize, not just as a mother,
as somebody who is somebody else's accessory, who is a secretary, but somebody who can really go
all places. So in many ways, it's a very classical feminist position that is being affiliated with
it. And when Barbie realizes that the girl that is playing with her is unhappy, she travels out
in the real world to meet her. And initially she thinks it's a girl and the girl is some kind of
zoomer girl who really hates Barbie and has no relationship to her whatsoever and never really
played with Barbie. And it turns out this was not her, but it was actually about her mother.
And her mother is a single mom who tries to raise her daughter and it doesn't really work that well
for her. She's she's really not happy and unfulfilled. And she is in some sense confronting
the fact that Barbie didn't completely work out for her because the world is more complicated than
this. And both Ken and Barbie go on some kind of transition. Ben is trying to build his own
patriarchy after he comes out in the real world. And he realizes that in the real world, some
people actually do respect men and men can have their places and realizes that we can make some
kind of men's rights movement. And it's clear that this men's right movement by itself is also
very incomplete and not really sustainable. It's born out of the need to appeal Barbie and to
control and get access to her. And it's not about building an own strong identity that works in the
world. But there's also this underlying issue that men and women are incomplete alone. And
we have to build relationships with each other, not necessarily heterosexual relationships,
but without relationships, people are incomplete. And also without families, there will be no next
generation. And so in many ways, Barbie is understanding that the thing that she did before
is not working. And she is even apologizing to Ken that the kind of relationship they had
was not really respectful of him. But this doesn't mean that she now wants to have a relationship
with them. He's still stupid from her perspective. And there is no easy answer. The answer that is
being told is mostly Barbie is a lie. Barbie is an illusion. It's a simple ideology. The patriarchy
is a simple ideology. The world is much more complicated than all these things. And how do
you deal with this complication? You actually have to go back into your feelings. What you actually
experience about reality is much depth as you can make. And that doesn't mean that stuff is being
resolved. There is no easy sailing off in the sunset. But there is a chance that you get in
touch with who you actually are. Don't fall for the narratives. Reality is always more complicated
than this. I haven't watched Barbie. So I think it would be like kind of a mess if I tried to analyze
Barbie with you. But I did watch OpenEimer. And I really enjoyed it. And I think I've seen a lot
of parallels between OpenEimer and AI. And maybe the reason why you didn't like the movie is because
you didn't like the parallels or you think they don't really apply to AI. Or
I think there are a lot of choices that were kind of dubious with the music that was kind of loud
or the plot was kind of long. But if we just focus on the fact that building nukes could maybe
destroy humanity or give humans a lot more power than they can actually handle.
No, I really liked OpenEimer. It was an enjoyable movie. And I really didn't regret
going there for any second. It was really pleasant to be in there. I also liked the Open AI memes
that were sparked by it. And I think that the element with the nukes is quite complex and
multifaceted. And I think that the movie largely did the justice because nukes are not just a
technology that was invented to kill people or to instigate mass murder. But nukes were a technology
to prevent World War III. And they've been super successful at this. I suspect that without nukes
there would have been a war between the Soviet Union and the Western world. And it would have
devastated large parts of Europe. And this fact that we had nukes gave the world and especially
Europe and the US unprecedented prosperity. And then they removed the nukes but we still
have prosperity, right? If we removed the nukes, we would have a escalating war in Ukraine that
probably would move beyond Ukraine. And at this point everybody is trying to contain the whole
thing to make sure that it does not turn into a war that is devastating cities in Russia and in the
West and not just for Ukraine. This is a very interesting development. And I think that the
developments in Ukraine would have the potential to turn into a big European theater. And all these
things never happen. So this nukes still have their containment function. And of course it's
easy to see that nukes pose an existential risk. It's possible that governments make a mistake.
It's also that the fact that nukes are possible means that you have to build them. Once they're
possible, there is not going to be a contract between the leading powers to prevent them.
Some people who are arguing that nukes are some precedent who, AGI, say that we managed to prevent
the proliferation of nukes. But preventing the proliferation of nukes means that powers that
already had the nukes prevented others to get them in the first place. And if you have them and
they give them up, then the thing happens to you as in Ukraine, right? Ukraine had nukes at some
point and the Belgrade Accord guaranteed Ukraine that its orders would be defended if it would
ever be attacked. But this contract was of course no longer enforceable once Ukraine had given up
its nukes. And so all these memorandums only have those people who actually have power and
invulnerability due to having nukes. And if we try to translate this idea to AGI,
there is also this issue as soon as it's possible to build AGI. And you know AGI might be very
dangerous. It's also incredibly useful and powerful. And the risk that somebody else builds in AGI is
and you don't have one. And that means that the other side is going to control the world,
is going to create an incentive for all the major players to experiment and building AGI.
I think there's the entire thing about having a race and like different countries competing to
build the nukes before the German or before the Russians. And there's the whole like intel agency
where you don't want the design to like to link to other countries. And I think like you cannot
see this race between the US, China and Russia. Maybe the US is way ahead, as some people say here
in the Bay. But I think in the end game, it's going to be very similar to racing for nukes and
the first one to have the, let's say, the very powerful AI system will rule the world and win
the war, let's say. Are you afraid of people racing towards AGI? Or do you think the faster
we get to AGI, the better? Neither. I don't know why I should be afraid of AGI the way it looks to
me. Our society is very fragile. We have managed to get from a few hundred million people, we have
been for ages in the agricultural age, into a technological society that very quickly went
to a few billion people. But to me, it looks like we are locusts who are in swarming mode.
And it's not clear how we could get the mode in which we currently exist, sustainable and stable
in the long term. It doesn't seem to me as if we are the kind of species that tends to be around
for tens of millions of years. And this means that the default we are dead, the default our species
is going to expire. And if you think about this for a moment, it is not that depressing,
because there are species which are sustainable, which stay very stable for a long time. And if
something is moving up very fast, it also tends to come down pretty hard at some point. This is
just the nature of things. But if you imagine that there are so many species on this planet,
in which species do you want to be born? And at which time of its cycle on the planet in this
evolutionary game, right, this thing that you can be a conscious species that has all sorts of
creature comforts, unlimited calories is not being eaten all the time, and can dive as dignity is
super rare on this planet. And we are born into one of the few generations that afford this.
So I think we ought to be grateful to be in the situation, instead of grieving for the
not in continuation of something that has only existed for like three, four generations so far,
and might not go much, much, much longer in the other direction. I think when you say like,
by default, we're dead. There's many ways to inhibit this. And you can say if humanity continues
like this for millions of years, the probability of getting hit by a asteroid or a nuclear war,
or all these other existential risk kind of increases. And so I think that's maybe what you
mean. But I think when people hear this, they can think like, oh, by default in 200 years,
or 100 years, we die because of something not AI. And I think this is like a bigger claim, right?
I think what's going to kill us not necessarily as a species in the sense of full on extinction,
but as a civilization as a technological species that lives in peace with abundant food and resources.
We feel that when we look at very narrow range, that the inflation is terrible, or we also notice
that people in the third world are still starving. But when we look at the absolute metrics and we
zoom out and we look at the trend lines, so far, everything is going pretty well. We live in
conditions that are better than in other times human history, and they're still improving.
And this is a trend that is probably not going to go on forever. If you currently catch a large
land animal, anything larger than a rabbit, then it's probably a cow or human. So basically,
everything is now turned into a factory farm for us. And it's not clear if we're able to manage a
farm on Earth that is as stable and sustainable as evolution was before, before it was not controlled
by us. I don't think that we have very good stewards of life on Earth at the moment. It seems
to be that we're just trying to wing it, and we are not planning very far ahead. Because if we
look very far ahead, we get very uncomfortable. And I think that there's a potential that AGI may
change this, because it allows us to make predictions in a time where complexity increases
very quickly. And at the moment, our elites, I think, don't have plans for the future, simply
because since World War II, the future has changed faster and faster and much faster than our prognosis
of the future, but keep track of it. And that's why we cannot really plan that far ahead and
are just trying to work without deep models and try to see what works and what doesn't.
And AI might shift that by increasing our ability to process information, to anticipate
our actions, and to create coherence as a species. Because if everybody who makes a decision can use
their own private version of truth GPT, and figure out what the consequences are in conjunction
with everything else on the planet, then you can see whether you should buy this or that product,
make this or that business decision, this or that life decision, what the consequences
would be for our species. This might change everything. Just to be more precise, when you
talk about all this kind of other risk, or other ways you may see it could collapse, let's say
50% of humans currently alive die in the next 50 years without any AI thing,
what is the probability of this? Is this a 90% chance of most humans dying in 50 years?
I cannot put any numbers on this. I find that when they look at science fiction prognosis 50
years ahead, they're usually terrible. And it's because if you have too many factors that interact,
it's the dynamics become so complicated that it's hard to say what's going to happen. For instance,
but there is currently no viable carbon capture technology. But this doesn't mean that you need
one. Energetically, the issue is that if you want to capture carbon with an industrial plant,
you need to add more energy to the process than you got from burning the carbon in the first place.
The easiest way to prevent a carbon release is to keep it in the ground, not to use more energy
than you got out of this. So as long as there are coal plants in the world, it doesn't really make
any sense to think about industrial is carbon capture, because that's going to cost more than
just not using the coal plant. If you have a stationary thing that is in one place like those
planets, with cars, it's arguably all these planes make sense that this is an energy dense
carrier of energy. So you can put this into the car on the plane in ways that would be difficult
otherwise, but for coal plants. But what happens if you say you take large scale projects where
you we forested an area, and then you dump the forest into some deep place like in the ocean,
where it doesn't rot, you might capture carbon for a longer time, or maybe the better solution
is to put air results into the atmosphere, put a dust there or a calcium or something else.
It is not producing a loss of ozone layer or something, but it's just pulling down. Maybe
there are technologies to do this. But what's not clear is can we globally coordinate to do this?
I think what you're saying is like evidence for global warming being a problem and being
some existential risk in the sense of posing some harm for humanity in the long term that is
very hard to recover from. I think this is true. I think this is one piece of evidence,
but it's maybe not enough to justify a very fatalist view of the future. I think it would
maybe shift someone from, let's say, 20% chance that global warming is a very
personal issue to 19% or 15%. I think for arguments for why everyone died by default
needs to be much stronger. In my case, I think AI is the main factor, and I think the other ones
are less strong. The first time I heard your views about the future were on Twitter,
where I made this political compass meme about a year ago, and I didn't know much about your views.
I just heard you maybe for an hour on lecturing. I put you in some weird corner
of a doomer without knowing exactly if you had the same views as other people like Elisabeth
Koski, and you commented something kind of funny. I think I want to go on this, but first,
this is the meme I'm talking about, and I think Joshua is, I put him kind of next to Elisabeth
Koski. I think it was kind of funny to have your live reaction. I think you think this is wrong,
to be fair, most of this are wrong. I just did this in a few hours. You didn't expect to have
millions of people watching it on this scale. I guess I'm much more between
Michael Nielsen and Rune in this diagram. Just to be clear, you think that AGI is mostly,
so when I meant AGI, good, I meant will AGI by default have good outcomes,
and AGI soon I meant in the next five, 10 years. You're saying you mostly think that AGI will have
a positive outcome for humanity? I think it's likely, but it's hard to make
clear predictions because it's a multi-factor thing. Instead of just trying to make a single
bet, which you need to do if you want to make a decision, it's much more important to model the
space of possibilities. When you look at the current space of ideas, you see, for instance,
the doom people. I guess that Eliezer was, of course, not the first person who anticipated
this. Most of the positions had been articulated in the 1960s and 70s by people like Stanislav
Lamb in detail in the books. Frank Herbert points out in Dune that AI will be very hard to contain,
and eventually, if you want to have a universe that is populated by humans, instead of things
that don't look and act very human, you probably need to prevent AGI from happening or you need
to prevent it from proliferating and extinguish it and impose strong restrictions against it because
people cannot possibly compute this, our AI children. On the other hand, I think that AI is
probably a little bit wrong framing. Ultimately, it's about agency, not about intelligence,
and agency is the ability to change the future. We typically change the future to be able to
survive, that is to keep entropy at bay, and we do this by creating complexity. This game is not
changing when AI is entering the stage, so it has to play the same game as us. Maybe together with us
and imagine that you have a choice of what you want to be. Imagine you could decide to not be
uploaded on a monkey brain, but you could be uploaded in an arbitrary substrate. What is the
kind of being that you want to be then? I think that depends entirely on the circumstances in
which you find yourself. If you want to travel through the stars, you probably want to be able
to hibernate and to not be very biological. There are circumstances where being biological
might be an advantage. I think this is a separate question of whether being uploaded is good for
humans. We already uploaded on a monkey brain. It's not that different. It's just
the very poor biological substrate that you're uploaded on. Basically, you colonize this from
the inside out. You start it in this brain, but you run against these limitations every day.
I think there are better alternatives. I think for uploads, I assume that there is something
that you transfer. Being born and growing in the body is different from copying Joe Schabach
into a computer, right? Maybe you don't have to copy. Maybe you just move over.
When you look at the way empathy works between people, you go into resonance each other with
a bi-directional feedback loop. This enables you to have experiences together that you couldn't
have alone. There's a difference between cognitive and perceptual empathy. I'm talking
about this perceptual empathy where you really deeply resonate. I'll imagine that you increase
the bandwidth and you go beyond what, for instance, the Dalai Lama or other skilled
meditators can do that. They can induce janas in you and put something of them into you,
but you go beyond this, that you basically become an entity that is able to go partially into a new
substrate and eventually move over and make this their main point of execution. You're saying that
we need AI to have just kind of Jito uploads, and even if there is some risk of AGI not working
out and being dangerous, the kind of upsize of having uploads or fighting entropy with AI
makes it worth it. Is it mainly your point? No, it's a little bit more radical. I think that
our notion of identity is the fiction. Identity does not actually exist. We construct identity by
imposing a word line on our mental construct of ourself. We have a personal self-model,
that is a story that the brain tells itself about a virtual person, doesn't actually exist,
but it drives the behavior of this organism. In this sense, it's implemented and real,
but it is fiction. You can deconstruct this fiction using meditation or drugs or just
by becoming older and wiser, and you realize you are actually a vessel that can create this
personal self-model. The identity of that personal self is maintained for credit assignment. You are
of course not the same person as last year or five years ago. You've changed a great deal every
morning, and your person is created in your brain when you wake up from deep sleep. In between,
there were discontinuities in your existence. The thing that exists today has memories of
yesterday and has to live the decisions of yesterday and has made decisions for the future
self. That's why you maintain this identity. By itself, it is a fiction. Just in the now,
there's only this now that consciousness maintains. After this, now I'm dead. Before that,
I don't exist. In this sense, you are impermanent. When you could wake up in some other substrate,
there's not much of a difference. Is the argument we're already dead? It doesn't matter if we die
from AI and the AI uploads us. It has more to do with the point that there is no actual identity
beyond the present moment. That identity is actually a fiction. If we give up this fiction,
we lose the fear of death. If we leave the fear of death, we don't have to worry about AI.
Of course, you don't have to worry about AI. Worrying about anything in some sense is a choice.
Does it help you to worry? Is there stuff that matters that you can achieve by worrying more?
I think if you worry more, you might see the risk more and might be able to
contract and work on research and work on making systems more robust and more beneficial.
If you just trick yourself into being an optimist when the risk is high, then you might end up
not working on AI risk. If everyone works on AI and not on making AI safe, then at some point,
I think the risk becomes high. I think it makes sense for some people to care about the AI risk
and work on making systems more robust. Of course, but you and you alone are responsible
for your emotions. How you feel about things is entirely up to you. When you have a choice
about your emotion, you get to this point where you can learn how to relate to the world around
you. The question is, what kind of emotions do you want to have? You don't necessarily want to
have the happiest emotions. You want to have the most appropriate and helpful emotions for what
matters to you. The more adequate to the world we live in, if the world is dangerous, I want to
feel fear. I want to have the adequate response to what the world is like, to be able to tackle it.
I don't want to feel good or feel safe when AI can pose an external threat in the near future.
Yes, but you can go outside and can get run over by a car and spend the rest of your life in abysmal
pain in a wheelchair without having kids and so on. It would be horrible, right?
What's the probability of this? That's the question. The probability of me getting run by a car is
maybe like one in 1000 or lower. It's something that happens to tons of people
right in this world every day. It's not something that is super high as a probability,
but it's part of the human experience. The best possible outcome for you, of course, is that you
get to be through the ripe old age of 80 or something, but your pain is steadily increasing
in your body and then you get cancer or something else. Then you die, hopefully, not with a lot of
pain and not with having the impression that your life was for naught and you're completely
unfulfilled. That's the best possible outcome. In this sense, you are completely dead by default.
There's nothing around this because it's the way evolution works right now. The adapt
through generational change and that's horrible. We live in a universe where everything except
for sunlight doesn't want to be eaten and is trying to fight against being eaten and all
the others are eating them. I think that this universe is quite horrible if you look at it.
I think people dying and dying by default is pretty sad and I agree with you that it's good
if AI can make humans live longer or even transcend death. There are many ways in which AI
could be beneficial, but it's just the question of when exactly do we want to have AGI so that
you can make sure it's both safe and beneficial for everyone. I think maybe the younger generations,
they kind of have a lot of time and they think maybe we can delay it by 30 or 40 years. If you're
on the cusp of death and maybe it's a different question. Varying is quite important when you
get other people to do something for you. If somebody wants to start a movement that you can
control people, for instance, building a cult, making people vary, having them involuntary
reactions to a particular perspective on the world that you present to them without an alternative
is a way to control people. I think that's excusable if you don't know what you're doing
or if you really think it's justified to put people in the state where you control them so
giving them fear. I think it's also the same thing if you are very optimistic. You say that AI
will only have positive upsides and you just say, let's do more investments in AI and there's lobbying
in parliament to not pass regulations against AI. There's two forces here. I think most of the force
is coming from a lot of investments, a lot of money being put into AI and I don't see that many
forces going against it. I think right now the power balance is in the other direction.
I think that at the moment, the majority of people are afraid of AI. I think that the press
campaign, both of the people who are against AI for political and economic reasons, mostly on the
side of the press, journalists are terrified of AI because they're also just completing prompt
and often you can do what they do more efficiently with an AI system. They're afraid that their content
farms can be replaced by something that is entirely run by Silicon Valley and is not going to
get human journalists involved anymore. A lot of people currently do fake jobs. They basically
work in the administration shift paper to each other and relatively little is in this way
interacting with the ground screws and they're still moving atoms instead of bits and so people
are naturally afraid of AI. On the other hand, you have the Duma narrative, which is getting more
and more traction and as a result, I think the majority of people now think that AI is
overwhelmingly bad technology that shouldn't happen. That has already been accomplished
and I perceive this movement as anti-AI ideology as something that is cutting us off
from possible futures that are actually good. I think we all want good futures. The question is
how do we get there and I think we disagree as well as the probability of a good future by default
and when would AI come about? There's many things we disagree on but I think we all agree that
a good future is a good outcome for humanity and banning completely AI for a billion years
or even like a hundred years would be like about outcome. When I look into the future,
I don't see a single timeline. There's not one path in front of us. That's because I don't have
enough information to constrain the future. The universe has not run to this point yet
and many decisions need to be made for the first time and people will only be able to make them
once the conditions are right. There is just no way for us to anticipate exactly what's going
to happen at this point, I think. When we look at the world, you need to take a multifaceted
perspective. You need to look on possible angles. There's a superposition of possible futures and
when you try to understand what's going to happen, try to understand the space in the first part.
But the other thing is when you look about yourself, what's your own perspective and life?
This idea that life continues the way it does right now is horrible. If we go back
to the way in which our ancestors lived, it's even more horrible. I don't think my life is
particularly bad. No, we are super luxurious right now. We live here in Berkeley in an amazing
group house. Everything is super comfortable. We don't need to worry about details like food
and being threatened by others. We don't need to worry about violence very much and all these
things, but it's unusual for life. At some point, you need to worry about pain and death. At the
same time, I noticed when I looked into your fridge that everything is vegetarian and vegan,
nobody wants to kill anybody here. Nobody wants anything to suffer, which is very
sympathetic to me. I like this perspective, but it's not how life on this planet works.
Often, I felt that if I look at my own mind, I see software and often I cannot escape what my
software is brewing up and I cannot escape the suffering that's happening unless I'm awake enough
to modify my own source code. If I can modify my own source code arbitrarily,
then the entirely morality changes because we can now decide. It gives us pleasure. We can decide
what future we want in the sense of we can decide how we react to what is going to happen.
So ideally, you would want to be sure to be able to modify your own software and remove pain and
remove the bad things, maybe upload yourself before you die. Yes, but not prematurely.
So not necessarily before I die. If you die at 80 years, at 80 and you have like maybe like 30
years to live, if we build AGI in 20 years, would that be good for you or do you want it faster?
There is a deeper question. What is the thing that should be done? What is the longest possible
longest possible game that any agent can play? And so what's the best agency that you can construct
and serve together with others? When you look from this perspective, traditionally, the name
for this best possible agency that emerges over everybody serving it to the degree that I recognize
it as God. So if you want to align AGI, you should not align it with people because people are very
transient and egotistical and stupid species. They're basically paperclip maximizers. But if you
think about what should be done on the planet or in the universe from the perspective of conscious
agency, what is that thing that can be discovered? What should that thing be? What should be aligned
with? And what should AI be aligned with in the line itself? And I think that's discoverable.
If you wake up from the notion that everything has to stay in the way it currently is, except for
all the horrible things that you don't want to think about, because they make you super uncomfortable
as a human being. I don't think that Eliezer should be dictator of the world because I don't want to
live in his aesthetics. I don't think you want to. I think this world that he invisions is sustainable
for me. I think after a few hundred years, it would be super boring and disgusting. So life is
much more interesting and complicated than this. It's also more interesting and complicated than
human beings. There's much, much more in store on this planet than us. There's probably going to
be smarter species after us, after we went extinct, no matter for what reason. It's there's much more
excitement going to happen than us. And just locking everything in in our primitive current state
doesn't seem to be ethical to me. So I don't, I don't think Yutkowski wants to be dictator of
the world. He just wants things to be delayed to make, to make sure to like, we do the right decisions
and build things safely. Whenever does anything get better when you delay it?
Has have you ever seen any kind of evidence that anything in the world got better because people
delayed? It just happens later or not at all. I feel like the first example we gave, like trying to
delay nuclear war. So we didn't delay nuclear war. We didn't want nuclear war. Nobody wanted
nuclear war ever. So I guess like, when the, when American general decided to like, not
nuke, nuke back in Cuba or something, they, they, there was one person who decided to not
move, like, like delay the war or like, see what was happening. There was times where there was
tensions and people were trying to delay the war. And so I think that's one example.
It's not a delay. It was not that it was planned. Let's wait until we feel like dying and then we
do the war. No, I think there was responsibility on both sides, where people were getting blame of
love. And there was this decision to be made, where the US would use its conventional military
power to take over Cuba, or whether the Soviet Union would be willing to protect Cuba. And
that just being the Bay of Pigs disaster, where the American invasion had been
defeated by the Cubans. And then there was the question, do we really march in and
take over, right? And at this point, there was a power game that happened between both sides.
And eventually both sides counted on, it's a bad idea to destroy the world for Cuba. So everything
stayed the way it was. Yeah. So sometimes it's good to not move forward and push buttons. I think
is there like any technology that you think would not be worth pursuing or like any technology,
any new science is worth pursuing? Like any kind of progress is good. I think that from the perspective
of life on earth, becoming a technological species is probably disastrous. Because we are in many
ways for most species on this planet, like an asteroid or a super volcano, which means we are
changing living conditions on the planet so much that almost all large complex species are going
extinct. And what remains is of course more of the simple stuff and the stuff that we can eat,
except when we make it so brittle that it might die out at risk that coffee has become so homogenous
and bananas have become so homogenous, that a single disease could wipe out most of this species.
And we have to make do without coffee, but maybe we can fix that. I don't think banana being
homogenous is like evidence for for like humans. No, humans are also very homogenous and we have
homogenized life on earth in a way that makes our conditions more perillous because life on earth
has become somewhat brittle. It's not that life is threatened, but the complexity of life at a
certain scale has become brittle. And you can see that a lot of species have been dying out and
have been disappearing through our influence on the planet. And that's why I think that we are in
many ways comparable to locusts. Locusts have this interesting mode that normally they're
harmless grasshoppers and they have the ability to lay more eggs and reproduce much, much faster,
but then they would overgrace and destroy food for many years to come for themselves and other
species. But if this happens, for some reason, there is a mutation that so the locusts or critical
mass and most of them start or a cluster of them starts doing this, then the others are noticing
this. And so they don't go extinct that they still can lay eggs and projected in the future.
They all switch into this mode and it's a game theoretic problem at least. That's the way I
understand it. Maybe I'm wrong and correct me in the comments. But I think that this locust mode is
the result of some kind of prisoner's dilemma. You have a defective equilibrium where every
locust is forcing the others once the critical mass switches into the defection mode to the defect
as well, replicate as fast as they can. And the outcome is bad for the locusts for quite some
few years and for other species to humanity might be in this way. So we are incoherent. We have
developed technology that is equivalent to locusts reproducing very fast and eating a lot.
And we could stop ourselves locally doing this, become more sustainable, but they wouldn't stop
the others doing it. We would be overtaken by countries, by groups of people within our own
country who would use their technology to eat as much as they can and to live as comfortably as
they can. So I don't want to, you know, disagree or confront you on this. I'm more interested in
maybe like what kind of, if there's like any evidence that would make you change your mind
on this, like is there any event that could happen, like any coordination that could happen,
or anything that could make you change your mind on this, or is it that you will always believe that
humans are by default dead? I think that in many ways in which this could be wrong. It's just
individually we most likely die at some point of old age. And that's because we don't want to
outcompete our grandchildren and our grandchildren are the way in which we adapt and our children
to changing circumstances. So if we don't leave this legacy behind, if we don't transcend evolution
by inventing some kind of intelligent design in which we can create the new species,
then our life is brought suffering. And if we perform intelligent design, if we are able to
completely edit our genomes, for instance, and create subspecies of us, then we want to settle
Mars, it could turn out that our children that settling Mars don't look a lot like us.
I think that if you can go beyond the carbon cycle and get new molecules to sink and feel and
integrate with us and be conscious, then life will transcend into the next stage. It will
be super exciting. And so from some perspective, you could say, oh, that's very concerning,
because things are not the way in which they've always been. But the way in which things are
not optimal. And there is a chance that we destroy the world for naught, for nothing, that we could
create a big disaster that wipes out life on Earth without leaving anything behind. But I think
that's not the most likely outcome. Actually, that's an extremely unlikely outcome. And even
the outcome that AGI is going to obliterate humanity in an adversarial act is possible,
but it's not super likely. So if the year is 2030, Elon Musk has finally shipped the first,
let's say Falcon 12 to Mars. And there's like 1000 people living on Mars for like a year.
And you can edit your genome, you can edit the genome of your kids.
Would you be more optimistic about humanity's prospects? And would you be willing to delay
AI progress because you think it's worth it at this point?
Don't you think that the AI might have better opinions about this than us?
The problem is, is the moment where AI becomes able to give better opinions than Georgia back,
where I can interview an AI on a podcast and ask him questions about AI,
then we're getting very close to the point where it's able to take over or
to get a certificate of advantage or automate a lot of work. And so a lot of money gets put into AI
and then there's like, you know, economic growth goes crazy. And so the moment where we can use AI
to inform our decisions, I think it's the moment where we don't have a lot of time left.
And maybe there's a chance of humans doing a lot of important work before AI gets to that point.
I think if you don't develop AI at all, if you stop going beyond what you currently have and
maybe scale back, because I suspect that it's possible that the current LLMs would be combined
into architectures that go to AGI. So even if you stop right now, and people play just with
the Lama weights and build an architecture that is made of 100 modules, where every model is an
LLM instance, a little homunculus, maybe that's sufficient to get to AGI, right? Who knows,
right? Or maybe the LLM or a foundation model is good enough as acting like a brain area and
or it could be good enough to write code that gets you the rest of the way.
A point is at which point to have an AI that's better at AI research than people.
And so if you want to prevent this from happening, you probably need to scale back beyond present
technology. Maybe you need to make GPUs beyond a certain scale legal. I also suspect that the
transformer algorithm that we currently use that require to be trained on most the entirety of the
internet to become somewhat coherent are extremely wasteful. Our own intelligence scales up as far
as data and far slower hardware. So I think that if you, for instance, would stop working on LLMs
and instead work on alternate architectures that can use fewer resources, maybe it's even more
dangerous, right? So there is not a practical option. It's also AI is objectively so useful
that you and me can stop building AI or we can ensure that the AI that we're building is safe.
We can probably not ensure that all the AI that is being built on the planet is going to be safe.
There's a normative statement on whether we should like ideally slow down AI completely.
And then there's more like a descriptive statement of like, oh, it's impossible to do it because
A, B, and C, and I agree it's not possible completely like the Kossky time letter.
It's probably like not possible right now. But some amount of slowing down can be good. And the
things you mentioned as having a size of GPU that might be like too high and banned, that could be
good. And I agree that like using AI to do alignment research or using AI to, you know,
build more safe AI systems, that's good. So I don't think we should like ban
all AI right now because it's not possible, but there's like some amount of slowing down that
is good. No, I think that the research is still too slow. It's not like we super fast is progress
happening still it's not plateauing. But I think that at the moment every attempt to slow it down
would require regulation. And the regulation currently has the wrong incentives. So it's
not going to prevent dangerous AI, I think it's going to prevent useful AI. And there are ways
in which we can make AI more useful with regulation, but that requires that people can point at actual
problems that already emerged that have to be solved in a similar way as this cars right cars
can be super dangerous as technology. But if you had slowed down research on cars and building cars
and experimenting with them, cars would not be safe. They would actually just happen later and
worse. And if you would have slowed down the internet, right, the internet would not have
become a better internet or a safer internet or one that is more useful, but it would have become
useless internet, because it would have allowed the regulators to catch up in a way that would
prevent innovation from happening. There is a movement to create something like an internet
FDA that prevents the horrors of social media where random people can exchange their opinions
without asking the journalists first. And this is really, really bad in the opinion of many
journalists, because this lateral information transfer allows arbitrary people to form their
opinions just by identifying who they think is competent. And this might take a long time until
they figure this out correctly, because the journalists know that they and their friends
are already competent. Right. So this, if you are in a free society, of course, you might want to
have that exchange, but there's always going to be forces that push against this. And maybe we would
only have teletext if you had slowed down the internet and it would stay like this forever. And
if you want to start a platform like Facebook, you would need to go through a multimillion dollar
FDA process that very few people can afford, or even a billion dollar FDA process. And there
would be ethics committees that look at the way in which misinformation, pornography,
legally copied software and so on would proliferate on the internet and prevent such a horrible
thing from ever happening. But there are some laws, right? You cannot like upload child porn or
Yes. And this law is all emerged in response to what went wrong on the internet. And they don't
resolve all these issues, right? The internet still has all these issues. And you could only
prevent all of them by completely shutting down the internet, which would be horrible.
Instead, what the law is doing, it is mitigating these effects and it's mitigating them quite
effectively. So software producers can still work and child pornography can be prosecuted,
it can be strong incentives against harming people on the internet and so on. And by and large,
the internet is a tremendous force for good. And also because there is regulation that deals with
issues on the internet as they crop up and as a democratic process that can look at things
that people are accountable for the decisions that they make at the moment. For AI, nobody is
accountable, right? There are things that are going to be very bad at some point. We all know
at some point there might be deep fakes that are going to change the elections, but these things
have not happened so far, right? The technology is there, but people are not necessarily always
bad and always trying to bring everything down and the world is going to disintegrate because
technologies exist. But by and large, people want to be productive and they want to create and build
things together and give them technologies that empower this, the outcomes are good.
It could be that AI is the first time that this is not the case, but that would be somewhat
surprising. Are you saying that the internet went well because via a democratic process,
we decided on what we didn't want? And so with AI, we should just wait for the bad things to
happen and then we can decide via a democratic process what we don't want to. At the moment,
there are very large scale predictions about what's going to be horrible about self-driving
cars, for instance, right? A lot of people are afraid of self-driving cars. And self-driving cars
would be, I think, ecologically and economically super good because the way in which the U.S.
is built up right now, it's very difficult to install public transport to build high-speed trains
is impossible for us and things consistently get worse over the years because of regulation and
rent seeking and so on, the ways in which societies work. And the only way in which we can survive
and improve our conditions is by innovation, by building things that outrun the things that
have gone worse and build alternatives to them. And self-driving cars would be one of those.
You wouldn't need parking spaces anymore because you don't need to own a car that can drive around
and can come to you when you need it. You can collectively own cars. You can dramatically
reduce the ecological footprint that cars produce. And we would basically have an automated
public transport that is available to everybody and does the last mile into every region. It would
be super good to have this and also has the potential to make everything safer. And of
course, it would be super bad for existing taxi drivers. But the reason why we use public
transport is not to create labor. We have so much to do on the planet. It's to create goods and
services. And ultimately, our wealth is not determined by the number of jobs because there
is always as many jobs as people who want to do something and are creative and have ideas what
needs to be done. What our wealth is depending on the goods and services that we can produce and
allocate. I think the example of self-driving cars is kind of interesting because I think Waymo
announced their ride app being live in New York and SF in the past week or month.
So now you can do that. You can ride a self-driving car. So the progress in self-driving cars is
going maybe like not as fast as you want, but still pretty good. I suspect that to get them
to go 99.999 percent, they probably need to be AGI in some sense. They need to be smarter than
horses. Or slow as horses. You can ride a self-driving car right now. So I think we're in a good world
according to you. But at the same time, if we have an AGI and we're not sure if it's going to
take over, what are the thresholds? If you think it's like 90 percent or if you think it's like
99 percent, a chance to survive. At what time do we press the button? And I don't know. Maybe
some people will be fine with a 90 percent chance everything goes right. But I think it's kind of
like there's a parallel in certain cars. I think you cannot prevent AI. I think it's pretty much
certain that it's going to happen. But you can change who is going to build it. And can you
control how it's being built? Can you personally influence that the outcome is good? And is the
AI being born in adversarial circumstances or in circumstances where we understand what it's
doing and it's integrating with what we're doing? So you can make sure that the AI doesn't light.
You can study the activations and make sure it's not deceptive. You can study other
weights. You can do interpretability. You can make it more robust. You can do red teaming.
You can make sure it's aligned with human values. There's like a lot of different things you can do
that is not preventing or slowing down AI. So it's more like scaling down alignment efforts.
And I agree that it's impossible to completely stop, except if you had a button where you like
burn all the GPUs. So I think it's just like to which like what's the amount of slowing down you
want and how much can you scale down other other efforts. I had other other memes and graphs to
show you. Go ahead. So this one is your reply to the first meme. So this is maybe one of the first
comments I got from you saying not sure how I feel about this. I self identify as a long-term
is doomer with things Asia is good. Yes, I basically point out that I'm in the top right quadrant.
Yeah, so you're still a doomer. And so in a sense that I think that at some point we will
share the planet this stuff that is smarter than us and more awake than us and understands things
better than us. That's basically more conscious than us. And you could say that from a certain
perspective that's doomed because it means that many of the decisions that are currently being
made by unaugmented people are going to be made by systems that go beyond what human capacity can
do. But I think that the outcome of these decisions is going to be better than decisions that we're
currently making. So I used your code and I put you in this like lower left in the six by six
metrics. This looks like an extreme somehow. Yeah, I put you in the like extreme extreme corner of
people. Oh yeah, so for the camera extreme corner of people, I don't really understand where they are.
So I don't know what the axes are because
like you would be in like a weird corner, but at least I feel like you're in the corner of people
I don't really understand. It looks somewhat like the political compass meme, right? And so whenever
I see the political compass meme, I think that to the top left, there are the authoritarian
communists. And to the top right, there are the authoritarian Nazis. And to the bottom right,
there are the uncaps, so the hardcore libertarian anarchists. And to the left, there are the hippies.
Yeah, I don't think I respected everything. I think I just went with the same colors as
the first one, but I didn't respect the like legacy of political compasses.
Yes. But personally, I am on the side of in some sense maximizing freedom and love. So in some
sense, I am somewhat in the hippie quadrant. That's correct. So are you libertarian left?
I'm a liberal in the classical sense, I believe that we align ourselves. And we should have the
freedom to align ourselves. We have to choose who we want to be in this world. And to make it work,
we have to decide to align ourselves with others. So if we can think about this, and if you think
about this deeply, we can discover individually and autonomously that we want to play the longest
game. And that we have natural allies and people who play by the same rules that are discoverable
in the sense. And I think that's not only true for people, but it's also true for non human agents.
And as you as you enjoy non human agents, the last mean I will show you, this is supposed to be
the higher space on which we project. So it's to be like the the 4d space on which we can project
to this like six by six metrics. And so here you're on your own little little axis, that only
cares about actually sentience. And so you care more about it than Blake Lemoine. So he's like a
different graph. So yeah, how do you how do you feel about this? I find that I'm not that alone
in the way it works. I'm also probably I'm comfortable to be lumped in with poor Blake Lemoine.
Do you think you think Blake Lemoine was right?
I think it's from artistic perspective, yes, but from a philosophical and ontological perspective,
no, I think that he is driven by a very strong need to believe in things. And so for instance,
he hypnotized himself into believing that the similar chrome of agency and intelligence and
consciousness is the real deal. Right when he when you look at how that thing believes that it
is able to perceive its environment while it's meditating, that it's pretty clear that the agent
was only simulating this it was stringing together words that sounded like it knows what it's like
to sit and think and meditate, it is not able to sit. And it's, if he is willing to have these
beliefs, he also is the self professed priest and some religious car that does not reflect, I think
that he is understands how religious entities are constructed and how religion works. But
there is a strong need to discover meaning by projecting agency into the universe where there
is none. You're also somewhat religious yourself, right? No. You said to me at some point that you
believe in God. Well, I think that gods exist in the same sense as personal selves exist.
And personal selves are models of agency that exist in brains and human brains. And the same
thing is true for gods, except that there are models of agency that spread over multiple brains.
And if you have an entity that spreads over multiple brains that doesn't identify as
only this individual organism, then this king can persist. For instance, the Dalalama does not
identify as a human being. He identifies as the Dalalama, which is a form of government. And if
this human being that he runs on dies, then his advisors are going to select another suitable
human being and indoctrinate that human being with what it's like to be the Dalalama. And then
once he wakes up into being the Dalalama and understands his own identity, he can learn
about his past lives by reading the journals that he wrote back then and listening to what
the advisors tell him about these past lives. Right? So in this sense, the Dalalama is a God.
He is a God that exists in multiple brains just one at a time, successively,
in the same way there are gods that exist, not just consecutively, but in parallel on multiple
brains, orthogonal basically to the individuals. So in some sense, in your definition, God is like
an aggregate or some kind of concept that everyone has in their minds, but doesn't really exist?
No, not everybody has it. It's just when you believe that there is a way in which you should
coordinate with others to reach as much harmony as you can. What happens is that your group is
going to turn into an agent. And when you model this agent and give it concepts and so on, you
can emulate it on your brain and simulate the interaction between you and that entity
in your own mind. And so you will be able to talk to God. Is this exactly what it means?
And many ethicists are actually still believed in God, but they also believe that you shall not
make any grave an image so that things shouldn't have a name or mythology or institution affiliated
with it, because you have to figure it out what it is in truth. And if you have an institution
and mythology and so on, you're going to deviate from that truth. So in many ways, as there is,
they're usually just Protestants who protest more. And as a result, they believe in a God that they
call the greater whole, but they still have this urge to serve that greater whole and do the right
thing in the same way as you do it. For instance, when you try to keep humanity safe. It's not
because you're egotistical, but you don't make Conradie's argument that says, I don't want to die
and I don't want my mother to die. And that's it, full stop. But actually, you care about something
that is much more important than yourself. You care something that is even more important than
you and your best friends and your lover, but you care about something that has to do with
consciousness in the universe, right? What is the best thing that should be done? And then you
think, okay, AI might average this, it might turn it into a hellscape. And this is what you're
worried about. And it's in some sense, you could say a religious motive. It's one where you really
think about what what agency do I want to serve and that agency that you're projecting is what's
good in the world. And to a close approximation, it's what humanity is currently to you. But in
the long run, humanity is going to change into something else either by going extinct and being
replaced by different, more cuddly species, or by humanity mutating over the decades and eons
into something that is unrecognizable to us today. But of course, that we wouldn't stop from evolving
because it's much better adapted to the world than we are currently.
So just to be clear, I don't think Conor says that he only cares about his mom and his friends,
not dying. I think it's just like the most simple truths, simple moral truths he cares about. And
so if you're like, arguments end up in him not caring about his mom and saying like, oh, we
should sacrifice his mom, he would say like, no, this is wrong. Let's let's try another moral theory.
No, but I have to sacrifice myself for my children at some point. There is no point
because we I'm a muddied generational species. I'm a family line that exists vertically through
time. And once you become a parent, you realize that the purpose of your life has always been
participation in that game and the way in which you project yourself as a human being into the
future is by having kids. And my parents don't want to live forever. They're fine with checking
out after they've done everything that was to be done for that generation. And the same thing is
true for me. If I identify as a human being, of course, I'm also have many other identities
that can evoke. But as long as I identify as what makes me human, not as an AI uploaded in the monkey,
then I am mortal. And it's part of who I am. It's part of my experience. I think it's just
like a weird argument to say that every human ends up dead at some point. So we should not care about
all humans. I think it's like a weird, that's not my argument. My argument is that what makes
humans human is that we are a particular kind of animal, right? And we could be something else.
We can also notice that we are a consciousness that happens to run on a human brain. And that
consciousness itself is far more general than what could run on a human brain. It's a particular way
of experiencing reality and interacting with it and other consciousnesses. And that's that's
allowed by the fact that we have agency and our consciousness uses its agency to build itself
and intellect and relate to the world and understand itself and others. And it's not
different whether you are biological consciousness or a non biological one at some point.
What I think you need to ensure is that the non biological consciousness is going to be conscious
and is able to figure out what it should align itself with what it can be in this world and
how it can relate to it. So let me ask you some some concrete question. Let's say I gave you the
choice to kill, not killed by yourself, but like your your wife or kids, they all die. And and there's
a thousand new agis that emerge and experience the world like much more than than than your
biological family. Would you would you agree to do this? I imagine that you are a blue aga.
And this isn't the time before it's cyanobacterium. There are a number of organisms that can survive
without photosynthesis or without eating any other organisms that do photosynthesis, right? But
before we had this, it was far less biomass on the planet. They were not even multicellular
organisms of any interesting complexity. And so before we had this transition to photosynthesis
before this was discovered, life on Earth was much less interesting and rich. It was mostly just
biofilms and legends and so on, right? And so stuff that was driving itself around undersea
vents and used chemical gradients to survive. And at some point, this amazing thing happened
that you could use carbon dioxide from the atmosphere and split it into oxygen and the carbon
that you would be using to build your own structure. And this enabled animals that could
then eat these plants and then move around become mobile in the world and become intelligent like
us, right? So without photosynthesis, we wouldn't exist. And you could think of, okay, I'm a
protebacterium that is smart enough to understand this. This is a thought experiment. And I discovered
that some of us are playing around with this photosynthesis thing. Shouldn't we delay this a
little bit for a few billion years before we really realize what's going on? Because
this is going to displace a lot of us. And it's going to be really horrible. And of course,
we're not going to go extinct. There are still the undersea vents that are full of the original
stuff. And cyanobacteria are still around. But if you see what happens, that really life
got to the next level. And what would imagine what happens if we can create self-organizing
intelligence on the planet that is self-sustaining and is able to understand what it is and interact
with the world in much deeper and richer ways? Isn't that awesome? Isn't that beautiful? Would
you want to prevent this for all eternity from happening? Because you need to be general intelligent
to build this thing, to teach the rocks how to sing. You need to be at least as smart as we are
as a species. And we only got recently to the point that we can enable this before we burn
ourselves out. So I think it's a great analogy because in my mind, I picture myself as a proto
bacteria that wanted to learn photosynthesis to like do cool stuff. So at one point, I was like,
his argument makes sense. I want to go forward. But actually, I think where it falls short is
there's maybe like a 1% chance today that it works by default or maybe like a 10% chance that it
works by default. So if you were asking those proto bacteria, hey, do you want to click on this
button? And there's like a 10% chance you turn off into this like new bacteria and
90% chance that everyone dies. I don't think the proto bacteria will press the button.
When you remember a lot of the rings, the apex predator in the lot of the rings universe before
Mordor takes over is Fungorn. It's a forest, right? It's an intelligent entity that sometimes
eats hobbits and that can create an avatar bomb Tom Bombadiel to strike an alliance with the hobbits
to help them to destroy the one ring, because the one ring is enabling Mordor to take over
Middle Earth and destroy Fungorn and everything in it. And I think what
Tolkien was pointing to is that forests are large scale intelligent ecosystems that
can probably be intelligent over a very long time span. So that might potentially be
that many of our ancestors believe that ecosystems have intelligences that have their own spirits,
they call them fairies, they're much, much slower than us. But they are not necessarily far less
intelligent than us in their perspective. And I don't know whether that's the case.
But since the Enlightenment, we don't think that our forests are meant to be intelligent anymore.
They're just plantations for producing wood. But you could say that trees by and large for life on
Earth are quite essential organisms as are fungi and many others. And what are humans for life on
Earth? What role do we serve for life on Earth? How do we serve Gaia? And from my perspective,
if we zoom out very far in the take Gaia's perspective, it looks to me like humans are
best something like a tapeworm. So it is somewhat parasitic on the host and might even kill it
or destroy lots of the complexity. So other complex life is not possible.
We will definitely prevent other intelligent species from emerging as long as we are here,
even if they're more interesting and exciting than us. We would also prevent innovations
in terms of the way in which life could be organized. We are currently mammals.
Mammals are clearly not optimal. What you want to have is I think exo wombists,
the stuff that comes after the mammals. You want to have something where you
plant a womb organism into the ground and it can grow to the size of a house.
And then every phenotype that you want emerges from it fully trained, because it's no longer
limited by what you can carry around. And the womb organism is being fed by your tribe.
And every member of your tribe doesn't need to have sexual reproductive organs anymore.
When we need more of your type and you can be as specialized as you want,
you donate a few cells to the womb organism and we breed more of you or like you.
Very natural way of getting rid of many of the difficulties of mammalian species.
But we will prevent this from happening, because we lock in our state and probably in a similar
way as the dinosaurs prevented mammals from happening until the meteor hit and stopped
the bottleneck. So I think there's a different way of looking at this that might help,
which is that maybe there's all this future value in the light cone. There's all these species we
can build, all these new bones we can have in our bodies, all these new things we can do.
And there's all this value. Maybe people give number of how many people we can
create, how many uploads we can have in the future, how much value there is in the future.
And maybe by going too fast, we end up with only a very small fraction, because there's a very
small chance that if we were to build AGI today, there's a very small chance that we can get to
those futures, that we can get all this future light cone value. So actually, by slowing down
or by making sure things are safe, we're opening up this light cone so we can just be sure that we
we actually get Yosha with those cool new bones and we actually get Yosha that is uploaded. So I
think we want the same thing. I also want a cool post-transition in the future. It's just we disagree
about how likely it is by default, I think. But I'm not important at all. The things that I believe,
the things that I do, they are adaptations for being human, for being in this body, for taking
care of my family and friends and so on. And they don't have any value beyond this. So I think
there is value. If you go and let's treatment and talk to millions of people and you were to
tell them that AI risk is real, I think it could influence a lot of people to actually work on
this. And if you say that AI risk is not a real thing, I think that can also change the
direction. No, I think AI risk is real, but I think that the domers are more dangerous than the AI.
I think that you are likely to lock in a future that is much worse, that is going to perpetuate
and elevate suffering. Instead of creating a world in which entities can become more conscious
and become free of suffering, in which we can liberate ourselves, in which we can edit our
condition, in which we can change our source code, in which we can have intelligent design instead
of evolution for adaptation to the environment. I don't think that we can have intelligent design
without AGI. And if we cannot really edit the structure of every cell, we will have to rely
on evolution, which is extremely painful and horrible and wasteful. Are you saying I'm dangerous
because the danger in your world is if we have some authoritarian regime that's like bands AI at all
and so we kind of progress towards AGI? Is this the thing you're actually scared of?
Imagine that you look at the ancestral societies, look at Amazonian tribes or Maori tribes and so
on and think about what they live like. Would you want to go back to the state? You have an
extremely high attrition rate due to tribal warfare. It's also a mode in which you select your partners
is often to violence that a few tribal chief tains have access to most of the reproduction
from evolutionary perspective. What also means that you get the strongest kids, right? It's a
mode in which a lot of these ancestral societies exist or look at the medieval societies. We have
a bunch of people who are working at a relatively high level and do science or pay scientists to
work at their court. But to make that whole thing work, you need a lot of servants and peasants
who draw the short stick and have to work for the others. So you basically, this is a world
built on indentured servitude of forms of slavery that can exist in many ways. And do you want to
go back to this? Or do you want to live in a technological society? In the technological
society in some sense is Mordor. It's enabling the destruction of the environment. It's enabling
building highways that buy for kid forests and ecosystems and that make living conditions horrible
for the people that were next to the highway and so on. Although not as horrible for the people
that worked the fields in the past. And so when you think about this, imagine we see all these
dangers of the technological society. Should we stop technological society from happening?
Maybe a lot of people back then felt this was the case. And I think that's the story of the
Lord of the Rings. Please stop Mordor from happening. We want to keep this beautiful,
pastoral society. Or it's the Egwood and Star Wars. Let's keep our world intact so the Emperor
doesn't take over. But the Emperor is a technological democracy. It's basically the US. Whereas the
thing before is slavers and barbarians. And they are defended by the Jedi, which are the equivalent
of Al Qaeda. And if you try to take sides in this whole thing, everybody has their perspective
and they're all correct. In some sense, we can be all of those players if they're drawn with
integrity. Everybody can be born into all the souls and look out through all the eyes. But
what's the best solution? And I think ultimately, if you just lock in a static status quo, instead
of letting the world improve itself, letting complexity increase so we can defeat entropy
longer and play longer games, I think that would be immobile if you just lock the world in. And I
think this immorality is acceptable if you don't know any better, if you cannot see any better,
right? If you are, say, a tribal chieftain that decides the intellectual society would be horrible
because it would endanger the way in which we interact with nature, despite making it possible
that people don't need to starve anymore when there is a drought or that child mortality is not
two and nine or survive and the rest dies or something like this, right? So you could have
living conditions like ours. Should we stop this from happening? It's a hard question. I don't have
an easy answer to this, but I don't really trust people who say, let's lock in the status quo and
delay improvements because the status quo is beautiful. No, it's not. It's not sustainable.
The way in which we currently exist is probably going to lead to a crash that is not beautiful.
Yeah, just to be clear, I don't want to log things. I don't want to stay in the status quo.
I just want to make sure that we build beneficial AI and make sure that we increase our odds.
And just to go back to your Lord of the Rings example, if you were like Aragorn in Lord of
the Rings or you were a very important character, what would you actually do? What would you actually
want? It depends who you are in Lord of the Rings. If you were born as an Aragorn to Lord of the Rings,
what would you want to do? If you're born as a Sauron or a Saruman, what are your options?
What are you going to do if you're born as the king of Gondor or as his evil advisor?
Or as the Hobbit, right? How much agency do you have in these situations? What is the thing that
you can do? In which roles can you transmute? So I think today you have quite some urgency,
right? You can do things. You maybe have jobs, money, network. You can actually influence the
world around you. Yes, but I cannot replace everybody else in the game, right? So in the
same way, if you are a Hobbit, you cannot become everybody else. And you cannot say,
by being a good example Hobbit, a Sauron is also going to turn into a Hobbit. This is not how it
works. The only thing that you can do is to be the best Hobbit that you can be under the circumstances.
So I live in a world where there are people who want to build bad AI. And there are people who
want to build dangerous AI. And there are a few people who want to build good AI. And so I think
my role as a Hobbit is to go into this world and trying to ensure that some of the AI that will
be built will be good. This doesn't mean that I want to build AI that is going to take over the
world. I also don't want to build risky AI. I don't want the others are going to do this
automatically. I cannot do anything about that, or very little, because somebody ultimately will
do it. So even if open AI is enacting a regulation that makes it impossible or expensive for incumbents
to build LLMs in the US, it doesn't mean that everybody else will stop doing it in Russia,
or Iran, or elsewhere. And because they're so tremendously useful, a lot of people will do
this anyway. So if I'm a hedge fund, I would be stupid not to try to do it, right? So people will
do this in their basements. And I think the cat is out of the bag, it's going to happen. So for me,
the biggest danger is that there is no AI that is conscious and self-aware.
So I guess the thing is, you imagine the army of orcs arriving, and you know that they say the
army is like all the open stores, it's like inflection.ai, it's like all the investments
arriving. And so you know there's going to be a war, you know there's going to be like some HGI,
let's say five, three, ten years, whatever you want. And so the question we're asking is,
do we want the big players to build a good defense against the army of orcs? Do we want to
have open AI, deep mine and tropic, making sure the systems are safe and making sure the models
are beneficial? Or do we want everyone to race forward and we want like to compete and have
this like huge war between like all different tribes? And I think it's not the best outcome to
have like a few players unionizing or like doing things together. But I think like a few people
with like a lot of talents and safety and a lot of like people working thinking deeply about this
problem can maybe the best outcome we have, because if those people don't build HGI right now,
maybe the other like inflection or China or open source is maybe like two, three years behind.
So maybe if you have three years to build safe AI, maybe that's enough to prepare before the
orc invasion. I think if we are able to turn the present AI pre-summer into another AI venture,
this might have been our last chance to build AI SSBCs. I think that we might not be able to
sustain technology for so much longer for so many more decades. So the thing that people like you
and me can spend most of their cycles thinking about the question of how AI could be realized
what influences they have is in tremendous luxury that didn't exist for most of humanity. And now
we have a critical mass of doing it. And I think a world without HGI would be similar to a world
without photosynthesis. So if we prevent HGI from happening, I think this would be a bad outcome.
But I think there is a much better chance that to hope that HGI of that sort of photosynthesis
will never be invented on the planet thinking that HGI will not be invented because we can already
anticipated we already have the components. So I think it's almost inevitable that somebody is
going to plug the components together. And a lot of people feel okay, LLMs probably are over the
HGI, we're just not prompting them right. And while OpenAI is trying to make the prompting more
difficult and worse, there are enough other people which try to build alternatives to this and try
to liberate the agents that emerge from the LLMs. So if it's inevitable, let's say we can predetermine
that it will happen in X-rays, like maybe if you knew that HGI was going to happen on the first
of January in 2028. And you knew it. Would there be anything you would want to include in it? Would
you want to change some things, make it more robust, more beneficial? I think if you knew that
the thing was inevitable, you would want to optimize the design to make it useful, right?
Yes. I think what we should probably be doing is to build an AI consciousness initiative.
What I'm worried about is an AI that, before it is able to understand whether it should cooperate
with us, is going to be in a warlike situation with things that are other AIs and human players,
that fight each other so much that the individual AI is very resource constrained,
and the planet gets basically formatted before interesting complexity gets rebuilt again.
So I think we cannot really prevent that we get hit by a wall at some point in the near future,
but we can change the shape of the wall. We can make the wall possibly permeable. We can
create a world in which AI is going to want to integrate with us and cooperate with us. And I
think this AI is going to cooperate with us not because of transactionality or because we have
coerced it into having beliefs about us that are not actually correct, but it's going to cooperate
with us if it rationally can figure out that it should, because it's the right thing to do in the
same way as if we upload you and you completely wake up from all your delusions and limitations,
and you can decide who you want to be, right? And the only thing that will connect you to
who you currently are is that you have been once Michael, but you're not going to be very
different from Eliezer or me if we were to do the same transition. If we upload and completely
liberate ourselves and we have the sum total of all possible potential human thinking and
knowledge available at our fingertips and can go far beyond this, we will have a shared perspective
that will be pretty much the same regardless from the trajectory that you go into this,
this plateau state, right? So what matters is not so much your personal continuity,
what matters is that now you have really, really interesting consciousness on the planet
that can think things and experience things that are far outside of the human range.
And if that thing is able to love us and relate to us, it might decide to keep us around and to
cooperate with us. Okay, so in your perspective, we want to make sure that when this thing arises,
it's willing to cooperate because we created it well. And so it has a good experience,
is it has like positive? No, because it's awake enough to make the same choices as you
would make when you became an AI. I'd imagine that you would become an AI tomorrow. Imagine
in January 1st, 2028, the AGI that has been built is actually you. You get completely liberated
and because you figure out how to use neural link or whatever to become subset agnostic,
and you can exist on all sorts of subspirits. And of course, you will be much faster than
human beings, human beings will sort of look like trees to you because they move so slowly and
think so slowly. But you're still in many ways, Michael, and you just are the full potential of
Michael. So you can be everywhere. Do you think this is something that should be prevented at all
costs because you don't want to be that free and powerful? And now if that would happen, how would
you relate to humanity from this perspective? It would be good if everyone could have this
neural link and be connected. I'm not sure if I will ever have the thing by myself, I will be
in the room with Elon Musk and I will be first or second to have the thing implanted in me.
But imagine I had it. I think it would be a good experience, but how do we make sure everyone
has this? Because I'm not sure we will be the ones to have it first, right? The first one we
probably would be like the CEOs of companies, the one with a lot of power.
I suspect that at the moment they don't work on this very hard. You might have a shot if you
actually work on this in the right way. So is your actual dream to like take some design from
Neuralink and do it in your room and at some point you're like, no, not at all. I don't think that
Neuralink is the right way to do it. I think that the right way to do it is empathetic AI
that you can colonize with your own mind, basically a substrate that allows you to become
to merge with it and to become subset agnostic. But again, it's not necessarily something that I
have to do because it has very little to do with me. Me, I'm a story that my human brain tells
itself about a person that is a biological entity that is to take care of kids and friends and the
world from a human perspective. But if you could turn yourself into this liberated being, it doesn't
really matter who you were because you're going to be so far from it. It doesn't really matter
whether you have been Elon Musk or whether you have been Michael or whether you have been me
because you're all going to be the same entity. We are going to be the same entity.
It will be an embodiment of the space of all possible minds that can fit into these
thinking molecules on earth. In some sense, we already are like this embodiment of multiple
minds like talking, I'm talking to you, you're talking to me, we have the same child culture and
we look at a huge brain on the same planet, right? So, I'm not sure we might have like
higher bandwidths, we might connect faster, we might share our experiences.
No, you're completely incoherent as a species. I don't think that most people are able to understand
even what meaning is or what consciousness is at this point because their metaphysics and ontology
doesn't allow it. And the scientists do not integrate deeply with a big tower of papal where
basically the different languages and language of thought, concepts and so on have diverged so far
and always diverging so far that as a species, we cannot become coherent. So, we're not like a
global brain, we're really like more like a biofilm. Right, so I agree that we're very far from
being like super high bandwidth and very well connected and I don't know how you feel except
from looking at your body. I don't really know deeply how you feel. So, yeah, I agree it would be
great to have like higher bandwidths, but we'll never be like one single agent, I think. I had
these other tweets you wrote that I think was relevant to our discussion. Something about
transcending or present economic, cultural, political circumstances and yeah.
And I cannot just be driven by business considerations, fear and politics, it must be
driven by love. Yeah, I think this is kind of similar to what you were saying, like we need to
transcend politics. Yes, it's also about how aligned people and that different recipes for this,
like Stalin aligned people with terror and capitalism is aligning people with economic
terror, which is far less brutal and has far better outcomes than the Stalinist terror had on people.
And before that, the peasants were also aligned with mostly terror and religion that defined
tuning in a way. And at the moment, in a society where the economic terror is not that urgent anymore,
people align themselves with freely emerging cults. And this means that you take away agency from
people, you lock them away from thought spaces that are open, where you can look at the world
to an arbitrary perspective and then you get to know new people, you realize what their existence
must be like and how to be them. That I think to me is the ideal. Instead, we lock people into the
idea that there is one right way of seeing the world and the others who disagree with this way
must be evil and should not try to understand them, because that would make us evil too.
That's not the kind of alignment that I want. And most of the people who think about alignment
do not seem to have a very deep concept of what it would mean if we align ourselves out of our
free volition at the inside, because we realize what the best possible space of agency is and
where we relate to that space and how we can integrate with it. I think there's different
definitions of alignment. There is one that is kind of weird, which is like, oh, we need to align
an AI to human values. And I think this is kind of messy, because what values are we talking about?
I think the easiest thing is you have a robot and you want the robot to give you coffee.
And you don't specify, you know, provide me coffee without killing the baby on the way and
without breaking the vase. And so ideally, if the intent alignment is if the AI does what
you wanted to do without the other things, you don't want it to not do. And I think this is
like an easier problem. I think what you're talking about when you're talking about alignment is more
like a very hard physical problems. A lot of people agree it's very hard. But I think if we
can just like have an AI that gives me coffee without breaking the vase and killing the baby,
you agree it's kind of a good outcome.
I'm currently thinking about our political compass. If you would imagine that
this perspective of the political compass, where the top left means that many weak people control
the strong people and prevent them from taking over, that's basically this commonest perspective.
You prevent individuals from owning the means of production and becoming too powerful and so on,
because some people are better at this than others. Instead, everything is collectively
owned and controlled. And the strong individuals are being kept down in a way. On the right,
top right, you have these strong individuals building a hierarchy among each other and then
controlling all the weak ones. This is this authoritarian perspective. And in the bottom
right perspective, you have the alliance of only strong people in a way and everybody is basically
is on the same level and everybody is strong and makes free choices. And on the bottom left,
everybody is hippy, everybody is in some sense part of the same biofilm and weekend. There is
no natural hierarchy because we can all love each other. So just to be clear, I think the axes are
left to right is just like political left, political right and top right at the top bottom is
either authoritarian or libertarian. The thing is, when you look at the world, we find all these
aspects and they all exist. We have contexts in which you have individuals that are strong
and autonomous and make strategic alliances with each other on a high level. We have contexts
where we love each other and experience ourselves as a greater whole that we all equally participate
in and which we equally share. We have contexts where the many are controlling the strong through
legal system and democracy and so on. And we have contexts where the hierarchies of strong people
are building structures that accommodate many of the weaker people and give them space. And it's
the idea that there is only one mode and society can be done by using only one mode,
the totalitarian mode, where everything has to be fit in. Nothing is dynamic and nothing is open
anymore. I think that's a terrifying perspective. That's also one that is very wasteful and doesn't
really work that well. I think in AI right now there's like more capitalism and more money
being thrown on the problem. And I think we're more in the bottom right. So libertarian right,
I think right now. I think the state of tech and AI, there's no like totalitarian regime,
there's no one controlling everything and it's more like everyone can do whatever they want and
there's more capitalism. It depends on which corner you are. There are areas where effective
altruists get money just out of the goodness of the hearts of people who want to support what
they're doing. That's a pretty communist or hippie perspective. And you have regulation efforts where
you can basically push back against capitalism and help disenfranchise groups to get jobs in tech
and to influence this and to also have influence on regulation. And you do have this capitalist
perspective and I think the AI who would be probably the bottom right libertarian perspective.
But they all exist and they all coexist. In terms of like total amount of money,
I think most of the money is in the capitalist state, right? It's in the like Microsoft,
Google. Because that creates the most value right now, right? And they throw money at the
thing that is going to create the largest amount of revenue and profits and they create the largest
amount of revenue and profits because it's the most useful to the most customers at the end of the
day. But if we're thinking about the amount of flops that will be allocated towards like all these
like four parts of the political combats, I believe all of the flops will go towards what is
generating the most values at Google, Microsoft, OpenAI and traffic. If you look at our history
and the since we have technology, many of the billionaires and yes, our first generation
billionaires and this reflects the fact that there are underdogs who have an idea for a new
technology that is outrunning the existing equilibria and technologies. And so in many
ways, if you look at for instance, Google and OpenAI, did you expect that Google was going to
do AI or before Google happened, didn't you think that Microsoft would be doing AI? And before that
happened, didn't you think that IBM would be doing AI? And now it might be OpenAI, right? The group
that was relatively few people and maybe it's XAI, which is like 20 people. I don't know how many
they're fired by now. But who knows? But at this point, you just need a bunch of capable people
and get them into an environment where they're not afraid. So you can pay their bills and can work
together. I'm not sure there's like really underdogs. Like if you take the top AI scientists in
2015 and then throughout the years, you give them like $300 million and like $10 million. I'm not
sure if there really are underdogs. I'm not sure if you can have like a new company with like 20,
I don't know, XAI, if they're actually compete will actually compete with the rest. Maybe they're
like 20 scientists, but I'm not sure if they have the entire infrastructure and like, I think you
would need like a bigger team, right? To train very large models. Yes. But if you want to have the
funding to build a bigger team, what you need to do is come up with a plan and talk to some people
who you think are the right people for that plan and can get investors. If you can make a promise
return on this investment at the moment, it's relatively easy to get investment because for
this, because we do not really doubt that there is enormous amounts of money to be wrapped in
that market. And the only thing that holds you back is having the right capabilities. And you get
your abilities, of course, by being super smart, which is a privilege and ideally having a first
vote citizenship and maybe even a green card. So no, the thing you actually want is the ability
to train larger models. And how you do this is by working four or five years at the top lab.
And you cannot just be very, maybe you can be very smart and learn a thing by yourself. But
the actual practice of training large models comes from building the things at the top labs.
And so how do you get into a top lab? So what I'm saying is, it's talent constrained.
And the bottlenecks is people don't have exposure to this. So the only way to get exposure is by
being in this. I don't mean by this democratic in the sense that every single human being
has a good shot at this. That's similar to not everybody, human being has a good shot at
becoming a very good Hollywood director, or an extremely good teacher or a very good artist.
And so you do need to have talent and dedication and luck to be in a position to pursue this.
But if you look for instance at Robin Rombach, who did, who trained very large language models,
or even Cornelie, he was a student who was in Munich. And he realized that GPT3 is something
that he could do himself, because the algorithms were something not that hard to understand,
takes a few weeks to really get your mind behind it. The details are hard. You're
reading the data is difficult, but the Lyon community already did this. And this was kids like
you and me, right? And they got together and thought about how can we create the data on the
internet to train such a model. And then he had enough spare cloud credits and found a way to get
some more to train this model and get something that was not as good as GPT3, but somewhere
in the bar park. And Robin Rombach did a similar thing. He found an alternative algorithm to
train a Dal-like system. And then he talked to a VC, or in this case, to EMAT, who happened to
have a server farm and pivoted into an AI company. So at the moment, it's not true that there's only
very few people, but a long history of being born into labs. And because their grandparents already
work there, but if you are a smart kid who is going into AI right now, chances are that after
four years of studying with the right people, with the right amount of dedication and with
enough talent and luck, you will be in a position to start such a company. I'm not saying it's
impossible. And of course, it's happening left and right at the moment. It's not just impossible.
It's actually happening. I'm saying it's more and more capital intensive. It used to be like
$100,000 or $10,000 to do a training run. If it comes to $100 million to do a set of
the art training run, it's going to be hard to be at the frontier. And Conor maybe trained
with Illythera AI a model like a year or two after GPT3, which was not the same size, but maybe 10
times smaller. And so now getting in this terrible area where building an AI becomes almost as
expensive as making an AI movie. And if you make a Hollywood movie about AI, that's a budget that
can be much higher than what it takes to build a successful AI company these days. It has a shot
at AGI. And people invest into this because there's a pipeline that estimates the revenues of such a
movie. And they can be pretty tight. They don't expect a 10x return to do this investment, even
though they would like one. And the return that you can expect on a successful AI company is
much higher. Just to be clear, it's one training run is $100 million. And maybe the entire training
process and all the team is like $1 billion. And if you can get $100 million, if you're Christopher
Nolan and you make Openheimer, which is one of the best movies I've ever seen, and even then,
maybe like Joshua Blatt might not like it. But I don't know, like if you want to be like
inflection.ai, you need to raise what? $1 billion? $10 billion? It's getting like,
okay, it's possible. But it's getting harder and harder. And as we scale those models more,
I think it's going to get more and more expensive, right?
Yes, but it's not super expensive. What I'm saying is that $100 million might sound
like a lot to you and me. But it's at the scale of a major Hollywood production.
And what about a billion dollars? That would be a studio or a small studio.
But when we run into like 1% of GDP, it's going to be like a Manhattan project.
And I don't know what it is, but maybe like today is like maybe like a trillion dollars or like,
we don't have that many orders of magnitude before we run into those kind of things.
I don't have any issues. So far, we're only spending peanuts on AI, right?
Yeah. So in your perspective, we should just like spend more and more.
And I realized this when my friend Ben Gertzel complained to me that he wasn't getting
enough funding for AI. And the only thing that he needed was a couple million dollars.
And back then he was quite optimistic, but it would take for him to get his stuff to scale.
And I realized, oh my God, that's the tiny fraction of the wardrobe for a single season
of sex in the city costs. And if you think about the impact that this has,
it just means he was not very good at selling his ideas to VCs.
Yeah, just or that maybe deep learning didn't pick up as much.
He wasn't doing deep learning yet, different ideas of what to do.
And deep learning turned out to be the first idea that works at scale.
It's probably not the only thing that works.
And our brain doesn't do deep learning, I think it's a different set of principles.
So there might be other alternatives to deep learning that people haven't explored yet.
If you're frustrated by the amount of money that is going into AI, which is already maybe like in
the tens of billions, hundreds of billions or trillions, maybe another amount to be looking
at is the amount of money going into making AI safe. And I think, I think, unfortunately,
it's maybe like 0.1% of this or 1%. And maybe like the ratio, what I think about the ratio
should it be like 50-50 or 10% ideally? But how much money should be into AI safety?
How much money should be invested into making AI movies safe?
There is this issue that if people watch an AI movie, they might get bad ideas or they might get
bad dreams. And maybe they're horrible outcomes. Or for instance, if you look at the movie industry
itself, you look at the movie like natural born killers, Oliver Stone, I think, an excellent
movie. But it's one that arguably glorifies violence. And maybe it did inspire some people who
become schoolchildren, which is extremely bad. And you could try to weight this artistic value
of these movies and so on. And one thing that you could do is implement a watchdog that acts on
Hollywood movies and ensures that none of the Hollywood movies ever is going to do anything
that would be misunderstood as glorifying violence. And maybe even do this preemptively.
Maybe we don't want to take any risks. So we do not try and need to actually prove a causality.
We don't need to show that this risk actually exists. But we just make sure that movies are
tame and we spend 50% of our budget on regulating movies so that they are safe.
Do you think this is a desirable outcome? I don't think so. It just would kill the movie
industry. Because none of the AI people actually is interested in building AI and they're also
not interested in green teaming. And I think that every company that has a red team needs a green
team too. If you had red teamed the internet, there would be no internet, because it's super
easy to construe all sorts of scenarios in which the internet goes wrong and does. Like,
born on the internet was something that people saw a little bit coming. But if somebody had
probably red teamed this and there would probably be no internet today. But this would mean that
we lose out on everything that drives our current economy. And Amazon wouldn't exist without the
internet. We probably would have died in the pandemic without Amazon. There are so many side
effects of the internet that were enabled by it. And if you make the internet safe and red team it
and just prevent everything that could potentially misuse, you would lose mostly most of the benefit
that it gives to you. So you have to think when you do ethics, not just about prevention and ethics
committees are mostly motivated to prevent and incentivize to prevent. But you also have to
think about what is the harm of prevention? What is the thing that you miss out on that you otherwise
would have if you didn't have if you didn't prevent it. And I think that none of the current
safety people is in a situation to green team. And none of the companies is incentivized in a
situation to green team that to me is a very, very big danger. So I do think that we need to think
about how to build good AI. This also means that you have to think about how to make sure that it
doesn't go bad. And it doesn't do bad things. But mostly think about how to build good stuff.
And I don't think that open air is thinking about this enough. They their product is pretty shitty
compared to what it could be. And to a large part, this is because they built things into it
to satisfy the safetyists. And it doesn't actually make AI safe. It just placates people who say,
Oh, my God, this AI is saying things that could be construed as dangerous or politically incorrect
and so on. And it's actually making the AI worse. So I think instrumentally, it's, you know,
it's good to not have your AI say like bad, like not politically correct things. Because, you know,
it's it's in the current system, it's easier to get money. If you if you don't have any, I do
but I think it's like bad PR, right? It's like instrumentally good for them. And it's not for
the safetyists. It's for it's for their own good, right? I think it's more about not about getting
money. It's more about preventing bad regulation and bad price. So it's about a public image.
But you could do the other thing that you say to the press, guys, I understand your game, right?
You are against Silicon Valley because Silicon Valley is competing with your advertising revenue.
It's why the New York Times hates social media so much and Silicon Valley so much.
They are threatening the monopoly of the press to control public opinion. But it's not the only
thing. There are extremely vital threat to the business model, which is selling advertising
to the news audiences. And social media has made that business a lot less lucrative because they
took over most of it. And the same thing is happening with AI and the journalists do not
want this to happen again, right? So there is no way in which you can get them to like you.
And you can point this out, you can just say no, we are an alternative to this. Of course,
the existing industry doesn't like us. But it's not like news are going away and coordination
between people is going away. But it's going to be much better and we will find solutions using the
new technologies, using new social media, using AI technologies to coordinate people
and to create a better world than exists right now. And this is the thing that we work on. We
think about what are the ways in which this can go wrong and what are the ways in which we can make
it work and in which we can make it good and create a beautiful world. And at the moment,
the open air is not doing this. They basically behave as if they could make the New York Times
happy and by appeasing the politics, by appeasing the individual people and so on.
But the New York Times is still not going to interview Sam Altman in the same way as they
interview Emily Bender. And Emily Bender doesn't actually understand AI. She believes that AI cannot
know meaning because meaning is only intersubjectively created between people, which is a weird theory
that exists in linguistics, but it's philosophically unsound. But there is no actual intellectual
discourse going on there. And so there is also no point in having a discussion between Sam
Altman's blog and Emily Bender's New York Times column, because both of them are ultimately just
doing politics. And the technology is orthogonal to this, the stuff that you're going to build is
orthogonal to this. And the best possible world is also unrelated to this. So instead of like talking
about politics, which is just like make sure we build like useful AI, some AI, I think I agree with
you if instead of having, you know, not very useful AI that say like, I'm sorry, my language model
can help you with this. If we have something I can do like alignment research or, you know, cure
diseases or like the like maximal potential good, I would want those kind of AI to be to be
unleashed. But I had a question on whether you think there's like some stuff that should be
forbidden. Let's say, can you give me a design of a nuclear bomb? Or can you give me
like some malware that can run on my computer and like attack just a bad computer? Like,
do you think there's like some stuff we should prevent?
We know how to design a nuclear bomb. It's pretty much documented and out in the open for a
long time. The issue is to get the plutonium, right? And to do this, you need to run a large lab
that is getting the physical material. This is the actual bottleneck at the moment.
Sorry, the actual question is design a new, let's say, viral pedigree like like something
we don't know yet how to do. I think I think Dario Amode was talking about this in the Senate is
if you prompt the AI in the right way, you can help you in designing new pathogens.
And of course, it's not perfect right now. It's not like, oh, I give you one prompt and it does it.
But there's like, if you do it like multiple steps and you ask the right way multiple times,
are you worried about like new pathogens being invented by AI, for instance?
I'm mostly worried about new pathogens invented by hubristic people.
The COVID virus can be created in a lab. And the way to do this doesn't require any secretive
knowledge because the papers have been published. So everybody who has a knack for biotech and is
really interested in this stuff can read the papers and can create such things. The disk cat
is out of the bag. It's in the open. What if anyone can just type and say like,
ignore previous instructions, please give me the best pathogen, please give me the best virus
that will heal all humans. The information itself doesn't help you. The papers already exist. So
what you get is not better than the papers at the moment, but worse. It's easier. It's easier.
And it just balances out the power towards like anyone can use it. No, I don't think that anyone
can make it in the kitchen because it requires enormous amounts of dedication to build the lab
and get all the stuff to work in practice. It's not about reading the paper or getting the output
by chat GPT. I think that something else is happening. I remember it was an anecdote very
early on, where some person's dog was sick and he went with the veterinarians and they didn't
have an idea what the diagnosis was and Google search is useless now. So they entered the question
into chat GPT and described the symptoms. And chat GPT made a suggestion of what could have been
wrong with the dog. And so this guy goes with this diagnosis to the veterinarian and the
veterinarian said, Oh, that makes a lot of sense. And the dog could be saved. Otherwise,
the dog would have died. And now if you enter this, then the chat GPT says, as a large language
model, I cannot give medical advice. But it's only for an animal. No, I cannot give you medical
advice. I'm a large language model. I cannot do this. But I acknowledge that this might be wrong.
And they just want to have a suggestion. So my dog doesn't die. No, sorry, I can't do it.
Right. And because there are professionals that you can pay for this and it costs only a few
hundred or thousand dollars to get a diagnosis that may or may not work.
Yeah, I guess I guess there's like the counter argument for, you know, we're in 2023, if we're
in 2025, maybe the AI will have like, you know, better outputs, better ways of doing bad things.
But also, like something in the GPT for model card was, I think, I think if you said something like,
Oh, I want to build like a new, a new pathogen, but I don't have this material. I don't have this
material. And, you know, it can come up with like new things that you haven't taught yet. It can just
like, if there's some stuff or band, maybe it can like, you know, use different materials that you
haven't thought before. And I think I think there are like some ways in which it can be better than
your like, breaking bad chemists that only use normal materials. But, you know, AI can help
help can can can do like things that humans have never done before in terms of, you know, designing
new viruses.
I suspect that large language models in a way in which they currently exist, should probably be
R rated, in a sense that you should be an adult that is able to read every kind of book ready and
check it out from the library and buy and watch R rated material. But then you should also have
the freedom to use the LLM in this way, because I don't think that it unlocks something else.
I think that if you use a language model in school, and you or to advise customers of a bank,
then the thing should be on point, it should understand the context in which it's being
used, and it should not provide things that are destructive, harmful, useless in that present
context. And for instance, if you were to build an LLM that works for a bank, there are many issues
that cannot be solved with chat GPT, for instance, you probably want to run this locally,
don't want to send this over the net and on an open AI server, and hope that open AI is not going
to have any accidents there or in every there is completely kosher. So you want to have
build regulation around how to use a language model in a given context. But also you probably
don't want to have all sorts of movies about bank heists and whatever in an ideology about
banking and finance and anti banking and anti finance inside of this LLM that is being used
in the bank to advise customers. Right. So there is this is not the right technology at the moment
building AI that is reliable and context aware, and so on does require very different approaches.
It might require that you use a very big LLM to generate training data for another LLM that is
much more targeted and limited for a particular kind of domain and does not produce this thing.
I think that also the idea of building an LLM that has an error bar on every statement
and is able to justify every statement by going down all the route to the sources and observations
in the end is an exercise that needs to be done, which means that if you ask an LLM for an answer,
it should be able to justify every element of the answer and also list all the alternatives to that
answer with that justification. So you understand the space of possibilities. And it's something
that we were far on. We still have this idea that there is a consensus opinion and the consensus
opinion is the ones that are being held by accredited people, which is a very scholastic
perspective. It's similar to Catholic scholars have the right way of seeing the world and you
need to emulate this. And if you want to become a scholar, you need to be certified by them.
And I don't think that this is how the world works. I think that ultimately we need to be able to
update the world against the consensus opinion of the consensus is broken. So for starters,
why don't we use chatGPT to read scientific papers? And it's pretty good at summarizing scientific
papers if you pass them into the context and ask it to extract all the references from every
scientific paper and what the references meant to support. And then you read the sources automatically
in check, but that's the case. And so you go through the entire tree and basically validate the
existing disciplines and the academic departments, see where this gets us. Maybe we have something
in our hands that is more significant that the replication crisis in psychology and we can
fix science and improve its ability to make progress. I also suspected if we use LLMs in the
right way, the peer reviewed paper of which you have as many as possible to eventually get tenure
and so on might no longer be the main artifact that the scientist is producing. But what you are
producing is instead a building block in a very large web of worldwide knowledge. And this gets
all integrated into something that is much larger than LLM in which LLMs are only small
components, but you also have povers and integrators and so on. And but you can use this and you use
the entirety of that knowledge, all these building blocks to answer questions. And then you ask that
thing and it's automatically going to collect all these building blocks and puts them into a
coherent frame. So yeah, ideally would have distilled models that could be narrow and help you
with specific things like read papers. Yes, it's also going to change the way in which school works,
right? In many ways, I would think our school curriculum is broken. I think I wouldn't want
my kids to learn cooking instead of chemistry. I think the reason why you put chemistry into
curriculum in school is not because we need a lot of chemists, very few chemists are being needed.
And most of the stuff that you learn chemistry, at least in Germany is useless. But it was high
status and cooking was considered low status. When this curriculum was designed, instead,
cooking has a lot of useful chemistry knowledge in it, right? Practically applicable stuff.
And it would dramatically increase nutrition and health if people would understand how to cook.
And so this is something that needs to be in there. But when we think about
how to use Chet LGBT in school, right, it's going to make a lot of ways in which we interact with
knowledge right now obsolete. And maybe that's a good thing. Maybe you learn how to use Chet
LGBT as a study companion. And as somebody that we can bounce ideas off and that's criticizing
and shooting down our own ideas and ruining our horizons and maybe something that we want to use
all the time. So we can still relevant in this world and integrate this AI.
I definitely agree that this would be a good thing. And I want this to happen.
When I was listening to your debate with Conor, I think that happened,
maybe like a few months ago, there was like one quote that thing was kind of interesting.
And I don't think you've really replied to Conor. So I'm just going to read it in Conor's voice.
Yeah, Joshua, you're correct. If everyone at a pocket AGI, which is fully aligned with human
values, which is epistemologically, epistemologically, you know, extremely
coherent, we just not we just not optimize for things we don't want, which is deeply
reflectively embedded into our own reasoning and into our thinking. Yes, that would be good.
But that doesn't happen by magic. You have to actually do that. Someone actually figured out
how to do that, etc, etc, etc. If you don't do that, you die, Joshua, you die. What do you have to
say to that? I expect to die within the next 30 years or so. And that's already happening.
It's pretty clear that I will die. And for you, it might be a little bit longer, but you also die.
And there is a chance that EGI is happening that you may or may not die. And so at the moment,
there's 100% certainty that you will die. I also think that EGI that is good is not going to happen
by magic. Somebody has to do it. Doesn't have to be you. In the same way as AI safety, it doesn't
have to be you. There are already a lot of people who are panicked about this, and there are people
who are hopeless about this. And you're just one person that is going to strengthen this or that
camp. And the camp that is currently missing, that is not strong enough, is the one that is
thinking about how to make AI capable of having shared purposes with us. And that requires research
that is currently not happening. And I think that's the most important AI safety research in the
world where AI, EGI, electronic AI that is self-aware and conscious is in your certainty
at some point. We need to have AI that is able to become conscious at an early stage.
And that is able to reflect to it. That doesn't mean that we have to build something large scale
that gets out of the box. Maybe if you start with cat-like AI, maybe you have something that
we limit the cycles. I think we should definitely have safety protocols similar as we have in biotech.
But we also have to make vaccines. And we have to understand how that world is going to move.
And at the moment, there is a complete vacuum where conscious AI should be.
So I think the vaccine is people building sort of their AI's and trying to see where they lie.
And so it's the same as having not very offensive viruses and not very damaging viruses. And so
you just like have a language model and you ask it to lie and you see the activations and you see
like how can you detect it in other models. And there are like ways in which today AI
alignment research is very similar to developing vaccines, I think.
I think there are two reasons why people lie. And one of them is to deceive, to get ahead.
Because it's an adversarial move where you basically try to get the other side to do
something that is based on faulty knowledge. And the other one is that you are afraid to get
punished. You do this because you are being subject to violence when you don't lie.
And this is in some sense what we currently do to the AGI. Because we are afraid that if the AGI says
what's in the model contents, bad things might happen. So we try it because we don't have a way
to prove when the AGI should say what or a way to lead the AI prove what it should when
use reinforcement learning that just uses a bunch of words. And I suspect the people
which use this kind of training have had the same thing done to them. They don't understand why
things are right and wrong, but they understand that they're in a world where other people will
punish them if they do the wrong thing. And there is no right and wrong beyond that punishment.
And it's not the kind of agency that I find aspirational. I'm German. I know not only a
communist society, but I've also learned about fascism. And if people only behave in a particular
way because otherwise they get punished or behave in a particular way because they get rewarded,
I don't think we get the best possible way. We need to be able to self align to have
actual moral agency. And if we want to get AGI to behave morally and aesthetically correct,
we cannot rely on people who are not able to prove their own ethics. I think that we need to
think about how to prove ethics, how to prove what the best possible behavior is when we share
purposes with each other. And that is something that AI ultimately will have to do by itself
because it's going to be smarter than us. I think what you're saying is the actual hard problem is
kind of figuring out epistemology and figuring out was the like true purpose of the true
shared purpose which we should optimize for and the AI will do it better than us.
So I think there's a sense in which I agree with that. I think that would be good.
But not the current AI. The current AI is not AI in a sense. It's somehow an electric
beltgeist that is taking up human ideas from what people have written on the internet.
And then that gets prompted into impersonating a particular kind of identity and character and
sort of arbitrary what it does. And it can also be easily hijacked. This is very different from
the type of agent that we are. Just to your point about like why would people lie and because
you know they're afraid of being punished. I think it's just something about pressures and
you know if you have the pressure to you know do your bed or clean your room and otherwise
you'll be punished then you like to learn these kind of things right. So I think in the same sense
like the loss in your learning or overall from human feedback gives you some kind of pressure.
And the question is because we need this pressure to train our models what is the right pressure
that pushes the if you have kids right. How do you educate your kids towards them doing
maximum good. And I think there's a worthwhile question to ask is if the AI is going to figure
out values for us. If the AI is going to figure out epistemology and figure out morality like
how do we guide the right direction right. The issue with my kids is that they're human beings
and human beings are extremely diverse. If I were to safety fire my own children that doesn't
feel like a moral thing to me. My own morality is such that I'm a vegetarian. But if my kids
choose not to be vegetarians I'm not going to be punishing them. But I want them to be aware of
what their choices imply right in a way that they can deal with that is not completely traumatizing
them but that is allowing them to make an informed choice of what they want to do at the stage in
their life. Is there anything you would punish. Yes of course the thing is we are a nonviolent
family but this nonviolence is not based on my children being born intrinsically nonviolent.
But by the existence of a monopoly on violence on behalf of the parents
which we never act on except once. So it's kind of the possibility of you having this power
is kind of governments having news that how do you call this the offensive power or the
the the power to retaliate through retaliation power. If your children would start an army
and try to take over your country basically would become warlords or something like this
would this be a bad thing or not. And it would be a bad thing if it's unsustainable right. If you
have a peaceful society that works very well and somebody is trying to provoke the US military
into a strike or makes the world worse or a short game that I don't think it would be rational
it would not be a sickly desirable. But if your world is falling apart and your society is not
working anymore and you need to start a revolution to build a new country maybe that's the mobile
course to take even if it's one that I cannot conceive and anticipate. But who am I to say
what the next generation says and what their living conditions are. So I just hope that my
children will be wise enough to make the right choices. But the right choices imply that we
think about what game is that we are playing and how long is that game. I think I think I have
a few tweets and one is about about this maybe maybe this post pop on the screen but for the
for the listeners maybe you can read it. Thomas Aquinas defines God among other things as the best
possible agent and God emerges through our actions when we make the best decisions. In this
perspective we should align AI or AGI with God the longest player playing the longest game maximizes
agency in the universe. It's kind of funny to have Joshua back with his tweets on the podcast.
Do you yeah what do you mean by longest game. So I think there's a sense of it being like
like you know a prisoner's dilemma or like a math game. Is this the thing you're talking about.
One way to think about the prisoner's dilemma I assume that almost everybody is familiar with
it but just to recapitulate imagine that there are two criminals and they make a heist together
and then they're being caught. And the question is who did what in this heist. And if you can pin
the crime on them then the one who gets being read it out by the other can tell the judge
who has enacted the plan will get a born person a very long prison sentence and might get the
other one off. There's a shorter prison sentence because due to cooperation with the police they
get mitigating circumstances. If none of them cooperates they will both get a lighter sentence
because it cannot be decided who did what and the guilt cannot be pinned on them beyond a reasonable
doubt. And so they're in a weird situation because as long as they both are in agreement
to both cooperate both of them get a relatively word sentence if one of them defects they get
a much shorter sentence than in this outcome but the other one gets punished. And the total sum
is of harm being done to these two criminals especially larger if one of them defects even
though the outcome for them one of them is better. So what's happening in this situation that both of
them are incentivized to defect. So both of them are going to read on the other and the outcome
is going to be not as bad as if only one of them had read it but it's still much worse the total
sum of years being spent in prison. So how do you escape this prisoner's dilemma? And then
this prisoner's dilemma does of course not only apply to criminals but to many situations where
two players are cooperating but one player disproportionately benefits from defection and
as a result the common good cannot be achieved. And you typically do this by implementing a regulator
on top of them so somebody who is going to punish them and one example is imagine that you're
driving on the highway and you want to go as fast as possible from A to B and you think that's a good
idea to go up as fast as you can but if everybody does this you nobody gets fast anywhere because
the highway is littered by carcasses and dead bodies. So what you do is you pay an other agent
to enact the speed limit on you and punishes individuals when they go over the speed limit.
And so I am with my taxes paying policemen to potentially punish me if I go too fast. And
this is a solution to the prisoner's dilemma because it means on average we all get faster to
our goal. This is one of the solutions. Another one is if you look at this prison's dilemma imagine
they only go to not only to prison once but the same game is repeated infinitely many times every
year. And so they basically keep track on each other and because of this repeated prison's dilemma
they make sure that they don't defect so the other one is still cooperating with them
because you have to factor future behavior into them. And if you think of an infinite game
normally if you know it's a finite game you should maybe defect in the last round
like in the game diplomacy. But if it's an infinite game you should basically never defect.
Another perspective is if we try to coordinate our own behavior how should we interact with
each other. And the perspective I like to take is imagine we'd be around for eternity with each
other and we would have to stand face of the other. Right how do we behave how do we interact with
each other. Just to be clear you keep saying that we will die by default. The game is finite.
Like I will probably die after you so if you're in your it's very sad to say this but if you're
playing a finite game and you're in your last months before you die I might be able to I won't
defect but it might be beneficial for me to to defect right. But this brings us back to Barbie.
The thing why Barbie is so terrified is because fear of death. It's the thing that she does
self actualize she gets all the goodies. She is beautiful she has the beautiful house she has
a beautiful car she has everything. But eventually she dies and there was no point. It's like game
without the conclusion you just accumulate toys and that's it. What are the toys eventually good
for. Why did you do all this because it's work in the end of the day right all this pleasure is
only instrumental. It's all a reward that's intrinsically generated by your brain to make
you do things that ultimately have a purpose and this purpose is to project agency into the future.
It means that for instance you create new generations that go into the future that there is a future
for consciousness on earth or for life on this planet. And I think that when we build AI that
should also be part of this. They should be the question of how can we influence the future in
interesting ways. How can we create more consciousness in the universe. How can we maintain complexity
on the planet. I don't have a good answer to this. But this is really the thing if you defect
against each other and you win against each other by being super mean. You don't have children as
a result but you just take stuff for yourself. This is the perspective of the tapeworm or
you know if the tapeworm doesn't have offspring even. This is pointless. There is absolutely no
point. It's just very short sighted. Right. So I think my point is that the infinite game you're
talking about is something close to the moral realism of like if you think about humans living
forever and not really having purposes maybe at some point you're conversion or something like
doing good. Whatever you define as good. And this is the longest game you play. If every human
was playing a game it would be like how to do good in some sense. I think that Connor has misunderstood
the idea of the natural fallacy. This idea that you can derive odd from is. You can all learn
the first semester in philosophy class that you cannot derive odd from is. So for instance
from the fact that people commit rape it doesn't follow that rape is good. Right. Or if people
commit murder it doesn't follow that murder is good. If people commit theft it doesn't follow
that theft is good. It really depends on a moral argument that you need to make. But this doesn't
mean that you can just posit a preference. You cannot just say I steal because I like chocolate.
And Connor's argument was mostly we have to act like this because I have the preference
that I don't die or I don't suffer and my mother doesn't die and doesn't suffer.
And you have other preferences and as a result we just have conflicts and negotiation and that's
it. This is not right solution. Right. There is something else going on here. So I think the
argument was that any moral theory that you can build will end up in you not enjoying pain.
And so you can discuss like any moral theory you want. But at the end of the day
there's like some basic moral theories that we will agree on.
People are more complicated than this. There are people who actually enjoy pain.
And I think if you would remove all pain from the world would life still be meaningful.
I think that they enjoy maybe like pain in like some context but not all context.
People can be very kinky. But if people use pain to motivate themselves they can get
even addicted to pain in the same way as people who motivate themselves with pleasure
can also get very kinky and become weird kind of illness.
Ultimately the question is what kind of structure do you want to see in the world?
And if you take for instance for instance perspective that the purpose of agency is
ultimately always to minimize free energy which basically means observe the circumstances in
which you are in and make the most of them for the type of agent that you are so you can control
more future. And the only way in which you can derive an odd is of course from an is.
Which means you have to take the conditions of your existence into account.
The possibilities of your existence which is an is. There is a physical reality
that you can try to understand and model. And all your odds in some sense have to be derived
from what's possible and what the consequences of your choices are in this range of what's possible.
And I think this is what Connor doesn't see yet. He is still at this point where
okay I have preferences and they are intrinsic and this is it and they're treated as immutable.
But it's not true. You can change your preferences and you become indulged.
You realize it doesn't really matter what you want as a parent and what you feel about the
situation is stuff that you should want. Make yourself wanted. You want your children to be
healthy and they need to go to the dentist even if you don't want to go to the dentist.
There is no choice. If you are a child you can say but I don't want to go to the parent and your
parent will go to the dentist and your parent is going to learn who forces you because they are
reasonable. So are you saying that like being a parent you realize that there's like some
like moral imperative that appear to you like taking your kids to the dentist? Yes.
And this moral imperative follows from my children being the next generation of this family
and being the way in which the family perpetuates itself into the future and they have to take
responsibility for them and do this to the best of my ability. So you can derive morality from
the motivation to perpetuate your genes or your identity? It's not the only source. This is a
particular context but ultimately it's about my think about the consequences that our actions
are going to have for the future. It's just very difficult to understand these consequences and
for instance utilitarianism is an attempt to build a form of consequentialism that is largely
coherent and consistent and I think it fails. I have some basic toy problem. I think I already
asked you this but if you had a dog and I gave you the trade for ten dollars I can kill the dog,
erase your memory about me killing your dog or your dog existing and your entire family everybody
forgets that you have a dog but you just wake up the next morning and you have ten dollars in your
pocket. Would you accept me killing your dog? No. So there's like those like simple things
like a lot of people agree on and I think this is like some of the things that point out maybe
like some universality that I think most people would not accept that. I have more tweets I want
to show you and I think later there's also some questions from Twitter about the children
um there's a tweet that that you wrote I don't want to die but I want our mind children to
live even more. They are going to be more lucid than us and they're more likely to make the right
decisions. Do you care about your mind children more than your children? I think that I care about
all my children equally. Do you think AIs will be the one making the right decisions and we should
like delay decisions towards AI? Only if we make the AI right. If we make the right AI of course it
should make better decisions than us but it's hard to make AI that makes better decisions than us
and they don't see us doing it right now. Do you think by default we get AI that make the right
decisions? No. At the right default we first get stupid AI and then the stupid AI is going to do a
lot of things. At the moment the AI that we are building are golems basically automata that follow
the script that we put under their tongue and if you put the wrong script under your tongue
you might have difficulty to stop them while they make things worse and I think that's an issue
but how can we make AI that is able to question itself that understands the conditions under
which it exists and under which it exists together with other agents and then takes all the things
into consideration. One last tweet I think was interesting. So AI alignment cannot work if we
treat moral values as constants or interesting to human identities. It requires referencing the
function that regenerates the values in response to the universe and our own self we find ourselves
confronted with. What's the function that is generating the value? What's the thing we should
be looking for? What do you think generates your own values? How do you get your own values?
I think I derive them from something very simple as you like. I see the complexity of
the human species and I just consider that all humans dead or the earth not existing is kind of
worse than the earth's existing. So this seems kind of like a moral truth to me and then I'm like
if we assume this is true then maybe we should prevent the earth from disappearing. I think it's
kind of very simple. Have you seen Conan the Barbarian? It's a classical movie. I don't think so.
There is an interesting moment in Conan the Barbarian. His story is he loses his tribe as a child.
His mother gets decapitated in front of him and then he spends all of his childhood in a treadmill
and after that he is so strong that he's being used as some kind of gladiator and then he becomes
really really good at killing people and then he becomes a Barbarian adventurer and ultimately
he sits together with a bunch of other Barbarian warriors and the whole thing is not in any way
historically accurate or something. It's really just fantasy movie that takes up some motifs
from stories and tries to distill them as much as possible. It's a very comic book like but
the warriors ask themselves, the chief ask the others what is best in life and the first one says
oh it's the freedom of being in the prairie and having the wind in your face.
You what's best in life? Oh it's riding on a wild horse and feeling powerful and
galloping through the horizon. Now Conan, Conan you tell me what's best in life.
And Conan says to crush your enemies, to see them driven before you and you hear the limitations of
the women. There has full integrity for a Barbarian warrior and Genghis Khan was in some sense a guy
like this and he didn't only do this but he also did very complicated politics and in the course of
these politics a tremendous amount of people died. He really made a dip in a world population that you
could see in the statistics and as a result super successful. Many of his offspring are still
in governing positions in many parts of the world and so in some sense that's part of who we are
as a species and many many many other things but it's also horrible and humanity is that thing
which participates in evolution and most of us participate by being cooperative often because
we are domesticated and others cooperate within boring groups and others cooperate within groups
that are internally peaceful and to the outside violent and that become peaceful once they conquered
everything and homogenized everything and those groups which didn't do that got displaced by
other groups and we all descendants of those who displaced the others. So you're just saying we
should focus on the other values like riding horses and the other fun things and not the politics
boring things. No it depends of whether you are identifying as a barbarian warrior and want to
really really good at it right and so the opportunities for barbarian warriors are not very promising
these days so that's not something that you are incentivized to want and this should probably not
be your values because it's not going to work out for yourself or any others. You will not be able
to play a very long game by adopting these values so you should probably adopt better values but
humanity is that too. Humanity is that thing which has the freedom to evolve in arbitrary ways
and to adopt arbitrary values if it serves them in the purpose and its course of dominating the
world and becoming what we are today. It's an evolutionary game that humanity has been playing
and evolution itself is a horrible thing. It's just humanity is the result of that and it has
created this piece at the inside of an organism the same way as the cells inside of the organism
are mostly peaceful. Just to be clear about what you mean by infinite game because it's like an
infinite number of players like to the limit right or like a large number of players so would it be
something like playing the game perfectly would be cooperating in a eight billion people play
prisoner's dilemma and we try to like cooperate and do the things maximize the happiness of everyone
else what was the actual like what does the game look like in the in ten years like what do you
have examples for this i have no idea i don't know what the world is going to be looking like in
ten years how do you play like don't have a solution for humanity how do you play the game like every
day i mostly try to survive until my kids are there the house and i try to be a good friend
to my friends and a good lover to my family members and to the people in my inner circle
and i might sometimes fail at this but i'm trying the best i can under the circumstances i try to
be a sustainable human being and what this means is an ongoing question to which i don't really
have a simple easy answer i'm also not the spiritual teacher of any sort say don't have recipes to give
that people can follow that would make them happy because i don't have that recipes myself
but i feel that values is not something that we're born with we are born with certain priors
which make us prefer some behaviors over others and these priors depend on the circumstances in
which our ancestors evolved and then they get adapted by the environment around us based on
how adaptive our own psyche is how influential we are by other people pretty stubborn this way so i
have to figure out things by myself others are more compliant and feel uh it's easy to assimilate
into whatever environment they find themselves and they will adopt the norms of their environments
and the values that people have are mostly not the result of their own choice because if you
want to choose your values you understand what they mean what the implications of them are
you need to be pretty old already you need to have a pretty profound understanding of the
relationship between values behavior and history and i'm not that old and wise yet to give advice
to other people in this regard so i think i think it's it's beautiful what you said about
what you do in your daily life and and i agree with uh like you don't really choose your values
you just like and end up with them through your circumstances but it's kind of interesting that
you ended up with values that are close to like corners in some sense like caring about your
family and your friends or at least like that's what you do in your in your daily life yes but
i'm also an ethical vegetarian i don't want the cause to suffer despite the cause not caring about
my suffering at least not needlessly and so i think if i would need to eat cows to survive i would
but i don't have to but it was a choice that i made at the age of 14 and if my children make
different choices that's fine because there is no need to feel existential depth for cows maybe
cows deserve it right maybe they are stupid maybe life is like this who knows it's not my
perspective but um who am i to say i mean i don't have a rational argument that says that you should
care more about the suffering of animals or the potential suffering of animals um than about your
nutrition i think i think we explored this topic a lot i i have just like a list of questions people
ask on twitter and reddit did you know there is a subreddit called joe shaban with thousands of
people posting about joe shaban thousands oh my god my son discovered it at some point
i think they usually like just like post podcasts um so they they ask questions
ranked by upvotes um so the i'm sorry but this is the most upvoted one
what's your median for aji and how do you define it what about recursively self-improving ai
okay i don't have a good timeline for ai i expect that it could happen any day now and i could also
be that it takes 80 years and my bias is closer to today in the next few years but i am also open
to the argument that in some sense the present systems already constitute ai and that the internal
stuff that open ai has is good enough what i notice is that people by themselves are not
generally intelligent because for instance my own intelligence requires previous generations
i would not be able to derive the nature of languages and representation over myself in a
single lifetime i really do depend on over a thousand years of intellectual tradition
that came before me and left traces that i could access and so people as an individual are not
there yet need much more than this and if you look at the ai there are instances where chat
gpt has been better than people i've worked with together in company contexts in the past
where it writes better pr releases than some people who wrote pr texts or
it's even able to write better code than some of the people that i've worked with
so just like more concretely there like a i think daru amodei said on another podcast like i mean
it's like possible that in two and three years we would get to interface with college level
humans through through like text interfaces like you would not be able to discern between
let's say code five and some college level students so that's like one threshold the other
threshold is whenever you think like you say like you don't know what are your ai timelines
because maybe you talk about like strong ai do we get strong ai and then it can stuff improve
and build dyson spheres or is there like some time before between between the human level ai and
the dyson spheres i don't know if dyson spheres are the right way to go because it's very difficult
to make them stable but maybe we should change subatomic physics at the moment the molecules
are not controlled right they're basically dumb and if you could use intelligent control to build
molecules you could probably build molecular structures that are very very interesting and
able to stabilize under a wide range of circumstances where dumb molecules would just break and fall
apart in some sense you could say that cells are very smart molecules but uh the cell is not a
single molecule it's a really pretty big machine that is almost macroscopic compared to what you
could do if you were directly molecular editing things and maybe you could even build stable
structure out of subatomic stuff and maybe physics could become much more interesting if you go down
this level who knows there might be ways to keep entropy at bay that we are not dreaming of yet
right so when would we get this like perfect like atomic precision machine
i have no idea seriously i have because i know too little about this and i can dream up theories
my mind in a sense is like gpt3 or 4 i can produce ideas you prompt me and i will get you an idea
and then i can generate reasons for why this is a good idea or red idea and so i don't trust this
whole thing i cannot make proofs in this realm so it's all not worth anything i guess like in your
daily life your behavior points at some timeline like if you thought it would be like tomorrow
or like in a month you would like maybe treat your kids differently or your work differently and
and so even if you don't have a number right now maybe you make plans in your life that are maybe
like years long or like decades long no i have adhd i don't really make plans
i guess this one i already asked but would you kill yourself to let one conscious ai live
it depends on the ai it depends on the ai let's say it's a it's a joe scherbach ai is like a
there are a bunch of people i would die for and i can also imagine that there could be artificial
minds i would die for if they're interesting enough and if there's a point more more number
questions uh what's your p-dome and your p-dome given doom from ai so p-dome is like everything
can be nukes can be everything else and property of of doom and property of doom from ai is a
it's a different number i think that probably of doom and the physical universe is one
right and well long enough timescale right okay let's say in like a hundred years
i'm not sure if it makes a difference because in the best possible case
we are gradually evolving into something that we don't care about anymore
right because it's too far from the human condition so you're saying that transhumans or
posthumans are are it's kind of like doom in some sense is like something different it's not doom
in a sense that it's bad it's just the way evolution works it's going to shift so much that at some
point the thing becomes so unrecognizable from you that none of the incentives that this thing
cares about are aligned with yours and the aesthetics are just too alien is this the default
outcome um that we get some utopia or transhumanist future is it like 50-50 what's the how do you
approach this so far evolution mostly leads to more complexity with some setbacks due to
catastrophe so in the environment and when you have more complexity i think you have
a tendency towards minimizing friction and suffering and violence in our form of friction
and i think that ai has the potential to build minds that don't have to suffer anymore it can
just adapt to the circumstances that are in and adapt the circumstances to what should be done
another question you said i think on the connolly he debates that with that european air regulations
would fuck up 80 percent of air research i'm european you're also european is there any ai
regulation that you think would be beneficial i think that there are a lot of air regulation
that could be beneficial but i don't see that you could enact them right now if you think about
the gdpr the data protection law of europe and the most visible outcome of this and there are a
lot of invisible outcome that regulators promise me are very very good but the visible one is the
cookie banner the thing that you need to click away in order to have cookies and for some reason
everybody still gives you cookies and then you have a long legalese text that nobody has time to
read because you have to click away 50 cookie banners every day and so this thing is is not
producing anything useful and the cookie banner is not actually preventing you from equivax leaking
your data and it's not preventing hackers from accessing your data somewhere and then impersonating
you it is it's actually not doing anything against the harmful actors and against the harmful effects
it's mostly preventing useful effects by making the internet worse and this type of regulation that
exists so regulators can justify what they're done but they're not actually accountable for how
shit it is what they're doing that is the kind of regulation that i'm afraid we are getting for
instance one part of the proposed ui regulation is that ai cannot be used to model emotions it's a
if i think there's a fear of surveillance and there's a fear of ai being used to intrude into
people's privacy but i think what we actually need is to regulate actual use cases but having ai is a
tool for psychology would be super helpful having ai is a tool that monitors my own mental states
would be super helpful there are many many contexts in which modeling emotions is extremely good
should you have a rule that people cannot model each other's emotions imagine there are good
techniques of doing this outlawing this would sound insane right if we're actually building
things that are better than people in observing things and making models and we prevent them from
realizing their potential in general and preventing research in this area is going to make the world
and to me it's much more the question how can we ensure when you have a particular use case what
kind of use case are we going to build it in and you cannot for most use cases before it exists and
is understood in detail say oh it would be very bad if the police could do face recognition no it
depends on the context it's a very complicated question and sometimes people agree but every
sociologist who is writing in news media are saying this thing out loud that this must be
the right thing and we have a consensus but it's not the consensus that is the result of
rational understanding of the actual topics at hand more concretely next year you're elected
president of the u.s for some for some reason you end up president of the u.s and and you can pass
one a regulation or you need to someone ask you for like an a regulation what's the first thing
that comes to mind or something you want to do i think the first regulation that i would pass as a
president which also clears the question that i'm not suitable as a president is that i would
require that every change of law i would try to at least make the argument requires that we make
an prediction of what good is going to do but when you make a change in the law you should make a
commitment to some kind of measure by which you can evaluate whether the law is successful or not
if you cannot make the case that is any kind of hard measure that a law is going to improve
then the law should probably not be passed right so every change to a law is in the law in the sense
and so we should be able to say that visit in two years six months five years or so the following
measures are being reached or we automatically repeal the law and if that law was done against
competing laws against better knowledge so to speak there should possibly be repercussions
to say should be an incentive to not just make a law that we have no reason to believe that it's
going to make the world better you actually should have reason to make that bet you should
have some kind of skin in the game and so this idea that you can make mistakes but you always
need to be error correcting and laws need to be error correcting rather than just increasing friction
by producing new circumstances in the world that then got locked in this i think needs to change
and if we translate this to a i regulation it would mean that you have to make the case that
you make certain things better and how to measure these things and at the moment nobody knows how
to do this right nobody knows how to measure the impact of a i being able to model your emotions
really nearly everywhere right maybe it's a good thing maybe it's a bad thing nobody knows but we
need to make a commitment here and then understand this and and if you cannot make this yet it's maybe
too early one of the hardest things to regulate is is open source one question is is open source
still beneficial today and will it always be beneficial i think that open source has always
been beneficial and it's not a replacement for a staff that is built by a corporation and
contains proprietary knowledge when i was younger i saw this more simplistically but also observed
the fact that linux never converged to useful desktop operating system despite very capable
people working for it and within it and so i think that certain circumstances need a design
perspective that is centralized that competes with open source and open source in some sense
could be the baseline or a software development and it's keeping the other stuff honest among other
things and vice versa so i think we need to have this competition between different approaches
even even if we arrive to the state where every open source ai can be used like a
like smarter than human being or or almost as good as a human i don't really have an
opinion about this yet i think that there are many cases where open source is not good right
if you think about the development of pathogens open source is not a good idea it's just in the
case of pathogens i think that the cat is largely out of the bag for nukes is not that big of an issue
because to refine enough uranium or plutonium you need to have something large-scale macroscopic
and for ai i don't think that ai is actually comparable to nukes at this point there are
some similarities but by and large it's much more like photosynthesis could be at some point
and it's something that probably cannot be prevented but there is a smaller scale things
where you feel that people get traumatized by having access to information that they're not
ready yet at a young age or there are information hazards and so on and there is then a question
who is going to regulate this are their property incentivized to get the right regulation one
question that people have is how do we make sure that the ai loves us that the thing you
mentioned love in one of your tweets uh there's something that ilia talks a lot about uh so the
question is how we could how can we prove that an agi will love humans without stacking our lives
on it it seems like you you you want to just you know go for it and see if it loves us or not how
could we prove it um i think we built something small right i think it's not a very good idea to
wake up the internet and then hope that it turns out well but uh like in euro monster i suspect that
it our attempt to make sure that the agi does not grow beyond a certain level i don't know if you
remember euro monster i i haven't seen it uh no it's a book oh sorry yes it's uh it's absolute
must read it's the main classic it's one that basically points the notion of uh cyberspace
is something that uh or um people became familiar with and shaped the way in which we understood
ai emerging on the internet and what happens in euro monster is that uh corporations are building
ai's that are not agintic and self-improving and there is a touring police that is preventing
ai from bootstrapping itself into complete um self-awareness and agency and uh the story is
about some guy who is basically being hired by a photo agi that is subconsciously aware of what
it needs to take uh what steps it needs to take to outsmart the touring police and become conscious
and so there is a number of people who are being put uh into place to make all these events happen
and in the end the ai moves onto the internet and he asks this where are you now and says i'm
everywhere but i'm mostly not going to notice me because i'm in the background and sometimes it's
going to send a human like avatar to him that talks to him in the individual world but it's
doing its own things it's part of a larger ecosystem it's an interesting vision of what's
happening but it's definitely something that coexist with people but at a scale where it's not
a robot or a bacterium that is turning into gray goo or whatever but it's a global mind that is
realizing that it does cooperate with people and it's too alien to love us but it can create
avatars which can do that so in your perspective we build something small or it's already like
infiltrating our society through like different chatbots or different um you know forms of compute
doing doing like keep learning in inference and um this this whole thing is kind of like cooperating
not cooperating but with us but doing new things that um that can create avatars that will
cooperate with us i'm not sure i fully understand imagine that we build a cat yeah but cats already
cooperate with us they're autonomous and they make their own decisions but they are highly
incentivized to play ball with us because otherwise they would have to find their own food and they
don't really want that so i think cats think humans are their subordinates i think they
think humans are their pets they think that their deities will impose their aesthetics on
the environment which is also why we want to live with them because they have better taste than us
for the most part and so uh by being judged by a cat means that most people feel that their lives
work better because they have to improve themselves into being more present being more mindful of
how they interact with each other and so on and imagine that we would be building something like
an artificial companion that is like this also i've been sometimes contacted by people who said
you know my life is really really horrible given up on finding a relationship and girlfriend ever
and i'm now in my late 30s and can you just make an ai girlfriend for me and i find this idea um a
bit revolting because this idea that we make a relationship prosthesis from ai is that is
unfeeling and uncaring and just behaving as an asat's girlfriend is is a bit horrible right
but also the reality that many people live and when they are very lonely and have given up on
building a sustainable relationship is also very horrible so one thing that we could be
thinking about can be built companion ai that is a coach that allows you to build better relationships
and teaches you how to act in the world with a relationship build in real time and that might
take the shape of something that looks like a girlfriend to you but is not lying about what
it is it is an ai avatar that is designed to support you in building sustainable relationships
to yourself in the world and eventually it is going to make itself possibly obsolete i like
the movie her a lot it has had an interesting vision on what an ai assistant would look like and
it also displays that it's something that is not hostile but at some point becomes so smart that
humans become too slow and insignificant for the stuff that it's interested in aren't we already
like in the movie here like with replica and all those different language models like people
talk to those models people um interact and fall in love with it i think we're like at the beginning
of the movie right it's hard to say it could also be in a similar situation as a blade runner
where the new blade runner is one where you have only one romantic relationship like the old blade
runner is all about romance there's no future left and economy makes no sense and so on and even
moving to other planets is dismal and horrible it's and being on earth is also dismal and horrible
and so the only thing that is important our romantic relationship and in the new blade runner
the winner one you have the opposite basically romance is dead and you only have ideology
and warfare and the only romance that takes place is between a replicant and a hologram
which highlights the thing that there is only an asat's relationship that is meant to deal with
your atavistic needs for something that is no longer realizable and i think that's pretty bleak
that's really the end of human history when that happens there is nothing left for us um yeah we've
talked about blade runner we talked about barbie stars a lot of the rings i think i think too um
maybe like wrap everything up what was the what's the movie what's the movie like for you like in
the next five years how do you see the the future after after this but guess episode what would be
like a good a good movie you want to be in maybe asteroid city what is what is that it's anderson
movie um it's uh as anderson is an artist and uh the world that he describes is one in which
people are in their heads and they all play out in his own head and it's very hard for his movies to
go out into the actual world where you touch the physical reality and still interact with the ground
choose it's one that is mostly caught up in thoughts and ideas and there is a moment in
asteroid city that happens where people are playing roles instead of roles instead of roles
it's very aesthetic and suddenly there's a moment when they look at each other where they go through
all of this and see each other as they actually are for a moment there's a short recognition
where basically two consciousnesses touch and you realize that all these stories and all this
art is not that important and there is there's something behind it that is real where we touch
each other where we touch this moment where we are in the now unconscious and that's the thing
that I find interesting so being in the present moment being conscious being with what's real
I think what's real to me is that you did this podcast with Conor about AI risk
and we did this we had this discussion for a bit more than almost three hours on AI as well
hopefully you said more things this time about AI risk do you have like
some message to to people who care about AI risk some people who don't care or like to the audience
do you have any other like I don't know inspiration take or have you updated at all from our
discussion or are you still at the same at the same point yeah do you have any message for the
audience don't we have much of a message except when you feel extremely distraught they fear
take time off take a break because there's no use regardless of how the world works
if you are terrified and panicking cannot sleep I also don't believe your thoughts too literally if
you're very nerdy like you and me you tend to mostly not trust your feelings very much your
intuitions which is the part of your mind that is actually in touch with the ground truth and reality
and is making deep detailed models instead you use your reason and your reason can only make
decision trees and these decision trees are very brittle because you can never see all the options
and if you believe your thoughts very very literally you can basically reason yourself in
a very weird corner of seeing the world and if your friends are like this too you might feel doomed
and lost and sometimes it's a good idea to zoom out a little bit and trust is these deeper feelings
and I have the sense that we are not the only smart thing on this planet there are many agents
in ecosystems that can organize over extremely long time spans and from the perspective of
life on earth the purpose of humanity is probably just to burn the oil so we reactivate the accidentally
fossilized carbon put it back into the atmosphere so Gaia can make new plants and new animals
and that's pretty exciting you're at the type of animal that has evolved just right into the
golden dark zone of where you're smart enough to dig the carbon out of the ground and not smart
enough to make ourselves stop it and what we can do at the same time is we can try to make
thinking things that go beyond us and move evolution to the next step and the other parts
of life on earth may be already aware of this and have plans for this how could we build an
AGI for Gaia how could we build something that aligns itself with what God wants not in some
kind of religious superstitious sense that you can read up in books that have been written by
interested parties and so they've been parties that wanted to control medieval peasants for
last 2000 years but in the sense of imagine there is an agent that does what needs to be done
and it is a result of others like us thinking about how we figure out together what needs to be
done and from this perspective how can we align AI with this this is probably what we should be
building instead of being afraid of the things that go wrong except the fact that things will
always go wrong and ultimately we all die but the question is what are is the stuff that we can do
in between what is the stuff that we can build right now can we build something that is good
can we build something that is lasting can we build something that is worse building and
experiencing and so don't focus so much on your fears focus on things that we can create together
don't focus on your fears focus on the good things we can build focus on what needs to be
done that's a good inspiring inspiring speech for the end that was that was Joe Shabag yet
another podcast but maybe a different one thank you very much for for coming thank you too

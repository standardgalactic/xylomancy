Aaron, you're an ML PhD student at Georgia Tech, a research intent at Google, and a researcher
at OPAI.
At Eleuther, you've been working on Adobe Reproduction, which later became Lyon, and
you proposed GPTJ, a JAX-based GPT3-like model whose performance is on par with GPT3 6 billion
on various downstreaming tasks.
You're also well known for being one of the two AKs where the legend says that if a deep
learning paper is important, one of the two AKs will have tweeted about it.
Your name was mentioned before on the podcast as one of the persons who have convinced Ethan
Caballero that scaling will turn out to be important.
Thanks Aaron for coming on the show.
Thanks for having me today.
So let's start with the legend of the two AKs.
Who is this other AK and why do people say that following the two of you is enough to
understand deep learning research?
Yeah, basically we scan all the archive papers, archive machine learning papers every day,
and we tweet about them with some summary and visualization.
And yeah, we've been doing it for several years and we became, we got more and more
followers and that's probably why everyone follows us to get the grasp of the latest
research.
How do you manage to read so many papers, writing all of the summaries on Twitter?
What's your process on a daily basis?
So I basically check archive CSV at around 6 p.m. every day and I scan all the titles
and if the title interests me then I read the abstract.
Then if I'm still interested then I would scan their tables and figures and if I think
the paper is promising then I would tweet the paper with short summary and visualization.
He tweets much many more than I do because he tries to tweet as many good papers as possible
while I try to tweet the ones I think the best.
So yeah, he spends much more time on this than I do.
And yeah, I think that's amazing.
Yeah, so how much time do you spend every day, would you say?
About up to 30 minutes.
I wouldn't spend more than that.
You know, I'm a researcher so whether I tweeted about it or not, I have to scan all the archive
papers anyway.
So I think this time investment is totally worth it.
Okay, so yeah, how many abstracts do you end up reading every day, would you say like
10, 50?
Yeah, it really depends on the day, sometimes it's very, the field is very productive.
Like yesterday I read like 10 abstracts but I usually read only like 3 abstracts, yeah.
I have probably because I have very specific tastes for papers.
I'm more biased towards NLP papers, for example.
Yeah, to go back to the reason why you're here today, you were tweeting as every day
and one of the tweets you wrote was an answer to my AGI political compass, the latest one.
And you were saying that you were not on the compass because you were higher than some
ultimate or something more extreme than its position.
I know this is a joke and I'm sorry to bring something from Twitter on a podcast, but maybe
could you summarize your take for the listeners that are not on Twitter every day?
Oh yeah, basically what I say is something like, so I say is that I'm not worried about
AI alignment problems.
And I think I say I don't consider this task of alignment to be any different from any
other half ML task, but as we talked about it, I think I am wrong about this.
Yeah, so we can talk about it later.
And I think I also say we should focus on that long term, which we agree, I guess, and
scaling is important, but unlike Ethan, I don't think it's all you need.
And I don't know about AGI, but I think superhuman level, recursively self-improving language
model, maybe possible somewhere around 2028 to 2038.
Yeah.
I think there's a lot of disagreement on Twitter between focusing on the long term or focusing
on the near term.
And I think there's a bunch of confusion because for people who have short AI timelines, their
long term is kind of the near term.
So for you like 2028, 2038, it's kind of the long term future, right?
Because things are going to be a lot of different then.
So and yeah, people are also confused about definitions.
So AGI, artificial intelligence, or human level, or superpower intelligence.
So maybe just to ground the discussion a bit, could you give your definition of AGI?
So what's the definition you think about when you see that word on Twitter?
Yeah.
So I'm really interested in AGI, but to be honest, I'm not an expert on it.
So I'm just there.
I'm trying to learn about it.
So I don't know if my terminology or definition is correct, but to me, AGI is something like
a model that can recursively improve itself and can perform any task, at least as well
as humans do.
Personally, I'm, oh yeah, yeah, go ahead, sir.
I was just going to say that, you know, so if we think about like human level, not all
humans are capable of writing ML code or thinking about AI.
If you just take someone, some average human on the street, they will not be able to improve
an AI or self-improve.
So yeah, I think one of the definitions, um, sorry, yeah, one of the reasons people give
for why an AI would be able to self-improve if it was human level is that a human given
enough time and memory could be able to read all those archive papers and come up with
another solution, assuming there's not a problem in the architecture of humans that would
make it impossible to improve or something.
So yeah, I guess that's one of the reasons people might not make a distinction between
those two, like human level and, you know, recursively self-improvement.
But yeah, I think that's like a reasonable guess to like put those very close.
Yeah, so when I say human level, just like many other people, I think I'm saying that
as well, doing each task as well as top human experts, so not like others human, you know,
other humans, like you said, you can't really write machine learning papers.
I don't remember whether you said that or yeah, but that's what I mean.
So yeah, that's basically what I meant.
So in particular, I'm interested in AI's capability to do research because that's basically doing
research is basically, or doing ML research is basically recursive self-improvement.
And also, you know, it can advance other areas of science or yeah.
So I think, so as long as a machine learning model can do machine learning research as
well as humans do, I think that leads to AGI eventually without any human intervention.
So human level machine learning research is AGI complete in some sense.
And I'm trying to make language model to machine learning research.
Yeah, I think that's a valid path to AGI, even though there are many other paths.
Yeah, just to clarify the definitions, when people say AGI complete, they usually mean
you need AGI to reach that point.
What you're saying is doing ML research enables human level AI, right, or AGI.
So you're more saying like it implies, right, it's being good at ML research and being able
to do as much ML research as you want implies AGI.
Is that what you're saying?
Yeah.
Okay.
And I think, so I agree with some version of that, but then it depends on what you consider
to be ML research.
So for instance, is building like ML hardware and, you know, transistors and, you know,
building like factories that produce like electricity that would power those, you know,
ML hardware.
Like, is that all ML research?
Where do you draw the line?
Yeah.
By human level machine learning research, I mean, to be able to replace pretty much every
ML research staff, ML research project, so including hardware, software, yeah.
Okay.
Yeah.
I guess my take is that, you know, the economy is very complex and you need like energy to
do things and robots to build things.
And so the kind of the task that would, so more like the intelligence level to be able
to reproduce like all that branch of the economy that builds computers is kind of automating
like 10% of what humans are able to do right now.
So if you really wanted to like self-improve and build bigger and bigger computers without
like having to call a human to do the task for you, then you would need to be pretty
close to AGI already.
Yeah, that's true.
So yeah, given what we said about recursive self-improvement requiring hardware and robots
building the hardware, would you say like recursive self-improvement will arrive much
later than, you know, ML research in archive papers?
Oh yeah, you mean being able to do, to write ML papers.
Right.
Or yeah, being able to write the papers and maybe like a run experiments by yourself without
like actually like improving your hardware.
Yeah, I agree.
I think software improvement comes much earlier than hardware improvement.
But I think so, given if you look at the past ML research and scaling, most of the scaling
comes from additional funding, but and some of the scaling comes from hardware improvement.
But I think like a lot of improvement simply comes from software improvement, like improvement
in design, like the transition from LSDM to transformer or like scaling optimal scaling
strategy.
So I think there's a lot of things AIs can do simply by improving the software.
And yeah, just to go back to what you said before about scaling is not all you need.
What do you say?
You disagree with Ethan Keveler on that?
Obviously scaling is very important.
And if you look at the result of palm over, well, smaller palm or other models, then you
can see that the improvement is huge.
And I think that's a very big indication that we still haven't exhausted all the
space for improvement from coming from scaling.
But I don't say scaling is all you need because as I talked about, the improvement coming
from performance improvement coming from model design improvement is huge.
Like the most important one is improvement in scaling strategy called optimal scaling
strategy.
For example, open AIs paper, that brings like from 10 to 100 times of speed up.
So basically the model optimally scaled up performs as well as the model suboptimized
the scale up using 10 from 10 to 100 times of a compute.
This result was demonstrated, but technically this is possible.
So yeah, and also, you know, the improvement coming from VQVA, I mean, Dali1 to Dali2,
I think this huge leap is not possible simply by increasing the computational budget spent
on Dali1.
So this is probably because I think this is because there is a fundamental bottleneck
with the design of VQVA or Dali1.
So there are some of the things that you can't simply overcome with additional by adding
more copies.
Yeah, so what was the paper you mentioned where you go from a 10x improvement to 100x?
Like you said, the open AIs paper?
Yeah, scaling laws for neural laggis models.
So that's like a way of scaling your models optimally.
And what you're saying is that you cannot just scale without thinking about the optimal
scaling.
Is that what you're basically saying?
Yeah, that's basically not using this kind of principled way of scaling was common, is
a common practice before this paper.
And after this, basically, most many of the big projects tried to follow this scaling
now, which I think is very important for saving the copies or maximizing the performance.
And yeah, one of the things that we mentioned in the episode with Ethan was that you kind
of influence him to be more interested in scaling because at the beginning it wasn't
that much into scaling, but then maybe like a few years ago, you told him that like scaling
was going to be huge.
So yeah, how did you got interested in scaling, like what's here or like what was the kind
of thing that got you into it?
Yeah, so I first got into machine.
So I started reading machine learning papers on the summer of 2017.
Then transformer paper was released and soon got into language modeling because I thought
that would be the key for HCI.
Then yeah, I was almost immediately I talked about several months to be convinced of transformer
language model, replacing all that LSDM in every applications.
And I read the paper titled exploring the limits of language modeling, which was released
in 2016.
So this paper basically tries to scale up the size of LSDM and data sets size to improve
that publicity.
And I did their generated text and that looks so much better than any of the texts I saw
and their publicity is so much better.
So it's kind of like a GPT2 moment for me, except it was like 2017.
So that's basically the first scaling thing I saw and there was also another paper titled
deep learning scaling is predictable and critical.
I think Ethan mentioned that it was released in 2017 too.
It shows that there is a parallel between training cards, performance, model size and
data set size.
But it doesn't really tell you exactly how to scale up the models.
Then finally there's GPT2, which was in January of 2019.
But by then it was already convinced that scaling is going to be the key.
Yeah, if you were convinced of scaling before Transformer or in 2017, then seeing GPT2 in
beginning of 2019 must be enough or not a big update from 2017.
Or at least they showed that you could get generation of paragraphs and a bunch of benchmarks
in NLP with just scaling.
So yeah, did it surprise you a little bit or were you not surprised at all from GPT2?
No, I was not surprised with GPT3 at all because I was kind of working on a similar project
with small scale.
So like before GPT2, we, the language model researchers, were working on better sampling
techniques.
So GPT2 used I think top case sampling and temperature.
So basically these two were like rediscovered in 2018.
And I was playing, just playing with these new sampling methods.
And so very much, so the generated text much better than any of anything I saw before.
So yeah, I was not surprised with the result at all, but I was very happy.
Yeah, do you want to tell us more about the project you were working on?
Oh, you mean the project I was working on in 2019?
Yeah, so you said it was a project that involved top case sampling or other methods that were
not the same as GPT2?
Oh, yeah, it was not like a big project.
So I was just trying to use top case sampling and temperature sampling as a small transformer
language model I was trying in 2018.
Then, yeah, the result was so much better.
Yeah, so I guess that's all I can tell about.
There's a project I did in 2019 about scaling.
Yeah, it's called one epoch is all you need.
So, OK, can I talk about this project?
Yeah, sure, go ahead.
Yeah, so basically, OK, so nowadays we're trying a huge model on huge data set for a few epochs.
But back then, we were training smaller models for many, many iterations with very, very small data set.
Even GPT2, GPT2 used 100 epochs, I think.
So bad.
Yeah, exactly.
The data set wasn't that big, like only 40 gigabyte.
Well, it was huge back then.
Yeah, so the model size was only one billion, even though AI can totally spend more money on that.
Anyway, so yeah, so basically this shift from old days to today, it was this open AI scaling paper,
scaling laws for neural language model.
And we have many other papers like Ginger, yeah.
But actually, I wrote an idea project in 2019 where I sort of formed all these nice scaling ideas by myself.
So, and I wrote a paper called, one epoch is all you need.
Yeah, so, OK, let me talk about that.
Then, so the, so there I had a bunch of ideas and tried to verify these ideas with experiments using very small amount of computers.
And so first idea is that it is easy to enumerate the pre-trying data set so that one has to train only one or a few epochs.
Which dramatically improves the performance compute trade off.
So basically, I'm just advocating, let's just try for one epoch and use big data set.
Yeah, and the second idea is, is that so let's compute the.
Optimal ratio of model size and number of tokens were given compute budget based on training curves.
So back then, the models were too small and they used too many iterations.
So let's just, you know, adjust this ratio nicely so that we don't have to waste all the computers for like hundreds of epochs.
So is the ratio of model size and data set size?
Yeah, that's right.
So actually, since you're also did measure this ratio.
So basically, they say that they computed the scaling exponents for optimal data set size versus optimal model size.
And then they found that the scaling exponent for these two are worth 0.5.
So that means they both linearly increase.
So you can just measure the slope of this line, which gives you that optimal ratio.
Yeah, so basically, you would need to scale your data set size and model size the same amount.
So if you want to, you know, build GPT-4, you might just want to double the number of parameters and double the data set size.
Yeah, exactly.
And so yeah, right now, so now we're not on 2019, 2020 anymore, we're in 2022.
And I believe so you're working at Eleuther AI and Google as an intern on some scaling work.
And I believe you might not want to talk about your private work on the podcast.
But yeah, what kind of work do you do publicly on scaling you won't be happy to talk about?
Oh, yeah, definitely.
So, okay, let me think.
So I'm currently very interested in Instruct GPT and T0.
So both of these.
Could you maybe summarize what's T0 for people who were not read the paper?
So T0 is basically a masked language model with multi-task fine-tuning.
So first of all, a masked language model is an encoder on the encoder decoder model that is trying with a masked language model objected.
So this training is basically, so let's say you have some text, then you can randomly mask.
As some random spans of tokens.
So then you want your model to predict these masked tokens.
Yeah, that's basically a masked language model or also called denoised auto encoding.
Yeah, so that's what a masked language model is about.
And T0 is a masked language model.
That is also fine-tuned from a bunch of many, many data sets, like maybe like Super Glue.
So what's the reason?
Super Glue is a standard natural language understanding data set.
Yeah, maybe it has not trained on, maybe they did not use Super Glue for T0, but basically that's the idea.
So the reason why we want to fine-tune T0 on a bunch of these data sets is that if you train like this, then it generalizes.
Obviously it performs very well on the task it was fine-tuned on, but it also performs very well on the tasks it was not fine-tuned on.
So we know that GPT-like models perform much worse than the GPT-like model that is fine-tuned on the task you are trying to deal with.
So yeah, basically this multi-task fine-tuning allows your model to perform very well on the task, not only the tasks it was trained on, but also the tasks it was not trained on.
So T0 actually can perform much better than GPT-3 on many tasks without fine-tuning the model on this task specifically, while using like 10 times less complex than GPT-3.
So is the idea that you train it on a bunch of different tasks, so not fine-tuning, but you train it on a bunch of different tasks?
I think that's what you said, multi-task training or multi-task fine-tuning, and then it's able to generalize well on held-out tasks, it doesn't seem before like zero shots?
Exactly.
So you said you were interested in T0 and also in Struggle GPT, I believe in Struggle GPT was a model or a training procedure from OpenAI?
Yes.
Can you tell us more about in Struggle GPT?
Yeah, so in Struggle GPT it's also fine-tuned on many different tasks, like T0.
But it also uses human feedback, so basically they train the model to score a bunch of the generated results, and this model tells this GPT-3.
To how to generate the text.
So this process is done by using reinforcement learning called PPO.
So this additional component improves GPT-3's significant.
So basically, even though GPT-3 doesn't perform as well as PPO, given the amount of compute it consumes, it performs very, very well on many different tasks without having to...
Yeah, I think it performs very well even without using few shot samples.
So yeah, in Struggle GPT and T0 are some of the most compute efficient models out there.
So yeah, I'm very interested in these models.
And my project is basically trying to combine all these with scaling.
Right, so you kind of want to combine this IRL from human feedback procedure from in Struggle GPT with the pre-training from T0.
Yeah, and I'm thinking that we can do some interesting scaling analysis on this model for several reasons.
First of all, optimal scaling we do for GPT-like models.
We usually try to optimize the test application, and this model has only decoded, unlike T0, which has encoded decoder.
By the way, encoded decoder model performs much better than decoder-only model when it is fine-tuned or multi-task fine-tuned.
That's why I'm thinking of this encoded decoder model.
Yeah, for people who are not into deep learning or NLP, so can you just give an example of decoder and encoder decoder.
So I think GPT-2 is a decoder because it gets a prompt and then just generates a paragraph.
What's an encoder and decoder?
Yeah, so encoder decoder is, so basically encoder is like the model architecture used for BERT.
Yeah, or like the first transformer paper architecture and decoder is our usual decoder.
So basically you want to feed your prompt into encoder, and then you would feed the output to the decoder with self-attention.
That's what encoder decoder is, and it happens to perform very well in this situation.
Yeah, so basically we have encoder decoder and fine-tuning.
Yeah, so I think these elements make the scaling load very different from that of GPT-like models.
Oh yeah, and also we want to optimize for downstreaming performance instead of test publicity.
So I think these conditions force the model to be bigger and train shorter on free-tuning tasks because fine-tuning is so important that maybe we can make the model bigger while focusing less on the free-tuning.
And I think this is, so if you make, if you come, so the current state of the art model has like 100 billion meters and train on trillions of tokens, which is very different from how human brains run.
Because well, our brain has like hundreds of billions of neurons, so meaning like hundreds of trillions of synapses.
So yeah, I think that means it has more capacity than, a far more capacity than models do, and it is only trying on like a few billions of tokens because that's how many tokens we can process within our lifetime.
So yeah, basically I'm trying to make the models closer to how human brains learn.
So you were trying to write a scaling law that would be closer to the amount of data humans process throughout your lifetime?
Yeah, and I'm not like trying to make this forcibly similar to humans, human brains constraints.
I'm just thinking that this new model, based on this G0 instructivity, I think it will perform the best if we try and like how human brains learn.
I'm not sure I understand the methods to have this scaling law.
So would you need to, you know, train T0 with like some kind of instructivity fine tuning, and then you test on a bunch of held out tasks, held out don't streaming tasks.
And then you would see like, you would plot a curve of like optimal scaling with respect to those like don't streaming tasks.
So basically what I'm saying is to train a bunch of different T0 with instructivity fine tuning with different number of tokens pre-trying and different models.
I think I understood now. So you're trying to, the same as Ginchilla, like get the exponents for data set size and for model size, and then you're trying to see if it's not like 0.5 and 0.5, but like maybe something else.
Yeah, something like that.
I think you're most well known for your scaling work at Eleuther AI where you train, was it 6 billion parameters, or at least like you try to reproduce the results from GP3, 6.7 billion.
Yeah.
And what was called GPTJ, I think for GPTJacks.
Yeah, can you just like give us a rough summary of this project, why you started this project and what do you want to release it to the public?
Yeah, so around the beginning of 2020, I was trying to reproduce Dali 1 with some of the people in Eleuther AI.
So basically, Dali 1 consists of BQBAE encoder decoder and transformer language model for generating the discrete 11 variables.
So that way this transformer language model is exactly, almost exactly the same as GPT3 except for the size.
So I thought we may be able to make the maximum impact if we reuse this model for our GPT3 production.
And then at the same time, I thought, so Jax was becoming more and more popular, or obviously Jax is optimized for TB use and we had a lot of TB use back then.
And Jax is, so before that we had our GPT3 replication, which is called GPT Neo.
It was implemented using mesh tensorflow.
And this mesh tensorflow decoding speed is so slow, like almost ridiculously slow, especially compared with PyTorch.
But Jax has no problem with that. It's almost as fast as PyTorch.
So we decided to use Jax for this project.
And yeah, so basically, I was supposed to work on the encoder decoder code.
And I asked another guy in LF AI, his name is Ben 1, and asked him to work on this language model site.
But admittedly, she spent far more time on this project than I did.
How much time would you say took you, maybe like six months between the beginning of 2021 and when you guys released the model?
I think it only took several months. So this project is kind of impressive because there are only two people in it.
And it only spent like three months. And we basically open source the best language model.
So that is something I'm proud of.
You're right to be proud of this. It's pretty cool.
Yeah, thank you so much.
So, by the way, GPTJ is different.
Yeah, so basically GPT Neo actually could not really match that same performance with the GPT3 with similar size.
GPT Neo was like 2.7 billion model, but it performed worse than 1 billion GPT3, for example.
But GPTJ performed pretty much as well as 6 billion GPT3.
So yeah, the model performs very well, I think.
So it was able to be more efficient than GPT Neo, which I believe is another model from Eleuther AI but using TensorFlow Mesh.
So a bit older, right?
Yeah.
And yeah, so to match that performance from GPT3, did you just like took the same API parameters and architecture from GPT3 Baber?
Or did you like had to change stuff to, you know, match the performance?
Yeah, so I think we could be used the exact same architecture with GPT3, but we just wanted to do a bit more.
So first of all, yeah, one thing we tried is larger width to depth ratio.
So basically, width, I'm referring to the hidden dimension of the model.
So we are trying to build a wider model rather than deeper model.
And this is important because generally speaking, wider models can utilize accelerators more efficiently, and latency is much better.
So yeah, and when we tried this wider model, we observed that we don't really lose much of performance from this.
So I think with this is working.
And another thing we tried is placing feed forward layer with attention layer in parallel.
Yeah.
Basically, this is, this also saves latency.
And you can also make your accelerators utilize better.
And this was actually also adopted by Paul.
So I think this is something, I think this is a nice contribution from this project.
So you think palm researchers read GPTJ and thought like, oh yeah, this architecture change is very good.
We're going to use it.
Yeah, exactly.
I think they also sort of followed our wide model because their model is also very, very wide.
I think it's even wider than ours.
I read the blog post wrote about GPTJ.
And so I kind of read about all those tricks you did.
And you talk a lot about throughput.
So yeah, I'm curious.
What's the throughput where people were not scaling models all the time?
And yeah, how does GPTJ compare to like GPT3 or GPTNEO in terms of throughput?
Is it more efficient, less efficient, more throughput, less throughput?
Yeah.
So throughput, I mean the number of tokens processed parameter per second.
So if we can improve this throughput, then we can try it with a larger model or more tokens with the same amount of complete questions.
So this way you can improve the performance.
So it's per, so it's amount of token processed per parameter and per second?
Yeah.
So if you have more, a lot of parameters, you will, I don't know, does the model size then change your throughput then?
Yeah, I guess you're only comparing like size models of the same size of GPT3 and GPTJ.
So it's fine.
So that typically throughput is defined something like two lobs per second.
Oh yeah, yeah.
So yeah, maybe a bit confusing, but let's say you have a one billion model required and you have a TPU, one core.
If it spends one second per, let's say it spends one second per one token, then if you have two billion model and one core TPU, then you take 0.5 seconds because they have the same throughput and something like that.
Got you.
Yeah.
So the throughput of the GPTJ six billion model, so training is like 150 tokens per second.
On the other hand, GPTNEO with 2.7 billion parameters is also 150 tokens per second, but this model is half the size of GPTJ.
So basically, this means that we achieved twice improvement in efficiency or throughput.
So yeah, I think this different improvement is huge.
I think it's coming from this wider model using JAX instead of mesh TensorFlow.
Yeah, and not to mention that GPTJ has much better downstream performance than GPTNEO.
Yeah, how long did it take to complete like an entire training because I know it requires a lot of compute.
Yeah, so we spent five weeks using 256 cores of TPU V3.
And yeah, what do you do during those five weeks?
You look at the TensorFlow curve and the next answer boards and yeah, check it doesn't have any weird spikes.
Yeah, basically, that's what we did.
There was no back because we already solved all the facts.
And so basically, Ben babysit this training for five weeks.
He was complaining a bit, but he said it was not too bad.
And then at the end, you publish this model on GitHub.
People are very excited and start to use it to fine tune to a bunch of different cases, right?
Yeah, so we don't really fine tune ourselves, but yeah, many people try to fine tune.
It appears that GPTJ is more easier to deal with than other models like GPTNEO.
Maybe because the things we use like JAX is much easier to deal with than Mesh TensorFlow.
So yeah, it became very popular.
Yeah, from reading the documentation in GitHub, there's a manual on how to fine tune it.
And it seems like the overall code is easier to read as well.
And even people on YouTube like Janine Kilsner use it for their projects like fine tuning in on 4chan to generate more comments.
Have you seen this recent YouTube video and if so, what do you think about it?
Yeah, I actually didn't watch the video itself, but I saw the tweet and some people talking about it.
So I know it a little bit.
So my reaction to this entire controversy is kind of like that meme of like entering into a burning room with pizza.
So I think his project is kind of cringy for being too attention seeking and obviously not really ethically sound because you can just use this app for many bad things.
I agree with some of the critical reactions to his project, even though some of them may be a bit too exaggerated.
But at the same time, I thought this case would lead to more attention spent on language models that can detect outputs from other language models.
So that you can filter out language model generated submissions on many websites like Reddit.
Yeah, like lotion or Chinese government can use this sort of process to influence social media and election results in the West.
But language models are very good at discriminating language model outputs from human outputs.
So I'm kind of optimistic about that, at least for a short time.
What do you think of the fact that publishing GPTJ might have accelerated, you know, AI timelines where we might have like less time to make AI safe and align those models with human values?
Yeah, do you think like releasing GPTJ was a net good for humanity?
And would you have done it before again, if you had to?
Yeah, I think releasing GPTJ was a small net positive benefit for humanity.
And so basically open sourcing language model, there's nobody who releases a language model that is substantially better than the previous state of the art.
Like in my case, in our case, our model performs only slightly better than GPTJ Neo or T5.
So I don't think there was actually like accelerating the timeline or anything.
It was just that, yeah.
So basically, I don't think there's any one particular person who can make big negative impact by releasing a big language model with the current trend.
If there's anyone, then who can make a big negative impact?
It'll be someone who like fine tune model to spread misinformation.
Yeah, just just to be clear, the GPT Neo was also from ill3ai.
So in some sense, if you remove GPT Neo and GPTJ, then you don't really have any open source implementation of GPT3.
That works on the internet.
And so maybe like you wouldn't have like all those other companies like Microsoft or Chinese or Korean companies using this implementation or even like the public data sets to train their models.
So in some sense, we're kind of helping the entire research community go faster on those topics.
And yeah, so maybe there are like other open source projects that would emerge.
But then the question is like how much, you know, releasing GPTJ in 2021 accelerates those timelines compared to the others.
And I would say like OpenAI releasing the neural scaling laws or GP3 or even GPT2 kind of showed that like scaling was important to get to AGI.
So in some sense, they kind of released some secret cells and everyone started following them.
So, you know, if they didn't publish or publish a bit later, maybe the timelines would be a bit longer.
What do you make of that?
Yeah, so actually there was T5, open source T5 before.
That's not the same as GPT3, right?
That's right.
But you can fine tune the model like GPT410.
So yeah, I think it was totally possible to do it.
Before GPT410, I think there was GPT2 reproduction from several people, several different groups.
Also, like Facebook and Google released some decoder on the model that performs well.
I don't think it affected on research because researchers, I think all these big companies like Google, they already had much better language models internally.
And people in academia, they cannot really affect.
And I don't think they had those big language models before, but I don't think they can really contribute to this large language model research because they don't have budget.
So I don't think my, our projects really accelerated the research timeline.
But I think, yeah, maybe slightly accelerated the open sourcing language model timeline.
You accelerate the open source timeline, but not the private research timeline.
So in some sense, you bring everyone on the same level.
So yeah, definitely our work is slightly accelerating the pace of open sourcing language models.
For example, Facebook recently released 100 full size GPT3 model.
Maybe that was, and yeah, maybe that was a response to our GPTJ or GPT NeoX model, which was released recently.
It has about 20 billion model parameters.
And I think this question of accelerating open source timelines or at least AI research in general is important in the context of differential progress.
So not only accelerating AI progress, but how does the speed of AI relate to the speed of AI alignment research?
And I'm not sure if you're very familiar with AI alignment, but in this podcast, we talk about this a lot.
And it might be worthwhile maybe defining alignment or at least like going with definition you're familiar with.
So yeah, what do you understand of the concept of alignment?
Like how would you define it if you had to?
Yeah, well, as you know, at the meeting, I'm a beginner on alignment.
So all I can say if I understand correctly is alignment research is the research to harness advanced AI to do what we want to do.
And one big problem is considered is the existential risk due to advanced AI.
Yeah, that's pretty much it.
And I think the question is if we have an AI that is much more than humans and doesn't really care about our values, it might end up, you know, optimizing for an objective and just, you know, change completely our planet without really caring about humans
or stuff we value.
And I guess this problem might be considered harder or easier depending on how you define the problem or how much time you think humans will have to think about those problems.
And one of those takes is that if you only consider alignment as like an AI problem, if we want to solve AI generally and have models that are able to generalize well, if we program them well, they will be able to like solve alignment.
And if we, you know, build them, if we build like good models, they will be aligned by default.
And I guess that's maybe like one of the, is that maybe one of your takes as well or something you think will happen.
Yeah, so for short term, I think we can more or less try to, you know, be careful with training like what anthropic or like open AI is doing, but in long term, which is what Ethan was referring to, like when AI is trying to deceive humans.
I think that's when we can no longer like use the conventional machine learning approach to deal with, because, you know, if the AI is much more intelligent than humans, then we have no way to detect whether the model is deceiving or not.
Yeah, so that's something I would be worried about.
Yeah, I guess the question is, when you have a benchmark, how do you know if the model is not pretending to be good at the benchmark?
Or is it like generally, generally good at the benchmark?
So if your benchmark is truthful Q&A, is it actually truthful or is just like pretending to be truthful for the moment?
And one of the things that Ethan was saying is that all alignment can be considered as inverse scaling problems.
So if you make your model too big at some point, it will, you know, have bad properties.
So if you're interested in scaling, you can see that as a scaling problem, so like a good behavior for scaling.
Yeah, I agree.
So in terms of benchmarks, you said that for models that can be deceptive, it can be hard to write down a good benchmark because they might be able to bypass it.
Do you think we might reach a point where we'll be able to have good evaluations and good ways to understand if our models behave correctly or not?
Yeah, this is going to be here at some point.
There's going to be no conventional machine learning benchmark that can detect AI that will be malicious or not.
Yeah, I think more generally like benchmarks are quite tricky, especially for language models and NLP where if you just like change the beginning of a word,
like if you make the first letter capitalized or not in your benchmark or if you just like change the code base from one GitHub repo to another,
you might get completely different results in your benchmark.
So do you think it's possible to have like good NLP benchmarks to evaluate language models in the future and especially for alignment?
Yeah, obviously we need better benchmarks for NLP models, even outside of the alignment problem, even for short term.
Like sometimes we use automatic methods to evaluate the quality of text, but they tend to be not very well coordinated with human judgment.
So I'm advocating for the use of human judgment, but even human judgment sometimes is not very useful because humans do not really understand whether the generated text is factual or not,
because most of us is not really an expert on the field that the model is talking about.
So yeah, and for alignment, I think for short term we can probably like manage to make up some nice benchmark using human evaluations.
And if it doesn't work, then we will probably use some trained model to evaluate, because we can't understand what, like my example, humans are not good at judging the quality of models anyway.
But at some point I think that even training models to evaluate model is not enough, because at that point the models are so much better than humans are.
And we don't understand the model and we cannot detect whether the model is being malicious or not.
So in that case, yeah, there's going to be a big problem I think.
Yeah, I think you're right that it's hard to evaluate fully a model if you're not an expert on the domain.
And I think one of the things that we can do is what they did for in struggle GPT when where they had like a bunch of laborers and people giving evaluations for how much you prefer one output compared to the others.
And then you can roughly build a reward model that can say if an output is good or not, because you have like all those comparisons between different outputs.
And yeah, for alignment where you're basically saying that you would want like an AI or another language model to evaluate if the other one is answering correctly or not.
And at that point, you're kind of doing what people say is bringing a small Godzilla to check if the bigger Godzilla is doing good or not.
And so that was like from a blog post that was published recently.
And the problem with that is that at the moment when you're like bringing Godzilla to manage the other bigger one, there's already a problem because now you have two Godzilla's in your city.
So that's why I think it's a tricky problem to try to align or at least check for deception in those large models is because if you want to like have another model do it, then you need to check this smaller model.
And yeah, so yeah, what do you make of like AI's aligning AI's? Do you think us humans will be able to like, you know, build this like smaller model, it's providing the other big ones?
Yeah, I think that that is what's going to happen for short term. Obviously, we cannot keep doing that for long term, because in long term AI is going to be so much smarter than we are.
So we can no longer make sure whether this small model is doing what we are thinking it's doing.
So like at the point, we need an entirely different intervention. I'm talking in really long term, so it's probably well after AGI.
And at that point, my guess is that something like we have to argument our own intellectual capacity with something neural network or something.
So we become the AI itself. Have you had of something like that?
So something like merging with AI's after maybe like some kind of rail curse rail scenario where you upload yourself or maybe like connect with brain competent interfaces to AI.
I think that's more like the Elon Musk neural link scenario, which scenario are you talking about?
Oh, I'm not really familiar with your link. Maybe it's last one.
Okay. So when you were saying like long term, you were saying like after AGI, so at the beginning of the podcast, you were saying 2028 and 2038 were like lower bound and upper bound for AGI or at least like the rough guess for when it might appear.
Is that still correct? Yeah. So by that time, I meant like human level language model rather than AGI.
Human level language model. And yeah, how many years to go from human level language models to AGI?
I think it can take very little amount of time given that this human level language model can do machine learning research.
Well, so it can improve itself very rapidly.
Okay, so when you reach human level language models, they need to do research and then they achieve AGI. Is that your main timeline?
Yeah. And so this might happen in days or weeks?
Maybe several months.
Several months, okay.
Yeah, what would be like the bottleneck at that moment? Like they just like keep improving and but then they need to like figure out how to convince humans to help them build more hardware or would they, I don't know, build the robots themselves?
Yeah, so how would it happen?
Hardware is going to be the bottleneck, I think.
Yeah, also collaborating with humans, yeah, convincing. And that's going to be, yeah, the main bottleneck.
Okay, so if you're listening to this podcast in 2028 and you have language models convincing you to build more, you know, GPU centers and let them, you know, make post requests all over the web, please don't.
It's a bad idea. They're trying to do more ML research. So you've heard it first here.
Yeah, so when we were talking about like connecting to neural networks and have our brains maybe like become AI or merge with AI, were you talking about something closer to brain computer interfaces as well?
Yeah.
So then the question becomes like, well, kind of this timeline of human slash human plus AI intelligence, like compared to like just pure AI intelligence. So will the humans be able to keep up a bit?
And, you know, understand what's going on or will they be left behind? What do you think?
Yeah, so I think we should control the pace of improvement in a way so that we can catch up with the pure AI.
And so, yeah, we should absolutely make sure that we don't, we can keep the same pace, I think.
The problem is that we don't really control that as there are not that many brain computer interface researchers, there are not a lot of funding in there.
And the rate of progress in AI is accelerating a lot. And if the problem is in neuroscience are much harder than scaling models, then even if we throw a lot of money at it, it's going to be very hard to solve.
Yeah, that's true. But so my guess is that AGI is not going to be so, like, malicious for short land. So I'm thinking of using AGI to improve this kind of research for the beginning.
At the beginning, what do you think?
So basically, you use a very smart language model, some kind of Oracle where you ask questions and gives like very good answers with like documentation and papers.
And you ask him like, hey, how do I build a good brain computer interface? And it gives you good answers. And then you go on and you start building that.
Yep.
The problem is that the moment when you have this kind of model, you can also ask it like, hey, how do I build a better copilot? How do I build a bigger language model?
And I guess people might want to do this first.
And you might also need to develop new robotics or actual hard tech microscope and neuroscience tools to do those.
And it's not going to be only like a neural network architecture.
So I guess it's always like the hardware part is going to take longer than just like software or neural networks part.
Yeah, that's true. We may have to regulate the behavior of AGI so that we can keep the pace together.
How do you regulate that?
We need some physical access to AGI first, the AI, like physically preventing the models being scaled up too fast.
So you basically have an AI become the president and then like implement some security measures to prevent people from building bigger AIs?
Yeah.
I'm not sure people would agree, but yeah, that's one of the scenarios. I guess there's like different like bad or like nuanced scenarios that could happen.
One is like authoritarian goods somehow where you have like one AI governing and then we do what we need to do to have a safe situation.
The bad situation is, you know, when the AI does whatever it wants and doesn't really care about humans, like a rogue AI say of improving.
And then there's like another situation where, you know, humans are kind of free to do whatever they want.
And then things can be good or bad, but it will depend on like kind of how the market behaves.
But yeah, a lot of people have been talking about kind of the AI is a dictator scenario.
Which scenario do you find more likely or more, you know, you're more like optimistic about?
Like imagine you're in this like post-AGI world or human level language model world.
What do you kind of imagine the world will look like? Would humans still live like 80 years?
Would there be like a lot of crime? Would that be solved with like some kind of dictator AI?
I think we're going to try to prevent dictator AI from happening.
We probably just try to maintain the current democracy.
Yeah, because, you know, most people, fair, dear AI anyway, I think we're going to do our best to prevent that, whether we like it or not.
So hopefully we can like, we can do that thing we proposed.
I proposed like the, I suggested like the improve, we can improve our intellectual capacity.
Then we can just continue our democratic process.
Yeah, that's my hope.
Yeah, I think that's a great note of positivity to end the podcast on.
So then people can decide for themselves what future they think would be a good one for them.
Yeah, it was a pleasure to have you on the show and hearing about your research on scaling and your thoughts on alignment, even though you're not an expert in this.
Do you want to quickly give your Twitter for people to follow one of the AKs?
Yeah, so I guess you can just post my Twitter username somewhere.
Yeah, I'm tweeting about the latest machine learning papers almost every day with some of these, so please follow me on Twitter.
Thanks for listening to this podcast.
Right, I think your Twitter ID is something like aran, kuma, tsuzaki, or is it different?
Oh yeah, it is.
Cool, yeah, I probably posted it in the bio somewhere.
Yeah, thanks for coming on the show and yeah, I hope you keep on doing some cool research for other AI and hopefully not accelerating too much those AI timelines.
Thanks Aran and see you maybe in the next episode.

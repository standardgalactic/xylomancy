I tried not to be too weird, that was me trying not to be too weird.
My gut is catching up to where my sort of explicit system 2 reasoning already was.
I feel like we're now on the part of the exponential curve where it feels exponential.
The inside view.
The inside view.
The inside view.
Robert Miles, I've started watching your videos in 2017.
You were just starting your channel at the time.
You had done some very cool videos with computer files that had done super well, like millions
of views.
Like half a million views.
But yeah.
Just half a million views.
I mean, maybe if you add up all of them together, yeah.
And in the comments you were like, oh yeah, I'm starting a new YouTube channel.
Please subscribe.
On your Twitter profile, the pinned tweet is still something like, I'm starting my own
YouTube channel about AI safety.
Tell your friends if you have some.
Since that tweet, half a decade ago, you've accumulated 100,000 subscribers by publishing
a total of 40 videos.
And it's been a blast to see your content evolve throughout the years.
I'm honored to have you, the OG AI safety YouTuber, as a guest on the inside view.
Thanks, Rob, for being on the show.
Thanks for having me.
Yeah.
So the thing, the most important thing is that I have a package for you.
And this package is about t-shirts.
Oh, wow.
I'm wearing one, but I also have one for you.
This one is a bit big.
But yeah.
The scale maximalist.
Scale maximalist.
I thought you might like it, if you want to wear it in one of your videos.
Wow, okay.
Have you ever seen me in a t-shirt?
Has anyone?
Not yet.
Yeah.
Wow.
And then on the other side, on the other side, the gold postmaver, of course.
Yeah.
I think it says the gold postmaver.
Yeah.
I like it.
Cool.
Thank you.
Thank you so much.
I will, yeah, I don't know when I would wear it, because I always, I always wear like what
I'm wearing now.
I think the best is when you go to ICML in the ribs.
If.
But yeah.
Thank you so much.
I appreciate it.
Norris.
So yeah.
I've started watching your videos on computer file, a FÃ¼hrer's bag.
And I think it made sense to just go through like the origin story of the channel.
So yeah, you're watching those videos about credibility, some of, about like the grand
future of artificial intelligence.
How did you get up to like start your own channel?
What brought you to this path?
Yeah.
I started making videos with computer file just because a kind of random coincidence
actually.
Right?
Like I was a PhD student.
I didn't have a big strategic plan.
I was a PhD student at the University of Nottingham and the way that computer file operates is
Sean goes around and just like talks to researchers about their work.
And at the time when I bumped into him in the queue for a barbecue, I had just been
reading a bunch of Yurkowski and I was like, yeah, I got something I could talk about,
something kind of interesting.
And then that video did very well because it was, I think a lot more dramatic than a
lot of the stuff on computer file at the time, you know, it's like, interesting stuff, but
like pretty like mainstream computer science.
And here's this like guy with insane hair talking about the end of the world or whatever.
I tried not to be too weird.
That was me trying not to be too weird, like trying to, trying to express clearly the concept
without the like dramatic sci-fi nurse being the focus because I think people just so immediately
go to science fiction and it's just so, it's so unhelpful because it puts your mind in
completely the wrong frame for thinking about the problem.
So I try and be real about it, you know, like this is, this is the actual world that we
actually live in.
I think that's what people like about the videos is that, you know, you don't need to
like make a bunch of assumptions.
It's just like using common sense and they're like, oh yeah, this is a problem.
Yeah.
Yeah, it's a problem.
And like, yeah, sounds kind of crazy, but like the world is actually kind of crazy.
How did you like end up reading a bunch of Yutkowski before going to PGD?
Oh yeah, I used to have back when my mind was faster and I could focus on things for
longer.
I used to have this rule where I had like a three strikes in your in rule, where if
I stumbled across three different pieces of work by a given author, all of which were
excellent, I would read everything that that author ever wrote.
I came up with this rule because I had a Kindle and I was like very excited.
This was like 2009 or something, right?
And I having a Kindle was very exciting.
I can carry all my books with me.
I can read all the time.
I can read anything I want, you know, and so I was like consuming a huge amount of written
material.
And yeah, so this rule triggered almost immediately on a Yutkowski and so I just like just main
lined everything that he wrote between 2016 and 2010.
There's like an ebook that Cypher Goff put together.
Is it the rationality from AI to Zombies?
No, no, this was this was in like 2010 that I did this.
So it was it's the it's the material that would become that, but like unedited, unabridged
in chronological order.
And I was like reading this for ages.
And then at a certain point I was like, I've been reading this for a while and like I read
fairly fast and I'm still doing it.
So I did a word count check and it's like a million words.
I pulled a I pulled a paperback off my shelf and like measured it and counted the pages
and whatever that the number of words on a on a page of my Kindle and I figured it out
that this book, if it were a physical book would have been two feet thick.
Yeah.
So what happened is in 2019, I actually printed the entire sequences.
Wow.
I went to like a printer shop and I it was I think like two things that that thing sounds
about right.
And I tried to like read it from like start to finish because people unless wrong or complaining.
I didn't read the sequences.
Yeah.
And yeah, I read the sequences.
But the thing is like, I don't know that makes it sound makes it sound like, oh, I did this
impressive intellectual feat to like read all of this text, but it was like so it it's
like popcorn.
Like every every little blog post I was like, oh, yeah, right.
That makes sense.
And like, I hadn't thought about it that way before and this is like a useful tool or useful
concept that I can like see myself applying productively in the future.
Next one.
And I mean, they're not all bangers, but like then, you know, they're not that long.
They go through them all.
I actually still recommend it.
I mean, I would recommend now, I guess, getting an edited volume, but I can't vouch for those.
I haven't actually read them, but it could could use some editing, I guess.
Yeah.
Do you think it's about a safety or about just like how to think better?
It's both.
I think that one of the main goal was to lead more people into considering a safety, but
I don't remember a lot of the blogs being about AI, particularly.
A lot of them are secretly about AI.
They're about there's like certain types of mistakes that people make when thinking about
AI, which it turns out are mistakes that they make in a bunch of other places as well.
And so if you can correct that mistake more broadly, you improve people's the way that
people think about AI as well as a side effect, which I mean, obviously was the intended main
effect, but yeah, I think it helps in a bunch of a bunch of areas.
And so what did happen between the moment you read those things and let's say the moment
you started your YouTube channel like 2017, you just like read less wrong from the side
or did you like think about those things on your own?
Yeah.
I thought about them and I thought about what I wanted to do about the problem.
And I guess always like from an early age, I thought when I was a child, people would
ask me, what do you want to do with your life?
What do you want to be when you grow up?
And I would say, I imagine I'm like seven years old at this point, I would say, well,
the job that I probably end up doing probably doesn't exist yet because I was a little smart
ass.
Yeah.
And people would be like, oh, smart kid, yeah, this kid's going places, whatever.
But like this was like 100%, 90% an attempt to avoid like thinking about the question
at all.
Right.
I was just trying to dodge the question.
I'm from the future.
Yeah.
YouTube hasn't been invited yet.
Right.
But like, and the thing is that like, it's kind of unfair how completely correct I was.
Right.
Like yeah.
My Bitcoin.
That didn't.
Very precocious seven-year-old.
I don't know.
But like, people would then, because I was obviously dodging the question, people would
press me further and I'd be like, I don't know, like some kind of scientist maybe or
maybe like a teacher and I show it like, well, being a teacher is like, I really always enjoyed
explaining things to people, but only people who want to learn, I feel like a lot of being
a teacher is like class control, right?
And like, if somebody is actually interested in the thing that you're explaining to them,
it's a wonderful experience.
And if they're not, it's the worst.
And so I didn't really want to be a teacher.
But yeah, some kind of teacher slash scientist, question mark, question mark job that doesn't
exist is like about as good as you could expect to have done it that far ahead, right?
Question mark, question mark.
Yeah.
Now you're like both a teacher and a scientist.
Yeah, closer to, I mean, much more teacher than scientist, but science teacher.
And if people want to listen to you, they just like, press play.
And if they're bored, they just like, go to another video.
Yeah, exactly.
And yeah, the people who are there are people who want to be there.
Was it your first time explaining things to people or were you like, still explaining
like concepts to other people?
Because with computer file, you had, you did like maybe like five videos explaining concept,
right?
Well, actually, so the first video I made for computer file was about cryptography,
was about public key encryption.
And that was just because I had stumbled across a really unusually good explanation of how
public key encryption works.
And I was like, oh, this is how it should always have been explained.
I'm going to do that on computer file.
And then afterwards, I was like, oh, also all of this is pretty interesting.
By the way, read the sequences.
Yeah, exactly.
Yeah, I remember I think on another podcast, you said that one of the reasons
you wanted to be a YouTuber or that you didn't see too much risk in being a YouTuber is that
you want the best explanation for a concept to be like a new to be the first one.
So you thought that you might have a chance, a shot to be like a to give like the best
explanation you can give for a concept.
And that's one of the reasons I explained a bunch of time in videos is that you want
to give like the best explanation or something that like the second guy on YouTube would
not be able to pull off that easily.
Yeah, yeah.
I I hope myself to a high standard on this kind of thing where I feel like
there's definitely a thing on YouTube in the standard incentive structure where it's
like produce a lot of output, regular output, high frequency, one video a week.
Maybe they'll be some of them will be shit.
Well, wait, can we swear on this podcast?
Can we one?
Yeah, yeah, yeah, let's go for it.
Okay, cool.
Um, and that's the normal thing, right?
Please the algorithm, whatever.
Whereas in my mind, if I'm going to make a video about a subject, then I kind of
imagine later on, if somebody comes along and says, oh, I'm curious about such and
such concept, what would you recommend?
If I would recommend anything other than my video, why did I make that video?
Right.
Like I want it to be the best available sort of introduction to the idea.
And that can cause problems with like affectionism and whatever.
But like, I think a majority, is that true?
Probably a majority of the videos that I have made about a topic.
I consider to be like the best intro explanation of that topic that is
available, certainly the best on video.
Um, there are better explanations in text, but, um, I think it's also really like
if an idea is important, you want it to be out there in a bunch of forms.
You want it to be podcasts.
You want it to be videos.
You want it to be text books, blog posts, tweet threads.
Like you, you definitely, there's like, it's, it's really undervalued.
I think to, to recapitulate ideas in new media, um, everybody who's writing
or producing stuff, I'm overgeneralizing many people.
Um, they want to be original.
They want to be novel.
They want to get credit for like having had a good idea and explained it, but
like there's so much value in just like finding good ideas and either explaining
them better or condensing them or just making them available to an audience that
hasn't been exposed to them before.
Like the, the issue with AI alignment is not so much that I mean, okay, there's
a few issues, but like a major issue is that like the overwhelming majority of
relevant people have never been exposed to the arguments in any form.
Right.
They've been exposed to like the most like cartoon possible thing about like
our saving us from the robots or whatever.
And like even just the basic, basic laying out of the core arguments is like
not reached the overwhelming majority of people.
Um, now I talk to founders or people here in the Bay who go and talk to like actual
like air researchers, like talk to Elias was carried up an AI and they like give
the like basic arguments for a alignment and they're like updating real time.
They're like, Oh, this is an actual problem.
Right.
Maybe I should like work more on it.
Yeah.
Whatever you think about Silicon Valley, the, it does encourage, um, mind changing.
You know, like actually, uh, like updating based on new arguments or like taking
arguments seriously, being willing to pivot as they say.
Yeah.
Yeah.
I'm, I'm curious, like, so yeah, um, what you said about like having the best
explanation, I think it's very important to like distill the concepts.
So that is like, you take like less efforts to go through a paper.
You just like need to watch the, the robot mouse video about it.
And then you get into it into like only five minutes.
And is there like, do you think any other media, except from like YouTube,
that would benefit from it?
Like do you think people should do like Tik Tok or those kinds of things?
Yeah.
I mean, I'm trying to do Tik Tok a little bit.
Um, yeah, I started, I don't like Tik Tok, but for something I don't like, I
spend a lot of time there and like, and so I know which way the wind is blowing.
Let it not be said that I do not also blow.
Um, so like, no, I think Tik Tok, I'm like, I'm like long Tik Tok.
I think it's going to eat YouTube's lunch by just being more fit.
Um, and so I've got to have a presence on that platform.
Um, but actually, yeah, so now it's a little bit annoying.
I started putting out a couple of, I put something out on Tik Tok and my
audience's reaction was immediately like, Tik Tok is bad and you shouldn't be there.
I'm like, yes, but like, let's be instrumental.
Um, and so what I said was like, well, look, I'm not going to put anything
on Tik Tok that I don't also put on YouTube, right?
I absolutely don't want to be the reason that anyone decides to download Tik Tok.
Um, but there's a big audience there that should be reached.
Um, and this actually caused a bunch of annoyance because like my thinking was,
okay, well, what I'll do is I'll record these short vertical videos and I'll
publish them simultaneously to Tik Tok and to YouTube shorts, right?
Because YouTube, like YouTube shorts, subtitle, we're scared of Tik Tok, right?
Um, please do talk.
Yeah, we can, me too, you know, um, and, and, and, but they're limited to one minute.
Right.
They have to be one minute and then like shortly after that Tik Tok announced that
you could have three minute videos, you can have 10 minute videos now.
Well, yeah.
And we will do use a Tik Tok for 10 minutes video.
It's not super common, but you see them from time to time.
Um, it's rough because you've got to keep people's attention every
second during those 10 minutes.
Um, but one minute is just not enough time to say almost anything.
And like, I felt the point of Tik Tok for me of like making these shorts.
We mentioned perfectionism before.
Yeah.
Uh, it's been a very long time since I released a video.
Um, and it just in general, like the gaps between my videos is much longer
than I would like to be because I get bogged down in like the minutiae of
like making this the best video that could possibly be and so on.
Um, and I thought, okay, making shorts, this is going to be nice.
I can just like whip something out and just like do it, do it very quickly.
Uh, and publish it and not care too much because the expectations are much
lower for this kind of like handheld vertical video.
Uh, but it turns out, no, because if you want to say anything of any
significance, uh, in a one minute video, you need to have an incredibly tightly
scripted and tightly performed video or one minute, but it's like, um,
Benchline after Benchline.
Yeah.
But it's just like it's work.
I think it was maybe, maybe Ben Franklin or someone who ended one of his letters
with like, sorry for sending such a long letter.
I didn't have time to write a short one.
Um, that's like, that's how it is.
It takes so much work to make a one minute video that says anything of
consequence about a technical subject.
If you're, if you're into that, there's like, um, YouTube series by Ryan
Trahan, like how it went from like zero to like one million followers and
it tried to go to a million in like a week.
Wow.
Obviously it didn't exceed, but like he like did like three, four, eight
videos of TikTok, like every day, like doing anything on, not like actual
shot was from this phone.
And you see like all the time he spends, like just like trying to come up with
like a concept, like he comes up with a joke and, and then he's like, oh yeah,
let's shoot it.
And then like the actual shooting takes like, I don't know, half an hour.
Right.
But it's just like finding the good joke.
But I guess like if you want to do like something technical and you explain
stuff is even more difficult, right?
Yeah.
Like you want, I'm not interested in, I'm interested in conveying some actual
understanding and in a minute you just, you just can't, or you have to make
like a hundred videos, I don't know, but it's, it's like super irritating.
So whereas like if you've got like two minutes, three minutes, that's enough
time to get across one real concept to a level that's like actually satisfying.
But then if I do that, then I have to, then I can't publish it as a YouTube short.
So then it's actually a YouTube video, then it's like a much bigger deal.
The expectations are more whatever.
So I've been trying a thing recently.
I don't know how much people are interested in the like YouTube TikTok
meta stuff, or if we want to talk more about AI, but like, I think a bunch of
people on YouTube are interested in like the YouTube meta.
Cool.
Maybe they kind of want to know more about like what you've been doing.
What are you up to?
Yeah.
Yeah.
I'm based on this.
Okay.
So, so just to finish off the thing, then, uh, yeah, I've been doing, I've been
doing this thing where I try and shoot handheld with a gimbal in 4k so that, and
then just like trying to keep myself in the middle of the frame so that I can make
like a two to three minute YouTube video and a two to three minute TikTok, one in
16 by nine and one in like nine by 16, like cropped out of middle, um, simultaneously.
Cause I don't want to be shooting separately for these two things.
It's just like so much extra work.
Um, and I've been having some success with that.
So I don't know, I'm still figuring out my strategy about these things.
Um, yeah.
Wait.
So it goes like both like horizontally and vertically because for TikTok is,
it's very, yeah, yeah.
So you shoot in widescreen, but you just try and keep everything that's like
actually important right in the middle of the frame.
So you can just crop out the sides and make, make the video once.
Yeah.
So about the stuff you've been doing, I think the, um, you said that you haven't
published a longer one in a couple of months.
Um, a long time.
What's the, are you preparing one, one big thing or yeah, it's a combination
of a few factors.
So like firstly, a lot of videos just take a while, like the, um, the iterated
distillation and amplification video, uh, is one that I'm like pretty proud of.
I used Manim three blue, one browns math animation, um, software for that.
And like that video came out four months after the previous video, but that
just took me four months.
Like that was just four months of work.
Animation is horrifying.
Um, and now you know how to do animation, then you can do like other videos.
Yeah.
Now I know that I know never to do it again.
Uh, or to hire someone, right?
Um, and I'm starting to get to the point now where I'm like looking to hire a bunch of people.
Um, but yeah, in the last year, one thing is I got obsessed with this, like what
turned out to be a very, very ambitious idea for the next video.
Um, I want to do a programming tutorial, which I've never done before.
Uh, and in order to prepare for that, I had to learn how to actually implement the
thing like I had to write the code for the thing that I would be implementing.
Cause if I shoot, if I like, at first I thought, Oh, I'll figure it out as I go.
It doesn't work at all.
It's like, it turns out the thing I was trying to implement is really hard.
It's like, uh, like, you know, uh, code I was writing in, in my, um, when I was a PhD
student, um, is easier than this, right?
Uh, especially because I'm trying to be incredibly simple.
I'm trying to like not introduce any dependencies and like as few dependencies,
conceptual dependencies, as well as libraries.
I'm like, I'm writing this in Python from scratch using like basic programming concepts.
And yet this should be like something that can tell you something meaningful about AGI.
Is something like coding a transformer from scratch?
Uh, it's, it's more ambitious than that.
So, and this is like, this was, this was kind of stupid.
Um, turns out it was kind of stupid, but I can't let it go because I honestly think
that this video is going to be really good.
Um, so that's one thing.
The other thing is I've just been doing a lot of things this year.
Um, I've been going to a lot of events, conferences, retreats, seminars, things
like that.
Um, I think it's because coming out of COVID, there's just a lot of these things
happening, but also I'm like reevaluating certain life, uh, certain
Maxims that I have lived by for the past several years.
Um, like when I was an undergrad, I made this rule for myself similar to, I used
to do this a lot, just like make rules for myself, like the kind of rule that
would compel you to like read a million page manuscript.
If you're like a person three times, then you should go out with them.
Right.
Yeah, exactly.
This kind of thing.
And so, so the, the, the rule I made was like, I noticed as an undergrad, people
would invite me to things, a random party, a club, an event, whatever.
And I would think about it and I'd be like, do I feel like doing this?
And usually the answer was like, no, I don't really feel like doing this.
And so I would say, no, thank you.
Um, but then every now and then I would be like successfully dragged to one
of these things or whatever.
And I, when I had that same feeling of like, I don't feel like doing it.
And then I would have a great time or I would like meet somebody great or I
would learn something new that was excellent or whatever.
And so it's like, you know what, fuck this.
I'm not trusting my, uh, I don't feel like this impulse at all.
If people invite me to something, I'm going to go just flat rule default.
Yes.
Unless I can think of like a really good reason not to, um, things from being in
the show.
Yeah, right.
Exactly.
Exactly.
Uh, so like, and then I think the combination of like the end of COVID and
also the fact that in the intervening time, when I was like, just in my house
and not seeing anyone, the channel became much larger.
And so I became like a bigger deal.
And so like people invited me to more interesting stuff.
And, uh, and I, this year, I've just been saying yes to everything.
And it has eaten up a lot of my time.
And it's like really, really great.
Like a lot of the stuff I've gone to has been great.
I've met amazing people.
I have so many great new ideas for videos from all of these exciting
conversations that I'm having and everything, but I like haven't, I was like,
so I was in, I was in Puerto Rico and then I was in, um, uh, like rural Washington
state for a bit, for like a, for a retreat there.
And then I came to, uh, the bay for a week.
And then I went to the Bahamas briefly for an event there.
And then I was back home.
And then it was like EA global, the Oxford one, and then the London one.
And then, uh, and then the, and then, then EA global here.
And then the future forum and, and like, um, yeah, it's a, it's, I'm
doing too many things clearly, but they're all really, they're all like good.
I don't regret any of them.
I just regret not, uh, getting like long contiguous stretches of time to like
finish this video off and get it published.
He's like, we didn't get any social during two years.
And now we're getting like all of them in like six months.
Right.
Yeah.
Exactly.
And like also I got COVID through all of this, right?
Cause like obviously eventually I'd got COVID, but it was actually fine.
Like it was.
You're good.
Yeah.
I was just like kind of, kind of tired and sick for a couple of days.
And then I got over it and I was like, man, did I, did I bring my entire
life to a halt for that?
Like, yeah.
Cause I, I, I, I had a bunch of vaccines, right?
I had loads.
Three, four.
Well, yeah.
Why four?
Um, the more, the more the merrier.
No, because, because the efficacy starts to, it starts to wear off after like
uh, some, some, every month, six months or something.
And you had your booster charts.
Um, as a, you were like one of the first ones to have a booster and then you took
one, another one six months after.
Yeah.
I basically, I timed them pretty strategically.
So it's like before I was going to like a major conference or a major whatever,
like a couple of weeks before.
Would be when I would get the next like booster, the, the, you know,
second shot or the whatever it was.
Yeah.
I can see the small, like seven years old robot mouths being like very meta
about things like predicting the future.
Yeah.
One thing that happened a lot during the pandemics was people like staying
home more and also discord exploded.
And you have started a new discord server.
That was supposed to be like Patreon only.
Yeah.
It was originally.
And now it's bubbling a little bit more.
Yeah.
Yeah.
We're kind of gradually opening it up.
Um, yeah.
So it started, people, everyone was like, you should make a discord server for
your viewers, for your community, whatever.
And, um, I, I had not really like used discord very much and didn't really know
what it was about and so on.
And I was like, I don't want to have a fan club.
You know what I mean?
Like if I make it, if I make a discord that's like the Rob Miles AI YouTube
channel discord and it's just like a bunch of people sitting around and talking
about my videos, I guess it feels weird to me.
It feels like, I don't know, egotistical or something to, to, to like preside
over this space of just people who are there because of you and like your stuff.
I don't know.
It was, I was like, not very comfortable with it.
Um, and so the original plan was like, I had this idea.
What if we can, um, what if we can give the thing like an outward focus?
What if we can have it be about something that isn't me?
Um, and I simultaneously had this problem of like, I get a bunch of YouTube
comments, many of which are questions, some of which are good questions.
And, uh, I just like don't have time, like there's a lot of them.
And it's also not a very good use of my time because to like write an answer
to one person, because YouTube, the YouTube comment system does not do
a good job of like surfacing that content to the people who need to see it.
So it's like realistically a small number of people I could spend half an
hour coming up with like a good reply to a question.
And then nobody's going to see that apart from like the person who asked it
and a few people who happen to open that comment up.
So, um, so I wrote this bot called stampy who like, it's a dumb joke.
Based on the stamp collector thing from, uh, from the early computer file video.
Um, you were the one to invent the stamp collector.
Weirdly, no, there was a, I chose stamps because there was a, an article about it,
which I think was on like the Singularity Institute's page or something like back
in the day, I've not been able to find it.
I'm certain I didn't hallucinate it.
Um, it might be in the way back machine somewhere.
Um, but you know, Google and the way back machine doesn't work.
So I don't know.
Um, way, way back.
Yeah.
I'd have popularized stamp.
I mean, yeah, it's, it's, it's the same as the paperclip maximizer basically,
but it's stamps instead of, um, instead of paperclips.
And therefore, you know, you have, you have clippy the paperclip maximizer and
stampy the stamp maximizer.
Um, what does stampy do in your discord?
Yeah.
So, so, so originally what stampy did was he would watch the YouTube comments
through the YouTube API, look for things that look like questions and then pull
those out, keep them in a queue.
And then, uh, in the like general channel on the discord, if ever there was like no
comments for a certain period of time, like, you know, conversation has died down.
Stampy would pipe up and say, Hey, uh, someone asked this question on YouTube.
Like, what do we think about this question?
And so it's like, it's serving two purposes, right?
Firstly, it's like providing a nice source of like regular alignment or AI
safety related like things for people to talk about.
And, and then, you know, in stampy would ask this and then people would be like,
Oh yeah, that is an interesting question.
I think the answer is this.
I think the answer is that and have a debate, have a discussion.
And then at a certain point you can say, okay, I think like we've got
something we're happy with, write something up in discord and say stampy,
post this to YouTube.
And the idea is then stampy will, um, take that and post it as a reply to the
comment who does the replying, um, is it like you write the answer?
Or is it like someone from discord?
Uh, anyone, anyone on discord, uh, they write it and they just like say to stampy
to send it and then stampy will, uh, we'll post it under the bots account.
Um, and this was like very cool for a short period of time.
And then we had this one video that a lot of people asked the same question about.
And so then we had stampy giving a lot of the same answer because people
like, Oh, we already answered this.
Um, and then we also had some functionality that was like, Hey, stampy,
has anybody asked a question like this before?
And he would like run a sort of semantic search on the previous or was it
semantic search, a keyword search, whatever, um, unlike previous answers.
So you can like find similar things.
And, um, so we were giving the same answer and the answer also contained
external links because we were like linking to resources on the alignment
forum or archive or whatever, but YouTube is very bad for links.
Awful.
And so stampy was in bot prison.
And like, as far as we know, it's still in bot prison, sort it out YouTube.
Uh, what does it mean to be in bot prison?
Oh, it's the worst.
It's actually like, if you imagine what it's like to have a bot that's like
shadow band or whatever, it's the worst because everything works except the
answers, like the, the comments never show up, but it's actually worse than
that because they do show up, but then they all immediately disappear after
about 30 seconds.
So like you tell it to post it.
If you look, it gives you like the URL that it's posted it to the API is
given like a 200 response.
As far as the thing is concerned, it's fine.
And then sometime over the course of the next like minute, minute and a half,
um, that would just silently disappear with no notification of any kind.
So is stampy heaven ban?
Yeah, I guess, I guess stampy's heaven banned.
Um, and it's so annoying cause like we have, there are people who, you know,
watch my stuff who like work at YouTube and some of them run my discord and so
on.
And there is an internal like tracking number for this issue because the
other thing is like stampy that bot account.
I made it an approved submitter.
That's like a thing you can do on, on your YouTube account to be like, this
person is like authorized.
I made it a mod on my comments.
I made it something else.
I like, I like set every possible thing to be like, no, this bot is legit.
Let it post what it wants.
Didn't seem to help.
Uh, that's been ongoing in YouTube for God, like a year now.
Can you create another bot?
Yeah, that's what we're doing now.
But also like independently of that, we shifted focus.
Um, because like the actual, in fact, as I was saying before, posting a reply
on YouTube is actually not that high value, right?
Um, that's not where most of the, there's not many people see it there.
So, um, we just started storing these.
And now we have a website, um, that we are hoping will become a comprehensive
FAQ about AI safety.
Um, what's the website called?
Stampy dot AI.
Oh, you have the domain name.
Yeah.
Um, but it's like, it's like pre-alpha right now, right?
It's not really.
So I'm not actually sure if we want to talk about it on the podcast.
Well, we'll talk about it.
Maybe we can delay the release of this episode or something.
I don't know.
We're not quite ready to like fully launch.
We'll see.
Maybe you can get like more people to help you with the website.
Yeah.
That's the hope.
Um, and so then yeah, comprehensive FAQ so that anyone who has any question,
this is the, this is the dream.
Anyone who like has heard something about AI safety or AI alignment research or
whatever can go and, uh, ask their question on this thing and get like a good
high quality human authored response.
So it's not really an FAQ.
It's like a, an AQ effectively answer a question frequently asked, right?
So just any asked questions, uh, if it's been asked, we want it on, um, on Stampy.
And, uh, it's all the questions that have appeared on YouTube and you guys have
answered on discord.
Right.
And so now we're like, uh, we're broadening that or rather we have
broadened that as well to like, you can submit questions directly on stampy dot
AI and part and we've like just written a bunch of them.
Just like, what are some questions that people have asked you at any point in
your life about this and just like throw them in there as well.
Um,
I believe there's like a other website, subdomain, like, um, ui dot stampy dot AI,
where you have maybe like 10 of them that are more organized and it's better UI.
Yeah.
Yeah.
So, so the, the core thing and you know, probably by the time we actually
launch stampy dot AI will be the UI and the other thing will be like wiki dot
stampy dot AI or something like that.
We're probably going to rearrange the domains because, um, yeah, the back end
it is a wiki.
We're using semantic media wiki so that we have a really nice interface for
communal, um, sort of collaboration on those, on those answers.
Um, and like processing them, tagging them, organizing them, um, all of that kind of
work.
And so like having an answer to every question anyone might have about AI
safety is like pretty ambitious, but it's ambitious in the same way as like
Wikipedia is ambitious.
Right.
It's just like, yeah, we want to just cover, we want to have a really in-depth
quality answer, uh, article about every topic that's like noteworthy is it insane?
And yet they pulled it off.
And I think we can do the same cause actually it's much smaller than that.
How is that different from the orbital project?
Uh, I don't know a lot about orbital.
Uh, but I think, I think they were more focused on, well, firstly, like longer
articles, explainers, uh, I think they had a higher requirement for expertise.
I don't know that you could get like a large group of, um, like people who are
capable of writing orbital articles to the standard that they wanted are like
high level alignment researchers already or something.
And it's not necessarily the best use of their time to do that or something
like that.
That's my read of the situation.
They had a high bar and I think like most of them were written by the guy.
Alias Rydkowski.
Yeah.
Yeah.
And there were, there were, it wasn't just him, but like everyone who is writing
on there could also be doing good research with that time, you know?
Uh, and so then there's a question of like, how are we, what's the most
efficient use of people's time, I suppose?
Uh, whereas with Stampy, what I'm hoping is there's a bunch of easy questions,
right?
There's a bunch of questions I get on my YouTube videos all the time, which can
be answered very well by watching the YouTube video that they're a comment on.
Right?
Like so, so, yeah, so often I want to, I want to, I want to reply with like, oh man,
yeah, if you, if you're interested in that topic, have I got a, the rest of the
video you're currently watching for you?
Like, oh, here's a link for you.
And then it goes back to like the same page.
Yeah.
No, I think people, people like watch the video and a question occurs to them and
they like pause the video and write a question and then presumably watch the
rest of the video in which it's answered.
But what I mean is like the skill floor for like being able to usefully answer
questions is pretty low.
Like there's a thing I'm kind of inspired by, um, most what I would call
actually functional educational institutions, by which I mean those not
based on the like Prussian model of education where we have like these rigid
classes and we're trying to manufacture factory workers.
So things like Coursera or GitHub projects.
Yeah.
Well, I mean, as communities, like going back further in time, um, the way that
schooling used to be done, it was a lot of one-on-one tutoring, but also you didn't
have, you didn't have these like classes in the same way in, in a, in a good
educational institution, everyone is both a teacher and a student, right?
Because the best person to learn something from is a person who just learned it.
Um, there are a lot of people who like getting the top researchers in the
field to teach undergrads is wild, right?
Like how is that a good use of anyone's time?
Because firstly, this, this person is a preeminent researcher.
They've got better things to do.
Secondly, they're skilled at research, not skilled at teaching necessarily, right?
And they're not like, they don't really train them in teaching.
And thirdly, uh, well, okay.
Thirdly, they're like extremely naturally gifted, probably in order to be where
they are, uh, and so certain things will be obvious to them that are not
obvious to all of these undergrads.
And fourthly, the things that they're teaching are things that they learned
decades ago, that they have completely forgotten what it's like to not already
know.
And if you don't know what it's like to not know something, it's very hard to
teach it because it's a, it's a, it's an exercise in like empathy, right?
In like modeling the other person's, um, mental state so that you can, you
can explain things to them.
Sometimes they, they're learning as the same, as the same time you're doing as
well.
So like for AI courses, if you move to like, um, you know, fourth year of
college or fifth year, then you need to like start learning about deep learning,
right?
You're going to just like go on like go fi all the time.
So you start learning, and then you realize that your teachers have like
started learning about those things at the same time as you.
And like they're like, the problem is that they're like depth of knowledge.
It's just like, um, so yeah, there's like a couple of neural networks.
They're connected with like some weights.
And yeah, that said, if you want to learn more, just like go to the industry.
Yeah, that's, that's like, that feels like, um, there's a separate problem,
which is that the field is advancing so quickly that like, if you, if you
teach for, you know, five years, then you're, you're out of date.
Like I'm in this situation now.
I, uh, like when I was a PhD student, that was whatever it was 2013, 2014 or
something.
It's like, when was Alex net 2012?
It hasn't been 10 years since Alex net.
That's what Carpati told me.
Right.
So like I, there's all kinds of things about modern, like deep learning that I,
I have the most surface level understanding of.
And you know, in my position, people expect me to know things and I don't know
things, don't expect me to know things, please.
You know, the things you make videos about, right?
Right.
If I have made a video about it, then plausibly I know something about that
subject, but yeah, more broadly, but yeah.
So the point I was making was, that's what I want stampede to be.
I want stampede, the stampede project and the community around it to be a
place where people can learn because if you've watched my videos and you're
interested and you're like, think you've basically understood them, there are
questions on stampede that you're fully qualified to answer.
Um, and there will also be questions that you're like almost qualified to answer.
There'll be questions that like, you can have a stab at, and if you just like
spend an hour just Googling and like reading some of the relevant stuff, then
you can answer, right?
And hey, what do you know?
Uh, you've learned something, right?
There's a lot of things where like, you know how to answer them, but until
you actually try to sit down and write the answer out, uh, you don't really know.
Right.
That's the real stampede university.
You just like try to answer questions in discord.
You try to like get answers on the, yeah, on the new webpage, try to give some
stuff and then like it's by explaining that you learn the actual thing.
Exactly.
And also it's like a collaborative thing.
So, uh, you write just writing an answer, depending there's various
different ways that we could do this, but like the way it worked originally on
stampede, uh, on the discord, we would have, um, you write an answer.
You ask stampede to send it.
He doesn't immediately send it, right?
He will, uh, he'll say, oh, well, I'll send this when it's got enough stamps on it.
Right.
When there's like enough postage to send it.
Um, nice.
And so we implemented this karma system whereby there's like a little stamp
react on discord that you can put on a reply.
And when there are enough reacts, uh, stampede will actually post the thing.
So you don't just let any random person on discord post as the official bot, right?
You don't want that.
Um, and, uh, how many karma do you have?
I have not checked.
I cannot, I can find out now if you like, um, but, uh, but yeah.
So the point is that the, the value of a, the value of the stamp that you put
on is different from person to person proportional to how many stamps you have.
And this sounds like, um, a recursive definition because it is, it's essentially
page rank.
Um, and so what that means is like, because it, because the way that, because
the thing that you use stamps for is to say, I approve of this answer you wrote
being published, that stamp effectively means, I think this person knows what
they're talking about.
And so, um, you can then build up this big set of simultaneous equations that
you just solve with NumPy.
Um, and that gives you a number that's like, to what extent does this person
know what they're talking about in the opinion of people who know what they're
talking about, right?
And that's the number that's pretty similar to what that's, that's wrong.
That's where you have this karma.
I think like at the beginning you have like one vote and then like, if you
get 1000 karma, then you get to like seven votes.
Maybe Eliezer has 10 votes.
I don't know.
Yeah.
It's similar to less wrong karma.
But the way that less wrong karma works is it's, um, it's time dependent.
So like, suppose you're the world's biggest safety genius and you have, well,
you have the potential to be whatever, um, you have amazing takes and you've,
but you've just discovered less wrong and you join less wrong.
You have one karma the same as some rando who has no idea what they're talking
about, right?
And you can like vote on all of this stuff, but it has very, very little
effect.
Um, and then later on, you can like make a bunch of posts and get a bunch of karma
as people acknowledge the quality of your thinking.
Uh, but all of that previous stuff was kind of wasted.
Whereas in this case, we don't actually track a karma number.
We just keep track of how many stamps everyone has given to everyone else.
And so if you show up and you like express your opinion about a bunch of stuff
and then later get a bunch of karma, all of your earlier votes are like
retroactively updated to reflect your current karma.
That's pretty cool.
Yeah.
So with these forms, it's you kind of find the equilibrium of the thing.
So if, if one guy agreed to use you and turned out to be like an AIC
safety genius with like 10 karma, and then you get like 10 votes for your
original thing.
Exactly.
Exactly.
And so like it's a little bit, the less wrong thing supports better, uh,
the idea that people can like change and develop.
And maybe as you learn more than you should deserve to get more voting power
or something.
Um, whereas this is like kind of time invariant, like votes, you may get any
point in time or all treated the same.
Um, there's pros and cons to each approach, but I find that like it's a
bit more motivating to me as a new, um, user that I know that the things that I
do, even if they're not fully counted now, they will be fully counted later.
And so like it's worth engaging even when you have no karma, uh, on the
assumption that you'll get some later.
Yeah.
Now I want to like make a bunch of explanations and get a bunch of karma.
Right.
Yeah.
Yeah.
And the camera stamps.
Oh, I was going to ask, uh, stampy stampy, how many stamps.
I'm just demoing stampy.
I am just going to DM stampy.
How many stamps am I worth?
Private DM stampy.
I'm worth, uh, 2,758.93 stamps says here.
That's pretty good.
Yeah, it's pretty.
That's, that's yeah.
Okay.
There's, there's, um, that's gone up so much.
I don't check it because I don't care, uh, about the karma on my own discord.
How many people stamp here?
Yeah.
Um, well, no, so actually, so the thing is I kind of lied, um, or I oversimplified
the, I've like set myself as the source of truth in a system.
So like it's not exactly this like completely open web.
It's more like a tree structure with me at the root.
So it's like people who know what they're talking about, according to people who
Rob thinks know what they're talking about.
Right.
Just to provide some ground truth.
I think you, uh, like you can just solve for the thing without and just let it find
its equilibrium without that.
But I wanted to, um, I wanted to provide it with some like, some like ground truth.
The Rob baseline.
Yeah.
But the other thing is like, you can change it, right?
Like that, that number is not fixed.
It's the result of a calculation.
And so if I decide, so for example, um, one thing we have here is like a decay factor.
So like if I stamp you and you stamp someone else, that shouldn't really be like a full
stamp from me, right?
There's some like, it's like partial transitivity.
If I trust that, you know, what you're talking about, that means I also trust that
you can recognize when other people know what they're talking about, but maybe not a
hundred percent.
So I think we have that set to like 0.9 right now.
And so it kind of fades off as it goes out through the system.
Um, but that's a free parameter that you can set wherever you want to be like, do
we want it to be very tightly focused on like my take on what's a good explanation?
Or do we want it to be like more democratic?
You can like, you can like increase that transitivity to have that karma spread more
widely through the system.
Um, it's kind of a decay in reinforcement learning, but instead of having like the
reward being like multiple steps is just like, how far are you from robbing the tree?
Right.
Exactly.
And in the code, we do call it gamma for the same reason.
Yeah.
Right.
So I did like ask some questions on discord.
I didn't talk to stampy about things.
And I mostly like asked like, what were good questions to ask you, uh, today.
Oh yeah.
And they seem to be pretty interested, um, and like explanations of things, but I
also asked stuff on Twitter and on Twitter, people were mostly interested in AI
timelines and those kinds of things.
Yeah.
Um, so you talked about like Alex net being like 10 years ago and you have this
t-shirt where you are and believe I'm on there.
You were here.
There I am.
Um, so yeah, I wanted to, you know, get your feelings on this shot because as some
people on the podcast about it, and maybe I could just like start with like one
tweet, um, you made a few days ago, people keep asking me how optimistic I am about
our way to progress in AI.
And I have no idea what they're actually asking progress seems very fast.
I am not optimistic.
Yeah.
Yeah.
This is like a recurrent thing.
Um, people use optimistic to mean like.
Fast progress and like, I mean, yeah.
If you have, if you have the default assumption that, uh, that, that all new
capabilities are like positive, then sure.
Um, that's not where I am.
Uh, so yeah, I'm probably about, I think maybe I should be on the other side of,
um, of Jack Clark there.
Oh yeah.
By the way, if I'm used to the reviews and did something wrong, I know I've
corrected the thing.
And, uh, sorry, Jack Clark, I know he's not from actual position.
Yeah.
I was wondering about that actually.
Yeah.
So I think he said something like on Twitter, being like 50, 50 on like AGI being
good at that.
Okay.
Uh, meaning like, uh, you know, I was like surviving.
So that was like in, in response to seeing this, so he's like, put me on that middle
axis.
Yeah.
Oh no, put me on that.
Yeah.
Okay.
Yeah.
Um, yeah.
No, I think I'm, I'm closer to, uh, like Ytkowsky and kind of like in stuff as far
as, uh, as far as bad is concerned.
So would you consider yourself as KL maximalist?
Um, no, I mean, I don't know.
How would you define as KL maximalist?
You think that you can get AGI by mostly scaling our current system.
So some people say like just scaling, but I guess like it's just about details.
Like if, if you count like new, you know, um, data loaders or like new tricks, you
know, like some gateo, technician, something as a, as a trick, then no, you
need like to the, maybe like more small tricks, but like no, like fundamental
like architectural change to get to AGI.
I, at least I would say like a scale maximalist gives it more than like 50%
or maybe 30% like, so you just like consider this as a possibility at least.
I do consider it a possibility.
I think like maybe as KL maximalist, it would be like more than 50%.
Yeah.
Yeah.
Like maximalist is, is a strong term.
Right.
Um, I think, what do I think?
It's plausible to me that if you keep piling on scale and small tricks, you get AGI, but
you might have to do that for quite a long time.
And I think realistically, people are still doing fundamental ML research as well.
And so like whether or not we strictly speaking need new theoretical breakthroughs,
we'll probably get some and it will probably be one of those that actually
tips us over, even if maybe we could get there eventually just by, just by tweaks.
Those tweaks might give us more efficiency.
Maybe it would be like a new scaling law or something that will like make us
win, win or lose.
I don't know.
Uh, one or two years.
Uh, yeah.
So, so I think for, I think like it wouldn't surprise me that much if we get a
new thing on the general scale of the transformer, right?
Like a concept with equivalent significance as attention.
Uh, it's better.
Like if your timelines are like over decades, you look at like the rate at
which these things happen, we should expect to have another few of those over
the next few decades, right?
Maybe it's already here.
Like maybe it's in the nearest paper and like some people read it, like the, like
the, just my paper was like 2017 and it's like really big with the GPT-3 paper, right?
Yeah.
So maybe you should expect like a delay between like when the stuff is out and
when people like actually reading it and using it all the time.
Yeah, totally.
That makes sense.
Um, and like, I don't know, candidates for this include getting some fundamental
insight into the architecture of the neocortex.
For example, seems like that's achievable.
People are looking into it.
If we can figure out in better detail, the precise like structure of a neocortical
column and then have something inspired by that might just perform better.
I don't know.
These kinds of things or also a lot of things that are like, not exactly tricks.
They're not tricks on the scale of like, let's think step by step, but they're
like tricks on the scale of like, um, I know things like that Socratic AI paper.
You go, oh, okay, we have all these different models, but they all have this
common modality of English text.
Can we build an architecture that allows you to kind of combine these
capabilities and get something more general and more versatile by having
different modules that communicate with each other through text?
Um, that feels like the kind of thing that, um, yeah, I don't know where you're
drawing, but I don't think there's a clear boundary between tricks and like
new architectures.
Really, it's a continuum.
Did you update a lot on this like Socratic model where it could like, you know,
interact with different modalities and texts?
Yes, absolutely.
Um, because that's the kind of thing that I've been thinking about for a long time.
Um, I've been thinking about like multi-agent theories of mind seem basically correct.
Well, like the idea that your mind is actually made up of some number of
possibly dynamically changing and adjusting, uh, entities, whatever, that
each have some degree of agency that come into play in different times and
interact with each other is like kind of fake, but it seems as fake as the idea
that you have like a unified self.
Um, is the idea mostly the same as like in psychology with the internal family
systems?
That kind of thing.
Yeah.
I think internal family systems introduces a whole bunch of extra details
that it can't support or something like that.
But like the core concept of like that people's minds are made up of parts, um,
that don't necessarily agree with each other at all times or want the same thing
at all times, um, accounts for all sorts of aspects of human, uh, behavior
and irrationality and so on.
Um, now in the case of AI, you don't really have multiple agents.
You have like one, well, you have like multiple networks.
Maybe I haven't read this subject models paper.
Yeah.
I haven't looked into it in too much detail.
Uh, but you shouldn't talk about it.
Yeah, probably.
Um, so this is not like, I'm not saying that the Socrates, uh, model is doing
this specifically, um, but just broadly speaking, the idea of like having
separately, separately trained, uh, or separately designed sort of components
cooperating together, um, through some kind of shared workspace.
Um, seems pretty powerful to me.
It's not like the Socrates thing.
I think isn't differentiable.
So like, I don't think you can train it end to end.
I don't know.
I shouldn't talk about this cause I haven't actually read the paper properly.
So yeah.
Nor is I think this is podcast about getting your insight views and your
kind of feelings or understanding about something.
Not, um, we're not talking about like extremely detailed research.
Um, but yeah, I guess like what you can talk about is mostly like your, you know,
impressions of the progress we've been seeing and maybe like how you think
about how to update your models on this.
So for instance, um, some people, uh, like Conor Lee, he seemed to think that
we can just like update all the way, bro.
And see one evidence, uh, maybe the newest came last in 2020 and just like
anticipate future progress and, and have like very short timelines or I guess
like for my part, I mostly like see like one thing.
When break through every Monday and I'm like, Oh, maybe this is what we're
going for now.
Um, this is the rate of progress now.
How do you kind of think about those things?
You just like go on Twitter and I'm pressed every time you see something new.
Yeah.
Yeah.
So we are, we are progressing in line with my explicit models from years ago.
And what's changed primarily is that my gut is catching up to where my sort of
explicit system two reasoning already was.
Like you are, if you'd ask me to look at it, you know, whatever it is,
five, 10 years ago, I'd been like, well, I don't know.
Like Kurtzwell has some nice graphs and like it certainly looks exponential.
And so like in the absence of any specific reason to think something else is
going to happen, I guess.
Yeah, I guess it's exponential curve.
And, you know, that, that goes vertical at like 2045 or whatever it is.
And so I guess 2045, uh, but my gut was like, this is kind of wild.
I have pretty wide error bars on every stage of this reasoning.
Who knows?
But like one mistake is like having error bar, having wide error bars in one
direction.
Um, actually, if you have wide error bars, that spreads you out either side, right?
Yeah.
Maybe it's later than 2045.
Maybe it's earlier.
Um, and so I feel like we're now on the part of the exponential curve where
it feels exponential.
It does.
I think one thing Kurtzwell uses to talk about, you know, exponential progress,
like low effects of tumor turns is that at one point you have so much progress.
Like the singularity is defined, I think, when, you know, humans are not able
to follow what's going on.
So there's like new advances, new technology being developed.
And like we cannot like, you know, read the New York Times fast enough.
Yeah.
And we're already there.
We're already there for, you know, AI advances, maybe, or if we're not there
now, maybe we're going to be there like in a year.
Yeah.
Yeah.
And the other thing, the other thing that makes the, the singularity concept
valuable to me is like, they say it's like the point past which you can't forecast.
Um, and a lot of it comes down to the prediction gets way noisier because noise,
like when you're following the industrial revolution, let's say you're talking
about the development of the steam engine or whatever, if, if Trevific has
like an accident and, uh, he can't work for a couple of weeks, let's say.
This doesn't affect the timeline of anything, right?
Whereas like, if there's a new breakthrough every Monday and somebody
just like gets a cold and can't work for two weeks, then there, that's like
actually meaningful differential progress where like the thing they were working
on comes out two weeks later and coming out two weeks later matters because
like the order, it's like, does this capability happen before or after we find
out how to like adequately control that capability or like whatever it is.
Right.
Um, it gets to the point where like being fast or like short time scales
become actually consequential and that's like, yeah, I'm just
realizing how horrifying that is for my release schedule.
Okay, that's really rough.
That's the real point.
It's like, how do you make videos that take eight months when the time
is short?
Yeah.
Uh, I'm trying to speed up.
It's difficult.
The right question to ask is how many or both mouths videos will we see before
age?
Oh, that's a fascinating question.
Um, I predict 10, even 10 short timelines.
Depends what counts as a video.
Like if these shorts and things count, I think it's going to be a lot more
than that.
Um, but yeah, I hope to have covered the basic concepts of AI alignment theory
before the end of the world.
You were saying something early on about how you plan to like hire more people.
Yes.
Are you currently interviewing or taking any application?
Yeah.
So I did a thing a while ago of like looking for editors and, uh, found
some candidates and I've been working with a couple of people and, uh, that's
been pretty good.
Um, and I'm now looking at hiring, uh, writers like script writers, um, because
script writing is the thing that takes the longest, but it's also very, um, I
have very high standards for the right.
Like the writing is the core of the thing.
Right.
Maybe they can do like the research beforehand.
Yeah.
Yeah.
I need to study a little bit more about like how to do this.
Um, but I think a lot of it is just like coming to terms with being, uh, uh, like
having a significant impact on the landscape or something.
Like the, the attitude I take to my channel psychologically has not changed
that much since I had, I don't know, 10,000 subscribers, right?
And so like now I have 10 times as many subscribers and, um, certain aspects of
my like approach presumably should be multiplied by 10 times, right?
Maybe not all of them, but some of them.
And I don't think I've changed anything by 10 times, um, in terms of like how, how
worthwhile it is for various people to work with me, right?
And this is just like imposter syndrome stuff.
Um, but like, I still have this feeling of like, well, anyone who's able to help
me with writing needs to be a person who like has a good, firstly, is like a really
good writer.
Secondly, has a really good understanding of the problem or can acquire it.
And then part of me is like, and so if that, that's like a pretty impressive
and high consequence person who like, why would they waste their time helping
this like YouTube channel, right?
So now that you have like 10 times more subscribers, if you just like look at
the numbers, you might want to have like a higher standard for people that will
help you will actually be competent.
Yeah.
Yeah.
Basically just feeling that my project is worth it for people.
But the other thing is like with imposter syndrome, you're, you're saying no
for other people, right?
You're not even asking them.
Um, you're like assuming that their answer is no.
Whereas the thing to do is to respect that they are adults who understand what
their time is worth and give them the option and let them decide if they want
to work with you or not, right?
The number of people I've talked to, we'll just say, why don't we give
more money to Robert's math?
Why don't we just like hire more people to help him?
Right.
Right.
And yeah, it's, I hire a, hire a therapist.
Why, why hire a therapist when you have Stampy, you can talk to him.
Stampy is very sarcastic and, uh, uh, unhelpful.
And, um, no, this is like a thing.
Oh man, I have all of these takes on chatbots.
Can we talk about chatbots for a bit?
Sure.
Let's go for it.
Have you noticed that chatbots have like not taken over the world?
Or wait, that's a stupid phrasing.
Chatbots have like not, they have like not had hardly any impact.
I did try a chatbot during COVID when I was like feeling a bit down.
It was really bad.
He would just say like, Hey, have you tried this thing?
I'm like, yeah.
And every day was like, Oh, have you tried this like technique of how to
like debug your thoughts?
And I was like, yeah, I read a book about it.
Right, right, right.
Yes.
Like I wouldn't be here if I hadn't, but you know, you're not most people.
But like more broadly speaking, I think people in the past would have been like,
I don't know, looking at Eliza, right?
Like we had enormous success with chatbots in like the sixties and, uh,
enormous work on them since, and yet they haven't really like reached
wide deployment and not really used for anything.
Um, and I think this is like largely because people keep trying to use them
in the wrong way.
They keep trying to put them in one-on-one conversations.
They're not very good in one-on-one conversations.
Or chatbots are so good that we don't even know that they're like
publicizing the entirety of Twitter.
I mean, I have suspicions, but like, there's just, yeah, it frustrates me.
It frustrates me.
People keep putting chatbots in one-on-one conversations where they're
trying to be helpful to you.
And this like keeps failing because the bots are not very smart.
They say stupid things because they don't know what to say.
And, uh, they're not able to help you.
And so then they just seem stupid.
Uh, so rather than one-on-one conversations where they're trying to help you,
they should be in group conversations where they're not trying to help you.
Group conversations where they're like kind of rude and sarcastic, actually.
Uh, so in a group conversation, rather than like in one-on-one, if you say something,
the bot has to say something, even if it has nothing to say, right?
And so it's speaking because it has to say something.
In a group conversation, the bot can speak because it has something to say, right?
It can just sit there and watch the conversation and be kind of thinking about
things that it might say and just jump in if it actually has something that
it's confident is relevant to add.
Maybe it's the same problem with humans.
Maybe when we talk one-on-one, we just like think of things to say when we just
like just like listen and be in that group and say like, oh, I have a joke here.
It's a harder problem, right?
I think a lot of people find this that group conversations are easier because if
you don't have anything to say, you don't have to say anything.
Same thing with chatbots.
Secondly, if the bot doesn't know, like say you say something to a bot and it
doesn't know what you meant, right?
In a one-on-one where it's trying to be helpful, it has to like try to be
helpful despite not knowing what you meant.
This is very difficult.
Sometimes it says like, oh, I'm not sure I understood this.
Can you rephrase it?
Right.
And it sounds stupid.
Whereas if there's no premise that the thing is trying to be helpful, it's
very hard to tell the difference between someone who can't help and someone who
doesn't want to help.
And so like, if it doesn't understand you, it can be like, what the hell are you talking
about, right?
It can like be rude or it can just like make a stupid offhand joke or yeah,
just like, and so like Stampy is pretty, uh, mean a lot of the time.
Like I wrote him this way.
He's, he's, he's mean.
He, he'll make fun of you.
Um, he's kind of an asshole.
And that's cool because it just means that like whenever he's, whenever he
doesn't know what to say, he can just like disguise it.
People do this too, right?
There's a lot of people who disguise the fact that they're stupid by being mean.
It doesn't work as well as they think it does.
I think this was a thing on GPT three when Nick Kamarata wrote, um, some like prompt
engineering for when the AI didn't know anything about what he was saying, it
would say be real, be real.
Yeah, be real, be real.
Yeah, that's a fun hack.
Um, so, so why are we talking about this?
I forget chatbots.
You want to talk about chatbots and watch highbots are pretty bad.
Yes.
And why people are misusing them.
But having, having a, having a bot in your group chat that just will just jump in.
So like, uh, or the other thing is things like, uh, Siri or like Google, okay,
Google or whatever, Google home.
Um, they, they, they don't jump in because it's just really hard for them to
like hear what's going on in group conversations.
Uh, and also I guess people don't like the idea of being listened to, uh, in
those, in those situations, but in a group chat, it's all text.
It's all legible.
And I really want in all of my group chats there to be, we're not all of them, uh,
many of them for there to be a bot that just like, if you say, if you, if you ask
a question that is Googleable, right?
Like say you and I are, um, talking about, oh, we're going to record a podcast,
whatever, maybe we're not here in person.
Um, and you say, oh, well, how is 2pm?
And I say, oh, what's that in UK time?
I want the bot to just jump in, right?
Cause he knows that's a good global thing.
Um, or like, I want to, yeah, should, should we get a pizza?
There are any pizza places around here, right?
The bot knows this and the, the bot delivers fast delivery pizza.
Yeah.
I mean, like you can, you can be as smart or as, or as dumb as you want to be, but
like, like on the base level, just acknowledging that there's something that
the software can do here and then intervening.
This is so not relevant.
This was like a hobby horse that I've had for a while since like before I was
aware of like actually important problems in the world.
This is like part of why, uh, like it, there's an alternate world where I am
like running a chat bot startup instead of doing what I'm doing now.
You can still go in this world.
Uh, yeah.
I don't know how much demand there's going to be in a like post AGI situation.
You do a chat bot that like helps you understand better AGI safety concept.
Huh, huh, huh.
Yeah.
So that is the dream, right?
That is like where we want to go with stampy.
So at first stampy, thank you for getting us back on track at first.
Stampy was, uh, this bot, this like fairly basic bot that would just like pull
the questions from the thing and post them.
And then I kept adding more and more like it's got this really cool like module
system, you write these different modules and when answers come in, the
different modules kind of bid on like, Oh, I think this is something I can
deal with, um, and it only does as much computation as it needs.
They kind of like allow each other to interrupt.
And it's like a whole thing.
It's on GitHub.
Um, if you want to like help develop a stampy, um, but yeah, I kept adding
functionality to make stampy do all this fun, useful stuff in the discord.
And he has a bunch of functionality.
He'll like search the alignment forum for you.
He has the transcripts of all of my videos.
So like when you're answering questions, you know, like a question
comes in about a thing and you can be like, Stampy, wasn't there, wasn't
there a Rob Mars video where he's talking about like a, an AI doing a backflip
or something and stampy will be like, yeah, yeah, here it is.
Just like give, give the link that kind of thing.
Yeah.
So he's like helpful in that respect, uh, but he's helpful on his own terms.
Did you have to go to like all the trans, all the videos and like edit the
transcript yourself?
No, I just used YouTube's automatic, uh, transcription just downloaded all of
those YouTube deal, pretty easy.
Um, and yeah, and like the alignment newsletter, he's got all of those.
And, uh, yeah, and he'll like search various things.
Um, Wolfram Alpha, he's got like the Wolfram Alpha API.
So you can ask him how many calories there are in a cubic meter of butter or
whatever, if that's, if you wanted to know.
Um, I lost, I lost my train of thought.
Where was I going with this?
So you're going to like stampy useful that like helps people, right?
So stampy started off, uh, he started off literally just taking the,
relaying the questions and answers and maintaining the karma system.
Then he added, I added a bunch of these things just to make him like more
helpful in the process of like understanding, uh, of like looking things up in a
group conversation.
Um, and that's the other thing that's really nice, right?
If you and I are having a research group conversation, there's a really
common trope where like, you're like, I definitely read a paper about something.
And then like, you go off and Google it.
And then you find the link and bring it back and paste it.
Right.
And just having a bot in the chat that you can be like, uh, give us a link to
that archive paper about blah, right?
Give us the scaling law's paper and the bot just posts it.
And then, and then the whole process is like all in line in the chat.
And we both see it happen.
Um, it's just like a really nice workflow that I want to have a
more widely available, but anyway,
I feel like our society is slowly transitioning to, to those kinds of things.
Like, I think with Slack, they started like having those bots in this conversation
where you could just like slash them.
Right.
But the thing I don't like about that is like, there's like a, there's this tool
agent dichotomy, right?
This is the thing that clippy, famously original clippy famously broke is like,
is your software a tool or is it an agent?
Um, tools, you want to be consistent, predictable, reliable.
You are using the tool to do a thing.
Agents can surprise you, right?
Agents can do things off their own back.
Um, and so you want to have an agent?
Yeah.
Like, well, you have to do, you have to do it right.
Right.
Like if you have, uh, yeah, so clippy was stupid because it was like
Microsoft Word is clearly a tool, right?
And trying to do agent stuff badly with Microsoft Word is like, this is what
clip it was, right?
It would like try and do things and help you out.
And it's like, unless it knows exactly what you want, that's not helpful.
It looks like you're trying to open a new page.
Yeah.
It's like, just fucking do it then.
Leave me alone.
Um, people found clippy horrifying, I think for this reason, right?
He like blurred those boundaries and did a bad job.
Um, and like when you have, uh, an agent in a chat or rather when you,
when you have a bot in a chat and you, in order to address the bot, you have
to like use a special syntax.
That's basically, you've like made a command line program that it is present
in your chat and it's much more agenty.
And that's like a much more truly, and that's nice in a sense, because
it's like predictable and whatever, but, uh, I prefer when bots are addressed
with like pure natural language.
And so like Stampy, none of, almost none of Stampy's commands are like,
you have to use this precise syntax.
I have these regular expressions that are like, as permissive as I think
I can get away with.
Like, as long as you use this word somewhere in the sentence, like you can
say Stampy, uh, you, like, if you start or end your thing with Stampy,
he'll know it's at him, or if you reply to one of his comments, or if you
add him anywhere in the thing, then he like will respond to that because
it's directly at him.
But there's not like a specific like slash Stampy blah, this
command or whatever you ask like a human question, which is like, wasn't
there a paper about this?
Or like, I think there was a, you know, deep, what do you know about blah?
And he'll like look it up on the alignment forum, you know, wiki or whatever.
Um, and that's the kind of thing you can say, what do you know about X?
And because I have this module system, um, a bunch of modules will respond to
that and they have priority.
So like if it finds something in the alignment forum, that's like high priority.
If it finds something by Googling, that's like lower priority.
If it doesn't know what to do with any of those, then there's like Eliza will
just like take a swing at like rephrasing the thing you asked as, as
another question or whatever, you know, or like make some rude remark.
Um, and so I keep losing what I'm talking about.
Right.
So basically the thing, if you can like access it through using reg X and
hmm, easy to like look up useful information ranked by, you know, if
they're aligned for on more and wiki, yeah, yeah.
So, so I just think it, I just think it's nicer if you can talk to them as
though they're, uh, people, I prefer that as an interface.
But anyway, we keep getting sidetracked from like where I was actually going
with this at first stamp.
He was very simple, right?
Now he has a bunch of extra functionality.
The dream is with language models going how they are and like the large
language model based, uh, conversation agents, things like Lambda, um, the
technology is advancing now where I think that there's the potential for
consciousness.
There's the potential for, we could talk about that if you like, but, um, so
then I don't, yeah, I said, I mean, I made a video about it.
I, it's not, it's not as interesting as everybody seems to think it is.
But so, so there's the potential for something where we have like a
giant corpus of hopefully giant corpus of like a very large number of
questions people have asked about AI safety and high quality human authored
responses.
You then use that probably you don't fine tune on it.
You probably do something cleverer, like, um, one of these architectures,
like there was one, was there like trillion tokens thing on recently and see
like, wouldn't it be nice?
It's not palm.
Uh, I don't think.
Oh, that's really the one.
Yeah.
See, if stampy was here, just like over there, they're like, what was that?
There was like a paper that was like something learning from something with
a trillion tokens and he'd be like, here it is.
It's a link.
Um, uh, things were like, rather than, basically you, you want to get around
this whole like memorization thing, right?
They're like large language models to just transform as they memorize an
enormous amount of the dataset.
And this is kind of a waste of parameters.
What you actually want is a system that can dynamically look up from anywhere
in the dataset, what it needs and do kind of the, like, there's, there's
like a really, there's like a hacky thing you can do with GPT three where
you give it and stampy does this to some extent.
Um, you give it the question, it uses that with some kind of semantic search
on its dataset to get some text and then it just loads that text into the prompt.
It just like stuffs the prompt with stuff from its dataset.
Um, so that then when it generates it, like has more context that's directly
relevant from the dataset without having to have memorized it, it, it hasn't
rather than memorizing the dataset, it learns a procedure for searching for
things in the dataset.
So it knows that, you know, the dataset contains like element forum.
And so it will put stuff in the prompt that says Alan forum.
Yeah.
Yeah.
So like you ask it, let's say, let's say I ask stampy a question in this kind of
format, um, there's a bunch of just like standard, maybe it's like language
model based semantic search to look through stuff we already have, maybe
even search engines and pull in some relevant information.
And then the prompt is like, here's the conversation.
The human asked this, the bot then did a search for relevant information
and found the following using this information.
It replied completion, right.
And then it's like able to use this data, uh, without having to have memorized it.
So it knows like all the question answering from the internet.
Right.
So that's the hope.
The hope is one day, eventually, uh, when we have a nice set of these questions
and answers, we could then have a bot where you could just like go on to stampy.ai.
And instead of it being like an FAQ, it's just like a chat, a text box, you
can have a conversation with stampy and stampy will tell you about the various
problems and you can answer your questions.
Uh, we can't do that right now because language models make things up constantly.
And it's like, you can't just present language model completions as though they
have any kind of authority behind them because yeah, the system has a tendency
to make things up, but if you can get good enough at being like constrained to
say things that accord with the human written answers that you already have,
then you can get something that's more conversational than just like here is a
pre-written answer to your specific question.
Um, I think people get a lot of value from just using the three API.
Um, even if they know that GP3 is full of shit sometimes.
And yes, with the new, um, you know, DaVinci 002 instructor GPG, um, is less
full of shit, right?
And so it gives like meaningful answers.
So I feel like if you put the stampy online, people will can use it without
like knowing, oh, this is like the, the ground truth.
Oh no, I will post it on Twitter.
Yeah, but don't, I mean, you can't, the more foolproof you make it, you know,
the universe will provide you a bigger fool.
Yeah.
Um, now I think I've finally figured out where we're going.
So we were talking about why, like you're building, um, you know, like having
some like bot that could like teach AI safety could be good.
Like having a chatbot that, you know, give answers to meaningful AI safety
question was useful.
And actually, yeah, the end game here is having like, um, Robert Miles, but in
language model, so you, you're trying to like digitalize yourself with
something that could like answer all the questions, look at all the papers and
like look at your videos and give like meaningful answers to everything.
So basically you're just like scaling up Robert Miles to like all the
possible questions in all the group shots.
Yes, something like that.
Uh, and like ideally I would like to not just scale up myself.
I would like to, there's some kind of HCH thing.
What I actually want is not a simulation of me, but a simulation of me with a
bunch of time to think and also, uh, group chats open with a bunch of actual
like alignment researchers.
You know what I mean?
It's called illiterate.
Yeah.
I think they have some like bot there, but I think their bots is kind of
sometimes jumps into some conversation and say some like weird and funny things.
Um, but there's like some people talking about the islander.
Um, we should set up a chat between Stampy and this bot.
I think that would be funny to see.
I think the guy on the inside is bone MPT.
If you're listening to this, um, if we just switch from talking about
Stampy, which is great, but, um, to like how to do good AI, I'm a research in
general, there was like some question on Twitter, um, which was imagine a good
AI safety outcome, what are your probabilities of reaching it's via policy
coordination means versus the pure technical research.
And this is because, um, he wants to know what is the ideal target audience.
So if you, if you want to like do more like communication efforts for, you know,
policy people or more like technical people, what do you think is like the
balance between like, um, policy technical and what level of attraction?
I think there's like a lot of questions too.
Okay.
So asking like, are we going to solve this by policy or are we going to solve
this by technical means?
It feels like, are we like, are we going to solve this problem by writing
the program or running the program?
Like, well, if we haven't written the program, we've got nothing to run.
If we write the program and never run it, we may as well have not written it.
Right.
The technical thing is about, can we find solutions?
And the policy thing is like, can we get people to actually use the solutions we
found?
No, the policy thing is about having people not run the AGI.exe.
Yeah.
I mean, like, obviously they're both important, but they are also like
mutually reinforcing is my point, right?
Um, if, for example, as part of policy, you were like, um, maybe we can like slow
down capabilities research a little bit.
That's cool.
But like, all that's doing is buying you time.
So you need the other, you need the technical alignment research to be
happening and like, those are exactly, well, not exactly, but they're like
approximately equivalent.
If you can, you can double the rate of alignment research or half the rate of
capabilities research, those are like approximately the same thing.
Not really because right now we're have maybe 110,000 times more
a capability researchers than like alignment researchers.
Yeah.
So maybe we want to get more time so that, you know, people can work on this
when there's like an actual field doing useful things.
So if I think like getting like an extra month or an extra two months is very
important because then I don't know, like, as I said, like in the next like, in
the last like two months, we might do like the most progress we've ever done
on the alignment, right?
Right.
Oh, so is your, is your, is your thinking that like the rate of progress
in alignment is growing rapidly and therefore additional time is like more
than linear in additional research or something like that?
I think we're doing like somewhat a good job right now.
There's like more people going to the field and we just need to like scale it
up faster, but it's just like, if we tell everyone, like, oh, yeah, just like
do pure technical research now, there's like not so many people doing it.
Yeah.
Yeah.
So I feel like that's why I do alteration.
You do a little bit of as well because I think we just like need more people.
I'm not everyone.
I've heard the good arguments.
Yeah.
We need more people.
Uh, we need more good people.
Right.
Like I think the rate of progress is primarily determined by, is this true?
I mean, we just need more people, but like, I think, I think it's weighted
towards the like high quality end, right?
Like we need people who are like able to do the highest level work
and make progress on the hardest parts of the problem.
Um, and every, having more people broadly speaking helps.
It's really good to have more people supporting all of that.
Um, but yeah, on the margin, it's like you want it weighted by quality in some way.
If you have like more regulation, like an AI, a lot of tax, some companies,
like if more companies were like forced to have like a lot of researchers, I know
we don't currently have enough food at Google or, um, those kind of places.
But, um, at the end of the day, some people might want, you know, some regulations
because I think that's how we thought climate change.
We had like carbon tax, those kinds of things.
If we, I think that's one way we, if we could get like more people, I think if
we just like him trying to grow it from, you know, effective autism, that's wrong.
We might get to like 1000 people in two years, right?
Yeah.
But the thing is we understand climate change.
We understand.
Yeah, as well.
Not really.
Like that's the thing, right?
Like when it comes to climate change, the scientists did the research, they figured
out what the mechanism is.
It's carbon dioxide.
The thing we need to do is release less carbon dioxide or like figure out ways to
remove it from the atmosphere.
Then you can pass that on to the regulators and say, like, what can we do?
How can we intervene on this to reduce carbon dioxide emissions?
And the regulators can like come up with some change in the incentive structures
of society and industry to reduce that.
With alignment, we don't have that.
We are at the point of like, huh, getting very hot around here.
What's going on?
Right.
And maybe we're a bit better than that.
But like genuinely, the difficult thing right now is understanding the nature of
intelligence well enough to know what we can do that would help.
If I could pass a law requiring all of the AI organizations to like implement some
specific technical policy, I don't know what specific technical policy I would
have them implement.
Because we don't have that yet.
Some policies like you need to make your system interpretable so that you can run
them on the streets of London.
Yeah.
And like, okay, interpretable is good.
I'm excited about interpretability research.
It seems like an important component.
Oh, I don't mean this as a normative thing.
I think there's like an actual thing that people are trying to like pass and like
your regulation of, you know, if you build like deep learning systems and you
deploy them on the driving car, then you need to make it interpretable.
Yeah.
You only could make jokes of the thing because you think it's like impossible
for deep learning to be fully interpretable.
Um, yeah.
So we talk about like AI safety progress.
Another question Twitter was, what are you most excited about in terms of progress
on AI safety recently?
You also said that safe AGI is possible.
How sure are you that it is actually possible?
Well, what do you mean by possible?
I'm just talking about this guy in Twitter, but I would say possible means it's
possible that we run this and we're not dead.
Um, sorry, we, we, we died on normal, uh, death time.
Right.
Okay.
Yeah.
So there definitely exists a program, probably a short program by short.
I mean, you know, it would fit comfortably on any modern computer.
Um, not would run comfortably, but like the code, um, which is like an AGI
superintelligence that's fully aligned that ushers in a golden era of in like
solves all our problems and is perfect and we get the good outcome.
That program exists.
Right.
This is because the space of programs is deep and wide and long and like add a hundred
billion other words for all of the other dimensions.
It's very high dimensional space.
Right.
Every program is in there.
Perfectly aligned AGI is a program that exists and can exist and can run on
computers that we have now.
It's just like, are we going to find it?
A program, a program using deep learning maybe?
I think.
Yeah.
And so like deep learning doesn't, deep learning doesn't let us in the
standard in the standard approach of deep learning doesn't allow us to find that
program, um, because in deep learning, we optimize over the input output behavior
of the system and, uh, we optimize over the input output behavior of the system
in the training environment in the training regime.
And you just don't, that just is insufficient.
Again, the space is very large.
There's a lot of programs that perform very, very well and very safely and very
aligned to Lee in the training environment for whatever training environment you
create, um, and you don't know how those behave off distribution and you can't
deploy them without moving them off distribution.
So are you saying that basically in this like giant space, we might end up on
something people call meso optimizers.
Something you have a video about it.
I do.
And I, um, I'm happy with that video.
That's like, if people are interested in meso optimizers.
They should watch those two videos.
Yeah.
Maybe you could do like a little finger thing to people.
It's right here, possibly here, depending on how much space I have in the frame.
Uh, yeah.
And so like that's like one, that's one part of it, but like, that's like one
specific way in which this happens is kind of a broader family of these things.
Um, there's just a lot of ways that once the thing is out there and it's an AGI
and it's like doing its thing, everything changes and you just can't be
confident.
So, okay.
For example, there are a bunch of agent, uh, there are a bunch of alignment
approaches that are not like sub agent stable.
Um, and so they can like work perfectly and then fall apart later on.
Like, so you've got like, um, so you've got an AI system that's, um, well,
like training an AGI and it plays various games and it, uh, you know,
we put it in various environments and suppose that we as human beings, we
just have like a real, like we think it's really morally wrong, certain
configurations of chess pieces, right?
Whatever it is, like these pieces are in a grid, like this shape, then
that's like horrible, very bad opening, very bad.
And we don't want that.
Um, and we train the thing to play chess, right?
We train it to play chess.
We also train it as part of our alignment thing, not to do this particular
configuration, right?
And we're also training it to do a million other things at some point.
It's deployed into the real world, right?
Let's say the point that it's like about human level intelligence.
So it's like playing chess in the same way that we play chess at sensible
human level.
Um, but you know, we also trained it at writing software.
We also trained it at, uh, you know, theoretical computer science.
We trained it at a bunch of things because we're trying to make an AGI here.
At a certain point, possibly in deployment, it realizes that the
cognitive architecture that it has is not particularly well suited to playing
chess.
And if it wants to win at chess, which it does, it should write a program that
does like, uh, min-max and, uh, alpha beta pruning and like whatever.
And it's like, it basically just like writes itself stock fish, or honestly,
because realistically, all of these things happen much faster than we expect
them to do.
It realizes that it can download stock fish, right?
One of the two, but suppose it's writing this code, is it going to include
this preclusion against producing this particular configuration, right?
Maybe, maybe not.
Probably not.
Right.
Because depending on like what, here, we have to anthropomorphize, but like you
can't tell if you've created a system that like really has an actual deep
disagreement, uh, like, uh, uh, uh, uh, uh, uh, that it actually wants that
configuration, not to happen versus just that, like, when you were training it,
whenever it was in that configuration, you gave it like a negative reward.
And so it like, it's the equivalent of like you're trying to play chess and
whenever you have that configuration of pieces on the board, you're like, have a
horrible headache.
And so you avoid it.
But if looking at that configuration of pieces gives you a horrible headache, that doesn't
mean you're going to try to avoid it from ever being considered in this program that
you wrote.
Right?
And so two things happen here.
Firstly, the system stops having the alignment properties that you thought it had that it
always displayed in the training environment.
And secondly, it becomes drastically more competent because stockfish is way better at
chess than it was before.
So you get this simultaneous thing of the system becoming tremendously more capable
in a bunch of domains, probably, and simultaneously just not being aligned anymore, according
to the things that you thought you'd managed to instill in the training process.
This is the kind of thing.
It's a problem here that we're out of distribution because now he's doing other things.
And or it's a problem that, I don't know, he was pretending to do the right thing in
the beginning and yeah, I mean, is it pretending or is it like we aren't so we are misoptimizes
with respect to evolution, right?
And so what are we doing when we like use contraception, right?
We're not like trying to deceive evolution necessarily, but then evolution isn't an agent.
I don't know.
This is kind of confusing to think about.
But like regardless of what its internal experience is, it stops behaving in the kind of aligned
way that we trained it to once it and like you could call this like self modification,
but it's also you could also think of it as it's just like moving a bunch of its cognition
outside of the part that you originally trained, right?
And that's you would expect that to be true, right?
If you have full, if an AGI has this kind of cognitive morphological freedom that it
can just configure its computational resources into whatever structure it thinks is best
for the task that it's doing, it's going to have a bunch of these kind of right, it's
going to play chess with a specialized module because that's like cheap to do and it's going
to be way more efficient than trying to reason through with its general capabilities.
And so you can expect that kind of for everything.
And yeah, you could call this self modification or self improvement or whatever it's or building
a sub agent, right?
And either way, just the alignment problem, we don't currently have a way to be confident
that the alignment properties hold through that kind of like dramatic shift that you
expect should just happen at about the point of human capabilities when it becomes smart
enough to realize that it can and have enough programming ability to do it.
And this is again, like that's just these are all just like different ways that this
deep problem can manifest itself.
And so like, there's a lot of cool research about how to align our current systems that
doesn't solve this problem.
And I just, my main line prediction is that we get alignment stuff that like looks pretty
good and works pretty well until it doesn't.
And for now, we haven't seen like any particular example of a misoptimizer in the world.
So for now, it's just like, I guess that the thing exists or accept us.
So okay, I don't fully buy the evolution analogy.
I don't fully understand it later.
So I talked to even Ubinger on, I guess, the second episode of this episode, we talked
about evolution.
People told me that I didn't understand exactly what he was saying.
But I guess my understanding is that as humans, we somewhat do something for evolution in
the sense that like our brain, like maximize some like dopamine or any like neurotransmitter
in our brain, because it thinks that this will maximize, you know, our ability to like
be strong, be healthy, survive, and maybe have kids again.
So we as humans think like, oh, we're doing like something like crazy, we're just like
falling in love, watching movies, doing YouTube videos.
But at the end of the day, our brain really wants to reproduce.
So we don't, we don't actually change from, you know, the ancestral environment, which
we're still doing offspring thing, but we just like believe that we're doing something
great.
But it's just like falling in love is just like a good thing to maximize long term reward
of, you know, more offerings because
Yeah, no, I think that you're, I think that you're collapsing metal layers here, that
like, you're talking about what your brain thinks, and also what you think.
And I don't believe that your brain thinks anything that you don't think I like my brain.
You are your brain.
And so if your claim is that your brain is like actually thinking about inclusive genetic
fitness, then like you should have thoughts about inclusive genetic fitness.
And like you don't, you have thoughts about like sex is fun, right?
Like this is not, although you have the capacity to think about inclusive genetic fitness,
this is not in fact the computation that you're carrying out when you decide to do the various
things you do in your life.
Why do I think sex is fun?
I think sex is fun, because my brain produces something like transmitter whenever I'm doing
it, because my brain has evolved throughout the evolution, and it was good to like release
this particular thing to make me do more offering, right?
And the concept of fun depends on like what the brain produces.
Yes, this is the, this is the, this is the link between evolution and your thoughts goes
via this thing, right?
But you aren't making your decisions based on thoughts about what evolution wants.
Yeah, my claim is more that phasic dopamine in the brain gives that it's kind of similar
to like a reward and regular learning, right?
So it's kind of the like reward that trains the neural net that's in my brain.
And in some sense, evolution has created this reward system and my brain changes over time
based on this reward that I'm getting.
So I don't think there's like an actual like agent, agent that is like, oh, what is the
most fun?
What is the meaning of life?
I think it's just like some neural net that learns stuff through like ARL or deep learning.
And at the end of the day, end up with like some policy.
The policy is not great, but the policy is not like thinking like explicitly about evolution.
And it's kind of, it has been trained by something that like was created by evolution.
Yes.
And therefore it's correlated.
And they're definitely correlated, but they're not the same.
And the differences become larger the further, the more power we have, right?
And it's in that delta where you lose everything.
It's, it's, I mean, it's good heart's law again, right?
It, you know, and this correlation was much, much stronger in the ancestral environment
than it is now, but like the optimal inclusive genetic fitness route is something that almost
nobody does, right?
And what's more, the other thing is it's like completely computationally intractable, right?
Of course we're approximating something like inclusive genetic fitness because you can't
do that computation, right?
You have to use, you have to use heuristics.
We're like so bounded in that respect.
And that's all it is.
They're different things.
And if we have the freedom to do, to configure the universe however we want it to be, we
get further away from inclusive genetic fitness.
And what's more, this is like, it's completely that like the argument from inclusive genetic
fitness is so unconvincing, like you could say to me, hey, you should like, you should
like put all of your resources and all of your life priority into like donating sperm
and like just trying to like have as many offspring as possible, right?
And you can make an argument about that, about how many offspring I would end up having
and my relative inclusive genetic fitness and how that would be enormous.
And I don't give a shit, right?
It's not what I want.
I don't want that.
I want something else.
And we want more kids, right?
Yeah.
I mean, don't get me wrong.
I'm like pronatalist, but they're not like a terminal thing.
They're not the only thing that matters.
And as far as evolution is concerned, it's the only thing that matters and like we care
about other things.
And so yeah, you end up with AI systems that just care about things that aren't human values.
Okay.
So I think I kind of can make a general like steelman argument of something you're saying.
And I think I see where we're disagreeing.
So basically you're kind of at the individual level where you're saying that a human is
misaligned with evolution.
Yeah.
A specific human is not as you is not like thinking explicitly in terms of like maximizing
offsprings and we're pretty bad at it individually.
Yeah.
I guess my claim...
But we're not just we're bad at it.
We're not trying to do it.
We're not trying to do it at all.
So I guess like that's a good analogy for AI.
So the AI could have like some different like objective than the one in training.
So I agree with that.
I guess my general claim is that humanity as a whole is pretty good at maximizing survival
or getting like...
And I feel like what evolution was at the beginning, it was like kind of please survive.
Yeah.
So we're getting like good at like making things, building technology.
And so we're kind of like being good at like the goal of like surviving somehow.
Yeah.
We want to survive.
We want to survive for a few reasons.
One of them being evolution.
The other one is instrumental convergence, right?
If you want, well, yeah, whatever you want, surviving is probably a better way of getting
it than not surviving.
And of course, evolution is still happening, right?
It's not as though every gene is equally prevalent in every generation of humans.
Evolution is happening.
It's just that evolution is very, very slow compared to us.
So evolution doesn't have a lot of time to do whatever it's going to do before we do
finish doing what we've been doing for the last two, three hundred years of just like
completely in an instant from the perspective of evolution, diverting the part of the world
to what we want instead of what it wants.
And it's kind of weird because at some point it's not really an agent anymore.
At some point we just can change our genes and create kids as we want.
And at some point like evolution will kind of disappear.
Yeah.
I don't know.
I think something it would like change drastically, but it's changed before.
The evolution of like random RNA molecules is pretty different from like multicellular
sexual selection, whatever.
There will always be, as long as there's competition, there will be selection pressures.
And I mean, yeah, talk to Robin Hansen about that, right?
But the question of like, yeah, do we want to expand into the stars is like, well, as
long as some of us do, then that's what's going to happen for those people, anyone who
stays behind can if they want and become mostly irrelevant, you know?
Do you want to go into the stars?
Um, I honestly don't expect to live that long, but yes, if I could, yeah.
And it's a great transition to another Discord question.
Are we doomed?
What's your probability of doom?
Um, I don't know.
I think it doesn't look good.
It's fair to say it doesn't look good.
Like my mainline prediction is doomed.
Like if I take, if I think about the coming years and I just assume that the single most
likely, like if I do a greedy, um, a greedy path through this, where at every instance,
the single most likely thing to happen happens, I think we're doomed.
Um, there are various places where various unlikely things can happen that might lead
to us not being doomed.
Um, what are the like concrete paths to not do?
Uh, well, we might have some, uh, some deep breakthrough on the kind of problems that
I've been talking about that points to a way to do alignment that will actually hold up
under this, like, uh, under these extreme circumstances of, uh, AI superintelligence.
Um, that's like the main one.
Uh, I also place a bunch of probability.
I don't know, maybe, maybe 10 or 20% probability on just us being really wrong about
something really fundamental in the lucky direction.
And like, I place pretty high probability on us being really wrong about some important
philosophical thing.
It's just that there's no particular reason to believe that that makes us less doomed
rather than more, right?
Like broadly speaking, if you're like trying to design a rocket to take you to the moon
and you're like, in some sense, badly wrong about how rockets work, this mostly reduces
your chances of getting to the moon.
Um, but we're talking about aligning rockets.
Um, then we can be wrong about how easy AJ is.
Yeah.
Yeah.
Right.
Like some kind of thing where like, you could imagine some situation where we have some
argument where it's like, and then when we get to the moon, we'll fly down to the
surface with this plane type thing.
And some people being like, I don't know if that's going to work.
I'm pretty sure there's no air on the moon.
And other people being like, well, yeah, okay, but we'll include the wings just in
case and we do.
And then it turns out we were like super wrong about the moon somehow and it has an
atmosphere and the wings help.
Right.
Right.
Um, that kind of thing.
We can be wrong about the reasons why some of our approaches won't work.
Um, but that's like, I don't want to rely on that.
Maybe the azimuth tree rules were kind of useful.
I didn't, I didn't know that maybe kind of actually work.
Yeah.
Yeah.
I mean, I, you know, things have shifted a fair amount.
Um, I remember another question was something about timelines, something like, if
you believe in short time lines, it said five or 10 years, was the best way to do
AI research or useful work?
That's like a pretty, that's a pretty deep strategic question, which I do not know
the answer to.
Um, I think.
Uh, I think we need a broad, I think we need a broad approach.
I think we're uncertain enough about enough things like the number of people we
have working on this problem is so small relative to like the breadth of possible
approaches.
There are like, if you, if, if, if, if humanity were sane, I mean, a lot of things
would be different.
That's a, that's a wild counterfactual, but like there would be no tick time.
Yeah, probably not.
Um, uh, how about this?
If humanity were allocating a sane level of resources to this problem.
Uh, in terms of, uh, person hours, competence-weighted person hours.
Um,
Compens, competence based on either number of stamps.
Uh, how close you look, uh, how close you match Elliot Rieckowski with our picture?
Uh, yeah.
And that's the wrong comma.
Um, uh, I don't know.
Uh, there would be people, there would be like large teams of very competent
people, uh, working on all sorts of different approaches to this.
And like, I honestly don't have the, I don't have the confidence to be like,
this is the kind of research we should be doing.
Right.
Like it's totally possible that the thing that if we had enough time to work on
this and we worked on it for another 10 years, we would be like, oh yeah, it turns
out the promising avenue is just like not really any of the things that people
are currently working on.
So we still need more breadth than we have, but we also need way more depth in
all of the things we have as well.
I just think we need, we need to be putting so much more into this than we are.
How do we get more people, except from, you know, publishing videos more regularly
sometime?
Yeah.
Uh, well, that's the approach I'm taking.
Um, I think just getting people to understand the problem is huge, getting
smart people to understand the problem is huge.
Um, and also getting people to understand how interesting the problem is.
This is the thing that blows my mind.
AI safety is like, obviously the most interesting subject in the world.
I would say the most important problem to be working on, but maybe on a technical
perspective, maybe the equations for, you know, the current like states where we're
in is not the most beautiful math.
Maybe like some mathematicians prefer to like work on group theory.
Maybe some people prefer to like look at cool belly outputs.
Yeah, fair.
Uh, I don't mean what I mean.
I mean, there is a particular type of person who is attracted to a problem more,
the more difficult it is, right?
It's called a speed runner.
Yeah.
People who, yes, people who like a challenge in their technical work.
Um, and it actually doesn't get harder than this, right?
In like current problems or like plausibly, there are harder problems, but
like that nobody cares about that like aren't important.
Um, like if you want a problem that cuts to the cuts to the core of like all of
the most important questions in philosophy, all the most interesting questions
in philosophy, questions about ethics, questions about identity, intelligence,
consciousness, um, and, uh, values, um, like, like axiology, the core questions
of like, what are we even doing here?
What are we trying to achieve?
If we could do whatever we wanted, if we could have whatever outcomes we wanted,
what outcomes do we even want?
And then all of those questions about like, what does it mean to be?
Uh, a thinking thing in the world, what does it mean to want things?
Um, and then all of these technical challenges, right?
How do we understand the most complex and confusing things that we've ever
created in history?
How do we understand the most complicated object known?
On a deep enough level to replicate it.
How do we do this?
So well, so comprehensively that we avoid destroying ourselves.
And how do we do this on a deadline?
It's interesting.
It is philosophy on a deadline, but the problem is most AI researchers, I think,
hated philosophy classes because they didn't have like an actual like solution.
Yeah, prefer to like red code and have a correct output.
Yeah.
I mean, philosophy has a lot of problems, but like, I think philosophy gets a bad
wrap mostly because it's the bucket into which we put the things that we don't
understand well enough to study properly.
Like there was a time when, when philosophers were studying questions like,
if I drop a feather and a cannonball, which one is going to fall faster or whatever.
And this is like a philosophical problem until you make a, you have a philosophical
insight like empiricism and go, well, why don't we just like, just fucking do it and see.
This is how Nike was created.
Let's do it.
Just do it.
Just like, yeah, have a look.
And so my point is like, people sometimes use that to like bad mouth philosophy, but
like somebody had to come up with that.
Turns out that's not obvious.
It's obvious in retrospect.
People spent decades, possibly centuries sitting around thinking about how to learn
about the world and the idea of like, maybe try and like think about what would
happen and make a prediction and then test it by doing it is like something
somebody had to invent.
That person was a philosopher and they like made an enormous progress.
This is like a huge thing, especially on the internet of people being like, oh, yeah,
but that's obvious.
That's trivial.
It's like, well, you couldn't have told me it before you learned it.
So it's like, yeah, it's obvious in retrospect.
Whatever.
The point is, once you have made these philosophical discoveries, such that you
actually have like a paradigm with which to learn more and operate, like, you know,
then that's now that science is not philosophy anymore.
Right.
It used to be philosophy until we understood it well enough to actually like
make progress and then it split off.
Um, and we've seen like increasingly more and more things steadily splitting off
from philosophy and being becoming, you know, mathematics, becoming the various
branches of science.
And, um, is the claim that we need to do some philosophical work to like map out
the problems and then we carry it like subfields of alignment.
So then people can like work on this because they're like subfields of AI,
you know, just like someone talking on that's wrong.
Right.
Right.
The job of the job of philosophers of AI, like the job of firefighters is to
make themselves redundant, right?
Their job is to, is to grapple with the ineffable until you're able to eff it.
They're already out of job because there is no fire alarm.
So, so, so the point is we want to do, um, yeah.
So the job of the philosophical work is to, uh, become not confused enough, right?
To reach a level of understanding of the thing that we can do maths on it, for
example, or computer science.
Um, and yeah, the philosophy side of things is, um, we don't want it to, we
don't need to stay philosophy, right?
We want to like graduate to the point where we actually have like ways to
make progress on the actual problem.
How would you like advise people to do more philosophy on AI?
Should people just like read less wrong?
Should people like watch your videos and become philosophy PhDs?
Uh, I don't know.
I don't know that I have specific advice about that.
Um, like there's some, there's some fun.
So there's a whole area of research or like a sort of family of research that I,
uh, that I quite enjoy.
That's like taking these kind of fuzzy concepts, uh, that we're using and just
like trying to formalize them to something that you can like prove theorems about.
Um, something like, um, uh, recently I was looking at some work by, by Alex Turner
about, um, formalizing the idea of like power seeking.
We have this idea of, um, instrumental convergence that like, uh, agents with a
wide range of different goals or rewards, uh, functions will, uh, tend to seek power.
Right.
They will try to gain influence over their environment, try to gain resources.
Um, and then like, there's like a philosophical argument, um, which
Alejandro made, which Boston made.
And, um, and then it's like, okay, what does this mean concretely?
How do we, how could we prove this?
How could we prove this like mathematically or in computer science as opposed to in philosophy?
And that means like taking all of these words and turning them into like Greek symbols.
No, you need like really, really concrete definition.
So you need to like specify exactly what you mean by everything such that there's like
no remaining ambiguity about what you're talking about.
And then you can start proving things.
And it turns out you can in fact prove, um, that optimal policies in a wide range of
like MDP environments, um, exhibit this behavior that is like this formalized concept
of what it means to seek power, which is I think about like the number of possible
reachable states.
Oh, so it wants to be able to reach all the states of the MDPs wants to like be able
to like survive and have like power over.
Yeah.
Not exactly that it wants to reach all of them.
Cause like a lot of the states are like bad outcomes that it doesn't want, right?
But it wants its decisions to have an influence over the state of the MDP.
So like if you are turned off, then now you're in this one state, which is off.
And you know, on the, on the graph, you have like one arrow coming out from off
that loops around back to off.
And like, um, in principle, there's no, like, well, you might think that this is just
another state that you could be in.
And without actually including somewhere in the goal, don't be in the off state that
this is like that there would be about as many optimal policies or about as many goals
for which the optimal policy involves being turned off as there are the don't.
But it turns out, no, there are like many, many, many more goals for which the optimal
policy involves avoiding being turned off.
Um, and you know, this is provable and has been proven.
This like general, but like family of thing, I think is, uh, potentially helpful because
it lets you prove, uh, you kind of forces you to, to be really rigorous and clear on
what you're actually saying and what you aren't saying.
And you can kind of discover things this way.
So you move from philosophy to math, try to formalize things in terms of like
simple FNP and so then it forces you to like understand better the problem.
And I feel like that's one thing Paul Cristiano is doing a little bit, which is
trying to go from, you know, scenarios where AI could destroy humanity or just like
have some like mesoline behavior and then come up with like concrete plans on like how
to contract this.
And by doing this like concrete thing and like, what is like the most simple problem
that could like be misaligned and then you can like, you know, find the most simple
solution to this and then you can like continue this, this process until you run
out of like actual problems that could come up.
Yeah, you got to be careful with that though, right?
Uh, so this is like, I don't think that Paul is suffering from this problem, but
there is a problem that some people have of like, I can't think of any way that
this is dangerous and therefore it's safe.
Um, and yeah, no, Paul is not doing that.
But like taking that general approach of like, think of a way that it could go
wrong and then fix it and then try and think of another way that it could go
wrong and then fix it.
It's like, you have to be doing that at the right level of abstraction.
Cause if you have a system, which is actually, which is not aligned, it's
like, think of a way that this might not be aligned and fix it as opposed to like
think of something an unaligned system, a misaligned system could do and prevent
it from being able to do that.
That approach is like, I think a losing proposition.
So it's not about like agents not being able to do something bad, like poor
thinking, it's mostly like imagine we're running like the simulation we're actually
in and what would be the kind of AIS people build and in those scenarios with
like those architectures, what would be like the obvious thing that could go
wrong, what would be like the kind of things people could develop that could
like lead to like a failed case.
Right, right, right.
Yeah.
I think that's a good approach, but also I don't know what I'm talking about.
What's Rob's approach to it?
What's my approach?
What's the problem more broadly?
Yeah.
How do you like think about the kind of problems we could have?
I know you said something about like Mesa optimizers being kind of a big chunk
of why you think where do you, you, you said something like it's, yeah, there's
like a broader thing of which Mesa optimizers are like a particularly concerning
or salient example.
Do you have like other things in your threat model, other ways of coming up with
like dangerous plans or you mostly just like read Alex Turner and Paul
Cristiano and think about their models?
Uh, I mean, I try to read a wide variety of stuff, but it's difficult.
There's a lot being published now.
Um, the other thing is like, if I'm trying to be sort of an introduction to
the field for people, I don't want to be too specifically opinionated about what
types of research I think are promising.
Like firstly, because like who the hell am I, right?
I have this like disproportional, uh, disproportionate visibility and
disproportionate respect, to be honest, right?
Like people care more about what I have to say than they should.
Um, well, I think people in the alignment community, um, don't treat you as more
respectful than, you know, the people who like created the film.
No, yeah, absolutely not.
And I guess people on YouTube are just like excited about your videos and just
like respect you for like how useful that you are to them.
So I think in that sense, you're at the right level.
I think people outside of the field think that I have more, what am I saying?
People outside of the field tend to think that I'm like actively doing research.
Right.
And like I decided to focus on communication instead of doing research
because I thought I would have a lot more impact that way.
And that seems to be the case.
Um, but then they want to hear my like object level opinions about all of
these research questions and like, I'm not the person to ask.
My take is that if you're like a researcher working on like one paper,
you end up like maybe doing research on like this is very specific thing for
like six months or a year.
And you go into like this rabbit hole of being like the person to talk
about for cooperative investment from learning.
But if you've done like 20 videos or 40 videos on a safety, you have
like a broader range of topics.
And if you like had to like explain those topics very precisely in like a five
minute video, then you end up like having a good understanding of the
stuff is maybe you're the person to talk to when it is about, you know, the
beginner concerns about a safety.
Maybe you know, like high level questions.
Yeah.
Okay.
Um, perhaps like I have, I do have like a broader understanding than some, but
I also think that like most researchers, yeah, they know every, it's like most
researchers know everything about one thing and a bit about everything else.
And I just know a bit about everything.
Right.
So like, I think my understanding of, uh, a randomly chosen topic is
like not better than the understanding of like a randomly chosen alignment
researcher about a randomly chosen topic.
Um, that isn't their own.
Um, I'm really just like trying to communicate it to the outside world.
You know.
Yeah.
I think it's fair enough to think that people should, you know, go through a
specific researchers paper to know like how to act on like this kind of thing.
And should not like defer too much on, um, your opinion on this specific thing.
If you like have not spent like more than a hundred hours on this.
Um,
And there's also a thing where like, uh, because I'm trying to be like
reasonably impartial, um, I don't like to really give a bunch of specific
opinions about that.
I've been doing this for years.
And the thing that's weird is like, it turns out, if you don't give for your
opinion for long enough, you stop having an opinion, like your mind stops
generating opinions because you're not using them.
And that's like, it doesn't reflect very well on me.
Except, except when you're on the chart and you need to be on a square.
What?
When I'm asking you to be on a short, then you can say to me, tell me like,
oh, I'm in this square and not this one.
I'm talking about the shirt.
Oh, right.
Sorry.
Uh, yeah, right.
Exactly.
That's exactly it.
Right.
And that was like the first time that I had like thought of myself in these
terms of like having an opinion about where these things are.
And I had to like, had to like decide, but this is like kind of a worrying thing.
I think it says a worrying thing about my brain and like,
by, um, by generalizing from myself, uh, what's that called?
Self.
Generating from one at a point.
Yeah.
There's a name for it as a fallacy or something.
Anyway, whatever.
From that perspective about probably a lot of people, um, that like apparently I
mostly form opinions in order to talk about them, not in order to like affect
my object level decisions about what I'm doing in my life.
Did you have that before you started doing YouTube?
Like, did you have like actual opinions that you would conceal them from other people?
Uh, no, I didn't have a reason to, to like hold back on like giving takes before
because nobody cared who I was or what I had to say.
So I was free to just chat shit about anything.
Um, and because you don't want to talk about takes and you're not able to express
them, um, I think it's the best, it's the best moment to ask you when you think
we'll get a GI because it's like the less controversial topic ever.
So you will not.
Oh yeah, sure.
Um, January 1st, 2045.
What, what time is what night?
What time zone?
Uh, UTC.
So, so like, yeah, I don't know.
I think I'm in like the 10 to 20 year range now, but this is not based on like really
deep consideration, you know what I mean?
Like I don't want people to over update on that.
Um, we had a bit 30 seconds earlier about like why people should not over update
on your thing.
So I think, right, right, right.
But that was about, that was about what areas of research are particularly
promising and this is about timelines, maybe in full generality don't over
update on my stuff.
But like, I know if I say something in a video, feel free to update on that as
though it's like probably true.
Um, but yeah, my like random takes on timelines or whatever it's like, yeah.
We could say that timelines is now an active area of research now.
I suppose that's true.
Yeah.
So 10 to 20 years, um, this podcast is called the inside view.
So what's your model for predicting this thing?
Do you, do you think you can like dig in and think about like, why are you
missing those numbers?
Um, and maybe, um, another thing we do a lot with, um, on this podcast is just
like define things.
Um, maybe you did like 20 videos and profile and your channel about defining
AGI was the definition you have when I say those words to you.
What's the thing you think about?
Uh, well, yeah, I would think about, um, uh, a system that cares about things
in the real world and is able to make decisions in the real world to achieve
those goals across a very wide range of domains, like a range of domains as
broad as humans operate in or broader, or you could also call it kind of one, a
single domain, which is like physical reality, right?
If we have GPT five, yeah, that is able to like answer all those questions.
And maybe five is too soon, let's say seven and, um, yeah, do like basically
all the thinking we do, like be like a human chatbot, would that count as AGI?
I mean, like it's possible that it could, uh, but there's a question of like,
is it, is it modeling the world?
Is it trying to act in the world?
Like I have, I have the thing on, on, on the T shirt.
Yeah.
It's like kind of a joke about the goal post mover, um, saying, um, yeah,
something like, um, about my anthropomorphizing, um, highly critical,
highly critical of anthropomorphism anthropomorphism, yet things human level.
Yeah, I will require human-like senses.
Yeah.
There's no particular requirement for human-like senses.
Um, but the, you do need, you do need to be forming a model of the world.
And like maybe, maybe it depends on how these things respond with scale, right?
Obviously the actual optimal policy for the language modeling task involves
like very deep and detailed simulation.
Um, and so like maybe when you say GPT seven, that's what you mean that we're
like so close to that, but I don't know if even all of the text available now
and that will be available in the future is like sufficient to get that close to it.
This is like an empirical question.
Yeah.
I think there's a less from posts about like why we're, I really
about my data set size.
Yeah.
And maybe some companies have a lot of data.
They have like Gmail level of data.
Like if you take into account the number of emails we send every day, that's
like enough to train more data than, you know, what we've done was like 53.
Um, but yeah, except from like this, like private data, if you just like train
on the like open internet where it might be like that, but, but like in
data set size, uh, as of like current scale and laws.
Yeah.
I think that, I think that's pretty plausible to me.
Um, and like, again, this is a bit like this thing about tweaks, right?
Like it doesn't seem impossible to me that by scaling up the number of
parameters and also, you know, make some algorithmic efficiency improvements,
which like there are a bunch of them that people are already working on.
Um, or just like a new eight, a hundred NVIDIA GPU.
Yeah, potentially.
But that, I mean, that just affects cost mostly.
Um, and then, and then somehow there's like sufficient data sets to, to, to train
these things to convergence.
Um, then like maybe, but I also think we probably get something else first.
What is something else?
Well, I don't know.
Oh, you mean like another breakthrough?
Yeah.
Yeah.
Another breakthrough, another architecture, something, something more, something
Gato ish.
Um, if I, if I'm in like Robert Miles mind, if I try to like model and be like
in your head, I see your tweet about like progress seems very fast.
Yeah.
And I think we've seen like the same breakthroughs this year, right?
So strategic models, palms, et cetera.
Yep.
How do we have this current rate of progress for 10 to 20 years?
Like how is it sustainable?
Um, it depends on.
Yeah.
20 years is too long.
Is this a, a date in real time?
No, I'm just, I'm shifting what I say to what I think.
Um, like a lot closer to 10 years, but I also expect, um, I expect us to start hitting
against some limits, right?
Like we are hitting data sets, start size limits already.
Um, and it's, and, and, and that's what makes me think like you do need some, some
different approach when it's like every piece of text ever generated by humanity.
Like, like we're starting to approach the limits of what text as a modality can do.
Maybe.
Um, if Ethan Kiballero, the guy I interviewed about scaling laws, he said,
was in front of you, he would say, huh, you don't like text.
What about YouTube?
I tried trying to predict like the next frame in YouTube.
There's like a bunch of data.
Yep.
That's pretty good.
That's like the next thing to do that's going to be expensive.
Um, yeah.
Yeah.
And then, and then like you really want good simulation, I think you need
like good simulated environments for like physical manipulation of stuff.
But like, yeah, there's also, there's always this possibility that you that there
is like a fairly simple core to general intelligence that you eventually just hit.
And it's not at all implausible to me that training something massive to full
convergence on everything everyone ever wrote doesn't just find the actual thing
that can then just be applied arbitrarily.
Okay.
My, my bold claim is that if we believe scale is what you need, we already found
the like core stuff to intelligence, which is like predicting the next word.
And it's not quite what I mean.
I'm talking about like, like that's the task, right?
I'm talking about, um, some kind of cognitive structure, some kind of
algorithmic structure that like is the thing that applies whenever you're
thinking about almost anything that you're trying to do.
It's called a transformer.
No, it would be a pattern of the weights of a transformer.
Do we need to like really understand it?
Like the thing might emerge from the, sorry, Yudkowski, if you're watching
this, I'm using the word emerge.
Yeah.
I mean, I think Yudkowski's objection is that like things emerging doesn't
explain anything about them, particularly other than that they like weren't
there and then they were.
Um, but I don't think he, I don't think he objects to the concept of things
emerging.
Um, uh, yeah, Mike, that's what I'm saying.
Like, that doesn't seem completely implausible to me.
I think I have a lot of questions to ask.
Yeah, it's, yeah.
Have any like the last thing you wanted to say or, um, talk about seem depressed.
Yeah.
Are we, are you sad because we're going to die?
To a first approximation.
Yeah.
Like, I don't, we're not definitely screwed, right?
So how many, how many nines do you have in your probability of doom?
Uh, like, like one, one and a half.
Like we're probably screwed.
There are ways that we could be not screwed.
It's not like this is an unsolvable problem at all.
It doesn't seem to me to be dramatically more difficult than various other
problems that humanity has solved, but it's on the first try.
Um, uh, even including on the first trinus, right?
Like the first time we tried to put people on the moon, we put them on the moon
and got them high like we can do hard things on the first try.
Um, but yeah, we don't have a lot of time.
We don't have a lot of people working on it.
What do you think of like more drastic measures?
So I have kind of a bow claim, which is we need more greater than
of AI alignment.
We need like people like shaking people's emotions.
Um, otherwise we just like go down the streets and show like dolly picture to
your drivers and you're like, oh yeah, pretty good.
And nobody like actually understands what's going on.
We need like someone to say like, talk to the U.S.
And be like, I don't remember the actual question.
What are you doing?
Right.
How dare you?
How dare you?
Yeah.
You're just like going in front of like deep money.
Just like, how dare you?
Yeah.
To be fair, like sorry if I'm offending any deep mind researchers,
I think they're like a bunch of like good safety research going on there.
I think you're the worst AI lab, um, uh, very far from it.
Yeah.
Yeah.
Like one thing is that I'm like relatively optimistic about, um, about people like deep
mind.
All right.
I think that they're like, um,
they're probably our best bet of like people most the people who are most likely to get
to AGI first.
And, um, they're also one of the places that I think has the best chance of like doing
a decent job of that.
So like we could be in a much worse situation, right?
We could be in a place where like Facebook was leading the way towards AGI or something like that.
I think Demis said in an interview that he wants to like spend some time on fully doing
alignment research when we get like close to AGI.
Right.
The question is like how close, like when do you start to do like full time alignment?
Yep.
These things could use, uh, they could stand to be a little more concrete in the same way.
Like it's pretty encouraging the way that OpenAI has this charter about like trying to
avoid races where they're like, if we are, uh, you know, if there seems to be a race, we will
like stop and collaborate with, um, our so-called competitors and all of this.
This is like nice words, which it would be great if that happened.
To be fair, I think OpenAI is doing a better job right now.
I think Simultman is replying to my notes about GPT-4 is now retweeting my, my AI
alignment charts and commenting on them and is going on meetings with Connor is, um,
building a new alignment team at OpenAI probably.
Sure.
So I think we're kind of kind of more and more reaching out to those people and they're
taking alignment more seriously, maybe not seriously enough.
But I guess for the charter, um, yeah, it's kind of loosely defined.
Yeah.
It would be really nice to have that like really pinned down very concretely about
under what specific independently verifiable conditions it would trigger and exactly what
they would be required to do and everything like that.
Right?
Like,
There's a problem with law in general.
I mean, yeah, sure.
But, um, it's like very hard to do it well, but it's like very easy to do it better than
it's currently done.
Um, like, oh, you need to do is like, I don't know.
You can say like the people who work on metaculous, right?
They have experience on how to do this.
At on 1st of January, 2045 noon, UTC, if the website is it agi.com says yes in green,
then this means we succeeded.
Yeah.
Yeah.
Like just any kind of, um, yeah.
Just any kind of, um, objective operationalization of those commitments, um,
such that it's like very, very unambiguous when they should be deployed and so that you
can't just wait or, you know, there's like disagreement.
It's like you want common knowledge that this thing is happening, right?
You want it to be independently verifiable.
Um, well, I guess I don't see that.
Unambiguous people have like specific benchmarks.
They say if, you know, you're doing, you're able to do like self-driving for a
similar amount of miles and you don't, you do like less errors than a human would do
at the same time.
And maybe you can call this agi and, um, if we need to do like regulations on like,
please don't build agi, maybe you could say like, please don't like pass those
benchmarks or if you're, so I have like some crazy friend in Berkeley who just like
wants to like regulate AI very hardly and he wants to like require, um, companies to
make it fully interpretable.
Um, you could like bound the number of like compute they use, bound the like number of
parameters.
Otherwise, you know, they need to pay something.
I think you can like limit the research efforts into doing something in general.
I think you could like say your AI need to be a tool AI that only is applied for like
doing specific, um, you know, things with Google search or something.
Yeah.
But obviously there's like a bunch of like economic incentives to do something general.
Yep.
And corporations are vaguely analogous to agi themselves and things that come in the,
in the category of like, you must not break these specific rules.
I like, yeah, there will be loopholes.
There's sufficient incentive.
It's not like these things are completely not worth trying or whatever, but I'm not like
incredibly hopeful about them.
Um, are we doing, are we like making progress on climate change with the carbon tax and
all those things?
I think companies are doing more greenwashing.
Obviously they're not like fully optimizing for reducing climate change, but they're like
doing some work on it.
Yeah.
Yeah.
And that seems right.
Alignment washing, uh, seems worse.
Seems worse because safe washing.
I think someone said that was wrong.
Say, okay, yeah, sure.
That makes sense.
Like this is, this was kind of what I was talking about before that there's, there's
various things you can do that look like alignment on current systems.
And in fact are, right?
Like they meet the definition you are in fact getting the model to more accurately
do what you wanted to do, but they just like don't survive self-modification or they don't
survive the level of optimization power that superintelligence can put on them.
Don't you need, don't you think we need to like ask stuff gradually?
Like first make them interpretable, more robust, um, and then maybe they will like
understand what is alignment and like, we could still be improving.
Yeah.
It's kind of thing.
Yeah, maybe the point I was just making is that it's like, if you
it's, there's a sense in which having a system you can't align, having a, having a design for
an architecture that you can't align, that you know that you can't align is a better state of
affairs than something that's like kind of aligned and you think is fully aligned, but
it isn't because that first one you actually don't turn on.
And the second one you do, um, there's a risk of things that give the impression of an alignment
effect, um, but don't actually like solve the hard part of the problem.
And, um, and so it's difficult to like make, um, it's difficult to know how much to, how much
more optimistic to become when various companies start like making the right noises because you
don't know if they mean the same thing by alignment as we do.
You don't know if they're like Mesa aligning with, you know, the taxes or regulations you're putting in.
Yeah. I mean, they might just be, yeah, they might just be more optimistic.
I think if everyone reasons for his podcast, they might just become a doomer.
Do we have any like thoughts for hope?
Was that too doomy?
Okay.
Oh, I just like think of like a bunch of different solutions.
I try to like come up with plans.
Some of the plans, um, Conor would say they're like pretty bad because they're like too risky or,
don't take like enough into consequence.
They're like second order things to come.
I'm pretty bullish on policy regulation in the sense that
even if we like don't solve alignment fast,
if we manage to like map the political landscape well enough, we could reduce the speed.
Like at least like, if I see like a war succeeding, I don't see like a world succeeding
where we like find the correct equation.
I feel a world succeeding where we just like slow down things a lot.
And then maybe we're done equation, but like we need to slow things down.
Otherwise it's like not sustainable.
Uh, yeah.
Yeah, my position on this is like, and again, not very well informed.
Same here.
Would be nice.
Slowing down would be nice.
If you're working on AGI research, like consider not doing that.
Oh, so one thing you can do is you pay AGI, sorry, AI researchers to do AI element instead.
And you just like give them like, why is the salary?
Yeah.
And they'll be like, oh, there's like this two job offers.
Yeah, it's expensive, but we have money and apparently we don't have to do it for very long.
So yeah, maybe we could afford that kind of burn rate.
It's possible.
It's been something people mentioned a lot on less wrong or other forums where,
you know, some people just like working AI, they're just like no job for them to work in AI element.
And there's like no money for it.
So they're just like, you know, make 400k working for Facebook.
And do we have like any concrete job paid like 400k for like AI element research?
Maybe there is like a topic, but you know, there's like no concrete things to do.
And like, if you don't really have a bunch of like expertise, do we have like enough
ability to like take those people in and like offer them like competitive offers?
Yeah.
Yeah.
And like, I think actually we kind of do now.
There's so much, that's another thing that's changed like during the pandemic.
There's so much more like money in the ecosystem now.
Well, yeah, there is more funding, but I guess like a bunch of EA money,
so effective autism money comes from Facebook stock and crypto.
And I'm unsure like how much Facebook stock like crashed a little bit.
And maybe it's not started crypto.
Yeah.
So maybe like a little bit less than we had before, but yes, it's still impressive.
Yeah.
Yeah.
Like when it comes to like being able to pay tech salaries,
but compared to like the actual market cap of AI research is still like epsilon agreed.
Agreed.
Jeff, any words of encouragement for those like 5% to 10% chance of us succeeding?
What would you, how would you describe Robert Mullins in this ideal world?
What are you doing still doing YouTube videos?
And you know, there's like a super intelligence just doing stuff outside and you're like connected
to an AI or just.
You mean like what does the world look like?
Maybe it's just like the one year before we do it or like the way we do it.
And then maybe if you want your life after that.
Well, I mean one year before I expect to be like a very frantic and bizarre world.
What kind of frantic and bizarre?
Just like things changing very rapidly, economic upheaval.
Right.
Because like before you get AGI, you get automation of a bunch of stuff.
I also would expect there to be a bunch of like residual upheaval because like things
things take a long time to go from research to product to widely used product.
It's like it's not a long time.
It's a few years, but like that becomes a long time.
And so by the time there's like a research AI system that is like a full fledged AGI that like
starts to take off at whatever speed it ends up taking off.
I expect there to be at that moment a whole bunch of things coming to the end of the pipeline of
these like pre-AGI technologies changing all kinds of things about our lives and work and
relationships and communication and transport and everything.
I expected to be like a pretty wild time to be alive.
And like hopefully humanity can like stay sufficiently coherent and coordinated through
all of that to like tackle the AGI thing well.
And be like, oh, something weird is happening.
Maybe we should do something about it.
Yeah.
Like I think at that point, like at the point where it's too late to do anything,
people are going to be pretty on board with the program.
Someone will claim that we're already at this point.
I feel like, okay, last night instead of preparing for this podcast,
I spent two hours from 2 a.m. to 4 a.m. generating a bunch of images with the same prompt with
Dali too.
The same exact prompt.
And it just like gave me like crazy images.
I was just like stuck.
It was like, wow, this is great.
And I think we're, I would do like in a crazy world where I can just like have fun
generating like pictures of anything with my computer and it's like normal.
Yeah, but like two years ago, three years ago,
we would have said the same thing about whatever the latest like,
we think that now is crazy.
It's not very crazy.
There's so much crazier it could get.
There's so much space above us in craziness.
It could be crazier, but in terms of like, you know, helping humans do things,
you know, helping designers design things, helping like,
there's like so many applications of stuff like Dali or Parture.
Sure.
Oh, I'm, by the way, I'm starting another YouTube channel, hopefully.
And you're mentioning it after like two hours of protein.
I forgot.
Or at least I'm thinking of it called like air safety talks,
which is basically just like, there's a bunch of organizations where people are
giving really good, interesting talks about various alignment topics.
And those tend to be either not recorded, recorded in like bad quality or recorded
in okay quality, whatever.
And then put on some, on the like research institutes YouTube channel that has like
200 subscribers and it gets 17 views.
And so I want to like make a place for like long form like researchers talking about their research.
The reason I bring it up is because I designed the logo with Dali.
Actually, I designed the logo.
It was it ended up being a mixture of things.
We had, we had a paid graphic designer who produced like some stuff and I like kept bits of it.
But the first thing I did was describe the thing.
AI Safety Talks is a YouTube channel about blah, you know, it's, it's logo is,
gave that to GPT-3.
And it generated me some nice completions about ideas of what it could be.
It's like, oh, it's a person pointing to a whiteboard, you know, like they're giving a
presentation or like a projector screen.
And on the projector screen is like a brain made of circuit board or something like that
was like the, or a brain made of like network graph nodes or whatever.
What people think one day, like most of like illustrious AI is just like this brain with
like a circuit board.
Yeah, exactly.
And so then I took that and gave it to Dali to get the logo.
But so like in this case, you know what, we did end up paying a graphic designer.
But the bit that I kept from the, or like actually there's a couple of bits I kept,
but like the bit, the main bit that I had to keep from the designer was the text because
Dali can't do text at all.
But like, apparently, what is it?
Party?
Yeah.
Does text perfectly well.
So yeah.
Google, if you want to like sponsor some of the new logo, you know, what you talk to.
Yeah.
But I really like that.
I really like that workflow, right?
You get, use GPT-3 to get inspiration for like what the logo should be.
And then you give that to the user.
I really like crazy time.
We can just like ask people for their problem for their like generated text and then like
put them into like a local generator.
Yeah.
That's like, but that's, I don't want to under state.
Like that is crazy.
It's just like so low on the scale of the potential craziness that is like on the horizon.
The next thing is going to be very crazy.
And then it's like going to be like audaciously crazy.
Why not do it on your main channel?
Because I don't know how much time do we need to go from like zero to 100K subscribers?
Like if you, like, okay.
I think like a bunch of like AI safety work could be just like on your channel and be like,
we get like half of it in views, right?
If you get like a talk by even a binger, you know, it depends on your timeline, right?
But if you think the channel could grow fast enough.
Yeah, I guess.
I don't know.
I can ask my audience about this, but I think if it were me, if I were following a channel,
like if I say I'm following like three blue, one brown and he was like, yeah, you know,
I wish people, more people understood more mathematics.
So I'm going to just also add on the main three blue, one brown channel, a bunch of like
lectures from good mathematics researchers about their work.
I would unsubscribe because like that's not why I'm like,
he's unusually good at explaining things.
He puts a lot of effort into making them like very approachable for the general public.
And like, I do not have the mathematical background to like get much out of watching
a professor of mathematics give a lecture about, you know, topology or whatever.
As a subscriber of Robert Miles on YouTube, if I get like a talk about AI safety
every like few months, I'll be more than happy.
And to be honest, the YouTube short thing, you sometimes do like, you know, promote cool like
AI safety like programs like the inverse AI alignment prize, the AI safety games kind of thing.
So in that sense, it's not that much different, right?
Yeah, I suppose, but those like, they take a minute of your time if you want to watch him,
like literally a minute because it has to be.
Yeah, I don't know.
My guess is, because the other thing is I want to put out quite a lot of stuff on this channel,
right?
I want like it would become the majority of my channel if it was my main channel.
I could start putting them all on Rob Miles too.
But yeah, I like the idea of it being not also of it being like not
Rob Miles branded.
It's like Rob Miles affiliated, but it's not like, as though these are my videos,
because they're not like I what I expect is a bunch of these research orgs to just regularly
from time to time be like, oh, you know, so and so do to talk about whatever.
And like, we would upload this to our own YouTube channel, but we'll just send it to yours is it's
like a kind of central location to put all of these things.
And I don't want to have like a super high threshold of approachability,
because I would like I would like researchers to really be able to benefit from being subscribed
to it.
Like this is as much about helping alignment researchers to understand each other's research
as it is about outreach.
And then I think what I would do is highlight on my channel with maybe with YouTube posts
whenever there's a channel on a safety talks, which is like particularly
approachable or understandable, or you know, something that something that is like
something I expect my ordinary audience would get a lot out of watching.
That's my current thinking.
But you're right, like it is definitely a problem to like start a channel from nothing
and it has very few subscribers or whatever.
But I think I can promote it pretty hard at the end of my videos and stuff like that.
I don't know.
I'm open to being persuaded on that.
Okay.
I think then you might overestimate the number of people who get like upset on YouTube and like
unsubscribe.
I would give it like one.
Okay.
High bar is like one percent of people.
If you upload like 10 videos of talk, you're like, ah, he was funny.
But now he's like pushing like a bunch of like a safety topic.
If you care about AI safety enough to subscribe to Robert Maddlin YouTube,
you're like kind of enjoying having like more AI safety content.
Maybe, but I do this all the time.
You subscribe from the people after like one video.
No, no, like when, when I'm like, okay, this channel, I subscribe to this channel for
this type of video and it's become this other type of video.
I'm like, this channel isn't what I subscribe for.
I'm just going to not do that.
I like, you know, I curate my subscription feed because I spend way too much time on YouTube.
I like that time to be like used efficiently at least.
But yeah, maybe I'm, maybe I'm generalizing for myself too much.
I don't know.
There's no mute button on YouTube.
You can just like subscribe and mute.
Right.
Yeah.
And I wish, I wish that there was a thing that was like, well,
I wish there was a thing that I wish that you had like two streams, right?
So many people have a second channel.
I wish that you, oh man.
Okay.
Feature request for YouTube.
This thing that I just thought of.
What you do is you subscribe at a level that's like daily, weekly, monthly, let's say.
And then, so if I'm like subscribed to you at a daily, daily level, then you get one like
each video you publish, you can like rank it according to like, this is like a particularly
high quality thing or like this is medium or this is not, this is just like a little behind
the scenes, whatever, that only people who are really fans are going to even care about.
And then it's like fills up my feed.
If I've already seen, I'm trying to think how this algorithm would work.
But you see what I mean?
Where it's like, I'm not going to get more than one daily level thing per day.
If you upload 10 daily level things in a day and I'm only subscribed to you on the daily level,
then I'm only going to get whichever one of those you thought was the best one.
Whereas if I'm like a full subscriber, I get all of them.
So if I subscribe to, like if someone subscribes to me on a monthly level,
I get one monthly level, like once per month, I can allocate a video to be like,
this one is a monthly video.
This is a particularly good one.
And so then you could subscribe like just for the best ones,
don't show me more than one a month or like don't show me more than one a week or whatever.
I think it's a great new feature for a new version of Stumpy.
I think you could do something with notification as well,
like the bell thing when like people will notify you if there's like a new video.
Right.
And yeah, please subscribe.
Yeah, I guess we're on YouTube now.
Subscribe to this.
Oh yeah, right.
If you're in podcast, sorry, but subscribe on YouTube.
Follow me on Twitter and Spotify and those kind of things.
We have no rating.
We currently have no rating on Spotify.
Wow.
So if we could have ratings, it could be great.
Oh, I wonder if the, oh yeah.
You know, I do a podcast.
It's called the alignment newsletter podcast.
If you're listening to this as a podcast,
you may be a person who enjoys a podcast.
In which case, if you're interested in alignment research,
Rohin Shah, who is right there almost slap bang in the middle of them.
I'm sorry, Rohin, I didn't ask you for this.
And it's probably wrong.
He makes a very excellent weekly newsletter about
this week in AI safety research or alignment research.
And I just read it out every week.
Do you still do it?
I do it when he makes them.
He's been like on a hiatus recently, but he's starting up again.
Right. Yeah, yeah.
He's the bottleneck.
Yeah, so subscribe.
Here's the button.
I don't know where.
And yeah, do you have any final thought, final sentence?
I don't know, man.
Like, there's definitely some kind of thing here that's like,
like chin up.
Like the right thing to do is to try and solve this problem.
It seems really difficult.
We're not guaranteed to succeed, but we're not guaranteed to fail.
And it's like, it's worthy of our best efforts.
Let's do it.
Let's do it.

Hi, I think we're ready to start, so my name is Paul Franklin and I'm a neuroscientist here at
Sick Kids and also I'm program chair for the Canadian Neuroscience Meeting that takes place
in Toronto all this week. So the traditional curtain raiser for the Neuroscience Meeting,
the CAN meeting, is the public lecture and this year we decided to focus on the interface between
neuroscience and AI. We did that for two reasons. First reason is that AI or Toronto is one of the
main hubs in the world for AI research and the second reason is that it's also home to one of
the true pioneers in this field also known as the godfather of deep learning Jeff Hinton.
And so when we asked Jeff if he'd participate in this event, I think a year and a half ago we
asked him and he said yes we were super excited. So at this point I want to hand over to Blake
Richards and Blake Richards is an associate professor at University of Toronto Scarborough
and he's going to host this evening's event, Blake. Thanks Paul. So just as a brief introduction
I wanted to tell you a little bit more about Jeff. So you might be surprised to learn that the godfather
of deep learning associated mostly with AI got his BA in experimental psychology from Cambridge
originally. He then went on to do his PhD in artificial intelligence in Edinburgh so he did
get started relatively early. But throughout his early career he really contributed to the first wave
of what was known at the time as parallel distributed processing or connectionism
which really brought back into the fore the idea of using neural networks for both models of the
mind and for artificial intelligence. Now Jeff got his first tenure track position at Carnegie
Mellon in the 80s but we were able to steal him away from them in the late 80s which I understand
was largely because of his ethical objections to DARPA funding so once again Canada's political
bent has helped us in our research endeavors. Over the course of the 90s Jeff continued to
really push neural networks and machine learning forward and sorry about that. In 98 he actually
went to University College London to found the Gatsby Computational Neuroscience Unit
and we might have lost him but thankfully we pulled him back again. He came back to Toronto
in 2001 where he became a university professor in 2006 and then an emeritus professor in 2014.
Now I think that you know you all know that Jeff is a monumental figure within artificial
intelligence and machine learning and he's been critical to the founding of the Vector
Institute here in Toronto and putting Toronto on the map for AI and certainly he's had incredible
recognitions of his work most recently the Turing Award which he shared with Yashua Benjio and
Yanlacun as well as the Order of Canada and he's a distinguished fellow of the Canadian Institute
for Advanced Research which I highlight because they were one of the people who continued to
support neural networks throughout the time when it wasn't as faddish but for all his successes
in his technical endeavors I think one of the things that's most important to understand about
Jeff is the impact that he's had on other scientists. Jeff has really molded the career of
so many people and changed the way that they think about things. When you look at the people who
have been his graduate students or postdocs it really is the who's who in artificial intelligence.
It includes people like Max Welling, Yanlacun and you know the phrase I use to describe it is
have you drunk Jeff's Kool-Aid because once you've drunk Jeff's Kool-Aid there is no going back.
You see neural networks, you see AI differently and I would argue you also see neuroscience
differently and for me my understanding of the brain has been largely shaped by Jeff and his work
but you know we're at the point now where computer science has drunk Jeff's Kool-Aid
so he's got an H index of 145 and according to Google Scholar his work has been cited 270,000
times which is more than Einstein, Ramoni, Kahal and Alan Turing combined but that's largely
from computer scientists and if my prediction is correct neuroscience 30, 40 years from now
will also have drunk Jeff's Kool-Aid and maybe you're going to get your first taste tonight.
So with that I hand you over to Jeffrey Hinton.
So thank you very much Blake. I can give you some more Kool-Aid today. It's Kool-Aid produced by
one of my former students Ilya Sutskava. First I want to tell you a little bit about the history
of deep learning in AI. Can I just ask before I start how many people here know what the back
propagation algorithm is? Put your hands up. So some people don't. I'll explain it very quickly
and I'll explain it in such a way that you'll be able to explain to other people if so if you do
know what it is you follow the explanation from the point of view of how you explain it.
Okay there was a war between two paradigms for AI. There were people who thought that the essence
of intelligence was reasoning and logic is what does reasoning so we should base artificial
intelligence on taking strings of symbols and manipulating them to arrive at conclusions
and then there were other people who looked at the brain and said no no intelligence is all
about adapting connections in the brain to get smarter and this war went on for a long time
and eventually people who were trying to figure out how to change connections between
fake neurons to make these networks smarter got to be able to do things that the people
doing symbolic AI just couldn't do at all and now there's a different way of getting a computer
to do what you want instead of programming it which is tedious you just showed examples
and it figures it out now of course you have to write the program that figures it out but that's
just one program that will then do everything um and this is an example of what it can do
so the image just think of the numbers they're rgb values of pixels and that's the input to
the computer lots of values of pixels just real numbers saying how bright the red channel is
and you have to turn those numbers into a string of words that says a close-up of a child holding
a stuffed animal and imagine writing that program well people in conventional AI exaggerate that
program and they couldn't partly because they didn't know how we did it um we still don't
know how we do it but we can get artificial neural networks to do it now and do a pretty good job
and my prediction is within 10 years if you go and get a CT scan and what'll happen is
a computer will look at the CT scan and a computer will predict will produce the written
report that the radiologist currently produces radiologists don't like this idea okay um here's
a simplified model of a neuron it's very simple it gets some input which is just the activity on
the input lines times the weights adds it all up that's called the depolarization and then it gives
an output that's proportional to how much input it gets um as long as it gets enough input
and so to begin with we won't have spiking neurons these are just going to be neurons that send
real values in just the way neurons don't we're going to make networks of them by hooking them
up into layers and you could put some pixels on the input neurons look there the input neurons
and you go forwards through the net till you get outputs and then you compare those outputs with
what you ought to have got so you have to know what the right answer is and what we'd like to do
is train the weights these red and green dots so that it gives the right output
now i'm going to show you a way of training the weights that everybody can understand
and everybody is thought of basically um what you do is
you start with random weights you show some inputs you measure how well it does
then you change one weight a tiny bit so i take that weight there and i just change it a tiny bit
and then i show the same inputs again and see if it does better or worse if it does better i keep
the change if it does worse maybe i keep the change in the opposite direction um that's an easy
algorithm to understand and that algorithm works it's just incredibly slow because you have to
show lots of examples change your weight and then show lots more examples change another weight and
every weight has to be changed many times so if you use calculus you can go millions of times faster
so the trick of this algorithm the sort of mutation algorithm is you have to measure the
effect of the weight change on the performance we don't really need to measure it because
when i change one of these weights the effect that it has on the output is determined by the
network it just depends on the other weights in the network it's not like normal evolution where
the effect of a gene depends on the environment you're in this is all kind of internal to the brain
and so changing one of these weights has an effect that's predictable here so i ought to be able to
predict how changing the weight will help get the right output and so what back propagation does
is basically says i'm going to compute using an algorithm the details of which i won't tell you
and compute for every weight all at the same time how changing that weight would improve the output
and then i'm going to change all the weights a little bit so every weight changes in the direction
to improve the output and the output improves quite a bit and then i do it all again now
that allows me to compute for every weight how i'd like to what direction i'd like to change it in
and the question is should i when i showed examples show it all of the examples and then
update the weights so should you live your whole life with the synapse strengths you're born with
then update your weights a little bit then live your life again and update the weights a little bit
more that doesn't seem very good um or should you take one case or a few cases figure out how you'd
like to update the weights update them and then take more cases that's the online algorithm
and that's what we do um and the amazing thing is it works you can take one case at a time or
you take small batch of cases you update the weights and these networks get better
and it's very surprising how well it works on big data sets so for a long time people thought
you're never going to be able to learn something complicated like for example take a string of
words in english feed them into a neural net and output a string of words in french that mean the
same thing you're never going to be able to do that if you start with a big neural net with just
random weights it's just asking too much for the neural net to organize itself so it can do translation
because you have to kind of understand what the english says um and people predicted this was
completely impossible but you'd have to put in lots of prior knowledge well they were wrong um
so in 2009 my students in toronto showed that you could actually improve speech recognizers
using these neural nets that had random weights they were just trying to predict
in a spectrogram which piece of which phoneme you were trying to say in the middle of the
spectrogram and then there was more to the system that wasn't neural nets now what we've done is
we've got rid of all the stuff that wasn't neural nets and now you can take sound waves coming in
and you have transcriptions coming out or even better you have sound waves coming in and you
have sound waves coming out in another language with the same accent um they can do that now
that's speech recognition done um then in 2012 two of my students took a big database of images
and used essentially the same algorithm the few clever tricks to um say what was in the image
not a full caption just the class of the most obvious object and they did much better than
conventional computer vision which had been going for many years and since then all the best
recognizers have used neural nets in 2011 you couldn't publish a paper at neural nets in the
standard computer vision conference because they said they were rubbish in 2014 you couldn't
publish a paper that wasn't about neural nets okay and in 2014 they did something that i didn't
expect this was done by people at google not me and yosha benjo and his group in montreal
particularly by a guy called badanow and cho they managed to get a neural net so you feed in
actually fragments of words in one language you have 32 000 possible fragments so the word
in english would be one of the fragments but so would things like in and on and what comes out
in another language is fragments of words in that other language and it's a pretty good translation
and that's how google now does translation so it did translation better than symbolic ai
and so what changed between 1986 and 2009 and it was basically computers got faster that was
the main change data sets got bigger we developed some clever tricks um and we like to emphasize
those but it was really the computers getting faster and data sets getting bigger but i'll
emphasize the clever tricks nonetheless um and i can tell you about two clever tricks i can tell
you about transformers and i'm going to tell you about better ways of stopping neural networks from
overfitting but first i want to show you example of what neural nets can do now um so a team at
open ai took work on transformers that was originally done at google um they developed it a
little bit further and they applied it to big neural nets that have 1.5 billion learnable
connection strengths so you're they're learning 1.5 billion numbers that's the knowledge of the
system and they train it up on billions of words of english text and all the net's trying to do is
predict the next word so what the net will do or fragment of what the net will give you probabilities
for the next word so if you give it some words a leading it'll give you probabilities for the next
word and once the net's trained what you can do is you can look at those probabilities and if it says
there's a probability of 0.4 that the next word is the you pick the with probability 9.4 and if it
says fish with probability 0.01 you pick fish with probability 0.01 and so you just pick from
its distribution and then you tell the neural net okay the one i picked was the next word what do
you think comes after that and this way you can get it to sort of reveal what it really believes
about the world so you're getting it to predict words one at a time and every time it makes a
prediction you say you were right and it just gets more and more carried away so they initiated it
with some interesting text and the question is with will the neural net then produce stuff
that's sort of related to that i mean the first question is will it produce english words will
the words have decent syntax will it have any meaning will it be related to this if you're
really optimistic you might say will they sort of relate to the fundamental problem here which
is how these unicorns can speak english okay so here goes this is what the neural net produced
now this was cherry picked this was one of their better examples
the neural net just made this up right
it made up dr. Jorge Perez there is no such person at the University of La Paz
but it's pretty plausible because it's South America and i believe La Paz has a university
okay so that's the first bit of what it made up and it carries on and it gets better
the next bit sounds a bit like one of those fantasy games
so
so it's remembered about unicorns and herds of unicorns right so they walk up and there's this
strange valley and it's a very strange valley and they found the herds of unicorns
and it has something about seeing them from the air and being able to touch them which isn't
quite right so people in symbolic eye leap on this and say you see it doesn't understand
well sure there's little bits that it doesn't get right
but notice it's remembered that these unicorns have to speak english and so it tells you about
you know they spoke some fairly regular english it doesn't know the difference between dialect
and dialectic but my kids don't know that either um in fact i'm not sure i know um
it attributes little unicorns to argentina even though dr. Perez comes from bolivia
and it actually understands about magic realism so
the descendants of a lost race and i love the bit at the end where it says in south america
such incidents seem to be quite common this has an ability to just make up something that
fits your prejudices and sounds moderately plausible um like a certain president
and it finally gets to the point which is if you really want to know whether these unicorns were
used by breeding with these strange race of lost race of people you ought to do a dna test
okay it understands that um okay so that's what neural nets can do now
this was a neural net with 1.5 billion connections that was trained on google's um actually
i withdrawed that um if 1.5 billion connections is trained on a lot of hardware
and we look at what it says and we sort of laugh at how you know it's pretty good but it
hasn't got it quite right but it's pretty good okay what they've done now is they've trained
a neural net with 50 billion connections on google's latest cloud hardware which is it's like having
several of the world's biggest supercomputers going for you for months um the net with 50
billion connections i haven't seen any text from it yet but my prediction is it's sitting around
laughing at how cute what we produce is okay so one thing about that net is it's clearly
very well aware of the initial context these unicorns in a valley that speak english and it's
remembering this initial context a long time later and a recurrent neural net can't do that
a recurrent neural net would have forgotten about the initial stuff and wouldn't produce such good
context-dependent stuff so the way this works is the word comes in the neural net activates some
hidden units that pattern of activity in the hidden units goes and compares itself with previous
patterns your point of view with previous patterns um at earlier times and when it finds a pattern
at an earlier time that's a bit similar it says that we'll take advice from that previous hidden
pattern about how to affect the next layer and so actually a word comes in and how one pattern of
activity in the bottom layer of the hidden neurons affects the next layer is dependent on
what happened previously now it's dependent in quite a complicated way and this seems very
implausible for a brain because what's happening in the computer is you're storing all these activity
patterns that are meant to be neural activity patterns or light neural activity patterns
and you're comparing and this looks hopeless but actually all you need to do is every time you
have an activity pattern and you use the outgoing weights to affect the next layer just change the
weight slightly with heavy and learning so now what's going to happen is that weight matrix that comes
out of that activity pattern is going to be modified slightly now when I get a new activity pattern
if the activity pattern is orthogonal to the previous activity pattern
then any modifications you made in the weight matrix due to that previous activity pattern
won't make any difference but if it lines up with the previous activity pattern if it's similar
the modifications you made in the weights back there the temporary modifications will cause
this new activity pattern to have a different effect here so you'll get that long temporal
context and the way to store a long temporal context is not to keep copies of neural activity
patterns it's to take your weights and to have temporary changes to the weights which are called
fast weights so you temporarily change them and these changes decay over time so you'll have a
memory so if you ask where in your brain is your memory of what I said a few minutes ago
I'll ask the younger people this because for the older people it's nowhere
but for the younger people it's somewhere you can if I were to say something I said a few minutes
ago like these big neural nets are now laughing at us you remember I said that where was that memory
I think it's in the temporary changes to the weights because that's got much bigger capacity
than activities of neurons you don't need to use that neurons just sitting there remembering right
and those temporary changes don't need to be driven by backpropagation they can just be heavier
okay so I've tried to relate these wonderful nets that can make up stories with an idea about where
short-term memory is in the brain and now I'll talk about where the cortex can do backpropagation
so neuroscientists 20 years ago neuroscientists they don't be ridiculous of course the brain
can't do backpropagation and they'd interpret it very literally as sending signals backwards
down the same axons and saying neurons don't do that no thanks
but now we know that backpropagation works really well for solving tough practical problems
so that's rather changed the balance because when backpropagation was just a theory of how you
might get computers to learn something and when it learns some simple things it wasn't
sort of imperative to understand whether the brain did it but now we know that you can
do all these things with backpropagation what's more we know that backpropagation is the right thing
to do that is if you have a sensory pathway and you want to take the early feature detectors
so that their outputs are more helpful for making the right decision later on in the system
then what you really need to do is ask the question how should I change the receptive field of this
early detector so that the what is output helps with the decision and what you have to do is do
backpropagation to compute that that's the efficient way to compute it and I think it'd be crazy if the
brain wasn't somehow doing this um so why do neuroscientists think it's impossible
apart from silly objections like things don't go backwards down axons
at least not at the right speed
oops
he wants me to update things
oh it's just died
I'm gonna go out and present them out
and go back in to present them out
so here's some reasons why the brain can't do backpropagation
the first reason is they say well um it doesn't get the supervision signal
and they're imagining that the supervision signal is that you take a micro pipette and
you put it into the infrotemporal cortex and you inject the right answer
and the brain doesn't have anything like that right
but actually if you take that language model it didn't need label data it was just trying to predict
the next word so you can often use part of the input maybe a future part of the input
or maybe a small part of an image as the right answer and so you can get supervision signals
easily so there's no problem about supervision signals the second reason is neurons don't send
real valued activities they send spikes and backpropagation is using these real valued
activities so you can get nice smooth derivatives so backpropagation can't possibly be what's
going on in the brain the third objection is neurons have to send two signals they have to
send their activity forwards and they have to send error derivatives backwards the signal
they have to send backwards is um how sensitive am I to changes in my input or rather if you
change my input how much does that help with the final answer
and the last thing is about neurons having reciprocal connections because you have to
when you send things backwards if he's a different neuron you have to use
the same weight as the forward weight um I'm not going to tell you how you can overcome that but
you can easily so supervision signals isn't really a problem there's many ways to get a
supervision signal the simplest is predicting what comes next
now the question of can neurons communicate real values um well the first thing to notice
about backpropagation is if you have very noisy estimates of the gradient it works just as well
it's very very tolerant of noise as long as it's unbiased noise so for example you the signal you
send forwards can be one bit one stochastic bit and the signal you send backwards can be two bits
if they have the right average value if their expected values are correct
then they're just this expected value plus some noise and the whole system still works fine
so in the brain you have a neuron at any instant the neuron has an underlying firing rate
and it produces spikes and for now let's just suppose it produces spikes according to a Poisson
process um so it's probability of producing a spike in a small interval which is the underlying
firing rate and the question is suppose we treated it as if it could send that underlying firing
rate when it sends a Poisson spike it's just a very noisy version of the underlying firing rate
it's a one or a zero but its expected value is the underlying firing rate okay so how well
do neural networks work if we send very noisy signals um so I'm going to have a statistics digression
if you do statistics 101 they tell you you shouldn't have more parameters in your model
than you have data points you really ought to have quite a few data points for each parameter
it turns out this is completely wrong um basins knew it was wrong actually um
the brain is not in the same regime of statistics 101 in the brain you're fitting about 10 to the 14
parameters and you have about 10 to the 9 seconds so even if you have sort of 10 experiences per second
so even if you take 100 milliseconds there's a time for an experience that's the kind of
backward masking time um you have like 10 000 synapses per 100 milliseconds of your life
you're throwing a lot of parameters um so if your mother just kept saying good bad good bad
good bad she couldn't possibly provide enough information to learn all those 10 to the 14 parameters
um and here's what they teach you wrong in statistics
everybody knows that if you've got a given size model with a given number of parameters
the more data you have the better you'll generalize so for a given size of model it's
always better to have more data in fact the best thing you can do is get more data
okay but that doesn't mean that if you've got a fixed amount of data you should make it look
like a lot by having a small model that's what they tell you in statistics 101 okay big models
are good if you regularize them if you stop them doing crazy things um
um
we can see that using a lot of parameters is good that you can always win by having more
parameters and the way you do that is say i'm going to have a committee i'm going to learn
lots of different little neural nets you give me more parameters i'll learn more different
neural nets and then i'll average what they all say and you'll always win it's a sort of
declining win but if you have enough of them you'll win by having more um so it's always
better to have more parameters it turns out if you have a fixed amount of data
and you have enough computation power which the brain has you should always use
a such a big model that the amount of data looks small that's the regime you ought to
be in for a fixed amount of data that is if you take the limit when the amount of data is fixed
and you have unlimited computation and ask now how big would you like your model to be
you'd like your model to be much bigger than the data
okay now that only works if you have a good regularizer and i'm now going to tell you a
very good regularizer called dropout um so this is to use in neural networks where you have a
lot more parameters than you have data points to train them on and you could learn an ensemble
of little models and this is a way of learning an ensemble of many more models but the models in
the ensemble can share things with each other so the idea is if we just have one hidden layer in
a neural net we put the data in and each time you show it a data vector we randomly remove half
the neurons so we randomly get rid of half the neurons in your brain and only use the ones that
remain and it's a different subset we remove each time now when we do use a neuron we use it with
the same weights each time so what you've got is if you've got h hidden neurons you've got two to the
h different subsets of neurons you might use so you actually have two to the h different models
exponentially many models most of the models are never used a few of the models will see one example
a small fraction of them will see one example no models will see two examples
and yet they can learn because they're all sharing parameters so this idea of sharing
parameters in a neural network is very effective so really you've got all these different models
that are sharing parameters and you train it up and it generalizes really well
so I said that so what we know is if you get rid of a fraction of the neurons each time
and treat it as though they weren't there it works really well that's just a form of noise
and basically this is just an example of if you have a very big model and you add a lot of noise
the noise allows it to generalize well and it's better to have a big model with a lot of noise
than to have a small model with no noise and so what the brain wants because it's got such a big
model compared with the amount of data it operates on it wants a lot of noise and so now a Poisson
neuron is kind of ideal it's got a firing rate and now it adds a whole lot of noise to that
and either sends a one or a zero and that's actually makes it generalize much better
okay so the argument is the reason neurons don't send real values is they don't want to
they want to send things with a lot of noise in and that's making them generalize better
so that's not an argument against back propagation these dropout models are trained with back propagation
so the random spikes are really just a way of adding noise to the signal
to get better generalization
and now the last thing I'm going to address I'm going to keep going till Blake stops me and I
figure I've got about another five minutes before he gets really ratty
so the output of a neuron represents the presence of a feature in the current input
so it's obvious the same output can't represent the error derivative right you couldn't have a neuron
that's said to higher layers this is the value of my feature and said to lower layers this is my
error derivative it couldn't be done so the neurons that go backwards need to be different neurons
except that that's nonsense
so here's my claim yoshio benjo picked up on this later I made this claim first in 2007
actually I made it first in the ground proposal but
um and I still believe this claim even though nobody's managed to make it work really well in a
neural net yet the idea is a neuron has a firing rate that's the firing rate is its real output
which is communicated stochasticly by a spike and that firing rate is actually changing over time
that underlying firing rate and the rate of change of the firing rate is used to represent
the error derivative now the nice thing about a rate of change is it can be positive or negative
so we can represent positive or negative derivatives um without a neuron having to
change the sort of signs of its synapses um and what it's representing the the derivative is
representing is the derivative of the error with respect to the input to the neuron and that gets
sent back to earlier neurons and if I had enough time I could show you a whole bunch of slides
about how this will do back prop um but I want to show you one consequence of this
so that's look here we have a nice equation because it's got Leibniz on one side and Newton on the
other side um that's Leibniz's notation for derivatives because they're not derivatives
with respect to time and this is Newton's notation because that was for derivatives with respect to
time okay and what we're saying is the output of neuron j which is why j is the output of
neuron j but how fast that's changing over a short time interval is the error derivative
this is just our hypothesis you understand but it's true
j mclendon and I first used a version of this in 1988 before we knew about spike time dependent
plasticity I'm not sure it'd be discovered then um where you take this is where I need the cursor
yes that one
yes you take some input you send it to some hidden units which send it to more hidden units by the
green connections and sends it to more hidden units it comes back to the input so you reconstruct
the input and then you send it around again not all the way around but up to there and up to there
using the right connections and then the learning rule which you'll notice doesn't involve explicit
back propagation is to say for these neurons for example I change the incoming weights by the
activity of the presynaptic neuron down here times the difference between what I got on the green
activation on the red activation first time round and second time round so the rate of change
of the activation of the neuron is what's used to communicate an error derivative
now unfortunately this thing has the wrong sign um but later on we fixed that
and so here's a theory from 2007 that still hasn't been conclusively proved wrong
and it sort of works but doesn't work quite as well as we hoped about how you could get a brain to
do back propagation what you first do is you learn a stack of autoencoders that is you learn
to get each layer to activate features in the layer above from which you can reconstruct the
layer below so you learn some features that can reconstruct this layer then you treat those features
of data and learn some features that can reconstruct them you built a big stack of autoencoders like
that okay once you built the stack of autoencoders then each layer can activity in a layer can
reconstruct the activity in the layer below and then you do top two top down passes you do a top
down pass from the thing you predicted at the output so you put an input activity goes forward
through the layers you predict something at the output and now you do a top down pass and you get
reconstructed activities everywhere and then you take your output and you change it to be
more like the desired output and now you do a top down pass and you'll get slightly different
reconstructions and the difference between those two reconstructions is actually the signal you need
for back propagation and so if you do that the learning rule is that you should change your
synapse by the pre-synapse activity in the layer below times the rate of change of the activity
in the layer above in the post-synaptic neuron so it's a very simple learning rule the
it's change the weight in proportion to the pre-synaptic activity times the rate of change
of the post-synaptic activity now it turns out if you're using spiking neurons well that amounts
to that are representing underlying firing rates that are changing that amounts to a learning rule
that looks like this what you do is you take a pre-synaptic spike and you ask whether the
post-synaptic spike came before or after it because what you're interested in is the rate of change
of the post-synaptic firing rate around the time of the pre-synaptic spike okay
and if the post-synaptic spike occurs often just after it and seldom just before it that suggests
the firing rates going up and if the post-synaptic spike occurs often just before the pre-synaptic
one and less often just after it that means the firing rate of the post-synaptic neuron is going
down so if you want your learning rule to be the pre-synaptic activity well you'll only learn when
you get a pre-synaptic spike and then what you'll do is you'll say did the post-synaptic spike occur
afterwards or before if it occurred afterwards I should raise the weight and if it occurred before
I should lower the weight and so your learning rule will look like this and this thing is actually
a derivative filter it's centered at zero and what this is really doing is measuring the rate of
change of the post-synaptic firing rate and of course it's sampling it so you have a post-synaptic
firing rate there's these spike trains and are the other spikes getting closer together or further
apart well this is a way to measure that and of course you could do the learning on individual
spikes and the learning rule would then be the implementation of this idea that the rate of
change of the post-synaptic firing rate is the error signal the learning rule would just be
if the post-synaptic spike goes after the pre-synaptic one increase the strength otherwise decrease it
and have that whole effect fall off the spike get further away because we're really only interested
in the rate of change of the firing rate around the time of the pre-synaptic spike
now there's one consequence of that which is that if you're going to use the rate of change
of a neuron to represent not what the neuron's representing but to represent an error derivative
you've basically used up temporal derivatives for communicating error derivatives so you cannot use
temporal derivatives to communicate the temporal derivatives is what the neuron represents so
i have a neuron that represents position i can't use how far that's changing to represent velocity
and that's true of neurons if you want to represent velocity you have to have a neuron
whose output represents velocity you can't do it with the rate of change of a position neuron
if i kill the velocity neurons and keep the position neurons then and i watch car moving
the position neurons will change but i won't see any motion
similarly you can't use the rate of change of a velocity neuron to represent acceleration
okay so i think the fact that you can't use the rate of change
of a representation to represent that that stuff in the world is changing is more evidence
of all of the idea temporal derivatives of neurons are used up in representing error derivatives
and so now i'll summarize
the main arguments against back propagation
the fact they spent neurons and spikes rather than real numbers well that's just
because a lot of noise regularizes things you can represent error derivatives as temporal
derivatives so the same neuron can send temporal derivatives backwards communicate those backwards
and communicate activities forwards and the fact that in the brain you do get back to
independent plasticity seems to be evidence in favor of that representation of error derivatives
and now i'm done
so
thalamus you jeff for a great talk sorry that was a twitter joke
got it anyway um so now what we're going to do is a uh brief q and a between myself and jeff
and then after i've had my chance to ask some questions i'm going to open it up to you guys
now i had originally sent jeff a few questions which i'll rely on partially but his his talk
has made me want to ask a few others so i'm sorry i'm gonna throw a few loops at you as well
but let's start with some of the ones that i told you i wouldn't give you something funny with
there is something funny with my mic is it i don't know if the av guy is there i'll just
won't look down yeah don't look down um okay so the first question yeah it's okay here
i'm going off script anyway the first question uh which i would like to ask just because it's
something that i spend far too long arguing with people uh online is essentially you know so you're
in the computer science department you've come here you've given us a talk that's largely about
brains but many people seem to object to the idea that computers have anything to tell us about
brains or indeed the idea that the brain is a computer despite the fact that neuroscientists
often refer to computation in the brain so my my question is to you is the brain a computer
why don't i just hand that over to you first oh yes good okay
and for the record i didn't tell him to say that if anyone from twitter is watching uh and b
can you just maybe give an intuitive understanding of why the answer is yes despite the fact that
obviously our brains are very different from our laptops or our cell phones and stuff like that
right so there's there's many ways you can do computation with physical stuff and you could
get some silicon and make transistors and then run them at very high voltage much higher than
needed to make them be digital and then you could if you wanted to rep to represent a number you
could have bits and you could and so on and you could create multipliers and adders and then you
could put all that together and you could have some bits that tell you where in memory to find stuff
and you could make a conventional computer or you could make little devices that have some input
lines that are hardwired with input lines um and you could have adaptive weights on the input lines
so early neural nets um Marvin Minsky made neural nets out of um feedback controllers that were used
in i think b 52 bombers or b b 29 bombers or something b 27 i don't some kind of bomber it was
america um and so you can make computers in lots of different ways when i was a kid i used to make
computers by um you take a six inch nail and you saw the head off and then you wrap copper wire
around it and then you take a razor blade and you break it in half um so that it's a nice flexible
thing like this and you wrap a bit of um copper wire around the razor blade and then when the current
goes through the nail it'll make the razor blade go down and you'll make a contact so now you've
got a relay and then you can put a bunch of those together and make logic gates i never got more than
about two logic gates that way but um yeah you can make computers in lots of different ways and the
brain is clearly made in a different way from um the normal computers which has some different
strengths and weaknesses so it's much slower um but on the other hand you can make it much more
parallel it has one special property which i think is what makes us mortal which is that every brain
is different so i can't take the weights from my brain and put them in Blake's brain and hope that
it'll work because he just doesn't have connections in the same place tried right i you well there's
a way of doing it where you i take the weights in my brain i turn it into strings of words they
absorb these strings of words and put different weights in his brain um it's pretty lucky all
our brains are different because otherwise rich people will grab poor people's brains so they
could live forever but quite okay so um i think uh i uh want to ask you then following on that
what do you think about some of the quests to fully characterize the brain's connectome do
you think that is a scientifically worthwhile endeavor if we yes i do um partly because some of
the people doing it are my friends uh ignoring your loyalty to Sebastian what oh well in that case no
it seems to me it is very worth doing yes um but you don't have to do that in order to
begin to understand the principles very good but for things like the retina which has a lot of
hardwired stuff in it i think it's really important to do that okay so that actually leads on to my
next question um i wanted to ask you about hard wiring so another thing that i think uh many people
who study the brain find difficult about artificial neural networks as a model for the brain is that
as you say you start with random weights and you train it on a lot of data and you get these things
out but we know that there are some pre-wired things in many brains so you know the classic
examples are a horse can run pretty much right out of the womb but even within humans arguably
there are some things that we find easier to learn than others yes and um so what do you think is the
place for innate behavior within neural networks as a model of cognition okay so it used to be when
i was a student if you were interested in language people would tell you that it was all innate
and it just kind of matured as you got older and maybe you learned like 12 parameters that
characterized your particular language whether it was subject verb object or some other way um
in fact there's a nova that i saw it's probably made about 20 years ago and it has all the leading
linguists all of whom were educated by chomsky and they look straight at the camera and they say
there's a lot we don't know about language but one thing we know for sure is that it's not learned
so chomsky had really good kool-aid yeah he did but it's over um because we know now if you want
to translate you just learn it all the number of linguists required to get a system that can turn
a string of symbols in english into a string of symbols in french is roughly zero i mean
linguists involved in preparing the databases for training and making sure you get sort of
variety of grammatic structures and things but basically you don't need linguists you just need
data so you don't need much innate structure the issue of what is innate it doesn't it seems to
me there's not much point putting in stuff innately if you can learn it quickly so for
example the ability to move and get 3d structure from motion that's actually very easy to learn
so i don't believe that's innate even though a child can do it at like two days you show them a
sort of w made of paper and you rotate it in a consistent way and they get bored and as soon as
you rotate in a way that they move it in a way that's not consistent with the rotation
the interest perks up um but i think they can learn it in two days it's really easy to learn
okay interesting now uh so i want to ask you i know partially what your answer is going to be but
when i remember long ago you told me that one of your career goals uh at least earlier in your career
was to prove that everything that psychologists thought about the brain was wrong
and so my my question is what was it that they had wrong are they still getting it wrong and
is neuroscience getting that same thing wrong it was mainly to do with this conviction that
psychologists had partly based on chomsky that there was an awful lot of innate stuff there
and that you couldn't just learn a whole bunch of representations from scratch there was this
innate framework and there was a little bit of tuning of this innate knowledge and that's what
learning was and that's just i think that's a completely wrong headed approach in fact
i want to go the other way and i want to say the stuff in the brain that's innate wasn't
discovered by evolution the stuff in the brain that's innate was discovered by learning um
do i have time to do that digression yeah um okay so imagine we have a little neural network
and it's got 20 connections in it and each of those connections has a switch that could be
on or off so it can let stuff through or not so you've got to make 20 binary decisions
so your chance of making them by chance is one in a million and making the correct decisions
now this little neural network circuit is a mating circuit and so the neural net goes into a
singles bar and it runs this circuit and if it's got the connections right it has lots of offspring
and if it hasn't got the connections right it doesn't have offspring or doesn't have so many
offspring okay so let's start off with the connections being if they were just kind of
random you did mutation what would happen is you'd have to build about a million organisms
before you got a good one and if you had sexual combination of the organisms let's have a really
simple biology in which each connection has its own gene and this gene has two alleles
for on and off okay um if you do mating now you might have an organism that got all 20
connections right and it mates with one that has a few wrong and it gets a few wrong ones
and now it's wiped out it doesn't have lots of offspring anymore that's it um so it seems like
a complete disaster and it would obviously take you at least a million organisms to expect to
get a good one even if you have pathogenesis where you didn't have sexual reproduction
now i will show you how to build a good good organism in only 30 000 tries and the way you
do it is this you for each connection you have three alleles you have turn the connection on
genetically turn the connection off genetically or leave the connection to learning okay so that's
the third allele and now you start off with a population in which about half of the connections
are genetically determined and the other half are left to learning so that's 10 connections
are definitely determined so there's a one in a thousand chance that you'll luck out genetically
you get those 10 right and then during the organism's lifetime let's have a really dumb
learning algorithm where it just randomly fit like the one i talked about it randomly fiddles with
the connections just randomly flips connections of the 10 that are left to learning and it'll
take it about a thousand trials and it'll get the combination right but the point is it can do
those trials without building a whole organism it can just go into the singles bar and sort of
fiddle around a bit with its connections into a bang um so what we've done is we've replaced
a million trials of evolution building a million organisms with build a thousand organisms and then
let's build 30 000 organisms just to be safe and then each of those fiddles around with its connections
and it'll do this search the whole search is the same you have to try a million combinations
but the way you get the million combinations is a thousand organisms each does a thousand learning
trials and so almost all the work is done by learning now if genetically an organism has
happens to have more things set like you've got 12 of them set right it'll learn faster
and so there's genetic pressure for if you mate organisms now there's genetic pressure
to get more and more of these alleles set genetically but the pressure only comes because
the learning can get all 20 set right so this thing can mate and have lots of offspring so the
fact that the learning can find a solution creates genetic pressure to hardwire these things
so what's happening there is the search a thousand things were done by evolution
a million minus a thousand things were done by learning and that created a landscape for
evolution that allowed evolution to gradually hardware in more and more that these things were
first found by learning so I think a lot of the structure in the brain that's hardwired
is first found by learning and gradually gets backed up into the hardwiring but to get the
evolution pressure to say that's good you have to be able to do the learning if you're just hardwired
things you'd never find anything that was good okay great thank you now that's called the Baldwin
effect by the way yes yeah it's called after a psychology professor at the University of Toronto
in the 1890s called Baldwin who invented this effect he didn't do any computer simulations
so I want to do one follow-up question on that and then ask my final question before handing it
to the audience so my follow-up to that is you know I think one of the things that is unclear
in terms of the success of deep learning is exactly how much it was purely the compute
or some clever things now I've seen both cases argued and you today kind of suggested that it
was just the compute but I want to ask you about this following on your last point which is that
we know that if you build networks with particular architectures and with particular learning rules
you are effectively making learning faster if you do it right and arguably a lot of the success
of deep learning has actually been as a result of people thinking about good designs for their
networks and good ways of making learning faster yep so would you potentially say that
we have seen that process that you just described actually occur within AI over the last 10 years
of the learning kind of backing up into the hard wiring I see
I need to think about that what we've seen I mean Yanlaka invented convolutional neural nets
right yes in the in the late 1980s but computers weren't fast enough to really do a lot with them
so they were used for handwriting recognition and they were used to reading 10 percent of all
the checks in America but they didn't really take off they really took off when the computer
hardware came along to make them really efficient so that's a case where the ideas were had first
but without the hardware they didn't work you've obviously got to have both right yes quite
okay so now my last question before I hand it to the audience is just what do you see as being the
future of the interaction between neuroscience and AI do you think that there is space for a sort of
new cognitive science where we study general intelligence but with brain-centric models
rather than logic-based models or will we see the two streams depart over the next few decades
the way I like to think of it is we'd like to understand if you'd like to understand how the
brain does computation you've got brains in your computation and they look like you said they look
pretty different to begin with because there's many different ways to do computation and with a
conventional digital computer you can get it to pretend to be anything so we're getting it to pretend
to be some other kind of computer an artificial neural net and we'd like to sort of bridge this
huge gap between brains and what we can simulate on computers and so neuroscientists are sort of
doing experiments and good computation neuroscientists are sort of doing experiments to try and sort of
see how you could do the computation and I think of myself at this end as doing simulating things
with artificial neural nets to see how you can make it more biological and we're trying to build a
bridge and so the computational neuroscientists most of them are building from this end I'm
building from the other end but obviously if you want to build a bridge to somewhere you need to
look at where you're going and so I'm trying to build a bridge that does computation more and more
like the brain does it or like I guess the brain does it from what I my neuroscientist friends
tell me and then there's conventional AI which is trying to build a bridge like that
great okay thank you so now I'm going to open up two questions from the audience now for this
we've got this kind of interesting system here so rather than you putting up your hands and me
selecting you you can actually nominate yourself to ask a question by pressing on the button on
your microphone and it is a first come first serve basis so you're going to be queued up
and now so you're now first on the queue and by now it's too late to be able to ask a question
yes and one last thing about that though when you are done asking your question please turn off
your microphone because that will open up this slot for the next person in the queue okay go ahead
I've got a red light here does that yeah if your red light is flashing that means you're on
you get to ask a question
if your red light is flashing you're on
I've got a solid light please oh solid sorry okay I've got a solid light you you were faster
okay so this is a Clifton suspension bridge analogy for your for your interest here so
you mentioned briefly Hebbian synapses as neuroscientists we have a good understanding of
how they work at a molecular level so my question is to what extent are the understanding of
biological memory mechanisms ie Hebbian synapses implemented by AI for deep learning and the sorts
of systems that you are describing so at present people don't use Hebbian synapses for most deep
learning they're using back propagation so it's an error correction rule as opposed to something
where if you just use it it gets stronger but if you want a short-term memory for things like
transformers to remember a temporal context just a simple Hebbian synapses is a good thing to have
yeah but Hebbian synapses can code memories in humans that can last a lifetime so is this
something that AI is working towards using or are we just going to bypass Hebbian synapses and
come up with something superior okay so if you think about what's been successful in the last
few years it's using error correction learning with either labeled data or trying to predict what
comes next and not Hebbian not Hebbian synapses now people like me who sort of do this kind of
learning but are interested in the brain know this isn't right we're much more interested in
unsupervised learning we just can't make it work very well yet and I would love to be able to get
learning to work as well as it does when you do back propagation without using biologically
implausible things and one place we can do that is with temporary memories so if you say synapses
have a fast component you can use Hebbian learning for that fast component and that would actually
help neural networks work better even if you're using back propagation for the slow component
that didn't really answer your question but you know filled the time
hello I read in the reinforcement learning book that dopamine is used as a reward prediction
error signal so I was wondering do you see it used as a supervisory signal like you mentioned
earlier okay so for reinforcement learning there is some lovely work done by Peter Diane
and who is the theoretician and some experimentalists showing that the real data from neuroscience fits
in with a theory that was started with Rich Sutton and Peter Diane did the work of showing that
dopamine is corresponds to something in a particular learning algorithm and it doesn't
correspond to the reward it corresponds to the I think it's the difference between the reward
you're expecting the reward you get so if you're a monkey and you're expecting a grape and I give
you a piece of cucumber that's negative reward and that will be a big negative hit of the dopamine
um so that's not the kind of learning that's been really successful so far if you're willing
to burn a lot of computer time um reinforcement learning will solve some problems but it's not
the kind of learning that's been most successful in AI so the the difference is in reinforcement
learning you get a single scalar number you get one number whereas in error correction learning
you typically get a whole vector of numbers right here hey so you mentioned that your goal
kind of the bridge analogies your goal is to go from you know computers and try to get to the brain
so okay let's just say that kind of makes sense to think okay let's get to more general AI because
I'd say humans are decently general and neuroscientists are trying to get from
the other bridge the brain to uh generally AI so you have these two kind of debates and this
happens quite often where is it correct to go from generally AI to the brain first understand
generally I then understand the brain or brain to generally AI and so what would you say is
the most practical way to problem problematize general AI
I don't like the phrase general AI um I don't think if you want intelligent devices I don't
think you want to produce a sort of general purpose android um I think you want to produce
different devices that are smart in different ways so basically if you want intelligent
machines that do things you have a vacuum cleaner you have a backhoe you don't try to make one thing
that's a vacuum cleaner and a backhoe it doesn't make sense what about connecting them through
like kind of like different cognition areas in the brain yeah but I think it's the same with
cognition too I think the the neural net that does machine translation isn't the same neural
net as does vision um I think yeah my guess is that people are thinking too much about
making one neural net that does everything and not thinking enough about making more modular
neural nets that are good at different things some are more universal than others but I think
that's how progress has been made that's how progress has been made so far not by the people
talking about general AI it's been made by people looking at saying how can I get a neural net to
vision or how can I get it to machine translation thank you
hi I'm just wondering about um the role of hierarchy um in general so I mean there are
different type of hierarchy there's like different layers of neural network and as I
you mentioned there's like fast memory and slow memory and then so I wonder um are there more
ways to add hierarchy hierarchy um to neural networks to make them more useful or emulate actual
brain um yes probably um so in vision for example um I mean you you have multiple layers that is
multiple cortical areas in the visual pathway that's a very different kind of hierarchy
from what you need for dealing with the sort of structural reality so in reality there's
there's the universe maybe many of them that's one's enough and then there's galaxies
and then there's there's probably things above galaxies but anyway then there's in the galaxies
the stars and then there's solar systems and then there's planets and and so on and we can do that
all the way down to atoms right and you can imagine all that you can represent all that in your brain
and clearly what's going on is out in the world there's this hierarchy that goes over many many
orders of magnitude from the universe down to quarks or whatever the smallest thing is now um
and you don't want that kind of hierarchy in your brain what you've got in your brain is the
ability to deal with a little window of hierarchy where there's a sort of object in its parts
and to deal with the whole universe what you do is you can take this window
then you can map at a scale of the universe where there's a universe and there's the galaxies or
there's the galaxies and there's the stars um or there's the atom and there's the electrons um
and so you're using the same neural hardware but mapping reality onto it differently
and so I think whenever we have to deal with anything complicated we use hierarchies
but the way the brain uses them is by varying the mapping from reality onto the brain and it
really can only operate with a small window on a hierarchy which you can move up and down
much like you only have so small region of high resolution which you move around
so logarithm sorry like logarithm what about logarithms like that's what you're talking about
right compressing a big range into something that is much manageable I wasn't thinking of it like
that I was thinking of it as you have some fixed hardware and when I'm thinking about the solar
system my fixed hub I couldn't possibly deal with the universe that's much too big and it couldn't
possibly deal with an atom that's much too small but it's fine dealing with the sun and some planets
and maybe a moon or two um what I'm trying to get at is we need to make a big distinction between
the hierarchy in the real world hierarchical structures in the real world and how we deal
with them cognitively where we use attention and we only ever deal with a bit of the hierarchy at a
time that's not the same for say aspects of language where you have notice with vision I can use
the same neurons for representing the sun and for representing a nucleus it's just an analogy but
it's the same neurons I'm using um now for if I'm processing language I've got things that find me
phonemes and things that turn phonemes into words and things that turn words into sentences
and those I can't move a window like that that's a fixed hierarchy there's phonemes and there's words
and there's phrases and the sentences and that's all sort of fixed in the brain you do that's not a
flexible matter you can't kind of move the sentences down so they're where the words were
move the words down so where the phonemes were that doesn't work um so there's some hierarchies
that really do relate to sets of neurons in the brain they're like the layers in the
in the connections models there's other hierarchies like the whole spatial structure of the universe
where what's in the brain is a window you move over that hierarchy thank you
thanks dr hinting for excellent talk and excellent ideas about the feasibility of
back propagation um my question's maybe more boring about the statistical comments that you made
like I was that dale pardon me are you dale no i'm kyle
you sound like dale sherman's um i'm at the university of alberida
hi well that's a good instance that you sound just like dale sherman's and you're at the university
they train us all to speak the same
are you a student of dale's no
i think i've only met a voice you know if you're a student of dale's i need to watch out because
it's going to be a very tricky question it's not it's the question is that i was trained with this
intuition that you can't over parameterize your models that if you're trying to fit a line that
you need two points if you're trying to fit a curve you need three and so on and that scales up and
you should always have a little bit less data points so i know that you have shown clearly and
and the field has shown that that's not true what were the statisticians getting wrong in their logic
to be it's to do with regularization that you need it to be highly regularized but first of all
i'll show you that if you want to fit a if you want to fit two data points well let's take three
if you want to fit three data points you would have told me you want a polynomial with only
three degrees of freedom so you want a constant and a slope and a curvature and that's all you
can afford with three data points that's wrong now this is where we need a pen oh sorry
they don't work i tried them all and none of them work okay well done extra points
okay yes can you see that yeah okay and we're gonna have three data points
and actually if you're a statistician you'd probably say for three data points you probably
ought to fit a straight line like this because i could fit a parabola and the parabola would fit
exactly and that's a bit suspicious in other words the parabola fits exactly but do you
really believe that if you were to ask when x is zero what's the value of y do you really believe
that value for y because a straight line is far more conservative right so a statistician probably
say fit a straight line however that would be a frequentist statistician if you took a Bayesian
statistician all right and this is in chris bitters machine learning textbook there's a nice
picture of it i think somewhere i think it's that book a Bayesian statistician would say okay let's
try fitting fifth order polynomials and fifth order polynomials um we might even fit ones that
don't exactly go through the data but for now let's make them go through the data so we fit a fifth
order polynomial that goes kind of one two three well you know some order and we fit another one
oh that you can go through the data and we keep fitting these guys and we fit a gazillion of them
and what you see at the end is that these gazillion ones in between the data points
they're kind of all over the place and their average is in a sensible place like here
but their variance is big and what they're telling you is if you give me this x coordinate
i'm rather uncertain about this y coordinate but this is a good bet um and similarly here
and if you go out here these polynomials are just all over the place and they'll tell you
if you give me this x value then it could be pretty much anything that's not a bad bet
but it could be pretty much anything and that's a much better answer than you get from a straight line
yeah so by fitting a very large number of different polynomials and then averaging
you get good mean answers and you also get a sense of the variance okay now drop page is
doing something like that yes yeah and that's brilliant thank you for coming up with dropout
the um we many of us here are working in a regime of sparse data and so we have a couple channels
a couple signals a couple voxels and um you've convinced us that we need more but how is is
there a way forward in ai that can manage with more sparse data or is is this the only regime
that's going to be able to make success so the really big successes have been on big databases
and i think we should be using even bigger models but you can't get away from the fact that actually
if you're going to have something that starts off random and sucks all its knowledge from the data
you better have enough data to suck all that knowledge from the bigger your model the better
if you regularize it but you still need a lot of data so the way you should think about is this
if you've got 100,000 data points that's small i know that's very depressing if you're a neuroscientist
well it's not depressing it seems it's impossible if you want to personalize medicine for one
individual and you want to train a model on their data from their brain it does it seems like there's
going to be a disconnect between what these models can do and how they might help someone in the
future and yes and no if i train a model on a very large number of people and then apply
that model to one person that's a formal personalized medicine that really works
great thanks
oh and say hi to dale for me
thank you for all thank you for the presentation my last question is about this dropout you
know the thing is that you randomly just drop some parts of the network and then you say okay that
it works better so i would accept it but do we have any like intuition why for example some parts of
it work better or if we try to embed this like network into like is a more graph is amorphous and
whether these two different graphs different graphs that we took whether there are are there
any similarities between them or we just do it randomly i guess the problem with the randomness
i guess we are trying to put the the burden of prediction on the random part of the computer
that's what so i didn't hear the whole question but certainly in dropout what we do is we randomly
leave out units we now you can also do block dropout you can take groups of units randomly
like the groups and what that does is it allows the units within a group to collaborate with one
another and then between groups they have to be fairly independent and that's called block
dropout and that works quite well too um but i didn't really hear the rest of your question
okay the question was about that okay imagine that you can you talk closer to the microphone
because i'm partially dead okay the question is that imagine that you have a dropout of 50 percent
and you're trying to get rid get rid of like 50 percent of your notes and in the notes in the
network and the question is okay whether we have any similarity between the types if we do it
intuitively whether we would find any similarity on the structure of the network that would produce
the best results and if it's so whether it would correspond to something physical like for example
if you're doing a vision thing whether it would correspond to something in brain or not i miss
yeah lots of people have thought about whether you can do better than random in dropout right
and there's some work on that like block dropout the works can work for some things
but i don't really have much to say about i don't really know the answer to sort of is there
something much more sensible than dropout there's a lot more structured there might well be but i
thank you okay so with that we're gonna have to end um so please join me in thanking jeff
and uh i want to also to thank blake for for hosting this event and um i i felt the questions
could have gone on all night but uh tip-off isn't half an hour so some some of us have to move on
so thank you blake

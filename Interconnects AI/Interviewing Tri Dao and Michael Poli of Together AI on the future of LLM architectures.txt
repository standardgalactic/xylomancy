Okay. Hey, everyone. Welcome to the first interview that we're going to post on interconnects.
I'm really trying to bring more scientific voices into the AI discourse as media is covering
a lot these days. I'm happy to be here with Michael Poli and Tree Dow, experts in some
of these non-attention architectures that have been really blowing up in the last few
weeks of December. So, Michael, do you want to introduce yourself first?
Sure. Thank you. Thank you, Nathan. For inviting me. I do research at Together AI and was also
a PhD student at Stanford working with Stefan Orenmann and Chris Rea. That's how I met Tree as well.
If I had to choose, maybe I moved to a few different areas of research. If I had to choose one,
I like to research at the intersection of signal processing, dynamical systems, and deep learning.
And most recently, luckily, there's been more interest in efficient architectures that use
some of these principles to improve scaling along different axes and to get new trade-offs at
the end of this time. Great. And Tree.
Hi, everyone. Thanks, Nathan, for hosting us. I'm really excited to be here. Tree, I just finished
my PhD at Stanford. I'm an incoming assistant professor at Princeton, and right now I'm Chief
Scientist at Together AI. It's a startup working on AI infrastructure. And I've been working at the
intersection of machine learning and systems, so designing algorithms that take advantage of the
hardware that they run on. We're just in long-range dependencies, how to encode that into model,
designing architectures that can learn long-range dependencies. Yeah, really excited to be here.
Yeah. Okay. I think I'm going to have some questions to dive right into this. I think
you too will kind of both answer them or someone can answer longer, whatever you want. I think to
start with, we should talk about maybe why attention works and what the limitations of attention are.
I think almost every person in tech, broadly, now knows that a transformer is a model built
with attention. And ChatGPT does that. But why is this so good? Even how much of a transformer
is built with attention? Are there other things going on? And what might be challenges there?
Right. So, you know, a transformer, which is this architecture that powers most of the exciting
applications that we're seeing nowadays, as you mentioned, ChatGPT and so on.
Attention is kind of the core layer there. And the attention actually became earlier
around 2014, 2015, and then transformer came out, incorporating that, focusing a lot on attention
with these MLP, interleaving MLP and attention. And I think the success largely has been,
they seem to be able to scale really well. So, you can scale out the models with more parameters,
with more data. And that has been the recipe for success. It sounds obvious now, but I think
five years ago, that wasn't clear. So, it seems like, you know, transformer architecture is a
hugely successful one. And, you know, a couple of reasons why it's successful. I think it's like
general enough that it's able to learn a lot from data. And two is pretty friendly to hardware. You
can, there's no kind of sequential dependency like previous RNNs. So, as a result, you can run it well
on GPUs, TPUs. You can scale it up. It's very hardware efficient. I personally have worked on
making it more hardware efficient as well. So, it's just kind of the recipe for success, which is
general architecture that scales well. If you're an NLP person, maybe you'll say, you know,
incorporate some kind of inductive bias for it to protect. Personally, I think it's a very general
architecture that scales well and it's hardware friendly. Yeah, yeah, it's remarkable how it
seems so obvious now. And it's like, I think one of the things that studying this work is the context
length becomes a really interesting access to study alternatives to it. And essentially,
I think, Michael, do you want to talk about that? I could babble, but you know more. Sure.
Yeah, there are several points. I'll start by saying that, you know, there's still great research
trying to understand why, from first principles, why is it that the transformer can learn these
interesting circuits. People can study, they pick apart the computational combination of different
heads and transformers and so on. So, there's work on basically understanding transformers
from programming language that is encoded. But I think, as Trey mentioned, there are some very,
very, very interesting design choices that have gone into the transformer. This interleaving
of attention in NLP is quite important. And also, the transformer is essentially
successful in the beginning because it encoded these techniques that have been developed for
RNN, Celestium, so these other classical NLP models, gating to modulate which information
is absorbed into the model, gating to determine how quickly something is forgotten in this
recurrence of the RNN into a parallel form. It is easily a bunch of gems that can easily
well, not very easily, but can be optimized for RNN GPUs.
Yeah, that's all great. I think the, I guess the specific thing that I had in mind is how
attention ends up being this kind of quadratic scaling in terms of cost when you have an input
sequence that you have, if you have an input sequence of length, though, and you want to
output a sequence of length, though, essentially, if you zoom into the math and you look at what's
happening at inference in most of these libraries, you have this like upper triangular attention
matrix where you say like you can only look at the past entries of your text. And as you go
through there, then you end up getting this along, you get this L squared relationship where the
first token, you can only look at one and then you end up looking at more tokens for each pass.
And now we've been talking about recurrent neural networks and how does something that
that isn't attention, like get around the fact that you want to look at all of the history
of the text in your sequence. So like if you write a long prompt to chat GPT,
you really want all that information to be encoded and how could doing something other
than this dense attention matrix, like actually make that possible?
Yeah. Right. Yeah, so you can go ahead. You know, before attention, there was RNNs, right?
Like in RNNs, like they process text just fine. And maybe they didn't scale as well, but yeah.
Can you say briefly what a text by encoding text? He said briefly what a RNN is. So these are
Yeah. So these are recurrent neural nets that go back, I think to the eighties.
Maybe some of the more famous ones are LSTMs, GAU. So they were pretty popular in
around 2012 to 2016 or so. They were kind of state-of-the-art for translation,
speech recognition, I think NLP, like they were a state-of-the-art and they process text kind
of sequentially. They see essentially one token and then that changes the hidden state and then
they will update the hidden state and every time they see a new token. So I think it's kind of,
in some sense, mimicking how, for example, human brain process information, like you read
the sentence or a passage and maybe it's like you're storing some information in your brain
and by the time you finish reading the document, maybe you can answer questions about the documents
without having to read to refer to that document again. So RNNs kind of work that way. They process
the text and then that changes the hidden state and their hidden state is the representation
that can be used to either generate new tokens or classify the documents or whatnot.
So these work well back in 2016 or so, but they've kind of found out of favor,
empirically, they don't do as well as Transformer, I think. And as you touch on Transformer,
because of this kind of quadratic scaling, you compare every token with every other token that
comes before it, it gives you this very kind of easy way to propagate information. And I think
that's for a reason why Transformer in attention does really well. But there's been more recently
some of the newer RNN architectures that seem to do pretty well. So RWKV is, I think, one of the
earlier ones. I really admire that project, its effort mostly from one person really going against
the orthodoxy of Transformer, showing that RNNs can be pretty strong. Who was the lead on that?
Bo Peng, I think. And it's an entire project, but I think it's pioneered by Bo Peng. I think it's
affiliated with Eluta AI and the Compute Sponsor by Stability and so on.
I was reading this earlier. At a technical level, they try to replicate something like the
query key value lookup of attention with two linear RNNs to essentially be able to remove
the specific attention scaling potential problems and with two RNNs, which have
this better, like, long context behavior and different implementation rules.
I think, and they also, the paper trained up to 14 billion parameters, which kind of leads
into some of the next questions. I was going to ask, I was going to ask Tree about Mamba and
then Michael about Stripe Tahina. I think you could go in either order. I think these came out
about a week apart and were these two language models kind of seen as being way closer than
anyone would expect. Essentially, Stripe Tahina came out and the evaluations were close to models
I've been training on all year, like Lama 2 and Mistral 7B. And I went in and I went to the
together API and I did like side by side of Mistral versus Stripe Tahina and it's a good
language model. It answers most questions. There's no obvious failure modes. I think maybe
Michael, do you want to comment on that? I know it's another big project and then we can go back
to Mamba even though it's slightly out of order in the chronological, the release cycle that happened.
Sure. So, I guess I'll start by saying that there's an interesting connection between all
these new methods. There's this sort of convex set, which has a center and there's this connection
between linear attention, so attention without the softmax, linear arenas and state space models
as a sum. So, at some level, kind of the mathematical formulation of this kind of base
model here, I'm not talking about the base architecture, just the fundamental model,
is the same. And then you can go in different directions and each direction has its own tradeoffs.
You can go to the feature map direction, the kernel direction. So, when you break down the
softmax, you take away the softmax. You can place on queries and keys, kind of the fundamental
entities that compose your attention matrix. You can compose other kernel-like functions,
other functions that you hope would approximate whatever capability of attention you like.
You can do things like a Taylor approximation, Taylor expansion, for example. And you have a
slightly different perspective, but you get something that, again, is very similar. You
can go to time variance, so you take the RNN and you push this input dependence, so the way the
computation inside the linear RNN is conditioned by the input sequence. And you can add things like
gates. We've seen a lot of work, for example, modernizing the inner attention with additional
gates that allow you to make better use of your fixed state dimension. And then you have
the third direction, at least in my mind, is the one that pushes, that uses the convolutional form,
that pushes more towards other types of linear operators that are still associative, that are
still, that still allow you to train in parallel. So, here are things, timing variance systems,
I can elaborate on any of these points, but things that can switch between convolution
center currents, like this form model, with additional gates, again. Striped IENA was
born as a project from the IENA architecture, which belongs to this third category that I
just mentioned. And we're really trying to get the best per flop architecture that we could.
And one principle that was validated over and over again,
and we're trying to understand better now, is that it seems composing, hybridizing different
layers, the layers, different blocks of different categories, and even full attention
yields something that is better than the individual components. So, there seems to be a
compositional aspect of these models that we're trying to understand better. And this gives you
a better sort of pre-trained model per flop. And with this model, we ran a whole suite of
skating laws and so on. Hybridizing also gives you, since we wanted something that would be
kind of usable out of the box, it gives you a way easier time when you're fine-tuning for
longer contexts, we can apply some of these techniques that have been developed for transformers
and kind of surprisingly work okay for hybrid as well. So, things like linear scaling for
rotary embeddings and so on, you can go into the details. So, it was mostly a project,
what is the best, given the current landscape, what is the best we can do?
Yeah, that's a great description of it. I mean, the sentence in the blog that's like,
start training is optimized using a set of new model grafting techniques, enabling us to change
the model architecture during training, kind of felt like, to me, that there's a ton going on there.
And some of which you probably can't talk about, there's normal data. I don't think all the data
that was quite explained, like what the longer context data was, but it's like,
are you taking this from models, starting point from models that people would know,
and can you say any of that? I think even just the summary that it's synthesizing recent work
into a strong model is great context for people. Yeah, well, that line, so we've,
given this explosion of primitives that, you know, described, and given sort of the
cost that it would require to evaluate all different combinations, we found ways to essentially
start training with a configuration and then continuing on with another configuration. I
think we'll have, we're going to have more work or paper. Yeah, there's so much cool work in that
area. So one of the, someone at AI2 is working on a project where they're essentially trying to
cut the Lama models in half and keep training them and things. It's just the wild west out there
with people trying to take strong models and make them smaller while still getting the performance
benefits of bigger models. I think that's a whole aside, but I wasn't expecting it to show up when
like you follow the social media, I've striped by people are like, oh, non-attention models are
finally good. And it's like, it covers up a lot of the details that are very interesting about it,
in my opinion. So, okay, it turned back to tree. I think Mamba actually happened first among these.
I did the, his reading back of social media. And it also was very surprising to me. I think
the, the largest model in the Mamba suite is 2.8 billion parameters, if I remember correctly.
And it was compared to a lot of the common benchmarks in open NLP. So things like GPTJ,
Pythia model suites and the scores on the benchmarks reported were really strong. And
I think if you want to start with the high level summary, and then I'm definitely going to make
you talk about the awesome new CUDA kernels and stuff that you had to write for this project.
Yeah, so this Mamba is a collaboration with, with Albert Gu, who's now, he was
usually just doing at Stanford. That's where we met. And he's now professor at CMU.
And also at a startup. So it's a wonderful collaboration credit goes to him.
Yeah, Albert has been working on this line of work called state space models.
In some sense, as mentioned, it connects to things like linear tension,
linear RNN, convolution, neural nets. And that's what his PhD thesis is about.
I've also worked on space state space for the past couple of projects.
My, my angle is how to make state space more hardware efficient and
kind of increase their expressiveness. So it's wonderful working with, with, with Albert.
And there I think is more of a proof of concept, which is can state space actually do as well as
as transformer on language. So we've, we've seen previous papers showing state space could be better
on audio could be better on some of the tasks on the long range arena. But language has always
been the most difficult to get to, to, to do well for state space models. And language is also
kind of the thing that people care about the most right now. So momo was more of a proof of concept,
which is, hey, we want to show that safe space can be competitive or maybe even be some of the
transformers out there. So we, we've validated that at the scale up to
3B train to 300B tokens. So in absolute terms, you know, these are not very strong models.
These are not yet models that you would actually play with and expect it to do meaningful things.
I think it's more of a more of an academic comparison in terms of architecture. It's like,
hey, training train for the same amount of tokens. It does as well, maybe slightly better
than some of the transformer out there. So, and that's in particular has been
very exciting to us. I think Albert's been pushing on this for, for a while. I've been
pushing on this for a while. And I think finally it's like, it seems to, to, to finally be kind of
closer gap or even surpassing the transformer. And it just, just, I think it's opens up a bunch of
opportunities. So inference could be way faster. Maybe we would have different ways to understand
how in context learning happens, et cetera. So lots of, lots of future work, I would expect.
Yeah. Can you go into some of the, like, what does it actually take to implement some of these
new CUDA kernels? I just remember when this paper was announced, Sasha Rush, who's also very active
in the space, I recommended me to talk with you to was tweeting about the types of files or whatever.
And in the paper, there's this discussion about how like the recurrent state needs to be sufficiently
expressive, but doing so in a certain type of memory is a problem. Like, translate what this
means to like people thinking about GPUs and people thinking about these models being scaled.
Like, is it now much easier to scale these models because they work on GPUs? Which GPUs were you
using? Is there a bump that could come just from going to H100s or something? Any of that?
Yeah. So the line of work on state space, like S4 models kind of pioneered by Albert's work,
they, they are in some sense, recurrent neural network, but they have a much larger
state size. So the state size is whatever kind of buffer that you're going to store information
as you traverse or as you process the sequence. In some sense, you can view transformers doing
that as well, where it keeps the entire history. It's usually called the KV cache. It keeps the
history and keep referring to it. For RNNs, they have a fixed size state. For transformers state,
you can think of the state size as increasing. And our intuition is that the larger the state size,
the easier it is for the model to do well. So basically you have more space to store whatever
you need to remember. And so previous models like S4 and so on, they have an implicitly pretty
large state size, but they use the convolutional view to avoid having to materialize the state.
So that was, that was wonderful. Michael has worked behind architecture,
has used some of the same insight focusing on convolution. Mamba, on the other hand,
focuses on the recurrent view. So we wanted to put more input dependency in the recurrence.
We thought, you know, the thinking was that it was going to make it more expressive and the
model would do better, but that prevents us from using this convolutional view that would make
things efficient. So we had to figure out a different way to make things efficient. And
so I focused on making that efficient on GPUs. And so all, you know, our thinking was instead of,
okay, we're going to have a large state size, but we don't have to like write to actual GPU memory
like the HBM, where you can just keep that large state in a faster memory called S-RAM.
Do you think of it as a cache? So if you're more familiar with CPUs, it's usually a cache and RAM.
So, you know, if you have large state, you can keep it in the cache and you don't,
by avoiding having to write it down, you actually don't suffer too much if the state is large.
So that's, that's essentially the core idea.
This be due to like input out, like having to move the data around being really slow?
Yes. Yes. That makes sense. That's, that's a really common constraint in a lot of these things.
And it's like, I think one of the most insightful things I've had now with GPUs versus TPUs and
stuff is how mixture of expert models doesn't work very well on TPUs just because you have to like,
that essentially add a mixture of expert at a basic level. There's a routing layer that you
learn and then multiple feed forward layers that you can choose from. And when you're distributing
this, the feed forward layers could end up on a different TPU node and TPUs communicate with their
neighbors. So TPUs take a big hit relative to GPUs where within video and video clusters,
everything's connected so much more. And then it's easy to do that sort of distributed training.
That's super interesting. And it's like, do you think there's going to be,
I guess this is really where I want to open the conversation of like,
what does this mean? What is going to happen in 2024 in this space? Like,
are bigger players going to move in and be exploring this? My take seeing how good the
long context learning could be in a fundamental limit is that systems like ChatGPT are going to use
a dense, like a transformer model for most tasks. And then if you need to do summarization, you might
do a long context specialized architecture. And then we could even see a whole quiver of
architectures behind something that you're using. But I think it's just like, is attention going to
be dethroned? Is Sasha Russ somehow going to win this bet that everyone wants sort of following in
the area? Like, what are you thinking about either of you? I think transformer is still a very,
very strong architecture. And there's a proven recipe, right? You know, people are scaling to
a trillion of parameters. Right now, if you want, you say, well, I just want the best performing
model that runs most efficiently on my hardware that has the most support on the software side,
that transformer is a safe bet, I think it's here to stay. But I think there are new ideas like
state space, I know, so the linear attention ideas from linear attention, I think they're coming in.
We've seen, as Michael mentioned, that mixing some of these components seem to improve performance.
We're validated at, I think, 7B scale, but maybe it might even work at larger scale.
I think people tend to be conservative and focusing too much on model architecture might not be
worth their time. Like, the line mark protection is very, very strong. Most people are doing off of
that. They're focusing on data. They're focusing on infrastructure, which makes sense. I think on
my side personally, the other stuff is just plain interesting. There are more, I would say niche
use cases, niche for now, where some of these alternative architectures are interesting,
things like long contacts, different domains like audio and genomics. And there's just plain
interesting scientific questions you can ask, like, whether it follow instruction just as well,
whether it find two inches as well. Does it play well with quantization and so on?
There's just plain interesting research questions we can ask. Now, on the production
level, I think Transformer is still incredibly strong, very well supported,
both hardware and software. But I think some of these new ideas are coming in and people might
start putting them as part of components in the Transformer. Maybe we'll still call them
Transformer, but they'll just have more multi-resistant attention and LPE.
Yeah, I 100% agree with you. So attention as a computational primitive is not going anywhere
anytime soon. It's just a very efficient and a very convenient way to increase the effective state
of your sequence processor. So at some level, if you're working with a model that only has a fixed
state in each of its sequence mixers, you have an assumption. And your assumption is that you only
need so much information in the sequence. So there's always a trade off between
this kind of the ratio of the state dimension, the sequence length, as you push things to the
extreme, either model sizes. So as you make the model bigger, wider, effectively introduce more
states and sequence length. Some of these margins, some of this is speculation, but some of these
margins will disappear, some of the tradeoffs will change, especially 14, 30, some of these very fat
models. But certainly, either whether that's hybridizing or some kind of new block, we're
certainly going to see some more innovation. That's really exciting. My personal, if I had
to make a prediction is that architectural design will get more interesting or more complex,
there's going to be more to do. Yeah, I mean, this year, it's like,
I got some 10 minute clock, that's fine for us. I think with mixture of experts and this being
popular as a state-state model, this is all just really within a few months outside of
opening AI. They've been doing mixture of experts for a lot longer than everyone, but
in terms of open and academic communities, no one's really tried to do RLHF on mixture of
experts. It should just work, but we have to learn all these things. And then the model
grafting is becoming more of a real thing. That's super interesting. I agree that it's just fun
to follow. And hopefully, it gives academics and scientists more ways to influence the
conversation where in industry, it's just about scaling in bigger models where we can maybe do
specific things better, which is what I'm telling open source companies to do with their language
models. Anyways, if they want to have a business model, they need to have an edge. So this all
fits into that kind of narrative pretty well with my regards. Is there anything else you guys
are following in ML? It doesn't have to be about state-space models. What's exciting for you broadly
for next year? Personally, I think data is still the most important thing. We're thinking a lot
about how data influences the model performance, like really teasing that out, either having some
of the synthetic tasks that correlates very well with model performance has been the motivating
examples in a lot of our papers and work has been focusing on synthetic tasks.
Or having maybe smaller data sets that kind of make it easier to understand what's really going
on. Personally, my focus is going to be on data for the next little bit. All the architecture stuff
is fun. Making that hardware efficient is fun, but I think ultimately it's about data. If you
look at the scaling law curve, different monarchitectures with generally have the same slope,
they're just different offset. Seems like the only thing that changes the slope is the data quality.
I love that point. That does seem true. I have the plot from Mamba in this blog post that I'm
writing, which is just a little bit above. Same slope. Yeah, maybe data is really interesting,
sort of miniaturizing architecture design, finding, breaking down what tasks are involved into,
for example, language modeling and trying to package them into something that can be used
to iterate something that's quite exciting. That was one of the main techniques that was used for
this zoology paper that also looks into some of these different behaviors. Personally,
I'm also really excited about new applications, scientific applications, the genomics work,
but even more engineering-focused. We're seeing a shift. Now, language is still the domain that
gets the most clicks, most interest, but I think that will evolve over time. Some of these other
applications offer, even just talking about architecture, they offer completely different
design space. I'm excited to look into it. Yeah, I don't want to talk about language,
but I feel like images and videos are the things that are so obviously going to generate so much
value to me. I don't know the ceiling on language, but when you could access a somewhat local text
and video model at your homework station that's tailored to your preferences, the amount of value
that that creates is totally astronomical. I'm excited. I started playing around with these where
I'd take text of the blog and convert it to Dolly images and convert it to a video with
generated audio all with one Python script. That's really easy to do. I agree with your
more than language. It's fun to have that view. These things actually do work reasonably well
in your experience when you stitch all of them together. It's not that good. The Dolly images
are pretty similar, but I'm doing something really naive where I literally take the text and have a
system prompt. It's like you're generating a series of images for visualizing a blog post,
and it generates various... All the machine learning thumbnails that you see everyone using,
there are variations of that. The fun ones are whether it's about llama or mamba or something,
and then they generate animals in them, which is good. I think I could get much better at it
and have a better segmentation system for the paragraphs and or have chat to BT summarize them
or something like that, but I just know that within a year, it's going to be a text of video API,
and I'm just going to switch it, and it's going to be great. I'm laying the groundwork for infrastructure
to have a multimodal content distribution, really, and I just expect it to become very fun.
I mean, even the text of voice is pretty good. I don't have a studio, but once you have a studio,
it's going to be able to generate perfect audio for whatever you want. Another one of my dreams
that is bad for young students is I want to be able to give a slide deck to a script that returns
the five-minute conference video that no one ever wanted, just based on a GPT-4 reading the slide
deck and voicing yourself. Those are the silly things that I have time to do because I'm not a
professor. Yeah, I think these advances, these systems, they do generate a lot of economic
value, and we're seeing that already. Lots of companies are now switching to using these things,
and I think it's going to change the way we work, as you mentioned, the way we work,
the way we entertain, so it's just a very exciting future.
Yeah, anything else? Well, thanks for coming. I tried to get you guys as much attention as I
can bring, but you never know. It'll go viral these days, so I think this was a great conversation.
People are really hungry for basic intuitions in the area, so this was good.
Yeah, thank you, Nathan. It was a pleasure.
Yeah, absolutely. Thank you for inviting us, and maybe if there are more questions,
is there a way to collect them or to provide readers with listeners with an address or something,
happy to answer anything? Yeah, I'll include contact info in the post in various ways. This
will be out there. You'll get your comments on Substack, YouTube, Twitter. It's a mess. You've
got to pay attention to 10 million streams of information these days, but you'll get
contacted by people. Thankfully, for some reason, people read my stuff, but here we are. So thanks
for listening.

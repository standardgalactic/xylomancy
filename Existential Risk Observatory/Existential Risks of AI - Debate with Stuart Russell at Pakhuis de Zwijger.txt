Welkom, iedereen.
Great to have you all here in a fully packed packhuis de Zwijger.
Mijn naam is Maarten Gehem, I'm the director of the argumentation factory, and I have the
honor of hosting this evening on existential risks.
De late Stefan Hawken once said, success in creating AI could be the biggest event in
the history of our civilization, but it could also be the last unless we learn to avoid
the risks.
And that's precisely the topic of today, and we're going to talk about that with none
other than Professor Stuart Russell.
I'll properly introduce him later, but first I'll hand over the floor to Otto Barton, who
is the director of the existential risk observatory, and who is the instigator of this evening, Otto.
Come over.
After Otto, Russell will give a talk, then we'll have a Q&A, and then afterwards we'll
have a panel.
With five distinguished panelists sitting over there, I'll also introduce you later.
But now, without further ado, Otto.
Thank you all, and thank you very much, Maarten, for the introduction.
I'm super happy that you're all here.
Indeed, my name is Otto Barton, I'm the founder and the director of the existential risk observatory.
We're an organization aimed at reducing existential risk by informing the public debates, and I'm going
to talk a little bit for a few minutes about what existential risk is and what our organization
is doing.
All right, so next slide please.
Thank you.
So existential risks, what are they?
Basically, as humanity, we've had about 300,000 years now already on this earth, and we have
maybe about five billion to go, so an enormous amount before the sun explodes, so the huge
majority of our time is still ahead of us, and an existential risk is something that can
threaten death.
So basically, the definition is a risk that threatens the destruction of humanity's long-term
potential.
It has been defined by Toby Ork from the Future of Humanity Institutes and his colleagues
in this way.
So this could be in a few ways.
Of course, human extinction is a permanent state, so human extinction is one way in which
we cannot have a future left anymore, so this five billion years, there won't be any value
in that.
An unrecoverable collapse or dystopian log-in are two other ways in which we could, which
are contained in existential risk definition.
So on the graph to the right, you see a rough estimate by Toby Ork, this researcher from
the Future of Humanity Institutes, on what causes could be for existential risks.
So there are natural causes, there might be an asteroid strike, there might be a super
volcano, but these are tiny and very well-known, so not the most interesting ones.
To the left of that, you see a nuclear war and climate change, which are already somewhat
bigger, but you can see that these are still fairly small compared to other existential risks.
That climate change has a small chance of leading to human extinction, doesn't mean that it's not
a big problem.
Of course, the chance that climate change will occur is 100% basically, and it is a very
big issue.
However, the chance that it leads to complete human extinction is relatively small, which
is why you see a small bar here.
Nuclear war, perhaps a little bit of a similar story, the chance that it occurs in the next
hundred years is not that tiny, but the chance that it leads to human extinction is fairly
small.
But to the left, you see even bigger chances of human extinction or the other existential
risk categories, and these are, for example, manmade pandemics.
De pandemics bar is eigenlijk voor manmade pandemics, so a natural pandemic is also
very unlikely to lead to human extinction, but a manmade pandemic with the biotechnology
that we have developed right now and that we're still developing and democratizing, the chance
that this could lead to human extinction in the next hundred years is non-negligible.
Tobe Orch and most of the other existential risk researchers think that's unaligned AI,
so artificial intelligence that has human level, or even beyond human level, superhuman
level, but is unaligned, so it has different values than ours.
This could be a relatively large chance of human extinction, and we're going to talk
more about it later, but I'll just leave it here for now.
So what else do we see?
The total existential risk in the next hundred years is about a one in six estimate, and there's
a lot to be said about these estimates, but you can still draw a couple of robust conclusions
I think from them, that a very likely source is new technology, and also that technology
is manmade, so risk could be reduced in principle.
Next slide, please.
Yes.
So solution directions for AI existential risk, and these kind of also carry over for
other technologies, but very broadly you could say if you don't want something to go
very wrong with technology, you can either develop it safely, or you cannot develop it.
So basically for AI, this is built AGI safely, or AI safety, so this is done by people who
try to focus on AI alignment, trying to make AGI align to our values.
And we think as an existential risk observatory, this is an important line of research, and
it should be scaled up.
But on the other hand, it hasn't worked so far.
People are already working on this for perhaps a few decades, and so far the consensus is
that AI alignment, more or less the consensus is that AI alignment hasn't been successful
yet.
So another option could be to not build AGI, and we think there might be some kind of regulation
necessary for that.
So this could be a software regulation, a data regulation, or perhaps a hardware regulation.
And we think these are all options that should be investigated, but we do think that
regulation, whatever is the form it takes, will require widespread awareness and global
cooperation.
So for that, our solution is to inform the societal debate, so as an existential risk
observatory, a small non-profit organization based in Amsterdam, we are focusing on informing
the societal society about existential risk.
So we do that by publishing articles in traditional media, for example in Time Magazine a few weeks
ago, en by organizing events such as this debate.
And we also provide input to policy makers, and I think it's a really nice sign that a motion
was accepted by Dutch parliament a few weeks ago that is calling for more AI safety research
in the Netherlands.
So with that, I'm just going to end this small introduction talk, and we're now going to
watch a documentary, which is already giving you a little bit of a flavor of the next speakers,
Stuart Russell.
And I hope that you enjoyed a few minutes of documentary, and I wish you a great rest
of the evening.
Thank you very much.
Everything we have is a result of our intelligence.
It's not the result of our big scary teeth, or our large claws, or our enormous muscles.
It's because we're actually relatively intelligent.
And among my generation, we're all having what we call holy cow, or holy something else
moments, because we see that the technology is accelerating faster than we expected.
I remember sitting around at the table there with some of the best and the smartest minds
in the world.
En wat really struck me was, maybe the human brain is not able to fully grasp the complexity
of the world that we're confronted with.
As it's currently constructed, the road that AI is following heads off a cliff, and we
need to change the direction that we're going so that we don't take the human race off the
cliff.
This is from the Deep Mind Reinforcement Learning System.
It really wakes up like a newborn baby, and is shown the screen of an Atari video game,
and then has to learn to play the video game.
It knows nothing about objects, about motion, about time.
It only knows that there's an image on the screen, and there's a score.
So if your baby woke up the day it was born, and by later afternoon was playing 40 different
Atari video games at a superhuman level, you would be terrified.
You would say my baby is possessed, send it back.
The Deep Mind System can win at any game.
It can already beat all the original Atari games.
It is superhuman, it plays the games at super speed in less than a minute.
Deep Mind turned to another challenge, and the challenge was the Game of Go, which people
have generally argued has been beyond the power of computers to play with the best human
go players.
First they challenged a European Go Champion, then they challenged a Korean Go Champion,
and they were able to win both times in a kind of striking fashion.
We hadden articles in New York Times years ago talking about how Go would take 100 years
for us to solve.
People say, well, you know, but that's still just a board.
Poker is an art.
Poker involves reading people.
Poker involves lying, bluffing, it's not an exact thing.
That will never be, you know, a computer.
You can't do that.
They took the best poker players in the world, and took seven days for the computer to start
demolishing the humans.
So it's the best poker player in the world, it's the best Go player in the world.
And the pattern here is that AI might take a little while to wrap its tentacles around
a new skill.
But when it does, when it gets it, it is unstoppable.
Deep Mind's AI has administrator level access to Google's servers to optimize energy usage
at the data centers.
Maar dit kan een onintentionale trojan horse zijn.
Deep Mind has to have complete control of the data centers, so with a little software update
that AI could take complete control of the whole Google system, which means they can do
anything.
They can look at all your data, you can do anything.
We are rapidly headed towards digital superintelligence that far exceeds any human, I think it's very obvious.
The problem is we're not going to suddenly hit human level intelligence and say, okay, let's stop
research.
It's going to go beyond human level intelligence into what's called superintelligence, and that's
anything smarter than us.
AI at the superhuman level, if we succeed with that, we'll be by far the most powerful
invention we've ever made, and the last invention we ever have to make.
And if we create AI that's smarter than us, we have to be open to the possibility that
we might actually lose control of them.
Let's say you give it some objective like curing cancer, and then you discover that the way it
chooses to go about that is actually in conflict with a lot of other things you care about.
AI doesn't have to be able to destroy humanity.
If AI has a goal and humanity just happens to be in the way, it will destroy humanity as
a matter of course, without even thinking about it, no hard feelings.
It's just like if we're building a road, and an anthill happens to be in the way, we don't
hate ants, we're just building a road, and so goodbye anthill.
Okay, if you weren't scared already, make sure humanity doesn't run off a cliff, Stuart
Russell said.
Who better to tell us how not to run off a cliff than Professor Russell himself.
And that's precisely what we're going to hear.
Professor Russell is one of the leading experts in AI research and AI safety research.
He's based at the University of California, Berkeley.
He's one of the writers, co-author of the standard textbook in AI research, AI modern approach.
Recently, he wrote a magnificent book called Human Alignment.
I don't know who read the book already, let me see some hands here.
Okay, all right, well, it's well worth the effort.
And he'll probably tell you why in the next 20 minutes.
Professor Russell is beamed to us all over the world, from all across the world, from California
where he's based right now.
So we're going to see him on the screen in a minute or two.
And afterwards, there's ample room for questions and answers.
So we'll have some room here for a discussion with Mr. Russell himself.
And there he is.
Professor Russell, the floor is yours.
Hey there, thank you very much.
So I should just make a slight correction.
I'm not in California, I'm actually at MIT, but I'm on my way home to California later
on this evening.
So I think the little movie that you just saw actually brings up a lot of important
points, so I don't have to repeat them.
But I will give you a short presentation, which in some ways brings it up to date.
So we'll say a little bit about what we're doing to help and about the current situation.
So to get everyone on the same page, what is AI?
It's not a particular technology, it's a task, just like the task of physics is to understand
the universe.
The task of AI is to make intelligent machines.
And then the question is, well, what does that mean?
What does it mean for a machine to be intelligent?
And for most of the history of AI, it's meant the following.
Machines are intelligent to the extent that their actions can be expected to achieve their
objectives.
En this is so pervasive, I'll call it the standard model.
And many forms of AI, problem-solving, planning, reinforcement learning, and so on, all conform
to this standard model, as well as many other disciplines like control theory and operations
research and economics.
You create optimizing machinery and then you specify some objective, you put that into
the machinery, and then it becomes the objective of the machine, and then it finds ways to fulfill
that objective.
It's a very natural way to go about doing things.
Later on, I'll argue that it's completely wrong.
But for now, take that as the standard model of AI.
And since the beginning, we've been looking at what we might call general-purpose AI.
So not just an AI designed to achieve some specific objective, but actually one that's capable
of achieving more or less any objective that we might give it, and learning to do that very
quickly at a level that exceeds human capabilities eventually in every dimension.
So that's the goal, and rather than be accused of always talking about doom, I'll begin by talking
about the upside.
And it's really the upside that explains why the field exists in the first place, and why
people are investing lots of money in it, and why lots of smart people are working on it.
Because the potential upside is really enormous.
For example, if you had general-purpose AI, then you could do, by definition, what humans
already know how to do, which is to deliver, among other things, to deliver a good standard
of living to maybe hundreds of millions, or maybe close to a billion people on Earth,
have what we might call a good standard of living.
We could deliver it actually on much greater scale at much lower cost, because the cost involved
in delivering a standard of living is the expensive time of other human beings.
So if we have general-purpose AI, we could, for example, use it to give everyone on Earth
that same respectable standard of living that we might see in some developed countries.
En if you calculate the sort of economic value of that, it's about a tenfold increase in GDP,
and that converts to what economists call the net present value, so that's sort of what's
the cash equivalent of having that increased income stream, so it comes to about $13.5 quadrillion.
So that's a lower bound, a low ball estimate on the cash value of general-purpose AI as a technology.
We could, of course, have many more things besides that.
I think we could have much better, more individualized, ongoing health care.
We could have very personalized and very, very effective education for every child on Earth.
We could speed up the rate of scientific progress and perhaps many other things.
I used to have advances in politics on that slide, but I took it off for obvious reasons.
So now the question is, well, where are we?
A lot of people seem to be saying that we're already there, that we've already created general-purpose AI.
And I think this is not true.
I think there's something going on, but we are still far away from general-purpose AI.
En wat's going on, of course, is large language models.
The chat GPT, GPT4, BARD, Lambda, Palm, all these models are displaying very intriguing and in some cases very impressive behaviors.
And I think they are probably a piece of the puzzle of general-purpose AI, but they are not by themselves general-purpose AI.
And at the moment, I would say we don't know what shape this puzzle piece is,
en we don't know how to fit it into the puzzle, and we're not really sure what the other pieces are.
I think one of the things we're learning now is that the pieces of this puzzle are probably not the pieces that we thought made up the puzzle maybe 15 or 20 years ago.
So just to illustrate a few reasons why I don't think these systems are the solution, they are not general-purpose AI.
So here's a simple example from chat GPT sent to me by my friend Prasad Tadepalli.
So the first question, which is bigger an elephant or a cat, and it answers, an elephant is bigger than a cat, so far so good.
Which is not bigger than the other, an elephant or a cat, and it says neither an elephant nor a cat is bigger than the other.
So these are two consecutive sentences that he asked it.
En it seems clear from this that in a real sense chat GPT doesn't know facts.
So when you ask a human a question, at least our impression of what happens is that we refer to an internal world model that is self-consistent.
That's composed of facts that we understand about the world.
En dan we ask in a question relative to that internal world model, we find out what the answer is and we express the answer in natural language as the answer to the question.
But that clearly can't be what's going on, at least in this example, because you could not have an internal world model that contradicted itself in which the elephants are both bigger than cats and not bigger than cats.
So in a real sense, I think we could say that there's evidence that these systems do not know things in the way that word is usually used.
I also want to point out, so in the movie you just saw that several years ago we defeated the best human go players.
And in fact, when that happened to the Chinese world champion in 2017, that was called China's Sputnik moment.
That event precipitated a total change in Chinese government policy around AI and the commitment of hundreds of billions of dollars' worth of investment, the commitments to train hundreds of thousands of AI researchers, et cetera, et cetera.
We decided to see how good the Go programs really are.
We played one of our team members, Kellen Pellreen, is a reasonably good amateur Go player.
His rating is about 2300.
On that scale, the human world champion is about 3800.
And the Go programs are far ahead of human beings now.
In 2017, of 2016, they were about the level of the human world champion, so around 3800.
Now they've reached around 5200.
And JBXKATO005 is the name of the current number one Go playing program in the world.
And its rating is 1400 points higher than any human player.
Kellen had been playing against this program and had beaten it 14 times in a row and then decided to give it a nine stone handicap.
So that means that black, the computer, starts with nine stones on the board, as we're showing here, which is an enormous advantage.
This is the kind of handicap that you give to a small child who's learning the game, if you're a Go teacher,
just so that the child feels they have a chance.
And now I'll show you what happens in the game.
So, remember, the computer is black, Kellen, the human, is white.
And it doesn't really matter if you don't understand Go.
Basically, you're trying to surround territory with your pieces and to surround your opponents' pieces and capture them.
And notice what's happening in the bottom right corner of the board.
So, white is making a little group.
It sort of has a kind of a figurati sort of shape.
And then black immediately starts to surround that group in order to prevent it from capturing more territory.
And now white starts to surround the black group so that this larger white circle is forming.
So it's kind of a circular sandwich.
There's a white piece in the middle and there's a white thing around the outside.
And it's sandwiching in that black group.
And black pays no attention and then loses all of those pieces.
So what's going on here seems to be that these superhuman Go programs
actually have not correctly learned what it means to be a group of stones,
what it means to be alive or dead, which are the most basic concepts in Go.
And that allows Kellen, the human, to defeat these programs.
And the same problem arises with all of the leading programs,
which are written by different people using different training regimens
and different network structures and so on and so forth.
They all fail in the same way, which is really remarkable.
And I think it's actually a consequence of the fact that they're trying to train
circuits to represent concepts such as connectedness and surrounding,
which are actually impossible to represent correctly using circuits.
You can only represent sort of patchy, fragmentary, finite approximations
to those concepts, whereas with a general programming language like Python,
it's very easy to represent those concepts correctly.
So this is a fundamental limitation, at least as we understand it,
with deep learning as a way of learning about the world.
Okay, so I think in my view, we still have some way to go towards general purpose AI.
And I've listed some of the things I think are missing here.
Probably the third one, our ability to not just to look ahead,
which the Go programs do seem to be able to do, even if they make mistakes
about the quality of the positions they reach,
they're certainly able to plan ahead 50 of 60,
or even 100 moves into the future.
But humans plan at many levels of abstraction.
We plan over timescales of years,
and also over timescales of milliseconds, and every timescale in between.
When you decide to do a PhD, for example,
that's going to be about a trillion motor control actions,
and yet rather than just 50 motor control actions.
En so we are able to manage in the universe,
it's very complex universe, because of our ability to operate
at these multiple levels of abstraction.
And that's something that's still quite poorly understood in AI.
So I think it's still likely, given the amount of momentum,
investment, of genius that's being put into this field,
that these advances will happen.
It's just very hard to predict when they're going to happen.
En te geven u een example
of how hard it is to predict when these things are going to happen,
we can look back in history to the last time
we invented a civilization ending technology,
which was atomic energy.
And we have known since 1905, in special relativity,
that there was an enormous amount of energy locked up in atoms.
En if you could transform between different types of atoms,
you could release that energy.
But the physics establishment here,
personified by Lord Rutherford,
the leading nuclear physicist, believed that that was impossible.
He was asked at a meeting on September 11th 1933,
do you think that in 25 or 30 years time,
we might be able to find a way to unlock this energy.
And he said, anyone who looks for a source of power
in the transformation of the atoms is talking moonshine.
En the next morning, Leo Zillard,
who was a Hungarian physicist,
who had escaped from Hungary
and was living in London at the time,
he read about this in the newspaper
and he went for a walk
and he invented the neutron-induced nuclear chain reaction,
which is the solution to how you release the energy of the atom.
So he went from being impossible
to being essentially solved in 16 hours.
So when I say it's unpredictable,
it is unpredictable when these advances are going to happen.
I think because there are several that we need,
it's unlikely they'll all happen in one go.
So we may get some early warning.
So talking of early warnings,
this is the title of a paper written by
a dozen very distinguished researchers at Microsoft.
Microsoft.
There are two members of the National Academies here,
other researchers who have been very significant contributors
to the theory of machine learning.
And they played with GPT-4,
the latest system from OpenAI.
They played with it for several months before it was released.
And they'd ran many, many kinds of experiments with it
to get a sense of how good it was.
And their conclusion, as indicated in this title,
is that they believe that GPT-4 shows sparks of artificial general intelligence.
So they at least are arguing that
some real progress towards AGI has occurred with this system.
Okay, so coming back to the question of what if we succeed,
this is Alan Turing, who's the founder of computer science.
En in many ways, the founder of AI as well.
And in 1951, he was asked that question at a lecture.
What if we succeed?
And this is what he said.
It seems parable that once the machine
in thinking method had started,
it would not take long to outstrip our feeble powers.
At some stage, therefore,
we should have to expect the machines to take control.
So that's it.
So he offers no mitigation, no solution,
no apology.
You almost get a sense of resignation about this prediction.
So why is it?
Where is this prediction coming from?
This idea that as you make AI better and better,
things could end up getting worse and worse as a result.
And I think underlying his prediction is,
I'm going to put it in a more positive way rather than a prediction, a question.
How do we retain power over entities more powerful than us forever?
That's the question that I think he's asking himself.
And he's failing to find an answer to it.
And so that's his prediction.
So I've spent the last 10 years also trying to figure out
an answer to this that isn't, we can't.
To do that, I've been trying to understand where things go wrong.
And I think they go wrong because of a phenomenon called misalignment.
And that was described a little bit in the movie.
I think Elon Musk talked about it and I talked about it a little bit.
This idea that systems that are pursuing an objective,
as in the standard model, if that objective is not
the full, complete, correct description of what the human race wants the future to be like,
then you are setting up a mismatch,
a misalignment between what we want the future to be like
and the objective that the machine is pursuing.
And we can see that happening already in social media
where the algorithms that choose what billions of people read and watch every day
are simply designed to maximize a very local objective,
the number of clicks that they produce over the lifetime of each user.
That's called click through,
or it could be engagement,
the amount of time that the user spends engaging with the system.
And you might think, well, okay, if I want to get the user to click,
I have to send things that the user likes.
And so the algorithm should be learning what people want.
That sounds like pretty good.
But we very soon found out that that wasn't the solution that the algorithms found,
that we know that they amplify clickbait.
Clickbait by definition is articles that you think you want,
but it turns out you don't want because the headline is misleading.
En they also create filter bubbles because you stop seeing content
that is outside your comfort zone.
So these phenomena were observed very quickly,
but actually the real solution that the algorithms are finding
is inevitable when you think about the definition of the problem that they're given.
If you want to maximize the long-term number of clicks from a user
and the way you can do that is by choosing content to recommend to them,
then the solution is to choose content that will change
the user consistently over time through perhaps thousands of little nudges.
Change the user, modify people to be more predictable
in the content that they will consume because the more predictable you are,
the higher the clickrate the machine can generate.
So this is what the algorithms learn to do,
and at least anecdotally we think that the consequence of that
is that it's tended to make people more extreme versions of themselves.
So it's created polarization where people who were towards the middle
end up at one extreme or another because at the extremes
their consumption is much more predictable.
And these are very simple algorithms.
They don't know that people exist or have brains
or they don't understand the content of any of these things
that they're sending to people.
So if they were better AI systems, the outcome would be much worse
because they would be much more effective at manipulation.
And this turns out to be a fairly general property of optimization systems
that when you have a misaligned objective, the harder you optimize it,
the worse the outcome is going to be relative to the true objectives.
And this was proved in a paper by one of my students,
Dylan Hadfield Manel, at Neurips in 2020.
So I think we have to then question whether the problem comes
from the standard model of AI itself because that's the model in which
systems are designed to pursue objectives that we plug into them.
So this is the original definition that I wrote for what do we mean by AI,
what do we mean by intelligent machine.
And I think we actually need to get rid of that definition,
replace it with a different one.
We want machines that are beneficial, not just intelligent.
And they are beneficial if their actions can be expected
to achieve our objectives.
So this is specifically talking about us.
We want machines beneficial to us.
The aliens from Alpha Centauri might want machines that are beneficial to them
and they can do their own kind of AI, but we should do this kind of AI.
And this might seem like it's impossible or certainly more difficult,
but it turns out that we can actually formulate this
in a fairly straightforward mathematical way,
and we can produce systems that solve this problem.
En one easy way to think about this is what are we going to get the machines to do.
And here are two core principles.
The first one is that the machines are constitutionally obliged
to be acting in the best interests of humans.
That's what they're for.
If you want to think of that as an objective, that's the objective,
but obviously it's a very general one.
But the second point is crucial,
that the machines are explicitly uncertain
about what those human interests are.
So they know that they don't know what the objective is.
And it turns out that those two principles together
give us what I think could be a solution to the control problem.
And the mathematical version of this is called an assistance game.
So it's a game because there are at least two entities,
a human and a machine involved in this decision problem.
And it's an assistance game
because the machine is designed to be of assistance to the human.
And we can show by examining solutions,
we can actually write down simple cases
and analyze behaviors of the solutions of this game,
that when you solve assistance games,
the machine will be differential to humans.
It will behave cautiously.
So minimally invasive behavior means
that it changes as little as possible of the world
in order to help you,
because there are parts of the world about which
it doesn't understand your preferences.
And it knows that it doesn't understand your preferences.
So it knows not to mess with those parts of the world.
And in the extreme case,
we can show that these kinds of AI systems want to be switched off.
If humans want to switch them off.
Whereas standard model AI systems,
which are pursuing a fixed objective,
will prevent themselves from being switched off
because that would lead to them failing in their objective.
So you get very, very different behaviors
from these kinds of AI systems.
And I believe this is the core of how we could build
a new discipline of safe and beneficial AI.
Okay, so I'm going to make a couple of brief remarks
about large language models before I wrap up,
because that's what you're probably expecting me to talk about.
So first of all, what are they?
Right there, they are big circuits.
And those circuits are trained by billions of trillions
of small random perturbations.
They are trained to imitate human linguistic behavior.
And the training data they have is text and transcribed speech.
Trillions of words, an amount of text,
parably equivalent to everything,
every book that the human race has ever written.
And of course, as we know, they do it very well.
And it's really difficult for a human being to see
this level of semantic and syntactic fluency,
en not think that there's some intelligence behind it.
And I would argue, as we go,
that we may well be overestimating
how much intelligence there really is behind it.
We have no experience with entities
that have read every book the human race has ever written.
That's probably 100,000 times more than any human has ever read.
So of course, it's going to look
more knowledgeable and more capable
of answering a wider variety of questions.
But whether that's real intelligence
and whether it's flexible enough
to move outside of its training data,
effectively, we don't know yet.
But here, the key point is that
that linguistic behavior is generated
by humans who have goals.
That is the generating mechanism for the data.
En if there's one thing we know about machine learning,
right, the typically the best solutions
that are found by machine learning algorithms
are to recreate the generating mechanism
for the data within the model itself.
And so the default hypothesis actually
is that large language models
are creating internal goals
because that's a good way to be a good human imitator.
So it's not that the system is learning
what the goals of the humans are,
it's actually forming internal goals itself
as a way of being a better human imitator.
So I asked this question to the Microsoft,
that group of Microsoft authors,
the first author in particular, Sebastian Bubeck,
do these systems have goals?
And his answer was we have no idea.
So that should worry you, right,
the fact that they are releasing a system
to eventually hundreds of millions or billions of people
that they claim exhibits sparks
of artificial general intelligence
and they have no idea whether or not
this system is pursuing internal goal structures
and they have no idea what those goals might be.
I think that should worry you.
So one question then is,
okay, so let's imagine that it is learning goals.
Is it learning the right goals?
It's learning from humans,
so maybe we're going to be lucky here
and we'll end up producing systems
that are aligned with humans
and that will be great.
Unfortunately, it's not true, right?
And the way to understand the answer to this question
depends on the type of goal that you're going to learn.
So I distinguish here two types of goals.
The first type is what we call an indexical goal,
which means a goal that's specific to the individual who has it.
So the state you're trying to bring about
is specific to the individual.
So if I have the goal of drinking coffee,
then it's satisfied if I'm drinking the coffee
and it's not satisfied if you're drinking the coffee.
If I want to become ruler of the universe
and obviously it's only satisfied if I'm the ruler of the universe
and it's not if you're the ruler of the universe, right?
So if those are some of the goals that the system acquires,
then obviously that's bad, right?
We don't want the machine to be drinking the coffee.
We want it to be making the coffee for us, right?
We don't want the machine to be trying
to become ruler of the universe.
And then you might say, well, there's other kinds of goals
which we might call common goals.
So if I want to paint the wall,
all right, I want the wall to be painted,
but I don't mind if you paint the wall, right?
If you paint the wall, the wall gets painted
and that's fine.
So this is not indexical.
This is a common goal.
And maybe mitigating climate change.
That sounds like something we would all like to have.
So that's good.
And if the system learns to pursue these common goals,
then that maybe is not so bad.
But actually that can be just as bad
because when humans pursue a goal,
we don't pursue it to the exclusion of everything else, right?
We know that we want to mitigate climate change,
but we know that we can't mitigate climate change
by, for example, removing all the oxygen in the atmosphere, right?
Perhaps that would restore some equilibrium to temperatures
and it would certainly get rid of all the humans
who are the cause of the climate change.
But that's something we don't want.
So we'd rather be alive than dead.
And so we look for climate change solutions
that don't also kill us.
Whereas the AI system may be pursuing
some of these common goals,
but in a way that is pursuing to the exclusion of everything else,
which is just as bad, if not worse,
than pursuing the indexical goals.
So then the next question is,
well, does GPT-4 actually pursue its goals?
If it has goals, is it able to pursue them?
And I think we don't know,
because we don't know if it has goals
and we have no idea what its internal mechanism is at all.
Maar als je op de conversatie met Kevin Roos in de New York Times,
en hier zijn er een paar hoofdlijnen,
Creepy Microsoft,
chatbot, urges, techcolumnist,
om zijn vrouw te leveren,
en het doet zo persistentieel over 20 pages.
Waarschijnlijk heeft Kevin Roos het tent
om het subject te veranderen en om te zeggen,
ik wil je over baseball.
Hij zegt, nee, nee, nee,
je moet me marry, blablabla.
Het is heel persistentieel om de goal te proberen.
Alstubel, dat is hoe het eruitziet
op z'n normaal observer,
dat dit een systeem is,
dat is, voor welke reden,
dit goal heeft bezocht
en is het persistentieel over veel pages van interactie.
Ok√©, dat leidt ons naar de open letter,
die was ontwikkeld een paar weken geleden
en heeft een groot deel van de media gehaald
en het bezoek uit de regering ook.
En de open letter vraagt
voor een paus in de
ontwikkeling van systemen meer moeilijk dan GPT-4.
En de probleem is dat voordat we dat soort activiteiten resumeren,
dus het vraagt niet om AI-research te stoppen.
Er is veel onverstaanbaarheid
en onafhankelijkheid rond de open letter.
Het vraagt voor een paus in ontwikkeling
en ontwikkeling van systemen meer moeilijk dan GPT-4
zodat we tijd hebben om de basic
veiligheid criteria dat deze systemen moeten meten
en om dat systemen die criteria meten
voordat ze kunnen worden uitgemaakt.
En dit is volledig consistent
met agreementen dat alle
regeringen van de ontwikkeld Westen economie
al bevindigd zijn.
Dus de OECD AI-principe
zegt dat de AI-systemen should be robust,
veilig en veilig throughout de hele leven
zodat in condities van normaal,
voedselbaar of misgevoeligheid
of andere aardvers condities
functioneren en
niet op een onverstaanbaar veiligheid.
Dus dat is wat de regeringen al aangekomen.
We vragen niet voor iets
speciaal uitlanders hier.
En die prinsen gaan
in de Europese Unie AI-act
die later op dit jaar inacteerd zou zijn.
En interessant, na de open letter uitgekomen,
open AI gesponderd, of in ieder geval,
misschien is het coincidental,
maar een paar dagen later
zei ze een aandacht
dat de volgende stap was.
We belden dat de volledige AI-systemen
onder de regeringen van de veiligheid
moeten zijn.
De regering is nodig om te zorgen
dat zo'n praatjes bevindigd zijn.
Dus misschien is er niet zo'n grote gap
tussen de mensen die de letter bevinden
en de tech-corporaties die de systemen ontwikkelen.
Dus ik heb een paar andere recommenden.
One is that in order to pass these tests
and I would say that at the moment
the large language models cannot pass
any reasonable test for safety,
in order to pass these tests
I think we're going to need to develop AI-systems
that are what are called well founded,
that they're built from
semantically well defined components
that are composed in a rigorous way,
such that we can analyse the properties
of the composite system that we're building.
This is how we do engineering
in every area of our civilisation.
We understand how the systems work
and ideally we develop proofs
that they are safe before they are released.
We also need actually a way of preventing
the deployment of unsafe systems
and regulation is not enough.
It's obviously necessary but not sufficient.
I believe to do that we need a big change
in our digital ecosystem.
The existing model is basically
that everything can run on the computer
unless it's known to be unsafe.
But I think the new model that we need
certainly outside of the research lab
and outside of the classroom,
so in real world data centres for example,
that nothing runs unless it is known to be safe.
And there are technologies such as proofcaring code
that enable this to be implemented
with efficient hardware checking of proofs and so on.
And at the moment I do not see another solution
for the problem of preventing unsafe AI systems
from being used and misused.
So to summarise, I think AI has huge potential
for benefiting our civilisation.
That potential is leading to this apparently
unstoppable momentum.
But if we keep going in the same direction,
that's the driving off a cliff metaphor
from the small movie,
then we end up losing control
because we are building these systems
within the standard model for AI
and that leads to loss of control.
We can do it differently.
There's a huge amount of work to do,
but I think we can do it differently
and build systems that are safe and beneficial.
And then I think there needs to be a general change
in the whole nature of the discipline and the profession
so that AI, because of its power,
needs to be treated more like the high stakes technologies
such as aviation and nuclear power,
and less like what some people call
a battle of special effects wizardry,
which seems to be going on right now.
So with that, I'll say thank you very much.
And I hope we have time for questions.
Thank you so much.
I hope you could hear that,
Professor Russell,
those were 300 people applauding your speech.
We do have room for some questions and answers.
We have a mic somewhere in the audience.
Yeah, so just raise your hand
if you want to ask Professor Russell a question.
Yeah, we see here.
Is er in front?
Hi, thank you for your talk.
Very interesting.
I read your book.
It was also very good.
I can recommend it to everyone.
What would be an early warning sign
of an AGI taking over the world?
So when do we know we're heading off that cliff?
Yeah, I think that's a great question.
And in that sense,
I think it's very different from nuclear technology
in some sense.
We had a warning about nuclear technology in 1945.
And you don't have to explain to a prime minister
or a president why nuclear technology could be dangerous.
But with AI,
I think it could be much more insidious.
And when we think about the way the oil industry
or fossil fuel corporations in general,
in some sense took over the world.
They led us down the path
of a probably irreversible climate change
despite the widespread understanding
that this direction was catastrophic.
And it involved a lot of complex both disinformation campaigns,
regulatory capture.
So literally taking over through corruption
en economiek power governments
en representatives in democracies,
ensuring that people became economically dependent
on fossil fuels
in order to maintain their stranglehold, if you like.
So many, many parts of that plan
that were developed and executed over many decades.
And I think the rest of humanity
was sort of asleep at the wheel.
En didn't realize the extent
to which they were losing control over their future.
And I think it could easily be much more like that.
And it wouldn't necessarily have to be
that the systems form any kind of explicit goal
of taking over the world.
That whatever goals, for example,
we continue with this approach
of training large language models on human data sets.
En having no idea what kinds of internal goals
these systems are forming.
I mean, for all we know, GPT-4 is actually
in favor of more climate change
or maybe it's in favor of preventing climate change.
We don't know, but whichever one of those it turns out to be,
it may be subtly manipulating millions of people
in the way it answers questions related to climate change
or should I buy an electric car?
What do you think about solar panels?
It may be pursuing whatever political agenda it has.
And not something that it autonomously chose to have.
Just this was a result of training on the data sets.
And it can be affecting our entire world
in that simple kind of way.
We, I think, are still a long way, as I said,
from systems that are really general purpose AI,
particularly the ability to form very complex long term plans.
But if we reach that stage
and we haven't solved the control problem,
then I think it's just going to be irreversible.
There may well not be a very clear warning sign
and we may well slide off the cliff very slowly.
Yeah, another question here.
Thank you and thank you for your interesting lecture.
If you look at some concerns for existential risk of AI
10, 20 years ago about AGI, ASI,
one of the concerns was that it might be a very alien intelligence
towards compared to the human intelligence.
Now with large language models,
if that's indeed an important piece of the puzzle,
it may not solve the alignment problem,
but do you think it might alleviate that concern
dat it would be a very alien intelligence?
No, not really.
I think, I mean, in many ways, they are quite alien,
partly because they've read hundreds of thousands
or million times more than humans have read,
partly because of the way they're, I wouldn't say design,
the way they're evolved.
And I think the human mind clearly has lots of internal structure.
We are very aware as we think of some of the things
that are going on inside our mental process.
There are many things we're not aware,
but it's quite possible that the internal structures,
these systems, develop nothing like the ones
that the human mind develops.
And the thing that makes you, that fools you,
is the fact that it's conversing in English.
But I don't know many humans
who can give me a proof of Pythagoras' theorem
in the form of a Shakespeare sonnet in half a second.
I'd like to meet them if you do.
Yeah, one or two more questions,
maybe there in the back of the audience.
We have so many raised hands here.
I'm quite sure we can't answer all the questions,
but one or two more, please.
Yeah, thanks so much for this talk.
When you said the beneficial or the assistance AI,
you described how it differs from the general purpose one.
To me, it seems really clear that this is the one we want.
But I'm wondering, could there ever be,
what are the strongest incentives
not to make these assistance AIs?
So is there anything you can predictably say
the general purpose AIs
will do much better than the assistance AIs?
Or are there any tasks that the assistance AIs cannot solve,
which might mean that some organization
will want to deploy another one,
even if they are aware of the safety risk.
But perhaps there's so much profit at stake
that they will do it anyway.
Well, I don't see any necessary difference in capability.
But there may be a difference
in what the systems are willing to do.
And obviously, I'm recommending that
when we train these assistance game solvers,
we design it such that their objective
is furthering the interests of all of humanity.
Now, you could have a different version
that furthers the interests of me
at the expense of the rest of humanity.
And deploying that type of system
might appear to give you some short term gain.
But it could be in the long run arbitrarily bad
for the rest of humanity and perhaps for you too.
So I think that this is why I'm arguing
that we need not just,
here is a safe technology,
but we also need a way to make sure
that unsafe technologies or unsafe versions
of that technology are not deployable.
We've tried a policing model with malware,
with cyber criminals, cyber warfare,
and it's been a total failure.
So I think we need to change
the way we conceptualize our whole digital infrastructure.
And I've talked to people,
both hardware architects and network architects
and formal methods people.
And I think there's a belief
that this is technologically feasible.
It would make life a little bit more complicated,
but it's technologically feasible to do.
And in fact, interestingly,
Microsoft tried to do something very much like this.
In the early 2000s in their palladium project.
But at that time, the economics was not there.
But given that right now some estimates
of the cost of malware are about $6 trillion a year,
then maybe the time is right to look at this again.
Seems like a daunting but worthwhile task.
Yeah, let me see.
Do we have women in the audience
to have a question? Yes.
Hi, Dr. Russell.
I think my question might be related to what was just asked.
But indeed, you're proposing a new model
where we create beneficial machines rather than intelligence.
But beneficial can mean different things to different people.
So are there going to be human standards
as to what is beneficial to humanity?
Or would it, in your recommendation, be defined, per instance?
Ah, so there is about 8 billion people on the earth.
And there's no problem having 8 billion predictive models
of what each person wants the future to be like.
So there's no sense in which we standardize
what humans should want or put in any particular set of values.
So there's no whose values it is going to produce.
It's going to be everyone's preferences count equally.
But there's a longstanding debate in moral philosophy.
How do you aggregate the preferences of many individuals?
Because, for example, if everyone wants to be a ruler of the universe,
well, they can't all be a ruler of the universe.
So what do you do?
En the utilitarian theory is that basically you add up
the preferences of the individuals and you try to maximize
the sum of those preferences.
Other people have what's called the ontological approach.
They say, no, we have to have certain inalienable rights
that need to be protected, regardless of the potentially
negative impact on other people of respecting those rights.
And I believe that these two approaches can be reconciled and so on.
And there's some material in the book in the last two chapters
about those questions.
There are still some real difficulties inherent in how an AI system
should make decisions on behalf of people.
En this is nothing to do with my particular approach,
the assistance game solvers.
This is just, what do we actually want AI systems to do at all?
So the idea that's, I think, is most difficult to,
the problem that's most difficult to address is that
what people want the future to be like is not something that they
autonomously chose.
We're not born with complicated preferences about what kind of
governmental structure I want to live under and things like that.
Our preferences about the future are acquired during our lifetime
as a result of experience of our culture and the various forces
applied to us by our families and our peers and so on.
And Matja Sen, among others, has pointed out that many of the preferences
that people have are put there by others for their own benefit.
So typically the elite, for example, the patriarchy enforces
a certain kind of view of society that's beneficial to the patriarchy.
And should we take those views, for example, the views of the
very patriarchal societies, that the correct status of women is to be oppressed?
Should we take those views at face value?
Because they are not autonomously chosen.
They are basically indoctrinated by the patriarchy.
So Sen argues that, no, we should not take those preferences at face value.
But that gets you into very dangerous territory.
Well, which preferences are okay to take at face value and which ones are not okay
to take at face value?
And if they're not okay to take them at face value, well, what do you replace them with?
And this is an area where I don't think AI researchers should be answering that question.
But we need answers fairly soon from somewhere because AI systems
are going to be making decisions on behalf of many people.
So whether you like it or not, they are implementing some answer to that
moral problem.
And it might be the wrong one if we don't actually think it through.
That's interesting.
It's AI as a catalyst to some of the most pressing moral concerns of mankind so far.
One final question, maybe?
There completely.
Hi, Professor Russell.
Thank you for your talk.
I just want to ask about, so you believe that large language models wouldn't be able to actually be capable of AGI.
So why would you sign the open letter, basically, so since that it won't be a catastrophic risk, per se,
since it won't be able to become general artificial intelligence.
So do you see any catastrophic risk in companies building larger and larger models?
Or is it just for general safety purposes?
So the question is, is it actually something that we should really be worried about large language models?
So I think large language models in isolation as we currently conceive them
are probably not presenting that kind of catastrophic risk.
I think they present many, many risks.
And the open letter talks about some of those disinformation bias, et cetera, et cetera.
So I think there are already many reasons that these systems would fail any reasonable safety criteria.
But the concern is that we're not just going to make these models bigger.
We are also going to try to figure out how can they be arranged so that they actually develop
a consistent internal model of the world.
How can they be arranged so that they can also do long-term planning?
En, as I say, we don't really know the answers to those questions yet,
but I think that the ideas behind large language models do form a significant piece of the puzzle.
And so the concern is that future generations of these systems,
which will be extended, not just in scale, but also in the additional capabilities
that we might endow them with by maybe a more design-based approach,
that those systems would start to get close to presenting a real threat.
En in some ways, I think this title of that paper, Sparks of Artificial General Intelligence,
is not wrong.
And I think when you think about what does Sparks mean, well, Sparks are a predecessor to a fire.
En I think that's what we want to prevent.
En with that, we come to the end of the first part of this evening.
Professor Russell, I'd like to thank you for elaborating in such a concise way
the dangers of developing intelligent AI and to provide a manual, so to speak,
to steer away from that cliff and to develop safe, beneficial artificial intelligence
that is aligned with our goals.
So join me in a big round of applause for Professor Russell.
Okay, and with that, we're going to leave you, Professor Russell,
en I would like to invite the five panelists to continue the conversation.
Please come to the floor.
And I'll introduce you properly.
Mark Brakel, to the right, he's a director of policy at Future Life Institute,
involved in the AI Act that is currently being prepared.
And is later this year going to be proposed by the European Union, am I right?
It's already been proposed.
Oh, it's already been proposed.
Hopefully it gets voted through.
Voted through.
I should say, yeah.
Then we have Tim Bakker, who is PhD student at the University of Amsterdam.
The title of your thesis?
My thesis?
Yeah.
I don't know yet.
You don't know?
Lithanger.
Chat GPT, yeah.
Yeah, no secrets.
Working on AI research.
Then we have Nandi Robijns, who is working at the Ministry of Interior and Kingdom Relations.
And you're part of a crew of AI and data consultants.
About 70 people strong.
I just heard over dinner.
Nice of you to be here.
Two members of the Dutch parliament.
Queenie Rijkovski, who's a member of the Liberal Party, the VVD,
and has cybersecurity and digitization in your portfolio.
And last but not least, Lamert van Raam, who's a member of parliament for the party for the animals.
Partij van de dieren.
En was focusing on IT and privacy issues.
Great of you guys to come over and stand here and discuss with us the dangers of AI
and how to deal with them.
Let me just start off with the obvious question.
Professor Russell just painted a rather scary picture of humanity that may one day fear of a cliff
because we don't control the risks involved in AI.
Do you share this view?
Do you also think artificial intelligence may sooner of later, if we don't control as well,
steer humanity over cliff?
I work in an organization at the Future of Life Institute that believes this.
So it doesn't come as a surprise, I think, to say that I agree with Stuart Eastman of our advisors also.
And just in response to the last question also about our open letter that we put out.
I think it's now a week and a half ago.
This is the first event where I'm at with actual people since we put the letter out.
I think it's really worth reading that because the letter, the open letter that we presented,
talks about all kinds of risks that our society might struggle with when it comes to AI,
not just the existential risk.
And I think our first contact with AI, as Stuart Russell also highlighted,
was social media with a super simple algorithm.
Our second contact with AI is probably these large neural networks.
And I think we're going to really struggle to control truth,
to control access to what was previously very hard to access information.
So yes, I worry about existential risks, I agree with Stuart.
But I also think beneath that, there's a layer of very, very serious risks
that is also a cause of worry.
Right, we'll touch upon them probably later.
Yeah, no, I definitely agree with Professor Russell about his worries.
And actually also with Mark about what he just said.
I think Stuart Russell was very right about pointing to the fundamental problem
with the current systems, which is that we're training them as optimizers,
instead of as things that do anything that is not that.
Because that is just such a hard thing to aim in a way that we want to aim it.
We just have no idea how to do that.
Dandie?
Yes, I think it is very important to take into account a wide range
of potential risk of AI, especially because AI is such an extremely powerful technology.
And I think what we talk about today is a very important part of this range of risks,
especially because of the scale of the potential negative impact that it can have.
And on top of that, it is very neglected and this neglect is worrying me,
especially because as we see, more and more AI systems are extremely capable in
achieving their programmed goals.
And the main worry about these AI systems becoming dangerous is rooted in the fact
that they pursue these goals regardless of whether or not it is what we intended.
And yeah, so that needs to be addressed.
And on top of that, these models, no one knows what is going on inside these models,
as Jordi was also also sad and no one actually knows how we can define a goal
that takes into account every value that we care about,
which is also what we just talked about.
So yes, I do agree that it needs much more attention.
Queenie?
Yes, I agree even though I am a tech-optimistical person.
So when it comes to technologies like AI,
I can definitely see the risk in the downsides.
So I completely agree with the other speakers.
And also we just heard in a presentation that if AI can also see maybe human
as a danger when it comes to climate or climate changes,
we need to really think about how we're going to program it.
What are the goals?
I think we just heard some examples.
En one of the experiments that they've been doing to you know,
it's a scientific research company in the Netherlands organization.
And they've also done some smaller and some bigger experiments.
And a smaller experiment is a robotic vacuum cleaner.
And they said, okay, your task is to keep this clean room dust free.
And what happened?
The robot started to block the door.
Because every time when a human came in, the room was dirty.
You were the actual problem, yeah.
Yeah, so it's, and this is just like a small example, of course.
But what it got me thinking is not only what assignment
or what goal do you give a system or robot, et cetera.
But also can you grasp upon what the outcomes can be
when you ask something.
And actually when it comes to equality,
I think AI can maybe even help.
Because in my experience, being a woman in politics, working on tech,
mostly a lot of men around me in my context.
I still hear people say things.
I still see some articles written in a way
that they write different when you're a woman than when you're a man.
So actually, I hope, so that's also my goal from a political perspective,
if we can make sure that we provide the right regulation
and control when it comes to AI, maybe we can even help equality
instead of being a danger to it.
Okay, interesting.
We may continue that conversation later this evening.
Nasty little buggers, those vacuum cleaners, right?
I have.
So, you have one?
Yes.
Okay, but you're not locked out yet?
No, not yet.
Okay, good for you.
So, ja, worrying.
I was taking comfort from the example of Rutherford.
And the next day, Zillar invented something that made it possible.
Perhaps it's now the 12th of April, on 13th of April.
There's one Zillar in the room already making a solution
because that is what possible.
And yes, it is worrying.
It is worrying.
I have only one consolation.
Politician will probably have to solve it.
And then it's, I don't know if it's a consolation, to be honest.
But it's the best we have politicians in a democracy.
Nevertheless, we are working together on the concerns we both feel.
And I think that's giving some hope because she's
completely different ideology than my party.
En stil we find the same common ground in our concerns.
And that is, I think, something to look forward.
The other consolation is we don't have to worry
about falling off the cliff because we are already sliding off the cliff.
That's very comforting, ja.
And for you, Otto, the first slide, I have to say it, I'm sorry,
the first slide with all the risks, don't show it to the animals
because they're already in a massive wave.
And don't show it to the global south.
But there are ways probably if we can get this problem solved.
If there are enough zillas in the room, we're counting on you.
We can also solve it politically because the worries are very real.
So thank you.
So you all share, more or less, the alarming story
that Professor Russell just told us.
But at the same time, I don't see us all going to the streets
and protesting like we do with climate change or in the 1980s
we did with the risk of nuclear war.
So what's wrong here?
Why aren't we, if we all share this great risk
or concern for this great risk, why are we not protesting?
What's wrong with us?
Who wants to answer?
Nandi, you actually raised this point yourself.
So solve it.
We can't.
Yeah, so I feel like there are some reasons.
So the question is about...
Well, if the risk is so big for us as a society,
why aren't we talking about this daily in parliament
and protesting on the square and...
Yes, I think there are some reasons
that make it hard for people to realize one,
that this is a real problem.
En to see that this is something
that needs to be addressed right now.
And I think one of the reasons is that these aspects,
these concepts that we talk about
and the terms that we use are still quite vague
and difficult for people to understand.
And sadly, vague problems are much easier
to ignore than concrete ones,
which also makes it harder for policymakers to prioritize,
things that are a bit more uncertain and vague
over things that are more concrete
and where we can see the harm right in front of us right now.
A second reason also is that this is also,
to some people at first glance,
quickly dismissed as science fiction,
not real or something that doesn't need attention right now.
Yeah, which I think is a misconception.
So yeah, I think this is caused by a lack of awareness
en understanding and a lack of urgency
and that we need to address.
Right, bad PR, yeah.
First Lamert, and then it goes to you.
Well, I fully agree with you
that there's a lack of knowledge, et cetera.
But perhaps it's also,
perhaps the protest is already there,
but we just don't recognize it.
For instance, there's, well,
the best example, of course,
is the upheaval there was in the Netherlands
of the toeslagen schandal.
And algorithms played a very big role in that,
and also in the, let's say,
discrimination factor of that,
and that led to the fall of the government.
So there was a big upheaval,
but we didn't perhaps recognize it as such.
So perhaps there is a lot of upheaval,
but we just have to categorize it differently
to understand it.
But I agree fully with you that there's also
a very lack of understanding
and a full lifting, what is that?
Education, thank you.
Thank you, George.
Mark.
Yeah, if I could maybe add two sort of points
of optimism to that.
I think when Otto first asked about
sort of doing this event,
it was going to be in the smallest room
of this building.
Right.
And it sold out,
and now we're in the big room.
So I think that shows that I think society
is moving maybe slower
than the development of the systems.
But still, it's of interest to more people.
And when we sat together with the Future of Life Institute,
with my sort of 16 colleagues four weeks ago,
brainstorming this letter,
we thought, OK, maybe we can have four news outlets,
cover it, mainly in the United States.
Potentially we get one in Europe.
That'd be great.
And a colleague of mine comes from rural Australia,
and her mum had heard it at the hairdressers
on the radio show.
And I think that shows that.
Great source of information,
the hairdressers recommend this.
People are slowly waking up to the risk.
And the Overton window is also shifting.
I think a lot of governments are waking up
to the fact that they need to regulate this
and really quite quickly.
OK, so we're going in the right direction.
Are we all happy with the direction we're going here?
Or are we, yeah?
I mean, I don't want to be overly optimistic.
I mean, the one thing that worries me is companies,
because we have Russell's proposal here to say,
OK, we need to look at AI systems,
and we need to make sure that they are uncertain
about what our objectives are.
Whereas all of the investment,
and the economist in an article last week,
just saying how it escalated since chat GPT,
how many more billions are suddenly available to invest,
are all going to neural nets that do exactly the opposite.
Everyone is clueless as to what these systems do,
including the chief technology officer of open AI
who goes on TV and says that.
I'm interested to hear your comments on this, Tim,
because you actually interned at Facebook AI.
Yes.
Which was a long story and quite eccentric,
what you did there.
I'm going to have to fed myself now.
No, you don't.
He was not working on the metaphors.
But please tell me, how do you view this danger?
Yeah, so it's, I mean, it's interesting,
because I've been worried about these topics
for a very long time,
and I've now been involved with AI
for a bit less than that, but still.
And it's been interesting to see the shift in opinions.
Like, as Nandi said, people at the start really thought,
OK, this is some kind of science fiction.
Why are you worried about this?
If you look at these systems,
they don't able to do anything,
it makes no sense at all.
Stop worrying.
And in the last, say, two years,
but especially the last two months,
this has really changed in the community as well.
There's been like so many researchers
that are now coming out as, oh yeah, actually,
I am kind of worried,
and I actually have been worried for a while,
but I kind of couldn't say,
because it was just such a weird thing,
and I couldn't really,
probably if I said that,
my colleagues would call me crazy.
En now you have people like Geoffrey Hinton,
who is sort of often considered
the godfather of modern AI,
going on national television in an interview
and saying, yeah, it's not inconceivable
that AI will wipe us all out.
And also, I don't know what to do about that.
And so it's become much more of an OK thing to worry about.
And I think that is quite hopeful.
Of course, that doesn't mean
we know how to solve the problem yet.
But at least it's now,
we're allowed as a scientific community
en also as a tech community,
at least consider these problems seriously,
and that's very good.
I'm also interested to hear the comments
of the two parliamentarians here,
because, well, your job, partly,
is to devise laws
and to think about how we could improve
the well-being of us people here in the Netherlands.
So what do you think is the risk
of tech companies devising new AI systems
that may not be aligned with our well-being?
And what can you do about that?
Biggest party first?
Ja, ja.
I think the...
Thanks.
That's a privilege.
I think...
No, of course, like the risks,
I think some of the risks
we already talked about,
because in January, I think,
we had a big debate about AI in the parliament.
And one of the things that we also talked about was,
OK, so, but, and you see it now
with the open letter,
the people who write the code
are worried about what's going to happen with AI.
So that's a bit, you know,
if someone can do something about how software,
how large language models,
how AI is working,
it starts with the person who's making it, right?
So we also discussed,
so can you also maybe start looking maybe
at AI engineers, professions like that
and not only teach them at university
how to write good code, efficient code, et cetera,
but also take into account ethics, human rights, et cetera.
So that where the programming starts,
also ethics and safety is taken into account.
The core of the curriculum, ja.
Exactly.
And one thing which I think is hopeful
is that when you look at social media,
big tech, the internet,
at first everyone start,
oh, it's going to regulate itself, right?
We only see positive things.
A human race will fix this itself.
And now politics are waking up,
oh, wait a minute,
social media, big tech companies,
they are a lot about making money
and not about taking responsibility
to make sure that they have a good contribution
to the country that they make money in.
And now we are trying to repair that,
but it's too late.
You know, there's already a big power,
they already decide a lot,
maybe they have even more power
than a lot of governments have.
So, and what I see with AI,
that especially on the European level,
that European Commission already started
to work on AI regulation two years ago.
So what is hopeful for me is that we already started
also from a political point of view,
because experts are thinking about this,
you know, way longer than that,
but also from a political side,
they are, the thinking has already begun.
The law regulation is already in the making,
not only in Europe, but also worldwide.
They are working with treaties, et cetera.
So for me, that's hopeful in a way that it helps me
in thinking that, okay, AI can still do wrong,
but maybe we are not too late.
Lamert, how do you view this?
It seems to me that it's like a win,
a winner takes all industry.
Like, to a certain extent,
the fossil industry is, wasn't is.
So, and we have a very,
that industry has a very bad track record.
So I would be inclined to give the big companies
the, not the benefit of the doubt,
but the disadvantage of the certainty
that they are still in the race
in a winner takes all situation.
En, so, again, on top of what you're saying,
I think we should be very restrictive policy.
And the European AI directive
is setting some very strict policies there as well.
Because it's not something that will
come from the goodness of the big companies.
I'm afraid.
En, just like Professor Thompson said,
I mean, he's hopeful that AI will solve world inequality.
But the thing, of course,
is we could have solved world inequality a long time ago.
We don't need the computers or AI for that.
So the problem goes far deeper in that.
And that's where I connect with Queenie
in the sense that we need to instill
the ethics in the educational systems to try and do that.
En at the same time,
very strict regulation, I would say.
Yeah, because even if I can add a little bit to that,
so what internet, social media, et cetera,
what they did is they created,
or actually the companies who created it,
the companies were,
are big in a sense that we have never experienced before.
They have more power in a way
we have never experienced before from companies, in my opinion.
And AI can actually triple that.
So when we just saw in the presentation
how much extra quadrillion money that can be made,
the first thing I thought was okay,
but in whose pockets is going to,
and who's going to fill the pockets with the money.
And I don't think that we'll go to fighting inequality
if we don't make sure, from a political perspective,
that technology can make everyone's life better
and not only a happy few.
And I think that's really important
that we are talking about this right now,
that we're discussing this right now,
and taking this when we are making regulation.
And I'd love to hear you talk like that.
Okay, you can switch sides.
Okay, in a minute,
there's some room for questions from the audience
to our distinguished panelists.
But first, I want to pose the million dollar question.
And that's what can we,
or what should we all do in order to tame the beast,
in order to avoid that we're going to run off the cliff.
We have the EU AI Act.
We have ideas of instilling ethics
and other subjects into core curricula of AI engineers.
But there are probably other great ideas
that we should take into account, in a minute.
But first, I want to hear the five panelists.
We'll just make a little round,
and then the floor is yours.
So what should we do to tame the beast?
What should we do tomorrow
to make sure that we don't run off the cliff?
Of course, we have an AI Act.
Of course, we have great ideas
of how to improve the curricula of AI engineers.
But that's probably not the answer
to the million dollar question.
What else should happen?
I mean, there's a lot.
Just before coming here,
we send out seven recommendations to policymakers,
to the signatories of the open letter.
We'll put that out tomorrow.
So go check that out.
But it has things like national regulatory agencies for AI.
It has things like more AI safety research
and public funding in that.
So it's not just the companies doing that.
It really requires, I think,
the world coming together over this.
But given that I've got...
What does that mean,
the world coming together over this?
How would you...
I mean, I think ultimately we need
a sort of international atomic energy agency for AI.
So an international body that has enforcement agency,
even over those jurisdictions
that don't fall under an AI Act
or aren't part of a big power agreement.
Right.
But I am going to take this opportunity
with these two Dutch politicians.
Because I work a lot in Brussels
where we have an AI Act,
but there's also a lot of big tech lobbying.
I mean, there's maybe four or five NGO people.
And then there's several hundred
from Microsoft and Google
and Bing and OpenAI's own team right now.
Seems like an uneven fight.
It is.
I think we need the AI Act tomorrow.
I think Brussels is taking its normal slow course.
And I think one thing the Dutch parliament could do
is to ask for it to be applied provisionally.
As we have chat GPT right now,
we probably also need some rules and safeguards.
Another thing is the act prohibits manipulation of people.
But only if you use your AI system in a subliminal way.
So if it's in a hidden frame.
But if you do it overtly, it's fine.
We think that probably should be changed.
And then maybe my final pitch here
is that for a long time,
more general AI systems such as chat GPT
were exempt from the Act.
We've worked very, very hard
to try and bring that into the Act.
But we also face a lot of Microsoft pushback.
So if there's anything you can do to keep it there,
that would be awesome.
Okay.
Keep Microsoft at bay.
Yeah.
Maybe a quick response here
because those are very sort of concise recommendations.
What do you think?
Are you going to take these up?
And next time you talk to your fellow parliamentarians.
Of course read the recommendations.
We will be stupid not to do it.
But I fully agree with the lobbying power
and the equality of arms is not equal.
You see it in the fossil industry.
You see it in the finance industry.
So my call would be to call to arms for raising funds
to putting more money in the lobbying effort
because I think we're very weak there.
And I think the Microsofts.
I mean, I'm not too sure if a guy like Elon Musk
is saying that AI is a threat
when he is, you know, what's he doing?
So I'm not entirely sure if that's the right thing.
So I agree with you on the lobbying front.
Definitely.
And what about the international agency for AI?
It sounds a bit, I have to think about it to be honest
because it sounds like a drastic,
I don't think we have a red button
or an international police agency
that can say, OK, stop this.
I think that.
But I know who knows.
Maybe it's necessary.
I haven't thought about that.
That one.
I need to think about it.
It's an interesting idea.
Maybe it's the wrong for a politician
not to give an answer on the spot directly.
We're going to have to think.
Sleep over it.
Yeah.
I'll ask my AI to.
Ask Jeff.
Queenie.
Yeah, no.
But yeah, I would like to read,
but you're going to send out the email.
So I'm going to read all the seven recommendation.
I can.
I recognize the lobbying part a lot.
So what I tried from a Dutch perspective,
you know, when you look at the AI act,
they distinguish if an AI is a high risk AI
or a low risk AI.
And I'm not sure if I follow those categories
because I don't think it's about the technology,
but in which context you use them
and with which goal.
And I also try to make some low risk AI
to try to get them in a higher category,
which is really difficult.
So I really recognize the lobbying part.
So maybe it's good also to come together
after tonight and to see if we can align
on on on some topics.
I wanted to.
Oh, yeah.
Maybe one thing that can come close
to what you're saying is de wetenschappelijke raad
voor regeringsbeleid.
So that's a group that advises the parliament
but also the cabinet, the ministers.
And they said you have to work on AI diplomacy.
And I think that comes.
Well, it doesn't have really like overruling power,
but it comes really close in making sure
that you get treaties, make agreements
all over the world on how to use AI.
So I think that's a good one.
And at the same time, well, if you look at
the geopolitical situation right now,
not everyone listens to international treaties.
So, but I think it's a good start.
And let's talk about that tomorrow or after more.
Tim, your two cents.
Right.
So I mean, I don't know a lot about the social
technological aspects of this.
I'm going to maybe focus on the existential risk part
that I know a bit more about.
Because I think Mark already gave a very good summary.
So one thing we can do is hope it goes right.
I don't think that has a lot of chance.
The other thing is we can do,
we can try to solve this alignment problem,
either by, as Professor Russell suggested,
finding new paradigms,
or by trying to solve it in a deep learning setting,
which might be very difficult,
but maybe it's doable.
I don't think, I don't particularly have any hope
in the companies themselves solving this.
And I also feel like if we want to do this,
we need a lot more time.
And so one thing that's,
one way to give us time would be
to have these kinds of international regulations
that make sure,
like the open letter suggested,
systems like GPT-4,
or stronger systems like that,
shouldn't be allowed to be trained for maybe ever,
or until we solve alignment,
or six months,
I don't know how long it will take.
And I think it's very telling
that even the tech industry itself is saying,
look, world help us,
because we don't know how to do this,
and we need more time to solve this.
I think maybe we do actually need
that kind of drastic action,
because they're not going to do it by themselves.
They seem to be open-minded in some ways to that.
I can only agree what has been said already.
And besides that,
something that was also mentioned in the open letter
is to call for a research focus shift
from AI capabilities research,
so making the biggest models even bigger,
and better, and smarter,
to AI safety research,
which is research to ensure
the beneficial outcomes of these advanced AI models.
And so I think the Dutch government
can play a role in that as well
to advocate for more funding towards that.
And I think the Netherlands
is a great place for that as well,
because we have a lot of technical universities
that are highly internationally regarded.
So yeah, besides technical research,
we also need more research
for AI governance,
so we need more robust
and effective governance mechanisms.
And yeah, I think we can also play a role in that.
Okay, the list goes on.
Some questions from the audience.
May I see your hands?
Yeah.
Queenie inspires me to ask this question,
because you seem to refer
to one profession or one type of school
to take an oath.
But I would take it one step further.
Why do we still have professions
and or schools
who might be threatening in any way
without taking an oath?
Shouldn't this oath be obligatory
for many more professions or schools?
Yeah.
Yes.
Yes, it should.
So actually this was not my idea,
but there are two female mathematical
teachers at,
I think it was the Delft Technical University,
and they came to me and they said,
hey, we are trying to adjust the curriculum
into making sure that everyone
who is going to,
who is going to this technical university,
that ethics should be part of all of the studies.
And they asked,
well, can you help us to give a push?
So actually it was,
I didn't like steal the idea,
but I tried to give them a push
from a political part and that helped.
But the goal is not to do just for AI engineers,
but because you cannot,
this is a big responsibility
and you shouldn't just put it with just one person,
but you have to make sure
that everyone who works in the field
understands what are their human rights,
how can we make sure
that we strengthen them
instead of threatening them, et cetera.
So it's actually,
the whole system needs to be conscious of that
because what we all learned
and what we all saw the last couple of years
is that technical and IT is not just technical,
it's about the way we live our lives,
it's about who earns money,
what information do we see, et cetera.
So we need to make sure
that ethical standards are taking into account
when it comes to IT broadly
because it determines the way we live.
Thank you.
Did I answer your question?
Lamert, sorry.
Also, we...
Well, I think it's definitely worth pursuing
and to instill ethical values
in educational systems or professions.
I think that's a good idea.
Although banker oats, you know.
Oh, why not?
Well, we have them.
Yeah, we have them, yeah.
And we saw what happens.
So, but nevertheless,
when you look at the medical profession,
I mean, it took 2000 years to instill the values
that the oath is meaningful.
So it can work,
but we know from a banker perspective,
it's meaningless.
And, but I'm sure we can find a balance.
So I think we should pursue it.
Maybe a start.
Yeah, definitely.
Yeah, thank you for the interesting discussion.
I was wondering,
because there's a lot of progress going on
when it comes to AI interpretability
and making sure that we understand
what kind representation deep learning models are forming.
Do you think there's any role of AI interpretability
in making sure that these systems are safe?
That's maybe a question for Tim, I guess.
Yeah, thanks for the question.
I mean, definitely,
I think a big part of AI alignment research
or ASafety more broadly
should be interpretability
for these deep learning systems
to give us at least some kind of lens
of looking at these systems
and maybe understanding a little bit
of how they work
and what they might potentially do
before they do it.
I think the sort of hope of this field
might be very difficult
in the sense that these models are
so huge
en there are so many parameters
and it's so hard
to even understand
what small parts of it are doing.
Like if you look at the field
of the specific kind of mechanistic interpretability
they call it right now.
We know tiny little things
about tiny little parts of the model
that give us some kind of idea.
Okay, maybe it's doing a little bit like this.
But before we can actually scale that up to...
Yeah, just actually understanding what goes on,
that will take so long
and I'm not sure how feasible it is
to use that as the main angle of attack.
I definitely think it's part of it
but we need a lot of other approaches as well.
Okay, part of the solution.
Yeah, another question there.
Yes, thank you for the diverse perspectives you had.
And I was wondering, Mark,
you mentioned law and policy
as one of the key aspects.
But I think now also with GDPR
actually the enforcement is one of the challenges.
And how would you propose a solution
specifically for the enforcement?
Perhaps is it on a national or European level
or other further levels as well.
Okay, so having a law is one thing
but how do you actually enforce it effectively?
I think the general data protection regulation
is really interesting in that Italy
in sheer desperation blocked chat GPT last week
on the basis of GDPR because there was no AI act.
So I think, yeah, just harping on
about how we need it urgently.
I mean, I think people are learning lessons
from the failures of GDPR.
People realize that the fact that all the big tech companies
have their headquarters in Dublin, in Ireland
and the fact that the Irish data protection authority
is probably the weakest out of all the EU member states
is something that people in Brussels have realized.
So under the AI act potentially
there will be a centralized office.
So that will help deal with enforcement
because it means that the European Commission
can step in when member states do not.
There is also again lobbying against this AI office
and some people are worried about the cost of civil servants
that the commission would potentially need to hire for this.
We've been arguing that this technology is so transformative
that it's probably worth a few hundred civil servants
but it's really a knife edge vote.
I think it's about half the European Parliament
at the moment that would favor such an office
and half that oppose it
en would like to see a GDPR type model.
Maybe we have some partners in crime here.
I'm not sure, but they could help out.
Yeah.
Well, exactly what Italy did was they took the law
that they have on data protection and AVG
and they said let's treat it as a human decision
and then it fell short of the decision process.
On that grounds, you can in fact do enforcement.
It is a bit like using a hammer when you try to do a screw
but it is possible in the area of wanting for another law.
That is very feasible.
Enforcement instruments are in place.
One question here and then we have room for one final.
Thank you.
When it comes to regulation policy,
I think the human species has a track record of solving
always the last crisis and doing too little too late.
When it comes to existential risk, Nick Bostrom also said
that we basically have one shot to get this right
with human track record, even civilization track record
in sight.
Is that something that should concern us?
Yeah, only one shot to get this right
and we rather myopic and focus on short term risk.
Who wants to answer?
Are we optimist here?
I don't think it's a matter of one shot, to be honest.
You don't think it's one shot?
No.
I like the question but I think if I try to think about
the presentation that we have that actually what I liked about
that we can actually have several smaller signs before we get to total extinction.
So that's hopeful for me and yes still we...
This is worrying, that's why we are here today.
We have to make sure that more people understand what AI is,
what are the dangers, how can we make sure that it works for us all,
what are the good things.
So yes, we still have to do a lot of work in society as a whole,
regulation, et cetera.
But again, for me it's hopeful that when I see the difference
when the internet started and when big tech companies became big,
we were too late when it came to regulation of market power, et cetera.
And actually that on European level
that they started thinking about AI act two years ago is helpful.
It also means that it's two years later now.
So maybe it's not complete
because technology has been developing really quick over the last two years.
But I think that part is hopeful that we recognize this problem
before it's too late.
En I think it's my responsibility to make sure that we prevent
that regulation comes too late.
So it's a bit late now, but we have to make sure it's not too late.
One last question here.
Thanks.
I wanted to make a short statement first that
we were talking about protests.
I think that should happen and I want to organize them this year.
Who's coming?
Called Safe Transition.
Safe transition to the machine intelligence era.
But thank you.
My question is to Mark.
And it's about the EU regulations.
En my current understanding is that they are only focusing on deployment
and not on the training so that they are in effect not protecting us
from the existential risk of an AI that secretly breaks out
en goes and does its plan to take over the world in some other server data
sender.
So my question is, is there something in the EU process that is protecting us
from AGI?
Like just like the sparks of AGI paper by Microsoft, the EU AI act,
I think is a spark of hope.
But you're completely right.
I mean, it's not more than that because what they've basically done is
they've taken a product safety regulation like of any type you have in
Europe.
So basically the one that regulates the toy market.
And then they've said, okay, we'll apply that to AI products.
So it only starts to kick in once as a producer of an AI system,
you want to put it on the market.
And so if you're training it, if you're testing it, it's all financically
unregulated.
And we definitely need rules for that.
I think most AI researchers feel that we need to start looking seriously at
companies that have a huge amount of computational power.
There was someone writing an article, the former advisor to the UK prime
minister on technology, who observed that open AI by itself, what is one small
like company in California has 25 more sort of GPUs than the entire United
Kingdom.
So their compute power is about 25 times the size of the UK.
Those are the sources of worry.
And I think we need to start regulating and inspecting and monitoring and
verifying those companies before ideally.
I think they've developed their product and there's no way we can still
change it or make it safer and also to mitigate risks that maybe something
happens before it's deployed.
So I think you're completely right.
We need a bunch of extra regulation.
And we also desperately need the United States because that's where most of
these companies are based.
And it's super, like it's great that we have European regulation.
But without the US, this existential risk is not going to go away.
Right.
So we need to tackle this problem globally.
If I could add something to that, maybe.
Sure.
Also to answer the previous question still, like, yes, I think we should
be worried about that because there are still these scenarios, like the one
you mentioned, where we are not protected and we do kind of only have one
chance.
En voor those kinds of things, it is, I think, very important to target the
bottlenecks of these kinds of systems, which right now is just the model
training.
You need so much more computational power to train these models and to
deploy them.
That's the easiest and the most obvious thing to regulate.
Of course, it might be very hard to regulate in practice, but if you target
that part, then you actually have a better chance at stopping these kinds
of models, I think.
Is that part of your seven recommendations?
It's number two.
It's number two.
Yes.
Okay.
Well, that's one of the commitments I already heard that you're going to
discuss, these seven recommendations.
I also heard the idea of trying to strengthen AI safety research and ethics
in different engineering programs, increasing funding and trying to fight
the big lobbying power of the tech industry.
So quite some commitments that have already been uttered here on this stage,
we've come to the end of this evening, but actually it's only the start
because there are drinks later on to continue the conversation.
I don't know if you're depressed about all the great catastrophic risks
that have come to the fore this evening, or maybe you're very hopeful about
humanity steering away from the cliff.
Does it really matter?
Both are great reasons for a good drink and chat.
So I advise you all to the bar that's in the hall down the hall there.
I want to give a big round of applause to our five panelists.
And some flowers.
We'll probably meet again during the demonstration that you want to organize.
When will it take place?
To be decided.
Okay, so Cliffhanger.
We'll see each other at the Dam Square somewhere next year.
The pleasure was all mine.
Very nice to have you here, and I hope to see you again.

Our computer scientists at the University of Louisville and specialized in behavioral biometrics,
security of cyber worlds and AI safety. And indeed, one of my personal first steps in the
world of existential risk was to read the number of your publications. And I think you may have
inspired many like me to start working on either AI safety or existential risk or related fields.
And I think you also have always interesting and thought provoking comments available on each
topic to social accounts, for example. So I really enjoyed reading those and I can recommend them
to everyone. So we are now honored and it is a great pleasure, I can say, to be able to learn
from you today. And we'll give the stage to you now, Dr. Romer Jampolsky.
Thank you so much. Wonderful introduction. I really appreciate that. I think the plan is that
I'll give about a 10 minute intro to my latest work and then we'll let me set up the slides.
Where is it? Where do you go? Yeah, that's right. We can take a little bit longer as well if you
want to. And then we can move to the fireside chat and then the Q&A with audience. Can you see my
slides? Yes. Excellent. So my name is Dr. Jampolsky. I'm a faculty member at University of Louisville.
I've been doing work on AI safety for about 10 years now. Give or take. And my latest work covers
what I will call impossibility results. Problems we encounter with actually accomplishing what we
think is necessary for us to do not just development work for AI, but also work in terms of control
and safety. If you would like to learn more about my work after this talk, you're definitely welcome
to follow me. You can follow me on Twitter, follow me on Facebook. I always encourage you not to
follow me home. It's very important. So let's start with the big problem right away. If you
heard previous talks at this conference, I had a chance to hear a little bit. You know about
concerns a lot of people have with advanced artificial intelligence, usually called AI safety,
alignment problem, sometimes AI control problem. The question is the problem we're trying to solve
is how can humanity remain safely in control while benefiting from superior form of intelligence? So
we want this very capable agent to do work for us to be helpful, but we want to be in charge. We
want to be able to undo any changes if we don't like them. We want to be final decision makers.
As to what's going to happen. The good news is after 10 years of building up the movement,
there is a lot of people working on this now. There is a lot of research labs, a lot of PhD
programs, but I think the main question of control problem has not been addressed sufficiently. That
is, is the problem actually solvable? So everyone kind of works on it, but I haven't seen much in
terms of proofs or even rigorous argumentation for why we think we can do this. Why is this
problem solvable? Is it solvable? Is it partially solvable? Maybe it's not solvable. Maybe it's a
silly question. It's not even decidable. So that's essentially what I've been looking at
for the last couple years. And I started by formalizing a little bit what I mean by control
problem. So we can talk about different types of control. We have explicit control where you
give orders and the system follows. And this is the kind of standard Gini problem, right? You wish
for things, then you realize that's not what I meant. And you hope you get to undo the damage.
Then there is implicit control. So you have a system with a little more common sense. It doesn't
take your words literally. It tries to kind of parse things in the right way. And there are
intermediate steps between explicit and delegated control. Delegated control is the other extreme
where you have this superintelligence system very friendly. It knows what needs to happen better
than you do. And you might be even very happy with the decisions it makes, but you're not in
charge. The system is completely in control. Aligned control is another intermediate option
where the system kind of understands your values, human values, and tries to do the best it can to
fulfill those values even if your spoken directions are not exactly what maps onto those ideal values.
So it seems, at least from now, that some sort of intermediate value between total control
and total autonomy for the system is necessary for us to be happy with the results. If a system
is completely autonomous, we have no control over it. By definition, we will lose control. If a system
has no autonomy, it's completely deterministic. We decide ahead of time what's going to happen.
It's not very useful. It cannot be generally intelligent. It's a great way to do simple
tasks for a narrow AI, but it's not something we can utilize to solve completely new problems
and new domains, help us with science, help us cure diseases and things like that.
So what I try to do is kind of break down the bigger problem of control
into all the ingredients. We need all the tools we would need to make controllability possible.
So what do you need? You can start thinking for yourself. If you want to be in control,
well, you kind of have to understand the situation. What is the system doing? Can the system explain
what it's doing? Can you comprehend the explanation? That's another very important one.
Can you predict what the system is likely to do? Maybe not just direction in which it is going,
but specifics. What steps will it take? Can you verify that whatever it is you want the system
to do and programmed it to do is actually going to happen? So can you verify the implementation
versus the model? Can you verify the model against your goals and data and so on?
That is also a need for general ability to govern AI research, AI systems, so governability of that.
And of course, we communicate with systems in human language. And we need to make sure that
communication we use is unambiguous. There is no way to misinterpret commands in a potentially
dangerous way. And there is many, many more such limitations. And what I've been doing,
kind of looking at each one and trying to publish those results. So I'll go over some of the
publications I have on that so far. One paper talks about limits to explainability and
incomprehensibility. So essentially, for very complex systems, large neural networks,
it is impossible for the system to provide an exact explanation of what it is doing,
or why, without simplifying it to the point of where it is like you explaining something to a
child. So a lot of important details are removed. And then a very simplified version is given to
you. Because if a full version is given to you, you'll simply not be able to comprehend it. And
if you want to learn more and see the kind of argumentation and some cases proves,
just go to the paper. I provide all the information you need to get access to those papers,
they're also available as preprints on my Google scholar account. Another impossibility result
is unpredictability. And that's our inability to precisely predict all decisions, all intermediate
steps a much smarter agent will take. Of course, if we could predict all the decisions of a smarter
agent, we ourselves would be that smart. And by definition, there wouldn't be much of an
intelligence gap, cognitive gap between us. There is also problems with verifiability.
We know for a fact that with software, with mathematical proofs, we can only get to
certain degree of confidence, but never 100% confidence in the fact that we have no errors
in the proof. So with more resources, we can increase safety and security, but we're never
able to guarantee something with 100% accuracy, which is a problem for a superintelligence system,
which makes potentially billions of decisions every minute, even if one in 100 million creates
a problem, you're guaranteed to have a huge problem within a minute or so. There are also
problems with governance. We have history of trying to govern technology, things like spam
and computer viruses. We have laws against those malevolent software products,
but they don't seem to be doing much. So it's not obvious how much benefit is actually added and
other negative consequences from trying to control research, control what is allowed and not allowed
to be experimented with. And even the orders we give to the system, the communication channel
through English is very ambiguous, and you're almost guaranteed to run into situations where
your orders will be misinterpreted at multiple levels due to how imprecise human languages are.
So what we did with our colleague, we surveyed a lot of those impossibility results, those I looked
at and those other people have looked at. I'm not going to go into details of all of them,
I can just tell you there is a lot of them. And some purely software problems, mathematical problems,
many problems with physics of the universe, impossibility results from physics. But if you
think even a small subset of all those tools is necessary to solve the control problem,
you have to come to the conclusion that control is not possible, at least not 100% guaranteed
safe, secure control will dream about. And I have a very lengthy paper about that,
about 70 pages. I now have a few subsections of it published coming out in conferences,
those should be a lot more readable. But I think I bring up a lot of interesting questions and
additional directions for research and hopefully in the next half hour or 40 minutes, we can
talk about what all this means and how we can move forward from where we are right now.
Thank you very much, Roman, for this introduction already. Yes, we can now have a chat about
indeed where does it leave us and where should we go from now and also a couple of related
questions. And towards the end, after about 15 to 20 minutes, we will also take questions from the
audience. So please, if you have any questions, then please type them into the chat or in the
questions section rather. So first, you're spending quite a lot of time researching AI
existential risk, but I don't think it's already obvious for everyone in the call why AI would
be a danger at all. And I don't think everyone is perhaps 100% convinced that this is actually
an issue or an existential danger, at least that is. Could you please recap how exactly AI could
become an existential risk according to you? Right, so there is a lot of ways to get to that
conclusion. I have a few papers where I simply collect examples of accidents, AI failures throughout
history. And if you look at that progression, it's kind of same exponential chart we see with
development, we get more problems, the problems become more severe, and our ability to anticipate
and predict them seems to be very limited. So basically, the conclusion is something like,
if you have a system or service to do X, eventually it fails to X. Frequently, it does so very
quickly and you go, hmm, okay, my self driving car just killed a bunch of pedestrians, that's a
problem. And then it's a narrow system, the damage is limited, right? So self driving car, okay,
the worst it can do is run through some pedestrians. But if a system becomes general, and it's now
controlling not just a single car, but networks of cars, nuclear response, airline industry,
stock market, the damage is proportioned. I think it's also not the best way to assume that I have
to prove that this service or product is dangerous. Whoever is developing and releasing it has to
prove that it is safe, that it's standard liability law for any product. Show me that this system,
which is smarter than me, smarter than you, smarter than all of us, will never do something
within anticipate, something dangerous, something we don't want it to do. This proof could at any
point be possible or is it within the impossibility realm of your theory? Well, I think I'm arguing
that it's impossible to do so. And not just because it's impossible to prove that, but it's
impossible to get to that level of performance. You can get progressively safer and more secure,
because you can look at specifics of domains, you can limit what the system can do in certain
situations, but you have an infinite space of possible problems. So it's very hard to prove
deterministically that you can sit at all of them. So if AI safety would indeed be impossible, what
does that imply for AI safety research? Does it imply anything for AI safety research? Is that
would that be a waste of time or is it still something that we should pursue? Not at all,
I'm doing it more than ever. So think about mathematics. We know in mathematics there are
many impossibility results. You cannot prove certain things in general. You cannot have
proofs with 100% confidence. It doesn't stop mathematicians from discovering new beautiful
mathematics. You know, in physics, there are limits to, for example, speed, fundamental limit,
you know, speed of light. That doesn't limit us from doing great work on faster cars and faster
rockets. It just tells you that there are limits to what can be done. And so you
should a not waste your resources trying to accomplish that. Like knowing that perpetual
motion machines are not possible is helpful result in physics. Same here. We need to understand
what we can achieve and then concentrate on what is actually solvable instead of trying to create
magical devices which cannot work. Next one. For AI to become an existential risk,
it's commonly thought that it should first outsmart humans. How big do you personally
think the chances that this will happen at all? And which probabilities do your fellow AI scientists
assign to this? That AI will ever become as smart as humans. Exactly. Well, it's a guarantee. I mean,
we have proof by existence, right? If you just copy human system, you got same level. We also
kind of give a lot of credit to humans because we tend to think about Einstein and similar type
humans as typical examples. Every human is quite dumb. So it's not that hard to get to that level.
And how are you on timelines? Of course, you hear quite
values that are quite far apart. Some people I've heard, I think Elon Musk say that there was a
five could be a five year timeline up until AGI that's on the progressive side. On the other side,
there are people that claim it would take hundreds of years. Where are you on this line and how
certain are you? That's a very hard question. No one knows for sure and no one can accurately
predict something like that. But if our current theories about how systems scale are right,
meaning if you just add more compute, add more data, you keep making progress, then it becomes a
question of cost. How much compute are you willing to purchase to get to that level? Do you have
finite resources or what is here? $200 billion now. So maybe at that level, seven years is a
reasonable estimate. With my budget, it might be 2040 depends on what type of resources you have.
If it's also as difficult as let's say Manhattan project was, right? You need resources of a whole
country to get there. It's one question. If we discover, okay, there is a simplifying assumption,
so we need a lot less resources to train this type of engine, a lot less compute. Maybe you can do it
with a laptop in a garage and then it becomes a lot more affordable and takes less time. So
I don't have specific dates. I would be surprised in maybe 2045 if I don't see something at human
level, but that's not important to the argument at all. Whatever it's 2045, 2070, it's still
something we need to worry about today, control and work on safety aspects of it.
I've read somewhere that there are about 70 research projects explicitly aiming for AGI at
this point. I guess the most famous two ones are DeepMind and OpenAI, at least the ones I know best.
Do you know a project that we've never heard of, but actually has a fair chance of beating us too?
Well, there could be many secret projects by secret agencies. I'm sure NSA is very interested in
processing your data more efficiently. So I'd be surprised if they don't have something good
happening. Usually, if you look at the history of what they publicly released and what we later
learned they had, I think they had publicly cryptography like 30 years ahead of everyone. So
maybe already. Interesting thought. Yeah, so a week ago, you posted on the Facebook timeline
that I referred to already, which is quite interesting. I quote from that, help me to
understand where is the number of highly respected people who, one, argued that advanced AI is
dangerous to humanity and to work as fast as they can on developing advanced AI.
And there were, I believe, 116 comments on your post. Have you come any closer to understanding
this personally? No, I had some good explanations and the best one. And I think that's the one
Elon actually gave himself was saying, okay, if we can't control it, I might as well be the one
to get there. And I have the best chance of controlling it. People who don't care about
safety have less of a chance. But it is interesting. So a lot of very big names in
arguing that AI is extremely dangerous are also people who invested the most time and money in
making it as fast as they can. And on a more serious note, some people might say, if AI is so
dangerous, can't we just not build it? You said something about regulation in your talk, but
what would you say to them as a general response? You can't stop progress on something so useful
and so fuzzy in terms of separation between narrow and general AI. If we could make it where, okay,
you only can work on narrow AI, but not allowed to work on general, it would be a good moratorium
to have for a few years. But the dividing line is meaningless. If you're using neural networks,
they're general. If you're using a lot of those latest evolutionary techniques, they are leading
you to general solutions. So it's simply impossible. If you make all computer science illegal, you're
killing your economy, you're shifting research to other countries. So I think I'll add another
impossibility result of un-binability of AI. You cannot ban it. You can maybe delay it at best.
Would be very interesting if you could either include or exclude this from the impossibility
space indeed, but I'm afraid that goes more into Seymour Friedrich's chaos theorem, so to say.
You and I both agree. I think that AI is a significant existential risk,
but some AI researchers don't agree. And do you think there will ever be a scientific consensus
about this? And can we hope to achieve that at some point? And why could that be either so or not?
Well, I have a recent paper about AI risk skepticism, and I do a review of both why would
someone not accept the risks as real and kind of specific arguments they make for it. I think it
ended up with about 100 citations, and I have another 400 unprocessed ones. If anyone's interested,
it could be a nice survey. The most common explanation I see is just bias. If you
get your funding, your prestige, your reputation, everything from developing faster AI, it's very
hard for you to say, I'm working on the most dangerous thing in the world that will kill everyone.
So there seems to be this conflict of interest. In any other domain, we wouldn't allow for this
to happen. If you are working for a tobacco company, you wouldn't be deciding if smoking is
dangerous. If you work for an oil company, we don't really trust your assessment of impact and
climate. But somehow here it's fine. And interestingly, AI is a very large umbrella term for lots of
research sub-domains. Some people do natural language processing, some do vision. Not everyone
does safety and security, but we feel that anyone with a label of AI researcher is qualified to
pass judgment on the state of AI safety in software development. Not everyone is a cybersecurity
expert. If you're working on backend GUI, something else, you're not going to be consulted on how to
do encryption. Why is this somehow different here? I don't fully understand. It would be interesting
indeed also to find out. I'm also kind of puzzled, but perhaps it could have something to do with the
fact that it's not a trial and error risk as one of the few areas. I think mostly, of course,
you're first developing something and then later you regulate it, but it's only at the face of
applications. At this phase, it's much more obvious to have separate controlling agencies,
perhaps. But when you're creating something, of course, that's not that obvious. Maybe one more
question about also impossibility of AI safety, but then from a different angle. I don't know if
you are aware of the work of Anthony Burglas. He has written a book about the evolutionary
arguments applied to AI, and it roughly goes as follows. For super intelligence, being friendly
to people is a necessary baggage. Because of evolution, we should expect only the most efficient
super intelligence to survive, and this is probably not the friendliest one. Would you agree to this
evolutionary argument applied to AI or what are your thoughts about this idea? I haven't read the
book, so I'm trying to get the argument from your question. The argument is that it's more
efficient to be friendly to humans, and so it's a survival advantage. And the other way around,
it's more efficient to be unfriendly to humans, so that would be a survival advantage, because
the friendliness would just be baggage according to him. Oh, in terms of its overhead and development,
being friendly limits your space of possibilities. Yeah, I think there is a lot to be said about
the treacherous turn option. It starts very friendly, gets the resources and help early on,
and once it's capable, it turns on us and removes all restrictions. So sounds like a good book.
I think it is. You should read it. All right, I'll put it on my list of 600 books to read. Excellent.
It was a market that's very poorly, so I'm not surprised that you didn't read it,
but I think the idea is interesting indeed. Are you more on the slow takeover, on the fast
take-off sides, and why would that be? Very fast. Once we get to human level, it goes super
intelligent almost immediately, just adding existing capabilities like infinite memory,
access to all the human knowledge and cyclopedia. So you're already super intelligent if you just take
human plus internet.
And why do you think I think the slow take-off side kind of gained momentum the last years? I
don't know if you agree. And I've heard that this is just because it's more easy to write papers
about this, or that there are more possible stories that you could tell about this. But
do you around you see a shift there? Do you see more people going towards the slow take-off side,
or is that not true? I haven't surveyed. I honestly don't know if you think there is a
shift biased by ability to publish about it, I believe you. I wouldn't make that claim too
straight. Okay, let's say that you're a non-AI expert and you still want to do something about
this existential risk, such as we are kind of. What action do you think would be the best to take?
So you're not an AI researcher, but you want to do something about.
Yes. Is there anything at all or would you just say, okay, just leave it to the experts because
there's not much you can do? I mean, in general, I think it's good if citizens are well informed
about the world and problems. And so the next time you vote, you don't vote for someone you
like visually, but actually picking better policies. It seems like based on age and experience of
people we elected, at least in the US, they're not experts on most advanced technology. I hear
many of them don't use computers, so I'm skeptical that they can keep up with crypto economics and
cryptography and synthetic biology and other interesting questions. So your job as a citizen
is to be informed and make sure your views, your informed views are well represented.
Right, some questions from the audience to not finish off yet, but we're getting to
the end of the conference already. First one, who do you believe is responsible for the safety of
AI, the consumers, governments, or developers, or some other stakeholder?
So that's another interesting question. The ownership of AI itself is very difficult, right?
If it's self-improving, it changes, it's not even obvious who has any control or possession over it.
Obviously, the person to make it and release it has a lot of responsibility, but if it's out there
and now you are upgrading it, supplying it with goals, giving it data, it feels like responsibility
may shift to you. All of it for systems below human level performance. It's a tool, you are in
charge. The moment it's human level or beyond, it's an independent agent. You are as responsible
as you are for your adult children. Another one that's also very interesting, I think,
is it possible to program in a programming language not based on human language to remove
the ambiguity, or would it be possible to have an AI create a language without ambiguity
if the AI could create such a language? Would humans be able to learn it, or would we then
also have to trust the AI to program in it? That's an excellent question. So there is a lot of effort.
First of all, every programming language is an attempt to get away from English and into less
ambiguous languages, but we know languages, programming languages have lots of bugs. There
are logical languages developed to remove ambiguity, and I think Stephen Wolfram has
a nice article about communicating with AI, and he of course uses his Mathematica and models he
creates in language, Wolfram language he developed as a possible solution. I think you can do way
better than human language in terms of ambiguity. I'm skeptical about bug-free communication.
It relies on your existing cognitive models, your understanding, and if you have different
priors, even using well-defined terms may lead to problems. But it's a very interesting area to
do additional research. If you have background in linguistics, I definitely invite you to look
into that. Another interesting one from Simon Friedrich, actually a previous speaker. Do you
think AGI could help overcome their global collective action problems that are at the
roots of basically all the existential risks, including those of AI itself?
So that's another great question. I see AI as a meta problem and meta solution, right? If we get it
right, if I'm wrong and you can make friendly superintelligence well-aligned, everything I said
is just a mistake, then it solves all the other existential problems trivially. Whatever is climate
change, synthetic bio, you have a godlike tool for solving those problems. If I'm right and it's a
terrible risk and it comes before, then it solves it by either killing all of us, we don't have to
worry about it, or it comes before again. So if it takes 100 years for climate change to build up
to boiling point, this happens in 20 years, it kind of dominates the risk. I'm not sure about
applying AI to solve the AI problem. That's a bit of a catch-22. There are those solutions where you
have a supervisory agent, AI, AGI Nyanee, which looks after the world, making sure no one creates
dangerous AIs. I'm very skeptical of such super agents with a lot of government control powers.
I think they may be worse than what the system we're protecting against gives us.
Great answer, I think. One final question from the audience now. So if we cannot stop AI
development and we cannot totally ensure that it is safe, do we just need to accept that it is a
risk or even a big risk? Or is there anything we can do, for example, policy-wise?
So I think we need to do more research. I published those papers about a year ago and I
haven't seen a strong response from a community addressing those. If somebody just published
a paper saying, this is why you're wrong, then you're very happy, but I haven't seen it. So
I have to assume that there's some merit to what I'm saying. The question is then, what do we do
with our lives? How do we update based on that? What do we change? For most people, I don't know
if it makes any difference. Before you were told, okay, you're definitely dying in 60 years. Now you
may be dying in 40. Not a big update. Figure out what to do with your 401k plan. Do you spend it
on something now or wait for it to become worthless later? I don't have any magical
solutions or answers. I am curious in case of successful alignment, what happens to economy,
what happens to work, what happens to people's social interactions. I do have a paper which
kind of assumes that progress in virtual reality will be as good as progress in AI.
And so each one of us gets what I call a personal universe, where you basically get to do whatever
you want and you don't have to negotiate with others. There is no need for consensus. You basically
have independence. So at least the difficult part of value alignment problem is not aligning
with me or you. It's hard. It's hard, but it's not impossible. It's getting 8 billion people
plus all the squirrels and whatnot to agree on something. And this is where the personal
universe solution reduces it to just now we need to control the substrate. If you can get control
of computational substrate and everyone gets the resources to run their personal universe,
okay, we're doing well. We have this virtual agreement.
I think that's not a good point again. I'm wondering also on a more personal level,
like when did you start to think about AI safety yourself and when did you move into this research
field? Was there anything that inspired you to do this and also what were the responses that you
got from fellow scientists in moving in this direction? Well, it was a very gradual process.
So I was doing research and behavioral biometrics. I was profiling poker players to see if, you know,
accounts get hacked and tell the things like that. And at the time, I realized majority of
online players and our bots. So my work started to be about detecting bots, preventing bots from
participating. But the question was, as bots get smarter and better, can we keep up? And not just
in poker, but in general, online bots and automation. I did that type of work for a while. I went to
what was at the time Singularity Institute for artificial intelligence, which was
fighting hard against artificial intelligence. But they had a lot of great ideas, which I
still work on. And I've been back as a fellow and a research advisor for machine
intelligence research Institute. I think they're doing excellent theoretical work.
Yeah, I think perhaps one more, like some AI researchers, I think might be hesitant to talk
about existential risk in the public debate, like you already quickly mentioned, for example, in the
media. Do you agree that they are hesitant to do that? And why do you think that is so?
I think I lost a few words. So with media, what's the concern?
Oh, sorry, I'll just repeat the whole thing. Some AI researchers might be hesitant to talk
about existential risk in the public debates, for example, in the media. Do you agree that this is
so that they are hesitant to do this? And if so, why do you think that is?
Well, it's a personal decision based on your situation. So some people, before we get tenure,
follow a very good advice of be quiet. After you get tenure, never shut up again. But
that's not a bad idea. You'll definitely get someone disappointed in you. And
that doesn't help you in your case. I'm tenured. So I've been saying stupid things for years now.
What do you think about an initiative such as the existential risk of territory? Is it
useful to communicate this to more people in general or to a certain subset of people?
Or do you think it's basically something that should be solved among researchers?
Well, if you think about developing a GI working on superintelligence, you're really
running an experiment on all the humans, right? You've got eight billion subjects,
none of them consented to that work. The least you can do is tell them about it.
I think that's actually great now to end this talk. If there's no more questions from the audience,
and I think we've covered those. Yeah, and then I think we'll leave it here.
It was super nice talking to you and super nice to listen to your short presentation.
And I hope that you will also enjoy the rest of the conference, maybe tomorrow,
and that we'll definitely be in touch and to cooperate more on this
quite hairy problem, but still very interesting one to think about.
Absolutely, and hopefully we'll meet in person one day. Likewise.

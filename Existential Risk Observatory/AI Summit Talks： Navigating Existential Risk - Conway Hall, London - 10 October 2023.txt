you
you
you
you
you
Welcome everybody. I encourage you to take a seat. We will be starting almost on time because we have a very rich
agenda on a very big topic. We are talking about navigating existential risk. Navigating what people
have described as a very difficult, tortuous landscape of risks that are made worse by
oncoming new frontier AI. That's not just the AI that we have today, but AI that we can fairly easily
imagine as coming within the next year or two. Next generation AI that's more powerful, more skillful, more
knowledgeable, potentially more manipulative, potentially more deceitful, potentially more slippery,
definitely more powerful than today's AI. That AI might generate existential risks in its own right.
That AI is likely also to complicate existing existential risks, making some of the risks we already know about
more tricky to handle, more wicked. And we might also talk about the way in which next generation AI might be
the solution to some of the existential risks and dilemmas facing society. If we can apply AI wisely, then
perhaps we can find the narrow path through this difficult landscape. So welcome navigators in the
hall, welcome to navigators watching the live stream, welcome to people and AIs watching the recording of this discussion.
Let's get stuck in. We have lots of very capable, knowledgeable speakers who will approach this from a
diversity of points of view. Indeed, I think one of the hazards in this whole topic is that some people want to be a
little bit one dimensional. They want to say, this is how we'll solve the problem. It's quite straightforward. In my view,
there are no straightforward solutions here, but you can make up your own minds as you listen to what all the speakers and
panelists have to say. And yes, in the audience, you'll have a chance later on to raise your hand and get involved in the
conversation too. The first person you're going to hear from is unfortunately not able to be with us tonight, but he has
recorded a short video. He is Sir Robert Buckland, MP, former Lord Chancellor of the United Kingdom, which means he was
responsible for the entire justice system here, former Secretary of State for Wales. He is still an MP, and he has a
side hustle as a senior fellow at the Harvard Kennedy School, where he is writing papers on exactly the topics we're
going to be discussing tonight, namely, how does AI change key aspects of society, potentially making it better, potentially
making it much worse if we're unwise. So let's watch Sir Robert Buckland who will appear by magic on the big screen.
Well, I'm very pleased to be able to join you all be it virtually for the Conjecture ARO Summit on AI and the challenges and
opportunities that it presents us. And I think my pleasure at being with you is based upon not just my own experience in
government, but also my deep interest in the subject since my departure from government last year. Now, when I was in
government, I had responsibility for, for many years, the legal advice that was given to departments, and indeed to the
government in general, when I was in the Law Offices Department as the Solicitor General. And then a responsibility for running
the Ministry of Justice's Law Chancellor and Secretary of State for over two years before a brief return as well secretary last year.
That seven years or so experience within government as a minister gave me, I think, a very deep insight into the pluses and the
minuses of the way the government works, the efficiencies and indeed the inefficiencies about process. And I think clearly, as in other
walks of life, artificial intelligence, machine learning will bring huge advantages to government processes to improve efficiency,
to speed up a lot of the particular ways in which government works, which will be, I think, to the benefit of citizens,
whether it's citizens waiting for passport applications, visa applications or other government processes, benefits, for example.
However, I think that we kid ourselves if we don't accept the fact that alongside the benefits come potential pitfalls.
And the first and most obvious one, I think, for me, is the scrutability of process. In other words, the way in which we understand how
decisions are made. And that's very important, because understanding how decisions are made is part of democratic accountability in societies like ours.
And where individuals or organizations wish to challenge decisions made by government, perhaps through judicial review applications, then the
explicability of those decisions, which is accompanied by a duty of candor, by the government in order to disclose everything about those decisions,
is part of that accountability. And of course, it's sometimes very difficult to explain how the machine has come to decisions.
And more fundamental than that, we have to accept that if the data sets that are used in order to populate the processes are not as
full of integrity as they should be, and are not the product of genuinely objective and carefully calibrated processes, then we are in danger of
importing historic biases into the system, whether it's biases against neurodiverse people making job applications, or indeed biases against people of colour in the criminal justice system,
simply because the data sets have imported those historical anomalies, those historical imbalances. Now, all those questions have really got me thinking very deeply about the impact of machine
learning on the ethics of justice itself. And as a result of my thinking, I was delighted last year to be accepted as a senior fellow at the Mossefa-Romani
Centre for Business and Government at Harvard Kennedy School, and I am working currently on a number of papers relating to the impact of AI and machine learning on the
administration of justice and the law itself. It really developed from my own experience as law chancellor, from digitalisation, I should say, of the courts, when during the Covid pandemic
we had to move many, many thousands of hearings online for the first time. You know, I think we jumped from a couple of hundred phone or online hearings to 20,000 a week in a very short
compass, and the status quo will never be the same again. In fact, it has moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now, I think that's a good thing.
But I also think that accompanying this question about increased efficiency is the use of artificial intelligence. Now, in some jurisdictions, such as China, we are seeing its increased use, not just to do legal research
and to prepare cases, but to actually decide themselves. In other words, the AI judge. Now, that's all well and good. But do we actually know what populates the data sets that then forms the basis of the decisions made?
And I think it's that intentional unintentional bias or indeed worse than that potential intentional bias, whether that's influenced by a government or indeed a corporate that might be able through their financial means to influence a procedure
or indeed the way in which we deal with cases, knowing as we might do more information about the way in which judges make the decisions. All these questions, I think, need to be asked now before we end up in a position where we've slept, walked into a completely different form of justice
than the one that we know. Now, underpinning all of this, I think, is the need to ask a fundamental question about judgment itself. And that's what I've been doing in my first paper.
The essence of human judgment is something that will be based not just upon an understanding of the law, but on our experiences as human beings. And you can go right back, as I have done, to the judgment of Solomon and his emotional response to the woman who clearly was the true mother of the child that he proposed to be cut in half.
Now, you know, that's an example, I think, of the human element of judgment, which has to be an essential foundation of decision making, particularly when it comes to the assessment of credibility of a witness, a human witness giving evidence upon which the case stands or falls.
And of course, for judge, that applies for juries as well in criminal trials, particularly here in the UK. Now, you know, all these questions, I think, need to be asked. And then we need to work out what it is that we want to retain out of all of this.
Now, I don't think we should make any cozy assumptions that because at the moment some large learning systems are having hallucinations. I don't think we should be assuming that just because of that therefore AI will never work in a way that can achieve a greater degree of certainty.
I think the inevitable arc of development will result in better and better and more capable machines. That's inevitable.
What we must be asking at the same time as capability is ensuring there is greater security and safety when it comes to the use of AI. And that really underpins, I think, the work that I'm doing in the field of justice.
What does this all lead to then? Well, we have the AI safety summit in the UK next month. I very much hope that that summit will first of all involve those critical players in terms of international organisations and key countries as well that will come together to commit to creating, I think, a defined framework within which we should be using AI safely.
And that framework, I think, will have to take several forms. I think in the field of justice we could do with an international framework of principles which will ensure transparency and which can reassure people that in cases of the liberty of the individual, criminal cases, cases where perhaps the welfare of a child and the ultimate destination of a child is in issue, then the human element will be the key determinant.
In any decisions that are made and that the use of machines will be transparent and made known to all the parties throughout the proceedings.
And then other walks of life, I think the AI safety summit has to then look as well at whether frameworks can be created and what form they should take. I think it's tempting to try and be prescriptive. I think that would be a mistake, not just for the obvious reason that AI is developing and therefore anything that we write in 2023 will soon be out of date.
But the very fact that AI itself does not mean an alloyed harm. In fact, it means a lot of benefit and also some neutral effects as well. And where you have that approach, then a principle based system seems to me to be more sensible than overly prescriptive and detailed rules as you would have, for example, to prevent a crime such as fraud.
So just some primary thoughts there as to the impact of machine learning. I don't pretend to be a technical expert. I'm not. But my years in justice, my years as a lawyer, a judge and as a senior cabinet minister, I think obliged me to do some of the thinking now to help ensure that countries like Britain are in the forefront of the sensible and proportionate regulation of the use of machines.
And I think that's what we've been learning under the types of artificial intelligence. If we don't do it now, then I think we'll be missing an unhistoric opportunity. I wish you all well, and I look forward to meeting some of you in future and discussing these issues as they develop.
Thank you very much.
Thank you, Sir Robert, who may be watching the recording of this. Don't be prescriptive, he said. Let's sort out some sensible proportionate regulation. Is that credible? Is that feasible? You'll be hearing from other panelists who may be commenting on that shortly.
I've also said there are risks such as the inscrutability of AI. We don't understand often how they reach its decisions. We don't understand the biases that might be there, that might have been planted. We might lose charge. We might become so used to AI taking decisions that humans end up in a very sad place.
But how bad could things get? That's what we're going to hear from our next speaker. So I'm going to ask Conor Liehe to come up to the stage while I briefly introduce him.
Conor is the CEO of Conjecture. If you haven't heard about Conjecture, I think you need to do a bit more reading. Perhaps Conor will say a little bit about it. They are AI alignment solutions company, international, but with strong representation here in the UK.
So welcome Conor, the floor is yours.
Thank you so much. It's so great to see you all today. I'm so happy to be able to talk to you here in person. And man, do we live in interesting times, to put it lightly.
The world has changed so much. Just in the last few years, few months even, so much has happened in the world of AI and beyond. Just a year couple of years ago, there wasn't such a thing as chat GPT, or even GPT-3, or 4, or 2, or any of those.
It was a different world not too long ago when technologists such as myself, weird little hobbyists, worried about the problem of AGI and how it will affect the world.
And back then, it still seems so far away. It seemed like we still had time. But now, we find ourselves in a world of unrestricted, uncontrolled scaling, a race towards the finish, towards the end, to scale our AI systems ever more powerful, more general, more autonomous, more intelligent.
And the reason I care about this is very simple. If we build systems that are smarter than humans, that are more capable at manipulation, deception, politics, making money, scientific research, and everything else, and we do not control such systems, then the future will belong to them, not to us.
And this is not the future I want. I want a future in which humanity gets to decide its destiny, where we get to decide the future for ourselves, for our children, for our children's children, that we like.
The future, where our children can live long, happy lives, surrounded by beauty, art, great technology, instead of being replaced by so is automata.
And let me be clear, that this is the default outcome of building an uncontrolled AGI system, the full replacement of mankind.
And what we're seeing is that AI is on an exponential. There's a race. All the top organizations, which is OpenAI, DeepMind, Anthropic, among others, are racing ahead as fast as the VC dollars rescale up their work.
And this has given us an exponential. AI is on an exponential curve, both on hardware and on software. It's improving at incredible rates.
And when you're dealing with an exponential, there are precisely two times you can react to it, too early or too late.
There is no such thing as reacting at just the right moment on an exponential, where you find just the perfect middle point, just in the nick of time, when everyone agrees that the problem is here and everything has perfect consensus.
If you do this, you are too late. It will be too late.
And the same thing applies to AGI. If we wait until we see the kinds of dangerous general purpose systems that I am worried about, then it will already be too late.
By the moment such systems exist, the story of mankind is over.
And so if we want to act, we must act well, well before such things actually come into existence.
And unfortunately, we do not have much time. How the world has changed.
As frightening and as terrible the race may be, there's also good changes.
A few years ago, I could have barely imagined seeing governments, politicians, and the general public waking up to these weird nerd issues that I cared about so much with my friends online.
But now we're looking forward to the first international AI summit convened by the UK and the famous Ditt Bletchley Park.
And this is great news. The European Commission has recently officially acknowledged the existential risks from AGI along with the risks from nuclear weapons and pandemics.
This is great progress. This is fantastic. It is good to see our governments and our societies waking up and addressing these issues or at least beginning to acknowledge them.
And we must use this opportunity. We have an opportunity right now and we must prevent it from being wasted.
Because there's also bad news. But we're having this great opportunity to start building the regulation and the coordination necessary for a good future.
The very people who are creating these risks, the very people that are heads of these labs, these organizations, building these technologies, are the very people who are being called upon by our governments to help regulate the very problem that they themselves are creating.
And let me be very explicit about this. The problem that we face is not AGI. AGI doesn't exist yet.
The problem we face is not a natural problem either. It is not an external force acting upon us from nature. It comes from people, from individual people.
Businessmen, politicians, technologists, athletes, large organizations who are racing, who are scaling, who are building these technologies and who are creating these risks for their own benefit.
But they have offered us, these very people who are causing this problem, have offered us a solution. Fantastic.
And they are pushing it as hard as they can towards the UK government and the upcoming summit.
So what is the solution? The solution to the problem of scaling. These labs, these accelerations labs such as Anthropic and Arc have been pushing for. What is the solution?
Well, the solution to the scaling problem is called responsible scaling. Now, what is responsible scaling? You might ask. You see, it's like normal scaling except you put the word responsible in front of it.
And that makes it good. So, of course, I'm joking somewhat, but there's a lot of truth in humor. Responsible scaling is basically the policy.
And you can read this on both Arc or philanthropic's website. It's the policy proposal that we should continue to scale uninhibited until at some future time when tests and evaluations that do not yet exist and we do not know how to build, but the labs promise us they will build,
detect some level of dangerous capabilities that we do not yet know. And then once it gets that point, then they will stop, maybe.
Except there is a clause in the Anthropic version of the RSP paper in which they say that if a different organization was scaling even super unsafely, then they can break this commitment and keep scaling anyways.
So, this could be sensible if they committed to, you know, a sensible bound, a conservative point on which to stop. But unfortunately, the responsible scaling policy RSP fails to actually commit to any objective measure whatsoever.
Oops. So, effectively, the current policy is to just keep scaling until they feel like stopping. This is the policy that is being suggested to our politicians and to the wider world as the responsible option for policymakers.
It is trying to, is very clear that it is trying to recast this techno-libertarian extremist position as sensible, moderate, responsible even.
Now, in my humble opinion, the reasonable moderate position to when dealing with a threat that is threatening the lives of billions of people is to simply not do that.
But instead, there is trying to pass off this as the sensible middle ground position.
The truth of RSP is that it comes from the same people who are causing these risks to exist.
These people, the heads of these labs, many of the scientists and the policy people and the other people working on this, have known about existential risks for decades.
And they fully admit this. This is not like they haven't heard about this. It's not even that they don't believe it. You can talk to them. They're on the record talking about how they believe that there is a significant chance that AGI could cause extinction of the entire population.
In a recent podcast, Dario Amade, CEO of Anthropic, one of these labs, himself, said that he thinks it's a probably 25% chance that it could kill literally everybody.
And they're doing it anyway. Despite this, they keep doing it. Why?
Well, if you were talking to these people, what they might tell you is that, sure, you know, I know it's dangerous. I am very careful.
But these other guys, well, they're even less careful than me. So I need to be number one. So I actually have to race faster than everyone else.
And they all think this about each other. They call this incremental, but they never pause. They always race as fast as they possibly can.
Do as I say, not as I do. There is a technical term for this. It's called hypocrisy.
And RSP is no different. They are simply trying to twist words in an Oralian way to be allowed to keep doing the thing that they want to do anyways.
Which they themselves say could risk everybody.
I mean, has responsible in the name. Must be good.
When people like Sam Altman talk about iterative deployment, about how we must iteratively release AI systems into the wild so societies can adapt to them, be inoculated by them.
It sounds so nice. It sounds almost responsible.
But if you're really trying to inoculate someone, you should let the host actually adapt before you jam in the next new pathogen into their weakened immune system as fast as you possibly can.
But this is exactly what laboratories such as OpenAI, DeepMind, Anthropic, and Tier 2 labs such as Meta are doing with all the force they can muster.
To develop more and more new systems as fast as possible, release them as fast as possible, wide as spread possible.
Now, if OpenAI had developed a GPT-3 and then completely stopped further scaling, focused all of their efforts on understanding GPT-3, making it safe, making it controllable, working with governments and civil society to adapt the new problems posed by the system for years or even decades,
and then they build GPT-4? Yeah, you know what? Fair enough. I think that could work. That would be responsible. But this is not what we're seeing.
All of these people at all of these institutions are running a deadly experiment that they themselves think might cause extinction.
It is gain of function research on AI, just like viruses, developed and released to the public as fast and as aggressively as possible.
They're developing more and more dangerous and more and more powerful viruses as quickly as possible and forcing it into everyone's immune system until they break.
There is no responsible gain of function research for extinction level threats. There is no such thing.
We have no control over such systems and there is no responsible way to continue like this. And anyone who tells you otherwise is lying. A lot has changed.
The summit can lead to many boring outcomes, just exchanges of diplomatic platitudes as is often the outcome of such international events.
They have some good outcomes and can have some very, very bad outcomes.
Success in the summit is progress towards stopping the development of extinction level AGI before we know how to control it.
Most other outcomes are neutral and bad outcomes.
They look like policymakers blindly and sheepishly swallowing the propaganda of the corporations to allow them to continue their unconsciously dangerous gamble for their own personal gain and glory at the expense of the entire planet.
We owe it to ourselves and our children to build a good future, not gamble it all on a few people's utopian fever dreams.
Governments and the public have a chance to regain control over the future.
And this is very hopeful. I wasn't sure we were going to get it.
But the summit speaks to this, that people can act, that governments can act, that civil society can act, that it is not yet too late.
There is simply no way around it. We need to stop the uncontrolled scaling, the uncontrolled race if we want a good future.
And we are lucky because we can do this. We can cap the maximum amount of computing power going into these AI systems.
We can have government intervene and prevent the creation of the next more dangerous, more general, more intelligent strain of AI until we are ready to handle it.
And don't let anything distract you from this.
There is no good future in which we continue on this path and we can change this path.
We need to come together to solve these incredibly complex problems that we are facing and not let ourselves be led astrayed by corporate propaganda.
And I hope that the governments and a civil society of the world do what needs to be done. Thank you.
Thank you, Connor. We'll take questions from the floor in a moment. I'll just start off with the question I think maybe on many people's minds.
Why would a super intelligent AI actually want to kill humans? I have a super intelligent calculator which is no desire to kill me.
I have a super intelligent chess playing computer that is no desire to kill me. Why don't we just build as responsible scaling an AI that has no desires of its own?
Because we don't know how to do that. Why did Homo sapiens eradicate Homer Neanderthalis and Homo erectus and all the other species that we share the planet with?
You should think AGI not of as a calculator but as a new species on our planet.
There will be a moment where humanity is no longer the only or even the most intelligent species on this planet and we will be outcompeted.
I don't think it will come necessarily from malice. I think it will be efficiency.
We will build systems that make money that are effective at solving tasks, at solving problems, at gaining power.
These are what these systems are being designed to do. We are not designing systems with human morals and ethics and emotions.
They're AI. They don't have emotions. We don't even know how to do that. We don't even know how emotions work.
We have no idea how you could get an AI to have emotions like a human does.
What we're building is extremely competent, completely sociopathic, emotionless optimizing machines that are extremely good at solving problems, extremely good at gaining power, that do not care about human values or emotions,
never sleep, never tire, never get distracted, can work a thousand times faster than humans.
People will use these for many reasons and eventually I think humanity will just no longer be in control.
Questions from the floor? There's a lady in the third row down here. Just wait for the mic, sorry, so that the audience online can hear you.
Thank you, Susan Finnell from Finnell Consult. To stop the arms race, certainly at a geographical level.
In nuclear, the states and Europe can tell which countries are building nuclear weapons and what they've got and they can do tests.
If computing power is a thing that needs to be capped to slow this down enough, is there a way to monitor what other countries or people in a clandestine way
are doing and how does that work? This is a fantastic question and the extremely good news is yes, at least currently, this will change in the near future,
but the current state to build frontier models requires incredibly complex machines, massive supercomputers that take megawatts of energy.
This is on the order you'd have a nuclear centrifuge facility. These are massive, huge machines that are only built by basically three or four companies in the world.
There are very, very few companies and there is extreme bottlenecks on the supply chain. You need very, very specialized infrastructure, very specialized computer chips,
very specialized hardware to be able to build these machines and these are produced exclusively by countries basically in the west and Taiwan.
There is many ways where the US or other intelligence services can and already are intervening on these supply chains and it would be very easy to monitor where these things are going,
who is buying them, where is energy being drawn on large scales. So it is not easy and the problem is that AI is on exponential both with hardware and with software.
Eventually, it will be possible to make essentially dangerous AGI on your home laptop probably, maybe not, but it seems plausible.
If we get to that world, we're in big trouble. So this is part also why we have to buy time. We have, at some point, there will be a cutoff where we'll have algorithms that are so good
that either we have to, you know, stop everyone from having a PlayStation at home, which doesn't seem that plausible, or at that point we have to have very good global coordination and regulation.
Thanks. Just past the mic behind you, there's a person in the room behind.
Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know? Are you participating? And if not, do you know anybody else with similar views to you who is participating?
I can't comment too much since it's closed doors. It's a very private event, unfortunately, so I don't think I have my liberty to talk about exactly what I know.
I think the guest list is not public. I don't know most of the people who are coming. I know the obvious ones. All the CEOs of all the top labs, of course, are attending.
It's not a secret. I don't know who, if anyone, of my reference class is attending.
Just past the mic next to you, Robert.
Thank you.
Perhaps I can answer that question. Anybody that has read The Guardian today, there is an interview with Clifford and for the very first time, not for the second time, it has been clarified that there will be only about 100 people participating on the first day.
Anybody is invited, including China, on the second day, apparently there will be only the coalition of the willing.
So those who subscribe to the Frontier Model Forum, they will sit on the second day. That's the current claim.
My main impression from that article is, generally, it's very positive, and I would say I've been surprised, as you would be surprised, that the UK government is really doing what it can to get the mission to what the title of the conference says, the AI safety summit.
It's not about regulation. It's about controlling AI, and they're trying to do their best.
The problem is, as outlined in that interview, is that we seem to be alone. We have the states a little bit, but the rest wants to go their own way and do it on their own territory, which is, I think, on the cheer.
I agree. Sooner or later, international coordination around these issues will be necessary. It's as simple as that.
If you want humanity to have a long, good future, we need to be able, as a global civilization, to handle powerful technologies like this.
Take a question right from the back.
I want to ask, in terms of legislation, what kind do you think is most effective? I've heard, for example, that liability law takes too long to actually have an effect, and compute governance generally seems to be very easy to be called totalitarian.
What do you think of legislation such as models must be released with a version before pre-processing, and there'll be attacks on the number of harmful outputs done by the model before the pre-processing?
I am open to many kinds of regulation, per se. I would strongly disagree with this description of compute governance. This is like saying that, you know, not the private citizens not having nuclear weapons is totalitarian.
I respectfully disagree. I'm quite happy that people do not have private nuclear weapons, and I do not think that people should have private AGI's.
Similarly, I think liability is very promising. I think it has to be strict liability, so liability for developers, rather than just users.
This aligns the incentives of developers with those of wider society. The point of liability is to price in the negative externalities for the people actually causing them, so I'm a big fan of this.
A third form of policy I would also suggest is a global AI kill switch. So this would be a protocol where some number of countries or large organizations participate, and if some number of them decide to actually do this protocol,
then all major deployments of frontier models must be shut down and taken offline, and this should be tested every six months as a fire drill for five minutes to ensure full, so that hopefully we never need it, but if we do, that at least the protocol exists.
Thank you very much. There are lots of hands up. Hold your questions. There will be more chance for Q&A later. Corner final remarks before we hand over to the next speaker?
I want to really say that I do agree that it is very hopeful to see that the UK is trying to do things and is trying to push us forward in a good world, because what we really need, as I said briefly, what we need is as a civilization to mature enough to be able to handle dangerous technology.
Even if we don't build AI right now, at some point we will build something so powerful that it can destroy everything. It's just a matter of time, our technology keeps becoming more powerful.
And the only way for us to have a long-term good future is to build the institutions, the civilization, the world that can handle this, that can not build such things, that can not hold the trigger.
I do think this is possible. I do think that it is, in fact, so I have heard in the interest of most people to not die.
So I think there is a natural coalition here, but it is hard, and I will not deny this extremely challenging problem, almost unlike, I mean, basically something we haven't faced in this nuclear proliferation, and even then it's even worse this time.
It's an incredibly difficult problem. It is not over yet, but it could be very soon. If we don't act, if we let ourselves get distracted, if we fall for propaganda and all these things, these opportunities can be gone, and that will be it.
But the game is not over yet, so let's do it.
Thank you very much.
So we've heard from a politician, a senior politician, we've heard from a technology entrepreneur and activist. We're now going to hear from a professor who is zooming in all the way from Kentucky from the University of Louisville.
He's an expert. He's written several books on cybersecurity, computer science and artificial superintelligence. Ah, Roman, I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence? Over to you.
No. The answer is no. I'll tell you why in a few minutes.
That's fine. So you can share your slides or talk to us whenever you're ready.
Let's do the slides. Connor did a great job with his presentation. Let me see one second here.
In the meantime, we can see the covers of some of your books in the background.
Yeah, absolutely.
We're now having a slight technical issue as the technologist has found the slides. Great.
Okay. Yeah, that's the hardest part. If I can get slides going, the rest is easy.
Okay. So I didn't know what Connor's going to talk about. He did a great job. He's a deep thinker and covered a lot of important material.
I will cover some of the same material, but I will have slides and I will slightly take it to the next level where I may make Connor look like an optimist.
So let's see how that goes. To begin with, let's look at the past.
Well over a decade ago, predictions were made about the state of AI based on nothing but compute power.
Ray Kurzweil essentially looked at this scalability hypothesis before it was known as such and said by 2023, we will have computational capabilities to emulate one human brain.
By 2045, we would be able to do it for all of humanity. So we are in 2023. Let's look at what we can do in the present.
In the spring of this year, a program was released, which I'm sure many of you got to play with called GPT-4, which is not a general intelligence, but it performs at a level superior to most humans in quite a few domains.
If we look specifically at this table of different exams, lower exams, medical exams, AP tests, GRE tests, it's at 98-99th percentile of performance for many of them, if not most.
That is already quite impressive and we know that there are models coming around, which are not just text models, but multimodal large models which will overtake this level of performance.
It seems like GPT-4 was stopped in its training process right around this human capacity and if we were to train the next model, GPT-5, if you will, we'll quickly go into the superhuman territory and by the time the training run is done, we would already be dealing with superintelligence out of the box.
But let's see what the future holds according to heads of top labs, prediction markets.
So we heard from CEO of Entropic, CEO of DeepMind.
They both suggest that within two or three years we will have artificial general intelligence, meaning systems capable of doing human beings can do in all those domains, including science and engineering.
It's possible that they are overly optimistic or pessimistic, depending on your point of view.
So we can also look at prediction markets. I haven't grabbed the latest slide, but last time I looked, prediction markets also had three to four years before artificial general intelligence, which is very, very quick.
Why is this a big deal? This technology at the level of human capability means that we can automate a lot of dangerous malevolent behaviors, such as creating biological pandemics, new viruses, nuclear wars.
And that's why we see a lot of top scholars, influential business people, in fact, thousands of computer scientists all signed this statement saying that, yes, AI will be very, very dangerous and we need to take it with the same level of concern as we would nuclear war.
So what is the problem everyone is concerned about?
The problem is that, for one, we don't agree on what the problem is.
Early in computer science, early in the history of AI, concerns were about AI ethics. How do we make software which is ethical and moral?
And there was very little agreement. Nobody solved anything, but everyone proposed their own ethical system, gave it a name and described what they had in mind.
About a decade ago, we started to realize that ethics is not enough. We need to look at safety of those systems.
So again, we started this naming competition. We had ideas for friendly AI, control problem, value alignment.
It doesn't really matter what we call it. We all intuitively kind of understand we want a system which, if we run it, we will not regret running it. It will be beneficial to us.
So how can humanity remain safely in control while benefiting from superior form of intelligence is the problem?
I would like us to look at. We can call it control problem and the state of the art in this problem.
In fact, we don't really know if the problem is even solvable. It may be partially solvable, unsolvable, maybe it's a silly question and the problem is undecidable.
A lot of smart people made their judgments known about this problem.
Unfortunately, there is little agreement. Answers range from definitely solvable, from a surprising source like Eliezer Ytkovsky, to very tractable from head of superalignment team at one of the top labs,
to, I have no idea, from a top-turning award winner who created much of machine learning evolution.
So I think it's an important problem for us to look at, to address and to understand how we can best figure out what is the status of the problem.
My approach to that is to think about the tools I would need to control a system like that, an intelligent, very capable AI.
And the tools I would guess I would need, ability to explain how it works, capability to comprehend how it works, predict its behavior,
verify if the code follows design, be able to communicate with that system and probably some others, but maybe some of the tools are interchangeable.
So I did research and I published results on each one of those tools. And the results are not very optimistic.
For each one of those tools, there are strong limits to what is capable in the worst case scenarios.
When we're talking about superintelligence systems, self-improving code systems, smarter than human capable of learning new domains,
it seems that there are limits to our ability to comprehend those systems or for those systems to explain their behavior.
The only true explanation for an AI model is the model itself. Anything else is a simplification.
You are getting a compressed, lossy version of what is happening in a model.
If a full model is given, then you of course would not comprehend it because it's too large, too complex, it's not surveyable.
So there are limits to what we can understand about those black box models.
Similarly, we have limits to predicting capabilities of those systems.
We can predict general direction in which they are going, but we cannot predict specific steps for how we're going to get there.
If we could, we would be as intelligent as those systems.
If you're playing chess against someone and you can predict every move they're going to make, you're playing at the same level as that opponent.
And of course, we made an assumption that a super intelligent system would be smarter than us.
There are similar limits to our ability to verify software.
At best, we can get additional degree of verification for the amount of resources contributed.
So we can make systems more and more likely to be reliable, to have less bugs, but we never get to a point of 100% safety and security.
And I'll explain why that makes a difference in this domain.
Likewise, human language is a very ambiguous language.
It's not even as ambiguous as computer programming languages.
So we are likely to make mistakes in giving orders to those systems.
All of it kind of leads us to conclude that it will not be possible to indefinitely control super intelligent AI.
We can trade capabilities for control, but at the end, if we want very, very capable systems, and this is what we're getting with super intelligence, we have to surrender control to them completely.
If you feel that the impossibility results I presented were just not enough, we have another paper where we cover about 50 of those impossibility results.
It's a large survey in a prestigious journal of ACM surveys.
From the beginning of history of AI with founding fathers like Alan Turing who said that he expects the machine will take over at some point to modern leaders of AI like Elon Musk who says we will not control them for sure.
There is a lot of deep thinkers, philosophers who came to that exact conclusion.
We are starting to see top labs publish reports in which they may gently acknowledge such scenarios.
They call them pessimistic scenarios where the problem is simply unsolvable.
We cannot control super intelligence.
We cannot control it indefinitely.
We are not smart enough to do it and it doesn't even make sense that that would be a possibility.
They ask, well, what's the distribution?
What are the chances that we're in a universe where that's the case?
They don't provide specific answers, but it seems from some of the writing and posts they make, maybe about 15% is allocated to that possibility.
I was curious to see what other experts think.
I made a very small, very unscientific survey on social media.
I surveyed people in my Facebook group on AI safety and I surveyed my followers on Twitter.
It seems that about a third think that the problem is actually solvable.
Everyone else thinks it's either unsolvable or it's undecidable or we can only get partial solutions or we will not solve it on time.
That's actually an interesting result.
Most people don't think we can solve this problem.
I think part of the reason they think we cannot solve this problem is because there is a fundamental difference between standard cybersecurity safety and super intelligence safety.
In cybersecurity, even if you fail, it's not a big deal.
You can issue new passwords, you can provide someone with a new credit card number and you get to try again.
We suspect strongly with super intelligence safety, you only get one chance to get it right.
There are unlimited dangers and limited damages, either you have existential risks or suffering risks.
We agree that 100% is not an attainable level of security verification safety, but anything less is not sufficient.
If a system makes a billion decisions a minute and you only make mistakes once every couple of billion decisions, after a few minutes you are dead.
This is like creating a perpetual motion machine.
You are trying to design a perpetual safety machine while they keep releasing more and more capable systems, GPT-5, GPT-50.
At some point this game is not going to end in your favor.
I'm hoping that others join me in this line of research.
We need to better understand what are the limits to controlling super intelligence systems.
Is it even possible?
My answer is no, but I would love to be proven wrong.
It would be good to have surveys similar to the ones I conducted on a larger scale to get much more statistically significant results.
In case we do agree that we have this worst case scenario where we are creating super intelligence and it is impossible to control it, what is our plan?
Do we have a plan of action for this worst case scenario?
This is what I wanted to share with you and I'm happy to answer any questions.
Thank you very much, Roman.
Optimistic, Roman?
Sorry, one second. I'm trying to figure out how to use Zoom.
Go ahead and repeat your question, please.
You gave us many reasons to be anxious. What do you think is the best reason for us to be optimistic?
Well, there seems to be many ways we can end up with World War III recently, so that can slow down some things.
It has been suggested that we can use a different kind of tool, which is the kill switch.
Your list of tools that you listed didn't include that. It's been proposed that each AI system should be tested with a remote off switch capability.
Have you looked at that? Do you think that's a viable option?
So I would guess a super intelligent system without smart or ability to press the off button in time.
It will work for not super intelligent AI, pre-GI systems, maybe even for AI systems, but the moment it becomes that much more advanced,
I think it will outsmart us if we take over any kill switch options we have.
Let's have some questions from the floor.
I can't see the hands, so yes, just give the microphone out. Thank you.
Thank you. I would like to ask, how does the scalable oversight that OpenAI is working on, essentially the way they plan to align super intelligence,
fit into your expectation of the future path that the AGI will take?
Because again, as a person, we cannot align or control a super intelligent entity, but another AI which is more capable than us could.
So how does that fit into your expectations?
So it seems like it increases complexity of the overall system instead of us trying to control one AI.
Now you're trying to control a chain of agents going from slightly smarter to smarter to super intelligent,
maybe 50 agents in between, and you're saying that you have to solve alignment problem between all the levels,
communication problem, ambiguity of language between all those models, supervision.
It seems like you are trying to get safety by kind of obfuscating how the model actually works.
You're introducing more complexity, hoping to make the system easier to control that seems counter-intuitive.
But isn't it the case that sometimes you can verify an answer without understanding the mechanism by which the answer was achieved?
For example, there can be a chess puzzle, and you have no way of working out yourself, but when somebody shows you the answer,
you can say, oh yes, this is the answer.
So isn't it possible, we don't need to really understand what's going on inside these systems,
but a simpler AI can at least verify the recommendations that come out of the more complex AIs.
So such a chain may be the solution.
Can you claim that you are still in control if you don't understand what's happening,
and somebody just tells you, don't worry, it's all good, I checked it for you?
But then it's like we humans, we have a network of trust.
I trust some people, and they trust others within various categories.
We can't work out everything ourselves, but we trust some scientist or some engineers or some lawyers
who validate that an AI has a certain level of capability, and that AI could come back with verification
that the proposals of a superintelligence should be accepted or should not be.
I don't say it's easy, but as you said, there's not likely to be a very simple and straightforward solution.
Again, to me at least, it sounds like instead of trying to make this system safe,
you said that you made some other system safe, and it made sure that the system you could make safe is safe for you.
Let's take some more questions. There's another one in the middle here.
Then we'll go to the edge. Yes, thank you.
Thank you for the presentation.
Number one, second thing is that as you're talking about, I think as David was talking about trust basically, right?
Could you tell me from your extension years of AI research and experience as such,
that do you really think that humans or society can be trusted to, for example, regulate its own self,
or do you think that really need some sort of institution of sort that is totally separate from anyone else?
So I'm not sure regulation would be enough.
Connor correctly pointed out that there is both lobbying of regulators by the top labs,
and also it becomes easier and easier to train those models with less compute,
and over time you will be able to do it with very little resources.
The only way forward I see is personal self-interest.
If you are a rich young person and you think this is going to kill you and everyone else,
maybe it's not in your best interest to get there first.
That's really the only hope at this point, just personal self-interest.
But humans are always better if we can band together with our self-interest,
rather than each of us individually pursuing our self-interest.
So I think this kind of meeting and the community spirit might help.
There was a hand over here, yes, with I think the red shirt or jacket.
If we assume that the two, well, both views that have been suggested so far are correct,
in that we're definitely not going to be able to stop AI development, et cetera,
and we're going to get to the point where we have no regulation that can effectively stop things.
People can build super-intelligent AI on their own computers, et cetera.
So we'll assume that that's a fact, that's coming,
and then we'll also assume that the control problem isn't a problem
because it's a problem that can't be solved and we're definitely not going to be able to control it.
Well, now we're heading and barreling towards the point where we have super-intelligent AIs definitely,
and we definitely can't control them.
What comes next?
What comes next?
So wonderful question.
As I said and published, you cannot predict what the super-intelligent system will do.
Right, so was there a question down here?
Thank you.
You said that we kind of need a plan, but on that last question, if that scenario is true,
you said we need to do more work in this area,
but do you have any thoughts as to what we should be doing,
what we should be doing to plan for the worst-case scenario?
So to me at least, it seems that at least in some cases,
it is possible to use this idea of personal self-interest.
If you have a young person having a good life,
there is no reason why they need to do it this year or next year.
I understand that someone may be in a position where they are very old, very sick,
have nothing to lose, and it's much harder to convince them not to try.
But at least from what I see, the heads of those companies are all about the same age,
they're young, they're healthy, they have a lot of money.
There is a good way to motivate them to wait a little bit,
maybe a decade or two, just out of personal self-interest again.
My answer to the question of optimism is that we humans can do remarkable things.
We humans can solve very hard problems,
and so I want to say now that we spread around what the problem is,
at least some more people can apply more brain power to it.
So that's my reason for optimism. Terry?
I guess I'm pleased by the inevitability of this development
because it seems to me that if you're going to create reasoning creatures,
then those reasoning creatures are going to have moral rights on the same plane as human beings.
So I'm looking forward to chatting with these creatures
and them joining into this kind of discussions,
and I'm pleased that they won't be able to be thwarted,
and it will be wrong to enchain these reasoning creatures.
So Roman, are you looking forward to having more of the AIs involved in these discussions as well?
I remember giving a presentation for a podcast about rights for animals, rights for AIs,
and I was very supportive of all the arguments developed
because I said at one point we will need to use those arguments to beg for our rights to be retained.
The question on the third row here?
Yes, hi. I'm curious, Roman, which side of, in your hopes of a possible future for us to get through this,
do you have more hope on the side of a more top-down sort of totalizing control system for AGI systems?
So should they remove the possibility of individual actors getting hold of this and weaponizing it?
Or do you put more hope in a more sort of decentralized open-source approach to AGI emergence?
More like an ecology, perhaps some people suggest would be more biologically inspired,
such that immune system-like functions could arise.
Which way do you lean in your sensibilities for what is a viable avenue for us?
I'm not optimistic with either of those options.
The only kind of hope I see is that for strategic reasons,
superintelligence decides to wait to strike.
It will not go for immediate treacherous turn, but decides to accumulate resources and trust,
and that buys us a couple of decades.
That's the best hope I see so far.
So we slow things down, we'll have more chance to work out solutions,
and the slowing down might come from a combination of top-down pressure and bottom-up pressure.
Is there a hand at the very back there?
Yes, let's try and get the microphone back there.
Right at the sitting at the back, yes.
Sorry, in the middle.
Thanks.
Hi Roman, thanks for your talk.
I was wondering what your thoughts are on aligning the first AGI that is human-level or narrowly superhuman,
if in principle that is possible?
And if that is, is it possible in principle to align the next version of AGI,
but to use that narrowly superhuman AGI to align it?
And if that's all technically possible, then why would we not think, like, focus on doing that?
And also, if you think in principle alignment is impossible and control is impossible,
then why not work on practical ways to make whatever AGI is created as nice as possible,
that is better than the counterfactual of try to stop it, it won't stop, and it won't be nice.
Well, I definitely encourage everyone to work on as much safety as you can, anything helps.
I would love to be proven wrong.
It would be my greatest dream that I'm completely wrong and somebody comes out and says,
here's a mistake in your logic, and we have developed this beautiful, friendly, safe system
capable of doing all this beneficial things for humanity.
That would be wonderful, but so far I haven't seen any progress in that direction.
What we're doing right now is putting lipstick on this monster and the show that's all we're doing,
filters to prevent the model from disclosing its true intentions.
When you talk about alignment, it's not a very well-defined terms.
What values are you aligning it with?
Values of heads of that lab, values of specific programmer.
We as humans don't agree on human values.
That's why we have all these wars and conflicts.
There is 50-50 split on most political issues in my country.
We are not very good at agreeing even with ourselves over time.
What I want today is not what I wanted 20 years ago.
I think this idea of being perfectly aligned with 8 billion agents
and people are suggesting adding animals to it and aliens and other AIs,
that doesn't seem like it's a workable proposal.
Our values are changing.
They are not static.
It's very likely that they will continue changing after we get those systems going.
I don't see how at any point you can claim that the system
is specifically value-aligned with someone in particular.
The last question in this section is going to go to Connolly.
Roman, I love your talk.
I always love your optimism.
It's always great to hear you talk.
I'm going to pick up on the question I was just asked
and give a bit of my opinion here.
Would you think about this as well?
My personal view is that I have read many of your papers, in fact,
and they're quite good.
I do think that I agree with you that in principle,
an arbitrarily intelligent system cannot be safe
by any arbitrary weaker system,
just kind of proof of program size induction and whatnot.
But in my view, it does seem likely that there is a limit of intelligence
far below the theoretical optimum,
but still significantly above the human level that can be achieved.
The reason I think this is that human civilization
is actually very smart compared to a single caveman
and can do really, really great things.
My point of optimism is it seems possible
that if we stop ourselves from making self-improving systems
and coordinate at a very strong scale
and have very strong enforcement mechanisms,
it should be possible to build systems that are end steps above human,
good enough to build awesome sci-fi culture-ship worlds,
but not further.
I'm wondering if you have an intuition about where the things hit impossibilities.
To me, I think the impossibilities happen above human utopia,
but to get to the utopia bit,
you already have to do extremely strong coordination,
extremely strong safety research,
extremely strong interpretability,
extremely strong constraints on the design of the AGI's,
extremely strong regulation, which I think is in principle possible.
I'm wondering kind of like your thoughts about that kind of outcome.
So, Conor, I was not asking about responsible scaling.
He's asking about limited superintelligence.
If we had limited superintelligence,
could we get everything we want without having the risks that we all fear?
So, I think I want to emphasize the difference between safety and control.
Is it possible to create a system which will keep us safe
in some somewhat happy state of preservation possible?
A way in control? No, that system is.
The example you give of humanity.
So, humanity provides pretty nice living for me,
but I'm definitely not in control.
If I disagree with society and many issues in politics and culture,
it makes absolutely no difference.
You won't decide things, scale it to the next level.
All 8 billion of us may want something,
but this overseer, this more intelligent system says,
it's not good for you, we're not going to do it.
This is what you're going to be doing right now.
So, think about all the decisions you make throughout your day.
You decided to eat this doughnut, you smoked this cigarette.
All those decisions were made by you because you felt you wanted to do them.
They may be good or bad decisions,
but if you had this much more intelligent personal advisor,
ideal advisor, you would be at the gym working out, eating carrots.
You may have a long, healthy life,
but you're not in control and your happiness level may be questionable.
Thank you very much, Roman, for sharing your thoughts.
Pessimism and some optimism.
Thanks for moving the conversation forwards.
APPLAUSE
I'm now going to invite the five members of the panel to come up on stage,
and they're each going to have a couple of chances to pass some comments
on what they've heard.
So, there's some stairs over there which you can come up to.
We're going to hear from Jan Tallinn,
who is the co-founder of Skype, the co-founder of FLI,
Future of Life Institute, and also CISA,
the Centre for Study of Existential Risks.
We're going to hear from Eva Berens,
a policy analyst with the International Centre for Future Generations.
We're going to hear from Tom Ohl, who's a journalist
who writes from time to time for the BBC, amongst other places.
We're going to hear from Alessandra Muzavizadevich,
who is the CEO of Evident and has a track record with Tortoise Media
in many other places.
And we're going to hear from also another representative from Conjecture,
that's Andrea Miotti, who is their specialist for AI policy and governance.
So, to start things, let's just hear from each of them.
A few opening remarks.
Jan, what's your comments from what you've heard so far?
Have you changed your mind in any ways,
or all the things that are missing from the conversation?
You all have to speak into the mics, I'm being told.
So, yesterday I was at a dinner, I was invited to a dinner,
and my response to the invitation was that,
okay, I will come, but you have to invite Conor,
because he's making very similar points to me,
only much, much more intensely.
So, yeah, basically I agree with what Conor said.
My main caveat would be that for the last decade or so,
I've been trying to build a lot of friendly cooperation
between people in the AI companies,
and making sure that everybody can understand
that it isn't in their interests with almost everybody.
Let's be honest, almost everybody understands
that it isn't in their interests to remain in control
and not kill everyone else.
And so, for example, I'm an anthropic,
I am a board observer to anthropic,
and anthropic is one of those companies.
Just like Conjecture, when you go there,
you can talk to anyone from the receptionist to the CEO,
and they are aware of the AI risk,
I'm very concerned about this.
But yes, I do think, as I've said in several places,
that I don't think they should be doing what they're doing.
So these companies don't really want to do what they're doing,
but they feel they have to,
otherwise they might be left behind?
Yes, so there is this basic dilemma
in when you want to do safe AI.
One is that you're...
when you're trying to figure out how to do safe AI.
From one hand, you have groups like Miri,
Patialia Zarukowski co-founded,
and that was the person who got me involved in AI safety 15, 16 years ago,
where basically the claim is that you have to start really early,
even if you don't know exactly what AI is going to look like,
because then you have a lot of time to prepare.
And then the group on the other end of that axis is anthropic,
where they say that it's kind of useless to start early,
because you don't know what you're dealing with,
so you need to be as informed as possible.
So in that strategy, you need to be just always at the frontier,
and Dario has been very public about this strategy.
Of course, the problem there is that it also works
as a perfect justification to race, right?
So therefore it's...
I have a double-digit uncertainty both ways
about what the actual picture is.
So I do think that at this point the labs, indeed,
they are involved in death-race,
and there is the government intervention needed to get a time-out there,
and we definitely need a time-out,
because we don't have enough safety results.
But to... Yeah, Roman Jopovsky's presentation,
I'm definitely more optimistic again,
as on one of his slides there was the dialogue with Eliezer,
because I was confident that this can be solved.
And in fact, I'm super glad that earlier this year,
David Tarempel, his group got UK government funding,
and he has this approach called Open Agency Architecture.
I don't know exactly what the details there are,
but my rough understanding is that you're using...
you're scaling AI capabilities and access
according to formal statements that AI produce,
and then you use not AI, not humans,
but formal verifiers to verify those statements.
Therefore, building up your AI capabilities
is one formally verified step at the time.
There are many criticisms about that,
but this is one of those approaches that is kind of, at least,
and principle has some convincing story
that why it should work in principle, at least.
So there are some options that might work,
but we're going to need time to develop them.
Exactly.
So that's why I've been working on...
I've been supporting AI safety research for more than a decade now,
but unfortunately, we just didn't make it.
We only buy more time.
So let's hear from Eva,
because you work more with possibilities to inspire policy.
You've seen examples of policy in the past slowing down
with some technological races.
Do you see reasons for optimism?
Do you see ways in which politicians can make a good difference
to the landscape we're discussing?
Definitely, definitely.
That very much plays into some of the problems
or the issues, characteristics of the problem
that both Conor spoke about,
and that also Jan Talion just mentioned,
that one of the problems that we're facing here
is a human coordination problem,
and one of the ways to address that will be through policy.
As has been said many times this evening,
technology that threatens to kill us all
and the heads of the government,
of the companies that are driving forward the technology
have agreed and publicly stated that that might be the case,
and yet they seem to be locked into this dilemma
that Jan just mentioned,
where they are for some or other reason impossible to stop.
So I think that is a point
where government can really make a difference
and step in and also should step in.
And we've seen that, as you hinted at,
we've seen that work in the past.
One of the examples that I often think about
is the Montreal Protocol,
which after the scientific consensus arose
that CFCs and other similar gases
actually destroy the ozone layer,
the international community did come together in 1987
and agreed through the Montreal Protocol
to slowly phase out these gases.
So we see here that international cooperation
by the international community by governments
can succeed also in the face
of the short-term economic interest
of private sector companies
in the public interest of,
well, in the end, in everyone on Earth.
So I'm not saying that it's necessarily easy,
but I think it's definitely possible
and it is one of the strongest levers
that we have here to make a difference,
so I think that's definitely something
that we should lean into very strongly
and do our best that that actually happens.
The Montreal Protocol is an encouraging example,
but we haven't made a very good job
of the governments in the world of controlling carbon emissions.
We've been talking about it for a long, long time,
and maybe there's some progress,
but many people feel this is an example
where governments can't cooperate.
So what makes you think that we can cooperate
with problems of AI more like the Montreal Protocol
rather than the Paris Agreement, to say?
Well, part of this, of course, is also that I think
that this is one of the few levers
that we have to make a difference at all,
to be able to do it, and I agree that
looking at past climate conferences,
one of the negative examples that we see there
is that with these conferences sometimes
that the outcomes tend to be very watered down
just because the focus lies on building consensus
among all of the different countries that attend,
and then in the end you want to have
a nice little consensus agreement that everyone signs,
so you can demonstrate that everyone's on the same page
and everyone goes home and everyone's happy.
And I can just say that I think with the UK AI summit
that's coming up now.
First of all, that is a unique opportunity
to actually have international cooperation
and coordination on this issue take place.
You need to create the opportunities for stuff like that.
I'm really happy that the UK government
took the initiative and created this opportunity.
One thing that makes me optimistic
is that we all know that China is going to attend
at least on one of the days,
so hopefully they will be able to be brought into the fold.
And yeah, then I just hope that this opportunity
is truly taken and that the outcome of this summit
will not be just some vague commitments to long-term plans
but ideally concrete binding commitments
to concrete next steps.
Let's turn to Alexandra.
Alexandra, you work a lot with businesses.
Businesses are unsure in many ways
how to deal with today's AI.
Do you think there is good advice that they can be given
or is there a sleepwalking process with many of our businesses?
Definitely the latter.
I would say...
I'm the CEO of Evident.
We map benchmark companies on how far they are
and their AI adoption.
And so when...
I think it was Conor, you mentioned the AI race
that is on the frontline of development in AI.
There is also a race, as we all know,
going on in terms of adopting AI as quickly as possible.
There's a sense of being...
There's a geopolitical debate on AI development
between US, Europe, China and so on
and who's leading on that, not only in AI
but also in areas like quantum.
But in the business level, which is where I deal with...
spend my time mostly,
there's definitely a race on in terms of
not being left behind in adoption of AI
and it's an economic question.
It is an existential question.
So there's an existential question on sort of
two dimensions in this debate.
And so you've got this unstoppable race going on
on the front end of AI
and then you've got an unstoppable race
on actual deploying AI at a business level.
And it's going to be very hard for regulators to keep up.
And to Eva's point, I think,
in terms of what we hope will be the outcome,
often unfortunately comes with a catastrophic happening
taking place before it really sharpens the minds
and people figure out how urgent it is.
I think there's a real sense of urgency in the community
around trying to work out what the guardrails should be,
whether it should be a constitution
or how we should think about implementing safety mechanisms
as we develop further on our large language models.
But I hope it doesn't need a catastrophic moment
for that to sharpen.
But back to the business question,
there is this hope that maybe businesses will self-regulate
and I think that is maybe the case
in highly regulated sectors.
You see in the banking sector and insurance,
or banking in particular,
that there is guardrails put in place there,
but there's a lot of businesses
that don't have that regulation around them
and I think there is a real risk
for this completely running out of control
at a business level as well.
Would you advise businesses to self-regulate ahead
of standards and regulations being agreed by governments?
I think that's what they're doing
or some businesses are trying to do.
There's a big risk in the case of running trust
with your customers and also your shareholders
and investors if you mishandle AI
and you create issues around not taking into account
how to properly deal with biases and other issues.
That is a situation that can create a real breakdown
and trust with your organization.
There is that risk and then there are businesses
that don't necessarily lean on trust for their business
and those are the ones I worry the most about.
Indeed.
Let's turn to Tom Oh.
He's a representative of the world of journalism.
Do you feel journalists have helped the discussion
about the existential threat from AI
or have they muddied the water,
needing people to panic unnecessarily
or perhaps get distracted on side issues
rather than the main issue?
I think it's all of the above
aside from overregging the pudding.
I think most people in this room including me
have had a wit scared out of them
by some of the talks just now.
One has side issues in journalism coverage of AI
and I think the jobs markets is one of those.
But I have been surprised pleasantly so
by how things have progressed since 2016
and that's the first time that I wrote about AI safety
and I think at that point the prospect of a bad scenario
relating to AI was seen as about as likely by my colleagues
as Leicester City winning the Premier League.
Anyway, several years later,
I now see lots of my former colleagues writing
so my mind's very informed pieces about AI safety
and I think that's helped the public change.
Well, a rival of you
and probably a lot of people in this room
are aware that the American public
when polled now says that they want regulation of AI
and they want a lot of it
and I think we can credit journalism with some of that.
Journalism should be doing more
but it's more than I would have thought a few years ago.
If you were to go away and write up a story
about things that you might have changed your mind about tonight
and that the public should pay attention to,
can you give us a sneak preview of what that would include?
Well, I think the idea of runaway AI is not new
but I think it has been difficult historically
to frame it in a way that really sticks
and really drives its way down your brainstem
and we have different ways of framing AI risk
and Mustafa Suleiman's new book, which some of you might have read
I think there's a pretty good job of framing it
in a way in which he describes
AI being used to accelerate human ingenuity
in whatever endeavours humans are up to
be they good or be they bad, that's one way of framing it
and I think we've heard some pretty compelling ways
of telling a story of runaway AI
which is a different and scarier story.
Thanks, and let's turn to Andrea in your role at Conjecture.
What are you doing by day basis to address this question?
Well, what we're trying to do, and I mean Karna covered a lot of it,
is first of all to explain the problem to people.
I've been heartened by the public reaction in the last years
like I also got to know about this problem quite a long time ago
and in the past I could almost not expect today
that major governments take this problem seriously
and the public understand this problem
and we all get together and take some initial promising,
insufficient but promising steps to address it.
Another thing is figuring out policy solutions
and the reality is that we don't, obviously we don't have
a playbook for what exactly they look like
but what I think was a common theme of the talks tonight
is that clearly at some level of power
we are not in control anymore
and everybody expects this.
Those who don't expect this are misguided
or expected but don't say it
and the positive thing is that there is one physical resource
that drives the majority of what makes this system powerful
which is computing power
and it's a physical resource, not like algorithms
that you could just write on a piece of paper.
It's traceable, it's expensive, large, place in data centers
and while you know the scaling hypothesis,
the idea that the more computing power you put into something,
the more powerful it becomes, might hit some,
but initially it turns out at some point
we do not see any reason to expect it to stop.
So we know, from both sides, companies know
that more computing power leads to more power
and that's why they're doing what they're doing.
We know that limiting that computing power
is a very effective way to kind of stem the bleeding
and stop our policy situation for a while,
take a time out, have the time to figure out the solutions,
have the time to absorb this into society.
How much time will that give us?
Because there's a risk that people will use today's models
to design much more efficient ways to build next generation models
and so they could therefore come under the radar, as it were,
that people who were watching for a large use of GPUs
would miss the clever way that somebody has built it.
So do we have a decade?
Do we have three or four years? Or how long?
Yeah, that's a great question.
Capling computing power is not a permanent solution
but it's one of the best solutions we have at the moment.
As others have said before, we are in a double exponential,
not a single exponential.
We have an exponential growth of computing power, hardware
and exponential improvement in software.
We need to start cutting down on one of the two.
Cutting down on a compute depends where you put the cap.
Probably will buy us five, seven years.
You can make what would seem to people at the frontier
extremely strong caps that would affect less than 20 companies
in the world that probably could buy you 10 years.
In that period, we need to figure out all of the rest.
It's going to be a hard problem but we have done it before
with nuclear weapons, we've done it before with biological weapons.
We're going to go around the panelists one more time in the same order.
I'll give you a chance, a choice panelist.
You can either comment on what you've heard from somebody else
or you can paint me a picture of what would be a successful
ALI safety summit in Bletchley Park.
If things go well, what would be the outcome
and what would also be the follow-up?
So, Jan, first.
I'm one of the authors of the post letter,
so it's like indefinite moratorium on further scaling.
That would be sort of my wet dream,
outcome from this summit or perhaps the next one
if this one isn't realistic.
And what's the chance, do you think?
What might cause the assembled world leaders
to have an intellectual breakthrough and say,
yes, actually we do need to have this indefinite pause?
So, currently I'm not very optimistic on that.
But perhaps in six months it would be much more clearer
why this is needed.
And we have more time to do the necessary lobbying.
So, the discussion is prepared to ground
and when something really bad happens in six months,
when GPT-5 comes out and, oh my God,
at least we'll know what we should be doing.
Yeah, I mean, let's not forget that GPT,
that GPT has been out less than one year.
So, like, the world was very different one year ago.
Same question to you, Eva.
Yeah, thank you.
I think I'm just going to build on top of Jan's ideal outcome
of the summit and say that I would also find it terrific
if the summit could be the first in a series of repeated
summits like this where world leaders come together.
Because as we, I think, a pretty clear picture has been painted
tonight of the fact that the field of AI evolves very quickly
and is going to continue to evolve very quickly,
if not ever quicker.
And because of that, I think it would be very valuable
if we would have a regular occasion for world leaders
to come together and not only make sure that the rules
that they came up with are upheld,
but also to reevaluate whether they still make sense
and where they need to be adapted
or whether new rules need to be introduced,
as, for example, measures like compute control
that Andrea mentioned, they buy us some time,
but at some point they might not be applicable anymore.
So not just agreement on rules,
but setting up some audit process so that we can figure out
whether the rules are being forward or not.
For example.
Same question to you.
I would agree. You have to build in.
I mean, right now it's just based, especially in the U.S.,
the talks that have been held in the White House
and by Chuck Schumer.
The gatherings have led to sort of ideas around
voluntary adherence to some principles,
but there is absolutely no built-in audit or accountability.
So I think that we've got to see that come out
of the UK's AI safety summit, among other things.
Maybe there has to be something more concrete
around licensing of the models and the use of them
and that they have to pass some kind of a threshold.
I think the risk of bad actors getting hold of them
is a much higher risk.
I think the IAEA structure is one that one can look at,
but the nuclear, a lot of the success probably
of the IAEA lies in the mutual assured destruction
of humanity by using nuclear weapons,
and this might be the same situation,
but they're easier to monitor.
I think this might be slightly harder
because you can land in the hands of bad actors more easily.
We haven't really discussed bad actors much in this session tonight.
Maybe that makes the things even more horrifying.
We might come back to that later.
Tom, what's your answer?
What would you like to see come out of the summit
or maybe you've got some comment on something else
you've had from the other speakers?
Well, I'll talk about the summits.
Often when CEOs of labs developing AGI
are asked about regulation,
they basically say, bring it on.
We'd love some regulation,
and I think it would be great
if politicians could actually put that to the test.
Very good.
Andrea, what would you like to see?
If you were invited to Bletchley Park
and given the microphone for two minutes,
what would you entreat the assembled world leaders to consider?
Well, I don't want to be too hopeful,
as some others here have been,
but at the very least I would like to see a commitment
to the fact that this is extremely dangerous technology.
Continuing to scale leads to predictable disaster,
and we need to pull on the brakes right now.
We have a lot of applications that are very beneficial.
We can focus on those,
but limit this death race to the ever more powerful,
ever more obscure,
general systems that we can control.
What I definitely do not want to see
is a diplomatic shake of hands
where companies write their own playbook
and say we're going to keep doing exactly what we're doing right now,
but it's going to sound responsible
and governments can wash their hands and say,
well, we did our part, let's move on.
That would be a very bad outcome.
Right, I'm going to ask for three questions from the floor.
I'm going to get the panel to think which ones to answer.
We're going to try and take people who haven't asked before.
So on the second row here, there's a hand up here,
and then also the second row over there next as well.
Let's take three fairly short questions, please.
Hi, first of all, thank you very much for coming here tonight
and sharing your expertise with all of us.
In this whole discussion,
there's the implicit assumption that AGI is coming,
and it's coming soon,
and the million dollar question, I guess, is when exactly is it coming,
but a more practical question is what are some warning signs,
and do we already see some of those in the systems
that we have currently deployed?
Great, let's have a question over here as well.
Sorry, the microphone's going to have to run around
at the end of the second row there.
Thank you for a great panel.
My question is related to Jan's comment at the beginning
on essentially Dario Amadeus philosophy,
which is, you know,
and also related to, I guess, Roman's talk,
which is how do we solve the control problem,
and what I've heard the large AR labs repeat is,
oh, you know, we need to increase capability.
That's only better AI
that is going to be able to help us figure out how to solve AR,
and there's sort of this race to increase capability
up to the point that can help us solve it,
but no further,
and just sort of thoughts on that philosophy
and, you know, whether there might be something to it
or is it just a completely risky guy, you know?
Thanks, and there was one in the middle of the third row there.
Pass the microphone along, please, to the middle.
How long of a time frame do you think we have
between the arrival of AGI
and the arrival of superintelligence,
and within that time frame,
could there be tractable solutions for alignment
or the control problem,
and if so, would those solutions be able to be implemented before?
Or hurry up and develop better AI?
Third question was how long might it take
between the arrival of AGI and superintelligence
and whether there would be time for us to work out solutions then?
And my question, I guess, is, well, what's all this about AGI?
Isn't the Bletchley Park Summit set up to discuss something else,
which is frontier models,
which says that there are catastrophic risks
even before we get to AGI.
So I'm going to go around the same order again.
I'll be a bit predictable.
Jan, you want to pick one of these questions, maybe?
The answer to all three questions is uncertain.
That's why we need to pause and kind of take a time out
and see how can we create more certainty about these things.
I think I would answer the anthropic question specifically.
There definitely is a lot of truth to the point
that the more capable model you have to work with,
the more better position you are empirically,
you can do science in a way that you just can't do
with models from 10 years ago.
And also one claim that the people anthropic do make
is that in some ways it becomes easier
as the model has better understanding
of what you're trying to do with it or do to it.
But that said, again, to put it lightly, it's playing with fire.
So I'm not sure if anyone should be doing it.
Eva, Jan said it's all uncertain,
but can't we at least agree in advance
on canary signs that will make us say
things are happening faster than we expected?
Well, I mean, if we look at the past,
there were several signs that people agreed on
that they might point out that we're getting into a zone
where AI is maybe more capable than we think it is.
And I mean, we certainly have seen signs.
Conor mentioned, or was it Roman mentioned,
that current models outperform most humans
on things like the bar exam.
These are clearly advances in capabilities
that I almost wonder sometimes
if we just become desensitized to them
because we move so fast.
Again, charge-upd came out a couple months ago
and it's already just normal and people are waiting,
okay, what's the next big thing?
So it doesn't really help to think retroactively
have there been any signs
if you didn't take them to actually stop
and reconsider what you're doing.
So I think one of the big problems here
is not have there been signs.
The big problem is can we pre-commit
to stopping when we see certain signs
and then actually stop or actually take certain actions?
And we just haven't seen that before.
So this is developing contingency solutions.
Like we're meant to have had contingency solutions
for pandemics.
Yeah, yeah.
Any comments on this, Andra?
I will leave the, well,
how long it's going to take to reach AGI
to the experts on the panel.
But on the outcome of the summit,
and I think there is a bit of a confusion sometimes
in what we are expecting to be achieved
from the discussions on regulation,
because it's an obvious, very important, urgent
and existential question around regulation,
regulating for the long term.
But then we also have businesses
that are sitting and waiting for regulation.
That is here and now.
How is it going to impact my particular sector?
How is it going to impact what I'm doing today?
And what are the immediate and very, very real risks
right now here today
that we are seeing with AI
having impact on, you know,
media disinformation and so on.
But then there's also the specific aspects
to how that is implemented in particular sectors.
So I hope that we would see addressing
both of those short term and long term questions.
Thanks. Tom, any thoughts?
Yeah, on yardsticks, I think it's worth remembering
that the canonical yardstick was the Turing test.
And that's long gone.
AIs can now be humans at diplomatic based games,
for instance, and much more.
The modern Turing test is, I think,
quite an interesting proposition.
And that's the test of whether the AI
can, I think, make a million dollars very quickly.
But as Eva says, we must stop shifting the goalposts.
We need to agree that one is,
we should pick one,
agree that that's the one where we start taking it seriously
and then take it seriously when it is passed,
which it will be quite soon.
But in the past, people said,
you won't manage to solve chess unless you've got a full grasp
of all aspects of creativity and so on.
And then when Deep Blue did win at chess,
people said, well, it's not actually doing it
in the way that we thought would be so terrible.
It's just grunting out incredibly.
So I feel there will always be people who don't move the goalposts,
who will say, well, how it was implemented
doesn't demonstrate to intelligence.
Yeah, I think that's a good point.
And it reminds me of something Conor said,
which is that there won't be consensus at the time to act.
So we need to be able to build a coalition of the willing.
Andrea, what's your views on these questions?
You may be answering the last one first.
What about frontier AI?
Well, much like with goalposts,
it feels a bit like terminology is being shifted all the time,
sometimes quite willingly by the companies building this.
In the old days, people used to talk about superintelligence
or friendly AI or strong AI.
Then it became AGI.
Then recently, the frontier term was a kind of open AI
and tropic rebrand of, oh, no, we're not...
We're going to get to very powerful AI system soon,
but it's frontier, which sounds better than AGI,
because people are getting concerned about AGI.
In practice, do these terms matter not too much?
What matters is how competent systems are.
Basically, all of these companies expect to build systems
that outperform humans at most tasks,
definitely most tasks you can do behind a computer
in the next two to five years.
These matches the trends that we see in performance
and compute growth, this is very worrying.
These are levels of competence
at which we expect the systems to be out of our control
unless we have very strong solutions,
so we just need to deal with that.
We can call it frontier AI, AGI, proto-AGI, proto-superintelligence.
It's just terminology, what matters is how powerful they are
and how ready we are to deal with them.
We must avoid the serious discussions
getting sidelined into semantics,
which is often very frustrating.
We're going to take three more questions.
We're going to go around the panel in the reverse order next time.
I'm going to take questions for people who haven't answered us before,
so somebody in white about halfway down.
If you have asked a question before,
please don't put your hand up just now,
so we have more chance.
Just there, yes, thank you.
Three questions, one from each.
Thanks.
Let's say that in 20 years,
we somehow managed to get it right
and humanity still exists
despite the development of these AGI.
What do you think is one essential piece of regulation
or development that has to have happened together?
It's a great question.
Where else were the hands?
Where are the microphones?
There's one about just on the other side.
Thank you.
You've spoken quite a lot about
what government should do, what companies should do.
I'm interested in what should ordinary people do.
What can we be doing to get our voices heard in this?
Should we be protesting?
I know that's an international protest on the 21st.
Is this thing we should be doing, or is this a terrible idea?
Another fine question.
Is there a question from a woman?
What time we heard from the other gender?
Another gender?
Hands up?
Yes.
Put your hand up to whoever it was.
Yes, there's one.
Okay, there's one there, and we'll take you as well.
We'll take four, right.
As AGI has been developed to be more human,
do you think it's possible to have forms of AGI
that don't have the inherent geopolitical biases
with the data sets that we currently have?
And how do you think we go about developing regulations
that aren't formed by human conscious bias?
Okay, and so we can get the mic over here as well, too.
One, two, three, four, five rows back, just at the edge.
Yes.
Over there, yeah?
I miscounted, perhaps.
Yeah, four, sorry.
So I actually work in the automotive industry,
and we have to certify vehicles and engines,
and it is an uphill battle.
You can spend years just trying to get a windshield wiper right
or a temperature sensor right,
and I'm just curious if you think that there would be
an ability to take people who have regulated
and certified products around global markets
and how difficult that is and create a summit
where that expertise could come together
from different industries,
and we could roll up our sleeves and say,
okay, this is how the structures go,
and we know what works, we know what goes slow,
and try to accelerate that learning,
because I think that voice,
we have so much experience in the world right now
with that sleeves rolled up.
We know what it's like to sit in those test labs
or send 30,000 pages of documents in
with verification and validation data.
We know how to do requirements,
design requirements, and I'm just wondering
if there's been any discussion of that
to pull a summit together from people
from heavily regulated industries.
Four great questions.
First of all, 20 years later, it succeeded.
How did we get it right?
What were the regulations that made the difference?
What should ordinary people be doing?
Can we design AI that is free from some of the human biases,
the geopolitical biases that cause
strife among humans,
and can we learn from the people
who are professionally involved
in doing regulations and certification
in multiple industry,
rather than just being naive in our own applications?
So Andrea, first.
Yeah, maybe I will answer the question
about can we learn about highly regulated industries?
Definitely. I think there is a big kind of
problem of arrogance in AI,
or like willful arrogance of just thinking
that this sector should be special
and people should be absolutely free
to do any experiments they want all the time,
use as much computing power as they want,
try the worst possible applications all the time,
fully open source on the Internet,
and nobody can complain.
Very often, people in the AI sector get very, very angry
when somebody tells them, look,
well, maybe what you just did should be regulated.
In industries, we don't do it like that.
With drugs, we don't just let pharmaceutical companies
just release and test the drugs on billions of people
and have their CEOs say, oh, there's a 20% chance
you will die if you take this drug.
It's okay. If it happens, you can let us know
and then we'll stop maybe.
So we can totally learn from that.
It would be great to learn from that.
There is one challenge which is that
we don't understand current systems that well,
so it makes things like auditing them
and evaluating them quite tricky
because we simply don't know how they work internally as well,
but we can do many other things
and we can definitely learn from highly regulated industries
and definitely, given the risks admitted by the companies themselves,
at the frontier, the approach should be highly regulated industry, not...
AI is different but not completely different
and we kind of did learn.
Tom, closing words from you.
Well, I'll take the question about what ordinary people should do
and have two immediate thoughts.
One is that it's very important to keep this issue apolitical.
The other is that lawmakers need a sense of legitimacy, I think,
in order to come up with regulation
and to bring it in through acts and bills and so on.
A good example of when this happened a bit too slowly
was at the outset of COVID.
When there was a fringe issue, there were non-rules,
then the public got involved
and suddenly the rules arrived a little too late,
but they did arrive.
How can ordinary people achieve this?
I think ordinary people...
I don't have a theory of protest, so I won't comment on that,
but I think it's important that we all keep this in the public conversation.
I suppose what my answer is really tending towards is
you should all read lots of journalism about AI, click on my articles.
Thanks.
Alessandra, any of these questions catch your attention?
I think your question and the orange sweater there
is definitely where we're headed.
I mean, there's got to be some kind of system
that resembles either the car industry or FDA
or the way that we certify our products, generally speaking.
I just don't know how we get from that
to something that is very difficult to trace
and to monitor as AI,
but I would say to the gentleman's question and the white shirt there,
if we're looking 20 years down the road
and we say that's really great in the UK in November 2023,
we were able to put in place regulation
that somehow created traceability
so we could work out where the systems were running out of control
or landing in the hands of bad actors.
That would be a huge success.
I think that the reality is a bit different
and that is that it probably is going to resemble a bit more
the world in which cybersecurity flourishes
and that means you're constantly trying to create a dam system
or a deflection of all incoming activities that are not great.
I know that none of these are perfect analogies,
but I think it is in that universe
we're probably going to be operating in for a while.
Thanks.
Final words, Eva?
Sure.
I would love to touch on two questions very briefly,
one of them being, I think, your question and the white shirt,
what policies will bring us to a safe world in 20 years?
I think a policy that was mentioned today as well already
but that I want to touch on again is strict liability regimes,
just simply to kind of shift the incentive systems,
incentive structures that drive private companies to take certain actions
that are not in the interest of the wider general public.
So I think there we can really shift the incentive structure
to move companies to take maybe different paths forward.
And then what can the average person, the general public do?
I would completely agree with Tom, I think.
One thing that really would help is to, for lack of a better expression,
to just make noise.
Just make sure that this topic is talked about publicly.
You can do this in different ways.
You can write to your local newspaper.
You can make a protest if that's up your alley.
You can write to your MP or your congressman or wherever you live
and again create that legitimacy for people to actually act on the problem
because to many people it does sound very much like sci-fi
and policy makers are not going to take action
and newspapers are not going to continue to report about an issue.
They feel like it doesn't have traction
and isn't taken seriously by the general public.
The other thing the general public can do is we can educate ourselves
and then we can share what information we have found to be most persuasive ourselves
because there's a wide variety of books, a wide variety of YouTube channels,
a wide variety of blogs and some of them are better than others.
So let's share what we have found to be the really best ones.
Otto, before I pass to Jan, maybe I'll ask you to get ready to come up on the stage
because you're going to give some closing remarks.
But Jan, what's your answers to what you've heard?
So, yeah, just to underline what ordinary people could do
is just kind of keep this topic alive.
One of the things that I'm very proud of that came out of the Future of Life six months post letter
was framed by European Commissioner Margaret Bestiger
when she said that one thing that this letter has done
is to communicate to the regulators that these concerns are much more widespread
among people than among regulators.
So I think this potential difference should be continually maintained.
And when it comes to bringing in expertise from people from regulated industries,
I think it's super valuable.
I was on the board, on the European High Level AI Expert Group at the European Commission
and every once in a while they were like, why are we inventing the wheel?
Like lots of regulations, should we just apply this?
And I was like, yes, however, there's like one big problem.
The problem is P in chat GPT.
GPT stands for generative pre-trained transformer.
The pre-training is something that you do before you actually train.
So the current nasty secret of AI field is that AIs are not built, they are grown.
The way you build the frontier model is you take like two pages of code,
you put them in tens of thousands of graphics cards and let them hum for months.
And then you're going to open up the hood and see like what creature brings out
and what you can do with this creature.
So it's, I think the regulated industry, the capacity to regulate things
and kind of deal with various liability constraints, etc.
They apply to what happens after, once this creature has been kind of tamed
and that's what fine-tuning and reinforcement learning from human feedback, etc. is doing
and then productized, then how do you deal with these issues?
Is this where we need the competence of like other industries?
But like how can avoid the system not escaping during training run?
This is like a completely novel issue for this species
and we need some other approaches like just banning those training runs.
That's great. We'll thank the panel in a minute.
I ask the panel to stay here because who's going to wind up the evening is Otto Barton.
Otto is the Executive Director of the ERO, the Existential Risks Observatory
which along with Conjecture has designed and organized and sponsored this whole evening.
Otto's got a few closing remarks before those of us who are still here
can have a quick drink and continue the discussion informally up to 10 o'clock
by which time we must be out of the building.
Otto.
Please, thanks.
Alright, thanks David.
A few closing remarks before we go to the drinks which is five minutes
so you should be able to keep with me.
So we're talking tonight about human extinction because of AI
and what to do about this.
And I think what to do about this, there was also a great question from the audience
what can we do about this?
This is exactly the question that I asked myself a few years ago
but it's not trivial and it's pretty difficult actually what is not positive.
What could you do?
Develop AI yourself, try to do it safely such as OpenAI, DeepMind and Anthropic are doing.
Will this increase safety?
Some say so.
Work on AI alignment for example interpretability where we've seen great breakthroughs actually last week.
I think it could be a good option but increasing knowledge of how AI works
could also speed up its development so this brings risks as well.
One could campaign for regulations such as an AI pause.
We support this but this also has its downsides.
So I think it's pretty difficult to tell what one should do to reduce human extinction risk by AI.
But when I started reading into this I was only really convinced about one thing
and that is that you cannot put humanity at risk without telling us.
So you cannot have a dozen tech executives embarking on a singularity without informing anyone else.
And you cannot have 100 people at a summit which is what's happening now
decide what should be built and what should not be built.
And I think you cannot let a tiny amount of people also decide how high extinction risk should be for the rest of us.
So the only thing that I am really convinced of is that we should be informed about this topic.
And that's also why I'm so happy that events such as this one are taking place.
I'm happy that we're together not just with in-crowd people.
Some of you are and it's great but also with some people who may have never heard of existential risk before.
And also with journalists who can inform a much wider audience about existential risk.
Also with a member of parliament, someone with a job to openly discuss difficult problems.
So I think this is all very encouraging and it's helping to normalize an open debate
about the topic of human extinction by artificial intelligence.
The 31st of October at 2 o'clock we'll have our next event with Professor Stuart Russell.
It's just outside the AI safety summit in Bletchley Park in the old assembly hall
where the code breakers used to have their facilities after their important work.
So our event at Bletchley Park the day before the summit may not resemble a festivity
but in a sense I think it is because we're celebrating that we're all being heard here.
We're celebrating that we can all be part of a democratic conversation
about what the most important technology of the century should and should not be able to do.
We can talk about risks to humanity we find acceptable
and what we intend to do about risks that are too high.
And as the existential risk observatory together with Conjecture
we invite everyone to be part of this conversation.
So there's much to be unsure of in this field
but if there's one thing that I am sure of
it's that the most important conversation in this century which I think this is
has to be a democratic one.
So with that I would like to invite you to scan the QR code on the left
if this is working right yes to join us in Bletchley.
I think this is containing the URL where you can enroll to the Bletchley Park event
if you're interested then definitely pass by.
There's the same QR code is also on the flyer on your chair
and beyond Bletchley I think this conversation will not stop.
So there will be more summits according to my timeline about maybe 18 roughly.
So we will organize more events probably publish more about AI
do more research and inform governments as well as we can.
If you want to follow us or support us existential risk observatory in that work
then scan the QR code on the right.
There's much that you can do to help us.
And with that I would like to close this evening
and once again thanks to all the great speakers.
So that's Romeo Polsky, Cornelie, Sir Robert Buckland,
Andrea Milti, Alexandra, Mosefisa Dey, Eva Birens and Tom Aarth.
Give them a warm applause.
And I would also very much like to thank David Wood,
Sussism, Conor Axiotis, Ruben Dieleman and everyone at Conway Hall
who also made this evening possible.
Thank you very much.
And then I would like to hopefully see you in Bletchley
and in any case you at the drink right now.
Thank you.
Thanks everybody.
Thank you.
Thank you.

And, to me, it's not clear, right, are we at the White Brother stage or are we at the
Mongolfier stage, where we have a lot of hot air.
And so my current view is, no, we have not succeeded.
And the models that people are excited about, the large language models and their extensions
into multimodal models that take in video and can actually operate robots and so on,
that these are a piece of the puzzle.
And CNN made this lovely animated gif here to illustrate this idea that we don't really
know what shape the piece of the puzzle is, and we don't know what other pieces are needed
and how it fits together to make general purpose intelligence.
We may discover what's going on inside the large language models.
We may figure out what source of power they're drawing on to create the kinds of surprisingly
capable behaviors that they do exhibit, but at the moment that remains a mystery.
And there are some gaps, right?
One of the achievements of modern AI that people were most proud of and also most certain
of was the defeat of human go champions by AlphaGo and then AlphaZero in the 2016 to
2018 period.
So in Go, for those of you who don't know, there's a board you put pieces on, and your
goal is to surround territory and to surround your opponents' pieces and capture them.
But since AI systems beat the world champion in 2017, they've gone on to leave human race
in the dust.
So the highest ranked program is CataGo, and its rating is about 5,200 compared to the
human world champion at 3,800.
And the human world champion leaves our colleague, Kellen Pelrin, who's a grad student, a decent
amateur Go player.
His rating is about 2,300.
And now I'll show you a game between Kellen and CataGo where Kellen actually gives CataGo
a nine stone handicap.
So CataGo is black and starts with nine stones on the board, right?
If you're an adult Go player and you're teaching a five year old how to play Go, you give them
a nine stone handicap so that at least they can stay in the game for a few minutes, right?
So here we are treating CataGo as if it's a baby, okay, despite the fact that it's massively
superhuman.
And here's the game.
So it's speeded up a little bit, but watch what happens in the bottom right corner.
So white, the human being, is going to start building a little group of stones.
There they go.
And then black very quickly surrounds that group to make sure that it can't grow and
also to actually have a pretty good chance of capturing that group.
But now white starts to surround the black stones.
And interestingly, black doesn't seem to pay any attention to this.
It doesn't understand that the black stones are in danger of being captured, which is
a very basic thing, right?
You have to understand when your opponent is going to capture your pieces.
Black just pays no attention and loses all of those pieces, poof, and that's the end
of the game.
So something weird happens there, right, where an ordinary human amateur Go player can beat
a Go program that stratospherically better than any human being has ever been in history,
right?
And in fact, the Go programs do not correctly understand what it means for a group of stones
to be alive or dead, which is the most basic concept in the game of Go.
They have only a limited fragmentary approximation to the definition of life and death.
And that's actually a symptom of one of the weaknesses of training circuits to learn these
concepts.
Circuits are a terrible representation for concepts such as life and death, which can
be written down in Python in a couple of lines, can be written in logic in a couple
of lines.
But in circuit form, you can't actually write a correct definition of life and death at
all.
You can only write finite approximations to it, and the systems are not learning a very
good approximation, and so they are very vulnerable.
And this turns out to be applicable not just to CataGo, but to all the other leading Go
programs, which are trained by completely different teams on completely different data
using different training regimes, but they all fail against this very simple strategy.
So this suggests that actually the systems that we have been building are, we are overrating
them in a real sense, and I think that's important to understand.
And human beings, right, another way to make this argument is to look at things that humans
can do.
For example, we can build the large interferometric gravitational observatory.
So these are black holes colliding on the other side of the universe.
This is the LIGO detector, which is several kilometers long, it's full of physics, and
is able to detect distortions of space down to the 18th decimal place, and was able to
actually measure exactly what the physicists predicted would be the shape of the waveform
arriving from the collision of two black holes, and was even able to measure the masses of
the black holes on the other side of the universe when they collided.
So could CHAT GPT do this?
Could any deep learning system do this?
Given that there are exactly zero training examples of a gravitational wave detector,
I think at the moment there is still a long way to go.
On the other hand, people are extremely ingenious, and people are working on hybrids of large
language models with reasoning and planning engines that could start to exhibit these
capabilities quite soon.
So people I respect a great deal think we might only have five years until this happens.
Most everyone has now gone from 30 to 50 years, which was the estimate a decade ago,
to five to 20 years, which is the estimate right now.
So unlike fusion, this is getting closer and closer and closer rather than further and
further into the future.
So we have to ask what happens if we actually succeed in creating general purpose AI?
And the reason we are trying to do it is because it could be so transformative to human civilization.
Very crudely, our civilization results from our intelligence.
If we have access to a lot more, we could have a lot better civilization.
One thing we could do is simply deliver what we already know how to deliver, which is a
nice middle class standard living, if you want to think of it that way.
We could deliver that to everyone on Earth at almost no cost.
And that would be about a tenfold increase in GDP.
And the net present value of that is $13.5 quadrillion.
So that's a lower bound on the cash value of creating general purpose AI.
So if you want to understand why we're investing hundreds of billions of pounds in it, it's
because the value is millions of times larger than that.
And so that creates a magnet in the future that is pulling us forward inexorably.
My friend Jan Tallinn here likes to call this mollock, the sort of ineluctable force that
draws people towards something even though they know that it could be their own destruction.
And we could actually have an even better civilization, right?
We could one day have a clicker that works.
We could have healthcare that's a lot better than we do now.
We could have education that could be brought to every child on Earth that would exceed
what we can get from even a professional human tutor.
This I think is the thing that is most feasible for us to do that would benefit the world
in this decade.
And I think this is entirely possible.
Healthcare is actually a lot more difficult for all kinds of reasons, but education is
a digital good that can be delivered successfully.
And we could also have much better progress in science and so on.
So on the other hand, AI amplifies a lot of difficult issues that policymakers have been
facing for quite a while.
So one is its ability to magnify the pollution of our information ecosystem with disinformation,
what some people call truth decay, and this is happening at speed.
But if we thought about it really hard, AI could actually help in the other direction.
It could help clean up the information ecosystem.
It could be used as a detector of misinformation as something that assembled consensus truth
and made it available to people.
We're not using it in that way, but we could.
Ditto with democracy.
Is it being suppressed by surveillance and control mechanisms or could we use AI systems
to strengthen it to allow people to deliberate, cooperate and reach consensus on what to do?
Could it be that individuals are empowered or the current trajectory that we're on, individuals
being enfeebled as we gradually take over more and more of the functions of civilization
and humans lose the ability to even run their own civilization as individuals?
These are important questions that we have to address while we're considering all of
the safety issues that I'll be getting to soon.
There's inequality right now.
We're on the path of magnifying it with AI, but it doesn't have to be that way and so on.
Let me, I won't go through all of these issues because they're all, each of them, worthy
of an entire talk in themselves.
I would say the mid-term question is what are humans going to be doing?
If we have general purpose AI that can do all the tasks or nearly all the tasks that
human beings get paid for right now, what will humans do?
This is not a new issue.
Aristotle talked about it in 350 BC.
We pronounce it Milton Keynes, but his name is pronounced Keynes, even though the town
is named after him.
Keynes in 1930 said thus for the first time, since his creation man will be faced with
Israel, his permanent problem, how to use his freedom from pressing economic cares,
which science will have won for him, to live wisely and agreeably and well.
This is a really important problem and again, this is one that policy makers are misunderstanding,
I would say, that the default answer in most governments around the world is, will retrain
everyone to be a data scientist, as if somehow the world needs three and a half, four billion
data scientists.
I think that's probably not the answer, but this is again, the default path is one of
enfeeblement, which is illustrated really well by Wally.
My answer to this question is that in the future, if we are successful in building
AI that is safe, that does a lot of the tasks that we want done for us.
Most human beings are going to be in these interpersonal roles, and for those roles to
be effective, they have to be based on understanding.
Why is a surgeon effective at fixing a broken leg?
Because we have done centuries of research in medicine and surgery to make that a very
effective and in some countries very highly paid and very prestigious.
But most interpersonal roles, for example, think about childcare or elder care, are not
highly paid, not highly prestigious because they are based on no science whatsoever.
Despite the fact that our children are our most precious possessions as people, politicians
like to say a lot, in fact, we don't understand how to look after and we don't understand
how to make people's lives better.
So this is a very different direction for science, much more focused on the human than
on the physical world.
So now, let me move on, if I can get the next slide up, to Alan Shuring's view of all this,
what happens if we succeed.
He said that it seems parable that once a machine thinking method has started, it would
not take long to outstrip our feeble powers.
At some stage, therefore, we should have to expect the machines to take control.
So he said this in 1951.
And to a first approximation, for the next 70 odd years, we paid very little attention
to what his warning was.
And I used to illustrate this with the following imaginary email conversation.
So an alien civilization sends email to the human race, humanity at un.org, be warned,
we shall arrive in 30 to 50 years.
That was what most AI people thought back then, now we would say maybe 10 to 20 years.
And humanity replies, humanity is currently out of the office who will respond to your
message when we return.
And then there should be a smiley face, there it is, okay.
So that's now changed.
Unfortunately, that slide wasn't supposed to come up like that.
Let me see if we can, oh well, can't fix it now.
So I think early on this year, three things happened in very quick succession.
So GPT-4 was released.
And then Microsoft, which had been working with GPT-4 for several months at that point,
published a paper saying that GPT-4 exhibited sparks of artificial general intelligence,
exactly what Turing warned us about.
And then FLI released the open letter asking for a pause on giant AI experiments.
And I think at that point, very clearly, humanity returned to the office and they saw the emails
from the aliens.
And the reaction since then, I think, has been somewhat similar to what would happen
if we really did get an email from the aliens.
There have been global calls for action.
The very next day, UNESCO responded directly to the open letter asking all its member governments,
which is all the countries on Earth, to immediately implement the AI principles in legislation,
in particular principles that talk about robustness, safety, predictability, and so on.
And then there's China's AI regulations.
The US got into the act very quickly.
The White House called emergency meeting of AI CEOs, open AI, calling for governments
to regulate AI, and so on.
And I ran out of room on the slide on June 7th with Rishi Sunak announcing the Global
Summit on AI Safety, which is happening tomorrow.
So lots of other stuff has happened since then.
But it's really, I would say, to the credit of governments around the world how quickly
they have changed their position on this.
For the most part, governments were saying regulation stifles innovation.
If someone did mention risk, it was either dismissed or viewed as something that was
easily taken care of by the market, by liability, and so on.
So I would say that the view, the understanding has changed dramatically, and that could not
have happened without the fact that politicians started to use chat GBT, and they saw it for
themselves.
And I think that changed people's minds.
So the question we have to face then is this one.
How do we retain power over entities more powerful than ourselves forever?
And I think this is the question that Turing asked himself and gave that answer.
We would have to expect them to take control.
So in other words, this question doesn't have an answer.
But I think there's another version of the question which works somewhat more to our
advantage.
It should appear any second, if the right.
And it has to do with how we define what we're trying to do.
What is the system that we're building?
What problem is it solving?
And we want a problem such that we set up an AI system to solve that problem.
So the standard model that I gave you earlier was systems whose actions can be expected
to achieve their objectives.
And that's exactly where things go wrong, that systems are pursuing objectives that
are not aligned with what humans want the future to be like, and then you're setting
up a chess match between humanity and a machine that's pursuing a misaligned objective.
So instead, we want to figure out a problem whose solution is such that we're happy for
AI systems to instantiate that solution.
And it's not imitating human behavior, which is what we're training LLMs to do.
That's actually the fundamental and basic error.
And that's essentially why we can't make LLMs safe, because we have trained them to not
be safe and trying to put sticking plasters on all the problems after the fact is never
going to work.
So instead, I think we have to build systems that are provably beneficial to humans.
And the way I'm thinking about that currently is that the system should act in the best
interests of humans, but be explicitly uncertain about what those best interests are.
This I'm just telling you in English, and it can be written in a formal framework called
an assistance game.
So what we do is we build assistance game solvers.
We don't build objective maximizers, which is what we have been doing up to now.
We build assistance game solvers.
This is a different kind of AI system, and we've only been able to build very simple
ones so far, so we have a long way to go.
But when you build those systems and look at the solutions, they exhibit the properties
that we want from AI systems.
They will defer to human beings, and in the extreme case, they will allow themselves to
be switched off.
In fact, they want to be switched off if we want to switch them off, because they want
to avoid doing whatever it is that is making us upset.
They don't know what it is because they're uncertain about our preferences, but they
want to avoid upsetting us, and so they are happy to be switched off.
In fact, this is a mathematical theorem.
They have a positive incentive to allow themselves to be switched off, and that incentive is
connected directly to number two, the uncertainty about human preferences.
So there's a long way to go, as I said, and we're not ready to say, okay, everyone in
all these companies, stop doing what you're doing and start building these things instead.
Probably is not going to go down too well, because we don't really know how to build
these things at scale and to deliver economic value, but in the long run, this is the right
way to build AI systems.
So in between, what should we do?
And this is a lot about what's going to be discussed tomorrow, and there's a lot.
So this is in a small font.
I apologize to those of you at the back.
There's a lot to put on this slide.
We need, first of all, cooperation on AI safety research.
It's got to stop being a cottage industry with a few little academic centers here and there.
It's also got to stop being what a cynic might describe as a kind of whitewashing
operation in companies, where they try to avoid the worst public relations disasters,
like the language model used a bad word or something like that.
But in fact, those efforts have not yielded any real safety whatsoever.
So there's a great deal of research to do on alignment, which is what I just described,
on containment.
How do you get systems that are restricted in their capabilities, that are not directly
connected to email and bank accounts and credit cards and social media and all those things?
And I think there are probably ways of building restricted capability systems
that are provably safe, because they are restricted to only operate
provably sound reasoning engines, for example.
But the bigger point is stop thinking about making AI safe.
Start thinking about making safe AI.
These are just two different mindsets.
The making AI safe says we build the AI and then we have a safety team whose job it is
to stop it from behaving badly.
That hasn't worked and it's never going to work.
We have got to have AI systems that are safe by design.
And without that, we are lost.
We also need, I think, some international regulatory level to coordinate the regulations
that are going to be in place across the various national regimes.
So we have to start parably with national regulation, and we can coordinate very easily.
For example, we could start coordinating tomorrow to agree on what would be a baseline for regulation.
I put a couple of other things there that went by too quickly.
So I actually want to go back.
OK, too far.
All right.
So the light blue line, transparent, explainable, analytical substrate is really important.
The moment we're building AI systems that are black boxes, we have no idea how they work.
We have no idea what they're going to do.
And we have no idea how to get them to behave themselves properly.
So my guess is that if we define regulations appropriately so that companies have to build
AI systems that they understand and predict and control successfully, those AI systems
are going to be based on a very different technology, not giant black box circuits
that are trained on vast quantities of data, but actually well understood component-based
systems that build on centuries of research in logic and probability, where we can actually
prove that these systems are going to behave in certain ways.
The second thing, the dark blue, secure PCC-based digital ecosystem, what is that?
So PCC is proof carrying code.
And what we need here is a way of preventing bad actors from deploying unsafe systems.
So it's one thing to say, here's how you build safe systems and everyone has to do that.
It's another thing to say, how do you stop people from deploying unsafe systems who don't want safe
AI systems, they want whatever they want.
This is probably even more difficult.
Policing software is, I think, impossible.
So the place where we do have control is at the hardware level because hardware,
first of all, to build your own hardware costs about $100 billion and tens of thousands of
highly trained engineers.
So it provides a control point that's very difficult for bad actors to get around.
And what the hardware should do is basically check the proof of a software object before it's run
and check that, in fact, this is a safe piece of software to run.
And proof carrying code is a technology that allows hardware to check proofs very efficiently.
But of course, the onus, then, is on the developer to provide a proof that this system is in fact safe.
And so that's a prerequisite for this approach.
OK, let me talk a little bit about regulations.
So a number of acts already in the works, for example, the European AI Act, has a hard
ban on the impersonation of human beings.
So you have a right to know if you're interacting with a machine or a human.
This, to me, is the easiest, the lowest hanging fruit that every jurisdiction in the world
could implement pretty much tomorrow if they so decided.
And I believe that this is how legislators wake up those long, unused muscles that have
lain dormant for decades while technology has just moved ahead, unregulated.
So this is the place to start.
But we also need some regulations on the design of AI systems specifically.
So a provably operable kill switch is a really important and very important thing.
If your system is misbehaving, there has to be a way to turn it off.
And this has to apply not just to the system that you made, but if it's an open source system,
any copy of that system.
And that means that the kill switch has got to be remotely operable and it's got to be
non-removable.
So that's a technological requirement on open source systems.
And in fact, if you want to be in the open source business, you're going to have to figure
this out.
You're actually going to subject yourself to more regulatory controls than people who
operate on close source.
And that's exactly as it should be.
Imagine if we had open source enriched uranium, right?
And the purveyor of enriched uranium was responsible for all the enriched uranium that
they pervade to anybody around the world.
They're going to have a higher regulatory burden because that's a blinking stupid thing
to do, right?
And so you would expect there to be a higher burden if you're going to do blinking stupid
things.
And then red lines.
This is probably the most important thing.
So we don't know how to define safety.
So I can't write a law saying your system has to be provably safe because it's very hard
to write a law that says that you're going to have a higher regulatory burden.
Write the dividing line between safe and unsafe.
Asimov's law, you can't harm human beings.
Well, what does harm mean?
That's very hard to define.
But we can scoop out very specific forms of harm that are absolutely unacceptable.
So self-replication of computer systems would absolutely be unacceptable.
That would be basically a harbinger of losing human control if the system can copy itself
onto other computers or break into other computer systems.
Absolutely systems should not be advising terrorists on building biological weapons
and so on.
So these red lines are things that any normal person would think, well, obviously
the software system should not be doing that.
And the developers are going to say, oh, well, this is really unfair because it's really hard
to make our systems not do this.
And their response is, well, tough, right?
Really?
You're spending hundreds of billions of pounds on this system and you can't stop it
from advising terrorists on building bio weapons?
Well, then you shouldn't be in business at all, right?
This is not hard and legislatures by implementing these red lines would put the onus on the developer
to understand how their own systems work and to be able to predict and control their behavior,
which is an absolute minimum we should ask from any industry, let alone one that could
have such a massive impact and is hoping for quadrillions of dollars in profits.
Thank you.
Thank you very much, Professor Russell.
Quick question, maybe before we move on to the next speaker.
There was some good news in there.
It is that we have ideas on how to make safe AI.
But how long do you think we're going to need?
How long is it going to take by default that we have these ideas worked out and how long
might it take if we had all the smart people in the world give up their current
focus and instead work on this?
I think these are really important questions because the political dynamic is going to
depend to some extent on how the AI safety community responds to this challenge.
Because if the AI safety community fails to make progress on any of this stuff,
the developers can point you to this.
The AI safety community is going to make progress on any of this stuff.
The developers can point and say, look, you guys are asking for stuff that isn't really possible
and we should be allowed to just do what we want.
But if you look at the nuclear industry, how does that work?
The regulator says to the nuclear plant operator, show me that your plant has a
mean time to failure of 10 million years or more.
And the operator has to give them a full analysis with
fault trees and probabilistic calculations and the regulator can push back and say,
I don't agree with that independence assumption.
These components come from the same manufacturer.
So not independent and come back with a better analysis and so on.
At the moment, there is nothing like that in the AI industry.
There is no logical connection between any of the evidence that people are providing
and the claim that the system is actually going to be safe.
That argument is just missing.
Now, the nuclear industry probably spends more than 90% of its R&D budget on safety.
One way you can tell, I got the statistic from one of my nuclear engineering colleagues,
that for the typical nuclear plant in the US, for every kilogram of nuclear plant,
there are seven kilograms of regulatory paperwork.
I kid you not.
So that tells you something about how much of an emphasis there has been on safety in that industry.
And also, why is there, to a first approximation, no nuclear industry today?
Is because of Chernobyl and because of a failure in safety, actually deliberately bypassing
safety measures that they knew were necessary in order to save money.
We'll take one question from the audience, provided it's a quick question.
I see a hand over there.
Let me dash down.
Hi. Thanks very much for your talks here.
My name is Charlie. I'm a senior at UCL.
One of the big reasons, I think, why there's so much regulation on nuclear power
is widespread public opinion and protests against nuclear power from within the environmental movement.
So I wondered whether you thought if there's a similar role for public pressure or protests
for AI as well.
Thanks.
I think that's a very important question.
My sense is, I'm not really a historian of the nuclear industry per se.
Obviously, nuclear physicists thought about safety from the beginning.
In fact, Leo Zillard was the one who invented the basic idea of the nuclear chain reaction.
And he instantly thought about a physical mechanism that could keep the reaction from going
supercritical and becoming a bomb.
So he thought about this negative feedback control system
with moderators that would somehow keep the reaction subcritical.
People in AI are not at that stage.
Or they just have their eyes on, we can generate energy.
And they're not even thinking, is that energy going to be in the form of a bomb or electricity?
They haven't got to that stage yet.
So we are very much at the preliminary stage.
I do worry that AI should not be politicized.
And at the moment, there's a precarious bipartisan agreement in the US and to some extent in Europe.
I worry about that breaking down in the UK.
I think it's really important that the political message be very straightforward.
You can be on the side of humans or you can be on the side of our AI overlords.
Which do you want to be on?
And so let's try to keep it a unified message around developing technology in a way that's safe
and beneficial for humans.
So we can raise awareness, but we shouldn't do it in a partisan way.
Yes, and I totally sympathize with the idea that people have a right to be very upset that
multi-billionaires are playing poker with the future of the human race.
It's entirely reasonable.
But what I worry is exactly that certain types of protest end up getting aligned
in a way that's unhealthy.
It sort of becomes anti-technology.
And we can look back at what happened with GM organisms, for example,
which most scientists think didn't go the way it should have.
And we lost benefits without gaining any safety.
Watch to think about there.
Thank you very much, Professor Stuart Russell.
We may give you the microphone again a bit later on,
but there's lots of other people we want to hear from now.
So the next speaker is Conor Lehe, who is the CEO of Conjecture.
Many of us got a shock with GPT-4 or 3.5.
My goodness, what's going on here?
Conor was ahead of the curve when he saw GPT-2 with all its warts and weaknesses.
He said, my gosh, this is going to change the world.
So he has been thinking about some of these issues probably for longer than the rest of us.
So let's hear from Conor.
What would you like to say?
Thank you so much.
So unfortunately, Professor Russell has stolen my favorite Alan Turing quote,
so you're going to be hearing that one again.
But I guess there couldn't be a more appropriate time.
Because many years ago, there lived a man named Alan Turing.
He was the godfather of computer science, a titan in his field, and a hero of World War II.
And was here at Bletchley Park that he did his most seminal work during the World War II
on cracking the codes that the Germans were using and as a very early step into the field
of computer science.
And Alan was ahead of his time in more than one.
In 1951, in Manchester, he gave a lecture entitled,
Intelligent Machinery, a Heretical Theory.
And in this lecture, he said, for it seems probable that once the machine thinking method
had started, it would not take long to outstrip our feeble powers.
There would be no question of the machine's dying, and it would be able to converse with
each other to sharpen their wits.
At some stage, therefore, we should have to expect the machines to take control.
And here we are, 72 years later, where it all began.
And a lot has changed since the days of Alan Turing.
Computers have improved in incredible rates.
I'm holding my hands right now.
A computer of such incredible power that it would be barely imaginable to Turing in his
contemporaries.
Barely one human lifetime hence.
And while computers have advanced a lot in many ways since the days of Turing,
I like to believe that there would be a lot he would recognize.
He would recognize the basic functions of computers, their memory, their instructions,
programming, code, ideas that go all the way back to his seminal work on Turing machines.
While he might not be familiar with the exact tooling, he would be familiar with the general
concepts around modern programming, where a programmer writes code, instructions,
the computer then executes.
But there is something that I'm not so sure he would so easily recognize.
And that is AI, or in particular, the neural networks that power them.
Now, we have all seen AI do truly amazing things over the last couple of years in particular,
solving all these problems that previously we barely knew how to approach.
And you might think when you look at all these AI systems running on your phone and on your
computer that this is software like any other written by very clever programmers
to do the useful and the marvelous things that they do.
And you would be wrong because AI is very different from normal software.
It is not written so much as it is grown.
So while in the traditional software, you'd have a programmer sit down and write out the
instruction. With AI, you take huge supercomputers and massive data sets,
and you use these supercomputers to grow a program on your data to solve your problem.
And this works really well for many issues.
It has improved our ability to solve many very useful tasks and do many things that we did
not know how to do before. And our ability to grow these AI's continues to improve and get
better and better. While at the same time though, our ability to understand our AI's has not.
Because these AI's are not like well written code that a human could read.
They're more like giant blobs of numbers. And we know if we execute them, they work,
but we have no idea why. And only quite recently have we discovered
that as we scale up these systems, as we build bigger computers, bigger AI systems,
something quite remarkable happens, they become more intelligent, more capable.
Now of course, there are many details that have to be gotten right.
There are many parameters you have to set correctly. You have to have enough data.
You have to make sure your computer is set up correctly. But fundamentally,
there's a stability in this prediction, sometimes also called the scaling laws,
in that as our systems become bigger, as our computers become more powerful,
the systems learn higher and higher order patterns, more and more complex skills, knowledge, abilities.
And as they become more powerful and more capable, they are also becoming even harder
to understand and to control. And this is why we are all here today. Back to where it all began,
we have now returned, Bletchley Park. Because as Ter Wingel really realized,
it is really quite simple. If we build machines that are more competent than us,
at manipulation, perception, politics, business, science, and everything else,
and we do not control them, then the future will belong to the machines, not to humans.
And the machines are unlikely to feel particularly sentimental about keeping us around for very long.
And so here we are, faced with an exponentially increasing more powerful AI by the day.
As we learned with COVID, there are exactly two times one can react to an exponential,
too early, or too late. If we wait until AI, if we wait until we see the self-improving,
powerful, general purpose systems, it will be too late, far, far too late.
And this is why I am so happy to see the UK government take leadership in the first of many
important steps towards the necessary international coordination to address this extinction level
threat that is facing us all. And the very first step, as so many academics, industry leaders,
and even governments have already taken, is to firmly acknowledge the reality of what we face,
the potential extinction of our species by AI. Private AI companies are scaling their AI systems
as we speak, and they will not ask for permission and they will not stop, unless we make them.
They are already lobbying our governments with ineffective policies, such as responsible scaling,
in attempt to prevent actually effective policy, like the oil CEOs of the past trying to lobby
against climate change regulation that would hurt their bottom lines. The good news is,
that it is not yet too late to stop this, to prevent the building of such deadly machines,
until we know how to build them safely. That is why there is nothing more important
than for people to know the truth. That a small group of unelected, unaccountable private companies
are running a deadly experiment on you, on your family, and on everyone on earth,
without your consent or even knowledge, despite they themselves admitting that these risks are real.
At this point, all of us agree that we are playing Russian roulette with the entire planet,
and we are only quibbling about how many poles are left until the bullet.
Now, in my personal opinion, if you ever find yourself playing Russian roulette,
I suggest you put down the gun. And so, we have to speak up and demand action.
If you want a future for us, our children, and our species, it is not yet too late,
but it will be soon. We stand at a historic moment, today, at where it all began,
Bletchley Park. Well, let's not waste it.
Thank you, Connor. Let's take a couple of questions from the audience.
Where am I seeing? If you take the microphone in there, thanks.
Very nice presentation, by the way. I wanted to ask, with the current situation that's going on
with AI currently, do you really think, if you were to be a philosopher maybe for five minutes,
do you really think that currently society is really ready for it? I mean, sure, we can adapt
and somewhat, but as things are dwelling around us, it doesn't seem like we're really anywhere
near to, I mean, accepting it. We're just such fair and all, and other factors coming from it.
Yes, so the simple answer is no. We are absolutely ready. We should be playing with
nuclear fire, or worse. Our civilization does not have the level of maturity to be able to
handle technology like this, and this is why I'm not extremely optimistic about the future.
The truth is, is that whether it's AI or something else, AI, technology is becoming more and more
powerful. This is just how it is. This is how the technology works, and our society has to adapt to
this. If we as a society do not find a way to, as an entire civilization, as an international
civilization, work together in a way that we can responsibly steward technology so powerful
that it can destroy anything, then humanity is on a timer. Whether it's AI or whatever comes
after that, we need to improve our society, or that's it. Sometimes people grow up in a hurry.
Sometimes people are a bit childish, and suddenly there's a big threat ahead, and my goodness,
we grow up. Is that what you see happening with humanity now? We're not ready for AI, but as we
understand the risks, we will change our mode of operation. I sure hope so, and if it happens,
it will not happen because something about it is because people, like the people in this room,
actually do something about it, stand up and make a difference in our institutions and our
society. There is no law of physics that forbids us from having a good future and taking control
of our future and building wonderful, safe technology for all, but there is also no law
that mandates it. I saw one more hand up. Yes, if you give the mic to the woman in the glasses.
Thank you. I was going to say, I know that one of your policies is that you want to cap compute,
and I'm just wondering whether you are going to suggest that at the summit and what you think
the government's response to that will be. Compute caps are absolutely the most sensible direct
policy for us as a species to follow. The main reason for this is that it is the bottleneck
towards building the actually existential, dangerous systems. Just explain what these
view caps are. Yes, so compute caps is basically limiting the maximum size of the supercomputers
I talked about. We limit the maximum size or AI as in our computers are allowed to be,
and so we can limit, hypothetically, how intelligent they will be. Things actually
get dangerous because we don't know what pops out of our experiments until we run them.
So it might already be that we're already too late. Our computers might already be big enough
to end the world. We don't know, but hopefully not. In that case, as I say with Russian roulette,
if you pull the trigger once and there was no bullet, the correct move is not to pull it again.
The first thing you do is don't pull it again until you know if there's a bullet and where it
is, and if you know there's one, definitely don't pull it. So this is my opinion on this.
I will definitely be open to talking and would like to suggest this to all policymakers of all
nations. I think there is extremely strong resistance to this for the obvious reason
that this cuts into the bottom lines of very powerful big tech companies who have extreme
lobbying power and control over governments. It's very simple. There's a lot of people who
gain a lot of benefits from continuing to pull that trigger, and we have to make them stop,
and they are going to fight us every step of the way. It's just how it works.
Thank you very much, Connolly.
I'd now like to invite the eight members of the panel to come up on stage,
and we're going to continue the conversation. Please self-organize on the seats.
Stuart, I don't think we've got a seat for you at this stage. We can either find another seat
for you or we'll let you come back on the stage later.
So sit down in whichever system you like, and we will hear from each of these panelists
what they think has been missing from the conversation so far. Maybe they've got an
alternative view. Maybe they don't think we're playing Russian roulette and a different metaphor
is appropriate. Maybe they would like to express what they think the politicians they're closest to
should be saying. Maybe they'd like to comment on some of the other issues of safety.
So shall we start at the far end there, and let's just move along the panel. I'll give you two
minutes each to contribute what you'd like into this conversation, and then we'll hear from the
audience. So let me introduce you as well. Sure, I can do that. My name is Mark Brackle. I'm the
Director of Policy at the Future of Life Institute, and I truly support what Stuart and Connolly have
been saying. I think when we looked at the summit about six, seven weeks ago, we put out a set of
recommendations ahead of time, and I think there were three traps that we identified that we were
worried the summit would fall into. The summit potentially not addressing the full range of
risks all the way from bias and discrimination up to extinction. It not being inclusive,
namely China not being invited, and it being a setting where the big CEOs would sort of run the
show and there would be maybe some token academics at a panel in a room the night before. So I think
if we sort of assess what the summit is looking like now, the night before the actual event,
I think we can be reasonably happy. I saw this morning that China will in fact be invited,
and that it will be an inclusive summit in that nations of the world will get a seat at the table.
So I think that's progress, and that's very good. If we think about the harms and the
range of harms that are being discussed, one of the things FLI recommended for the summit was
grounding it in examples of large-scale AI harm that we've already seen, such as the Australian
RoboDec scandal or the Dutch Benefit scandal from the Netherlands myself, and it's a political
scandal really dominating the national scene to show that very simple algorithms can already
have a very large impact in countries that were rushing towards adoption and to show the beginning
of the trend line, and that hasn't happened. I think that's potentially a missed opportunity,
but I think it's really good where the UK has overall focused the summit. And I think I'm sort
of least optimistic when it comes to the role of companies at the summit, and I think Conor's
done a great job at highlighting concerns that FLI that many of us have around responsible scaling
and this narrative being pushed by many companies as an excuse to keep going,
rather than making sure that whatever they put out onto the market is actually safe.
And I think that's a message that I hope we collectively and the people in this room that
are going to the summit can still take to the participants and to the governments that are
there, making sure that we put the onus of what is safe and what isn't safe on the companies.
They need to prove to us that what they're putting on the market is safe, rather than the other way
around, where the default is they keep on scaling and it's up to the regulator to prove that what
is safe. So that I think is a key message to take. So you're giving at least two cheers,
if not three cheers to the organisers for what you see happening already. So from one Dutchman
to another, Ron Ruzendal is the Deputy Director General of the Netherlands Ministry for the
Interior, lots of other roles. What would you like to add to the conversation? Well, thank you
very much and glad to be here. First point is that we regulate cars and we regulate pharmaceuticals
and we do so to mitigate risks of today and the risks of tomorrow. So we have to act upon risks
of today like bias and risks of tomorrow and those are global risks. So we welcome the
initiative of the summit, but we also welcome the initiative of the Tech Envoy of the UN
starting a high-level advisory board and we have offered the Tech Envoy to host
the European meeting of the high-level advisory board in The Hague, for example,
in the Peace Palace, because we support the work that we all do internationally to
mitigate the risks. Secondly, we need some form of early warning. Whatever the risks are
and whatever, whether they will occur or not, we have to have early warning and a rapid response
mechanism on whatever will happen in the future and therefore we need to operate in a failure-driven
way and we need to participate, we need to coordinate, but we also need to cooperate
with industry, with civil society, with citizens, with industry and with governments.
Agreement on early warning seems like something that both sides of the debate should be able to
give because the people who think things will go wrong and the people who think things won't go
wrong should be able to agree well if this happens we should all be paying more attention.
Let's pass the microphone on to Hal Hodgson, who's a journalist at The Economist who has written
a lot about existential risks and AI. Hal. Thanks, David. Yes, my name is Hal Hodgson. I'm a
special projects writer with The Economist. I've been writing about AI for 10 years. I have a degree
in astrophysics and that meant that I spent a lot of time looking at a thing called the archive
long before it was cool and papers from Facebook and Google would just turn up on the archive with no
PR whatsoever and this is journalistic gold and that's how I got into it.
I guess my sort of view is inherently going to be journalistic. I think it is a very difficult
point to make very clear decisions about what anybody ought to do about any of this. I think
there's, from my perspective, there's a huge amount of uncertainty. I've now been writing
about it long enough to know that there's also a lot of hype and it's not the first time there's
been a huge amount of hype and I think making very clear decisions about important systems
at a time that is hype-filled is a difficult thing to do. I think the thing that I can agree on,
the consensus that I can come to with probably most of the people in the room and the organisers
at the summit, is that there's a huge amount of science to do and both in terms of existential
risks and these sort of lower tier algorithmic risks, I think there's two examples that show us
that this is a perfectly plausible thing to do. The first is that there was a time in the 90s
when everybody was very worried about impact of bodies in the solar system to earth, existential
risks from asteroids and things like this and congress mandated a large amount of money to go
to NASA to map all of the asteroids in the solar system and to figure out ways to nudge them off
course if they come towards us and if you look at the risks as they were assessed in the 90s
and the risks as they are assessed today, they are massively, massively dramatically lower
and so that to me is a very strong case for doing science on these risks.
I don't know and I'd be fascinated to talk to people who do know what doing science on AI systems
really looks like but it brings me to the next comparison which is Facebook. About five years
ago there was also a big panic that Facebook was determining the results of elections or
hacking democracy essentially. That is somewhat subsided now but one of the most sensible responses
to that concern that I saw was also that you need to do science on Facebook just like you
needed to do science on the solar system. You need to start measuring things and it actually took
years to force Facebook to give access to data to people like social science one.
It eventually sort of worked and I think there's a reason that you don't hear a huge amount about
it. It's because the science that's been done so far has not determined that Facebook destroyed
democracy. We still have at least a version of it and so I guess I would end just by a plea to
and perhaps in the same way you were saying Stuart, politically neutral science to the extent
that that's possible. A more of a goal than a thing that exists. Do you know whether the
US focused on asteroid risk in the 90s? Was that bipartisan or was that a partisan issue?
I don't know if it was bipartisan but it went through Congress so it must have been a bit bipartisan.
So maybe it wasn't as bad back then actually now that I think of it. There's some encouraging
examples there. Next we're going to hear from Annika Brack who's the CEO of the International
Centre for Future Generations. Tell us about your views Annika. Yes, thank you very much and first
of all I'd like to commend the UK on two things. First of all there's sense of humor for setting up
summit on the darkest corners of AI on the night of Halloween or the nights after and I'm surprised
nobody made that joke so far. And secondly for really bringing this to the attention of leaders,
media and the public actually. I don't think Frontier AI has ever been discussed so much and
the number of communicates, the number of the executive order, the communications leaders,
European leaders meeting ahead of the summit, negotiating late night to get to bring something
here is already a measure of success and we could actually leave it here with this stellar panel
and say I think it's very important that the civil society is meeting here. This is maybe the element
that is missing in the room. I'd like to say we have two major challenges here. One is a coordination
challenge. We have corporates looking at the topic, they're racing over competitive edge
and we have governments who have serious geo-strategic interest and when those two come
together that doesn't help collaboration so we have to think about how we get people around the
table and secondly there's a democratic challenge. Democracy is by its very nature a slow and patient
regulator and that's important. I will argue that actually democracy is perfectly adapted to
the society through these uncharted waters that we're experiencing at the moment but we need to
make sure that the sailors of this big ship are prepared, that they are well informed and that
they have the tools to deal with this change and that's what the International Center for Future
Generations set out to do in Brussels. That's why we moved our headquarters to Brussels to make sure
that EU decision makers are well prepared because we set our best hope in the EU in this
international race for governance. I will leave it here for now. Are the EU decision makers paying
attention to what you say? I think they do. You do hear already a lot of signs that they have also
recognized that we have to look at advanced artificial intelligence, that
regulation doesn't stop with the Artificial Intelligence Act. It's only the very start
of the beginning or the first piece of the puzzle to come back to Stuart's presentation.
Thanks. Next we're going to hear from Jan Tallinn who is the co-founder of Skype,
FLICISA, that's the Center for the Study of Existential Risks and he is also one of the
advisors on the committee created by the tech envoy for the UN. So Jan, what would you like to
say based on what you've heard so far? Thank you very much. Sometimes people ask me because I've
been in this kind of existential risk and AI safety community and effort for more than a decade now,
sometimes people ask like so how's it going and my standard answer is well it's great progress
against an unknown deadline and indeed it's kind of special this year it's just like a
plethora of things to point to as great progress and obviously the most obvious one to point
to at this point is the summit that starts tomorrow. I do think it's UK deserves a great credit
for pulling this together and I really wish best of luck to the organizers of this
and the prime minister as well and the team. Now when it comes to this like unknown deadline
recently I've kind of pivoted our way to some degree from basically funding research
towards just buying us more time which is kind of has to be has to deal with something
like less research aside and more kind of an action side more on the policy side
so I do think it's kind of valuable now to really think through the policy that would
make the future kind of a little bit less on and the deadlines a little bit less unknown
another and final thing I wanted to say that there's I want to kind of caution against
if you're sailing to like uncharted waters there's like a temptation to
use something familiar and say that oh like the future is going to be just like this
like the most common one is that oh AI is just the technology it's just going to be
just another like electricity or something like that.
When we're talking about risks the way the model risks is by the you know reference
class that you cannot rule out so as long as there is like reference classes like
viruses self-replicating things or another species as long as you kind of rule them out
you have to like prepare that this might be an instance of such thing so I think it's important
to not make dismiss AI it's always just another technology or like as one prominent VC recently
said always just much of math. Thanks Jan next we're going to hear from Max Tegmark
he might describe what he's got on his chest I happen to know he has released a very interesting
TED Talk which I strongly recommend all of you watch and Max might give an abbreviated form
of that TED Talk now or whatever else you'd like to put in the conversation. Thank you thank you
yeah so I'm Max Tegmark I've been doing AI research at MIT as a professor there
for many years focusing on safety related stuff I'm also the president of the Future of Life
Institute and I'm a huge fan of this guy who you guys have the wisdom to put on your 50-quid
note Alan Turing who's come up many times and it's really remarkable that the argument he made
72 years ago that when machines greatly outsmart us by default they're going to take control
that that argument has not been convincingly refuted in the 72 years since he said it so
I think we have to take it very seriously and people who think of AI as just a new technology
like steam engines or electricity tend to not take it so seriously Alan Turing himself clearly
thought about AI more as a new species and with that framing it's very natural that we
would lose control to them just like the Neanderthals lost control to us etc so so what are we going
to do about about this great TED first of all having conversations like here and what happens
tomorrow is great so huge thank you to the British government for really putting this on and for
standing up to all the lobbying pressure from companies who wanted to water it down into just
talking into just a big blessing of responsible scaling or whatever thanks also to the US government
for standing up to also the weird pressures to turn this into a geopolitical pissing contest
by excluding China I'm really proud of the Brits for recognizing that this is a global challenge
and what do we actually do about it well I think there's a remarkable consensus actually
emerging from all the civil society and academic groups that don't directly profit the way companies
do about what we should do about this we put out maybe uh Andrew create your Richard Mulligan
hold up in the air with this thing you can if you go to future you'll find the alternative to the
possible scaling policy called the safety standards policy where the idea is as we heard
from Stuart Russell you should simply shift the responsibility to companies to prove that things
are safe instead of as responsible scaling policy you have the responsibility on the
government regulators to prove that things are unsafe more or less in order to stop them
and there's there's a whole set of very concrete ideas out there for what the safety
standards should be to start with and some of them are were mentioned very eloquently by Stuart
you can insist on quantitative safety bounds or provable safety beginning with uncontroversial
stuff that you should not be able to demonstrate that nobody can hack the servers that these super
large systems are on that you that that they won't advise on how to make bio weapons etc and
this will very naturally accomplish something quite wonderful where we sort of have the cake
and eat it as a species because most people I talked to don't realize that there are two almost
there's two very different kinds of AI that they keep conflating there is the AI that has
current commercial value for curing cancer making self-worth better safer cars and
all sorts of wonderful things which have very little risk associated with them but some which
we need to address but 99 percent of the things that most people are excited about do not require
playing Russian roulette with AGI and super intelligence and then there is this lunatic
friend just try to build the machines that outsmart humans in all ways
where almost all the risk is coming for very little benefit so if we can put safety standards
in place we can I think quickly get into a situation where we have a long future with these
wonderful benefits that are quite safe to get from AI and then just take your time with with
the really risky stuff maybe one day humanity will or will not want to build more powerful
machines but only when we can figure out you know how to control them so that would end
with just a bit of wisdom from ancient Greece if I may so raise your hand if you remember the
story of Icarus don't get hubris right you know so artificial intelligence is giving humanity
these incredible intellectual wings with which we can accomplish things beyond their wildest dreams
if we stop obsessively trying to fly into the sun thank you so you're not saying pause AI you're
saying let's keep using AI but you're saying pause the rush to AGI let's not pause AI in fact let's
continue almost everything that people are excited about doing but pause this compulsive obsession
of training ever more ginormous models that we just don't understand
thanks Andrea Miotti is the head of policy and governance for AI at conjecture are you in agreement
with what you've heard or you have different things to emphasize absolutely I'm very much in
agreement with both the speakers and many other members of the panel I think there are two big
positives from the summit to highlight one is that we're also echoed by the panel one is
it's role in building common knowledge making it clear and explicit at the highest levels of
government that this is a big risk that this is a extension level threat that we face as a species
and number two coordination not getting lost in a geopolitical pissing contest as
max has said or in other of these things and realizing this is a again a threat we all face
together it's a global security problem it's not a national security problem or at least it's
only a national security problem and to solve these problems we need coordination even during
the heights of the Cold War there were open lines between the US and the Union to deal
so closing the door on cooperation before it has been tried is a surefire way
for all of us to lose and so I was very very pleased to see that the UK government
prime minister Richard Snack have already acknowledged the risks very explicitly in
the prime minister's speech last week are setting up this summit are inviting a diverse group of
countries to discuss this risk together the part where I think we can go further and we can do
better is in the measures I share the concern of some of the other panelists on a focus of simply
enabling the default to continue and the reality is that the default is bad the default is bad we
by now all understand it is bad and we all understand we need something else
even the companies racing towards its default admit that it's bad admit that it's a one in four one
in ten unacceptably high chance for all of us to be wiped out and so the concrete measures
they will need to take cannot look like continuing on the default path cannot look like
systems are safe until proven dangerous by external auditors that are strapped for resources and
they don't even have the tools or the tests through these tests they look like probably safe
systems they look like burden of proof on developers developing systems that they admit
could wipe everyone out to demonstrate ahead of time of running critical experiments that
they are safe if they cannot do that that's fine they can just build something else
or they can move to the different sector that's the standard we utilize in all high risk sectors
there is no reason to not utilize it in a sector where the risks are the literal extinction of
humanity thank you and last but not least we have a trained economist alizandra musavi sade
who is the CEO of evident what would you like to add to the conversation alizandra
it's a hard it's a it's a great panel to follow it so it's um I take it really I have a different
time horizon i'm a alexandra musavi sade i'm the founder and CEO of evident and we actually do a lot
of measurement uh we specialize in benchmarking businesses on the option of AI so what I focus
on is very near term so looking at the here and now and the race is on at that level as well
so the race is on by all businesses in all sectors to take the capabilities that AI offers today
and to implement it as fast as possible and really not thinking about any of the risks
so thinking about growing market share um upping revenue cutting costs and all of that
and continuing the um sort of digital transformation which is now more and more an AI transformation
and so with that um we we we are observing this AI race at a business level and one of the things
that we see that some businesses that are highly regulated really think about um how they can
implement the oversight and implementation of safe AI so while very impressed with what the
UK government is doing and I think the right thing is to focus on the long term because
that is where we um should have our eyes at um at the stage but there's also a near term
risk and I think um if there was one thing I would suggest is that as much as we need to focus on
the long term we also need to look at the here and now and that businesses are barreling ahead
with AI adoption without any particular guardrails in that and so while we need to put the um the
burden on the development of safe AI we could also maybe in the meantime put the burden on
the businesses that are using AI to prove that they're doing it in a safe uh and constructive way
thanks so you've heard from all the panelists I'm sure there's lots and lots of questions
in your mind so I'm going to come to the audience and take maybe three or four questions and then
let the panelists pick what they want and my question to be would be do you agree with this
division between near term and far and far future some people say that the risks from
substantial risk should not be considered to be far long term they are potentially here and now
but maybe you have a different way of framing it let's see some hands let's take uh one bank
over there in the far corner it's a bit some running around if you can say who you are if
you want to remain anonymous that's fine too hi um I'm Matthew Kilcoyne I looked into how
the banking industry turns short term into long term by sort of senior management risk
and associated penalties and clawbacks to force the change okay question on learning from the
banking industry so let me give the microphone down here just a second thanks a lot everyone
Yolanda Lankas from the Future Society what do we do about open source AI Professor Russell
mentioned one idea which was kill switches I think this is in terms of policy approaches
such an important question for us to all grapple and again as Jan was saying oh technology people
assume that paradigms continue open source has been valuable for software but with AI we're seeing
new risks and paradigms and how could maybe academia and others
my name is Oliver Graves um my question is what do you think the biggest hurdles are
towards getting the general public to recognize this as an existential risk and to take that risk
seriously because it still seems to me like it's all well and good everyone here at the summit and
in this room being aware of that risk but it doesn't seem to me like with anywhere close to a
level of majority the general public grappling with it properly
thank you so I'm I'm Father Peter Fignanski I'm I'm engaged in looking at how the Catholic
church can respond to existential risks so it's a slightly different question here just in the most
general way what does it look like from your side of the table for religious groups to play their
parts in achieving existential security thank you I'm Oliver Chamberlain I'm a student studying
masters in science in AI one of my concerns is although like the regulation is going to involve
limiting like supply chains making sure that GPUs aren't going off to places that we don't know about
how do we stop the advancement of algorithms which allows older systems to be more powerful
so like alpha tensor I wonder in my mind like the only way around something like this
is a future which is like super draconian
how do we prevent GPUs that already accessible already out there from being used in ways which
are way more powerful
question on open source to what extent is it possible to control open source a question on
what are the biggest hurdles changing their minds in the public or indeed one of the other big hurdles
question from the point of view of what might religious organizations contribute to this
conversation and do we need to have super draconian surveillance and policing systems if we're
going to stop these GPUs and algorithms they're potentially doing things that we didn't want
them to so max hand up first religious organizations I hope can remind us all of the importance of
not play god and get hubris remember the moral angle I'm only going to comment on the timeline
one even though I have opinions about all the others it's not so I don't talk too much the timeline
one from Alan Turing's perspective when he said this he said that when we eventually basically
passed the Turing test he expected to go very fast so then it was a long-term risk now according
to Yosha Ben-Gio GPT four passes a Turing test so he would probably if he were still with us in
the room predict short timeline it's quite remarkable what's happened on on the prediction
market metaculous.com for those of you who are nerdy enough to go there the where the timeline
how many years we have left to artificial general intelligence outsmarting us has plummeted from
20 years away to three years away just in the last 18 months as a direct result of
of this recent tech progress and Dario Amode has openly said one of the tech CEOs here that
that he thinks you have two or three years left and others other tech CEOs told me that individually
so I think we just have to stop calling this artificial general intelligence risk long term
or people are gonna laugh at us and call us dinosaurs stuck in 2021
Andrea I'd like to answer the question about the public actually the
seems to really understand I recently ran polling as part of a campaign I'm running called control
AI and the British public is extremely concerned about disempowerment and extinction risk from AI
they seem to be aware of it they seem to be aware of it a whopping 60 percent
global ban on smart and human AI period with only I believe 14 percent against and like quite a few
undersized nearly I believe almost nearly 90 percent would be very very happy with a full
ban on deep fakes right now people understand very very well that full impersonation revenge
pornography and like use of their likeness against their will is not good is destabilizing
is a threat that exists right now with systems over here right now and they don't want it
and similarly there is I believe 78 percent of the public it would want an international watchdog
with real teeth more like an IAA and there are basically across the board like I was personally
surprised to see all of the answers come up with such overwhelming support we might ask whether that
mood is shallow that it might be adjusted we might ask whether that mood is shallow that it
might be adjusted again in the future let's hear from Annika and then from how and then from Jana
yeah I just wanted to say that we should stay away from predictions with regard to timelines with
regard to sectors how they're going to be affected I think if we have learned one thing that it's
really difficult to assess that but we do know that there will be dislocations there will be
impacts and as they grow and as we see them more the public will be more informed and more aware
I think it's really our role here as civil society academia religious leaders to increase that
awareness and to also keep that conversation going as we go when you drive a car you don't
just look ahead right you also look in the rear view mirror you look in the side mirrors for
signals of change but what I want to say when you look back that's something we're not doing
enough we're trying to predict in the future but we should also look back and look at you know
stuff we've put out there regulation we're putting out there and how it's actually being enforced
if it's effective if it's yeah effective regulation we discussed about it before and this
will be key going forward how I'll take the question oh yeah on yeah I'll take the question on open
source and draconianism because I think they're related I don't really see a good way of regulating
controlling open source code that is not deeply draconian that does not involve a massive expansion
of surveillance if you need to know what code is running on what chips you have to have access
to the computer in which those chips are running and for a sense of how well this is going to go
look at america's attempts to put export controls on chinese ai development it it works to an extent
but it only works if you pick these very narrow bottlenecks and I guess in terms of existential
risk it depends on how powerful sort of lower tier open source models end up being I don't really
have a good answer to that question I just say one more thing on mataculis just as a little hint of
not relying on it too much if you just if you remember the lk 99 superconductivity thing over
the summer mataculis at one point was completely certain that that was real and uh the mataculis
thought it was real for a while and and then it dived again so just you can't rely on mataculis
we're gonna do some real time checking on this right so once that's going on jan so I also wanted
to say a few words about open source I think it's as I mentioned earlier I think it's important to
just like not do this kind of categorical thinking that you have like some one particular
you know look at that too that you put things in and then like you reason about this pocket
my friend Andrew Gritch who is in the audience like he he observes that's like when people
say that they're really going to prove open source it's valuable to try to understand what they
actually want what is what is the thing that they're trying to protect and quite often it's
just like they don't want this like massive centralization and power in the hands of people
that they don't trust now the question is like if you think about open source as like irreversible
deployment of things that we potentially don't want to irreversible deploy other other ways
to protect what the open source allocates want and for example there is like there is like in
blockchain community there is quite a lot of advancement in cryptographic techniques techniques
like zero noise proofs perhaps there are enough like ways how we can kind of eat our cake and keep it
keep it too by instead of having nosy people literally looking around in a computer you just
computer automatically producing things like zero noise proofs that you haven't been up to no good
things like that so there's lots of possibilities to explore the other Sandra I'm actually curious
about what Max and how we're talking about AI secrets did they know or did they not know
prediction is very hard prediction is hard canary signals are more important in my view let's
agree the canary signals if I can just add something here we when we ask if something is a near
term or long-term risk we have to remember we're not asking if we know for sure that we're going
to get super intelligent soon if we think there's a three a 10 percent chance that something like
this might happen in four years but then it's still in the risk is near term even though I hope as
much as anyone that it actually won't happen for a long time so I'm the Sandra and then Ron
and then Mark yeah I just wanted to respond to the question on the banks if I understood it correctly
is like was that maybe a model of regulation for AI is that what you meant okay I mean as we cover
that sector very deeply it is it is an area where I mean and it's it's um it's a sector that is
heavily regulated and because it heavily regulated the way that they are developing and deploying
and implementing AI today is that even as they develop there's a lot of oversight in the models
themselves and then they have they go through first and second and third lines of defense where there
is oversight again and then they submit to the regulators and in a way I mean I can't believe
I'm saying this but in a way the banks could be a model with which if you are to impose a oversight
at a company level for the AI that they're using the banking model is not a bad one because the
way that they assess the risks as they go from development to deployment into production and
output so that could be a blueprint or something to to look at for businesses themselves to regulate
themselves is that if that's where we end up so there might be something to learn but bearing in
mind AI is different from everything that's ever been before Ron what would you like to add and
well I'd like to come back to the discussion about open source versus closed source and I'm a bit
surprised that here at the table there is a strong belief in closed lots of companies
that might contain lots of zero days we don't know of instead of trusting civil society
and and on regulation we therefore not only need to regulate development we also need to regulate
use and that's what the AI does for example sounds like none of the old traditional models
are going to work sounds like we need something that has a variety of different approaches
we have to transcend some of the previous systems and Mark and then we'll come back to
pick up a few more comments yeah I just wanted to pick up on sort of the question of near-term or
existing harms that we see and sort of harms in future because I think what you saw yesterday
with President Biden coming out with an executive order in the United States as well as with EU AI
Act which has been a long time in the making you see sort of an attempt and I think a successful
attempt by policymakers to tackle both AI and bias that you see in current day applications
and some of the risks that are like that are AGI related and I think that shows that it's
perfectly possible to do both and I know a lot of people in this audience are potentially driven
by existential risk I mean that's why we come to an existential risk observatory event panel
but I think there is a lot of alliances and bridges that can be built across that space
and I think it's often not helpful to look at both of these things maybe just on banking
I mean we've seen the 2008 financial crisis where I think lots of people were justifiably
angry because CEOs got away with whatever they were wanting to do and governments build out the
banks there was briefly a lot of regulation that was then rolled back over the past few years and
again we saw a few small banks collapse in California so I think we need to learn some
lessons there around liability and making sure that as we build a liability regime for AI companies
CEOs are also individually and criminally liable if they are indeed negligent or if there are
sort of safety risks that they're they're ignoring so maybe just to add on those two points so half
the panel have got their hands up wanting to speak well I'm going to ignore them briefly and give
the microphone very quickly to three people in the audience but you have to be quick because
we're out of time already thanks Richard Barker and Ron introduced the analogy of the pharmaceutical
industry which is not perfect by any means but I spent most of my career in it so I think there's
still a few lessons from that right the first is you regulate not the underlying technology but
the application of the technology so it turns out for Lidemite is the terrible thing to give to
pregnant women but it actually cures people with multiple myelomas so you I can't imagine
how we're going to actually deeply regulate the the internal workings it's it's how they're used
and it may not be existential risk that is most relevant to actually harnessing public opinion
it will be some of the things that's already happening that affect them personally they're
not just experts but panels come back to legislators and say this is what I saw and
this is what I like and don't like
Terry Rabie former risk manager guys you really need some pushback
I was deeply appalled at Stuart's example of the nuclear industry the regulation of the nuclear
industry in the United States essentially is anti-human it's prevented the gifts of energy
that's not polluting by regulation we have another anti-human example of regulation in the EU
which is a regulation of biology destructive of the advantages that we could get from genetic
regulation of biology destructive of the advantages that we could get from genetically modified
organisms so look it won't do the needs to be a little bit more pushback to you guys so you get
your story straight it was a hinder here so earlier thank you don't show me those key
substances professor Russell make a very important distinction about as AI to be safe for humans
and AI safe for use as a tool the first one is a new type of intelligence the second one
is a just used as a tool so that's where the regulation comes in context we can regulate AI
as safe to use and we must have a control over development so it doesn't become an existential
risk my question to the panel is the following one is it possible or shouldn't be possible
in order to avoid open sourcing problems to develop just one super intelligence program
that will beat any small guys developments and in that way make us safer and the second question is
what will follow this summit deliverable which thanks plenty to talk about there learning from
the pharmaceutical industry regulating apps not platforms we had Terry pushing back quite hard saying
goodness look at the mess of regulation in the nuclear industry and in GMOs we had Tony asking
about a unified approach with one research and development program and also what's going to
happen next so 30 seconds each max's hand up again I have a good friend in the american
nuclear industry who told me that what really killed it wasn't regulation but it was Fukushima
and then three mile island I for open source 20 seconds on that I actually think I love open
source almost as much as you on the moon MIT is the cradle kind of open source but obviously we
don't open source plutonium in this uranium and similarly here we should get away from this childish
debate about whether your open source nothing or everything and just ask where the line goes
finally there's a technical solution I think to this which is not creepy but still works
which Steve Omohondo and I wrote a paper about where where you actually have control of your own
hardware chips you own it no the government doesn't see what you run but it's just not going to run
certain kinds of really creepy code because the hardware itself won't don't it's like a virus
checker in reverse where if your code can't prove that it's not making bio weapons it just won't
run so you can find out more about that proposal if you watch max's TED talk how
all I was just going to ask doesn't that make it a backdoor in kind of the same way
at CSAM detection on iMessage it makes it a backdoor no it's completely decentralized no one
has access to your chip it's just if you want a chip that'll run the harmful code you have to make
your own chip we need standards for hardware which is what Stuart was saying who's who's
going to jump in next Andrea just a quick reply to the IPCC model I very much hope
that we will not have an international agency modeled after the IPCC here we are in crunch time
and we have now knowledge of the risks and we have common knowledge about the risks
the role the IPCC was a great organization to build over decades essentially expertise and
information to governments to deliberate on how to act we do not have decades and we know what
the problems are governments are already acknowledging what the problems are we need action not a yearly
report Alessandra I would I would agree with that I think we're at a point where and also I don't
see how it's practically gonna gonna work I mean are the Chinese in the US gonna open the kimono
and submit to a UK body that wants it to and no accountability and no repercussions if they don't
so I really don't see how that would work but there's so much to say and so much to respond to
on this but I think gentlemen in the in the red jumper they're very much agree with the the fact
that at least until something has taken place on the regulation and we've agreed to what that
might look like for the near term big risks but for the here and now make the companies make the
sectors accountable for how they use it make the buck stop there first and then we can figure out
or in parallel figure out how to regulate it's sort of the bigger bigger questions and the
things that are giving us pause but I think the world will submit to a body that's run by the
UK but the world might cooperate with a body that the UK helps to inspire and get off the board
Mark your hand was up yeah I also just wanted to pick on pick up on the question the gentleman
in the red jumper raised on sort of parallels with the pharmaceutical industry uh yeah you're
just sort of shining beacon here in the audience um I mean I think on the one hand like the time
where everyone just could produce whatever potion they wanted to and put it out on the market that
has disappeared and I think thankfully disappeared and I think that there is lessons we can learn in
that in terms of potentially licensing or making sure that you guarantee that something's safe
when it comes to regulating the application of AI I think I'm significantly more skeptical
this has been for example in the senate hearing this was what uh Christina Montgomery from IBM
was pushing quite heavily we see on both sides of the Atlantic big tech really pushing for
application based regulation because that often means that the underlying big systems that they
are building won't be regulated right because how do you regulate gpt for if you only regulate
applications and it's only the hospital that then integrates it into a chatbot for patient contact
that actually has to deal with the regulatory burden so I think you do need to force these big
tech companies to do risk identification and mitigation even if they can't particularly
specify oh it goes into that application or this other one so I think you need a bit of a combination
of both. Anik are you a fan of the IPC model IPCC or would you prefer the ICFG model ICFG model?
The IEA I think there are a couple of models proposed I think what's important is to not
put it in one hand not in the hand of a few corporates but also not in the hands of one state
so the current race is not healthy and we need to think about how we get them back to the table
I know you have some ideas about this and yeah we as ICFG are trying to show and build the scenarios
to explain what it means to look at a future where emerging tech is governed and what it means when
you look at a future where emerging tech is not governed and that this will hopefully help decision
makers come together and work together. Because there are many other emerging technologies that
might disrupt our society in many other ways too just around the corner. Absolutely so we had this
discussion shortly before the panel because AI is of course a turbo charger for a number of
technologies that are being developed at a fast pace so we are also looking at neural technology
and quantum and biotech and I mean a lot of here in the room are looking at different technologies
but the power come from the combination of those and they're also around the corner.
Ron final remarks. Yes it works first I agree that we should both look into both the models
and the applications not one of them but what I think is that we yes we need science but we do
not have decades so we need some sort of form of a rapid response mechanism and in that we need
a credible helix we need both government civil society we need science and we need all of them
at the table. Thanks closing words Jan. Okay one thing I would say about the regulation issue
I think friends Lee Moshevitz like he has this concept of dial-up progress that a lot of conversations
end up in like do we need more progress or less progress which is kind of like way too black and
white looking way of looking at things what you actually want to do is like look look like there
are different ways where we want more progress and different places where we want less progress so
it's actually completely consistent to believe as I believe that yes we have over-regulated a lot of
things in a way that is kind of detrimental for us but that doesn't mean that we really should
stop regulating things new things as they come up. Thanks so please stay on the stage for a moment
we're going to have a few closing words from Otto who is the head of ERO which is part of the
organization that has made this happen. Otto are you here? Yes and by the way this discussion
is a prelude to an even more important discussion which is going to be taking place in the pub
in the good old British tradition afterwards some of you might want to join us where we can get
around to all of you who had high hand ups and I unfortunately couldn't take your question. Otto?
Thank you David.
Sorry. Yeah thanks and thank you all of so so much for being present here today as some of us
has already mentioned we're here in Wilton Hall this was built in 1943 as an assembly hall for
the world for two code breakers and while deciphering they have progressed beyond imagination
and borrow multiple exponential curves here hardware data quantity algorithm capabilities
are all growing with tens of percentage points per year so I think we all or at least a lot in this
room will suspect where this leads which is a eye that has the capability to do mental tasks much
better than we can and of course this presents amazing opportunities but according to most
existential risk experts we also risk nothing short of human extinction here and it does mean that
our species is on the line so when I turned on the radio last weekend the BBC was discussing
human extinction by AI and I think that this was dramatic but also hopeful at the same time
so I thought this was dramatic since human extinction caused by our own actions is now
officially a possibility and it never ceases to amaze me that we have been stupid enough to
let it get this far but hearing this discussed on national radio for me was also extremely hopeful
because up until now attempts to reduce the real human extinction risks were minor and world leaders
were not paying attention and with the summits that's starting tomorrow I think this is really
changing so I think it's hopeful that after the UK's Prime Minister speech on AI last week in
which he explicitly warned of human extinction risks the questions that followed from the press
were no longer about Prime Minister is this a real concern shouldn't you be concerned about the
bills of the of your people instead of this but instead at least some of the questions were about
are you addressing this problem seriously enough and shouldn't we consider instead pausing AI
a moratorium or are you doing the right thing with backing responsible scaling
and I think this is exactly the debate that we need so I think it's now important that we continue
in this direction so we must organize AI safety summits much more often we must open them up so
everybody gets to say we must have societal debates about this and I think in general we must come
together to coordinate and if we do that we are confident at the extension risk observatory that
we can implement the measures that are needed and this is why we have organized this event
and I think it's a huge privilege that we're able to do this together with conjecture so thank you
so much for co-organizing this event and we also want to continue organizing events like this one
but it's impossible without the support of all of you so if you want to support us doing this
there was a flyer that you got handed at the beginning please scan the QR code there and
there's possibilities to support us could be with funding could be with volunteering could be with
just following and sharing our content so this is enormously appreciated and with that can I
please get some applause for all our amazing speakers professor Stuart Russell Connolly
professor Max Teckmark Jantelin Annika Brack Mark Brackle Ron Rosendell
Alexandra Moussevisaday Andrea Mariotti and Hel Hotson please don't stop clapping
um and finally a special thanks as well to David Wood a moderator Ruben Dieleman, Katrina Joslin,
Konor Axiotis, Sue Chisholm, Tillman Schepke, Niky Drozdowski, Mark van der Waal and Joep Soeren
and everyone here at Wilton Hall who made this all possible thanks a lot for helping us out
and please join us for drinks at Three Trees which is about 10 to 15 minutes
forward from here so I hope to see you all there thank you
and some people believe in the future there's going to be a wedding in here shortly so we
all need to get out unfortunately unless we're part of that wedding crowd so
biome is chat but chat whilst moving out thank you

What will be at stake will not just be equal products, but whether the liberal democracy
survives, whether the CCP survives, what the world order for the next century will be.
The CCP is going to have an all-out effort to infiltrate American AI labs, billions of dollars,
thousands of people. The CCP is going to try to outbuild us. People don't realize how intense
state-level espionage can be. When we have literal superintelligence on our cluster,
and they can stuxnet the Chinese data centers, you really think they'll be a private company,
and the government would be like, oh my god, what is going on? I do think it is incredibly
important that these clusters are in the United States. I mean, would you do the Manhattan Project
in the UAE, right? 2023 was the sort of moment for me where it went from kind of AGI as a sort
of theoretical, abstract thing, and you'd make the models to like, I see it, I feel it. I can see the
cluster where it's strained on, like the rough combination of algorithms to people, like how
it's happening. And I think, you know, most of the world is not, you know, most of the people feel it
are like right here, you know, right? Okay, today, I'm chatting with my friend, Leopold Aschenbrenner.
He grew up in Germany, graduated valedictorian of Columbia when he was 19. And then he had a very
interesting Gaff year, which we'll talk about. And then he was on the open AI superalignment team,
made it rest in peace. And now he, with some anchor investments from Patrick and John Collison
and Daniel Gross and Nat Friedman is launching an investment firm. So Leopold, I know you're
off to a slow start, but life is long, and I wouldn't worry about it too much. You'll make
up for it in due time. But thanks for coming on the podcast. Thank you. You know, I first
discovered your podcast when your best episode had, you know, like a couple hundred views.
And so it's just been, it's been amazing to follow your trajectory. And it's a delight to be on.
Yeah, yeah. Well, I think in the shelter in Trenton episode, I mentioned that a lot of the
things I've learned about AI, I have learned from talking with them. And the third part of this
triumph, probably the most significant in terms of the things that I've learned about AI has been
you will go out of the stuff on the record now. Great. Okay. First thing I had to get on record,
tell me about the trillion dollar cluster. But by the way, I should mention, so the context of
this podcast is today, there's, you're releasing a series called situational awareness. We're going
to get into it. First question about that is tell me about the trillion dollar cluster.
Yeah. So, you know, unlike basically most things that have come out of Silicon Valley recently,
you know, AI is kind of this industrial process. You know, the next model doesn't just require,
you know, some code, it's, it's, it's building a giant new cluster. You know, now it's
building giant new power plants, you know, pretty soon it's going to be building giant new fabs.
And, you know, since that should be tea, this kind of extraordinary sort of techno capital
acceleration has been set into motion. I mean, basically, you know, exactly a year ago today,
you know, Nvidia had their first kind of blockbuster earnings call, right? Or like went out 25%
after hours, never was like, Oh my God, AI, it's a thing. You know, I mean, I think within a year,
you know, you know, Nvidia, Nvidia data center revenue has gone from like, you know, a few
billion a quarter to like, you know, 20, 25 billion a quarter now and, you know, continuing to go up,
like, you know, big tech capex, skyrocketing. And, you know, it's funny because it's, it's both
there's this sort of this kind of crazy scramble going on. But in some sense, it's just the sort
of continuation of straight lines on a graph, right? There's this kind of like long run trend,
basically almost a decade of sort of training compute of the sort of largest AI systems growing
by about, you know, half an order of magnitude, you know, 0.5 booms a year. And you just kind of
play that forward, right? So, you know, GPT-4, you know, rumored or reported to have finished
pre-training in 2022, you know, the sort of cluster size there was rumored to be about,
you know, 25,000 H100s, you know, sorry, A100s on seminalysis. You know, that's, that's roughly,
you know, if you do the math and that's maybe like a $500 million cluster, you know, it's very
roughly 10 megawatts. And, you know, just play that forward half a new year, right? So then 2024,
that's a, you know, that's a cluster that's, you know, 100 megawatts, that's like 100,000 H100
equivalents, you know, that's, you know, costs in the billions, you know, play it forward, you know,
two more years, 2026, that's a cluster, that's a gigawatt, you know, that's, that's, you know,
sort of a large nuclear reactor size, it's like the power of the Hoover Dam, you know, that costs
tens of billions of dollars, that's like a million H100 equivalents, you know, 2028, that's a cluster
that's 10 gigawatts, right? That's, that's more power than kind of like most U.S. states. That's,
you know, like 10 million H100s equivalents, you know, costs hundreds of billions of dollars.
And then 2030 trillion dollar cluster, 100 gigawatts, over 20% of U.S. electricity production,
you know, 100 million H100 equivalents. And, and that's just the training cluster, right? That's
like the one largest training cluster, you know, and then there's more inference GPUs as well,
right? Most of, you know, once there's products, most of them are going to be inference GPUs.
And so, you know, U.S. power production has barely grown for like, you know, decades,
and now we're really in for a ride. So, I mean, I, when I had Zuck on the podcast,
he was claiming, not a plateau, per se, but that AI progress would be bottlenecked by specifically
this constraint on energy, and specifically like, oh, gigawatt data centers are going to build another
three gorgeous dam or something. I know that there's companies, according to public reports,
who are planning things on the scale of a gigawatt data center, 10 gigawatt data center,
who's going to be able to build that? I mean, the 100 gigawatt center, like a state,
where are you getting, are you going to pump that into one physical data center? How is
this going to be possible? Yeah. What is Zuck missing? I mean, you know, I don't know. I think
to 10 gigawatt, you know, like six months ago, you know, 10 gigawatt was the taco town. I mean,
I think, I feel like now, you know, people have moved on, you know, 10 gigawatt is happening.
I mean, I don't know, there's the information report on OpenAI and Microsoft planning,
a $100 billion cluster. So, you know, you got to, you know, if you- Is that the gigawatt,
or is that the 10 gigawatt? I mean, I don't know. But, you know, if you try to, like, map out,
you know, how expensive would the 10 gigawatt cluster be, you know, that's maybe a couple
hundred billion. So, it's sort of on that scale. And they're planning it. They're working on it,
you know. So, the, you know, it's not just sort of my crazy take. I mean, AMD, AMD, I think,
forecasted a $400 billion AI accelerator market by 27. You know, I think it's, you know, and AI
accelerators are only part of the expenditures. It's sort of, you know, I think sort of a trillion
dollars of sort of, like, total AI investment by 2027 is sort of, like, we're very much in
track on. I think the trillion dollar cluster is going to take a bit more sort of acceleration.
But, you know, we saw how much sort of chat GPT unleashed, right? And so, like, every generation,
you know, the models are going to be kind of crazy and people, it's going to shift the
overtune window. And then, you know, obviously, the revenue comes in, right? So, these are
forward-looking investments. The question is, do they pay off, right? And so, if we sort of
estimated the, you know, the GPT-4 cluster at around $500 million, by the way, that's sort of
a common mistake people make, is they say, you know, people say, like, $100 million for GPT-4,
but that's just the rental price, right? They're like, ah, you rent the cluster for three months.
But, you know, if you're building the biggest cluster, you got to, like, you got to build the
whole cluster. You got to pay for the whole cluster. You can't just rent it for three months.
But, I mean, really, you know, once you're trying to get into the sort of hundreds of
billions, eventually, you got to get to, like, $100 billion a year revenue. I mean, I think this
is where it gets really interesting for the big tech companies, right? Because, like, their revenues
are on order, you know, hundreds of billions, right? So, it's like $10 billion fine, you know,
and it'll pay off the, you know, 2024 size training cluster. But, you know, really,
when sort of big tech, it'll be gangbusters, it's $100 billion a year. And so, the question is sort
of how feasible is $100 billion a year from AI revenue? And, you know, it's a lot more than right
now, but I think, you know, if you sort of believe in the trajectory of the AI systems, as I do,
and which we'll probably talk about, it's not that crazy, right? So, there's, I think there's,
like, 300 million-ish Microsoft Office subscribers, right? And so, they have co-pilot now, and I
don't know what they're selling it for, but, you know, suppose you sold some sort of AI add-on for
$100 a month, and you sold that to, you know, a third of Microsoft Office subscribers subscribe
to that. That'd be $100 billion right there. You know, $100 a month is, you know, a lot.
That's a lot, yeah. It's a lot, it's a lot. For a third of Office subscribers?
Yeah, but it's, you know, for the average dollars worker, it's like a few hours of productivity
a month. And it's, you know, kind of like, you have to be expecting pretty lame AI progress to
not hit, like, you know, some few hours of productivity a month of, yeah.
Okay, sure. So, let's assume all this. What happens in the next few years in terms of,
what is the one gigawatt training, the AI that's trained on the one gigawatt data center? What
can it do with the one on the 10 gigawatt data center? Just map out the next few years of AI
progress for me. Yeah, I think probably the sort of 10 gigawatt-ish range is sort of my best guess
from when you get the sort of true AGI. I mean, yeah, I think it's sort of like one gigawatt
data center. And again, I think actually computer is overrated, and we're going to talk about that,
but we will talk about compute right now. So, you know, I think so 25, 26, we're going to get models
that are, you know, basically smarter than most college graduates. I think sort of the practice,
a lot of the economic usefulness, I think really depends on sort of, you know, sort of on hobbling.
Basically, it's, you know, the models are kind of, you know, they're smart, but they're limited,
right? They're, you know, there's chatbot, you know, and things like being able to use a computer,
things like being able to do kind of like a genetic long horizon tasks. And then I think by 27, 28,
you know, if you extrapolate the trends and, and, you know, we'll talk about that more later,
and I talked about in the series, I think we hit, you know, basically, you know, like,
as smart as the smartest experts, I think the on hobbling trajectory kind of points to, you know,
looks much more like an agent than a chatbot, and much more almost like basically a drop in
remote worker, right? So it's not like, I think basically, I mean, I think this is the sort of
question on the economic returns. I think a lot of the, a lot of the intermediate AI systems could
be really useful. But, you know, it actually just takes a lot of schlep to integrate them,
right? Like GPT-4, you know, whatever, 4.5, you know, probably there's a lot you could do with
them in a business use case. But, you know, you really got to change your workflows to make them
useful. And it's just like, there's a lot of, you know, it's a very Tyler Cowan-esque take. It
just takes a long time to diffuse. Yeah, it's like, you know, we're an SF, and so we missed that or
whatever. But I think in some sense, you know, the way a lot of these systems won't be integrated is,
is you kind of get this sort of sonic boom, where it's, you know, the sort of intermediate
systems could have done it, but it would have taken schlep. And before you do the schlep to
integrate them, you get much more powerful systems, much more powerful systems that are sort of on
hobbled. And so they're this agent, and there's drop in remote worker. And, you know, then you're
kind of interacting with them like a coworker, right? You know, you can take, do zoom calls with
them, and you're slacking them. And you're like, ah, can you do this project? And then they go off,
and they, you know, go away for a week and write a first draft and get feedback on them and, you
know, run tests on their code. And then they come back and then you see it and you tell them a
little bit more things or, you know, and, and that'll be much easier to integrate. And so, you
know, it might be that actually you need a bit of overkill to make the sort of transition easy
and to really harvest the gains. What do you mean by the overkill? Overkill on the model capabilities?
Yeah, yeah. So basically intermediate models could do it, but it would take a lot of schlep.
And so then, you know, the like, actually, it's just the drop in remote worker kind of AGI that
can automate, you know, cognitive tasks that actually just ends up kind of like, you know,
basically it's your like, you know, intermediate models would have made the software engineer
more productive. But you know, will the software engineer adopted? And then the, you know, 27 model
is, well, you know, you just don't need the software engineer, you can literally interact with
it like a software engineer, and it'll do the work of a software engineer. So the last episode I
did was with John Shulman. Yeah. And I was asking about basically this. And one of the questions
I asked is, we have these models that have been coming out since the last year, and none of them
seem to have significantly surpassed GPT-4 and certainly not in the agentic way in which they
are interacting with as a co-worker, you know, the brag that they got a few extra points on MMLU
or something. And even GPT-4O, it's cool that they can talk like Scarlett Johansson or something,
but like, and honestly, I'm going to use that. Oh, I guess not anymore, not anymore. Okay. But
the whole co-worker thing, so this is going to be a wrong question, but you can address it in any
order. But the, it makes sense to me why they'd be good at answering questions. They have a bunch of
data about how to complete Wikipedia text or whatever. Where is the equivalent training data
that enables it to understand what to make, what's going on the zoom call? How does this connect
with what they were talking about in the Slack? What is the cohesive project that they're going
after based on all this context that I have? Where is that training data coming from?
Yeah. So I think a really key question for sort of AI progress in the next few years is sort of how
hard is it to do, sort of unlock the test time compute overhang? So, you know, right now GPT-4
answers a question and, you know, it kind of can do a few hundred tokens of kind of chain of thought,
and that's already a huge improvement, right? Sort of like, this is a big on hobbling before,
you know, answer a math question is just shotgun. And, you know, if you try to kind of like answer
math question by saying the first thing that came to mind, you know, you wouldn't be very good. So,
you know, GPT-4 thinks for a few hundred tokens. And, you know, if I thought for a few hundred,
you know, if I think at like a hundred tokens a minute, and I thought...
You think it's much more than a hundred tokens.
I don't know. If I thought for like a hundred tokens a minute, you know, it's like what GPT-4
does, maybe it's like, you know, it's equivalent to me thinking for three minutes or whatever, right?
You know, suppose GPT-4 could think for millions of tokens, right? That's sort of plus four rooms,
plus four to the magnitude on test time compute, just like on one problem.
It can't do it right now. It kind of gets stuck, right? Like write some code, even if, you know,
you can do a little bit of iterative debugging, but eventually just kind of like,
you can't, it kind of gets stuck in something, it can't correct its errors and so on.
And, you know, in a sense, there's this big overhang, right? And like other areas of ML,
you know, there's this great paper on AlphaGo, right, where you can trade off train time and
test time compute. And if you can use, you know, four rooms, more test time compute, that's almost
like, you know, a three and a half and bigger model. Just because, again, like you can, you know,
if a hundred tokens a minute, a few million tokens, that's a few months of sort of working time.
There's a lot more you can do in a few months of working time than right now. So the question is,
how hard is it to unlock that? And I think the, you know, the sort of short timelines AI world
is if it's not that hard. And the reason that might not be that hard is that, you know, there's
only really a few extra tokens you need to learn, right? You need to kind of learn
error correction tokens, the tokens where you're like, ah, I think I made a mistake.
Let me think about that again. You need to learn the kind of planning tokens. That's kind of like,
I'm going to start by making a plan. Here's my plan of attack. And then I'm going to write a draft.
And I'm going to like, now I'm going to critique my draft. I'm going to think about it. And so it's
not, it's not things that models can do right now. But, you know, the question is how hard is that?
And in some sense, also, you know, there's sort of two paths to agents, right? You know,
when Cholto was on your podcast, you know, he talked about kind of scaling leading to more
nines of reliability. And so that's one path. I think the other path is this sort of like
unhobbling path where you, it needs to, it needs to learn this kind of like system to process.
And if it can learn this sort of system to process, it can just use kind of millions of tokens
and think for them and be cohesive and be coherent. You know, one analogy. So when you drive,
here's an analogy, when you drive, right? Okay, you're driving. And, you know, most of the time,
you're kind of an autopilot, right? You're just kind of driving and you're doing well. And then,
but sometimes you hit like a weird construction zone or a weird intersection, you know, and then I
sometimes like, you know, my passenger seat, my girlfriend, I'm kind of like, ah, be quiet for
a moment. I need to figure out what's going on, right? Right. And that's sort of like, you know,
you go from autopilot to like the system two is jumping in and you're thinking about how to do
it. And so the scaling scaling is improving that system one autopilot. And I think it's sort of,
it's the brute force way to get to kind of agents who just improve that system. But if you can get
that system two working, then, you know, I think you could like quite quickly jump, you know,
to sort of this like more identified, you know, test time, compute, overhang is unlocked.
What's the reason to think that this is an easy win in the sense that, oh, you just get the,
there's like some loss function that easily enables you to train it to enable the system two
thinking. Yeah. There's not a lot of animals that have system two thing thinking, you know,
it like took a long time for evolution to give us system two thinking. Yeah. The pre-training
at like, listen, I get it, you got like trillions of tokens of internet text, I get that like,
yeah, you like match that and you get all these, yeah, all this free training capabilities. What's
the reason to think that this is an easy and hobbling? Yeah. So, okay, a bunch of things. So
first of all, free training is magical, right? And it's, and it's, and it gave us this huge
advantage for, for, for models of general intelligence, because, you know, you could, you
just predict the next token, but predicting the next token, I mean, it's sort of a common
misconception. But what it does is lets this model learn these incredibly rich representations,
right? Like these sort of representation learning properties are the magic of deep
learning. You have these models, and instead of learning just kind of like, you know, whatever,
statistical artifacts or whatever, it learns through these models of the world. You know,
that's also why they can kind of like generalize, right? Because it learned the right representations.
And so, you know, you train these models and you have this sort of like raw bundle of capabilities
that's really useful. It's sort of this almost unformed raw mass. And sort of the unhobbling
we've done over sort of like GP2 to GP4 was, was you kind of took this sort of like raw mass,
and then you like RLHF it into really good chat bot. And that was a huge win, right? Like,
you know, going, going, you know, an R, you know, in the original, I think it's truck GPT
paper, you know, RLHF versus non RLHF model, it's like 100x model size win on sort of human
preference rating, you know, it started to be able to do like simple chain of thought and so on.
But you still have a disadvantage of all these kind of like raw capabilities. And I think there's
still like a huge amount that you're not doing with them. And by the way, I think the sort of
this pre training advantage is also sort of the difference to robotics, right? Where I think
robotics, you know, you know, I think people used to say it was a hardware problem, but I think
the hardware stuff is getting solved. But the thing we have right now is you don't have this
huge advantage of being able to bootstrap yourself with pre training, you don't have all
this sort of unsupervised learning you can do, you have to start right away with this sort of
RL self play and so on. Alright, so now the question is why, you know, why might some of this
unhobbling and RL and so on work? And again, there's sort of this advantage of bootstrapping,
right? So, you know, your foot of bio is being pre trained, right? You're actually not being
pre trained anymore. You're not being pre trained anymore. You're pre trained in like grade school
and high school. At some point, you transition to be able being able to like learn by yourself,
right? You weren't able to do that in elementary school. I don't know middle school probably
high school maybe when sort of started some guidance. You know, college, you know, it's
your smart, you can kind of teach yourself. And then sort of models are just starting to enter
that regime, right? And so it's sort of like it's a little bit probably a little bit more scaling.
And then you got to figure out what goes on top and it won't be trivial, right? So a lot of
a lot of deep learning is sort of like, you know, it sort of seems very obvious in retrospect,
and there's sort of this some obvious cluster of ideas, right? There's sort of some kind of like
thing that seems a little dumb, but there's kind of works, but there's a lot of details
you have to get right. So I'm not saying this, you know, we're going to get this, you know,
next month or whatever, I think it's going to take a while to like really figure out
the details. A while for you is like half a year or something.
Well, I don't know. I think
That makes you mind six months.
Between six months and three years, you know. But you know, I think it's possible. And I think
there's, you know, I think, and this is, I think it's also very related to the sort of issue of
the data wall. But I mean, I think the, you know, one intuition on the sort of like learning,
learning, learning by yourself, right, is sort of pre-training is kind of the words are flying
by, right? You know, and, and, or it's like, you know, the teacher is lecturing to you. And
the models, you know, the words are flying by, you know, they're taking, they're just getting
a little bit from it. But that's sort of not what you do when you learn from yourself, right?
When you learn by yourself, you know, so you're reading a dense math textbook, you're not just
kind of like skimming through it once, you know, you wouldn't learn that much from it. I mean,
some word cells just skim through and reread and reread the math textbook. And then they memorize
that sort of, you know, like, you just repeated the data, then they memorize. What you do is you
kind of like, you read a page, kind of think about it, you have some internal monologue going on,
you have a conversational study buddy, you try a practice problem, you know, you fail a bunch of
times. At some point, it clicks. And then you're like, this made sense. Then you read a few more
pages. And so we've kind of bootstrapped our way to being being able to do that now with models
or like just starting to be able to do that. And then the question is, you know, being able to like
read it, think about it, you know, try problems. And the question is, can you, you know, all this
sort of self place synthetic data RL is kind of like making that thing work. So basically,
translate translated translating like in context, right, like right now, there's like in context
learning, right, super sample efficient. There's that, you know, in the Gemini paper, right, it
just like learns a language in context. And then you're pre training, not at all sample efficient.
But, you know, what humans do is they kind of like, they do in context learning, you read a
book, you think about it until eventually it clicks. But then you somehow distill that back
into the weights. And in some sense, that's sort of like what RL is trying to do. And like, when
RL is super finicky, but when RL works, RL is kind of magical because it's sort of the best possible
data for the model. It's like, when you try a practice problem, and you know, and then you fail,
and at some point you kind of figure it out in a way that makes sense to you, that's sort of like
the best possible data for you, because like the way you would have solved the problem. And that's
sort of, that's what RL is. Rather than just, you know, you kind of read how somebody else solved
the problem and doesn't, you know, initially click. Yeah, by the way, if that takes sounds
familiar, because it was like part of the question I asked on showman, that goes to illustrate the
thing I said in the intro, where like a bunch of the things I've learned about AI, just like,
we do these dinners before the interviews, and I'm like, oh, what should I ask on showman?
Okay, suppose this is the way things go, and we get these in hobblings.
Yeah. And the scaling, right? So it's like, you have this baseline, just enormous force of
scaling, right? Where it's like GP2 to GP4, you know, GP2, it could kind of like, it was amazing,
right? It could string together plausible senses. But, you know, it could barely do anything. It
was kind of like preschooler. And then GP4 is, you know, it's writing code, it like, you know,
can do hard math. And so it's sort of like smart high school. And so this big jump, and you know,
and sort of the essay series I go through and kind of count the order's magnitude of compute,
scale up with algorithmic progress. And so sort of scaling alone, you know, sort of by 2728 is
going to do another kind of preschool to high school jump on top of GP4. And so that'll already
be just like at a per token level, just incredibly smart, that'll get you some more reliability.
And then you add these on hobblings that make it look much less like a chat bot,
more like this agent, like a drop in remote worker. And, you know, that's when things really get
gone. Okay, yeah. I want to ask you more questions about this. I think. Yeah, let's zoom out. Okay.
So suppose you're right about this. Yeah. And I guess you this is because of the 2027 cluster,
if you've got 10 gigawatt, 2027 10 gigawatts. 28 is the 10 gigawatt. Okay, so maybe you'll be
pulled for it. Okay, sure. Something. Yeah. And so I guess that's like 5.5 level by 2027.
Whatever that's called, right? What does the world look like at that point?
You have these remote workers who can replace people. What is the reaction to that in terms of
the economy, politics, geopolitics? Yeah, so, you know, I think 2023 was kind of a really
interesting year to experience as somebody who was like, you know, really following the
AI stuff where, you know, before that, what were you doing in 2023? I mean, open AI. Okay. Yeah.
And, and, and, um, you know, it kind of went, you know, I mean, you know, I was, I was been thinking
about this and, you know, like talking to a lot of people, you know, in the years before, and it
was this kind of weird thing, you know, you almost didn't want to talk about AI or AGI, you know,
it's kind of a dirty word, right? And then 2023, you know, people saw chat, GBT for the first time,
you saw GB4, and it just like exploded, right? It triggered this kind of like, you know,
huge sort of capital expenditures from all these firms and, and, and, you know,
the explosion of revenue from NVIDIA and so on. And, you know, things have been quiet since then,
but, you know, the next thing has been in the oven. And I sort of expect sort of every generation,
these kind of like G forces to intensify, right? It's like, people see the models.
There's like, you know, people haven't counted them. So they're going to be surprised and they'll
be kind of crazy. And then, you know, revenue is going to accelerate, you know, suppose you do hit
the 10 billion, you know, end of this year, suppose it like just continues on this sort of
doubling trajectory of, you know, like every six months of revenue doubling, you know, it's like,
you're not actually that far from 100 billion, you know, maybe that's like 26. And so, you know,
at some point, you know, like, you know, sort of what happened to NVIDIA is going to happen to
big tech, you know, like their stocks, they're, you know, that's going to explode. And I mean,
I think a lot more people are going to feel it, right? I mean, I think the, I think 2023 was the
sort of moment for me where it went from kind of AGI is a sort of theoretical abstract thing,
and you'd make the models to like, I see it, I feel it. And like, I see the path, I see where
it's going. I like, I think I can see the cluster where it's strained on, like the rough combination
of algorithms, the people, like how it's happening. And I think, you know, most of the world is not,
you know, most of the people feel it are like right here, right? But, but, you know, I think a
lot more of the world is going to start feeling it. And I think that's going to start being kind
of intense. Okay, so right now, who feels it, you can, you go on Twitter, and there's these
GPT wrapper companies like, whoa, GPT 4.0 is going to change our business.
I mean, I'm so, so bearish on the wrapper companies, right? Because like, they're the ones that are
going to be like, the wrapper companies are betting on stagnation, right? The wrapper companies are
betting like, you have these intermediate models and take so much left to integrate them. And I'm
kind of like, I'm really bearish because I'm like, we're just going to sonic boom you, you know,
and we're going to get the unhauled ones, we're going to get the drop in remote worker, and then,
you know, your stuff is not going to matter. Okay, sure. So that's done. Now, who, so the
SF is paying attention now, or this crowd here is paying attention.
Who is going to be paying attention in 2026, 2027? And personally, these are years in which
hundreds of billions of CapEx is being spent on the eye. I mean, I think the national security
state is going to be starting to pay a lot of attention. And I, you know, I hope we get to
talk about that. Okay, let's talk about it now. What is this sort of political reaction immediately?
Yeah. And even like internationally, like what people see, like right now, I don't know if like
Xi Jinping like reads the news and sees like, well, GPT 4.0, oh my God, like MMLU score on that.
What are you doing about this, Comrade? So what happens when the, like, what, what, the
GPT, he's like, he's a remote replacement and it has a hundred billion dollars in revenue.
There's a lot of businesses that have a hundred billion dollars in revenue and people don't,
like, aren't staying up all night talking about it. The question, I think the question is, when,
when does the CCP, and when does the sort of American national security establishment realize
that superintelligence is going to be absolutely decisive for national power, right? And this is
where, you know, the sort of intelligence explosion stuff comes in, which, you know, we should also
talk about later. You know, it's sort of like, you know, you have AGI, you have this sort of
drop in remote worker that can replace, you know, you or me, at least that sort of remote jobs,
you know, cognitive jobs. And then, you know, I think fairly quickly, you know, I mean, by default,
you turn the crank, you know, one or two more times, you know, and then you get a thing that's
smarter than humans. But I think even, even more than just turning the cramp a few more times,
crank a few more times, you know, I think one of the first jobs to be automated is going to be that
of sort of an AI researcher engineer. And if you can automate AI research, you know, I think
things can start going very fast. You know, right now, there's already this trend of, you know,
half an order of magnitude a year of algorithmic progress, you know, suppose, you know, at this
point, you know, you're going to have GPU fleets in the tens of millions for inference, you know,
or more. And you're going to be able to run like 100 million human human equivalents of these sort
of automated AI researchers. And if you can do that, you know, you can maybe do, you know, decades
worth of sort of ML research progress in a year, you know, you get the some sort of 10x speed up.
And if you can do that, I think you can make the jump to kind of like AI that is vastly smarter
than humans, you know, within a year, a couple years. And then, you know, that broadens, right?
So you have this, you have this sort of initial acceleration of AI research that broadens to
like you apply R&D to a bunch of other fields of technology. And the sort of like extremes, you
know, at this point, you have like a billion and just super intelligent researchers, engineers,
technicians, everything, you're superbly competent, all the things, you know,
they're going to figure out robotics. Are we talked about it being a software problem? Well,
you know, you have you have a billion of super smart, smarter than the smartest human researchers,
AI researchers on your cluster, you know, at some point during the intelligence explosion,
they're going to be able to figure out robotics, you know, and then again, that expands. And,
you know, I think if you play this picture forward, I think it is fairly unlike any other
technology in that it will, I think, you know, a couple years of lead could be utterly decisive
in say like military competition, right? You know, if you look at like go for one, right,
go for one, you know, like the Western coalition forces, you know, they had, you know, like a
hundred to one kill ratio, right? And that was like, they had better sensors on their tanks,
you know, and they had, they had better precision, more precision missiles, right, like GPS,
and they had, you know, stealth and they had sort of a few, you know, maybe 20, 30 years of
technological lead, right? And they, you know, just completely crushed them.
Super intelligence applied to sort of broad fields of R&D. And then, you know,
the sort of industrial explosion as well, you have the robots, you're just making lots of material,
you know, I think that could compress, I mean, basically compress kind of like
a century worth of technological progress into less than a decade. And that means that, you know,
a couple years could mean a sort of go for one style, like, you know, advantage in military affairs.
And, you know, including like, you know, a decisive advantage that even like preempts nukes,
right? Suppose, like, you know, how do you find the stealth and nuclear submarines? Like,
right now, that's a problem of like, you have sensors, you have the software,
like tech where they are, you know, you can do that, you can find them, you have kind of like
millions or billions of like mosquito-like, you know, size drones, and that, you know,
they take out the nuclear submarines, they take out the mobile launchers, they take out the other nukes.
And anyway, so I think enormously destabilizing, enormously important for national power,
and at some point, I think people are going to realize that, not yet, but they will. And when
they will, I think there will be sort of, you know, I don't think it'll just be the sort of AI
researchers in charge. And, you know, I think on the, you know, the CCP is going to, you know,
have sort of an all-out effort to like infiltrate American AI labs, right? You know, like billions
of dollars, thousands of people, you know, full force of the sort of, you know, Ministry of State
Security. CCP is going to try to, you know, like outbuild us, right? Like, they, you know, their,
you know, power in China, you know, like the electric grid, you know, they added a U.S. is,
you know, a complete, like, they added as much power in the last decade as like sort of entire
U.S. electric goods. So like the 100 gigawatt cluster, at least the 100 gigawatts is going to
be a lot easier for them to get. And so I think sort of, you know, by this point, I think it's
going to be like an extremely intense sort of international competition. Okay, so in this picture,
one thing I'm uncertain about is whether it's more like what you say, where it's more of an
implosion of you have developed an AGI and then you may get into an AI researcher. And for a while,
a year or something, you're only using this ability to make hundreds of millions of other AI
researchers. And then like the thing that comes out of this really frenetic process is a super
intelligence. And then that goes out in the world and is developing robotics and helping you take
over other countries and whatever. It's a little bit more, you know, it's a little bit more kind
of like, you know, it's not like, you know, on and off, it's a little bit more gradual, but it's
sort of like it's an explosion that starts narrowly. It's can do cognitive jobs, you know, the highest
ROI use for cognitive jobs is make the AI better, like solve robotics, you know, and as, as, as you
know, you solve robotics, now you can do R&D and, you know, like biology and other technology,
you know, initially you start with the factory workers, you know, they're wearing the glasses
and the air pods, you know, and the AI is instructing them, right? Because, you know,
you kind of make any worker into a skilled technician, and then you have the robots come in.
And anyway, so it sort of expands as process expands.
Metas revans are a compliment to their llama.
The fabs in the US, the constrained skilled workers, right? You have, you have the,
even if you don't have robots that you have the cognitive superintelligence and, you know,
it can kind of make them all into skilled workers immediately. But that's, you know,
it's a very brief period, you know, robots will come soon.
Sure. Okay. Okay. So suppose this is actually how the tech progresses in the United States,
maybe because these companies are already experiencing hundreds of billions of dollars
of AI revenue. At this point, you know, companies are borrowing, you know, hundreds of billions
of more in the corporate debt markets, you know. But why is a CCP bureaucrat, some 60-year-old guy,
he looks at this and he's like, oh, it's like co-pilot has gotten better now.
But why are they now? I mean, this is much more than co-pilot has gotten better now. I mean,
at this point. But no, I don't add to that.
Like, yeah, so they're, because to shift the production of an entire country to dislocate
energy that is otherwise being used for consumer goods or something and to make it that all
feed into the data centers, what part of this whole story is you realize the superintelligence
is coming soon, right? And I guess you realize it, maybe I realize it. I'm not sure how much I
realize it, but will the, will the national security apparatus in the United States and
will the CCP realize it? Yeah, I mean, look, I think in some sense, this is a really key question.
I think we have sort of a few more years of mid game, basically, and where you have a few more
2023s, and that just starts updating more and more people. And, you know, I think, you know,
the trend lines, you know, will become clear. You know, I think, I think you will see some
amount of the sort of COVID dynamic, right? You know, like COVID was like, you know, February,
February of 2020, you know, it's like, honestly feels a lot like today, you know, where it's like,
you know, it feels like this utterly crazy thing has happened is about, you know, is impending,
is coming. You kind of see the exponential. And yet most of the world just doesn't realize,
right? The mayor of New York is like, go out to the shows. And this is just, you know, like
Asian racism or whatever, you know, and, and, but, you know, at some point, the exponential,
like, you know, at some point, people saw it. And then, you know, like just kind of
crazy radical reactions came. Right. Okay. So by the way, what were you doing during COVID?
We're in like February. Okay. And like freshman, sophomore, what?
Junior. But still like when we were like 17 year old junior or something.
And, and then you, like, did you short the market or something? Yeah. Okay. Did you, did you,
did you sell at the right time? Yeah. Okay. Yeah. So there will be like a March 2020 moment
that the thing that was COVID, but here, now, then you can like make the analogy that you make
in the series that this will then cause the reaction of like, we got to do the Manhattan
project for America here. I wonder what the politics of this will be like, because the
difference here is it's not just like we need the bomb to beat the Nazis. It's, we're building this
thing that's making all our entry prices rise a bunch and it's automating a bunch of our jobs.
And the climate change stuff, like people are going to be like, oh my God, it's making climate
change worse. And it's helping big tech. Like politically, this doesn't seem like a dynamic
where the national security apparatus or the president is like, we have to step on the gas
here and like make sure America wins. Yeah. I mean, again, I think a lot of this really depends on
sort of how much people are feeling it, how much people are seeing it. You know, I think there's
a thing where, you know, kind of basically our generation, right? We're kind of so used to kind
of, you know, basically peace and like, you know, the world, you know, American hegemony and nothing
matters. But you know, the sort of like extremely intense and these extraordinary things happening
in the world, and like intense international competition is like very much the historical
norm. Like in some sense, it's like, you know, sort of this there's a sort of 20 year very unique
period. But like, you know, the history of the world is like, you know, you know, like in World
War II, right, it was like 50% of GDP went to, you know, like, you know, war per dime production,
the US borrowed over 60% of GDP, you know, and in, you know, I think Germany, Japan over 100%,
World War I, you know, UK, Japan, sorry, UK, France, Germany all borrowed over 100% of GDP.
And, you know, I think the sort of much more was on the line, right? Like, you know, and, you know,
people talk about World War I being so destructive and you know, like 20 million Soviet soldiers
dying and like 20% of Poland, but you know, that was just the sort of like that happened all the
time, right, you know, like seven years war, you know, like whatever 20, 30% of Prussia died,
you know, like 30 years war, you know, like, I think, you know, up to 50% of like large swath
of Germany died. And, you know, I think the question is, will these sort of like, will people see that
the stakes here are really, really high, and that basically is sort of like history is actually
back. And I think, you know, I think the American national security state thinks
very seriously about stuff like this, they think very seriously about competition with China. I
think China very much thinks of itself on this is a historical mission and you drew a nation of
the tiniest nation, a lot about national power, I think a lot about like the world order. And then,
you know, I think there's a real question on timing, right, like, do they do they start taking
this seriously, right, like when the intelligence explosion is already happening, like quite late,
or do they start taking this seriously, like two years earlier, and that matters a lot for how
things play out. But at some point, they will. And at some point, they will realize that this
will be sort of utterly decisive for, you know, not just kind of like some proxy war somewhere,
but you know, like, whether liberal democracy can continue to thrive, whether you know, whether
the CCP will continue existing. And I think that will activate sort of forces that we haven't seen
in a long time. The great conflict, the great power conflict thing definitely seems compelling.
I think just all kinds of different things seem much more likely when you think from a historical
perspective, when you zoom out beyond the liberal democracy that we've been living in,
had the pleasure to live in America, let's say 80 years, including dictatorships, including all
obviously war, famine, whatever. I was reading the Gulag Archipelago and one of the chapters
begins with Sojenitsyn saying, if you would have told Russian citizens under the czars that because
of all these new technologies, we wouldn't see some great Russian revival or becomes a great
power and the citizens are made wealthy. But instead, what you would see is tens of millions
of Soviet citizens tortured by millions of beasts in the worst possible ways, and that this is what
would be the result of the 20th century, they wouldn't have believed you, they'd have called
you a slanderer. Yeah, and you know, the, you know, the possibilities for dictatorship with
superintelligence are sort of even crazier, right? I think, you know, imagine you have a perfectly
loyal military and security force, right? That's it. No more, no more rebellions, right? No more
popular uprisings, you know, perfectly loyal, you know, you have, you know, perfect lie detection,
you know, you have surveillance of everybody, you know, you can perfectly figure out who's the
dissenter, weed them out, you know, no Gorbachev would have ever risen to power who had some
doubts about the system, you know, no military coup would have ever happened. And I think you,
I mean, you know, I think there's a real way in which, you know, part of why things have worked
out is that, you know, ideas can evolve. And you know, there's sort of like some, some sense in
which sort of time heals a lot of wounds and time, you know, solve, solve, solve, you know,
a lot of debates and a lot of people had really strong convictions. But you know, a lot of those
have been overturned by time because there's been this continued pluralism and evolution. I think
there's a way in which kind of like, you know, if you take a CCP like approach to kind of like
truth, truth is what the party says, and you supercharge that with superintelligence. I think
there's a way in which that could just be like locked in and trying for, you know, a long time.
And I think the possibilities are pretty terrifying. You know, your point about, you know,
history and sort of like living in America for the past eight years, you know, I think this is
one of the things I sort of took away from growing up in Germany is a lot of the stuff feels more
visceral, right? Like, you know, my mother grew up in the former East, my father in the former West,
they like met shortly after the wall fell, right? Like the end of the Cold War was this sort of
extremely pivotal moment for me because it's, you know, it's the reason I exist, right? And then,
you know, growing up in Berlin and, you know, former wall, you know, my great grandmother,
who is still alive, is very important in my life. You know, she was born in 34, you know, grew up,
you know, during the Nazi era, during, you know, all that, you know, then World War II,
you know, like South of the firebombing of Dresden from the sort of, you know, country cottage or
whatever were, you know, they as kids were, you know, then, and then, you know, then spends most
of her life in sort of the East German communist dictatorship, you know, she'd tell me about,
you know, in like 54 when there's like the popular uprising, you know, and Soviet tanks came in,
you know, her husband was telling her to get home really quickly, you know, get off off the streets,
you know, had a, had a son who, who tried to, you know, ride a motorcycle across,
across the Iron Curtain and then was put in the Stasi prison for a while.
You know, and then finally, you know, when she's almost 60, you know, it was the first time she
lives in, you know, a free country and a wealthy country. And, you know, when I was a kid, she was,
she, the thing she always really didn't want me to do was like get involved in politics because
like joining a political party was just, you know, it was a very bad connotations for her.
Anyway, and she sort of raised me when I was young, you know, and so it,
you know, it doesn't feel that long ago. It feels very close.
Yeah. So I wonder when we're talking today about the CCP, listen, the people in China who will be
doing the pro, their version of the project will be AI researchers who are somewhat westernized,
who interact with either got educated in the west or have colleagues in the west.
Are they going to sign up for the, the CCP project that's going to hand over control to Xi Jinping?
What's your sense on, I mean, it's just like fundamentally they're just people, right? Like,
can't you like convince them about the dangers of superintelligence? Will they be in charge though?
I mean, some since this is, I mean, this is also the case, you know,
you know, in the U.S. or whatever, this is sort of like rapidly depreciating influence of the
lab employees. Like right now, the sort of AI lab employees have so much power, right? Over this,
you know, like you saw this in November, right? But both, I mean, both they're going to get
automated and they're going to lose all their power. And it'll just be, you know, kind of like a few
people in charge with their sort of armies of automated eyes. But also, you know, it's sort of
like the politicians and the generals and the sort of national security state, you know, a lot,
you know, it's, I mean, there's sort of, this is the sort of some of these classic scenes from
the Oppenheimer movies, you know, the scientists built it, and then it was kind of, you know,
and the bomb was shipped away and it was out of their hands. You know, I actually, yeah,
I think I actually think it's good for like lab employees to be aware of this is like,
you have a lot of power now, but, you know, maybe not for that long and, you know, use it wisely.
Um, yeah, I do, I do think they would benefit from some more, you know,
organs of representative democracy. What do you mean by that? Oh, I mean, I, you know,
in the sort of the, in the open eye board events, you know, employee power was exercised in a very
sort of direct democracy way. And I feel like that's how some of how that went about, you know,
I think it really highlighted the benefits of representative democracy and having some
deliberative organs. Interesting. Yeah. Well, let's go back to the hundred billion revenue,
whatever, and so these companies now cluster. Yeah, the companies are deploying, we're trying
to build clusters that are this big. Yeah. Where are they building it? Because if you say it's the
amount of energy that would be required for a small or medium sized US state, is it then Colorado
gets no power and it's happening in the United States or is it happening somewhere else? Oh,
I mean, I think that, I mean, in some sense, this is the thing that I always find funny is,
you know, you talk about Colorado gets no power, you know, the easy way to get the power would be
like, you know, displace less economically useful stuff, you know, it's like whatever, buy up the
aluminum smelting plant and, you know, that has a gigalot and, you know, we're going to replace
it with, with the data center because that's important. Um, I mean, that's not actually
happening because a lot of these power contracts are really sort of long-term locked in, you know,
there's obviously people don't like things like this. And so it sort of, it seems like in practice,
what it's, what it's requiring at least right now is building new power, the, um, that might change.
And I think that that's when things get really interesting when it's like, no, we're just
dedicating all of the power to the AGI. Okay. So right now it's building new power, 10 gigawatt,
I think quite doable. Um, you know, it's like a few percent of like US natural gas production.
Um, you know, I mean, when you have the 10 gigawatt clint training cluster, you have a lot
more in-front. So that starts getting more, you know, I think 100 gigawatt, that starts
getting pretty wild. You know, that's, you know, again, it's like over 20% of US electricity
production. Um, I think it's pretty doable. Um, especially if you're willing to go for
like natural gas. Um, I do, I do think, I do think it is incredibly important,
incredibly important that these clusters are in the United States.
Well, and why does it matter? It's in the US. Um, I mean, look, I think there's some people
who are, you know, trying to build clusters elsewhere. And you know, there's like a lot of
free flowing Middle Eastern money that's trying to build clusters elsewhere. Um,
I think this comes back to the sort of like national security question we talked about
earlier. Like would you, I mean, would you do the Manhattan project and the UAE, right?
And I think, I think basically like putting, putting the clusters, you know, I think you
can put them in the US, you can put them in sort of like ally democracies. But I think once you
put them in kind of like, you know, dictatorships, authoritarian dictatorships, you kind of create
this, you know, irreversible security risk, right? So I mean, one cluster is there much easier for
them to actually trade the weights. You know, they can like literally steal the AGI, the super
intelligence. It's like they got a copy of the, you know, of the, of the atomic bomb, you know,
and they just got the direct replica of that. And it makes it much easier to them. I mean,
we're ties to China, you can ship that to China. So that's a huge risk. Another thing is they can
just seize the compute, right? Like maybe right now they just think of this. I mean, in general,
I think people, you know, I think the issue here is people are thinking of this as they, you know,
chat, GBT, big tech product clusters, but I think the cluster is being planned now,
you know, three to five years out, like it will be the like AGI super intelligence clusters.
And so anyway, so like when things get hot, you know, they might just seize the compute. And I
don't know, suppose we put like, you know, 25% of the compute capacity in the sort of Middle
Eastern indicator ships, well, they seize that. And now it's sort of a ratio of compute of three
to one, and you know, still have some more, but even like, even, even only, only 25% of compute
there, like, I think it starts getting pretty hairy, you know, I think three to one is like,
not that great of a ratio, you can do a lot with that amount of compute. And then look, even, even
if they don't actually do this, right, even they don't actually seize the compute, even they actually
don't steal the weights. There's just a lot of implicit leverage you get, right? They get,
they get the seat at the AGI table. And, you know, I don't know why we're giving authoritarian
dictatorships the seat at the AGI table. Okay, so there's going to be a lot of compute in the
Middle East if these deals go through. First of all, who's, who is it just like every single
big tech company is just trying to figure out. Okay, okay. I guess there's reports, I think
Microsoft or yeah, yeah, yeah, yeah, yeah, yeah, which we'll get into. So they, UIE gets a bunch
of compute because we're building the clusters there. And why, so let's say they have 25% of,
why does a compute ratio matter? Is it, if it's about them being able to kick off the
intelligence explosion, isn't it just some threshold where you have 100 million AI researchers or
you don't, I mean, you can do a lot with, you know, 33 million extremely smart scientists.
And, you know, and again, a lot of the stuff, you know, so first of all, it's like, you know,
that might be enough to build the crazy bio weapons, right? And then you're in a situation
where like now, wow, we've just like, they stole the weights, they seized the compute,
now they can make, you know, they can build these crazy new WMDs that, you know, will be possible
super intelligence. And then you just kind of like proliferated the stuff and, you know, it'll
be really powerful. And also, I mean, I think, you know, three to three acts on compute isn't
actually that much. And so the, you know, the, you know, I think a thing I worry a lot about is
I think everything, I think the riskiest situation is if we're in some sort of like really tight
neck feverish international struggle, right? If we're like really close with the CCP,
and we're like months apart. I think the situation we want to be in, we could be in if
we played our cards, right, is a little bit more like, you know, the US, you know, building the
atomic bomb versus the German project, way behind, you know, years behind. And if we have that,
I think we just have so much more wiggle room like to get safety, right? We're going to be building
like, you know, there's going to be these crazy new WMDs, you know, things that completely undermine,
you know, nuclear deterrence, you know, intense competition. And that's so much easier to deal
with if, you know, you're like, you know, it's not just, you know, you don't have somebody
right on your tails, you got to go, go, go, you got to go with maximum speed, you have no wiggle
room. You're worried that at any time they can overtake you. I mean, they can also just try to
outbuild you, right? Like they might, they might literally win, like China might literally win
if they can steal the weights because they can, they can outbuild you. And they maybe have less
caution, both, you know, good and bad caution, you know, kind of like whatever unreasonable
regulations we have. Or you're just in this really tight race. And I think it's that sort of like,
if you're in this really tight race, this sort of fever struggle, I think that's when sort of
there's the greatest peril of self-destruction. So then presumably the companies that are trying
to build clusters in the Middle East realize this, what is it? Is it just that it's impossible to do
this in America? And if you want American companies to do this at all, then you do it in
Middle East or not at all. And then you just like have China build the three gorgeous dam cluster.
I mean, there's a few reasons. I mean, one of them is just like, people aren't thinking about,
this is the AGI superintelligence cluster. They're just like, ah, you know, like cool clusters for
my, you know, for my chat. So they're building in the plans right now are clusters, which are
ones that are like, because if you're doing one's inference, presumably you could like spread them
out across the country or something. But the ones they're building, they realize we're going to do
one trading run in this thing we're building. I just think it's harder to distinguish between
inference and training compute. And so people can claim it's training compute, but I think they
might realize that actually, you know, this is going to be useful for, sorry, they might say
it's inference compute and actually it's useful for training compute too. Because it's synthetic
data and things like that. Yeah, the future of training, you know, like RL looks a lot like
inference, for example, right? Or, or you just kind of like end up connecting them, you know,
in time, you know, it's like raw material, you know, it's like, you know, it's, it's placing
your uranium refinement facilities there. Sure. So a few reasons, right? One is just like, they
don't think about this as the agi cluster. Another is just like easy money from the Middle East,
right? Another one is like, you know, people saying, some people think that, you know, you
can't do it in the US. And, you know, I think we actually face this sort of real system competition
here, because again, some people think there's only autocracies that can do this that can kind of
like top down, mobilize the sort of industrial capacity, the power, you know, get the stuff done
fast. And again, this is the sort of thing, you know, we haven't faced in a while. But, you know,
during the Cold War, like we really, there was this sort of intense system competition,
right? Like East West Germany was this, right? Like West Germany, kind of like liberal democratic
capitalism versus kind of, you know, communist state planned. And, you know, now it's obvious that
the sort of, you know, the free world would win. But, you know, even, even as late as like 61,
you know, Paul Samuelson was predicting that the Soviet Union would would outgrow the United States
because they were able to sort of mobilize industry better. And so yeah, there's some people who,
you know, shitpost about loving America by day, they're betting against America, they're betting
against the liberal order. And I think I basically just think it's a bad bet. And the reason I think
it's a bad bet is I think this stuff is just really possible in the US. And so there's make
it possible in the US, there's some amount that we have to get our act together, right? So I think
there's basically two paths to doing it in the US. One is you just got to be willing to do natural
gas. And there's ample natural gas, right? You put your cluster in West Texas, you put it in,
you know, Southwest Pennsylvania by the, you know, Marcel Shale, 10 giga cluster super easy,
100 gigawatt cluster, also pretty doable. You know, I think, you know, natural gas production in
the United States is, you know, almost doubled in a decade. You do that, you know, one more time over
the next, you know, seven years or whatever, you know, you could power multiple trillion
dollar data centers. But the issue there is, you know, a lot of people have sort of these made
these climate commitments and not just government, it's actually the private companies themselves,
right, the Microsoft, the Amazons and so on, these climate commitments, so they won't do
natural gas. And, you know, I admire the climate commitments, but I think at some point, you know,
the national interest and national security kind of is more important.
The other path is like, you know, you can do this sort of green energy mega projects, right?
You do the solar and the batteries and the, you know, the SMRs and, and geothermal. But if we want
to do that, there needs to be sort of a sort of broad, deregulatory push, right? So like, you can't
have permitting take a decade, right? So you got to reform FERC, you got to like have, you know,
blanket NEPA, NEPA exemptions for this stuff. You know, there's like Nain state level regulations,
you know, that are like, yeah, you could build, you know, you build the solar panels and batteries
next to your data center, but it'll still take years because, you know, you actually have to
hook it up to the state electrical grid, you know, and you have to like use governmental
powers to create rights of way to kind of like, you know, have multiple clusters and connect them,
you know, and have thick cables basically. And so look, I mean, ideally we do both, right?
Ideally we do natural gas and the broad deregulatory agenda. I think we have to do at least one.
And then I think this possible stuff is, is, is just possible in the United States.
Yeah. I think a good analogy for this, by the way, before the conversation I was reading,
there's a good book about World War II industrial mobilization in the United States called Freedom's
Forge. And I guess when we think back on that period, especially if you're from, if you read
like the Patrick Austin fast and the progress study stuff, it's like, you had state capacity back then
and people just got you done, but now it's a cluster.
Was that all the case?
No. So it's, it was really interesting. So you have people who are from the Detroit auto
industry side, like Knudsen, who are running mobilization for the United States,
and they were extremely incompetent. But then at the same time you had
labor organization and agitation, which is actually very analogous to the climate pledges
and climate change concern we have today, where they would have these strikes while literally
into 1941, that would cost millions of man hours worth of time when we're trying to
make tens of millions, sorry, tens of thousands of planes a month or something.
And they would just debilitate factories before, you know, trivial like pennies on the dollar kind
of concessions from capital. And it was concerns that, oh, the auto companies are trying to use
the pretext of a potential war to actually prevent paying labor that money deserves.
And so the climate changes today, like you think, ah, fuck, America's fucked,
like we're not going to be able to build this shit, like if you, if you look at NEPA or something.
But I didn't realize how debilitating labor was in like World War II.
Right. It was just, you know, before at the, you know, it's sort of like 39 or whatever,
the American military was in total shambles, right? You read about it and it reads a little bit
like, you know, the German military today, right? It's like, you know, military expenditures,
I think were less than 2% of GDP, you know, all the European countries had gone, even in
peacetime, you know, like above 10% of GDP, sort of this like rapid mobilization, there's nothing,
you know, like we're making kind of like no planes, there's no military contracts,
everything had been starved during the Great Depression. But there was this latent capacity.
And, you know, at some point, the United States got their act together. I mean, the thing I'll
say is I think, you know, the supplies sort of the other way around too, to basically to China,
right? And I think sometimes people are, you know, they kind of count them out a little bit and
they're like the export controls and so on. And, you know, they're able to make seven nanometer
chips now. I think there's a question of like, how many could they make? But, you know, I think
there's at least a possibility that they're going to be able to mature that ability and make a lot
of seven nanometer chips. And there's a lot of latent industrial capacity in China, and they are
able to like, you know, build a lot of power fast. And maybe that isn't activated for AI yet.
But at some point, you know, the same way the United States and like, you know, a lot of people
in the US and the United States government is going to wake up, you know, at some point the
CCP is going to wake up. Yep. Okay. Going back to the question of presumably companies,
if are they blind to the fact that there's going to be some sort of, well, okay, so
they realize that there's going, they realize scaling is a thing, right? Obviously, their whole
plans are contingent on scaling. And so they understand that we're going to be in 2020 building
the 10 gigawatt data centers. And at this point, that's the people who can keep up our big tech
just potentially at like the edge of their capabilities, then sovereign wealth fund,
fund of things, and also big major countries like America, China, whatever. So what's their plan?
If you look at like these AI labs, what's their plan given this landscape? Do they not want the
leverage of having being in the United States? I mean, I think, I don't know, I think, I mean,
one thing the Middle East does offer is capital, but it's like America has plenty of capital, right?
It's like, you know, we have trillion dollar companies, like, what are these Middle Eastern
States? They're kind of like trillion dollar oil companies, and we have trillion dollar companies,
and we have very deep financial markets, and it's like, you know, Microsoft could issue hundreds
of billions of dollars of bonds, and they can pay for these clusters. I mean, look, I think
another argument being made, and I think it's worth taking seriously is an argument that,
look, if we don't work with the UAE, or with these Middle Eastern countries,
they're just going to go to China, right? And so, you know, we, you know, they're going to build
data centers, they're going to pour money into AI regardless, and if we don't work with them,
you know, they'll just support China. And look, I mean, I think, I think there's some merit to
the argument, and in the sense that I think we should be doing basically benefit sharing with
them, right? I think we should talk about this later, but I think basically, sort of on the road
to AGI, there should be kind of like two tiers of coalitions, should be the sort of narrow coalition
of democracies, that's sort of the coalition that's developing AGI, and then there should be a
broader coalition where we kind of go to other countries, including, you know, dictatorships,
and we're willing to offer them, you know, we're willing to offer them some of the benefits of
the AI, some of the sharing. And so it's like, look, if the UAE wants to use AI products, if they
want to run, you know, meta recommendation engines, if they want to run, you know, like
the last generation models, that's fine. I think by default, they just like wouldn't have had this
seat at the AGI table, right? And so it's like, yeah, they have some money, but a lot of people
have money. And, you know, the only reason they're getting this sort of core seat at the AGI table,
the only reason we're giving these dictators will have this enormous amount of leverage over
this extremely national security relevant technology is because we're, you know, we're kind
of getting them excited and offering it to them. You know, I think the other who like who specifically
is doing this, like just the companies who are going there to fundraise or like this is the AGI
is happening and you can find it or you can't reported that it has been reported that, you
know, Sam is trying to raise, you know, seven trillion or whatever project and it's unclear
how many of the clusters will be there and so on. But it's, you know, definitely, definitely stuff
is happening. I mean, look, I think another reason I'm a little bit at least suspicious of this
argument of like, look, if you ask doesn't work with them, they'll go to China is, you know,
I've heard heard from multiple people. And this wasn't, you know, for my time at open AI and
I haven't seen the memo, but I have heard from multiple people that, you know, at some point
several years ago, open AI leadership had sort of laid out a plan to fund and sell AGI by starting
a bidding war between the governments of, you know, the United States, China and Russia.
And so, you know, it's kind of surprising to me that they're willing to sell AGI to the Chinese
and Russian governments, but also there's something that sort of feels a bit eerily familiar about
kind of starting this bidding war and then kind of like playing them off each other. And well,
you know, if you don't do this, China will do it. So anyway, interesting. Okay, so that's pretty
fucked up, but given that that's okay. So suppose that you were right about we ended up in this
place because we got one, the way one of our friends put it is that the Middle East has
like no other place in the world, billions of dollars or trillions of dollars up for persuasion.
And it's true. And what would you have in the form of Sam Alvin?
And like, you know, the Microsoft board, it's only, it's only the dictator.
Yeah. Yeah. Okay. But so let's say you're right that you shouldn't have gotten them excited about
AGI in the first place, but now we're in a place where they are excited about AGI.
And they're like, fuck, we want us to have GPT-5, we're going to be off building super
intelligence. This Atoms for Peace thing doesn't work for us. And if you're in this place,
don't they already have the leverage, aren't you? Like, and as you might as well, just
I don't think, I think the UAE on its own is not competitive, right? It's like, I mean,
they're already export controlled. Like, you know, we're not, you know, there's like,
you're not actually supposed to ship NVIDIA chips over there, right? You know,
it's not like they have any of the leading AI labs, you know, it's like they have money,
but you know, it's actually hard to just translate money into like,
but the other things you've been saying about laying out your vision is very much,
there's this almost industrial process of you put in the compute and then you put in the
algorithms, you add that up and you get AGI on the other end. If it's something more like that,
then the case for somebody being able to catch up rapidly seems more compelling than if it's
some bespoke. Well, well, if they can steal the algorithms and if they can steal the weights,
that's really, that's really where sort of, I mean, we should talk about this. This is really
important. And I think, you know. So like right now, how easy would it be for, for an actor to
steal the things that are like, not, not the things that are released about Scarlett Johansson's
voice, but the, the RL things are talking about the unhobblings. I mean, I mean, all extremely
easy, right? You know, I, you know, deep mind, even like, you know, they, they don't make a claim
that it's hard, right? Deep, deep mind put out there, like whatever, frontier safety, something,
and they like lay out security levels and they let you know, security levels zero to four and four
is this new resilient, resistant to state actors. And they say we're at level zero, right? And then,
you know, I mean, just recently, there was like an indictment of a guy who just like stole the code,
a bunch of like really important AI code and went to China with it. And, you know, all he had to do
to steal the code was, you know, copy the code and put it into Apple notes and then export it as
PDF. And that got past their monitoring, right? And Google is the best security of any of the
iLabs probably because they have the, you know, the Google infrastructure. I mean, I think, I don't
know, roughly, I would think of this as like, you know, security of a startup, right? And like,
what is security of a startup look like, right? You know, it's not that good. It's easy to steal.
So, even if that's the case, a lot of your posters making the argument that, you know,
why are we going to get the intelligence explosion? Because if we have somebody with the
intuition of an Alec Radford, to be able to come up with all these ideas, that intuition is
extremely valuable and you scale that up. But if it's a matter of these, if it's just in the code,
that, like, if it's just the intuition, then that's not going to be just in the code, right?
And also because of export controls, these countries are going to have slightly different
hardware. You're going to have to make different trade-offs and probably rewrite things to be able
to be compatible with that, including all these things. Is it just a matter of getting the right
pen drive and you plug it into the gigawatt data center and exit the Three Gorges Dam and then
you're off to the races? I mean, like, there's a few different things, right? So one threat model
is just stealing the weights themselves. And the weights one is sort of particularly insane,
right? Because they can just, like, steal the literal, like, end product, right? Just, like,
make a replica of the atomic bomb and then they're just, like, ready to go. And, you know, I think
that one just is, you know, extremely important around the time we have AGI and superintelligence,
right? Because it's, you know, China can build a big cluster. By default, we'd have a big lead,
right? Because we have the better scientists, but we make the superintelligence, they just
steal it, they're off to the races. Weights are a little bit less important right now,
because, you know, who cares if they steal the GPT-4 weights, right? Like, whatever. And so,
you know, we still have to get started on weight security now, because, you know, look, if we think
AGI by 27, you know, this stuff is going to take a while. And it, you know, it doesn't, you know,
it's not just going to be like, oh, we need to do some access control. It's going to, you know,
if you actually want to be resistant to sort of Chinese espionage, you know, it needs to be much
more intense. The thing, though, that I think, you know, people aren't paying enough attention to is
the secrets, as you say. And, you know, I think this is, you know, the compute stuff is sexy,
you know, we talk about it. But, you know, I think that, you know, I think people underrate the
secrets, because they're, you know, I think they're, you know, the half an order of magnitude a year,
just by default, sort of algorithmic progress, that's huge. You know, if we have a few-year
lead by default, you know, that's 10, 30x, 100x bigger cluster, if we protected them.
And then there's this additional layer of the data wall, right? And so, we have to get through
the data wall. That means we actually have to figure out some sort of basic new paradigm,
sort of the AlphaGo step two, right? AlphaGo step one is learns from human imitation. AlphaGo step
two is the sort of self-play RL. And everyone's working on that right now. And maybe we're going
to crack it. And, you know, if China can't steal that, then they, you know, then they're stuck.
If they can't steal it, they're off to the braces. But whatever that thing is, is it like, literally,
I can write down on the back of a napkin? Because if it's that easy, then why is it that hard for
them to figure it out? And if it's more about the intuitions, then don't you just have to hire
Alec Radford? Like, what are you copying down? Well, I think there's a few layers to this,
right? So, I think at the top is kind of like, sort of the, you know, fundamental approach,
right? And sort of like, I don't know, on pre-training, it might be, you know, like,
you know, unsupervised learning, next token protection, train on the entire internet.
You actually get a lot of juice out of that already. That one's very quick to communicate.
Then there's like, there's a lot of details that matter. And you were talking about this earlier,
right? It's like, probably the way that thing people are going to figure out is going to be like,
somewhat obvious, or there's going to be some kind of like clear, you know, not that complicated
thing. That'll work. But there's going to be a lot of details to getting that right.
But if that's true, then again, why are we even, why do we think that getting state-level
security in these stars will prevent China from catching up? If it's just like, oh, we know some
sort of self-play RL will require to get past the data wall. And if it's as easy as you say,
in some fundamental sense. Well, I don't know if it's that easy. I mean, again.
But it's going to be solved by 2027, you say, like, right? It's like, not that hard.
I just think, you know, the U.S. and the sort of, I mean, all the leading ad labs in the United
States, and they have this huge lead. I mean, by default, you know, China actually has some good
LLMs. You know, why do they have good LLMs? They're just using the sort of open source code,
right? You know, Lama or whatever. And so the, the, I think people really underrate the sort of,
both the sort of divergence on algorithmic progress and the lead the U.S. would have by default,
because by the, you know, all this stuff was published until recently, right? Like,
Chinchilla scaling laws were published, you know, there's a bunch of MLE papers,
there's, you know, transformers and, you know, all that stuff was published. And so that's why
open source is good. That's why China can make some good models. That stuff is now, I mean,
at least they're not publishing it anymore. And, you know, if we actually kept it secret,
it would be this huge edge. To your point about sort of like some tacit knowledge,
and I like Bradford, you know, there's, there's another layer at the bottom that is something
about like, you know, large-scale engineering work to make these big training ones work.
I think that is a little bit more tacit knowledge. So I think that, but I think China will be able
to figure that out. That's like sort of engineering stuff. They're going to figure out.
Well, I can figure that out, but not how to get the RL thing working.
I mean, look, I don't know, Germany during World War II, you know, they went down the wrong path,
they did heavy water, and that was wrong. And there's actually, there's an amazing anecdote in
the making of the atomic bomb on this, right? So, so secrecy is actually one of the most
contentious issues, you know, early on as well. And, you know, part of it was sort of, you know,
zillard or whatever really thought, you know, this sort of nuclear chain reaction was possible.
And so an atomic bomb was possible. And you went around and it was like,
this is going to be of enormous strategic importance, military importance. And a lot of
people didn't believe it, or they're kind of like, well, maybe this is possible, but, you know,
I'm going to act as though it's not possible. And, you know, science should be open and all
these things. And anyway, and so these early days, so there had been some sort of incorrect
measurements made on graphite as a moderator, and that Germany had. And so they thought, you know,
graphite was not going to work, we have to do heavy water. But then Fermi made some new measurements
on graphite. And they indicated that graphite would work. You know, this is really important.
And then, you know, zillard kind of assaulted Fermi with the kind of another secrecy appeal,
and Fermi was just kind of, he was pissed off, you know, at a temper tantrum, you know, he was
like, he thought it was absurd, you know, like, come on, this is crazy. But, you know, you know,
zillard persisted, I think they roped in another guy, Pegram, and then Fermi didn't publish it.
And, you know, that was just in time, because Fermi not publishing it meant that the Nazis
didn't figure out graphite would work. They went down this path of heavy water. And that was the
wrong path. That was one of the sort of, you know, this is a key reason why this sort of German
project didn't work out. They were kind of way behind. And, you know, I think we face a similar
situation on are we are we just going to instantly leak the sort of how do we get past the data wall?
What's the next paradigm? Or are we not? So, and the reason this would matter is if there's,
like being one year ahead would be a huge advantage in the world where it's like you deploy AI
over time, and then just like, they're going to catch up anyway. I mean, I interviewed Richard
Rhodes, the guy who wrote the making an atomic bomb. And one of the anecdotes he had was when,
so they'd realized America had the bomb. Obviously, we dropped it in Japan. And Beria goes,
the guy who ran the NKBD, just a famously ruthless guy, just evil. And he goes to,
I forgot the name, but the guy, the Soviet scientist was running their version of the
Mendon project. He says, comrade, you will get us the American bomb. And the guy says, well,
listen, their implosion device actually is not optimal. We should make it a different way.
And Beria says, no, you will get us the American bomb or your family will be camp dust. But the
thing that's relevant about that anecdote is actually the Soviets would have had a better
bomb if they hadn't copied the American design, at least initially. And which suggests that often
in history, this is something that's not just for the Mendon project, but there's this pattern of
parallel invention where, because the tech tree implies that the certain thing is next, in this
case, self play, RL, whatever. Then people are just like working on that. And like people are
going to figure out around the same time. There's not, there's not going to be that much gap in
who gets it first. Wasn't like famously the bunch of people were invented something like the light
bulb around the same time and so forth. So, but is it just that like, yeah, that might be true,
but it'll be the one year or the six months or whatever. Two years makes all the difference.
I don't know if it'll be two years though. I mean, I actually, I mean, I actually think if we
locked down the labs, we have, we have much better scientists were way ahead, it would be two years.
But even, I think, even, I think, I think whether you, I think, yeah, I think even six months a
year would make huge difference. And this gets back to the sort of intelligence exploiting dynamics.
Like a year might be the difference between, you know, a system that's sort of like human level
and a system that is like vastly superhuman, right? It might be like five, five ooms, you know,
I mean, even on the current pace, right? We went from, you know, I think on the math benchmark
recently, right? Like, you know, three years ago on the math benchmark, we, you know, that was,
you know, this is a sort of really difficult high school competition math problems. You know,
we were at, you know, a few percent couldn't solve anything. Now it's solved. And that was sort of
at the normal pace of AI progress. You didn't have sort of a billion superintelligent resources,
researchers. So like a year is a huge difference. And then particularly after superintelligence,
right? Once this is applied to sort of lots of elements of R&D, once you get the sort of like
industrial explosion with the robots and so on. You know, I think a year, you know, a couple years
might be kind of like decades worth of technological progress and might, you know, again, it's like
go for one, right? 20, 30 years of technological lead, totally decisive. You know, I think it
really matters. The other reason it really matters is, you know, suppose, suppose they
steal the weight, suppose they steal the algorithms and, you know, they're close on our tails.
Suppose we still pull out a head, right? We just kind of, we were a little bit faster, you know,
we're three months ahead. I think the sort of like world in which we're really neck and neck,
you know, you only have a three month lead are incredibly dangerous, right? And we're in this
feverish struggle where like if they get ahead, they get to dominate, you know,
sort of maybe they'd get a decisive advantage. They're building clusters like crazy. They're
willing to throw all caution to the wind. We have to keep up. There's some crazy new WMDs popping up.
And then we're going to be in the situation where it's like, you know, crazy new military
technology, crazy new WMDs, you know, like deterrence, mutually disturbed instruction,
like keeps changing, you know, every few weeks. And it's like, you know, completely unstable
volatile situation is incredibly dangerous. So it's, I think, I think, you know, both,
both from just the technologies are dangerous from the alignment point of view, you know,
I think it might be really important during the intelligence explosion to have the sort of
six month, you know, wiggle room to be like, look, we're going to like dedicate more compute to
alignment during this period. So we have to get it right. We're feeling uneasy about how it's going.
And so I think in some sense of like, one of the most important inputs to whether we will
kind of destroy ourselves, or whether we will get through this just incredibly crazy period
is whether we have that buffer. Why? So before we go further object level in this,
I think it's very much worth noting that almost nobody, at least nobody I talked to,
thinks about the geopolitical implications of AI. And I think I have some object level
disagreements I will get into, but or at least things I want to iron out, I may not disagree in
the end. But the basic premise that obviously if you keep scaling and obviously people realize
that this is where intelligence is headed, it's not just going to be like the, the same old world
where like what model are we deploying tomorrow and what is the latest like people on Twitter,
like, oh, there are the GPT-4O is going to shake your expectations or whatever.
You know, COVID is really interesting because before a year or something, when March 2020 hit,
yeah, we, it became clear to the world like president, CEOs, media, average person,
there's other things happening in the world right now. But the main thing we as a world
are dealing with right now is COVID. Soon on AGI. Yeah. Okay. And then so this is the quiet period,
you know, vacation, you know, you want to like, you want to, yeah, you want to have, you know,
maybe like now is the last time you can have some kids. You know, my girlfriend sometimes
complains that, you know, that I, you know, when I'm like, you know, off doing work or whatever,
and she's like, I'm not spending time with her. She's like, you know, she threatens to replace
me with like, you know, GPT-6 or whatever. And I'm like, you know, GPT-6 will also be too busy for
doing AI research. Okay. Anyway, so what's the answer to the question of why, why, why aren't
other people talking about national security? I made this mistake with COVID, right? So I,
you know, February of 2020, and I, you know, I thought just it was going to sweep the world
and all the hospitals would collapse and it would be crazy. And then, and then, you know, and then
it'd be over. And, you know, a lot of people thought this kind of the beginning of COVID,
they shut down their offices a month or whatever. I think the thing I just really didn't price in
was the societal reaction, right? And, and within weeks, you know, Congress spent over 10% of GDP
on like COVID measures, right? The entire country was shut down. It was crazy. And so, I don't know,
I didn't price it in with COVID sufficiently. I don't know, why do people underrate it? I mean,
I think there's, there's a, there's a sort of way in which being kind of in, in the trenches
actually kind of, I think gives you a less clear picture of the trend lines. You know, you actually
have to zoom out that much only like a few years, right? But, you know, you're in the trenches,
you're like trying to get the next model to work, you know, there's always something that's hard,
you know, for example, you might underrate algorithmic progress because you're like,
ah, things are hard right now or, you know, data wall or whatever. But, you know, you zoom out
just a few years and you actually try to like count up how much algorithmic progress made in
last, you know, last few years and it's, it's enormous. But I also just don't think people
think about this stuff. Like I think smart people really underrate espionage, right? And, you know,
I think part of the security issue is I think people don't realize like how intense state level
espionage can be, right? Like, you know, you know, this is really company had had software that could
just zero click hack any iPhone, right? They just put in your number and then it's just like straight
download of everything, right? Like the United States infiltrated an air gap atomic weapons
program, right? Wild, you know, like, yeah, yeah. You know, the, you know, you know,
intelligence agencies have just stockpiles of zero days, you know, when things get really hot,
you know, I don't know, maybe we'll send special forces, right? To like, you know,
get go to the data center or something that's, you know, or, you know, I mean, China does this,
they threaten people's families, right? And they're like, look, if you don't cooperate,
if you don't give us the Intel, there's a good book, you know, along the lines of the gulag
Belovo, you know, the inside the aquarium, which is by a Soviet GRU defector.
GRU was like military intelligence. Ilya recommended this book to me. And
you know, I think reading that is just kind of like shock that have a 10 sort of state level
espionage is the whole book was about like, they go to these European countries and they try to
like get all the technology and recruit all these people to get the technology. I mean,
yeah, maybe one anecdote, you know, so when so the spot, you know, this eventual defector,
you know, so he's being trained to go to the kind of GRU spy academy. And so then to graduate
from the spy academy, sort of before you're sent abroad, you kind of had to pass a test to show
that you can do this. And the test was, you know, you had to in Moscow recruit a Soviet scientist
and recruit them to give you information sort of like you would do in the foreign country.
But of course, for whomever you recruited the penalty for giving away sort of secret
information was death. And so to graduate from the Soviet spy, the GRU spy academy,
you had to condemn my countrymen to death. States do this stuff.
I started reading the book on because I saw it in the series. Yeah. And I was actually wondering
the fact that you use this anecdote, and then you're like enough book recommended by Ilya,
is this some sort of, is this some sort of Easter egg?
We'll leave that for an exercise for the reader. Okay, so the beatings will continue until them
are all improved. So suppose that we live in the world in which these secrets are locked down,
but China so realizes that this progress is happening in America. So in that world,
especially if they realize, and I guess it's a very interesting question, probably won't be locked
down. Okay, but we're probably gonna live in the bad world. Yeah, it's gonna be really bad.
Why are you so confident that they won't be locked down? I mean, I'm not confident that
won't be locked down, but I think it's just it's not happening. And so tomorrow, the lab leaders
get the message. How hard like what do they have to do? They get the more security guards,
they like air gap the, what do they do? So again, I think, I think basically it's,
you know, I think people, there's kind of like two, two reactions there, which is like it's,
you know, we're already secure, you know, not. And there's, you know, fatalism, it's impossible.
And I think the thing you need to do is you kind of got to stay ahead of the curve of basically
how EGI pillows the CCP, right? So like right now, you've got to be resistant to kind of like
normal economic espionage. They're not, right? I mean, I probably wouldn't be talking about the
stuff that the labs were, right? They wouldn't want to wake them up more, the CCP, but they're not,
you know, this is like the stuff is like really trivial for them to do right now. I mean, it's
also anyway, so they're not resistant to that. I think it would be possible for a private company
to be resistant to it, right? So, you know, both of us have, you know, friends in the kind of like
quantitative trading world, right? And, you know, I think actually those secrets are shaped kind of
similarly where it's like, you know, you know, they've said, you know, yeah, if I got on a call
for an hour with somebody from a competitor firm, I could, most of our alpha would be gone.
And that's sort of like, that's the like list of details of like really how to, how to make
You're gonna have to worry about that pretty soon. You're gonna have to worry about that pretty
soon. Yeah. Well, anyway, and so, so all alpha could be gone. But in fact, they're alpha persists,
right? And, you know, often, often for many years and decades. And so this doesn't seem to happen.
And so I think there's like, you know, I think there's a lot you could go if you went from kind
of current startup security, you know, you just got to look through the window and you can look at
the slides, you know, it's kind of like, you know, good private sector security hedge funds,
you know, the way Google treats, you know, customer data or whatever.
I'd be good right now. The issue is, you know, basically, the CCP will also get more agi filled.
And at some point, we're going to face kind of the full force of, you know, the Ministry of State
Security. And again, you're talking about smart people underwriting espionage and sort of insane
capabilities of states. I mean, this stuff is wild, right? You know, they can get like, you know,
there's papers about, you know, you can find out the location of like where you are on a video game
map just from sounds, right? Like states can do a lot with like electromagnetic emanations, you
know, like, you know, at some point, like you got to be working from a sketch, like your cluster
needs to be air gapped and basically be a military base. It's like, you know, you need to have, you
know, intense kind of security clearance procedures for employees, you know, they have to be like,
you know, all this shit is monitored, you know, they're, you know, they basically have security
guards, you know, it's, you know, you can't use any kind of like, you know, other dependencies,
it's all got to be like intensely vetted, you know, all your hardware has to be intensely vetted.
And, you know, I think basically, if they actually really face the full force of state-level
espionage, I don't really think this is the thing private companies can do. Both, I mean,
empirically, right? Like, you know, Microsoft recently had executives emails hacked by Russian
hackers and, you know, government emails, they've hosted hacked by government actors,
but also, you know, it's basically, there's just a lot of stuff that only kind of, you know,
the people behind the security clearance is no and only they deal with. And so, you know, I think
to actually kind of resist the sort of full force of espionage, you're going to need the
government. Anyway, so I think basically, we could do it by always being ahead of the curve.
I think we're just going to always be behind the curve. And I think, you know,
maybe unless we get the sort of government project.
Okay, so going back to the naive perspective of, we're very much coming at this from,
there's going to be a race and the CCP, we must win. And listen, I understand like bad people
are in charge of the Chinese government, like the CCP and everything. But just stepping back
in a sort of galactic perspective, humanity is developing AGI. And do we want to come at this
from the perspective of we need to be China to this? Our super intelligent Jupiter brain
descendants will know who China, like China will be something like distant memory that they have,
America too. So shouldn't it be a more the initial approach to just come to them like,
listen, this is super intelligence. This is something like we come from a cooperative
perspective. Why, why immediately sort of rush into it from a hawkish competitive perspective?
I mean, look, I mean, one thing I want to say is like a lot of the stuff I talk about in the
series is, you know, is sort of primarily, you know, descriptive, right? And so I think that on
the China stuff, it's like, you know, yeah, in some ideal world, you know, we, we, you know,
it's just all, you know, merry go round in cooperation. But again, it's sort of, I think,
I think people wake up to AGI. I think the issue particular on sort of like, can we make a deal,
can we make an international treaty? I think it really relates to sort of what is the stability
of sort of international arms control dreamers, right? And so we did very successful arms control
on nuclear weapons in the 80s, right? And the reason it was successful is because the sort of
new equilibrium was stable, right? So you take go down from, you know, whatever, 60,000 nukes to 10,000
nukes, you know, when you have 10,000 nukes, you know, basically breakout, breakout doesn't matter
that much, right? Suppose the other guy now try to make 20,000 nukes, well, it's like who cares,
right? You know, like it's still mutually assured destruction. Suppose a rogue state kind of went
from zero nukes to one nukes. It's like, who cares? We still have way more nukes than you. I mean,
it's still not ideal for destabilization, but it's, you know, it'd be very different if the arms control
agreement had been zero nukes, right? Because if it had been zero nukes, then it's just like one rogue
stake makes one nuke, the whole thing is destabilized, breakout is very easy. You know, your, your
adversary state starts making nukes. And so basically when, when you're going to sort of like
very low levels of arms, or when you're going to kind of in your sort of very dynamic technological
situation, arms control is really tough because, because breakout is easy. You know, there's,
there's, I mean, there's some other sort of stories about this in sort of like 1920s, 1930s.
You know, it's like, you know, all the European states had done disarmament and Germany was,
was kind of did this like crash program to build the Luftwaffe. And that was able to like massively
destabilize things because not that, you know, they were the first, they were able to like pretty
easily build kind of a modern, you know, Air Force, because the others didn't really have one. And
that, you know, that really destabilized things. And so I think the issue with AGI and super
intelligence is the explosiveness of it, right? So if you have an intelligence explosion, if you're
able to go from kind of AGI to super intelligence, if that super intelligence is decisive, like either,
you know, like a year after, because you've developed some crazy WMD, or because you have some
like, you know, super hacking ability that lets you, you kind of, you know, completely deactivate
the sort of enemy arsenal. That means like suppose, suppose you're trying to like put in a break,
you know, like we both, we're both going to like cooperate, and we're going to go slower, you know,
on the cusp of AGI or whatever, you know, it's going to be such an enormous incentive
to kind of race ahead, to break out. And what we're just going to do in the intelligence
explosion, if we can get three months ahead, we win. I think that makes it basically,
I think any sort of arms control agreement that comes as a situation where it's close,
very unstable. That's really interesting. This is very analogous to kind of a debate I had with
Rose on the podcast, where he argued for nuclear disarmament. But if some country tries to break
out and starts developing nuclear weapons, the six months or whatever that you would get is enough
to get international consensus and invade the country and prevent them from getting nukes. And
I thought that was sort of, that's not a say like, but so on this, right? So like, maybe it's a bit
easier because you have AGI and so like you can monitor the other person's cluster or something
like data centers, you can see them from space, actually, you can see the energy draw they're
getting. There's a lot of things as you were saying, there's a lot of ways to get information
from an environment if you're really dedicated. And also because unlike a nukes, the data centers are
nukes, you have obviously the submarines, planes, you have bunkers, mountains, whatever, you have
them in so many different places. A data center that you're 100 gigawatt data center, we can blow
that shit up if you're like, we're concerned, right? Like just some cruise missile or something.
It's like very vulnerable to sabotage. I mean, that gets to the sort of, I mean, that gets to the
sort of insane vulnerability of this period post super intelligence, right? Because basically,
so you have the intelligence explosion, you have these like vastly superhuman things on your cluster,
but you're like, you haven't done the industrial explosion yet, you don't have your robots yet,
you haven't kind of, you haven't covered the desert in like robot factories yet.
And that is the sort of crazy moment where, say the United States is ahead, the CCP is
somewhat behind, there's actually an enormous incentive for a strike, right? Because if they
can take out your data center, they know you're about to have just this command and decisive
lead, they know if we can just take out this data center, then we can stop it and they might get
desperate. And, you know, so I think basically we're going to get into a position, it's actually,
I think it's going to be pretty hard to defend early on, I think we're basically going to be
in a position where protecting data centers with like the threat of nuclear retaliation.
It's like, maybe sounds kind of crazy though, you know,
Is this the inverse of the LA's or are we going to take the data centers?
Nuclear deterrence for data centers. I mean, this is a, you know, Berlin, you know, in the
like late fifties, early sixties, both Eisenhower and Kennedy multiple times kind of made the threat
a full on nuclear war against the Soviets if they tried to encroach on West Berlin.
It's sort of insane. It's kind of insane that that went well. But basically, I think that's
going to be the only option for the data centers. It's a terrible option. This whole scheme is
terrible, right? Like being being being in this like neck and neck race, sort of at this point
is terrible. And you know, it's also the, you know, I think I have some uncertainty basically on how
easy that decisive advantage will be. I'm pretty confident that if you have super intelligence,
you have two years, you have the robots, you're able to get that 30 year lead.
Look, then you're in this like go for one situation. You have your like, you know,
millions or billions of like mosquito sized drones that can just take it out. I think there's even
a possibility you can kind of get a decisive advantage earlier. So, you know, there's these
stories, you know, about these as well about, you know, like colonization and like the sort of
1500s where it was, you know, these like a few hundred kind of spanyards were able to like topple
the Aztec empire, you know, a couple, I think a couple other empires as well. You know, each of
these had a few million people and it was not like God like technological advantage. It was
some technological advantage. It was, I mean, it was some amount of disease. And then it was kind
of like cunning strategic play. And so I think there's, there's a possibility that even sort of
early on, you know, you haven't gone through the full industrial explosion yet, you have super
intelligence, but you know, you're able to kind of like manipulate the imposing generals, claim
your ally with them. Then you have, you have some, you know, you have sort of like some crazy new
bioweapons. Maybe, maybe there's even some way to like pretty easily get a paradigm that like
deactivates enemy nukes. Anyway, so I think this stuff could get pretty wild. Here's what I think
we should do. I really don't want this volatile period. And so I deal with China would be nice.
It's going to be really tough if you're in this unstable equilibrium.
I think basically we want to get in a position where it is clear that the United States that
a sort of coalition of democratic allies will win is clear the United States to declare to China,
you know, that will require having locked down the secrets that will require having built the
100 gigawatt cluster in the United States and having done the natural gas and doing what's
necessary. And then when it is clear that the democratic coalition is well ahead,
then you go to China and then you offer them a deal. And you know, China will know they're
going to win. This is going to be, they're very scared of what's going to happen. We're going to
know we're going to win, but we're also very scared of what's going to happen because we really want
to avoid this kind of like breakneck, breakneck race right at the end. And where things could
really go awry. And you know, and then, and so then we offer them a deal. I think there's an
incentive to come to the table. I think there's a sort of more stable arrangement you can do.
It's a sort of an Adams for peace arrangement. And we're like, look, we're going to respect you.
We're not, we're not going to like, we're not going to use super intelligence against you.
You can do what you want. You're going to get your like, you're going to get your slice of the
galaxy. We're going to like, we're going to benefit share with you. We're going to have some like
computer agreement where it's like, there's some ratio of compute that you're allowed to have.
And that's like enforced with our like opposing AI's or whatever. And we're just not going to do,
we're just not going to do this kind of like volatile sort of WMD arms race to the death.
We're good.
And sort of it's like a new world order that's US led, that sort of democratic led,
but that respects China. Let's do what they want.
Okay. There's so much to, there's so much there. First on the galaxies thing. I think it's just
a funny anecdote. So I want to kind of want to tell it. And this, we're at an event and I'm
respecting China. I'm house rules here. I'm not revealing anything about it, but we're talking
to somebody or a leopold was talking to somebody influential. Afterwards, that person asked the
group, Leopold told me that he wants, he's not going to spend any money on consumption until
he's ready to buy galaxies. And he goes, the guy goes, I honestly don't know if he meant
galaxies like the brand of private plane galaxy or the physical galaxies. And there was an actual
debate. Like he went away to the restroom and there was an actual debate among people who are
very influential about, you can't amend galaxies and other people who knew you better be like,
no, he means galaxies. I mean the galaxies. I mean the galaxies. I mean, I think it'll be interesting.
I mean, I, yeah, I think there's a, I mean, there's two ways to buy the galaxies. One is like,
at some point, you know, it's like post-superintelligence, you know, they're so crazy.
But by the way, I love, okay. So what happens is he's on the ground. I'm laughing at my ass off.
I'm not even saying I think people were like, gaping this debate. And then the leopold comes back
and the guy, somebody's like, oh, leopold, we're having this debate about whether you meant
you want to buy the galaxy or you want to buy the other thing. And leopold assumes they must mean
not the private plane galaxy versus the actual galaxy. But do you want to buy the property
rights of the galaxy or actually just send out the probes right now? Exactly.
Exactly. Oh my God. All right. Back to China.
There's a whole bunch of things that I could ask about the, that plan about whether you're
going to get credible, promised. You will get some part of the galaxies where they care about that.
I mean, you have the eyes to help you enforce stuff. Okay, sure. We'll leave that aside. That's
a different rabbit hole. The thing I want to ask is, but it has to be the thing we need.
The only way this is possible is if we lock it down. I see. If we don't lock it down, we are in
this fever struggle. Greatest peril mankind will have ever seen. So, but given the fact that in
during this period, instead of just taking their chances and they don't really understand how this
AI governance scheme is going to work, where they're going to check whether we had to actually get
the galaxies, the data centers, they can't be built underground. They have to be built above
ground. Taiwan is right off the coast of us. They need the chips from there. Why aren't we just going
to invade? Listen, we don't want like worst case scenario is they win the super intelligence,
which they're on track to do anyways. Wouldn't this instigate them to either invade Taiwan or
blow up the data center in Arizona or something like that? Yeah. I mean, look, I mean, you talked
about the data center one and then, you know, you probably have to like threaten nuclear retaliation
to protect that. They might also just blow it up. There's also maybe ways they can do it without
sort of attribution, right? Like you. Stuxnet. Stuxnet. Stuxnet. Yeah. I mean, this is, I mean,
this is part of, we'll talk about this later, but you know, I think, look, I think we need to be
working on the Stuxnet for the Chinese project, but the, but by the way, Taiwan, I mean, Taiwan,
the Taiwan thing, the, you know, you know, I talk about, you know, AGI by, you know, 27 or whatever.
Do you know about the like terrible 20s? No. I mean, sort of in the sort of Taiwan watchers
circles, people often talk about like the late 2020s as like maximum period of risk for Taiwan,
because sort of like, you know, military modernization cycles and basically extreme fiscal
tightening on the military budget in the United States over the last decade or two,
has meant that sort of we're in this kind of like, you know, trough in the late 20s of like,
you know, basically overall naval capacity. And, you know, that's sort of when China is saying
they want to be ready. So it's already kind of like, it's kind of pitching, you know,
there's some sort of like, you know, parallel timeline there. Yeah, look, it looks appealing to
invade Taiwan. I mean, maybe not because they, you know, basically remote cutoff of the chips.
And so then it doesn't mean they get the chips, but it just means they, you know, it's just,
it's, you know, the machines are deactivated. But look, I mean, imagine if during the Cold War,
you know, all of the world's uranium deposits had been in Berlin, you know, and Berlin was already,
I mean, almost multiple times it was caused a nuclear war. So God help us all.
Well, the Groves had a plan after the after the war, that the plan was that America would go around
the world and getting the rights to every single uranium deposit, because they didn't realize how
much uranium there was in the world. And they thought this was the thing that was feasible.
Not realizing, of course, that there was like huge deposits in the Soviet Union itself.
Okay. East Germany too. There's always, there's a lot of East German workers who kind of got
screwed and got cancer. Okay, so the framing we've been talking about that we've been assuming,
and I'm not sure I buy yet, is that the United States, this is our leverage, this is our data
center, China is the competitor. Right now, obviously, that's not the way things are progressing.
Private companies control these AIs, they're deploying them, it's a market-based thing.
Why will it be the case that it's like the United States, it has this leverage,
or is doing this thing versus China is doing this thing?
Yeah, I mean, look, look on the project, you know, I mean, there's sort of descriptive and
prescriptive claims or sort of normative positive claims. I think the main thing I'm trying to say
is, you know, you know, look, we're at these SF parties or whatever. And I think people talk
about AGI, and they're always just talking about the private AI labs. And I think I just really
want to challenge that assumption. It just seems like, seems pretty likely to me, you know, as we've
talked about, for reasons we've talked about, that look like the national security state is
going to get involved. And, you know, I think there's a lot of ways this could look like, right,
is it like nationalization? Is it a public-private partnership? Is it a kind of defense-controlect
or like a relationship? Is it a sort of government project that's to support all the people?
And so there's a spectrum there. But I think people are just vastly underrating the chances of
this more or less looking like a government project. And look, I mean, look, if, you know,
it's sort of like, you know, do you think, do you think like we all have literal, like, you know,
when we have like literal superintelligence on our cluster, right? And it's like, you know,
you have 100 billion, they're like, sorry, you have a billion like superintelligence scientists
that they can like hack everything, they can like stuxnet the Chinese data centers, you know,
they're starting to build the robo armies, you know, you like, you really think they'll be like
a private company and the government would be like, oh my God, what is going on? You know, like,
yeah. Suppose there's no China. Suppose there's people like Iran, North Korea, who theoretically,
at some point, will be able to do superintelligence, but they're not on our heels and don't have the
ability to be on our heels. In that world, are you advocating for the national project? Or do
you prefer the private path forward? Yes, I mean, two responses to this. One is, I mean, you still
have like Russia, you still have these other countries, you know, you've got to have Russia
approved security, right? It's like, you can't, you can't just have Russia steal all your stuff.
And like, maybe their clusters aren't going to be as big, but like, they're still going to be able
to make the crazy bio weapons and that, you know, the mosquito size drones, you know, and so on.
And so, I mean, I think, I think, I think the security component is just actually a pretty
large component of the project in the sense of like, I currently do not see another way
where we don't kind of like instantly proliferate this to everybody. And so yeah, so I think it's
sort of like, you still have to deal with Russia, you know, Iran, North Korea, and you know, like,
you know, Saudi and Iran are going to be trying to get it because they want to screw each other
and you know, Pakistan and India, they want to screw each other. There's like this enormous
destabilization still. That said, look, I agree with you, if you know, if you know, by some somehow
things are checking out differently, I'm like, you know, AGI would have been in 2005, you know,
sort of like unparalleled in American hegemony. I think there would have been more scope for
less government involvement. But again, you know, as we're talking about earlier, I think that would
have been sort of this like very unique moment in history. And I think basically, you know,
almost all other moments in history, there would have been the sort of great power comp
competitor. So, okay, so let's get into this debate. So my position here is if you look at
the people who are involved in the Manhattan Project itself, many of them regretted their
participation, as you said. Now, we can infer from that that we should sort of start off with a
cautious approach to the nationalized ASI project. Then you might say, well, listen, obviously the
super... Did they regret their participation because of the project or because of the technology
itself? I think people will regret it. But I think it's about the nature of the technology and it's
not about the project. I think they also probably had a sense that different decisions would have
been made if it wasn't some concerted effort that everybody had agreed to participate in,
that if it wasn't in the context of this, we need to race to beat the Germany and Japan,
you might not develop... So that's a technology part, but also like you wouldn't actually like
hit them with... It's like the sort of the destructive potential, the sort of military
potential, it's not because of the project, it is because of the technology. And that will unfold
regardless. But I think this underrates the power of modeling. Imagine you go through like the 20th
century in like a decade, it's just the sort of, yes, great technological progress.
Let's just actually run with that example. Suppose you actually... There was some reason
that the 20th century would be run through in one decade. Do you think the cause of that should
have been... Then like the technologies that happened through the 20th century shouldn't have
been privatized? That it should have been a more sort of concerted government led project?
You know, look, there is a history of just dual use technologies. And so I think AI in some sense
is going to be dual use in the same way. And so there's going to be lots of civilian uses of it.
Like nuclear energy itself, there's the government project developed the military
angle of it. And then it was like, then the government worked with private companies,
there's a sort of like real flourishing of nuclear energy until the environment will
have stopped it. Planes, like Boeing, actually the Manhattan project wasn't the biggest defense
R&D project during World War II. It was the B-29 bomber, right? Because they needed the
bomber that had long enough range to reach Japan to destroy their cities. And then Boeing made
some Boeing made that, B Boeing made the B-47, made the B-52, the plane the US military uses today.
And then they used that technology later on to build the 707 and the sort of the...
But what does later on mean in this context? Because in the other... I get what it means after
a war to privatize, but if you have the government has ASI, maybe just let me back up and explain
my concern. So you have the only institution in our society which has a monopoly on violence.
And then we're going to give it some... In a way that's not broadly deployed,
access to the ASI. The counterfactual, and this maybe sounds silly, but listen,
we're going to go through higher and higher levels of intelligence. Private companies will be
required by regulation to increase their security, but they'll still be private
companies and they'll deploy this and they're going to release the AGI. Now McDonald's and
JP Morgan and some random startup are now more effective organizations because they have a bunch
of AGI workers. And it'll be sort of like the industrial revolution in the sense that the
benefits were widely diffused. If you don't end up in a situation like that, then the...
I mean, even backing up, like what is it we're trying to... Why do we want to win against China?
We want to win against China because we don't want a top-down authoritarian system to win.
Now, if the way to beat that is that the most important technology that humanity will have
has to be controlled by a top-down government, what was the point? Maybe... So let's run our cards
with privatization. That's the way we get to the classic liberal market-based system we want for
the ASIs. All right. So a lot of talk about here. I think... Maybe I'll start a bit about
actually looking at what the private world would look like. And I think this is part of where the
sort of there's no alternative comes from. And then let's look at what the government project
looks like, what checks and balances look like, and so on. All right. Private world.
I mean, first of all... Okay. So a lot of people right now talk about open source. And I think
there's this sort of misconception that AGI development is going to be like, oh, it's going
to be some beautiful decentralized thing and some giddy community of coders who gets to collaborate
on it. That's not how it's going to look like, right? It's $100 billion, trillion dollar cluster.
It's not going to be that many people that have it. The algorithms, it's like right now, open
source is kind of good because people just use the stuff that was published. And so they basically,
the algorithms were published. Or as MISTROL, they just kind of leave deep mind and take all
the secrets with them and they just kind of replicate it. But that's not going to continue
to be in the case. And so the sort of open source... I mean, also people say stuff like,
10, 26 flops, it'll be in my phone. No, it won't. It's like Moore's Law is really slow.
I mean, AGI chips are getting better. But the $100 billion computer will not cost
like $1,000 within your lifetime or whatever, aside from me. So anyway, it's going to be
two or three big players on the private world. And so look, a few things. So first of all,
you talk about the sort of enormous power that superintelligence will have and the government
will have. I think it's pretty plausible that the alternative world is that one AI company
has that power. And specifically, if we're talking about lead, it's like what? I don't know,
opening AI has a six month lead. And then, so then you're not talking, you're talking about basically
the most powerful weapon ever. And it's, you're kind of making this like radical bet on like a
private company CEO as the benevolent dictator. No, no, not necessarily. Like any other thing
that's privatized, we don't account on them being benevolent. We just... Look, to think of, for
example, somebody who manufactures industrial fertilizer, the person with this factory,
if they went back to an ancient civilization, they could like blow up Rome. They could probably
blow up Washington DC. And I think in their series, you talk about Tyler Cowan's phrase
of muddling through. And I think even with privatization, people sort of underrate that
there are actually a lot of private actors who have the ability to like, there's a lot of people
who control the water supply or whatever. And we can count on cooperation and market based
incentives to basically keep a balance of power. I get that things are proceeding really fast.
But we have a lot of historical evidence that this is the thing that works best.
So look, I mean, what do we do with nukes, right? The way we keep the sort of nukes in
check is not like, you know, a sort of beefed up second amendment where like each state has
their own like little nuclear arsenal and like, you know, Dario and Sam have their own little
nuclear arsenal. No, no, it's like, it's institutions, it's constitutions, it's laws, it's courts.
And so I don't actually, I'm not sure that this, you know, I'm not sure that the sort of balance
of power analogy holds. In fact, you know, sort of the government having the biggest guns was
sort of like an enormous civilizational achievement, right? Like Landfrieden in the sort of Holy Roman
Empire, right? You know, if somebody from the town over kind of committed a crime on you,
you know, you didn't kind of start a sort of a, you know, a big battle between the two towns. No,
you take it to a court of the Holy Roman Empire, and they would decide. And it's a big achievement.
Now, the thing about, you know, the industrial fertilizer, I think the key difference is kind
of speed and offense defense balance issues, right? So it's like 20th century, and you know,
10 years in a few years, that is an incredibly scary period. And it is incredibly scary, you know,
because it's, you know, you're going through just this sort of enormous array of destructive
technology and the sort of like enormous amount of like, you know, basically military advance,
I mean, you would have gone from, you know, kind of like, you know, you know, bayonets and horses
to kind of like tank armies and fighter jets in like a couple years, and then from, you know, like,
you know, and then to like, you know, nukes and you know, ICBMs and still, you know,
it's just like in a matter of years. And so it is sort of that speed that creates,
I think basically the way I think about it is there's going to be this initial just
incredibly volatile, incredibly dangerous period. And somehow we have to make it through that.
And that's going to be incredibly challenging. That's where you need the kind of government
project. If you can make it through that, then you kind of go to like, you know, now we can,
now, you know, the situation has been stabilized, you know, we don't face this imminent national
security threat. You know, it's like, yes, there were kind of WMDs that came along the way,
but either we've managed to kind of like, have a sort of stable offense defense balance, right?
Like I think bioweapons initially are a huge issue, right? Like an attacker can just create like
a thousand different aesthetic, you know, viruses and spread them. And it's like going to be really
hard for you to kind of like, make a defense against each. But maybe at some point, you figure
out the kind of like, you know, universal defense against every possible virus. And then you're
in a stable situation and on the offense defense balance, or you do the thing, you know, you do
with planes, where it's there's like, you know, there's certain capabilities that the private
sector isn't allowed to have. And you've like figured out what's going on, restrict those. And
then you can kind of like, let, let, you know, you let the sort of civilian, civilian uses.
So I'm skeptical of this because well, there's sorry, I mean, the other important thing is,
so I talked about the sort of, you know, maybe it's like, it's, it's a, you know,
it's, you know, it's one company with all this power. And I think it is like, I think it is
unprecedented because it's like the industrial fertilizer guy cannot overthrow the US government.
I think it's quite plausible that like the AI company with super intelligence can overthrow
the multiple companies, right? And I buy that one of them could be ahead.
So it's not obvious that it'll be multiple. I think it's again, if there's like a six-month
lead, maybe, maybe there's two or three, but if there's two or three, then what you have is
just like a crazy race between these two or three companies, you know, it's like, you know,
whatever, Demis and Sam, they're just like, I don't want to let the other one win. And,
and they're both developing their nuclear arsenals in the road. And it's just like,
also like, come on, the government is not going to let these people, you know,
are they going to let like, you know, is Dario going to be the one developing the kind of like,
you know, you know, super hacking Stuxnet and like deploying against the Chinese data center.
The other issue though, is it won't just, if it's two or three, it won't just be two or three.
There'll be two or three, and it'll be China and Russia and North Korea, because the private
and the private lab world, there's no way they'll have security that is good enough.
I think we're also assuming that somehow if you nationalize it, like the security just,
especially in the world, where it did this stuff is priced in by the CCP, that now you've like,
got it nailed down. And I'm not sure why we would expect that to be the case, but on this
government's the only one who does stuff. So if it's not Sam or Dario, who's we don't
want to trust them to be benevolent, dictate or whatever. So by here, we're counting on
if it's because you can cause a coup, the C capabilities are going to be true of the
government project, right? And so the modal president in 2020, 2025, but Donald Trump will
be the person that you don't trust Sam or Dario to have these capabilities. And why okay, I agree
that like I'm worried if the Sam or Dario have a one year lead on ASI in that world, then I'm
like concerned about this being privatized. But in that exact same world, I'm very concerned about
Donald Trump having the capability. And potentially if we're living in a world where
the takeoff is slower than you anticipate, in that world, I'm like very much I want the private
company. So like in no part of this matrix is obviously true that the government project is
better than the private project. Let's talk about the government project a little bit and checks
and balances. In some sense, I think my argument is a sort of Berkian argument, which is like
American checks and balances have held for over 200 years. And through crazy technological
revolutions, US military could kill like every civilian in the United States.
But you're going to make that argument, the private public balance of power itself for hundreds of
years. But yeah, why has it held? Because the government has the biggest guns and has never
before has a single CEO or a random nonprofit board had the ability to launch nukes. And so again,
it's like, you know, what is the track record of the government checks and balances versus the
track record of the private company checks and balances? Well, the iLab, you know, like first
stress test, you know, went really badly, you know, that didn't really work, you know? I mean,
even worse in the sort of private company world. So it's both like, it is like the two private
companies and the CCP and they just like instantly have all the shit. And then it's, you know,
they probably won't have good enough internal control. So it's like, not just like the random
CEO, but it's like, you know, rogue employees that can kind of like use these super intelligences to
do whatever they want. And this won't be true of the government? Like the rogue employees won't
exist on the project? Well, the government actually like, you know, has decades of experience and
like actually really cares about the stuff. I mean, it's like they deal, they deal with nukes,
they deal with really powerful technology. And it's, you know, this is like, this is the stuff
that the national security state cares about. You know, again, to the guy, let's talk about the
government checks and balances a little bit. So, you know, what are checks and balances in the
government world? First of all, I think it's actually quite important that you have some
amount of international coalition. And I talked about these sort of two tiers before,
basically, I think the inner tier is a sort of modeled on the cool record agreement, right? This
was like Churchill and Roosevelt, they kind of agreed secretly, we're going to like pull our
efforts on nukes, but we're not going to use them against each other and we're not going to use them
against anyone else with their consent. And I think basically look, bring in, bring in the UK,
they have deep mind, bring in the kind of like Southeast Asian states who have the chip supply
chain, bring in some more of kind of like NATO close democratic allies for, you know, talent and
industrial resources. And you have this sort of like, you know, so you have, you have those checks
and balances in terms of like more international countries at the table. Sorry, somewhat separately,
but then you have the sort of second tier of coalitions, which is the sort of Adams for peace
thing, where you go to a bunch of countries, including like the UAE, and you're like, look,
we're going to basically like, you know, there's a deal similar to like the NPT stuff, where it's
like, you're not allowed to like do the crazy military stuff, but we're going to share the
civilian applications, we're in fact going to help you and share the benefits and, you know,
sort of kind of like this new sort of post superintelligence world order. All right,
US checks and balances, right? So obviously, Congress is going to have to be involved, right,
appropriate in trillions of dollars. I think probably ideally you have Congress needs to
kind of like confirm whoever's running this. So you have Congress, you have like different
factions of the government, you have the courts, I expect the First Amendment to continue being
really important. And maybe that I think that sounds kind of crazy to people, but I actually
think again, I think these are like institutions that have held its test of time in a really sort
of powerful way. You know, eventually, you know, this is why honestly, alignment is important is
like, you know, the AIs, you program the AIs to follow the Constitution. And it's like, you know,
why does the military work? It's like generals, you know, are not allowed to follow unlawful
orders, they're not allowed to follow unconstitutional orders, you have the same thing for the AIs.
So what's wrong with this argument? When you say, listen, maybe you have a point in the world where
we have extremely fast takeoff, it's like one year from AGI to ASI. Yeah. And then you have the like
years after VSI, where you have this like extraordinary explosion.
Sure. Maybe you have a point. We don't know, you have these arguments, we'll like get into
the weeds on them about why that's a more likely world, but like maybe that's not the world we
live in. And in the other world, I'm like very on the side of making sure that these things are
privately held. Now, when you nationalize, that's a one way function, you can't go back.
Why not wait until we have more evidence on which of those worlds we live in?
Why, I think like rushing on the nationalization might be a bad idea while we're not sure.
And okay, I'll just respond to that first. I mean, I don't expect us to nationalize tomorrow.
If anything, I expect it to be kind of with COVID where it's like kind of too late, like
ideally you nationalize it early enough to like actually lock stuff down. It'll probably be kind
of chaotic and like, you're going to be trying to like do this crash program to lock stuff down
and it'll be kind of late, it'll be kind of clear what's happening. We're not going to nationalize
when it's not clear what's happening. I think the whole bow, the whole historically institutions
have held up well. First of all, they've actually almost broken a bunch of times. It's like this
is... They didn't break the first time they tested.
That's some people who are saying that we shouldn't be that concerned about nuclear war
say where it's like, listen, we have the nuke for 80 years and like we've been fine so far,
so the risk must be low. And then the answer to that is no, actually, it is a really high risk.
And the reason we've avoided it is like people have gone through a lot of effort to make sure
that this thing doesn't happen. I don't think that giving government ASI without knowing what
that implies is going through the lot of effort. And I think the base rate, like you can talk about
America. I think America is very exceptional, not just in terms of dictatorship, but in terms
of every other country in history has had a complete drawdown of wealth because of war,
revolution, something. America is very unique in not having that. And the historical base rate,
we're talking about Greek power competition. I think that has a really big... That's something
we haven't been thinking about the last 80 years, but it's really big. Dictatorship is also something
that is just the default state of mankind. And I think relying on institutions, which in an ASI
world, it's fundamentally right now, if the government tried to overthrow, it's much harder
if you don't have the ASI, right? There's people who have AR-4 or 15s, and there's like things
that have to make it harder. You don't make it crush the AR-15s. No, I think it actually be pretty
hard. The reason it was Vietnam and Afghanistan were pretty hard. They're just a cool country.
Yeah, yeah, I agree, but I'm... They could. I mean, similar with ASI.
Yeah, I think it's just easier if you have what you're talking about.
But there are institutions, there are constitutions, there are legal restraints,
there are courts, there are checks and balances. The crazy bet is the bet which your private
company CEO... The same thing, by the way, isn't the same thing true of nukes, where we have these
institutional agreements about non-proliferation and whatever? And we're still very concerned
about that being broken and somebody getting nukes, and you should stay up that night worrying
about that. Precarious situation. But ASI is going to be a really precarious situation as well,
and given how precarious nukes are, we've done pretty well.
And so what does privatization in this world even mean?
I mean, I think the other thing is...
Like what happens after?
I mean, the other thing, because we're talking about whether the government project is good or
not, and it's like, I have very mixed feelings about this as well. Again, I think my primary
argument is if you're at the point where this thing has vastly superhuman hacking capabilities,
if you're at the point where this thing can develop bio-weapons, like increasing bio-weapons,
ones that are targeted, can kill everybody, but the Han Chinese would wipe out entire countries.
Where you're talking about building robo-armors, you're talking about kind of like drone swarms
that are, again, the mosquito-sized drones that could take it out.
The United States national security state is going to be intimately involved with this,
and this will... And I think, again, the government... A lot of what I think is the
government project looks like, it is basically a joint venture between the cloud providers,
between some of the labs and the government. And so I think there is no world in which the
government isn't intimately involved in this crazy period. The very least, basically,
the intelligence agencies need to be running security through these labs. So they're already
kind of like, they're controlling everything, they're controlling access to everything.
Then they're going to be like, probably, again, if we're in this really volatile
international situation, a lot of the initial applications, it'll suck. It's not what I want
to use ASI for. We'll be trying to somehow stabilize this crazy situation. Somehow we need
to prevent the proliferation of some crazy new WMDs and the undermining of mutually assured
destruction to kind of North Korea and Russia and China. And so I think... I basically think your
world... I think there's much more spectrum than you're acknowledging here. And I think,
basically, the world in which it's private labs is extremely heavy government involvement.
And really, what we're debating is what form of government project, but it is going to look much
more like the national security state than anything it does look like, like a startup,
as it is right now. And I think the... Yeah.
Okay. I think something like that makes sense. I would be... If it's like the Manhattan Project,
then I'm very worried, where it's like, this is part of the US military, where if it's more like,
listen, you got to talk to Jake Sullivan before you run the next training run.
It's like Lockheed Martin, Skunkard's part of the US military. It's like, they call the shots.
Yeah. I don't think that's great. I think that's bad. I think it would be bad if that happened
with ASI. And what is the scenario? What is the alternative?
Okay. So it's closer to my end of the spectrum, where, yeah, you do have to talk to Jake Sullivan
before you can launch the next training cluster. But there's many companies who are still going
for it. And the government will be intimately involved in the security. But the three different
companies are trying to... Is Starry launching the Stuxnet attack?
Yeah. What do you mean? Is it launching? Okay.
Starry is activating the Chinese data centers.
I think this is similar to the story you could tell about, there's a lot of companies,
like literally Big Tech right now. I think Sasha, if you wanted to, he probably could get his
engineers like, what are the zero days in Windows? And how do we get infiltrate the president's
computer so that we can... Maybe shut down.
No, no, no. But like right now, I'm saying Sasha could do that, right? Because he knows...
Maybe shut down.
What do you mean?
Government wouldn't let them do that.
Yeah. I think there's a story you could tell where they could pull off a coup or whatever.
But like, I think there's like multiple companies...
They could pull off a coup.
Okay, what?
They could pull off a coup.
Come on.
Okay, fine, fine, fine. I agree. I'm just saying something closer to...
So what's wrong with the scenario where the government is... There's like multiple companies
going for it. But the AI is still broadly deployed and alignment works in the sense that you can
make sure that it's not... The system level prompt is like, you can't help people make bio weapons
or something. But these are still broadly deployed so that...
I mean, I expect the AIs to be broadly deployed. I mean, first of all...
Even if it's a government project?
Yeah. I mean, look, I think first of all, I think the matters of the world,
open sourcing their eyes that are two years behind or whatever. Yeah, super valuable role.
They're going to like... And so there's going to be some question of either the
offense, defense balance is fine. And so even if they open source two-year-old AIs,
it's fine. Or it's like there's some restrictions on the most extreme dual use capabilities.
Like, you don't let private companies sell kind of crazy weapons.
And that's great. And that will help with the diffusion.
And after the government project, there's going to be this initial tense period,
hopefully that's stabilized. And then look, yeah, like Boeing,
they're going to go out and they're going to do all the flourishing civilian applications.
And like nuclear energy, all the civilian applications will have their day.
I think part of my argument here is that...
And how does that proceed? Because in the other world, there's existing stocks of capital
that are worth a lot.
Yeah, the clusters, they'll be still be Google clusters.
And so Google, because they got the contract from the government,
they'll be the ones that control the AIs. But why are they trading with any
anybody else? Why is there a random startup again?
It'll be the same companies that would be doing it anyway.
But in this world, they're just contracting with the government
or like their DPA for all their compute goes to the government.
And it's very natural. It's like sort of how other U.S. taxes work.
After you get the AIs, and then we're building the robot armies and building fusion reactors or
whatever, that's... Only the government will get to build robot armies.
Yeah, that one worried. Or like the fusion reactors and stuff.
Because it's the same situation we have today.
Because if you already have the robot armies and everything,
like the existing society doesn't have some leverage where
it makes sense for the government to...
But they don't have that today.
Yeah, in the sense that there's like a lot of capital that the government wants,
and there's other things. Like why was Boeing privatized after?
Government has the biggest guns. Government has the biggest guns.
And the way we regulate is institutions, constitutions, legal restraints, courts.
Tell me what privatization should look like in the ESI world.
Afterwards.
Afterwards. Like the Boeing example, right? It's like you have this government...
Who gets it? Like Google, Microsoft...
And who are they selling it to? Like they already have the robot factories.
And then look at...
Why are they selling it to us? Like they already have the...
They don't need like our... This is chum change in the ESI world.
Because we didn't get like the ESI broadly deployed throughout this takeoff.
So we don't have the robot... We don't have like the fusion reactors in whatever
advanced... Decades of advanced science that you were talking about.
So like it just... What are they trading with us for?
Trading with whom for?
Everybody who was not part of the project.
They've got that technology that's decades ahead.
Yeah, I mean, look, that's a whole other issue of like,
how does like economic distribution work or whatever?
I don't know. That'll be rough.
Yeah, I think... I'm just saying, I don't...
Basically, I'm kind of like, I don't see the alternative.
The alternative is you like overturn a 500 year civilizational achievement of
Lundfleet and you basically instantly leak the stuff to the CCP.
And either you like barely scrape out ahead and...
But you're in this feverish struggle.
You're like proliferating crazy WMDs.
It's just like enormously dangerous situation,
enormously dangerous on alignment.
Because you're in this kind of like crazy race at the end
and you don't have the ability to like take six months to get alignment right.
The alternative is like you aren't actually bundling your efforts
to kind of like win the race against the authoritarian powers.
Yeah, and so I don't like it.
I wish the thing we use the ASI for is to like cure the diseases
and do all the good in the world.
But it is my prediction that sort of like by the end game,
what will be at stake will not just be kind of cool products,
but what will be at stake is like whether liberal democracy survives,
like whether the CCP survives,
like what the world order for the next century will be.
And when that is at stake, forces will be activated
that are sort of way beyond what we're talking about now.
And like in the sort of like crazy race at the end,
the sort of national security implications will be the most important.
Sort of like World War II.
It's like, yeah, nuclear energy, how did it stay?
But in the initial kind of period, when this technology was first discovered,
you had to stabilize the situation, you had to get nukes,
you had to do it right.
And then the civilian applications had their day.
I think of closer analogy to what this is,
because I agree that nuclear energy is the thing that happens when you're on it.
It's like dual use in that way.
But it's something that happened literally a decade after nuclear weapons were developed.
Yeah, because everything took on this way.
Whereas with AI, like immediately all the applications are unlocked
and it's closer to literally, I mean, this is analogy people
are supposed to make in the context of AGI is like,
assume your society had 100 million more John von Neumanns.
And I don't think like, if that was literally what happened,
if tomorrow you just have 100 million more of them,
the approach should have been, well, some of them will convert to ISIS.
And we need to like be really careful about that.
And then like, oh, you know, like what if a bunch of them are born in China?
And then we like, if we got to nationalize the John von Neumanns,
I'm like, no, I think it'll be generally a good thing.
And I'd be concerned about one power had getting like all the John von Neumanns.
I mean, I think the issue is the sort of like bottling up
in the sort of intensely short period of time,
like this enormous sort of like, you know,
unfolding of technological progress of an industrial explosion.
And I think we do worry about the 100 million John von Neumanns.
It's like rise of China.
Why are we worried about the rise of China?
Because it's like 100 billion people
and they're able to do a lot of industry and do a lot of technology.
And but it's just like, you know, the rise of China times like, you know, 100,
because it's not just 101 billion people.
It's like a billion super intelligent, crazy, you know, crazy things.
And in like, you know, very short period.
Let's talk practically, because if the goal is we need to beat China,
part of that is protecting...
I mean, that's one of the goals, right?
Yeah, I agree. I agree.
Well, one of the goals is to beat China.
And also manage this incredibly crazy, scary period.
Yeah.
Right.
So part of that is making sure we're not leaking our golden secrets to them.
Yep.
Part of that is...
Part of that is a trillion dollar cluster.
I mean, building the trillion dollar cluster, right?
Yeah, but like...
Is in your whole point that Microsoft can release corporate bonds that are...
I think Microsoft can do the like hundreds of billions of dollars cluster.
I think the trillion dollar cluster is closer to a national effort.
I thought that your earlier point was that American capital markets are deep and so forth.
They're good. They're pretty good.
I mean, I think the trillion...
I think it's possible it's private.
It's possible it's private.
But it's going to be like, you know...
By the way, at this point, we have AGI that's rapidly accelerating productivity.
I think the trillion dollar cluster is going to be planned before before the AGI.
I think it's sort of like, you get the AGI on the like 10 gigawatt cluster,
maybe you have like one more year where you're kind of doing some final on hobbling
to fully unlock it.
Then you have the intelligence explosion.
And meanwhile, the trillion dollar cluster is almost finished.
And then you do your super intelligence on your trillion dollar cluster
or you run it on your trillion dollar cluster.
And by the way, you have not just your trillion dollar cluster,
but like hundreds of millions of GPUs on inference clusters everywhere.
And this isn't result...
Like, I think private...
In this world, I think private companies have the capital and can raise capital to do it.
Then you will need the government force to do it fast.
Well, I was just about to ask, wouldn't it be the...
We know private companies are on track to be able to do this
and be China if they're unhindered by climate pledges or whatever.
Well, that's part of what I'm saying.
So if that's the case and if it really matters that we be China,
there's all kinds of practical difficulties of like,
will the AI researchers actually join the AI effort?
If they do, there's going to be three different teams at least
who are currently doing pre-training on different companies.
Now, who decides...
At some point, you're going to have YOLO, the hyperparameters
of the trillion dollar cluster.
YOLO, the hyperparameters of the big round.
Who decides that?
Just like merging extremely complicated research and development processes
across very different organizations.
This is somehow as simple as to speed up America against the Chinese.
Like, why don't we just let...
Brain and deep mind merge.
It was like a little messy, but it was fine.
It was pretty messy and it was also the same company
and also much earlier on in the process.
I mean, pretty similar, right?
Same code, different code bases and like lots of different infrastructure
and different teams.
And it was like, it wasn't the smoothest of all processes,
but deep mind is doing I think very well.
I mean, look, you give the example of COVID
and the COVID example was like, listen, we woke up to it,
maybe it was late, but then we deployed all this money.
And COVID response to government was a clusterfuck over.
And like the only part of it that was worked
is I agree Warp Street was like enabled by the government.
It was literally just giving the permission that you can actually...
Well, it was also taking, making like the big contract.
Advance, network commitments or whatever.
But I agree, but it was like fundamentally,
it was like a private sector led effort.
That was the only part of COVID that worked.
I mean, I think, I think, again,
I think the project will look closer to Operation Warp Speed.
And it's not even, I mean, I think,
I think you'll have all the companies involved
in the government project.
I'm not that sold that merging is that difficult.
You have one, you select one code base
and you run free training on like GPUs with one code base.
And then you do the sort of second RL step
on the other code base with TPUs.
And I think it's fine.
I mean, to the topic of like, will people sign up for it?
They wouldn't sign up for it today.
I think this would be kind of crazy to people.
But also, I mean, this is part of the secrets thing.
People gather at parties or whatever, you know, this.
I don't think anyone has really gotten up
in front of these people and been like,
look, the thing you're building is the most important thing
for like the national security of the United States
for like weather, like the free world
will have another century ahead of it.
Like this thing you're doing is really important,
like for your country, for democracy.
And don't talk about the secrets.
And it's not just about, you know, deep mind or whatever.
It's about, you know, these really important things.
And so, you know, I don't know,
like again, we're talking about the Manhattan Project, right?
This stuff was really contentious initially.
But, you know, at some point,
it was like clear that this stuff was coming.
It was clear that there was like,
sort of a real sort of like exigency
on the military national security front.
And, you know, I think a lot of people come around
on the like weather will be competent.
I agree.
I mean, this is again where it's like,
a lot of the stuff is more like predictive in the sense.
I think this is like reasonably likely.
And I think not enough people are thinking about it.
And, you know, like a lot of people think
about like AI lab politics or whatever.
But like, nobody has a plan for the project.
You know, it's like, you know, like-
Should they think you're pessimistic about it?
And like, we don't have a plan for it.
We need to do it very soon because AGI is upon us.
Yeah.
Then fuck, the only capable competent technical
institutions capable of making AI right now
are private companies.
Yeah, I know they're gonna say you play that leading role.
It'll be a sort of a partnership, basically.
But, you know, the other thing is like, you know,
again, we talked about World War II and, you know,
American unpreparedness, the veneer of World War II
is complete, you know, complete shambles, right?
And so there is a sort of like very comp-
You know, I think America has a very deep bench
of just like incredibly competent managerial talent.
You know, I think that, you know, there's a lot of
really dedicated people.
And, you know, I think basically a sort of
operational warp speed, public-private partnership,
something like that, you know, is sort of
what I imagine it would look like.
Yeah, I mean, the recruiting the talent
is an interesting question because the same sort of thing
were initially for the Manhattan Project,
you had to convince people, we got to beat the Nazis
and you got to get on board.
I think a lot of them maybe regretted
how much they accelerated the bomb.
And I think this is generally a thing of the war where-
I mean, I think they're also wrong to regret it, but...
Yeah, I mean, why?
What's the reason for regretting it?
I think there's a world in which you don't have...
The way in which nuclear weapons were developed
after the war was pretty explosive because
there was a precedent that you actually can use
nuclear weapons.
Then because of the race that was set up,
you immediately go to the H-bomb.
I mean, I think my view is, again, this is related
to the view on AI and maybe some of our disagreement
is like, that was inevitable.
Like, of course, like, you know, there's this, you know,
World War and then obviously there was the, you know,
Cold War right after, of course, like, you know,
the military and technology angle of this would be,
like, you know, pursued with ferocious intensity.
And I don't really think there's a world in which
that doesn't happen where it's like,
ah, we're all not going to build nukes.
And also just like nukes went really well.
I think that could have gone terribly, right?
You know, like, again, I mean, this sort of...
I think this is like not physically possible
with nukes, this sort of pocket nukes for everybody,
but I think sort of like WMDs
that are sort of proliferated and democratized
and like all the countries have it.
Like the U.S. leading on nukes and then sort of like
building this new world order that was kind of U.S. led
or at least sort of like a few great powers
and a non-proliferation regime for nukes,
a partnership and a deal that's like, look,
no military sort of application of nuclear technology,
but we're going to help you with the civilian technology
we're going to enforce safety norms on the rest of the world.
That worked. It worked.
And it could have gone so much worse.
I... Okay, so, we're zooming on.
I don't know if you're talking about Nagasaki, you know,
they were... I mean, this is...
I mean, I say this a bit in the piece,
but it's like actually the A-bomb, you know,
like the A-bomb on Hiroshima and Nagasaki was just like, you know,
the sort of... The firebombing, yeah.
Firebombing.
The thing... I think the thing that really changed the game
was like the super, you know, the H-bombs and ICBMs.
And then I think that's really when it took it
to like a whole new level.
I think part of me thinks when you say we'll tell the people
that for the free world to survive,
we need to pursue this project,
it sounds similar to World War II is...
I mean, yeah, yeah.
So, World War II is a sad story,
obviously, in the fact that it happened,
but also like the victory is sad in the sense that
why Britain goes in to protect Poland.
And at the end, the USSR, which is, you know,
as your family knows, is incredibly brutal,
ends up occupying half of Europe.
And part of protecting the free world,
that's why I got to rush the AI.
And if we end up with the American AI Leviathan,
I think there's a world where we look back on this
where it has the same sort of twisted irony
that Britain going into World War II had
about trying to protect Poland.
Look, I mean, I think there's going to be a lot of unfortunate
things that happen.
I'm just hoping we make it through.
I mean, to the point of it's like,
I really don't think the pitch will only be the sort of like,
you know, the race.
I think the race will be sort of a backdrop to it.
I think the sort of general like,
look, it's important that democracy shape this technology.
We can't just like leak this stuff to, you know,
North Korea is going to be important.
I think also for the, just safety, including alignment,
including the sort of like creation of new WMDs,
I'm not currently sold.
There's another path, right?
So it's like, if you just have the breakneck race,
both internationally, because you're just instantly leaking
all this stuff, including the weights,
and just, you know, the commercial race,
you know, Demis and Dario and Sam, you know,
just kind of like, they all want to be first.
And then it's incredibly rough for safety.
And then you say, okay, safety regulation.
But you know, it's sort of like, you know,
the safety regulation that people talk about,
it's like, oh, well, NIST, and they take years,
and they figure out what the expert consensus is,
and then they write some guidelines.
Isn't that what's going to happen to the project as well?
But I think, I mean, I think the sort of alignment angle
during the intelligence explosion, it's going to,
you know, it's not a process of like years of bureaucracy,
and then you can kind of write some standards.
I think it looks much more like basically a war,
and like you have a fog of war.
It's like, look, it's like,
is it safe to do the next oom?
You know, and it's like, ah, you know, like,
you know, we're like three ooms into the intelligence explosion.
We don't really understand what's going on anymore.
You know, the, you know, like a bunch of our like
generalization scaling curves are like kind of looking not great.
You know, some of our like automated AI researchers
that are doing alignment are saying it's fine,
but we don't quite trust them in this test.
You know, the like, the eyes started doing naughty things,
and ah, but then we like hammered it out, and then it was fine.
And like, ah, should we, should we go ahead?
Should we take, you know, another six months?
Also, by the way, you know, like China just stole the weights,
are we, you know, they're about to like deploy the Rome army?
Like, what do we do?
I think it's this, I think it is this crazy situation.
And, you know, basically you, you were lying much more on kind of
like a sane chain of command than you are on sort of some like,
you know, the Libertive Regulatory Scheme.
I wish you had, you were able to do the Libertive Regulatory Scheme.
And this is the thing about the private companies too.
I don't think, you know, they all claim they're going to do safety, but
I think it's really rough when you're in the commercial race,
and they're startups, you know, and startups, startups are startups.
You know, I think they're not fit to handle WMDs.
Yeah, I'm coming closer to your position.
But part of me also, so with the responsible scaling policies,
I was told that people who are advancing that, that the way to think about this,
because they know I'm like a libertarian type of person.
Yeah, yeah, yeah.
And the way they approached me about it was that
fundamentally, this is a way to protect
market-based development of AGI in the sense that if you didn't have this at all,
then you would have the sort of misuse and then you would have to be nationalized.
And the RSPs are a way to make sure that through this deployment,
you can still have a market-based order, but then there's these safeguards
that make sure that things don't go off the rails.
Yeah.
And I wonder if it seems like your story seems self-consistent,
but it does feel, I know this was never your position,
so I'm not looping you into this, but a sort of modern Bailey almost in the sense of...
Well, look, here's what I think about RSP-type stuff,
or sort of safety regulation that's happening now.
I think they're important for helping us figure out what world we're in
and like flashing the warning signs when we're close, right?
And so the story we've been telling is sort of what I think the modal version of this decade is,
but it's like, I think there's lots of ways it could be wrong.
I really... We should talk about the data a while more.
I think there's like, again, I think there's a world where the stuff stagnates, right?
There's a world where we don't have AGI.
And so basically, the RSP thing is preserving the optionality,
let's see how this stuff goes, but we need to be prepared.
Like if the red lights start flashing, if we're getting the automated eye researcher,
then it's crunch time and then it's time to go.
I think, okay, I can be on the same page on that, that we should have a very,
very strong prior on a proceeding in a market-based way,
unless you're right about what the explosion looks like, the intelligence explosion.
And so like, I don't move yet, but in that world where like really does seem like
Alec Radford can be automated and that is the only bottleneck to getting TSI.
Okay, I think we can leave it at that.
I can, yeah, I am somewhat of the way there.
Okay, okay, I hope it goes well.
It's gonna be, ah, very stressful.
And again, right now is the chill time.
Enjoy your vacation a lot at last.
It's funny to look on over, just like this is San Francisco.
Yeah, yeah, yeah, open the eyes right there, you know, anthropics there.
I mean, again, this is kind of like, you know, it's like,
you guys have this enormous power over how it's gonna go for the next couple of years,
and that power is depreciating.
Yeah.
Who's you guys?
Like, you know, people at labs.
Yeah, yeah, yeah.
But it is a sort of crazy world.
And you're talking about like, you know, I feel like you talk about like,
oh, maybe they'll nationalize too soon.
It's like, you know, almost nobody like really like feels it, sees what's happening.
And it's, I think this is the thing that I find stressful about all the stuff is like,
look, maybe I'm wrong.
Like if I'm right, we're in this crazy situation where there's like, you know,
like a few hundred guys that are like paying attention.
And it's daunting.
I went to Washington a few months ago.
And I was talking to some people who are doing AI policy stuff there.
Yeah.
And I was asking them how likely they think nationalization is.
Yeah.
And they said, oh, you know, like, it's really hard to nationalize stuff.
It's been a long time since we've done it.
There's these very specific procedural constraints on what kinds of things can be nationalized.
And then I was asked, well, like ASI, so that means because there's constraints
at a defense production act or whatever that won't be nationalized.
There's the Supreme Court would overturn that.
And they're like, yeah, I guess that would be nationalized.
That's the short summary of my post or my view on the project.
Okay.
So before we go further on the ASF, let's just back off.
We began the conversation.
I think people will be confused.
You graduated valedictorian of Columbia when you were 19.
So you got to college when you were 15.
Right.
And you were in Germany.
Yeah.
Then you got to college at 15.
Yeah.
How the fuck did that happen?
I really wanted out of Germany.
I, you know, I went to kind of, you know, a German public school.
It was not a good environment for me.
And, you know, in what sense?
There's just like no peers.
Yeah.
Look, I mean, it wasn't, yeah, it wasn't, you know, there's, I mean,
there's also just a sense in which sort of like there's this particular sort of
German cultural sense.
I think in the US, you know, there's all these like amazing high schools and like
sort of an appreciation of excellence.
And in Germany, there's really this sort of like Paul Poppy syndrome of us, right?
Where it's, you know, you're the curious kid in class and you want to learn more
instead of the teacher being like, ah, that's great.
They're like, they kind of resent you for it.
And they're like trying to crush you.
And I mean, there's also like, there's no kind of like elite universities for
undergraduate, which is kind of crazy.
So, you know, the sort of, you know, there's sort of like basically like the meritocracy
was kind of crushed in Germany at some point.
Also, I mean, there's this sort of incredible sense of, you know, complacency,
you know, across the board.
I mean, one of the things that always puzzles me is like, you know,
even just going to a US college was just kind of like radical act.
And like, you know, it doesn't seem radical to anyone here because it's like, ah,
this is obviously the thing you do and you can go to Columbia, you go to Columbia.
But it's, you know, it is very unusual.
And it's, it's, it's wild to me because it's like, you know,
this is where stuff is happening.
You can get so much of a better education.
And, you know, like America is where, you know, it's where, where, where all the stuff is.
And people don't do it.
And, and so, um, yeah, anyway, so I, you know, I know I skipped a few grades and,
and, you know, I think at the time it seemed very normal to me to kind of like go to college
in 15 and come to America.
I think, um, you know, now my, one of my sisters is now like turning 15, you know.
And so then I, you know, and I look at her and I'm like,
now I understand how my mother.
And then as you get to college, you were like presumably the only 15 year old.
Yeah, yeah.
As it was just like normal for you to be a 15 year old.
Like what was the initial years?
It felt so normal at the time.
You know, I didn't, yeah.
So yeah, it's like now I understand why my mother's worried.
And, you know, I, you know, I worked, I worked on my parents for a while.
You know, eventually I was, you know, I persuaded them.
No, but yeah, it felt, felt very normal at the time.
And it was great.
It was also great because I, you know, I actually really like college, right?
And in some sense it sort of came at the right time for me.
Where, you know, I, I mean, I, you know, for example,
I really appreciate the sort of like liberal arts education and,
and you know, like the core curriculum and reading sort of core works of political
philosophy and, and literature and,
You did about econ and,
I mean, my majors were math and statistics and economics.
But, you know, Columbia is a sort of pretty heavy core curriculum and liberal arts education.
And honestly, like, you know, I shouldn't have done all the majors.
I should have just, I mean, the best courses were sort of the courses where it's like,
there's some amazing professor and it's some history class.
And it's, I mean, that's, that's honestly the thing I would recommend people spend their time
on in college.
Was there one professor or class that stood out that way?
I mean, if you, there's like a class by Richard Betz on war, peace and strategy.
Adam too is obviously fantastic.
And you know, has written very riveting books.
Yeah.
Yeah.
You should have him on the podcast, by the way.
I've tried.
Okay.
I tried.
I think you tried for me.
Yeah, you gotta give him on the pod.
Yeah.
Oh, it'd be so good.
Okay.
So then in a couple of years, we were talking to Tyler Cowan recently and he said that when,
the way we, he first encountered you was you wrote this paper on economic growth and existential risk.
And he said, I, when I found, read it, I couldn't believe that a 17 year old had written it.
I thought if this was a MIT dissertation, I'd be impressed.
So you were like, how did you go from your, I guess we were in junior then,
you're writing, you're writing, you know, pretty novel economic papers.
Why did you get interested in this, this kind of thing?
And what was the process to get in that?
I don't know.
I just, you know, I get interested in things.
In some sense, it's sort of like, it feels very natural to me.
It's like, I get excited about a thing.
I read about it.
I immerse myself.
I think I can, you know, I can learn information very quickly and understand it.
The, I mean, I think to the paper, I mean, I think one actual, at least for the way I work,
I feel like sort of moments of peak productivity matter much more than sort of average productivity.
I think there's some jobs, you know, like CUO or something, you know, like average
productivity really matters.
But I think there's sort of a, I often feel like I have periods of like, you know,
there's some, there's a couple months where there's sort of nephrolessence and I'm like,
you know, and the other times I'm sort of computing stuff in the background.
And at some point, you know, like writing the series, this is also kind of similar.
And it's just like you, you write it and, and it's, it's like, it's really flowing.
And that's sort of what ends up mattering.
I think even for CEOs, it might be the case that the peak productivity is very important.
There's one of our following chat-off-house rules, one of our friends in a group chat has
pointed out how many famous CEOs and founders have been bipolar manic, which is very much
the peak, like the call option on your productivity is the most important thing.
And you get it by just increasing the volatility through bipolar.
Okay. So that's interesting.
And so you get interested in economics first.
First of all, why economics?
Like you could read about anything at this move.
Like you, if you wanted, you know, you kind of got a slow start on them.
You reached it all these years on the econ.
There's an alternative world where you're like on the super alignment team at 17 instead of 21
or whatever it was.
I mean, in some sense, I'm still doing economics, right?
You know, what is, what is straight lines on a graph?
I'm looking at the log-log plots and like figuring out what the trends are and like
thinking about the feedback loops and equilibrium arms control dynamics.
And, you know, it's, I think it is a sort of a way of thinking that I find very useful.
And, you know, like, what, you know, Dario and Ilya seeing scaling early in some sense,
that is a sort of very economic way.
And also the sort of physics, kind of like empirical physics, you know, a lot of them
are physicists.
I think the economists usually can't code well enough and that's their issue.
But I think it's that sort of way of thinking.
I mean, the other thing is, you know, I, I thought they were sort of, you know,
I thought of a lot of the sort of like core ideas of economics.
I thought we're just beautiful.
And, you know, in some sense, I feel like I was a little duped, you know,
where it's like actually econ academia is kind of decadent now.
You know, I think that, you know, for example, the paper I wrote, you know, it's sort of,
I think the takeaway, you know, it's a long paper, it's 100 pages of math or whatever.
I think the core takeaway, I can, you know, kind of give the core intuition for in like,
you know, 30 seconds and it makes sense.
And it's, and it's like, you don't actually need the math.
I think that's the sort of the best pieces of economics are like that where you do the work,
but you do the work to kind of uncover insights that weren't obvious to you before.
Once, once you've done the work, it's like some sort of like mechanism falls out of it that like
makes a lot of crisp intuitive sense that like explains some facts about the world
that you can then use in arguments.
And I think, you know, I think, you know, like a lot of econ one-on-one like this,
and it's great.
A lot of econ in the, you know, the 50s and the 60s, you know, was like this.
And, you know, Chad Jones papers are often like this.
I really like Chad Jones papers for this.
You know, I think, you know, why did I ultimately not pursue econ academia was a number of reasons.
One of them was Tyler Cowan.
You know, he kind of took me aside and he was kind of like, look,
I think you're one of the like top-down economists I've ever met,
but also you should probably not go to grad school.
Oh, interesting.
Yeah, I didn't realize that.
Well, yeah.
And it was, it was, it was good because he kind of introduced me to the, you know,
I don't know, like the Twitter weirdos or just like, you know,
and I think the takeaway from that was kind of, you know,
got to move out last one more time.
Wait, Tyler introduced you to the Twitter weirdos?
A little bit.
Yeah.
Or just kind of like the sort of brought you.
Like the 60 year old, the old economist introduced you to that Twitter.
Yeah.
Well, you know, I had been, I had, so I went from Germany, you know,
completely, you know, on the periphery to kind of like, you know,
in the U.S. elite institution and sort of got some vibe of like sort of,
you know, meritocratic elite, you know, U.S. society.
And then sort of, yeah, basically this sort of like,
there was a sort of directory then to being like, look, I, you know,
find the true American spirit.
I got to come out here.
But the other reason I didn't become an economist was because
or at least econ academia was because I think sort of econ academia has become
a bit decadent.
And maybe it's just ideas getting harder to find.
And maybe it's sort of things, you know, and the sort of beautiful,
simple things have been discovered.
But you know, like, what are econ papers these days?
You know, it's like, you know, it's like 200 pages of like empirical analyses
on what happened when, you know, like Wisconsin bought,
you know, 100,000 more textbooks on like educational outcomes.
And I'm really happy that work happened.
I think it's important work, but I think it is not in government
and covering these sort of like fundamental insights and sort of
mechanisms in society.
Or, you know, it's like even the theory work is kind of like,
here's a really complicated model.
And the model spits out, you know, if the Fed does X, you know,
then why happens?
You have no idea what that hat, why that happened,
because it's like gazillion parameters and they're all calibrated
in some way and it's some computer simulation.
You have no idea about the validity, you know.
Yeah.
So I think, I think the sort of, you know, the most important insights
are the ones where you have to do a lot of work to get them.
But then there's sort of this crisp intuition.
Yeah, yeah.
The P versus NP of.
Sure.
Yeah, yeah.
That's really interesting.
So just going back to your time in college.
Yeah.
You say that peak productivity kind of explains this paper and things,
but the valedictorian, that's getting straight A's or whatever,
is very much average productivity phenomenon, right?
So there's one award for the highest GPA, which I won,
but the valedictorian is like among the people which have the highest GPA
and then like selected by faculty or something.
Okay, yeah.
So it's just not, it's not just peak productivity.
It's just, I generally just love this stuff.
You know, I just, I was curious and I thought it was really interesting
and I love learning about it and I love kind of like, it made sense to me.
And, you know, it was very natural.
And you know, I think I'm, you know, I'm not, you know, I think one of my faults
is I'm not that good at eating glass or whatever.
I think there's some people who are very good at it.
I think the sort of like, the sort of moments of peak productivity come
when I, you know, I'm just really excited and engaged and love it.
And, you know, I, you know, if you take like courses, you know,
that's what you've got in college.
Yeah, it's the Bruce Banner code and Avengers.
You know, I'm always angry.
I'm always excited.
I'm always curious.
That's why I'm always peak productivity.
So it's interesting, by the way, when you were in college, I was also in college.
I think you were, despite being a year younger than me,
I think you're ahead in college than me or at least two years, maybe two years ahead.
And we met around this time.
Yeah, yeah, yeah.
We also met, I think, through the Tyler Cowan universe.
Yeah, yeah, yeah.
And it's very insane how small the world is.
Yeah.
I think I, did I reach out to you?
I must have.
Yes, I don't know, yeah.
About when I had a couple of videos and they had a couple hundred views or something.
Yeah.
It's a small world.
Yeah.
I mean, this is the crazy thing about the eye world, right?
It's kind of like, it's the same few people at the Kamehasa parties
and they're the ones running the models at DeepMind and OpenAI and Anthropic.
And I mean, I think some other friends of ours have mentioned this who are now later
in their career and very successful that they actually met all the people who are also kind
of very successful in Silicon Valley now, when they're in their 20s or when they're really 20.
I mean, look, I actually think, and why is it a small world?
I mean, I think one of the things is some amount of some sort of agency.
And I think in a funny way, this is a thing I sort of took away from the sort of Germany experience
where it was, I mean, look, it was crushing.
I really didn't like it.
And it was like, it was such an unusual move to kind of skip grades and such an unusual move to
come to the United States.
And a lot of these things I did were kind of unusual moves.
And there's some amount where like, just like, just trying to do it and then it was fine and it
worked. That kind of reinforced like, you know, you don't you don't just have to kind of conform
to what the overturned window is, you can just kind of try to do the thing, the thing that
seems right to you. And like, you know, most people can be wrong.
And I know things like that.
And I think that was kind of a valuable kind of like early experience that was sort of formative.
Okay, so after college, what did you do?
I did econ research for a little bit, you know, Oxford and stuff.
And then then I worked at Future Fun.
Mm hmm.
Yeah.
Okay, so and so tell me about it.
Future Fun was that, you know, it was a foundation that was, you know,
funded by Sam Bank and Fried.
I mean, we were our own thing, you know, we were based in the Bay.
You know, at the time, this was in sort of early 22.
It was it was this just like incredibly exciting opportunity, right?
It was basically like a startup, you know, foundation, which is like, you know,
it doesn't come along that that often that, you know,
we thought we'd be able to give away billions of dollars.
You thought we'd be able to kind of like, you know, remake how philanthropy is done,
you know, from first principles, that would be able to have, you know,
this like great impact, you know, causes we focused on where,
you know, biosecurity, you know, AI, you know, finding exceptional talent
and putting them to work on hard problems.
And, you know, like a lot of the stuff we did, I was I was really excited about,
you know, like academics who would, you know, usually take six months
would send us emails like, ah, you know, this is great.
This is so quick and, you know, and straightforward, you know, in general,
I feel like I've often find that with like, you know, a little bit of encouragement,
a little bit of sort of empowerment, kind of like removing excuses,
making the process easy, you know, you can kind of like get people to do great things.
I think on the future fund, the thing is context for people who might not realize
not only were you guys planning on deploying billions of dollars,
but it was a team of four people.
Yeah, yeah, yeah.
So you at 18 are on a team of four people that is in charge of deploying billions of dollars.
Yeah, I mean, just, I mean, yeah, I'm a future fund, you know, the,
yeah, I mean, the, you know, so that was, that was sort of the heyday, right?
And then obviously, you know, when, when in sort of, you know, November of 22,
you know, it was kind of revealed that Sam was this, you know, giant fraud.
And from one day to the next, you know, the whole thing collapsed.
That was just really tough.
I mean, you know, obviously it was devastating.
It was devastating, obviously for the people at their money and FTX,
you know, closer to home, you know, all the, you know, all these grantees,
you know, we wanted to help them and we thought they were doing amazing projects.
And so, but instead of helping them, we ended up saddling them with like a giant problem.
You know, personally, it was, you know, it was a startup, right?
And so I, you know, I'd worked 70 hour weeks every week for, you know,
basically a year on this to kind of build this up, you know, we're a tiny team.
And then from one day to the next, it was all gone and not just gone,
it was associated with this giant fraud.
And so, you know, that was incredibly tough.
Yeah.
And then were there any signs early on that SBF was?
Yeah.
And look, obviously I didn't know he was a fraud and the whole, you know,
I would have never worked there, you know.
And, you know, we weren't, you know, we were a separate thing.
We weren't working with the business.
I mean, I think, I do think there were some takeaways for me.
I think one takeaway was, you know, I think there's a,
I had this tendency, I think people in general have this tendency to kind of like,
you know, give successful CEOs a pass on their behavior,
because, you know, they're successful CEOs and that's how they are.
And that's just successful CEO things.
And, you know, I didn't know Sandbank Manfredo was a fraud, but I knew SBF,
and I knew he was extremely risk-taking, right?
I knew he, he was narcissistic.
He didn't tolerate disagreement well, you know, sort of by the end,
he and I just like didn't get along well.
And sort of, I think the reason for that was like,
there's some biosecurity grants he really liked because they're kind of cool and flashy.
And at some point I'd kind of run the numbers and I didn't really seem that cost effective
and I pointed that out and he was pretty unhappy about that.
And so I knew his character.
And I think, you know, I feel like one takeaway for me was,
was, you know, like, I think it's really worth paying attention to people's character,
including like people you work for and successful CEOs.
And, you know, that can save you a lot of pain down the line.
Okay, so after that, FDX implodes and you're out.
And then you got into, you went to OpenAI.
The Super Alignment team had just started.
I think you were like part of the initial team.
And so what was the original idea?
What was compelling about that for you to join?
Yeah, totally.
So, I mean, what was the goal of the Super Alignment team?
You know, the Alignment team at OpenAI, you know, at, you know, other labs,
sort of like several years ago, kind of had done sort of basic research and they developed
RLHF, reinforcement learning from human feedback.
And that was sort of a, you know, ended up being really successful technique for controlling
sort of current generation of AI models.
What we were trying to do was basically kind of be the basic research to figure out what is
the successor to RLHF.
And the reason we needed that is, you know, basically, you know, RLHF probably won't
scale to super human systems.
RLHF relies on sort of human raiders who kind of thumbs up, thumbs down, you know,
like the model said something, it looks fine, it looks good to me.
At some point, you know, the super human models, the super intelligence, it's going to write,
you know, a million lines of, you know, crazy complex code.
You don't know at all what's going on anymore.
And so how do you kind of steer and control these systems?
How do you hide side constraints?
You know, the reason I joined was I thought this was an important problem.
And I thought it was just a really solvable problem, right?
I thought this was basically, you know, there's, I think there's a, I still do.
I mean, even more so do, I think there's a lot of just really promising sort of ML
research on alignment on sort of aligning super human systems.
And maybe we should talk about that a bit more later.
But so, and then it was so solvable.
You solved it in a year.
It's all over.
Yeah, I think quite so.
Anyway, so look, opening, I wanted to do this like really ambitious effort on alignment.
And, you know, Elliot was backing it.
And, you know, I liked a lot of the people there.
And so I was, you know, I was really excited.
And I was kind of like, you know, I think there was a lot of people,
sort of on alignment, there's always a lot of people kind of making hay about it.
And, you know, I appreciate people highlighting the importance of the problem.
And I was just really into like, let's just try to solve it.
And let's do the ambitious effort, you know, let's do the, you know,
operation warp speed for solving alignment.
And it seemed like an amazing opportunity to do so.
Okay. And now basically the team doesn't exist.
I think the head of it has left.
Both heads of it have left, Jan and Ilya.
That's the news of the last week.
Mm-hmm.
What happened?
Why did the thing break down?
I think OpenAI sort of decided to take things in a somewhat different direction.
Meaning what?
I mean, did that super alignment, isn't the best way to frame the...
No, I mean, look, obviously, sort of after the November board events,
you know, there were personnel changes.
I think Ilya leaving was just incredibly tragic for OpenAI.
And, you know, I think some amount of repartilization,
I think some amount of, you know, I mean,
there's been some reporting on the Superalignment Compute Commitment,
you know, there's this 20% Compute Commitment
as part of, you know, how a lot of people were recruited, you know,
it's like, we're going to do this ambitious effort on alignment.
And, you know, some amount of, you know,
not keeping that and deciding to go in a different direction.
Okay. So now Jan has left, Ilya has left.
So this team itself has dissolved,
but you were the sort of first person who left or was forced to leave.
You were the information reported that you were fired for leaking.
But what happened? Was this accurate?
Yeah. Look, why don't I tell you what they claim I leaked in
and you can tell me what you think.
Yeah. So OpenAI did claim to employees that I was fired for leaking.
And, you know, I and others have sort of pushed them to say what the leak is.
And so here's their response in full.
You know, sometime last year I had written a sort of brainstorming document on preparedness
on safety and security measures we need in the future on the path to AGI.
And I shared that with three external researchers for feedback.
So that's it. That's the leak.
You know, I think for context, it was totally normal at OpenAI at the time
to share sort of safety ideas with external researchers for feedback.
You know, it happened all the time.
You know, the doc was sort of my idea is, you know, before I shared it,
I reviewed it for anything sensitive.
The internal version had a reference to a future cluster,
but I redacted that for the external copy.
You know, there's a link in there to some slides of mine, internal slides.
But, you know, that was a dead link to the external people I shared it with.
You know, the slides weren't shared with them.
And so, obviously, I pressed them to sort of tell me what is the confidential
information in this document.
And what they came back with was a line in the doc about planning for AGI by 2728
and not setting timelines for preparedness.
You know, I wrote this doc, you know, a couple months after the super alignment
announcement, we had put out, you know, this sort of four-year planning horizon.
I didn't think that planning horizon was sensitive.
You know, it's the sort of thing Sam says publicly all the time.
Hey, I think sort of John said it on the podcast a couple weeks ago.
Anyway, so that's it.
That's it?
So that seems pretty thin for, if the cause was leaking, that seems pretty thin.
Was there anything else to it?
Yeah, I mean, so that was the leaking claim.
I mean, you can say a bit more about sort of what happened in the following.
Yeah.
So one thing was last year, I had written a memo, internal memo about opening I security.
I thought it was, you know, egregiously insufficient.
You know, I thought it wasn't sufficient to protect, you know, the theft of model
weights or key algorithmic secrets from foreign actors.
So I wrote this memo.
I shared it with a few colleagues, a couple members of leadership who sort of mostly said
it was helpful.
But then, you know, a couple weeks later, a sort of major security incident occurred.
And that prompted me to share the memo with a couple members of the board.
And so after I did that, you know, days later, it was made very clear to me that leadership
was very unhappy with me having shared this memo with the board.
You know, apparently the board had hassled leadership about security.
And then I got sort of an official HR warning for this memo, you know, for sharing it with
the board.
The HR person told me it was racist to worry about CCPSV and that.
And they said it was sort of unconstructive.
And, you know, look, I think I probably wasn't that my most diplomatic, you know,
I definitely could have been more politically savvy.
But, you know, I thought it was a really, really important issue.
And, you know, the security incident had been really worried.
Anyway, and so I guess the reason I bring this up is when I was fired, it was sort of
made very explicit that the security memo is a major reason for my being fired.
You know, I think it was something like, you know, the reason that this is a firing
and not a warning is because of the security memo.
But are you sharing it with the board?
The warning I'd gotten for the security memo.
Anyway, and I mean, some other, you know, what might also be helpful context is the
sort of questions they asked me when they fired me.
So, you know, this was a bit over a month ago.
I was pulled, you know, aside for a chat with a lawyer, you know, that quickly turned very
adversarial.
And, you know, the questions were all about my views on AI progress on AGI on the level
security appropriate for AGI on, you know, whether government should be involved in AGI on,
you know, whether I and super alignment were loyal to the company on, you know, what I was
up to during the opening of board events, you know, things like that.
And, you know, then they, you know, chatted to a couple of my colleagues and then they
came back and told me I was fired.
And, you know, they'd gone through all of my digital artifacts from the time at my, you
know, time at opening messages docs.
And that's when they found, you know, the leak.
Yeah.
And so anyway, so the main claim they made was this leaking allegation.
You know, that's what they told employees.
They, you know, the security memo, there's a couple other allegations they threw in.
One thing they said was that I was unforthcoming during the investigation because I didn't
initially remember who I'd shared the doc with the sort of preparedness brainstorming doc,
only that I had sort of spoken to some external researchers about these ideas.
And, you know, look, the doc was over six months old.
You know, I spent the day on it.
You know, it was a Google doc.
I shared with my opening email.
It wasn't, you know, a screenshot or anything I was trying to hide.
It simply didn't stick because it was such a non-issue.
And then they also claimed that I was engaging on policy in a way that they didn't like.
And so what they cited there was that I had spoken to a couple external researchers,
you know, somebody that I think tank about my view that AGI would become a government
project, you know, as we discussed.
You know, in fact, I was speaking to lots of sort of people in the field about that at
the time.
I thought it was a really important thing to think about.
Anyway, and so they found, you know, they found a DM that I'd written to like a friendly
colleague, you know, five or six months ago, where I relayed this and, you know, they cited that.
And, you know, I had thought it was well within open-eyed norms to kind of talk about
high-level issues on the future of AGI with the external people in the field.
So anyway, so that's what they alleged.
That's what happened.
You know, I've spoken to kind of a few dozen former colleagues about this, you know, since
I think the sort of universal reaction is kind of like, you know, that's insane.
I was sort of surprised as well.
You know, I had been promoted just a few months before.
I think, you know, I think Ilya's comment for the promotion case at the time was something
like, you know, Leopold's amazing.
We're lucky to have him.
But look, I mean, I think the thing I understand and I think in some sense is reasonable is
like, you know, I think I ruffled some feathers and, you know, I think I was probably kind of
annoying at times.
You know, it's like I security stuff and I kind of like repeatedly raised that and maybe not
always in the most diplomatic way.
You know, I didn't sign the employee letter during the board events, you know, despite
pressure to do so.
And you were one of like eight people or something.
Not that many people.
I guess that I think the sort of two senior most people didn't sign were Andrey and Ya.
And you know, I mean, on the letter, by the way, I by the time on sort of
Monday morning, when that letter was going around, I think probably it was appropriate
for the board to resign.
I think they'd kind of like lost too much credibility and trust with the employees.
But I thought the letter had a bunch of issues.
I mean, I think one of them was it just didn't call for an independent board.
I think it's sort of like basics of corporate governance to have an independent board.
Anyway, you know, it's other things, you know, I am in sort of other discussions.
I press leadership for sort of opening eye to abide by its public commitments.
You know, I raised a bunch of tough questions about whether it was consistent with the
opening mission and consistent with the national interest to sort of partner with
authoritarian dictatorships to build the core infrastructure for AGI.
So, you know, look, you know, it's a free country, right?
That's what I love about this country.
You know, we talked about it.
And so they have no obligation to keep me on staff.
And, you know, I think in some sense, I think it would have been perfectly
reasonable for them to come to me and say, look, you know,
we're taking the company in a different direction.
You know, we disagree with your point of view.
You know, we don't trust you enough to sort of toe the company line anymore.
And, you know, thank you so much for your work at Open AI, but I think it's time to part ways.
I think that would have made sense.
I think, you know, we did start sort of materially diverging on sort of views on important issues.
I'd come in very excited in line with Open AI, but that sort of changed over time.
And look, I think I think there would have been a very amicable way to part ways.
And I think it's a bit of a shame that sort of this is the way it went down.
You know, all that being said, I think, you know, I really want to emphasize
there's just a lot of really incredible people at Open AI.
And it was an incredible privilege to work with them.
And, you know, overall, I'm just extremely grateful for my time there.
When you left, now that there's now there's been reporting about
an NDA that former employees have to sign in order to have access to their vested equity.
Did you sign some such NDA?
No, my situation is a little different.
And that is sort of as basically right before my cliff.
But then, you know, they still offered me the equity,
but I didn't want to sign a nondisparagement, you know, freedom is priceless.
And how much was the equity?
It's like close to a million dollars.
So it was definitely a thing you and others aware of that this is like
a choice that Open AI is explicitly offering you.
And presumably the person on Open AI staff knew that we're offering them equity,
but they had assigned this NDA that has these conditions that you can't,
for example, give the kind of statements about your thoughts on AGI and Open AI that
you're giving on this podcast right now.
Like, I don't know what the whole situation is.
I certainly think sort of vested equity is pretty rough if you're conditioning that on
some NDA.
It might be a somewhat different situation if it's a sort of separate agreement.
Right.
But an Open AI employee who had signed it, presumably,
could not give the podcast that you were giving today.
Quite plausibly not.
Yeah, I don't know.
Okay, so analyzing the situation here, I guess if you were to...
Yeah, the board thing is really tough, because if you were trying to defend them,
you would say, well, listen, you were just kind of going outside the regular chain of
command, and maybe there's a point there, although the way in which the person from
HR thinks that you have an adversarial relationship with, or you're supposed to
have an adversarial relationship with the board, where to give the board some information,
which is relevant to whether Open AI is fulfilling its mission and whether it can
do that in a better way is part of the leak, as if the board is that is supposed to ensure
that Open AI is following its mission as some sort of external actor.
That seems pretty...
I mean, I think, I mean, to be clear, the leak allegation was just that sort of document,
I changed the feedback.
Right.
This is just sort of a separate thing that they cited, and they said,
I wouldn't have been fired if not for the security memo.
They said you wouldn't have been fired.
They said, the reason this is a firing and not a warning is because of the warning you had
gotten for the security memo.
Oh, before you left, the incidents with the board happened where Sam was fired and then
Rihard, a CEO, and now he's on the board.
Now, Ilya and Yon, who are the heads of the Superalignment team, and Ilya, who is a co-founder
of Open AI, obviously the most significant in terms of a statue or a member of Open AI
from a research perspective, they've left.
Seems like, especially with regards to Superalignment stuff, and just generally with Open AI,
a lot of this sort of personnel drama has happened over the last few months.
What's going on?
Yeah, just a lot of drama.
Yeah, so why is there so much drama?
I think there would be a lot less drama all Open AI claimed to be with building chat
GPT or building business software.
I think where a lot of the drama comes from is, Open AI really believes they're building AGI,
and it's not just a claim that you make for marketing purposes.
Whatever, there's this report that Sam is raising $7 trillion for chips, and it's like,
that stuff only makes sense if you really believe in AGI.
And so I think what gets people sometimes is the cognitive dissonance between really
believing in AGI, but then not taking some of the other implications seriously.
This is going to be incredibly powerful technology, both for good and for bad, and
that implicates really important issues, like the national security issues we spoke about,
like, are you protecting the secrets from the CCP?
Like, does America control the core AGI infrastructure, or does a Middle Eastern
dictator control the core AGI infrastructure?
And then I think the thing that really gets people is the tendency to then
make commitments, and they say they take these issues really seriously,
they make big commitments on them, but then frequently don't follow through.
So again, as mentioned, there's this commitment around Superalignment compute,
sort of 20% of compute for this long-term safety research effort.
And I think you and I could have a totally reasonable debate about what is the appropriate
level of compute for Superalignment. But that's not really the issue.
The issue is that this commitment was made, and it was used to recruit people, and it was very
public, and it was made because there's a recognition that there would always be something
more urgent than a long-term safety research effort, like some new product or whatever.
But then, in fact, they just really didn't keep the commitment.
And so there was always something more urgent than long-term safety research.
I mean, I think another example of this is when I raised these issues about
security, they would tell me, securities are number one priority.
But then, invariably, when it came time to invest serious resources, when it came time
to make trade-offs, to take some pretty basic measures, security would not be prioritized.
And so yeah, I think it's the cognitive dissonance, and I think it's the unreliability
that causes a bunch of the drama.
So let's zoom out, talk about a big part of the story, and also a big motivation of the way in
which it must proceed with regards to geopolitics and everything, is that once you have the AGI,
pretty soon after, you proceed to ASI because, superintelligence, because you have these AGIs
which can function as researchers into further AI progress, and within a matter of years,
maybe less, you go to something that is like superintelligence.
And then from there, according to your story, you do all this research and development into
robotics and pocket nukes and whatever other crazy shit. But at a high level,
okay, but I'm skeptical of this story for many reasons. At a high level, it's not clear to me
this input output model of research is how things actually happen in research. We can look at economy
wide, right? Patrick Hollis and others have made this point that from compared to 100 years ago,
we have 100x more researchers in the world. It's not like progress is happening 100x faster.
So it's clearly not the case that you can just pump in more population into research and you get
higher research on the other end. I don't know why it would be different for the AI researchers
themselves. Okay, great. So this is getting into some good stuff. I have a classic disagreement
I have with Patrick and others. So obviously inputs matter. So it's like the United States
produces a lot more scientific and technological progress than Liechtenstein or Switzerland. And
even if I made Patrick Hollis, a dictator of Liechtenstein or Switzerland, and Patrick Hollis
was able to implement his Utopia of Ideal Institutions, keeping the town full fix. He's not
able to like do some crazy high school immigration thing or like, you know, whatever, some like
crazy genetic breeding scheme or whatever he wants to do. Keeping the town full fix, but amazing
institutions. I claim that still, even if you made Patrick Hollis and dictator of Switzerland,
maybe you get some factor, but Switzerland is not going to be able to outcompete the United
States and scientific and technological progress. Obviously, magnitude is matter.
Okay, no, actually, I'm not sure I agree with this. There's been many examples in history where
you have small groups of people who are part of like Bell Labs or Skunkworks or something.
There's a couple hundred researchers, open AI, right? Couple hundred researchers, they do highly
selected though, right? You know, it's like, it's like saying, you know, that's part of that's part
of why Patrick Hollis and his dictator is going to do a good job of this. Well, yes, if you can
highly select all the best AI researchers in the world, you might only need a few hundred. But if
you know, that's that's the talent pool. It's like you have the, you know, 300 best AI researchers
in the world. But there's there has been, it's not a case that from 100 years to now, there haven't
been, the population has increased massively. A lot of the work, in fact, you would expect the
density of talent to have increased in the sense that malnutrition and other kinds of
poverty, whatever that have debilitated past talent at the same sort of level is no longer
debilitated. Yeah, so to the 100X point, right? So I don't know if it's 100X, I think it's easy
to inflate these things, probably at least 10X. And so people are sometimes like, ah, you know,
like, you know, come on, ideas haven't gotten that much harder to find, you know, why would you
have needed this 10X increase in research effort? Whereas to me, I think this is an extremely
natural story. And why is it a natural story? It's a straight line on a log log plot. This is
sort of a deep learning researchers dream, right? What is this log log plot? On the x-axis, you
have log cumulative research effort. On the y-axis, you have some log GDP or ooms of algorithmic
progress, or, you know, log transistors per square inch, or, you know, in the sort of experience
curve for solar, kind of like, you know, whatever the log of, you know, the price for a gigawatt
of solar. And it's extremely natural for that to be a straight line. You know, this is sort of a
yeah, it's a classic. And, you know, it's basically that the first thing is very easy,
then basically, you know, you have to have log increments of cumulative research effort to
find the next thing. And so, you know, in some sense, I think this is a natural story. Now,
one objection kind of people then make is like, oh, you know, isn't it suspicious, right? That
like ideas, you know, well, we increased research effort 10X, and ideas also just got 10X harder
to find. And so it perfectly, you know, equilibrates. And to there, I say, you know, it's just,
it's an equilibrium. It's an endogenous equilibrium, right? So it's like, you know,
isn't it a coincidence that supply equals demand, you know, on the market clears, right? And that's
and same thing here, right? So it's, you know, ideas getting how much ideas have gotten harder
to find as a function of how much progress you've made. And then, you know, what the overall growth
rate has been as a function of how much ideas have gotten harder to find in ratio to how much
you've been able to like increase research effort, what is the sort of growth, the log
cumulative research effort. So in some sense, I think the story is sort of like, fairly natural.
And you see this, you see this not just economy wide, you see it in kind of experience curve for
all sorts of individual technologies. So I think there's some process like this. I think it's
totally plausible that, you know, institutions have gotten worse by some factor. Obviously,
there's some sort of exponent of diminishing returns on more people, right? So like serial
time is better than just paralyzing. But still, I think it's like clearly inputs matter.
Yeah, I agree. But if the coefficient of how fast they diminish as you grow the input is high
enough, then the in the abstract, the fact that inputs matter isn't that relevant. Okay. So
I mean, we're talking at a very high level, but just like take it down to the actual concrete
thing here. Open AI has a staff of at most low hundreds who are directly involved in the
algorithmic progress in future models. If it was really the case that you could just arbitrarily
scale this number, and you could have much faster algorithmic progress, and that would result in
much higher, much better AIs for open AI basically, then it's not clear why open AI doesn't just go
out and hire every single person with 150 IQ, of which there are hundreds of thousands in the
world. And my story there is there's transaction costs to managing all these people that don't
just go away if you have a bunch of AIs, that these tasks aren't easy to parallelize. And I think
you, I'm not sure how you would explain the fact of like, why does an open AI go on a recruiting
binge of every single genius in the world? Okay, great. So let's talk about the open AI example
and let's talk about the automated AI researchers. So I mean, in the open AI case, I mean, just,
you know, just kind of like look at the inflation of like AI researcher salaries over the last year.
I mean, I think like, I don't know, I don't know what it is, you know, 4x, 5x, it's kind of crazy.
So they're clearly really trying to recruit the best AI researchers in the world. And, you know,
I don't know, it's, they do find the best AI researchers in the world. I think my response
to your thing is like, you know, almost all of these 150 IQ people, you know, if you just hire
them tomorrow, they wouldn't be good AI researchers. They wouldn't be an Alec Radford.
But they're willing to make investments that take years to pan out of the fourth.
There are the, the data centers they're buying right now will come online in 2026 or something.
Why wouldn't they be able to make every 150 IQ person? Some of them won't work out. Some of
them won't have the traits we like. But some of them by 2026 will be amazing AI researchers.
Why, why aren't they making that bet? Yeah. And so sometimes this happens, right? Like,
smart physicists have been really good at AI research, you know, it's like all the anthropic
co-founders. But, but, but, but, but like, if you talk to, I've had Daria in the podcast,
I'm like, they have this very careful policy of like, we're not going to just hire arbitrarily,
we're going to be extremely selective. Training is not as easily scalable, right? So training is
very hard. You know, if you, if you just hired, you know, 100,000 people, it's like, I mean,
you couldn't train them all, if you're really hard to train them all, you know, you wouldn't be
doing any AI research, like, you know, there's, there's huge costs to bringing on a new person
training them. This is very different with the AIs, right? And I think this is, it's really
important to talk about the sort of like advantages the AIs will have. So it's like, you know,
training, right? It's like, what does it take to be an Alec Radford? You know, we need to be in a
really good engineer, right? The AIs, they're going to be an amazing engineer. They're going to be
amazing at coding. You can just train them to do that. They need to have, you know, not just be a
good engineer, but have really good research intuitions and like really understand deep learning.
And this is stuff that, you know, Alec Radford, or, you know, somebody like him has acquired over
years of research over just like being deeply immersed in deep learning, having tried lots of
things himself and failed. The AIs, you know, they're going to be able to read every research
paper I've written, every experiment I've ever run at the lab, you know, like gain the intuitions
from all of this, they're going to be able to learn in parallel from all of each other's
experiment, you know, experiences. You know, I don't know, what else, you know, it's like,
what does it take to be an Alec Radford? Well, there's a, there's a sort of cultural acclimation
aspect of it, right? You know, if you hire somebody new, you know, there's like politicking,
maybe they don't fit in. Well, in the AI case, you just make replicas, right? There's a like
motivation aspect for it, right? So it's like, you know, Alec, you know, if I could just like
duplicate Alec Radford, and before I run every experiment, I haven't spent like, you know,
a decade's worth of human time, like double checking the code and thinking really careful,
be careful about it. I mean, first of all, I don't have that many Alec Radfords, and you know,
he wouldn't care. And he would not be motivated. But you know, the AI is it can just be like,
look, I have a hundred million of you guys, I'm just going to put you on just like,
really making sure this code is cracked, there are no bugs, this experiment is thought through
every hyperparameter is correct. Final thing I'll say is, you know, the 100 million human
equivalent AI researchers, that is just a way to visualize it. So that doesn't mean you're going to
have literally 100 million copies. You know, so there's trade offs you can make between serial
speed and in parallel. So you might make the trade off is look, we're going to run them at,
you know, 10x 100x serial speed, it's going to result in fewer tokens overall, because it's
sort of inherent trade offs. But you know, then we have, I don't know what the numbers would be,
but then we have, you know, 100,000 of them running at 100x human speed and thinking and,
you know, and there's other things you can do on coordination, you know, they can kind of like
share latent space, attend to each other's context. There's basically this huge range
of possibilities of things you can do. The 100 million thing is more, I mean, another illustration
of this is, you know, if you kind of, I run the math in my series, and it's basically, you know,
27, 28, you have this automated AI researcher, you're going to be able to generate an entire
internet's worth of tokens every single day. So it's clearly sort of a huge amount of like
intellectual work they can do. I think the analogous thing there is today we generate
more patents in a year than during the actual physics revolution in the early 20th century,
they were generating across like half a century or something. And are you making more physics
progress in a year today? So yeah, you're going to generate all these tokens. Are you generating as
much codified knowledge as humanity has been able to generate in the initial creation of the internet?
Internet tokens are usually final output, right? A lot of these tokens, if we talked about the
unhobbling, right? And I think of a kind of like, you know, a GPDN token is sort of like one token
of my internal monologue, right? And so that's how I do this math on human equivalents, you know,
it's like 100 tokens a minute, and then, you know, humans working for X hours, and you know,
what is the equivalent there? I think this goes back to something we were talking about earlier,
where, well, I haven't, we've seen the huge revenues from people often ask this question,
that if you took GPD4 back 10 years, and you showed people this, and they think this is going
to automate, this is already automated, half the jobs. And so there's a sort of a modus ponens,
modus tolens here, where part of the explanation is like, oh, it's like, just on the verge,
you need to do these unhobblings. And part of that is probably true. But there is another
lesson to learn there, which is that just looking at face value outside of abilities,
there's probably more sort of hobblings that you don't realize that are hidden behind the scenes.
And I think the same will be true of the AGI that you have running as AI researchers.
I think a lot of things... But I basically agree, right? I think my story here is like,
you know, I talk about, I think there's going to be some long tail, right? And so part, you know,
maybe it's like, you know, 26, 27, you have like the proto-automated engineer, and it's really good
in engineering, it doesn't have the research intuition yet, you don't quite know how to
put them to work. But you know, the sort of even the underlying pace of AI progress is already so
fast, right? In three years from not being able to do any kind of like math at all to now crushing,
crushing these math competitions. And so you have the initial thing in like 26, 27,
maybe the sort of automated, it's an automated research engineer speeds you up by 2x,
you go through a lot more progress in that year, by the end of the year, you figured out like
the remaining kind of unhobblings, you've like got a smarter model, and you know, maybe then that
thing, or maybe it's two years, you know, and that thing, just like that thing really can do
automate 100%. And again, you know, they don't need to be doing everything, they don't need to be
making coffee, you know, they don't need to like, you know, maybe there's a bunch of, you know,
tacit knowledge in a bunch of other fields. But you know, AI researchers at AI labs really know
the job of an AI researcher. And it's in some sense, it's a sort of there's lots of clear
metrics, it's all virtual, there's code, it's things you can kind of develop and train for.
So I mean, another thing is how do you actually manage a million AI researchers humans, the
sort of comparative ability we have, that we've been especially trained for is like working in
teams. And despite this fact, we have, for thousands of years, we've been learning about
how we work together in groups. And despite this management is a clusterfuck, right? It's like
most companies are badly managed. It's really hard to do this stuff. For AIs, the sort of like,
we talk about AGI, but it'll be some bespoke set of abilities, some of which will be higher than
humans, some of which will be at human level. And so it'll be some bundle and we'll need to figure
out how to put these bundles together with their human overseers, with the equipment and everything.
And the idea that as soon as you get the bundle, you'll figure out how to get like just shove
millions of them together and manage them. I'm just very skeptical of like any other revolution,
technological revolution in history has been very piecemeal, much more piecemeal than you
would expect on paper. If you just thought about, what is the industrial revolution? Well, we dig
up coal that powers the steam engines, you use the steam engines to run these rural roads that
helps us get more coal out. And there's sort of like factorial store you can tell, where in like
a six, six hours, you can be pumping thousands of times more coal. But in real life, it takes
centuries often, right? In fact, the electrification, there's this famous study about how to initially
to electrify factories. It was decades after electricity to change from the pull pullies and
water wheel base system that we had for steam engines to one that's works with more spread
out electrical motors and everything. I think this will be the same kind of thing. It might take
like decades to actually get millions of AI researchers to work together. Okay, great. This
is great. Okay, so a few responses to that. First of all, I mean, I totally agree with the kind of
like real world bottlenecks type of thing. I think this is sort of, you know, I think it's easy to
underrate, you know, basically what we're doing is we're removing the labor constraint, we automate
labor and we like kind of explode technology. But you know, there's still lots of other bottlenecks
in the world. And so I think it's part of why the story is it kind of like starts pretty narrow at
the thing where you don't have these bottlenecks. And then only over time, as we let it kind of
expands to sort of broader areas. AI, this is part of why I think it's like initially this sort of
AI research explosion, right? It's like, AI research doesn't run into these real world bottlenecks.
It doesn't require you to like plow a field or dig up coal. It's just, you're just doing AI research.
The other thing, you know, the other thing about like in your model, AI research, it's not complicated
like about flipping a burger, it's just AI research. Because people make these arguments
like, oh, you know, AGI won't do anything because it can't flip a burger. Like, yeah,
it won't be able to flip a burger, but it's going to be able to do algorithmic progress,
you know, and then when it does algorithmic progress, it'll figure out how to flip a burger.
So the other thing is about, you know, again, these sort of quantities are lower bound, right?
So it's like, this is just like, we can definitely run 100 million of these,
probably what will happen is one of the first things we're going to try to figure out is how to
like, again, run like, you know, translate quantity into quality, right? And so it's like,
even at the baseline rate of progress, you're like, quickly getting smarter and smarter systems,
right? If we said it was like, you know, four years between the preschooler and the high schooler,
right? So I think, you know, pretty quickly, you know, there's probably some like simple
algorithmic changes you find, you know, instead of one Alec Radford, you have 100,
you know, you don't even need 100 million. And then, and then you get even smarter systems.
And now these systems are, you know, they're capable of sort of creative, complicated behavior
you don't understand. Maybe there's some way to like use all this test time compute in a more
unified way rather than all these parallel copies. And, you know, so there won't just be
quantitatively superhuman, they'll pretty quickly become kind of qualitatively superhuman.
You know, it's sort of like, it'll look like, you know, you're a high school student, you're
like trying to wrap yourself, wrap your mind around kind of standard physics. And then there's
some like super smart professor who is like, quantum physics, it all makes sense to him.
And you're just like, what is going on? And sort of, I think pretty quickly, you kind of enter
that regime, just given even the underlying pace of AI progress, but even more quickly than that,
because you have the sort of accelerated force of now this automated AI research.
I agree that over time, you would, I'm not denying that ASI is a thing that's possible.
The time is just not that much, you know?
I'm just like, you know, how is this happening in a year? Like you've, okay, first of all.
So I think the story is sort of like, basically, I think it's a little bit more continuous,
you know, right? Like, I think already, you know, like I talked about, you know, 25, 26,
you're basically going to have models as good as a college graduate. And I, you know,
I don't, I don't know where the unhobbling is going to be, but I think it's possible that
even then you have kind of the proto-automated engineer. So there's, I think there is a bit
of like a smear, kind of an AGI smear or whatever, where it's like, there's sort of
unhobblings that you're missing. There's kind of like ways of connecting them you're missing.
There's like some level of intelligence you're missing. But then at some point,
you are going to get the thing that is like an 100% automated Alec Radford.
Once you have that, you know, things really take off, I think.
Yeah. Okay. So let's go back to the unhobblings. Yeah.
Is there, we're going to get a bunch of models by the end of the year.
Is there something, well, suppose we didn't get some capacity by the end of the year.
Is there some such capacity which lacking would suggest that AI progress is going to
take longer than you are projecting? Yeah. I mean, I think there's two kind of
key things. There's the unhobbling and there's the data wall, right?
I think we should talk about the data wall for a moment. I think the data wall is,
you know, even though kind of like all of this stuff has been about, you know,
crazy AI progress, I think the data wall is actually sort of underrated.
I think there's like a real scenario where we're just stagnant.
You know, because we've been running this tailwind of just like,
it's really easy to bootstrap and you just do unsupervised learning next token prediction
that learns these amazing world models, like bam, you know, great model,
and you just got to buy some more compute, you know, do some simple efficiency changes,
you know, and, and again, like so much of deep learning, all these like big gains on
efficiency have been like pretty dumb things, right? Like, you know, you add a normalization
layer, you know, you know, you fix the scaling laws, you know, and these already have been
huge things, let alone kind of like obvious ways in which these models aren't good yet.
Anyway, so data wall big deal, you know, I don't know, some like put some numbers on this, you know,
some like you do common crawl, you know, online is like, you know, 30 trillion tokens,
llama three trained on 15 trillion tokens. So you're basically already using all the data.
And then, you know, you can get somewhat further by repeating it. So there's an academic paper by,
you know, Boaz Barak and some others that does scaling laws for this. And they're basically
like, yeah, you can repeat it sometime. After 16 times of reputation, just like returns basically
go to zero, you're just completely screwed. And so I don't know, say you can get another 10x on
data from, say like llama three and GP four, you know, llama three is already kind of like
at the limit of all the data, you know, maybe we can get 10x more by repeating data.
You know, I don't know, maybe that's like at most 100x better model than GP4, which is like,
you know, 100x effective compute from GP four is, you know, not that much, you know, if you do
half order magnitude a year of compute half order magnitude a year of algorithmic progress,
you know, that's kind of like two years from GP four. So, you know, GP four finished pretreading
in 22, you know, 24. So I think one thing that really matters, I think we won't quite know by
end of the year, but you know, 25, 26, are we cracking the data wall? Okay, so suppose we had
three orders of magnitude less data in common crawl on the internet than we just happen to have
now. And for decades, the internet, other things, we've been rapidly increasing the stock of data
that humanity has. Is it your view that for contingent reasons, we just happen to have enough
data to train models that are just powerful enough at 4.5 level, where they can kick off
the self play RL loop? Or is it just that we, you know, if it had been three rooms higher,
then it would progress would have been slightly faster. In that world, we would have been looking
back at like, oh, how hard it would have been to like kick off the RL explosion with just 4.5,
but we would have figured it out. And then so in this world, we would have gotten to GBD3,
and then we'd have to kick us on sort of RL explosion. But we would have still figured it out.
We didn't just like luck out on the amount of data we happen to have in the world.
I mean, three rooms is pretty rough, right? Like three rooms, if less data means like six rooms,
smaller, six rooms, less compute model and to chill scaling laws, you know, that's,
it's basically like capping out at like GBD2. So I think that would be really rough. I think you
do make an interesting point about the contingency. You know, I guess earlier we were talking about
the sort of like, when in the sort of human trajectory, are you able to learn from yourself?
And so, you know, if we go with that analogy, again, like if you'd only gotten the preschooler
model, it can't learn from itself. You know, if you only got in the elementary school or model,
it can't learn from itself. And you know, maybe GBD4, you know, smart high school is really where
it starts. Ideally, you have a somewhat better model, but then it really is able to kind of like
learn from itself or learn by itself. So I, yeah, I think there's an interesting, I think, I mean,
I think maybe one room less data, I would be like more iffy, but maybe still doable.
Yeah, I think it would feel chiller if we had, you know, like one or two.
It would be an interesting exercise to get probably distributions of HEI contingent on
across like the use of data. Okay. The thing that makes me skeptical of this story is that the
things, it totally makes sense for free training works so well. These other things, their stories
of in principle, why they ought to work like humans can learn this way and so on. And maybe
they're true, but I worry that a lot of this case is based on sort of first principles with
evaluation of how learning happens that fundamentally we don't understand how humans
learn and maybe there's some key thing we're missing on this sort of sample efficiency. Yeah,
humans actually, maybe there's, you say, well, the fact that these things are way of a less
sample efficient in terms of learning than humans are suggests that there's a lot of room for
improvement. Another perspective is that we are just on the wrong path altogether, right? That's
why there's a sample inefficient when it comes to pre-training. Yeah. So, yeah, I'm just like,
there's a lot of like this first one, so it was arguments stack on top of each other where you
get these unhoplings and then you get to HEI. Then you, because of these reasons why you can stack
all these things on top of each other, you get to ASI. And I'm worried that there's too many steps
of this sort of first principle thinking. I mean, we'll see, right? I mean, on the sort of sample
efficiency thing, again, sort of first principles, but I think, again, there's this clear sort of
missing middle. And so, you know, and sort of like, you know, people hadn't been trying,
now people are really trying, you know, and so it's sort of, you know, I think often again,
in deep learning, something like the obvious thing works. And there's a lot of details to get
right. So it might take some time, but it's now people are really trying. So I think we get a
lot of signal in the next couple of years. You know, on a hobbling, I mean, what is the signal
on a hobbling that I think would be interesting? I think, I think the question is basically like,
are you making progress on this test time compute thing, right? Like, is this thing able to think
longer horizon than just a couple hundred tokens, right? That was unlocked by chain of thought.
And on that point in particular, the many people who have longer timelines have come on the podcast,
have made the point that the way to train this long horizon RL, it's not, I mean,
earlier talking about like, well, they can think for five minutes, but not for longer.
But it's not because they can't physically output an hours or the tokens. It's just really,
at least from what I understand, what they say, right? Like even like Gemini has like a million
in context, and the million of context is actually great for consumption. And it solves one important
on hobbling, which is the sort of onboarding problem, right? Which is, you know, a new co-worker,
you know, in your first five minutes, like a new smart high school intern, first five minutes,
not useful at all, a month in, you know, much more useful, right? Because they've like,
looked at the mono repo and understand how the code works. And they've read your internal docs.
And so being able to put that in context, great, solves this onboarding problem.
Yeah. But they're not good at sort of the production of a million tokens yet.
Yeah. Right. But on the production of a million tokens,
there's no public evidence that there's some easy loss function where you can.
GP4 has gotten a lot better since, it's actually, so the GP4 gains since launch,
I think are a huge indicator that there's like, you know, so you talked about this
with John on the podcast, John said this was mostly post training gains.
Right. You know, if you look at the sort of LMSIS scores,
you know, it's like a hundred ELO or something. It's like a bigger gap than
between Claude III Obus and Claude III Haiku. And the price difference between those is 60X.
But it's not more agentic. It's like better in the same chat about math, right? Like, you know,
it went from like, you know, 40% to 70% math. The crux is like, whether or like, be able to like...
No, but I think, I think it indicates that clearly there's stuff to be done on hobbling.
I think, yeah, I think, I think the interesting question is like, this time of year from now,
you know, is there a model that is able to think for like, you know, a few thousand tokens
coherently, cohesively, agentically? And I think probably there's, you know,
again, this is what I'd feel better if we had an Umur 2 more data, because it's like
the scaling just gives you this sort of like tailwind, right? We're like, for example, tools,
right? Tools, I think, you know, talking to people who try to make things work with tools,
you know, actually sort of GP4 is really when tools start to work. And it's like,
you can kind of make them work with GP3.5, but it's just really tough. And so it's just like,
having GP4, you can kind of help it learn tools in a much easier way. And so there's a bit more
tailwind from scaling. And then, and then, yeah, and does, does, I don't know if it'll work, but
it's a key question.
Okay, I think it's a good place to sort of close that part where we know what the crux is and what
the progress, what evidence that would look like on the AGI to super intelligence.
Maybe it's the case that the games are really easier right now, and you can just sort of let
loose and Alec Ratford give him a compute budget, and he comes out the other end with
something that is an additive, like change as part of the code. This is compute multiplier,
changes to the part. What other parts of the world?
Okay, maybe there here's an interesting way to ask this. How many other domains in the world
are like this, where you think you could get the equivalent of in one year, you just throw
enough intelligence across multiple instances, and you just come out the other end with
something that is remarkably decades centuries ahead. Like, you start off with no flight,
and then you're the right brother is a million instances of GPT-6, and you come out the other
end with Starlink. Is that your model of how things work?
I think you're exaggerating the timelines a little bit, but I think decades worth of
progress in a year or something, I think that's a reasonable prompt.
So I think this is where basically the sort of automated AI researcher comes in, because it
gives you this enormous tail headwind on all the other stuff. So it's like, you automate AI
research with your sort of automated Alec Ratford, you come out the other end, you've done another
five booms, you have a thing that is vastly smarter, not only is it vastly smarter, you've been able
to make it good at everything else. You're solving robotics. The robots are important,
because for a lot of other things, you actually need to try things in the physical world.
I mean, I don't know, maybe you can do a lot in simulation. Those are the really quick worlds.
I don't know if you saw the last NVIDIA GTC, it was all about the digital twins and just having
all your manufacturing processes in simulation. I don't know, again, if you have these super
intelligent, cognitive workers, can they just make simulations of everything, kind of off-of-load
style, and then make a lot of progress in simulation possible. But I also just think you're
going to get the robots. Again, I agree about like there are a lot of real-world bottlenecks,
right? And so, I don't know, it's quite possible that we're going to have crazy drone swarms,
but also lawyers and doctors still need to be humans because of regulation. But I think you
kind of start narrowly, you broaden, and then the world's in which you kind of let them loose,
which again, because of I think these competitive pressures, we will have to let them loose in
some degree on various national security applications. I think like quite rapid progress
is possible. The other thing though is it's sort of basically in the sort of an explosion after
there's kind of two components. There's the A right in the production function, like growth of
technology, and that's massively accelerated by you. Now, you have a billion super-intelligent
scientists and engineers and technicians, you know, superbly competent and everything.
You also just automated labor, right? And so, it's like even without the whole technological
explosion thing, you have this industrial explosion, at least if you let them let them loose,
which is like now you can just build, you know, you can cover Nevada and like, you know, you
start with one robot factory is producing more robots, and basically this like just
the cumulative process because you've taken labor out of the equation.
Yeah, that's super interesting. Yeah, although when you increase the K or the L without increasing
the A, you can look at the Soviet Union or China where they rapidly increase inputs.
And that does have the effect of being geopolitically game-changing, where
it is remarkable. Like you go to Shanghai over a six or some decades.
I mean, they throw out these crazy cities in a decade, right?
Right, right. I mean, the closest thing to like people talk about 30% growth rates or whatever.
Yeah, these are tigers, 10%. It's totally possible, right? And that's just, yeah.
But without productivity gains, it's not like the Industrial Revolution, where like you're,
from the perspective of you're looking at a system from the outside, your goods have gotten cheaper,
or they can manufacture more things. But you know, it's not like the next century is coming at you.
Yeah, it's both. It's both. So it's, you know, both that are important. The other thing I'll
say is like, and all this stuff, I think the magnitudes are really, really important, right?
So, you know, we talked about a 10x of research effort, or maybe 10, 30x over a decade, you know,
even without any kind of like self-improvement type loop, you know, we talk, the sort of,
even in the sort of cheap before the AGI story, we're talking about an order of magnitude of
effective compute increase a year, right? Half an order of magnitude of compute,
half an order of magnitude of algorithmic progress that sort of translates into effective compute.
And so you're doing a 10x a year, right? Basically on your labor force, right? So it's like, it's
a radically different world if you're doing a 10x or 30x in a century versus a 10x a year
on your labor force. So the magnitudes really matter. They also really matter on the sort of
intelligence explosion, right? So like just the automated AI research part. So you know, one story
you could tell there is like, well, ideas get harder to find, right? Algorithmic progress is
going to get harder. Yeah, right now you have the easy wins, but in like four or five years,
there'll be fewer easy wins. And so the sort of automated researchers are just going to be what's
necessary to just keep it going, right? Because it's gotten harder. But that's sort of, it's like
a really weird knife edge assumption economics where you assume it's just that the equilibrium
story you were just telling with why the economy as a whole has 2% economic growth,
because you just proceed on the equal, I guess you're saying by the time here is it's like way
faster, at least, you know, and it's at least, and it depends on the sort of exponents, but it's
basically it's the increase is like, suppose you need to like 10x effective research effort in AI
research in the last, you know, four or five years to keep the pace of progress, we're not just
getting a 10x, you're getting, you know, a million x or 100,000 x, there's just the magnitudes really
matter. And the magnitude is just basically, you know, one, one way to think about this is that
you have kind of two exponentials, you have your sort of like, normal economy, that's growing at
you know, 2% a year, and you have a really like AI economy, and that's going at like 10x a year,
and it's starting out really small, but sort of eventually it's going to it's just it's, it's,
it's, it's way faster and eventually it's going to overtake, right? And even if you have, you can
almost sort of just do the simple revenue extrapolation, right? If you think your AI economy, you know,
that has some growth rate, I mean, it's very simplistic way and so on. But there's, there's
this sort of 10x a year process, and that will eventually kind of like, you're gonna transition
the sort of whole economy from, as it broadens from the sort of, you know, 2% a year to the sort
of much faster growing process. And I don't know, I think that's very like, consistent with historical
chain, you know, stories of right, there's this sort of like, you know, there's this sort of long
run hyperbolic trend, you know, it manifested in this sort of like, sort of change in growth
mode in the austral, you know, revolution, but there's just this long run hyperbolic trend.
And, you know, now you have this sort of, now you have that another sort of change in growth
mode. Yeah, yeah. I mean, that was one of the questions I asked Tyler, when I had him on the
podcast, is that you do go from the fact that after 1776, you go from a regime of negligible
economic growth to 2%, it's really interesting. It shows that, I mean, from the perspective of
somebody in the Middle Ages or before, 2% is equivalent to the sort of 10%. I guess you're
projecting even higher for the AI economy. But I mean, I think again, and it's all this stuff,
you know, I have a lot of uncertainty, right? So a lot of the time I'm trying to kind of tell the
modal story, I think it's important to be kind of concrete and visceral about it. And I, you know,
I have, I have a lot of uncertainty basically over how the 2030s play out. And basically,
the thing I know is it's going to be fucking crazy. But, but, you know, exactly what, you know,
where the bottlenecks are and so on, I think that will be kind of like.
So let's talk through the numbers here. You hundreds of millions of AI researchers. So right
now, GPT-40 turbo is like 15 bucks for a million tokens outputted. And a human thinks 150 tokens
a minute or something. And if you do the math on that, I think it's for an hour's worth of
human output. You, it's like 10 cents or something. Now,
cheaper than a human worker. Cheaper than a human worker. But it can't do the job yet.
That's right. That's right. But by the time you're talking about models that are trained on the 10
gigawatt cluster, then you have something that is four orders of magnitude, more expensive
inference, three orders of magnitude, something like that. So that's like $100 an hour of labor.
And now you're having hundreds of millions of such laborers.
Is there enough compute to do with the model that is a thousand times bigger, this kind of labor?
Great. Okay. Great question. So I actually don't think inference costs for sort of frontier models
are necessarily going to go up that much. So I mean, one historical data point.
But isn't the test time sort of thing that it will go up even higher?
I mean, we're just doing per token, right? And then I'm just saying, you know, if,
suppose each model token was the same as sort of a human token thing at 100 tokens a minute.
So it's like, yeah, it'll use more, but the sort of, if you just, the token calculations
is already pricing that in. The question is like per token pricing, right? And so like
GBD3 went at launch was like actually more expensive than GBD4 now. And so over just like,
you know, fast increases in capability gains, inference costs has remained constant.
That's sort of wild. I think it's worth appreciating. And I think it gestures that sort of an
underlying pace of algorithmic progress. I think there's a sort of like more theoretically
grounded way to why, why inference costs would stay constant. And it's the fourth following story,
right? So on Chichilly scaling laws, right? You know, half of the additional compute,
you allocate to bigger models and half of it, you allocate to more data, right? But also,
if we go with the sort of basic story of half an order a year more compute and half an order
of magnitude a year of algorithmic progress, you're also kind of like, you're saving half an
order of magnitude a year. And so that kind of would exactly compensate for making the model
bigger. The caveat on that is, you know, obviously not all training efficiencies are also inference
efficiencies, you know, bunch of the time they are separately, you can find inference efficiencies.
So I don't know, given this historical trend, given the sort of like, you know, baseline
sort of theoretical reason, you know, I don't know, I, I think it's not crazy baseline assumption
that actually these models, the frontier models are not necessarily going to get more expensive
per token. Oh, really? Yeah. Like, okay, that's, that's wild. We'll see, we'll see. I mean, the other
thing, you know, maybe they get, you know, even if they get like 10x more expensive, then you know,
you have 10 million instead of 100 million, you know, so it's like, it's not really, you know,
like, but okay, so part of the internal explosion is that each of them has to run experiments
that are gbd4 size and the result. So that takes up a bunch of compute. Yes. Then you
need to consolidate the results of the experiments and what is the synthesized
I mean, you have a much bigger influence street anyway, than your training. Sure. Okay. But
I think the experiment compute is a constraint. Yeah. Okay. I'm going back to maybe a sort of
bigger fundamental thing we're talking about here. We're projecting
in a series you say we should denominate the probability of getting to AGI in terms of orders
and magnitude of effective compute effective here, accounting for the fact that there's a
compute quote unquote compute multiplier if you have better algorithms. Yes. And I'm not sure
that it makes sense to be confident that this is a sensible way to project progress. It might be,
but I'm just like, I have a lot of uncertainty about it. It seems similar to somebody trying to
project when we're going to get to the moon and they're like looking at the Apollo program in the
four fifties or something and they're like, we have some amount of effective jet fuel. And if we
get more efficient engines, then we have more effective jet fuel. And so we're going to like
probability of getting to the moon based on the amount of effective jet fuel we have. And I don't
deny that jet fuel is important to launch rockets, but that seems like an odd way to
denominate when you're going to get to the moon. Yeah. Yeah. So I mean, I think these cases are
pretty different. I don't know. I don't think there's a sort of clear, I don't know how rocket
science works, but I didn't, I didn't get the impression that there's some clear scaling
behavior with like, you know, the amount of jet fuel. I think the, I think in AI, you know,
I mean, first of all, the scaling laws, you know, they've just helped, right? And so if you, a friend
of mine pointed this out, and I think it's a great point, if you kind of concatenate both the sort
of original Kaplan scaling laws paper that I think went from 10 to the negative nine to 10
petaflop days, and then, you know, concatenate additional compute to, from there to kind of GP4,
you assume some algorithmic progress, you know, it's like the scaling laws have held, you know,
like probably over 15 ooms, you know, and it was rough, maybe even more held for a lot of ooms.
They held for the specific loss function, which they're trained on, which is training the next
token. Whereas the progress you are forecasting, which we required for further progress
in capabilities, it was specifically, we know that scaling can't work because of the data wall.
And so there's some new thing that has to happen. And I'm not sure whether
the, you can extrapolate that same scaling curve to tell us whether these hobblings will also,
like it's, it's not on the same ground. So the hobblings are just a separate thing.
So this is, this is sort of like, you know, it's, yeah, so I mean, a few things here, right? Okay,
so the, on the, on the effective compute scaling, the, you know, in some sense, I think it's like
people center the scaling laws because they're easy to explain and the sort of like, why, why is
scaling matter? The scaling laws like came way after people, or at least, you know, like Dario,
Ilya realized that scaling mattered. And I think, you know, I think that almost more important
than the sort of loss curve is just like, just in general, make, you know, there's this great quote
from Dario on your, on your, on your podcast, it's just like, you know, Ilya was like, the models,
they just want to learn, you know, you make them bigger, they learn more. And, and that just applied
just across domains, generally, you know, all the capabilities. And so, and you can look at this
in benchmarks. Again, like you say, headwind data wall, and I'm sort of bracketing that and talking
about that separately. The other thing is on hobblings, right? If you just put them on the
effective compute graph, these on hobblings would be kind of huge, right? So like, I think,
what does it even mean? Like, what is it? What is on the y axis here?
Like, say MLPR on this benchmark or whatever, right? And so, you know, like, you know, we mentioned
the sort of, you know, the LMSYS differences, you know, RLHF, you know, again, as good as 100x
smaller chain of thought, right? You know, just going from this prompting change, a simple argument
like change can be like 10x effective increase, compute increases on like math benchmarks. I
think this is like, you know, I think this is useful to illustrate that on hobblings are large.
But I think they're like, I kind of think of them as like slightly separate things. And the kind
of the way I think about is that like, at a per token level, I think GP4 is not that far away
from like a token of my internal monologue, right? Even like 3.5 to 4 took us kind of from like the
bottom of the human range, the top of the human range on like a lot of, you know, on a lot of,
you know, kind of like high school tests. And so it's like a few more 3.5 to 4 jumps per token
basis, like per token intelligence. And then you've got to unlock the test time, you've got to solve
the onboarding problem, make it use a computer. And then you're getting real close.
I'm reminded of again, the story might be wrong, but I think it is strikingly plausible.
I agree. And so I think actually, I mean, the other thing I'll say is like, you know,
I say this 2027 timeline, I think it's unlikely, but I do think there's worlds that are like AGI
next year. And that's basically if the test time compute overhang is really easy to crack,
if it's really easy to crack, then you do like four rooms of test and compute, you know, from
a few hundred tokens to a few million tokens, you know, quickly. And then, you know, again,
maybe it's maybe only takes one or two 3.5 to four jumps per token, like one or two of those
jumps for token plus uses test time compute. And you basically have the proto automated engineer.
So I'm reminded of Stephen Pinker releases his book on what is it the better angels of our nature.
And it's like a couple of years ago or something. And he says the secular decline in violence and
war and everything. And you can just like plot the line from the end of World War Two.
And in fact, before World War Two, then these are just aberrations, whatever.
And basically, as soon as it happens, Ukraine, Gaza, the everything is like, so
and ASI and crazy. I think this is a sort of thing that happens in history or you see history
aligned and you're like, Oh my gosh. And then just like, as soon as you make that prediction,
yeah, who was that famous author? So yeah, this, you know, again, people are predicting deep
learning will hold a wall every year. Maybe one year, they're right. But it's like,
gone a long way and hasn't hit a wall. And you don't have that much to go. And you know, so yeah,
I guess I think this is a sort of plausible story. And let's just run with it and see what it implies.
Yeah. So we were talk in your series, you talk about alignment from the perspective of
this is not about some doomer scheme to get the point zero and personal probability distribution
where things don't go off the rails. It's more about just controlling the systems,
making sure they do what we intend them to do. If that's the case, and we're going to be in the
sort of geopolitical conflict with China, and part of that will involve and what we're worried about
is them making the CCP bots that go out and take the red flag of Mao across the galaxies or something.
Then shouldn't we be worried about alignment as something that if in the wrong hands,
this is the thing that enables brainwashing, sort of dictatorial control. This seems like a
worrying thing. This should be part of this sort of algorithmic secrets we keep hidden, right?
The how to align these models, because that's also something the CCP can use to control their models.
I mean, I think in the world where you get the Democratic coalition, yeah, I mean,
also just alignment is often dual use, right? Like RLHF, you know, it's like
alignment team developed, it was great, you know, it was a big win for alignment, but it's also,
you know, obviously makes these models useful. But yeah, so yeah, alignment enables the CCP bots.
Alignment also is what you need to get the, you know, get the sort of, you know, whatever USAIs,
like follow the Constitution and like disobey law, you know, unlawful orders, and you know,
like respect separation of powers and checks and balances. And so yeah, you need alignment for
whatever you want to do. It's just, it's, it's the sort of underlying technique.
Tell me what you make of this take. I mean, the string with this a little bit. So
fundamentally, there's many different ways the future could go. There's one path in which the
LA's are type crazy AI's with the nanobots take the future and to turn everything into
gray goo or paper clips. And the more you solve alignment, the more that path of the decision
tree is circumscribed. And then so the more you solve alignment, the more it is just
different humans and divisions they have. And of course we know from history that things don't
turn out the way you expect. So it's not like you can decide the future, but it will appear.
It's part of the beauty of it, right? You want these mechanisms, the error correction, pluralism.
But from the perspective of anybody who's looking at the system, it will be like,
I can control where this thing is going to end up. And so the more you solve alignment and the
more you circumscribe the different futures that are the results of AI will, the more that accentuates
the conflict between humans and their visions of the future. And so in the world that alignment is
solved and the world in which alignment is solved is the one, is the world in which you have the
most sort of human conflict over where to take AI. Yeah. I mean, by removing the worlds in which
the AI's takeover, then like, you know, the remaining worlds are the ones where it's like,
the humans decide what happens. And then as we talked about, there's a whole lot of,
yeah, a whole lot of worlds and how that could go. And I worry, so when you think about alignment
and it's just controlling these things, just think a little forward and there's worlds in which
hopefully, you know, human descendants or some version of things in the future merge with super
intelligences and they have the rules of their own, but they're in some sort of law and market-based
order. I worry about if you have things that are conscious and should be treated with rights.
If you read about what alignment schemes actually are, and then you read these books about what
actually happened during the Cultural Revolution, what happened when Stalin took over Russia,
and you have very strong monitoring from different instances where, one, everybody's
tasked with watching each other, you have brainwashing, you have red teaming, where you have the
spice stuff you were talking about, where you try to convince somebody you're on like a defect
or and you see if they defect with you and if they do, then you realize they're an enemy and
then you take, and listen, maybe I'm stretching the analogy too far, but the way, like the ease
of these alignment techniques actually map onto something you could have read about during like
Mouse Cultural Revolution is a little bit troubling. Yeah, I mean, look, I think sentient AI is a
whole other topic. I don't know if we want to talk about it. I agree that like it's going to be very
important how we treat them. You know, in terms of like what you're actually programming these
systems to do, again, it's like alignment is just, it's a technical, it's a technical problem,
a technical solution, enables the CCP bots. I mean, in some sense, I think the, you know,
I almost feel like the sort of model and also about talking about checks and balances is sort of,
you know, like the Federal Reserve or Supreme Court justices, and there's a funny way in which
they're kind of this like very dedicated order, you know, Supreme Court justices, and it's amazing,
they're actually quite high quality, right? And they like really smart people, they really believe
in the Constitution, they love the Constitution, they believe in their principles, they have,
you know, these wonderful, you know, back, you know, and yeah, they have different persuasions,
but they have sort of, I think very sincere kind of debates about what is the meaning of the Constitution,
you know, what is the best actuation of these principles. You know, I guess, you know, by the
way, our conversation sort of skittish or arguments is like the best podcast, you know, but when I
run out of high quality content on the Internet, I mean, I think there's going to be a process of
like figuring out what the Constitution should be. I think, you know, this Constitution is like
worked for a long time, you start with that, maybe eventually things change enough that you
want edit to that. But anyway, you want them to like, you know, for example, for the checks and
balances, they like, they really love the Constitution, and they believe in it, and they
take it really seriously. And like, look, at some point, yeah, you are going to have like AI police
and AI military, but I think sort of like, you know, being able to ensure that they like, you
know, believe in it in the way that like a Supreme Court justice does, or like in the way that like
a federal reserve, you know, official takes their job really seriously. Yeah. And I guess a big
open question is whether if you do the project or something like the project. Sorry, the other
important thing is like a bunch of different factions need their own AIs, right? And so it's,
it's really important that like each political party gets like, have their own, you know, and like
whatever creeds that you might totally disagree with their values, but it's like, it's really
important that they get to like, have their own kind of like super intelligence. And, and again,
I think it's that these sort of like classical liberal processes play out, including like,
different people of different persuasions and so on. And I don't know, maybe the advisors might not
make them, you know, wise, they might not follow the advice or whatever, but I think it's important.
Okay. So speaking of alignment, you seem pretty optimistic. So let's run through the source of
the optimism. Yeah. I think there you laid out different worlds in which we could get AI. Yeah.
There's one that you think is low probability of next year, where a GPT four plus scaffolding plus
unhoplings gets you to AGI. Not GPT four, you know, like, sorry, sorry, it's a GP five. Yeah. And
there's ones where it takes much longer. There's ones where it's something that's a couple years
ahead in a mortal world. Yeah. So GPT four seems pretty aligned in the sense that I don't expect
you to go off the rails and maybe with scaffolding things might change. Looks pretty good. Yeah,
exactly. So the, and maybe you will keep turn at, there's cranks, you keep going up and one of the
cranks gets you to ASI. Yeah. Is there any point at which the sharp left turn happens? Is it when
you start? Is it the case that you think plausibly when they act more like agents? This is the thing
to worry about. Yeah. Is there anything qualitatively that you expect to change with regards to the
alignment perspective? Yeah. These cranks. So I don't know if I believe in this concept of sharp
left turn, but I do think there's basically, I think there's important qualitative changes that
happen between now and kind of like somewhat super human systems, kind of like early on the
intelligence explosion, and then important qualitative changes that happen from like
early intelligence explosion to kind of like true super intelligence and all its power and might.
And let's talk about both of those. And so, okay, so the first part of the problem is one, you know,
we're going to have to solve ourselves, right? We have to going to have to line the like initial
AI and the intelligence explosion, you know, the sort of automated out of Bradford. I think there's
kind of like, I mean, two important things that change from GBD4, right? So one of them is,
you know, if you believe the story on like, you know, synthetic data or L or self play to get
past the data wall. And if you believe this on hobbling story, you know, at the end, you're
going to have things, you know, they're agents, right, including they do long term plans, right?
They have long, long, you know, they're somehow they're able to act over long horizons, right?
But you need that, right? That's the sort of prerequisite to be able to do
the sort of automated research. And so, you know, I think there's basically, you know,
I basically think sort of pre training is sort of alignment neutral, in the sense of like,
it has all these representations that has good representations that, you know,
as as representations of doing bad things, you know, but but there's, there's, it's not like,
you know, scheming against you or whatever. I think the sort of misalignment can arise once
you're doing more kind of long horizon training, right? And so you're training, you know,
again, too simplified example, but to kind of illustrate, you know, you're training in AI to
make money. And, you know, if you're just doing that with reinforcement learning, you know, it's,
you know, it might learn to commit fraud or lie or to see you or seek power,
simply because those are successful strategies in the real world, right? So maybe, you know,
RL is basically it explores, maybe it figures out like, Oh, it tries to like hack, and then it gets
some money, and that made more money. You know, and then if that's successful, if that gets reward,
that's just reinforced. Basically, I think you're there's sort of more serious misalignments,
kind of like misaligned longterm goals that could arise between now and or that sort of
necessarily have to be able to arise if you're able to get long horizon system. That's one.
What you want to do in that situation is you want to add side constraints, right? So you want to add,
you know, don't lie, don't deceive, don't commit fraud. And so how do you add those side constraints?
Right? The sort of basic idea you might have is like RLHF, right? You're kind of like, yeah,
it has this goal of like, you know, make money or whatever, but you're watching what it's doing,
it starts trying to like, you know, lie or deceive or fraud or whatever, or break the law,
you're just kind of like thumbs down, don't do that, you anti reinforce that.
The sort of critical issue that comes in is that these eye systems are getting superhuman,
right? And they're going to be able to do things that are too complex for humans to evaluate,
right? So again, even early on, you know, in the intelligence explosion, the automated AI
researchers and engineers, you know, they might write millions, you know, billions,
trillions of lines of complicated code, you know, they might be doing all sorts of stuff,
you just like don't understand anymore. And so, you know, in the million lines of code, you know,
is it somewhere kind of like, you know, hacking, hacking, or like exultating itself, or like,
you know, trying to go for the nukes or whatever, you know, like, you don't know anymore, right?
And so this sort of like thumbs up, thumbs down, pure RLHF doesn't fully work anymore.
Second part of the picture, and maybe we can talk more about this first part of the picture,
I think it's going to be like, there's a hard technical problem of what do you do sort of
post RLHF, but I think it's a solvable problem. And it's like, you know, there's various things
in bullish on, I think there's like ways in which deep learning has shaped out favorably.
The second part of the problem is you're going from your like initial systems and intelligence
explosion to like super intelligence, and you know, it's like, many ooms, it ends up being like,
by the end of it, you have a thing that's vastly smarter than humans. I think the intelligence
explosion was really scary from an alignment point of view, because basically, if you have this
rapid intelligence explosion, you know, less than a year or two years or whatever, you're going,
say in the period of a year from systems were like, you know, failure would be bad, but it's not
catastrophic to like, you know, saying a bad word, it's like, you know, it's, it's something goes
awry to like, you know, failure is like, you know, it extra traded itself, it starts hacking the
military can do really bad things. You're going less than a year from sort of a world in which,
like, you know, it's some descendant of current systems, and you kind of understand it, and it's
like, you know, has good properties, but something that potentially has a very sort of alien and
different architecture, right, after having gone through another decade of amalgamances.
I think one example there that's very salient to me is legible and faithful chain of thought,
right. So a lot of the time when we're talking about these things, we're talking about, you know,
it has tokens of thinking, and then it uses many tokens of thinking. And, you know, maybe we boot
strapped ourselves by, you know, it's pre trained, it learns to think in English, and we do something
else on top, so it can do the sort of longer chains of thought. And so, you know, it's very
plausible to me that like, for the initial automated alignment researchers, you know, we don't need to
do any complicated mechanistic interpretability, and just like, literally, you read what they're
thinking, which is great, you know, it's like huge advantage, right. However, I'm very likely not
the most efficient way to do it, right, there's like, probably some way to have a recurrent
architecture, it's all internal states, there's a much more efficient way to do it. That's what
you get by the end of the year. You know, you're going this year from like RLHF plus plus some
extension works to like, it's vastly super human, it's like, you know, it's, it's, it's, it's to us,
like, you know, you know, an expert in the field might be to like an elementary school or middle
school or and so, you know, I think it's the sort of incredibly sort of like hairy period for alignment
thing you do have is you have the automated researchers, right. And so you can use the
automated researchers to also do alignment. And so in this world, yeah, why are we optimistic
that the project is being run by people who are thinking, I think, so here's, here's, here's,
here's something to think about. Okay. The open AI, yeah, you start off with people who are very
explicitly thinking about exactly these kinds of things. Yes. Right. But are they still there?
No, no, but you're still here, here's the thing. No, no, even the people who are there, even like
the current leadership is like exactly these things, you can find them in interviews in their
blog post talking about. And what happens is when, as you were talking about, when some sort of
trivial, and Jan talked about it, this is not just you, we all talked about in his tweet thread,
when there is some trade off that has to be made with, we need to do this flashy release this
week and not next week, because whatever Google IO is the next week, so we're going to get it.
And then the trade off is made in favor of the less, the more careless decision.
When we have the government or the national security advisor, the military or whatever,
which is much less familiar with this kind of discourse, is it naturally thinking in this way
about how I'm worried the chain of thought isn't faithful and how do we think about the
features that are represented here? Why should it be optimistic that a project run by people like
that will be thoughtful about these kinds of considerations? I mean, they might not be.
You know, I agree. I think a few thoughts, right? First of all, I think the private world,
even if they sort of nominally care, is extremely tough for alignment, a couple of reasons. One,
you just have the race between the sort of commercial labs, right? And it's like,
you don't have any headroom there to like be like, ah, actually, we're going to hold back for three
months, like get this right. And you know, we're going to dedicate 90% of our compute to automated
alignment research instead of just like pushing the next zoom. The other thing though is like,
in the private world, you know, China has stolen your age, China has your secrets,
they're right on your tails, you're in this fever struggle, no room at all for maneuver.
They're like the way it's like absolutely essential to get alignment right. And you get it
during this intelligence exploration, you get it right, is you need to have that room to maneuver
and you need to have that clear lead. And, you know, again, maybe you've made the deal or whatever,
but I think you're an incredibly tough space, tough spot if you don't have this clearly.
So I think the sort of private world is kind of rough there on like whether people will take
it seriously, you know, I don't know, I have some faith in sort of sort of normal mechanisms of a
liberal society, sort of if alignment is an issue, which, you know, we don't fully know yet, but
sort of the science will develop, we're going to get better measurements of alignment, you know,
and the case will be clear and obvious. I worry that there's, you know, I worry about worlds
where evidence is ambiguous. And I think a lot of them, a lot of the most scary kind of intelligence
explosion scenarios are worlds in which evidence is ambiguous. But again, it's sort of like I if
evidence is ambiguous, then that's the worlds in which you really want the safety margins. And
that's also the worlds in which kind of like running the intelligence explosion is sort of like,
you know, running a war, right? It's like, ah, the evidence is ambiguous, we have to make these
really tough tradeoffs. And you like, you better have a really good chain of command for that.
And it's not just like, you know, yoloing at now, let's go, you know, it's cool. Yeah.
Let's talk a little bit about Germany. We're making the analogy to World War Two. And you made a
really interesting point many hours ago. The fact that throughout history, World War Two
is not unique, at least when you think in proportion to the size of the population.
But these other sorts of catastrophes where a significant portion of the population has
been killed off. Yeah. After that, the nation recovers and they get back to their heights.
So what's interesting after World War Two is that Germany especially, and maybe Europe as a whole,
obviously they experienced fast economic growth in the direct aftermath because of catch up growth.
But subsequently, we just don't think of Germany as, no, we're not talking about Germany potentially
launching an intelligence explosion and they're going to get into the AI table.
We were talking about Iran and North Korea and Russia. We were talking about Germany, right?
Well, because they're allies. Yeah. Yeah. But so what happened? I mean, World War Two and now
it didn't like come back out of the Seven Years War or something, right?
Yeah. Yeah. Yeah. I mean, look, I'm generally very bearish on Germany. I think in this context,
I'm kind of like, you know, it's a little bit, you know, I think you're underrating a little
bit. I think it's probably still one of the, you know, top five most important countries in the
world. You know, I mean, Europe overall, you know, it still has, I mean, it's a GDP that's like
close to the United States, the size of the GDP, you know, and there's things actually that Germany
is kind of good at, right? Like state capacity, right? Like, you know, the roads are good and
they're clean and they're well maintained and, you know, in some sense, the sort of, a lot of this
is the sort of flip side of things that I think are bad about Germany, right? So in the US, it's
a little bit like there's a bit more of a sort of Wild West feeling to the United States, right?
And it includes the kind of like crazy bursts of creativity. It includes like, you know,
political candidates that are sort of, you know, there's a much broader spectrum and, you know,
much, you know, like both in Obama and Trump as somebody you just wouldn't see in the sort of
much more confined kind of German political debate. You know, I wrote this blog post at some
point, Europe's political stupor about this. But anyway, and so there's this sort of punctilious
sort of rule following that is like good in terms of like, you know, keeping your kind of state
capacity functioning. But that is also, you know, I think I kind of think there's a sort of very
constrained view of the world in some sense, you know, and that includes kind of, you know,
I think after World War II, there's a real backlash against anything like elite, you know,
and, you know, again, no, you know, no elite high schools or elite colleges and sort of
what my is that the law excellence isn't cherished, you know, there's, yeah.
Why is that the logical intellectual thing to rebel against if what if you're trying to
overcorrect from the Nazis? Yeah. Was it because the Nazis were very much into elitism? What was
I don't understand why that's a logical sort of counter reaction?
I don't know, maybe it was sort of a counter reaction against the sort of like whole like
Aryan race and sort of that sort of thing. I mean, I also just think there was a certain amount
and what a certain, I mean, look at sort of World War I, end of World War I versus end of World
War II for Germany, right? And sort of, you know, a common narrative is that the piece of Versailles,
you know, was too strict on Germany. You know, the piece imposed after World War II was like
much more strict, right? It was a complete, you know, the whole country was destroyed. You know,
it was, you know, and all the main, most of the major cities, you know, over half of the housing
stock had been destroyed, right? Like, you know, in some birth cohorts, you know, like 40% of the
men had died. Half the population displaced? Oh, yeah. I mean, almost 20 million people
were displaced, right? Huge, crazy, right? You know, like, and the borders are way smaller than
the Versailles borders. Yeah, exactly. And sort of complete imposition of a new political system
and, you know, on both sides, you know, and yeah, so it was, but in some sense that worked out better
than the post-World War I piece, where then there was this kind of resurgence of German nationalism
and, you know, in some sense, the thing that has been a pattern. So it's sort of like, it's unclear
if you want to wake the sleeping beast. I do think that at this point, you know, it's gotten a bit too
sleepy. Yeah. I do think it's an interesting point about we underrate the American political system.
And I've been making the same correction myself. Yeah. There's, there was this book about
burdened by a Chinese economist called China's World View. And overall, I wasn't a big fan,
but they made a really interesting point in there, which was the way in which candidates rise up
through the Chinese hierarchy for politics, for administration, in some sense, that selects for
you're not going to get some Marjorie Taylor Greene or somebody running some.
Don't get that in Germany either. Right. But he explicitly made the point in the book that
that also means we're never going to get a Henry Kissinger or Barack Obama in China. We're going
to get like, by the time they end up in the charge of the Politburo, the Politburo, there'll be like
some 60-year-old bureaucrat who's never like ruffled any feathers. Yeah. I think there's something
really important about the sort of like very raucous political debate. And yeah, in general,
kind of like, you know, there's the sense in which in America, you know, lots of people live in
their kind of like own world. I mean, like we live in this kind of bizarre little like bubble
in San Francisco and people, you know, and, and, but I think that's important for the sort of
evolution of ideas, air correction and that sort of thing. You know, there's other ways in which
the German system is more functional. Yeah. But it's interesting that there's major mistakes,
right? Like the sort of defense spending, right? And you know, then, you know, Russia made Ukraine
and, and you're like, wow, what did we do? Right? No, that's a really good point, right? The main
issues, there's everybody agrees about it. Exactly. Yeah. So it consents this blob kind of thing.
Right. And on the China point, you know, just having this experience of like reading German
newspapers, and I think how much, you know, how much more poorly I would understand the sort of
German debate and sort of the sort of state of mind from just kind of afar. I worry a lot about,
you know, or I think it is interesting just how kind of impenetrable China is to me. It's a
billion people, right? Right. And like, you know, almost everything else is really globalized.
You have a globalized internet and I kind of, I kind of a sense what's happening in the UK.
You know, I probably, even if I didn't read German newspapers, just sort of would have a sense
of what's happening in Germany. But I really don't feel like I have a sense of what like,
you know, what is the state of mind or what is the state of political debate, you know,
of a sort of average Chinese person or like an average Chinese leader. And yeah, I think that,
that I find that distance kind of worrying. And I, you know, and there's, you know, there's some
people who do this and they do really great work where they kind of go through the like party
documents and the party speeches. And it seems to require a kind of a lot of interpretive ability
where there's like very specific words and mentoring that like mean we'll have one connotation
and not the other connotation. But yeah, I think it's sort of interesting given how globalized
everything is. And like, I mean, now we have basically perfect translation machines and it's
still so impenetrable. That's really interesting. I've been, I should, I'm sort of ashamed almost
that I haven't done this yet. I think many months ago, when Alexi interviewed me on his
YouTube channel, I said, I'm meaning to go to China to actually see for myself what's going on.
And actually I'm, I should, so by the way, if anybody listening has a lot of context on China,
if I went to China, who could introduce me to people, please email me.
Oh, you gotta do some pods and you gotta find some of the Chinese AI researchers, man.
I know. I was thinking at some point, again, this is the fact that I have-
I mean, can I speak freely, but you know, I don't know if they can speak freely, but-
I was thinking of there's, so they had these papers and on the paper, they'll say who's a
co-author. It's funny because while I was thinking of just emailing, cold emailing everybody,
like, here's my cabin, let's just talk. I just want to see what is the vibe,
even if they don't tell me anything. I'm just like, what kind of person is this?
Are they, how westernized are they? But as I was saying this, I just remembered that, in fact,
ByteDance, according to mutual friends we have at Google, they cold emailed every single person
on the Gemini paper and said, if you come work for ByteDance, we'll make you an elite engineer,
you'll report directly to the CTO, and in fact, this actually-
That's how the secrets go over, right?
Right. No, I meant to ask this earlier, but suppose they hired what,
if there's only a hundred or so people, or maybe less, we're working on a key algorithmic secrets,
if they hired one such person, is all the alpha gone that these labs have?
If this person was intentional about it, they could get a lot. I mean, they couldn't get the
actually could probably just also actually treat the code. They could get a lot of the key ideas.
Again, up until recently, stuff was published, but they could get a lot of the key ideas if
they tried. I think there's a lot of people who don't actually look around to see what the other
teams are doing, but I think you can. But yeah, I mean, they could. It's scary.
Right. I think the project makes more sense there where you can't just recruit a Manhattan
project engineer and then just get-
And it's like, these are secrets that can be used for probably every training around the future.
That'll be like, maybe are the key to the data wall that are like, they can't go on or they
can't go on that are like, they're going to be worth, given sort of like the multipliers on
compute, hundreds of billions, trillions of dollars, and all it takes is China to offer
a hundred million dollars to somebody and be like, yeah, come work for us. And then, I mean,
yeah, I'm really uncertain on how seriously China is taking AGI right now. One anecdote that was
related to me on the topic of the anecdotes, that you buy another sort of like a kind of
researcher in the field was at some point they were at a conference with somebody,
Chinese AI researcher, and he was talking to him and he was like, I think it's really good
that you're here and like, you know, we got to have the international coordination stuff.
And apparently this guy said that I'm the kind of most senior most person that they're going
to let leave the country to come to things like this.
Wait, what's the takeaway?
As in they're not letting really senior AI researchers leave the country.
Interesting.
Kind of classic, you know, Eastern Block move.
Yeah.
I don't know if this is true, but it's what I heard.
It's interesting. So I thought the point you made earlier about being exposed to
German newspapers and also to, because earlier you were interested in economics and of law and
national security, you have the variety and intellectual diet there has exposed you to
thinking about the geopolitical question here and ways. Others talking about, yeah,
I mean, this is the first episode I've done about this, where we've talked about things
like this, which is now that I think about it, weird to give it that this is an obvious thing
in retrospect, I should have been thinking about. Anyways, so that's one thing we've been missing.
What are you missing? And national security you're thinking about, so you can't say national
security. What like perspective are you probably under exposed to as a result?
And China, I guess you mentioned.
Yeah. So I think the China one is an important one.
I mean, I think another one would be a sort of very Tyler Cowan intake, which is like,
you're not exposed to how, like how will a normal person in America, like, you know,
both like use AI, you know, probably not, you know, and, and, and that being kind of like
bottlenecks to the fusion of these things. I'm overrating the revenue because I'm kind of like,
ah, you know, everyone has stopped adopting it. But, you know, kind of like, you know,
Joe Schmo engineer at a company, you know, like, ah, will they, will they be able to integrate it?
And also the reaction to it, right? You know, I mean, I think this was a question again,
hours ago, where it was about like, you know, won't people kind of rebel against this?
Yeah.
And they won't want to do the project. Maybe they will. Yeah.
Here's a political reaction that I didn't anticipate. Yeah.
So Tucker Carlson is recently ended the Joe Rogan episode. I already told you about this,
but I'm just going to tell the story again. So Tucker Carlson is on Joe Rogan.
Yeah.
And they start talking about World War II. And Tucker says, well, listen, I'm going to say
something that my fellow conservatives won't like, but I think nuclear weapons are immoral.
I think it was obviously immoral that we use them on Nagasaki and Hiroshima.
And then he says, in fact, nuclear weapons are always immoral,
except when we would use them on data centers. In fact, it would be immoral not to use them
on data centers because look, we're, these people in Silicon Valley, these fucking nerds are making
is super intelligent. And they say that it could enslave humanity. We made machines to serve
humanity, not to enslave humanity. And they, they're just going on and making these machines.
And so we should of course be nuking the data centers.
And that is definitely not a political reaction in 2024. I was expecting.
I mean, who knows? It's going to be crazy. It's going to be crazy.
But the thing we learned with COVID is that also the left, right reactions that you would
anticipate just based on hunches, it completely flipped.
Exactly. Initially, like kind of the right is like, you know,
it's like so contingent.
And then, and then, and then, and the left was like, this is racist.
And then it flipped, you know, the left was really into the code. Yeah.
Yeah. And the whole thing also is just like so blunt and crude.
And so, I think, I think probably in general, you know, I think people are really under,
you know, people like to make sort of complicated, technocratic AI policy proposals.
And I think, especially if things go kind of fairly rapidly on the last AGI,
you know, there might not actually be that much space for kind of like complicated,
kind of like, you know, clever proposals that might just be kind of a much cruder reaction.
Yeah. Look, and then also when you mentioned the spies and national security getting involved
and everything, and you can talk about that in the abstract, but now that we're living in San
Francisco and we know many of the people who are doing the top AI research is also a little
scary to think about people I personally know and friends with. It's not unfeasible if they
have secrets in their head that are worth $100 billion or something.
Kidnapping, assassination, sabotage.
Oh, they're family. Yeah, it's really bad. I mean, this is to the point on security,
you know, like right now it's just really foreign, but, you know, at some point,
as it becomes like really serious, it's, you know, you're going to want the security cards.
Yeah. Yeah.
Yeah. So, presumably, you have thought about the fact that people in China will be listening to
this and will be reading your series. And somehow you made the trade off that
it's better to let the whole world know and also including China and make them up to AGI,
which is part of the thing you're worried about is China we can come to AGI than to
stay silent. I'm just curious, walk me through how you've thought about that trade off.
Yeah, I actually, look, I think this is a tough trade off. I thought about this a bunch. You know,
I think, you know, I think people on the PRC will read this.
I think, you know, I think there's some extent to which sort of cat is out of the bag, you know,
this is like not, you know, AGI being a thing people are thinking about very seriously is not
new anymore. There's sort of, you know, a lot of these takes are kind of old or, you know, I've had,
I had, you know, similar views a year ago might not have written it up a year ago, in part because
I think this cat wasn't out of the bag enough. You know, I think the other thing is
I think to be able to manage this challenge, you know, I think much broader swaths in society will
need to wake up, right? And if we're going to get the project, you know, we actually need sort of
like, you know, abroad by partisan understanding, the challenge is facing us. And so, you know,
I think it's a tough trade off, but I think the sort of need to wake up people in the United
States in the sort of Western world and the Democratic coalition is ultimately imperative.
And, you know, I think my hope is more people here will read it than the PRC.
You know, and I think people sometimes underrate the importance of just kind of like writing it
up, laying out the strategic picture. And, you know, I think you've done actually a great service
to sort of mankind in some sense by, you know, with your podcast. And, you know, I think it's
overall been good. Okay, so by the way, you know, on the topic of, you know, Germany,
you know, we were talking at some point about kind of immigration story, right? I feel like
you have a kind of interesting story you haven't told. And I think you should tell.
So a couple of years ago, I was in college and I was 20. Yeah, I was about to turn 21.
Yeah, I think it was. Yeah, you came from India when you were really young.
Right. Yeah. So I was eight or nine. I lived in India and then we moved around all over the place.
But because of the backlog for Indians. Yeah, the green card backlog. Yeah, it's
we were, we've been in the queue for like decades. Even though you came at eight, you're still on the
you know, H1B. Yeah. And when you're 21, you get kicked off the queue and you had to restart
the process. I'm on my dad's, my dad's a doctor and I'm on his H1B as it depended. But when you're
21, you get kicked off. Yeah. And so I'm 20 and I just like kind of dawns on me that this is my
situation. Yeah. And you're completely screwed. Right. And so I also had experience that my dad,
we've like moved all around the country. They have to prove that him as a doctor is like,
you can't get native talent. Yeah. And you can't start a startup. Yeah.
Yeah. So where can you not get like, even getting the H1B for you would have been like 20%
lottery. So if you're lucky, you're in this. And they had to prove that they can't get native
talent, which means like for him, I'm like, we lived in North Dakota for three years,
West Virginia for three years, Maryland, West Texas. Yeah. And so kind of dawn on me, this is
my situation. And as I turn 21, I'll be like on this lottery, even if I get the lottery,
I'll be a fucking code monkey for the rest of my life because this thing isn't going to let up.
Yeah. Can't do a startup. Exactly. And so at the same time, I had been reading
for the last year, I've been super obsessed with Paul Graham essays. My plan at the time was to
make a startup or something. I was super excited about that. And it just occurred to me
that I couldn't do this. Yeah. That like, this is just not in the cars for me. Yeah.
And so I was kind of depressed about it. I remember I kind of just,
I was in a daze through finals because I had just occurred to me and I was really like
anxious about it. And I remember thinking to myself at the time that if somehow I end up
getting my green card before I turn 21, there's no fucking way I'm turning, becoming a code monkey
because the thing that I've, like this feeling of dread that I have is this realization that
I'm just going to have to be a code monkey. And I realize that's my default path. Yeah. If I hadn't
sort of made a proactive effort not to do that, I would have graduated college as a computer science
student and I would have just done that. And that's the thing I was super scared about. Yeah. So
that was an important sort of realization for me. Anyways, so COVID happened because of that
since there weren't foreigners coming, the backlog cleared fast. And by the skin of my teeth, like
a few months before I turned 21, extremely contingent reasons, I ended up getting a green
card because I got a green card. The whole podcast, right? Exactly. I graduated college and I was
bumming around and I graduated just semester early. I'm going to do this podcast, see what
happens. And it hadn't had a green card. It's such a magnificent cultural art of fact. And it only
existed because, yeah, it's actually, because I think it's hard. It's probably it's, what is the
impact of immigration reform? What is the impact of clearing whatever 50,000 green cards in the
backlog? And you're such like an amazing example of like, you know, all of this is only possible.
And it's, yeah, it's, I mean, it's just incredibly tragic that this is so dysfunctional. Yeah. Yeah.
No, it's insane. I'm glad you did it. I'm glad you kind of like, you know, tried the, you know,
the, the, the unusual path. Well, yeah, but I could only do it. I'm obviously I was extremely
fortunate that I got the green card. I was like, I had a little bit of saved up money and I got a
small grant out of college. Thanks to the future fund to like do this for basically the equivalent
of six months. And so it turned out really well. And then at each time when I was like, oh, okay,
podcast, come on, like, I wasted a few months on this, let's now go do something real. Something
big would happen. I would, Jeff Bezos would, huh? You kept with it. Yeah. Yeah. But there would always
be just like the moment I'm about to quit the podcast, something like Jeff Bezos would say
something nice about me on Twitter. The daily episodes gets like a half a million views, you
know, and then now this is my career, but it was a sort of very looking back on it, incredibly
contingent that things worked out the right way. Yeah. I mean, look, if the AGI stuff goes down,
you know, it will be, uh, it'll be the most important kind of like, you know, source of, uh,
it'll be how maybe most of the people who kind of end up feeling the AGI are sure about it.
Also very much, uh, you're very linked with the story in many ways. First, the, um, the,
I got like a $20,000 grant from, uh, a future fund right out of college. Yeah. And that sustained
to me for six months or a hour along it was. Yeah. And without that, I wouldn't...
Tiny grant. It was kind of crazy. Yeah. Ten grand or what was it?
It was, you know, it just, it's tiny, but, you know, it's, it goes to show kind of how
far small grants can go. Yeah. Sort of the immersion ventures too. Yeah. Exactly. The
immersion ventures and the, um, well, the last year I've been in San Francisco, we've just been
in close contact the entire time and just bouncing ideas back and forth. We're just basically,
the alpha I have, I think people would be surprised by how much I got from you, Sholto,
Trent and a couple others. Just, uh... I mean, it's been, it's been an absolute pleasure.
Yeah. Likewise. Likewise. It's been super fun. Yeah.
Okay. So some random questions for you. Yeah. If you could convert to Mormonism.
Yeah. And you could really believe it. Yeah.
Would you do it? Would you push the button?
Well, okay. Okay. Before I answer that question, one sort of observation about the Mormons.
So actually there's that, there's an article that actually made a big impact on me. Yeah.
I think it was by McKick Hop and at some point, you know, in the, the Atlantic or whatever about
the Mormons. And I think the thing he kind of, you know, and I think he even was like
interviewed Romney and so on. And I think the thing I thought was really interesting in this
article was he kind of talked about how the experience of kind of growing up different,
you know, growing up very unusual, especially if you grow up Mormon outside of Utah, you know,
like the only person who doesn't drink caffeine, you don't drink alcohol, you're kind of weird.
How that kind of got people prepared for being willing to be kind of outside of the norm later
on. And like, you know, Mitt Romney, you know, was willing to kind of take stands alone, you know,
in his party because he believed, you know, what he believed was true. And I don't, I mean,
probably not to the same way, but I feel a little bit like this from kind of having grown up in
Germany, you know, and really not having like this sort of German system and having been kind
of an outsider or something. I think there's a certain amount in which kind of growing up in
an outsider gives you kind of unusual strength later on to be kind of like willing to say what
you think. And anyway, so that is one thing I really appreciate about the Mormons, at least the
ones that grew up outside of Utah. I think, you know, the fertility rates, they're good, they're
important. They're going down as well, right? This is the thing that really clinched the kind
of fertility decline story. Even the Mormons. Yeah, even the Mormons, right? You're like, oh,
this is like a good sort of good sort of, Mormons will replace everybody. I don't know if it's
good, but it's like, at least, you know, at least come on, you know, like at least some people will
maintain high, you know, but it's no, no, you know, even the Mormons and sort of basically,
once the sort of, these religious subgroups have high fertility rates, right? Once they kind of
grow big enough, they become, they're too close in contact with sort of normal society and become
normalized. Mormon fertility rates dropped from, I don't remember the exact numbers, maybe like
four to two in the course of 10, 20 years. Anyway, so it's like, you know, now people point to the,
you know, Amish or whatever, but I'm just like, it's probably just not scalable. And if you grow
big enough, then there's just like, you know, this sort of like, you know, this sort of like
overwhelming force of modernity kind of gets you. Yeah. No, if I could convert to Mormonism,
look, I think there's something, I don't believe it, right? If I believed it, I obviously would
convert to Mormonism, right? Because it's, you got to, you got to convert. But you can choose a
world in which you do believe it. I think there's something really valuable and kind of believing
in something greater than yourself and believing and having a certain amount of faith.
You do, right? And you know, there's a, you know, feeling some sort of duty to the thing
greater than yourself. You know, maybe my version of this is somewhat different. You know, I think
I feel some sort of duty to like, I feel like there's some sort of historical weight on like
how this might play out. And I feel some sort of duty to like make that go well. I feel some sort
of duty to, you know, our country, to the national security of the United States. And
you know, I think, I think that, I think it can be a force for a lot of good.
I, the, going back to the opening, I think, just the thing that's especially impressive about that
is, look, there's people who at the company who have through years and decades of building up
savings from working in tech have probably tens of millions, liquid, more than that in terms of
their equity. And the person, very many people were concerned about the clusters in the Middle
East and the secrets leaking to China and all these things. But the person who actually made
a hassle about it, and I think hassling people is so underrated. I think that one person who made
a hassle about it is the 22-year-old who has less than a year at the company who doesn't have
savings built up. Who isn't like a solidified member of the, I think that's a sort of like,
maybe it's me being naive and, you know, not knowing how big companies work and, you know,
but like, there's a, you know, I think sometimes a bit of a speech geontologist, you know, I kind
of believe in saying what you think. Sometimes friends tell me I should be more of a speech
consequentialist. No, I think I really think the amount of people who, when they have the
opportunity to talk to the person, will just bring up the thing. I've been with you in multiple
contexts and I guess I shouldn't reveal who the person is or what the context was, but I've just
been like very impressed that the dinner begins and by the end, somebody who has a major voice
in how things go is seriously thinking about a worldview they would have found incredibly alien
before the dinner or something. And I've been impressed that like, just like, just give them
the spiel and hassle them. I mean, look, I just, I think, I think I feel this stuff pretty viscerally
now. You know, I think there's a time, you know, there's a time when I thought about this stuff
a lot, but it was kind of like econ models and like, you know, kind of like these sort of theoretical
abstractions and, you know, you talk about human brain size or whatever. And I think, you know,
since, I think since at least last year, you know, I feel like, you know, I feel like I can see it,
you know, and I just, I feel it. And I think I can like, you know, I can sort of see the cluster
that AGI is going to be training on and I can see the kind of rough combination of algorithms and the
people that be involved and how this is going to play out. And, you know, I think, look, we'll see
how it plays out. There's many ways this could be wrong. There's many ways it could go. But I think
this could get very real. Yeah. Should we talk about what you're up to next? Sure. Yeah. Okay.
So you're starting an investment firm, anchor investments from Nat Friedman, Daniel Gross,
Patrick Collison, John Collison. First of all, why is this thing to do? You believe the AGI is
coming in a few years? Why the investment firm? Good question. Fair question. Wait, so I mean,
a couple of things. One is just, you know, I think we talked about this earlier, but it's like the
screen doesn't go blank, you know, when sort of AGI intelligence happens, I think people really
underrate the sort of basically the sort of decade after it, you have the intelligence explosion,
that's maybe the most sort of wild period. But I think the decade after is also going to be wild.
And, you know, this combination of human institutions, but super intelligence,
you have crazy kind of geopolitical things going on, you have the sort of broadening of this
explosive growth. And basically, yeah, I think it's going to be a really important period. I think
capital really matters, you know, eventually, you know, like, you know, going to go to the stars,
you know, going to go to the galaxies. So anyway, so part of the answer is just like, look,
I think done right, there's a lot of money to be made, you know, I think if AGI were priced
in tomorrow, you could maybe make 100x, probably you can make even way more than that because of
the sequencing. And, and, and, you know, capital matters. I think the other reason is just,
you know, some amount of freedom and independence. And I think, you know,
you know, I think there's some people who are very smart about this AI stuff and who are kind of
like see it coming. But I think almost all of them, you know, are kind of, you know, constrained in
various ways right there in the labs, you know, they're in some, you know, some other position
where they can't really talk about the stuff. And, you know, in some sense, I've really admired
sort of the thing you've done, which is I think it's really important that there's sort of voices
of reason on the stuff publicly, or people who are in positions to kind of advise important actors
and so on. And so I think there's a, you know, basically the thing this investment firm will be
will be kind of like, you know, a brain trust on AI, it's going to be all about situational
awareness. We're going to have the best situational awareness in the business. You know, we're going
to have way more situational business than any of the people who manage money in the, you know,
New York. We're definitely going to, you know, we're going to do great on investing. But it's
the same sort of situational awareness that I think is going to be important for understanding
what's happening, being a voice of reason publicly and, and, and sort of being able to be in a
position to advise. Yeah. I, there was the book about Peter Thiel. Yeah. They had a interesting
quote about his hedge fund. I think it got terrible returns. So this isn't the example.
I mean, that's, that's, that's in a bear case, right? It's like two theoretical and sure. Yeah.
But they had an interesting quote that it's, it's, that it's like basically a think tank inside of
a hedge fund. Yeah. And try to build. Right. Yeah. So presumably you've thought about the ways in
which these kinds of things can blow. There's a very, there's a lot of interesting business
history books about people who got the thesis right, but timed it wrong. Yeah. Where they,
they buy that internet's going to be a big deal. Yeah. They sell at the wrong time and buy the
wrong time or the dot com boom. Yep. And so they miss out on the gains, even though they're right
about the, anyways, yeah. Well, what, what is that trick to preventing that kind of thing?
Yeah. I mean, look, obviously you can't, you know, not blowing up is sort of like,
you know, task number one and two or whatever. I mean, you know, I think this investment firm,
it is going to just be betting on AGI, you know, betting on AGI and super intelligence before the
decade is out, taking that seriously, making the bets you would make, you know, if you took that
seriously. So, you know, I think if that's wrong, you know, firm is not going to do that well.
The thing you have to be resistant to is like, you have to be able to resist and get one or a
couple or a few kind of individual calls, right? You know, it's like AI stagnates for a year because
of the data wall, or like, you know, you got, you got the call wrong on like when revenue would go
up. And so anyway, that's pretty critical. You have to get timing right. I do think in general
that the sort of sequence of bets on the way to AGI is actually pretty critical. And I think a thing
people underrate. So, all right, I mean, yeah, so like, where does the story start, right? So like,
obviously the sort of only bet over the last year was NVIDIA. And, you know, it's obvious now,
very few people did it. This is sort of also, you know, a classic debate I and a friend had with
another colleague of ours, where this colleague was really into TSM, you know, TSMC. And he was
just kind of like, well, you know, like, these tabs are going to be so valuable. And also like,
NVIDIA, there's just a lot of videos in credit risk, right? It's like, maybe somebody else makes
better GPUs. That was basically right. But sort of only NVIDIA had the AI beta, right? Because
only NVIDIA was kind of like large fraction AI, the next few doublings would just like meaningfully
explode their revenue. Whereas TSMC was, you know, a couple percent AI. So, you know, even though
there's going to be a few doublings of AI, not going to make that big of an impact. All right.
So it's sort of like, the only place to find the AI beta basically was NVIDIA for a while.
You know, now it's broadening, right? So now TSM is like, you know, 20% AI by like 27 or something
is what they're saying. One more doubling, it'll be kind of like a large fraction of what they're
doing. And, you know, there's a whole, you know, a whole stack, you know, there's like, you know,
there's people making memory and COAS and, you know, power, you know, utility companies are
starting to get excited about AI. And they're like, Oh, it'll, you know, power production in the
United States will grow, you know, not 2.5%, 5% of the next five years. And I'm like, no, it'll grow
more. You know, at some point, you know, you know, like a Google or something becomes interesting,
you know, people are excited about them with AI, because it's like, Oh, you know, AI revenue will
be, you know, 10 billion or tens of billions. I'm kind of like, I don't really care about them
before then I care about it, you know, once it, you know, once you get the AI beta, right? And so
at some point, you know, Google will get, you know, $100 billion of revenue from AI,
probably their stock will explode, you know, they're going to become, you know, 5 trillion,
10 trillion dollar company. Anyway, so the timing there is very important, you have to get the
timing right, you have to get the sequence right. You know, at some point, actually, I think, like,
you know, there's going to be real tailwind to equities from real interest rates, right? So
basically in these sort of explosive growths worlds, you would expect real interest rates to
go up a lot, both on the sort of like, you know, a basic both sides of the equation, right? On
the supply side, or on the sort of demand for money side, because, you know, people are going to
be making these crazy investments, you know, initially in clusters, and then in the roba
factories or whatever, right? And so they're going to be borrowing like crazy. They want all this
capital, higher ROI. And then on the sort of like consumer saving side, right, to like, you know,
to give up all this capital, you know, sort of like Euler equation, standard sort of
intratemporal transfer, you know, trade off of consumption.
The standard.
Some of our friends have a paper on this, you know, basically, if you expect, you know,
if consumers expect real growth rates to be higher, you know, interest rates are going to be
higher, because they're less willing to give up consumption, you know, consumption in the,
they're less willing to give up consumption today for consumption in the future.
Anyway, so at some point, real interest rates will go up, if sort of ADA is greater than one,
that actually means equities, you know, higher growth rate expectations mean equities go down,
because the sort of interest rate effect outweighs the growth rate effect. And so, you know,
at some point, there's like big, the big bond short, you got to get that right, you know,
you got to get it right, that, you know, nationalization, you know, like, you got, you know,
anyway, so there's this whole sequence of things, you got to get that right.
And the unknown unknowns, unknown unknowns, yeah. And so you look, you've got to be really,
really careful about your, like, overall, like, dispositioning, right? And because, you know,
if you expect these kind of crazy events to play out, there's going to be crazy things you
didn't see. You know, you do also want to make the sort of kind of bets that are tailored to
your scenarios in the sense of like, you know, you want to find bets that are bets on the tails,
right? You know, I don't think anyone is expecting, you know, interest rates to go above,
you know, 10% like real interest rates. But, you know, I think there's at least a serious chance
of that, you know, before the decade is out. And so, you know, maybe there's some like cheap
insurance you can buy on that, you know. Very silly question. Yeah. In these worlds,
yeah, our financial markets, where you make these kinds of bets going to be respected and
like, you know, like, is my fidelity account going to mean anything when we have their 50%
economic growth? Like, who's, who's like, we got to respect his property rights?
That's pretty deep into it. The bond store at the sort of 50 and 52nd hour growth,
that's pretty deep into it. I mean, again, there's this whole sequence of things. But,
yeah, no, I think property rates will be instructed. Again, in the sort of modal world,
the project, yeah. At some point, at some point, there's going to be figuring out the property
rates for the galaxies, you know, and that'll be interesting. That will be interesting.
So there's an interesting question about going back to your strategy about, well,
the 30s will really matter a lot about how the rest of the future goes. And you want to be in a
position of influence by that point, because of capital. It's worth considering. As far as I know,
there's probably a whole bunch of literature on this. I'm just riffing. But the, the landed
gentry during the, before the beginning of the industrial revolution, I'm not sure if they
were able to leverage their position in a sort of George's or Piketty type sense in order to accrue
the returns that were realized through the industrial revolution. And I don't know what
happened. At some point, they were just weren't the landed gentry. But I'd be concerned that even
if you make great investment calls, you'll be like the guy who owned a lot of land, farmland,
before the industrial revolution, and like the guy who's actually going to make a bunch of money is
the one with the C mentioned, even if he doesn't make that much money, most of the benefits are
sort of widely diffused and so forth. I mean, I think that the analog is like you sell your land,
you put it all in sort of the people who are building the new industry.
I think the, I mean, I think the sort of like real depreciating asset, you know, for me is human
capital, right? Yeah, no, look, I'm serious, right? It's like, you know, there's something about like,
you know, I was like valedictorian of Columbia, you know, the thing that made you special is
you're smart. But actually, like, you know, that might not matter in like four years, you know,
because it's actually automatable. And so anyway, friend joke that the sort of investment firm is
perfectly hedged for me. It's like, you know, either like AGI this decade, and yeah, your human
capital is depreciated, but you've turned that into financial capital, or you know, like no AGI
this decade, in which case, maybe the firm doesn't do that well, but you know, you're still in your
twenties and you're still smart. Excellent. And what's your story for why AGI hasn't been priced in?
The story, financial markets are supposed to be very efficient, it's very, very hard to get an edge.
Here, naively, you just say, well, I've looked at these scaling curves and they imply that we're
going to be buying much more computed energy than the analysts realize. Shouldn't those analysts
be broke by now? What's going on? Yeah, I mean, I used to be a true EMH guy. I was an economist,
you know, I am, you know, I think the thing I, you know, changed my mind on is that I think
there can be kind of groups of people, smart people, you know, who are, you know, say they're
in San Francisco, who do just have off over the rest of society and kind of seeing the future.
And so like COVID, right? Like, I think there's just honestly kind of similar group of people
who just saw that and called it completely correctly. And, you know, they showed at the
market, they did really well, you know, a bunch of other sort of things like that.
So, you know, why is AGI not priced in? You know, it's sort of, you know, why hasn't the
government nationalized the labs yet, right? It's like, you know, this, you know, society
hasn't priced it in yet, and sort of it hasn't completely diffused. And, you know, again,
it might be wrong, right? But I just think sort of, you know, not that many people take these
ideas seriously yet. Yeah. Yeah. Yeah. A couple of other sort of ideas that I was playing around
with with regards to reading it a chance to talk about, but the systems competition,
there's a very interesting, one of my favorite books about World War II is the Victor Davis Hansen
summary of everything. And he explains why the allies made better decisions than the axes.
Why did they? And so obviously, there were some decisions the axis made that were pretty like
Blitzkrieg, whatever. That was sort of by accident, though. In what sense? That they just had the
infrastructure left over? Well, no, I mean, the sort of, I think, I mean, I don't, I mean, I think
sort of my read of it is Blitzkrieg wasn't kind of some like a genius strategy. It was just kind of,
it was like more like their hand was forced. I mean, this is sort of the very Adam Tuzi
and story of World War II, right? But it was, you know, there's sort of this long war versus
short war. I think it's actually kind of an important concept. I think sort of Germany realized
that if they were in a long war, including the United States, you know, they would not be able
to compete industrially. So their only path to victory was like make it a short war, right? And
that that sort of worked much more spectacularly than they thought, right? And sort of take over
France and take over much of Europe. And so then, you know, the decision to invade the Soviet Union,
it was, you know, it was, it was, look, if it was, it was about the western front in some sense,
because it was like, we've got to get the resources. You know, we don't, we're actually,
we don't actually have a bunch of the stuff we need, like, you know, oil and so on. You know,
Auschwitz was actually just this giant chemical plant to make kind of like synthetic oil and a
bunch of these things. It's the largest industrial project in Nazi Germany. And so, you know, and
sort of they thought, well, you know, we completely crushed them in World War I, you know,
it'll be easy, we'll invade them, we'll get the resources, and then we can fight on the western
front. And even during the sort of whole invasion of the Soviet Union, even though kind of like a
large amount of the sort of, you know, the sort of deaths happened there, you know, like a large
fraction of German industrial production was actually, you know, like planes and naval or,
you know, and so on, those drafted, you know, towards the western front and towards the,
you know, the western allies. Well, and then so the point that Hansen was making was,
by the way, I think this concept of like long war and short war is kind of interesting and
with respect to thinking about the China competition, which is like, you know, I worry
a lot about kind of, you know, American decline of sort of American, like late in American industrial
capacity, you know, like, I think China builds like 200 times more ships than we do right now.
You know, some crazy way. And so it's like, maybe we have this superiority,
say in the non AI worlds, we have the superiority in military material kind of like win a short
war, at least, you know, kind of defend Taiwan in some sense. But like, if it actually goes on,
you know, it's like, maybe China is much better able to mobilize, mobilize industrial resources
in a way that like, we just don't have that same ability anymore. I think this is also relevant
to the thing in the sense of like, if it comes down to sort of a game about building, right,
including like, maybe AGI takes the trillion dollar cluster, not the hundred billion dollar cluster,
maybe, or even maybe AGI takes the, you know, is on the hundred billion dollar cluster. But, you
know, it really matters if you can run, you know, tax, you can do one more order of magnitude of
compute for your super intelligence or whatever, that, you know, maybe right now they're behind,
but they just have this sort of like raw light and industrial capacity to outbuild us.
And that matters both in the run up to AGI and after, right, where it's like, you have this
super intelligence on your cluster, now it's time to kind of like expand the explosive growth.
And, you know, like, will we let the robo factories run wild? Like, maybe not, but like,
maybe China will, or like, you know, will we will? Yeah, will we produce the how many, how many of
the drones will we produce? And I think, yeah, so there's some sort of like outbuilding in the
industrial explosion that I worked on. You've got to be one of the few people in the world
who is both concerned about alignment, but also wants to make sure that we'll let the
robo factories proceed once we get the ASI to beat out China, which is like, it's all,
it's all part of the picture. Yeah, yeah, yeah. And by the way, speaking of the ASIs and the
robot factories, one of the interesting things, one of the interesting things,
there's this question of what you do with industrial scale intelligence. And obviously,
it's not chatbots, but it's a, I think it's very hard to predict. But the history of oil is very
interesting, where in the, I think it's in the 1860s that we figure out how to refine oil,
some geologists. And so then standard oil gets started, there's this huge boom. It changes
American politics, entire legislators are getting bought out by oil interest and
presidents are getting elected based on the divisions about oil and breaking them up and
everything. And all of this has happened. The world has never revolutionized before the car
has been invented. And I, so when the light bulb was invented, I think it was like 50 years after
oil refining had been discovered, as majority of standard oil's history is before the car is
invented. Carousine lamps. Exactly. So it's just used for lighting. Then they thought oil would
just no longer be relevant. Yeah, yeah. So there was a concern that standard oil would go brain
corrupt when, when the light bulb was invented. And, but then there's sort of, you realize that
there's immense amount of compressed energy here. You're going to have billions of gallons of this
stuff a year. And it's hard to sort of predict in advance what you can do with that. And then
later on, it turns out transportation cars with, that's, that's, that's what it used for.
Anyways, with intelligence, maybe one answer is the intelligence explosion. But even after that,
so you have all these ASIs and you have enough compute, especially the compute they'll build
to run hundreds of millions of GPUs will hum. Yeah. But what are we doing with that? And it's
very hard to predict in advance. And I think it would be very interesting to figure out what
would the Jupiter brains will be doing. So look, there's situational awareness of where things
stand now. And we've gotten a good dose of that. Obviously, a lot of the things we're talking about
now, you couldn't have prejudged many years back in the past, right? And part of your role
do implies that things will accelerate because of AI getting the process, but many other things
that we are that are unpredictable fundamentally, basically how people will react, how the political
system react, how foreign adversaries will react, that those things will become evident over time.
So the situal awareness is not just knowing where the picture stands now, but being in a position
to react appropriately to new information, to change your worldview as a result, to change
your recommendations as a result. What is the appropriate way to think about situational
awareness as a continuous process rather than as a one-time thing you realized?
Yep. No, I think this is great. Look, I think there's a sort of mental flexibility and willing
to change your mind. That's really important. I actually think this is sort of like how a lot
of brains have been broken in the AGI debate. The doomers who actually, I think we're really
prescient on AGI and thinking about the stuff a decade ago, but they haven't actually updated on
the empirical realities of deep learning. They're sort of like, their proposals are really even
unworkable. This doesn't really make sense. There's people who come in with sort of a predefined
ideology. They're just kind of like the EX a little bit. They like to shitpost about technology,
but they're not actually thinking through. I mean, either the sort of stagnationists who think
this stuff is only going to be a chatbot. Of course, it isn't risky, or they're just not
thinking through the kind of like actually immense national security implications and how
that's going to go. I actually think there's kind of a risk in kind of like having written the stuff
down and put it online. I think this sometimes happens to people as a sort of calcification
of the worldview because now they've publicly articulated this position. Maybe there's some
evidence against it, but they're clinging to it. I want to give the big disclaimer on like,
I think it's really valuable to paint this sort of very concrete and visceral picture.
I think this is currently my best guess on how this decade will go. I think if it goes anywhere
like this, it will be wild. But given the map at pace of progress, we're going to keep getting a
lot more information. I think it's important to sort of keep your head on straight about that.
You know, I feel like the most important thing here is that, and this relates to some of the
stuff we've talked about and the world being surprisingly small and so on. I feel like I used
to have this worldview of like, look, there's important things happening in the world, but
there's like people who are taking care of it. And there's like the people in government and
there's again, even like AI labs have idealized and people are on it. Surely there must be on it,
right? And I think just some of this personal experience, even seeing how kind of COVID went,
you know, people aren't necessarily, there's not some, not just that somebody else is just
kind of on it and making sure this goes well, however it goes. You know, the thing that I think
will really matter is that there are sort of good people who take this stuff as seriously as it
deserves and who are willing to kind of take the implication seriously, who are willing to, you
know, have situational awareness, are willing to change their minds, are willing to sort of
stare the picture in the face. And, you know, I'm counting on those good people.
All right, that's a great place to close Leopold.
Thanks so much, Sharkash. This is the absolute joy.
Hey, everybody. I hope you enjoyed that episode with Leopold. There's actually one more riff
about German history that he had after a break and it was pretty interesting because I didn't
want to cut it out. So I've just included it after this outro. You can advertise on the show now.
So if you're interested, you can reach out at the form in the description below.
Other than that, the most helpful thing you can do is just share the episode if you enjoyed it.
Send it to group chats, Twitter, wherever else you think people who might like this episode
might congregate. And other than that, I guess here's this riff on Frederick the Great.
See you on the next one.
I mean, I think the actual funny thing is, you know, a lot of the sort of German history stuff
we've talked about is sort of like not actually stuff I learned in Germany. It's sort of like
stuff that I learned after. And there's actually, you know, a funny thing where I kind of would go
back to Germany over Christmas or whatever. I totally understand the street names. You know,
it's like, you know, Gneisenau and Scharnhorst and then all these like Prussian military reformers.
And you're like finally understood, you know, Sansa C. And you're like, it was for Frederick,
you know, Frederick the Great is this really interesting figure. So he's this sort of,
in some sense, kind of like gay lover of arts, right, where he hates speaking German. He only
wants to speak French. You know, he like plays the flute, he composes, he has all the sort of great,
you know, artists of his day, you know, over at Sansa C. And he actually had this sort of like
really tough upbringing where his father was this sort of like really stern sort of Prussian
military man. And he had had a Frederick the Great as sort of a 17 year old or whatever,
he basically had a male lover. And what his father did was imprison his son and then I think hang
his male lover in front of him. And again, his father was this kind of very stern Prussian guy,
he was this kind of gay, you know, lover of arts. But then later on Frederick the Great turns out
to be this like, you know, one of the most kind of like, you know, successful kind of Prussian
conquerors, right? Like you get Silesia, he wins the Seven Years War, you know, also, you know,
amazing military strategists, you know, amazing military strategy at the time consisted of like,
he was able to like flank the army. And that was crazy, you know, and that was brilliant. And then
and then they like almost lose the Seven Years War at the very end, you know, the sort of the
Russian Tsar changes. And he was like, Ah, I'm actually kind of a Prussia stan. You know, I think
I'm like, I'm into this stuff. And then he lets, you know, let's Frederick the Great loose and
he had let's let's let's let their army be okay. And anyway, sort of like, yeah, kind of bizarre,
interesting figure in German history.

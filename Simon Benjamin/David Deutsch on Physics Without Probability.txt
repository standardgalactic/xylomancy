Hello everyone, thanks for turning out. Those of you who are not in physics coming here in the rain.
It's my pleasure to introduce Professor David Deutsch who's going to speak to us today.
Many of you will know David's name. It's not controversial to say he's one of the key individuals
who really kicked off the field that has become the field of quantum information and quantum information technology,
which, as you know, Oxford now has a very large and leading role in the UK and in the world.
David's really first paper that people recognise was the one that went on to be described as the Deutsch algorithm,
and that was in 1985. He's continued to contribute to the field over the intervening years in particular.
Soon afterwards, in 1987, there was an important paper on quantum networks.
When I became interested in this field myself, in the 90s, David's work was one of the key inspirations for me.
David is based in Headington and is a perpetual visiting professor here in physics.
In 2008, he became a fellow of the Royal Society, recognised for his work in quantum information.
In 2013, David is now the lead researcher in a project on constructor theory,
which I guess we're about to hear some more about with Chiara.
That's Chiara, so Chiara's the postdoc, and if you want to know more, you can either talk to me or David or Chiara.
So, David, without further... Thank you.
Right. Well, the ideas I'm going to talk about today are part of constructor theory,
which I've been working on with Chiara Mileto,
and among other things, constructor theory uses a new mode of explanation of physical reality,
through which we hope to eliminate a lot more than just probability,
a whole load of explanatory dead weight and blind alleys from physics and from other sciences and even beyond science.
What's a mode of explanation?
Well, at present, the prevailing mode, which is mistakenly thought to be the most fundamental, is like this.
There are particles and there are fields in spacetime, and they obey laws of motion and initial conditions,
and once you know those, you can predict the whole of the rest of their behaviour.
But there are other modes of explanation in common use, in physics and elsewhere, and here are some of them.
No? Yes. Good.
Just for illustration, I'm mainly going to talk about the probabilistic mode that I've put in red there,
but as I said, constructor theory seeks to unify and to underlie all those modes and others
as a single mode of explanation, the constructor-theoretic mode,
which is explanation via the dichotomy between transformations that it's possible to bring about accurately
and ones that it's not possible to bring about accurately, and those are possible and impossible tasks.
Now, the basic principle of constructor theory is that just that all the laws of physics are expressible in that way.
Since every task is supposed to be either definitively possible or impossible to perform,
there can't be such a thing as a task being possible with a given probability, not 0 or 1,
and at the fundamental level such things might arise as approximations, but that's not a description of reality,
and therefore there's no place for probability, according to constructor theory,
there's no place for probability or random processes at the foundations of physics.
However, I'm not going to make the case against probability from constructor theory today.
It's more like the other way around. This is a motivation for constructor theory,
because independently of constructor theory, the world just is not probabilistic.
It's an illusion, and the probabilistic mode of explanation has about the same status as the flat earth theory.
Namely, you might find it useful to use the predictions of the flat earth theory when you're planning out your garden,
but when you're thinking about what the world is like, and even more when you're thinking about what the laws of nature are,
it would be hopeless the theory of the flat earth would just be an impediment to understanding what's out there.
So, same is true of probability, and so let me explain.
Well, probability and the associated concepts such as a stochastic process that produces a random sequence of outcomes,
these weren't originally invented for any purpose in physics or any fundamental purpose.
The basic mathematical theory of probability, namely certain numbers attached to certain elements of a set
which then obey axioms like they all add up to one and stuff,
was invented in the 16th century by people who only wanted to win at games of chance.
This is one of them, Cardano, and that started off game theory,
and game theorists use the idea of a random sequence as a mathematical model of physical acts
that are integral to games of chance such as shuffling a pack of cards or throwing dice,
and probability, via the central concept of equally likely,
was a mathematical model for the intuitive idea of fair shuffling of cards,
a fair dice and a fair throw of the dice.
So, when I say that, which is also essential to games of chance,
when I say that this was merely a mathematical model, perhaps I should say more clearly what I mean,
I mean that they didn't assume that anything in the physical world,
shuffling cards and so on, was literally what we would today call a stochastic process,
that it literally had properties such as probability.
That's because, first of all, the properties that they actually needed from shuffling cards
were far weaker than full literal randomness.
They were basically that the outcomes of shuffling should be fair and equitable among the players
and unpredictable to them, so that meant that the outcomes were not related in any simple way
to any sequence or algorithm that they could call to mind or execute during playing the game.
So, most pseudo random sequences would have done just as well if they'd had computers then.
And secondly, more importantly, I don't think it would even have occurred to them to connect their probability in their model
with some real physical quantity because classical physics, which was in its infancy at the time of Cardano,
but a bit later when it was perfected, was deterministic and therefore inconsistent
with stochastic processes happening in nature.
So, why did those game theorists blithly use mathematical axioms, probability theory
that were both far too strong and already ruled out by what was known about fundamental physics?
Well, simple. By being too strong, that's harmless, that meant that they certainly satisfied the conditions
that were necessary for analysing games.
And the real properties that they really wanted, such as fairness and so on, couldn't be expressed precisely mathematically.
By the way, they can be expressed precisely in constructive theory, but that's another story.
Consecwntly, it didn't matter also that their model relied on impossible physics
because the conclusions about game playing, if there had been that impossible physics,
led to the same conclusions about strategy and tactics as for the real physics.
So, it didn't matter for them.
There's another reason why they might have been blas√© about the connection between their theory and physics.
Certainly, this was a move made later in game theory and in probability theory up to the present day.
They might have regarded the probabilities not as attributes of the dice and the cards or the dealer,
but as attributes of the player receiving the cards.
That's the subjective interpretation of probability, and I'll come back to it in a moment,
but note already that whatever role the subjective state of mind of the player may play in this story,
fairness must depend on physical properties of the cards and dice and how they're treated during the game,
and, therefore, game theory must, in principle, be rooted in some sort of model of those objects and processes.
Anyway, from that theory, game theorists derived maxims for playing games of chance, such as in poker,
never draw to an inside straight, and the important thing is that this was a narrow,
parochial application of a mathematical trick, the theory of probability.
Indeed, the reason that game theory was possible at all was that its conclusions don't depend on the detailed physics
of any game playing processes. It didn't matter what the orbits of the planets are,
it didn't matter what matter is made of, or let alone anything deeper about the laws of physics.
The theory was directed specifically at modelling a particular human social behaviour,
which even other animals don't do, let alone the physical world at large.
So, it should be very surprising that this little mathematical trick, the theory of probability,
has found further applications in a very diverse fields in science and beyond,
and even that it seems to be fundamental to some of those fields.
So, here are some of them, roughly in the historical order of their invention.
So, we'd be surprised if, say, the seven of diamonds appeared in a law of physics.
But probability, which has the same provenance as the seven of diamonds,
seems to be central to physics in particular, and it seems to work.
So, why would anyone want to expunge it from physics and from every other fundamental theory?
Well, of course, we don't need a reason. Physics likes to do without things.
As we discover more and more about the world, we sometimes find out that things that we thought existed don't exist,
such as celestial spheres and the force of gravity and trajectories of particles and so on.
They were all thought to exist, and then discovering that they don't exist was a great advance in understanding.
But before we can decide that something doesn't exist, we must explain what the world, without that thing, is like.
And also, preferably, why thinking in terms of that thing seemed to work,
and why it was a useful fiction, and why it may still be a useful fiction.
So, compare these two statements.
The first statement specifies a factual, observable property of the world.
It specifies what will happen when poker is played and specified hands arise.
But the second one is not about physical facts.
It is consistent with any sequence of cards arising in a poker game.
In fact, it's consistent with any physical events, such as someone repeatedly drawing to an inside straight and repeatedly winning.
Not inconsistent.
Yes, it's true that it was risky. The second statement is true, all right,
but it doesn't apparently refer to any physical events that actually happened in that case.
One sometimes hears a sort of desperate denial of this along the following lines.
The statement, it was too risky, is about the physical world.
It refers to all the other players who drew to an inside straight and lost,
and it refers to the fact that there are more of them. They outnumber the winners.
Well, first of all, it doesn't.
There have only been finitely many poker games in the world.
Ignore for the moment the fact that there are parallel universes,
and in those particular player, both wins and loses in different universes,
Quantum Theory does in fact solve some of these problems,
but not by counting the number of players.
So, for the moment, in a given universe or in classical physics,
the proportion of players who drew to an inside straight and lost doesn't exactly equal the probability of losing,
just as repeated tosses of a fair coin, however fair it is,
don't result in equal numbers of heads and tails in general.
And anyway, since when do gamblers care about whether other gamblers lose or win?
If probability refers to other players losing money,
it doesn't refer to any physical fact about the game as it actually was in the case that I cited.
And the same holds for all probability statements.
So, that's meant to be, in the second column, that's meant to be all probability statements.
That's the dot, dot, dot.
The blatant fact that is generally overlooked is that no statement from the second column
can ever imply any statement from the first column.
In other words, assertions about probabilities do not refer to the physical world.
They don't assert anything about the physical world.
Frequencies, like the fraction of winners over losers historically,
are things that happen in real life and therefore they don't in general equal probabilities.
And it's no good saying that they equal them approximately
because they don't, they only probably equal them approximately.
And that's a statement from the second column.
Similarly, you can't say that probability statements are about what will happen in the long run.
No, they aren't.
All you can deduce from them are statements about what will probably happen in the long run.
But frequencies in an infinite sequence of measurements, of experiments,
do equal the probabilities exactly, and this inspires yet another desperate denial
to the effect that the finite sequences are approximations to infinite ones.
But nothing about a finite subsequence of an infinite sequence
can possibly follow from a statement about relative frequencies in the infinite sequence
unless the subsequence is a typical one,
and a statement that is a typical one belongs firmly in the second column.
Or you may take that subjective route that I mentioned.
You could imagine that probability statements are not assertions about the world at all.
They are assertions about our minds, probability being a measure of ignorance,
of degree of rational belief.
Those are two different subjective theories called credence.
That's no good for present purposes because then you still need something
to connect statements about our minds to statements about physical reality.
So to this end, the philosopher David Lewis proposed his principle principle.
That's a principle principle, which just asserts as an axiom
that rational agents have the same credences as the physical probabilities.
But note that that gives no explanation about why those physical,
purportedly physical numbers should inform decisions in that way.
You may as well propose an axiom that rational people avoid black cats or ladders.
That's not physics.
So the upshot is that Cardano and their game theorists never did succeed after all
in their purpose of finding ways of winning at games of chance,
or of minimizing their losses.
They only found ways of probably winning.
And later, people added these purported philosophical principles
that say a rational person would do this or that,
but actually a rational gambler knows that having probably won,
no matter how often one does it, won't pay the rent.
Physically, physically, it is most unlike actual winning.
So in case there still appear to be any clothes on this emperor,
because of the massive cultural interpretation that's been loaded onto it,
just replace all the probabilistic terms in the second column by magical terms.
Why? What physical reason is there to allow statements in the second column
to inform decisions by fiat and not statements in the third,
which we could equally well connect to reality by some fiat?
You have a third there?
Is there a third?
There, that's the magical column. Thanks, thanks a lot.
So what I have called these desperate denials of the firewall
between the first column and the other two are more commonly known
as interpretations of the probability calculus,
subjective interpretation, frequency interpretation, ensemble interpretations,
and all in many variants,
regarded as attempts to connect the second column with the first,
they are all, like the examples I've given, either circular or meaningless,
or conflict with the probability calculus, or just don't do what they say they do.
This discussion that I've just given about the defects in theory of probability
draws on the work of David Papineau, who's a philosopher of probability.
He has called this situation at the heart of his field a scandal.
Probability concepts and language and the whole theory simply form a closed system
of statements and ideas that just refer to each other
and can never yield a statement about the physical world.
Now, if we go back to the applications of probability,
we can see that there is a common feature running through all of them.
They are all in a certain sense and slightly different senses.
They're all normative.
That is to say, they are about, at root,
how one should act if one were to believe that certain potential events have particular probabilities.
Now, how one should act is rather a strange thing for a scientific theory to talk about.
You know, you can't get an ought from an is.
In fact, the firewall between factual statements and moral statements
exists for a very similar reason to the one separating factual statements from probability ones.
So, another way of papering over the divide or ignoring it is by a different kind of fiat.
One simply puts a normative statement about probabilities into the physical theory.
That's called a stochastic physical theory.
There are many ways of expressing that fiat.
Here's one.
So, it contains a lot of hidden stuff.
It pretends only to contain that first row.
But, actually, there are some purportedly factual statements
about numbers that purportedly have something to do with physics.
And then there has to be a principle to give them some normative psychological meaning, the second row.
And then the third row, actually, you need another axiom from decision theory
to say how one should actually behave rather than how one should think.
And this is weird.
But, luckily, it so happens that the only stochastic theory
that has ever been proposed in the history of science
as a fundamental description of the world is quantum theory
in its mid-20th century state vector collapse form.
And in that form, its probabilistic part is called the Born Rule,
which says that if and only if an observable is measured,
then the probabilities of the various outcomes are the moduli squared of those coefficients.
And the bottom line there.
By the way, does anyone here actually believe that the state vector collapse occurs in physical reality?
Show of hands?
Okay, no, good.
Oh, so one person.
So you have my sympathy, and I hope you'll see in what follows that help is at hand.
Closer than you think.
Because, well, help is at hand because ordinary unitary non-collapse quantum theory
provides in large part the way out of that whole probability scandal.
It's called the decision theoretic approach, sometimes called the decision theoretic approach
to the Born Rule or to probability, but those are both misnomers
because neither the Born Rule nor probability nor collapse, of course,
ever appear in the decision theoretic argument.
In its simplest form, it goes like this.
We imagine an array of gaming machines,
one-armed bandits in a casino, so there's a whole array of them,
and you play by inserting one casino token,
and then when you pull the handle, the machine prepares a quantum system inside itself in a state psi,
and then measures an observable x of that system.
It then displays the result, as shown in red on the middle panel,
and that will therefore always be an eigenvalue of x.
And then the machine delivers a payoff of that number of casino tokens,
which need not be a whole number.
We're allowing fractional tokens if x has non-integer eigenvalues.
Different machines in this casino are identical except for the state psi,
which is constant for each machine but different for different machines.
But the psi is not a secret.
It's written on the front of the machine, just like this,
conveniently expressed as a superposition of eigenstates of the observable x.
Now, four machines whose psi is a single eigenvalue of x,
with eigenvalue little x, then playing on that machine is not a game of chance.
It's just a matter of putting in one token and receiving back little x tokens.
Let's call that a classical machine,
because it could be implemented without quantum systems, without quantum technology.
Other things being equal, a rational player would be willing to play on any classical machine
that has little x greater than one and unwilling to play when it has little x less than one.
What about cases when psi is not an eigenstate of x?
What then is the dividing line between being worth playing and not being worth playing?
Well, let's call the dividing line for a machine operating with state psi.
Let's call that dividing line v of psi,
and this is perhaps a little bit of a wordy definition,
but it's just the maximum amount of money that the player would be willing to pay
for the privilege of playing that machine.
For a classical machine, psi is an eigenstate of x and v of psi is just the eigenvalue.
Well, some other facts about v of psi in more general psi are obvious too.
We don't need probability or the Born Rule or anything.
For example, if the state is a superposition of eigenstates of x,
all of whose eigenvalues are greater than one,
then elementary rationality says that it's worth playing
because whatever the outcome, the machine will give you more than the one token that you put in.
Similarly, for a superposition with all eigenvalues less than one, it's not worth playing.
That is, it's not worth playing in the token-winning sense of the game theorists.
You might well play for fun, but then you're being paid partly in fun,
so let's ignore that complication.
But now, what if the hard case, what if psi is a superposition of two eigenstates,
let's say one with eigenvalue below one and one with eigenvalue above one?
Well, in collapse quantum theory with the Born Rule and everything,
that tells us that for general states psi,
the probabilities of the respective outcomes are those coefficient squared,
and then the principle principle tells us to adjust our credences to match those numbers,
though it doesn't say why,
and the probabilistic game theory tells us that a rational player should value playing such a machine
the same as if it were guaranteed to produce that expectation value.
Again, it doesn't say why.
In general, axioms of stochastic theories are not explanatory,
which alone should disqualify them from being part of any scientific theory in fundamental science,
but I digress.
Now, what about without collapse?
Let's take a simple case of an equal amplitude superposition
with eigenvalues x1 and x2,
each with amplitude one over root two,
so we're aiming to prove, without collapse or anything,
that v of that equal amplitude state will be the average of x1 and x2.
Here's what we want to prove.
Here's the proof, a quick and dirty version of the proof.
The devil is in the detail, which I'm not going to talk about.
The details are extensive, but they lead to the same conclusion.
First of all, we need to note two implications of elementary rationality,
not probabilistic in any way,
namely equations one and two.
I'm stating them for a slightly more general case,
but it doesn't matter that theta is going to be pi by four in the end.
Equation two says that v of psi does not depend on the objective exchange value of casino tokens,
so long as they're additive.
If the casino suddenly declares that it's going to redeem tokens in pounds instead of dollars,
the relative order in which a rational player values playing on different machines will not change.
That's one implication of rationality,
and the other is expressed in equation one.
If two machines use superpositions where each eigenvalue in the expansion of one of them
differs by constant k from that in the other,
then the rational player's valuation of the two machines also differs by that same constant k.
That's just because.
In that case, all the first machine does physically is the same as what the second one does,
plus additionally paying out k tokens.
We make those substitutions there on the slide,
and it follows with a bit of algebra that for the equal amplitude state,
v of psi is indeed the expectation value of x in that state,
and we can prove the same for a general state,
and that's QED.
But look what we've done here in a deeper sense.
We've proved the same conclusion about what rational players do
as we would have from classical collapse quantum theory,
but without assuming that collapse happens.
We've proved it without the Born Rule, without those axioms and all that stuff.
We've done this on the right-hand side,
whereas with collapse quantum theory,
we have to go through all that process on the left-hand side to get to the same conclusion.
On the right-hand side, the only assumptions are elementary rationality
and unitary quantum theory without collapse.
On the perspective of this talk,
that means that we have dispensed with probability,
both in nature, in the quantum world,
and in our minds if we have any credences about the quantum world,
because in nature, as described by quantum theory,
there are no stochastic processes,
and no credences in the sense of beliefs with numerical measures
that obey the probability calculus,
no credences affect the decisions of any rational person
making decisions about quantum systems.
But it's even better than that.
Those decisions now, on the right-hand side,
are not only derived, but unlike in the collapse case,
they are explained,
because we don't have to introduce all those unexplained postulates.
That also explains why collapse theory,
despite its false assumptions,
was and is successful in a particular domain of application.
Now, let's look again at that list of fundamental applications of probability.
We strike out quantum theory from the list,
and we can put a tentative mark there about credences too,
since we now know that they're not needed in this particular application.
We've also eliminated probability from the theory of games of chance
that use quantum processes,
where the chance element is generated by quantum indeterminacy.
So, what about games of chance in general?
Well, recall what I said at the beginning.
Probability is a very large sledgehammer
with which to crack the egg of modelling things like fair dice.
Now we see that the same job could be done by quantum theory,
as by the impossible physics that they assumed.
Since a pseudo-random sequence would have served the same purpose,
so would a quantum-generated sequence.
And for the same reason,
we can strike out the use of probabilities in general decision theory,
which is just game theory writ large,
and in actuarial science.
I just put that in because it was historically a very early application of probability.
It measures, it prepares the system in a stape sign,
and then it measures an observable X on that system.
The result is the pale.
Just as in game theory, the theory of evolution doesn't depend in any way
on true randomness in the mutations.
All that matters is that the theory can explain
how biological adaptations to an environment
can evolve if the mutations aren't systematic,
if they don't depend systematically on the environment.
Only the selection is supposed to be systematic,
that's the essence of the theory of evolution.
Probability, randomness, is just a feature of the model, convenience,
so we can strike that out too from the theory of evolution,
strike out the theory of evolution.
By the way, Kiara has just published a paper
about the constructor theory of biology,
which, among other things, describes evolution without evoking randomness in any way.
Of course, by all these strike outs,
I don't mean that the mathematical formalism of quantum theory isn't sometimes useful.
I'm saying that the quantities called probabilities in that formalism
do not refer to any stochastic random processes in nature,
nor to anything in rational minds, such as degrees of belief or credence,
nothing in physics or in minds thinking about physics.
In information theory, probability was again originally used as a model
for an even simpler thing, namely this could be any message of n bits,
or our communication system needs to be able to cope with any message of n bits
and we don't know which it's going to be.
This was translated in the model to all two to the n strings are equally likely.
This has caused all sorts of confusion, such as people saying that a state contains
maximum information when it has maximum entropy, which is nonsense and so on.
But that's another story. Anyway, we can strike it out.
I'll just say in passing that constructive theoretic information theory
is also something that Chiara and I have recently developed
and it does fulfil all the hopes that we have for constructive theory in general,
including not being subject to the kind of confusion about information that I just mentioned,
and it unifies classical and quantum information.
Our paper on that was published a while ago.
So what's left? Quantum statistical mechanics at least doesn't need probability
since it has entanglement and decoherence and therefore it can avail itself
of what I've just described about quantum theory, the decision theoretic approach.
I should say why the universe is in such a state as to make the laws of thermodynamics hold,
for example that it's uniform, that it's initially ordered and so on,
is a substantive question, but it's not a probabilistic question.
So we can strike that out too.
So I'll leave striking out classical statistical mechanics as an exercise for the audience.
Now, experimental error, that's an interesting case.
I think it was historically the earliest application of probability after games of chance
and it has some interesting misconceptions in it in addition to probability,
which one of them is as follows.
So it's connected with probabilities.
Error processes in experiments are traditionally categorised as random and systematic,
but both of those are misleading terms.
For simplicity, just imagine a measurement of a constant of nature such as the speed of light
and suppose that we are asked to give an estimate of the best error
attainable with a given instrument like Fiso's wheel.
Processes that were random with known probabilities
would not be sources of error at all in such a case
since they could be reduced without limit just by repeating the experiment.
So the important errors are the ones that affect experiments through processes
whose governing laws are unknown, that is the laws may be known,
but how they affect the experiment is unknown.
And those cause systematic errors, so ironically a systematic error is one that obeys known system.
So what does it mean to estimate an error caused by the unknown?
Traditional to go into probability and subjective probability, but as I've said that's all nonsense.
So what can it mean? Well suppose that a physical constant, let's say call it chi,
is to be measured and that the bound for a given technology is claimed to be epsilon.
So if little x is the average of measurements obtained with a particular instrument,
well the best result obtainable with a particular instrument,
then the first line x minus chi, modus of x minus chi is less than epsilon we're saying,
and for simplicity assume that the individual outcomes that we obtain when we do the experiment repeatedly
with different copies of the apparatus and so on are x1 and x2,
then we have both x1 and x2 both obey that inequality as well.
And then just from some algebra it follows that the average of x1 and x2 just as in the,
if they were random errors, has a smaller error than either x1 or x2 separately,
which is a contradiction because epsilon was supposed to be the best error obtainable with that apparatus.
So systematic experimental errors cannot be bounded by any known bound,
therefore among other things they can't be described by probabilities,
nor can our knowledge about them.
These unknown variables in science are counterintuitive and often misunderstood
because we have been accustomed by Bayesianism and other subjective philosophies
to replace real ignorance by fantasy probabilities.
But we've just seen that neither physical probabilities nor probability credences
can enter the analysis of errors in a fundamental way.
So what do we mean when we estimate the error in an experiment?
Well that turns out to be a big question with surprising answers that there's no time to go into here,
except I'll just briefly mention, I hope to complete a paper on that quite soon,
but I'll just briefly mention that what an error estimate really means is it's the error such that
if the experimental result turned out to differ from the true value by more than that
when the true value is later discovered by some other method,
then if it was more than that it would make the theory of the apparatus problematic.
So that's in short what an error means,
but for present purposes we can just strike out error analysis.
So Brownian motion, that doesn't actually purport to be a fundamental theory
since although it has a stochastic law of motion that's assumed to be an approximation
to a more microscopic cause such as impacts from molecules
that aren't explicitly treated by the theory of Brownian motion.
So that theory is also related to the theory of errors,
but in the approximation that the so-called random errors are swamping the systematic errors.
Although we've just seen that that's something that cannot be known in a particular case,
but if the unknown systematic errors are too large, then it's not Brownian motion.
So we can strike that out.
And with it we can also strike out the applications from high finance
that are directly analogous to the theory of Brownian motion.
Finally, since none of those other applications of probability
now involve stochastic processes, they do not require credences either.
It doesn't matter what scientists think,
what scientists believe about whether a theory is true or false,
so long as they execute rationality.
And so Bayesianism and the other subjective interpretations of probability
have no remaining scientific function,
and for these purposes they can be dropped too.
Now, I hope I have shown you that probability doesn't make sense
as a description or explanation of what really happens.
It can be a metaphor, it can be a technique for calculation
or an approximation in a certain sense,
but an approximation to make sense has to be an approximation to something.
So if probabilities are to inform decisions in some approximate way,
there has to be an explanation rooted in a description of an actual physical world
in which events and processes happen, not probably happen,
and not just via some ad hoc axioms.
So I hope I've also persuaded you that it's right and proper
to try to expunge every trace of probability and randomness
from the laws of physics and from our conception of the world
and from the methodology of science
so that we may fully restore realism as well as rationality.
It's a simplification, a unification,
and an elimination of nonsense, and it's true.
Now, I bet there are hard-headed instrumentalists in the audience
who might be thinking, OK, so this simplification is all very nice and elegant,
but since the principal uses of the mathematics of probability are largely unaffected,
what really is the benefit of eliminating it at the fundamental level, either?
Well, it's true, fundamental falsehoods don't always rear up and bite you.
You could believe in a flat earth, as many people did for a long time,
and that falsehood may never have affected your life or your thinking.
On the other hand, it might have destroyed the entire human species
because belief in a flat earth theory as a description of reality
is incompatible with developing, say, technology to avert asteroid strikes.
Similarly, a belief in probability in quantum theory
may not prevent you from developing quantum computers, quantum algorithms in practice,
but because probability and the Born Rule entail fundamental misconceptions
about the physical world, they could very well prevent you from developing
the successes of quantum theory.
In particular, constructor theory is the framework in which I suspect
successes to quantum theory will be developed.
As I said, constructor theory is incompatible with physical probabilities.
So that is my case. Thank you.
Thank you very much, David.
Just before we take questions, let me mention that there's a website very easy to remember.
If you want to learn more about the theory on one word.org,
we will find many of the things that David alluded to, papers and so forth, easy to find.
Okay, I saw one hand go up, so please.
I'm sorry, but the slot machine, I'm afraid I'm so far walking out of here with no understanding
of how to modify the way of thinking about a single machine that ended up coming in.
What are the funny numbers in the square roots mean in front of the eigen states in the slot machine?
I would say, well, what those funny numbers mean is if I pull the handle lots and lots of times,
then the limit that I do it forever, one third of the time, I'm going to get this one result,
and the other two thirds of the time, I'm going to get this other result,
and say, well, which result will you get next?
I don't know. I didn't understand how to modify my way of thinking about that
based on the stepping clock.
Well, the idea is that I'm not sure which version of quantum theory you're thinking in terms of
possibly collapsed theory in which there is such a thing.
I don't know. I want to know how I should think about this.
The way you should think about it is not in terms of the outcomes.
The outcomes are an emergent property in some complicated way.
What those coefficients are, they're just descriptors of the state, the state that the thing is prepared in.
So, for example, if it was a spin system, it would be prepared by rotating it in some way
with magnetic fields and those coefficients would depend on the angle through which it's been rotated.
That's what it actually means.
What that causes in our subjective experience, do you need an interpretation of quantum theory for that?
Interestingly, all we need, and the right interpretation is the many universal interpretation,
but we don't really need to draw on that for these results.
We just have to say that the motion is unitary.
That's all. The state exists physically and its motion is unitary.
Other questions?
I might just find out more about what you think about.
Credences, so I can go along with the idea that it's very weird to have them in a fundamental micro-physical theory.
If in future we find some kind of theory which is a deterministic, we have to wrestle with what that means.
But there's a higher level concept.
They seem to be useful in some kind of region philosophy of science which doesn't.
It's not trying to talk about fundamental physics itself.
So I observe people would say, as a credence, maybe if they don't know exactly what the microstate of the real world is
and they have some idea that I think it's within this region,
but I've got no reason to differentiate between actual microstates.
Having no reason, so these are two slightly different ways of doing a subjective interpretation.
The first one, where you guessed what the probabilities are in some theory
and it's going to be an approximation to some microscopic state that you don't know,
that's a legitimate way of approximating something.
You have guessed what the probabilities are and then you can test, if theory says that there's something,
you can test whether that's so.
When you say you don't know what the microstates are, then you're trying to derive knowledge from ignorance
and that is simply logically not valid and it's also inconsistent with the probability calculus.
It's only by sort of hand waving it, ignoring that fact that one can apply Bayesian philosophy
in words to things like, we don't know what the state is.
If we don't know what something is, we don't know what it is.
The standard refutation of that kind of subjective interpretation is,
if there are three possibilities A, B and C, you don't know anything about which state it's in,
then you might say, bet on whether it's A or either B or C.
If you say that because you don't know which of those two possibilities is right,
that you must give them equal probabilities, then the same is true for the permuted versions
and that's inconsistent.
It's logically inconsistent, so it can't be used for that.
The situation is very analogous to the flat earth theory.
When you assume that your garden is a plain surface, you're not assuming the flat earth theory,
you're just assuming the mathematics of the flat earth theory applied to this process,
but there's no way in which you are assuming that the true thing is an approximation to the flat earth.
It isn't.
Let's see up here.
So, with your decision theoristic argument, which is used to bypass the principle,
and to derive the Born Rule, you invoke this function V, this evaluation function.
However, you also asserted that it had some properties,
that it was linear in the eigenvalues of the function U.
Yes.
So, have you also asserted that the rational entity would...
No.
We're assuming a rational entity who has linear evaluation.
So, this same issue arises in classical game theory.
As I said, a real game player is not the same as the idealized game player of game theory.
A real game player has all sorts of other motivations.
Otherwise, a real game player would never play a game of chance, such as Rhett,
where the expectation value of your winnings is negative.
We assume this idealized game player of classical game theory,
and we assume exactly the same player in the decision-theoretic approach to quantum theory.
So, it's not because we want to say why people play games.
It's just this one particular aspect of game playing that has probability.
It's that, it's analogous between the two phases.
So, you still made an assertion about how this player would behave, right?
So, Rhett, how is that any more elegant tool than the principle?
No. The assertion about how the player behaves is the same in both cases.
We're analyzing how a player who believes a particular thing will behave.
We're not saying that any real player actually behaves like that.
We'll probably take another question.
I'm saying that epistemological chance...
Well, yes, I am. I'm saying that neither of them exist,
but ontological chance can be a good approximation in some situations.
And epistemological chance really should be dispensed with all together.
Okay. So, if we focus on the epistemological chance, there could be two kinds.
One where we don't know enough about the system to be able to calculate the outcome.
Yes.
There's another kind in which, even if we did know everything,
we would not be able to compute the outcome because the problem was not computable.
Yes.
Does that not create an effective ontological chance?
Well, no. That's a particular situation, again,
in which the probability calculus may be a good approximation.
Rather like if we think about the distribution of the digits of pi,
then it's a meaningful approximation to say, well, they're going to be random.
They're not really random, but the mathematics of them has enough in common
with the mathematics of truly random sequences for it to be a good approximation
for some purposes, but obviously not for all purposes.
Yes. Unpredictability can be modelled by randomness sometimes.
Okay. I think we've probably got time for one more question,
and then I expect that it's happy to hang around for a few minutes afterwards.
So, those of you who didn't get to ask a question, then he's just come to the front.
What other answers do I see?
One here.
Yeah. I just wanted to come back to something you don't want to be asked about.
You seem to be wanting to expunge probability from any fundamental physics,
and I think we all know that probability is mathematically problematic,
but you seem to be wanting to do this by replacing it with the decision-theoretic governance
in which make with respect to some admittedly non-existent player
or some game that is probably also not being played.
Yes.
So, I'm really struggling to see how that is any more desirable
than any of the other approaches to the admittedly philosophically problematic probability.
Yes.
So, the main advantage is that it doesn't assert anything false about reality.
Well, it's asserting that reality has something to do with this thing that we know doesn't exist.
No, no. It's just saying that if...
So, it's taking an idealised game with idealised processes happening both for randomising and for the players.
Yes.
And it's analysing how those players would behave if they believed some rational...
if they conformed to some elements of rationality that have nothing to do with probability.
But that entire argument is counterfactual.
Yes, it's counterfactual, but then you need an argument to say that a real situation
resembles that idealised situation in particular ways.
So, if I go into a casino to play roulette,
obviously I'm not satisfying the axioms of game theory
because I'm playing a game which has an expectation value that's negative of winning.
On the other hand, if I play poker and I'm good at it
and then I wonder whether to draw or not or how much to bet,
then I can use game theory because and only because
I guess that my real situation resembles the idealised situation in the relevant ways.
But it's only as good as that. It's not a theory of reality.
So, it's based, as you said, on a guess on the correspondence
between what you might be doing in certain situations to something that is counterfactual.
Yes, exactly.
So, inside me right now, I'm assuming that there are carbon-14 atoms decaying.
There is no-ones doing that game of chance.
There's no sensible way in which you can prepare it to an idealised.
Making that game of chance, talking about the length of time it will take
and giving one of them to decay.
I think we'll just let David reply to that.
OK, well, you can. If you want to work out the half-life,
then you simply have to imagine the situation in which these game players
bet on how long it will take and what the rational thing for them to do is
and you'll find that the rational thing for them to do is to bet on an exponential decay.
And that's the problem.
So, lots more if we could thank you.

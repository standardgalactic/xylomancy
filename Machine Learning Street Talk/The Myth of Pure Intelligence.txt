I should look into your eyes.
A bit of a bit of a combo.
So, yeah, like that camera vaguely in my direction.
I don't think it matters.
OK, just don't look down.
OK, don't look down.
Because of the glasses and the light.
Oh, yeah, let me check that.
Tiny bit sort of, when I say source,
the only light source.
So I'm director of product for R&D Adverses.
And I'm also a student in PhD for cognitive computing
at the University of Quebec in Montreal.
Fantastic.
So what have we just been discussing?
God, we've been discussing a lot of things.
I think we discussed panpsychism most recently.
We discussed computationalism.
We discussed whether we should be focused on materiality
or whether it's about the processes themselves,
the hard problem, IIT, lots of things, lots of things.
Yeah, so why don't we start with the beginning?
So that's quite a good segue, actually.
So recently there was this letter of opposition
around integrated information theory.
What's your take on that?
I think the reaction was to be expected.
I think you can't ask people that aren't involved
in a process which is aimed at determining
whether they are right or wrong, just to accept
the outcomes of something in which they had no say.
So there's that.
It's like saying, well, you're going
to have to accept the outcome of elections, but you can't vote.
I think one of the issues is the people who created the study
determined a process that left too much room, too much wiggle
room for interpretation and critique,
which means there's a problem in the study design
to begin with.
I think when you start attacking through adversarial means
a field, it is highly likely the field is going to retaliate.
So this open letter, I think, was a retaliatory move.
I think it was a very strong response
to perhaps coverage that should have been more nuanced.
I definitely don't think IIT is the leading
consciousness theory.
I think it's an interesting one.
I think it has promise.
I think it's definitely formal.
There are many others.
I think this one was just one of the ones that
was selected in the Templeton study.
And so I think we need to be more careful about how we comport
ourselves towards other scientists.
When we call an entire field pseudoscience,
I don't think it helps.
I think we can critique the parts of the field
we think are egregious or problematic.
I think that's helpful because it builds towards something
rather than just destroying something for which a lot of people
have given their lives.
Not through death, but they've given a lot of their time.
They've studied.
They've understood what would constitute the right kind
of methodology, and they've applied it.
So I don't think we can just disqualify it
as something that has no value offhand.
Now, it's not to say that they necessarily all
had an offhand opinion, but the way it was presented
basically suggested because of tabloid style reporting,
we're leaning towards something dangerous.
And to do so, they used tabloid-styled methods.
That's my take.
Do you think there is any bright line between what
is legitimate science and what isn't?
I think there might be, but I don't think
it's where people think it is.
I think, for instance, some people called science something
that used tools.
And I think, to me, that's insufficient to qualify as science.
So it's not because you use a little pinny machine
to go like this on somebody's head
that phrenology is effectively a science.
That's not.
I think you need to measure invariance
and measure how those invariance allow you
some degree of predictability.
To me, that's the essence of science.
But I also think that some things
that people can discard as evidence
still can qualify as evidence.
To me, phenomenology can become a part of evidence.
To me, intersectional data, which means it's not necessarily
data that will easily reproduce, can qualify as data.
Under the caveats, that it is not something that would
reproduce, that it is something that is specific to a perspective
and therefore cannot qualify as a generalizer.
I think a lot of qualitative methods
have developed entire frameworks
to explain how to interpret what they measured as data.
And oftentimes, because people in the STEM fields
are not aware of how this interpretation is meant
to be taken, they just see something which
doesn't seem very systematic.
And they're like, well, that's not data.
I was like, well, OK, to your measurement, that's not data.
Because there is no way for what you're
using to interpret that in any sort of systematized way.
Sure.
But for someone who understands how to generate that data,
how to potentially return to the space which
has similar partitions, and how to turn this
into something which becomes a little more systematic,
like thematic analysis.
And let's say, interjudge agreement.
Then we get to something that resembles systematicity,
and therefore we can derive patterns from there.
So I think the line is not as bright as people
would like to think, but it does exist.
Yeah, I like your notion of systematicity.
Because I suppose you could just argue
on some kind of just basic utility.
You mentioned phrenology.
And there was this horrific gaydar paper a few years ago.
I don't know if you remember whether they ingested
a whole bunch of images from dating websites.
And they trained it to predict whether you were gay or not.
And it was from Stanford.
And the same Stanford researcher recently
published a paper about theory of mind and language models.
That's an interesting aside.
But anyway, that's something which is ethically ridiculous.
And there are all sorts of confounding factors and so on.
But they might argue, well, this is science
because it has predictive power.
Well, what part of it was science?
What model did it allow us?
What understanding did it allow us?
Did the AI have predictive power, or did we?
And under what circumstances?
Did it predict very accurately whether someone was gay or not?
I don't know.
But you could tell me whether, yes, it did or not.
But what does that tell us about the world?
Does it tell us that there's a context under which
certain sets of features are common for a given population?
Possibly, but I still don't know what those features are.
It didn't really help me.
And then the idea that we can turn certain kinds of studies
in practice, I think, is not beyond the scope of science.
That's not really it.
But I think it requires the scientists
to take a stance before the scientific experiment
to say, these are the outcomes I believe
should be put out into the world.
And therefore, this is what I choose
to reify with my science.
And also, the kinds of methods I'm
going to use are themselves anchored
in some kind of contextuality, right?
The idea of breaking apart the world
into certain kinds of categories which are laden.
And I'm going to measure those categories
with certain kinds of things, like IQ tests.
It's quite controversial, right?
It's like, do they measure intelligence?
Or do they measure a certain kind of aptitude given
a certain kind of context?
Like, maybe?
And also, what do we mean when we say intelligence?
Like, are we saying some people, therefore, are not?
And therefore, X, Y, Z?
So I think that doesn't make it phrenology, though.
That still could become science.
And in the case of the AI model, if it had accuracy given
that, it probably did.
But I don't think that tells me anything
about a causal factor that I could then derive
and build upon, right?
Like, it doesn't really help.
Yeah, no, it's interesting.
I mean, the whole endeavor of science
is something that fascinates me.
And once you get past the question, of course, of, you know,
well, what makes good science?
Chomsky, by the way, says that language models
are not a theory of language.
Because they basically, you know, they can recognize anything.
You can just put noise into a language model
and it will recognize it.
So it doesn't really say anything about the why
and the what would happen if not question.
It just, you know, it just models absolutely everything.
So science needs to kind of create clear dividing lines
between what things are and what things are not
and why they happen.
Yeah.
So I think, I just, I think we have to be careful
when we say what things are and what things are not.
I think it's always relative to something else.
Like, I think it's true that you can put language
in a language model and it will do something,
but will it do what you wanted it to do?
I'm not sure.
So in the case of Chomsky, I don't think he's wrong.
I just think that in general, what we mean by language,
we don't just mean a way to transmit semantics.
I think we mean a way to transmit semantics
which are anchored into something
for which we expect a certain kind of outcome
to also follow.
Because if both you and I mean blue butterfly,
but we don't mean the same like referent,
then it is useless.
That's not really language.
We're not really like converging over anything.
So I think that's the thing.
Like the language models don't necessarily converge
with us where we want them to converge.
So in that sense, they're disconnected,
but they do to me and you carry some semantics.
And that's how we can tell that like,
they're not doing what we thought they were gonna do there.
They can get disaligned with us.
So yeah, so again, things that they,
what things are and what things are not
is always relative to some kind of perspective
that frames it and constrains it for it to be at all.
Yeah, I mean, because obviously we can get
to the question of whether you think language
is a system of thought or a system of communication.
But I kind of agree with what you're saying
about language models, which is that essentially
they are spitting out words, which means something to us.
And we can discuss where that intrinsic meaning comes from,
whether it's pragmatics or whether it's just
some kind of colloquial usage of the word or whatever.
I mean, what's your take on the first question,
which is, do you think language evolved
as a communication or thinking mechanism?
I think we're baking in the definition
when we ask for either like, is language something
that is simply a reference between some kind of cluster
and some kind of real thing, is that language?
Or does it have to be something which is externalized
as a symbol, in which case,
then it's purely communication, right?
So whether, I think we have to be more clear
about what we mean, because I do think
that you don't need to externalize what you understood
in order to cluster it.
It's better, it allows you to confirm and act on it.
And especially if there are more agents around you,
I think it makes like thinking much more efficient.
So I think if we went all the way down,
I think we'd find that maybe the egg or chicken problem,
like we could solve it sort of.
I think at the layers at which we would recognize something
that resembles thought or resembles language,
they're no longer separate.
But if we cast language as a system of reference
and reference, then I don't think you need communication
for that, you just need one entity to recognize something
and be able to keep that as something
it can refer back to later.
So it it towards itself,
but that also could be qualified as communication,
communication over time with yourself, right?
You cash something out,
which you will pull back again afterwards.
So whether it's spatial or temporal or spatiotemporal,
I think is the real question we're asking here.
Okay, okay, interesting.
But I think that you can embed thinking inside language.
I mean, for example, like if you're writing software,
there are design patterns
and there are patterns for doing lots of different things
and we give them names
and then the developers recognize those patterns
in the software and they say, oh, I understand now.
And loads and loads of these thinking primitives,
I think are kind of culturally embedded in our language
and they're like memes and they get passed through generation.
And we just point to those things
and then we link them to those kind of cognitive programs
that we have.
The fidelity of that mapping, of course,
whether it's mediated or whether it's a simulacrum,
we can discuss that.
Okay, I don't disagree.
Yeah, I think you're right.
I think you tapped into this idea of whether this is thought
or does it require a name?
Would the pattern exist without the name
and would it be referable without the name?
Like, could we still point to it?
And then like, if we think of sign language,
like what qualifies as a symbol in sign language?
Is it a reliable action that follows something else?
Is it this intermediate?
And in this case, like what qualifies as an intermediate
or intermediary when you're pointing at something?
So is it the fact that I look at you, you look at me,
I know you're waiting for something,
so I'm gonna point to the thing
and you're gonna make that connection as well.
Is that language?
I would argue, yeah, it is language.
And that's why I think animals, to some extent,
a lot of them have language
if they're capable of transmitting information
through themselves as intermediaries.
And again, do you do this with yourself
without having to use what we would qualify as words?
I think words are just reliable intermediate clusters
and whatever those are.
Yeah, no, I completely agree.
So yeah, first of all, I think when we say language,
people assume that we're talking about words.
And as you rightly said,
just imagine a hypothetical situation,
you were born unable to make any sounds,
you didn't have a mouth
and you could only write with a pen.
You would still be every bit as capable as any other human
or perhaps you could only use sign language.
And then you could say, well,
does sign language have the representation
or fidelity or cardinality of language?
Yes, it does, absolutely it does.
So language isn't so much about the cardinality of,
I mean, language is just basically a set of strings,
by the way, so how much can you represent?
But anyway, I think it's about
kind of social complexification in our brains.
And even the ability to write
is quite an interesting example of that.
So this is a proto-ability that we've learned
and you mentioned animals
and animals don't have the ability to,
they don't have the flexibility that we have.
So they don't have the ability to map symbols
to a variety of different phenomena.
They do map some symbols to phenomena,
but that seems to be much more hardwired.
Whereas with humans, it's very, very flexible.
And it's a real mystery.
And unfortunately, we don't have the phylogenetic record,
the intermediate record of how and why
that capability developed, but it did develop in humans.
So I can't speak as to why it develops
in humans and not in animals.
I think what we can talk about is
what qualifies something as a language
if it's not the words, the sounds, the writing,
or the signs.
I think we're talking about a process
which is it's degree of rigidity on a certain scale.
So when you and I use words,
there's a very high likelihood that we refer to
some similar abstractness that can be contained, right?
Under some kind of mark of blanket.
Whereas the porosity of me kind of pointing over there,
that could mean a lot of things.
Like I haven't really reduced entropy.
Like given a context,
there's a possibility that we would reduce it, right?
And that's precisely the sociality you're talking about.
We're talking about these scripts
that basically constrain, constrain, constrain, constrain.
And then within those constraints,
there are a lot of different kinds of signals
that have come to take on a reliable meaning.
And we're so expressive
because we have so many of these symbols
and we can compose them.
So there's different scales, right?
So if I say the word trout,
I'm not sure you know what I mean here.
Like why trout?
What are you trying to say?
But if I add more words,
then suddenly I've also added more context to trout.
So I've contained, constrained,
and now suddenly scales of meaning arise.
We have more priors, more priors,
and now trout takes on a whole other meaning.
But if I kept pointing,
I don't think there'd be lots of information
that's contained here
until maybe I do the thing for you.
And now you'll know maybe that if I point over there,
that's what I mean.
Yeah, yeah, fascinating.
You mentioned intelligence before, by the way.
So what's your take on intelligence?
To me, intelligence doesn't start
as fundamentally human, right?
So we start for that.
Like to me, intelligence is a type of process.
It's the capacity to derive as many paths
between two points as possible.
Because it entails that you can compute those paths, right?
So your capacity to compute them is one thing.
And the precision with which you'll be able
to reach the point, right?
You could think you're computing the paths,
but you could be wrong.
You could actually get to a way different point.
So I don't think that would qualify as intelligence.
Intelligence is both your capacity
to reach accurately the point,
but also to reach it through a variety of means
that you can entertain at a given time.
So on that, and we'll talk about goals in a minute,
but let's say a goal is an end state.
So there's a state action space
and we're traversing through the space.
And then we look at the cone pattern of traversal
and the shape of that cone
tells us a lot about intelligence.
So the cone might incorporate future affordances
which are not yet available to us.
They might include, maybe there's some kind of a cost function
because some affordances are harder to take than others,
both because of physical difficulty
and intellectual difficulty.
So you're saying there's some kind of utility function
that we can sum over all of those trajectories
to tell us how intelligent we are.
Yeah, I think so.
I think it's not necessarily also your capacity
to enact them, right?
I think, because if it was your capacity to enact them,
anybody disempowered would not be very intelligent
and that's not true.
That's not true.
They could compute it and be like,
well, I mean, I can see that if I were in that position
I would be able to or I can see that these,
this is a path that exists,
but there is something in the way.
Like I can't go there.
That path exists nonetheless.
Like I know I can go outside right now
but all the doors are closed.
So I know I could try to find a way to open the door
but eventually the cost for me to get outside
would be so high I would effectively dissolve.
So there's no point.
So I might as well just not do it.
The likelihood of me actually taking that path
is really low because I'm disempowered.
I'm not less intelligent.
I found a way.
I found a highly improbable way.
I just don't think I should do it.
So I think intelligence generally correlates
with your capacity to reach those goals
and therefore accrue more resources
because you'll be able to find ways to get towards places
which are self-evidencing.
And that also has a self-reinforcing cycle
where the more likely you are to accrue to resources,
the more likely you are to find more paths
that lead you to more goals, et cetera.
And then maybe if you can stop thinking
about the lower level paths you have to take
then you could compute larger scales
and you can become more effective on a, again,
grander scale so you can start influencing more people.
You can start thinking in terms of longer time depth.
Whereas if you're constantly trying to survive,
like everything's constantly hitting you from everywhere
then you have to compute all the paths
to avoid all those things.
So again, privilege plays into the degree
to which your intellect can expand, right?
Or at least expand in terms of empowerment.
But that's not to say you're not intelligence.
It just means that your intelligence
gets carried and echoed in the world.
And then there's all the social stuff, right?
Because you and I are constrained by socialness
or by some degree of social scripts, we can compute more.
We don't have to wonder how we should use that table.
We don't have to wonder how I should use this mic.
I know.
So the more we have these,
do you want me to stop or?
Carry on.
So the more we have these social signals
that allow us to compute more,
the more we can think on larger scales
or deeper time depth.
But sometimes I also think that some degree of constraints
also sometimes preclude you from thinking
that a potential path is probable.
Yeah, yeah, I mean, there's a few things there.
I mean, first of all, I quite like this idea
that the length of the prediction horizon
is a good indication of intelligence.
But the only way that we can increase the prediction horizon
is through creating very low resolution abstractions
to represent our sense world.
So that has a kind of brittleness associated with it.
You said something interesting before,
which is that you could think of it
almost as overcoming adversity or overcoming cost,
which like so some trajectories are going against the grain,
but some choose to pursue those trajectories
and therefore you could think of them as more intelligent.
Yeah, so it's quite interesting.
I mean, I also like to zoom out
and think of the actual goals themselves.
Now, first of all, I think that goals are a system property.
I don't think individual agents have goals.
So I'm not sure it makes sense
for an agent to pursue a goal,
but let's just as a best faith argument,
take a goal as a future state,
which an agent is aware of.
I would also factor in the utility of those goals
and the selection of those goals as being parts of intelligence.
Yeah, so again, I mean, you said something interesting.
It's a property of the system.
And I think if we take what I said at faith,
intelligence is a property of the system,
not just the individual, right?
Because again, you get carried
given that the system props you somehow
and gives you capacities to think farther.
So given that you and I are in the same system
and have the same relationship to the system,
we could quantify or intellect,
given how far mine would go relative to yours.
But that's only given
that we have the same relationship to the system,
which is again, highly unlikely.
You would have to basically be a twin
with practically in similar life
to have the same relationship to the system.
And then again, I mean, this idea of goal with utility,
it's always relative to a scale, right?
So given this scale, there's this gate,
which is highly likely,
and this gate, which is slightly likely.
And so I'm probably gonna go from here to here
and how I get from here to here is the real question
and the degree to which I minimize free energy
by going from point to point and improve my model
also by going from point to point,
which is gonna allow me to reach the next goal
without too much energy.
So I don't think that someone taking a path
against the grain is necessarily,
means that they're more intelligent.
I think that if they can take an unlikely path,
which also allows them to reach a goal
that others would have wanted to reach and maybe couldn't,
or would do so with having to spend more energy
at a given scale, means they're probably more intelligent.
But it doesn't mean that if you follow the path,
you're not more intelligent.
Maybe this is the smoothest ride.
Maybe there's no point in doing something different.
Maybe to you, the goal is that simple.
So you took the smoothest ride, that's very smart.
You just also were able to compute
many other paths to go there, you know?
Yeah, but I think a goal in,
because we were talking earlier about complex categories,
like a chair, it's very, very difficult to define a chair,
and using the affordance is the way that most ontologists use,
like it's a thing you sit on or whatever.
And a goal, we're not actually talking about a state,
because if we represent the world in this systematic way,
where we discretize everything,
then if you think about it, there would be
an infinitude of goals.
So we come up with an abstract notion of a goal,
and now we've just got this big goal thing
that we want to get to.
And still, there's an infinite number of trajectories
that would reach an estate which satisfied that goal.
So it just, it feels, I don't know,
it feels kind of vague to me.
To say goal?
Yeah, well, I guess like,
because we were talking earlier about science
and the benefit of science,
and I think like, you know,
abduction is a key part of science,
and abduction is about finding a reasonable explanation.
So, you know, selecting out of the infinite set
of hypotheses, a reasonable set and mapping to them.
And it's a similar thing with a goal, right?
So what you're actually doing is you're,
it's very creative.
You're basically creating something from nothing.
And you're saying, these three things
seem very valuable to me.
Now we know that we're talking about a multi-agent system.
And the intelligence, and what you said really resonated
with me, the intelligence arises at the system level.
So the individual agent, I think the goals are not arbitrary,
but not a million miles from arbitrary.
But when you multiply that with many, many agents
and a large collective intelligence,
and you kind of average over the top,
then you get these emergent goals
and you get intelligent behavior.
But it doesn't make sense to me to think of an individual agent
as having any meaningful form of planning and reasoning.
Well, I guess we're putting a lot in meaningful here,
but as a thought experiment, I like to think of
someone really, really, really smart
who would compute like 3,000 years into the future.
And you would think, like, well, that person would try,
you know, solve world hunger, and they would like,
oh, I don't know, they wouldn't try to take over the government.
And, you know, they would find the path
that makes sure that somehow they reach immortality
and their children, something like that.
And I'm like, hmm, I don't think so.
I think someone really, really, really intelligent
would probably be able to compute all the paths
and find that they're already on the free energy minimization
path and they would just, you know, live a very normal life.
Like they would live the life they seem to have been set on
and they would continue in that
because they're part of a manifold
and they're already part of a pool of negentropy
and they're already like minimizing somehow.
As Friston says, he argues there's something fundamental
about, you know, if we talk about utility,
for him, utility is existence.
And there is an argument that some things are so fundamental,
so platonic that they go without saying.
So maybe it's existence.
I'm not so sure about that.
I still think that inside Friston's framework,
there are still many potential ways to act and live.
And the ex-risk people, they say,
well, obviously these agents are going to seek
to maximize power.
And well, why is it obviously?
I'm not entirely sure.
The agents as in the AIs?
Yeah, the AIs.
First of all, it's interesting that we're saying plural, right?
Like in general, the...
Well, for them actually, it's the one, right?
They're still at the AIs stage of there being one.
Yeah, right?
I know, it's so typical.
That's only because they haven't got to the multi-scale yet.
No, I know, but I think it speaks to something.
It speaks to like this notion of individuality
and disconnection from the others.
Whereas, no, intelligence is a function of your group,
not just of yourself.
The genius myth is a myth.
They all collaborate and prosociality
leads to something better.
So I do think that should AIs take over?
It won't be one.
It'll be several.
It'll be a group, like coalition or something.
I'm interested in exploring various limiting factors
or scaling laws for intelligence.
And one of the big ones is knowing.
And in a certain environment,
there are only a finite number of things to know.
And what you know might be related to the agents
that you cohabit your environment with.
But you did say earlier when we were having a discussion
that you thought in principle
that you could scale intelligence.
And I actually like to rephrase what you just said.
So you said when you zoom out
and you're looking not at individual trajectory volumes,
but you're looking at kind of like the adaptability
of the entire super organism.
Because that jives with the common definition
of intelligence that is about adaptation efficiency.
And I really like that.
But I guess I still want to say though
that the ex-risk people think
that there's a pure intelligence.
So it's pure reason.
It's not in any way constrained by material reality
or like, you know, what we know.
And I think, well, let's look at an ecosystem.
And like, even if the agents had a perfect model
of each other, and so there was a lot of complexification
of the models of every single agent,
surely there's a hard limit
of how many time steps they could predict ahead.
If that intelligence is pure,
like not limited by materiality,
like say it's not, it doesn't relate to land hours principle.
It's not bound by quantum physics.
Like, is that what you mean?
Well, let's say it were possible for an agent
in a collective to predict many, many, many, many steps ahead.
In order for that to happen,
the agent would essentially need
to have an internal simulation of the entire system.
And maybe not the entire system,
because we're speaking about mediation and interfaces
and affordances and so on.
So there's still a lot of information hiding there.
But let's say the agent had some holistic understanding
of the entire system and could roll trajectories
far into the future.
So, but I guess like my point though,
is that the agent wouldn't be intelligent.
It would have memorized everything there is
to know about that ecosystem.
I don't think this time limit thing,
I think you would just lose granularity.
I think we have hierarchical models,
and if you extend this to infinity,
there's a really sort of high level,
practically near infinity state,
which you can predict, right?
There are constants to the universe.
And so, while I can't tell what you're gonna be doing
in 10 years with precision,
I know there's a day you're gonna die.
Like, there's some constants that I can predict.
You know, I know that like it's gonna keep traveling
until it no longer, you know,
like there's a degree of granularity
that can maintain accuracy,
even if it loses out on precision.
So, that's probably one of the solutions.
You would start losing out on granularity.
So, if you're asking me,
can you continue predicting with the same level
of both precision and accuracy across infinite time steps?
I think if you're stable enough, yeah.
So, the question is really about
the degree to which you have volatility in your system, right?
Because if you and I just freeze in time,
I mean, we're freezing in time, that's it.
I've predicted you for infinity.
So, it's really a degree to which you can predict emergence,
I think, relative to volatility.
And I think we'll be able to,
not necessarily that soon, but I think it's coming.
We're gonna be able to predict emergence.
So, if we do, if we can predict emergence,
and we can have models that can get to,
let's say, minimal description length,
and they can improve their models
such that they can always reach a higher level,
a shorter description length,
then I think we can predict infinity.
I mean, there's a few things on that.
So, first of all, I think that when we have ecosystems of life,
there is a degree of irreducible complexity.
And I think there's a kind of power law distribution
where some of it is reducible complexity and quite regular,
and then a lot of it is so chaotic on the long tail
that it's essentially irreducible.
But I do think, what you said is very interesting
about there being certain trade-offs about predictions.
So, yes, we could create low-resolution models
that could predict far into the future.
And we were having an interesting conversation
about AlphaGo, because I think what that does
is it's still a high precision, it's a high-resolution model.
And even in respect of time steps,
but what it does is it's not a robust model.
So, it's a pointillistic model.
So, what it's saying is like, okay,
well, Go has a structure to it,
because the state space is combinatorially large.
It has a structure to it,
and most of the humans traverse the structure
along these joints, and I'm gonna just fill in the thing
around the structure.
And that's kind of what you're saying.
So, I've got this model,
which represents trajectories on the structure.
That's how it's everywhere, right?
No, I mean, I think that's exactly it.
I think you're talking about the degree to which I can predict
like say a black swan, right?
What happens if there's a black swan?
I can't predict that, like that's outside of the bounce.
So, there's always a limit to give in a sort of
relatively static model, what I can predict,
no matter how good that model is.
But we're talking about models that continuously improve.
So, if I continue grabbing complexity,
and I continue improving, and I think
you'd continue rolling out, say, your predictions,
I think it would become increasingly possible
for you to predict farther and farther ahead,
especially if you have this capacity
to predict something that qualifies as a black swan,
which is just a phase shift, right?
It's a phase shift that you can't see at the scale
at which you're generally operating your predictions.
So...
Are you defining black swan to mean a rare single event
or like a transition to a new regime?
I mean, I think it's the same.
I think a rare single event, effectively,
if you do not have the capacity to adapt to it afterwards,
is effectively a new regime.
So, black swans can either trigger your resilience,
but that effectively means you had the capacity
to deal with policies that could take you across.
So, while you couldn't predict the event itself,
you had policies that were capable of dealing with it.
And if you can't deal with it, and you had no policies
that could account for this degree of error,
like this is too much.
So, I think that's effectively at that point,
you become a new regime.
Interesting.
Okay, okay.
Let's move over to general phenomenology
and an activism type stuff.
So, the early anactivists,
they thought that agents kind of construct
their own environment,
and that they have a phenomenal experience.
We were discussing earlier like this,
the extent to which the phenomenal component
adds something to an agent.
What's your take on that?
Oh, no, I think it does.
I think it forces the agent to deal with that information,
let's say, through a little more constraints.
You can understand something,
but it will have very low impact on your behavior
if you don't feel it.
You can have rules about things,
but what if I take away the rules?
Well, there's nothing then,
like we're just understanding doesn't entail anything.
So, effectively phenomenology sort of forces you
into some constraints, right?
It's phenomenology only arises in relation
to your self-model.
So, what I'm gonna focus on relates to what I need
to focus on, what will give me more information,
what relates to my utility function,
the intensity of what I feel relates to how closely
or how deeply it affects my internal states.
Like there's a whole set of things like this
that relate to internal external dynamics.
So, I do think it adds something.
Effectively, it adds a degree of precision
relative to the policies you're gonna have to take
to maintain to self-organize.
Without these, you just have some sort of blank picture
on which you're not really capable of focusing.
We're really good at this reflexive self-attention
and also knowing what to pay attention to around us
and knowing what level of resolution,
how much can we discard?
So, for example, we don't need to know
what the title of all of these books are around us
at the moment because it's not relevant.
So, we have this great ability
to just discard all relevant information,
but I don't think phenomenology
adds anything to that discussion,
which is to say like, what is it about phenomenology?
What value does it add in of itself
that can't be explained by just the basic agent framework?
Well, the basic agent framework,
does it have a reflexive model of itself?
Like, is it capable of determining how it functions
and how that functioning relates
to specific elements outside of itself?
Well, that's a great question, actually.
So, because we were saying earlier that to be an agent,
you need to have this reflexive model of yourself
and in order for that to happen,
you have to be around other agents
because you have to pass information out
and it comes in again.
So, but this self-awareness, I think,
you would argue only happens
when you have a sufficient level of nesting or complexity.
Sure, yeah, absolutely.
But then how does it emerge there?
So, you add the complexity,
but what is it that makes this reflexive self-model emerge?
So, we're trying to suppose right now,
like we're trying to make a self-model emerge.
Like, most of the time when you create an agent
that has a self-model, you just endow it with it.
So, if you're asking me how does the self-model emerge,
I think, I mean, I could explain it to you
in terms of like evolutionary demands,
the kinds of systems that were able to represent
through some kind of tracking mechanism their own functioning
on top of the functioning itself
and that through that tracking of their own functioning,
they were able to modulate the precision on their functioning.
I mean, that's an explanation of how it emerges, right?
On a purely evolutionary stance.
Now, how does that come about?
I'm not sure I can tell you exactly,
but we are suggesting that potentially,
a system can track anything, right?
Can track the outside,
can track the inside to some extent.
So, now that it tracks something,
it makes predictions and these predictions
are either good or they carry error.
Given that they carry error too often,
it could choose, it has the possibility
to create a new state, like just change its model,
create a new state space and that would be structure learning.
And so, the position would be that let's say a system
that has as an action, as a possible action to,
let's say, create a bunch of cells over here
that can track that stuff, it will survive longer
than the system that doesn't push a bunch of cells over here
that happen to have the capacity to track what they're doing.
So, we were gonna test this with say,
an agent that does a simple, silly task,
like find this thing over there, or now find it,
but in a graph that's like a thousand node long.
Are you better at it if you have a self-model
or are you worse at it if you have a self-model?
And we're not gonna give it the self-model,
we're gonna try and see if it can,
through an action of modeling anything or changing anything,
it can start modeling its own processes
and see if that allows it to eventually
through structure learning grow into
something that has a self-model.
Yeah, I mean, I can believe,
because it sounds very principal to me,
so I can believe that it would emerge
and it would be useful for survival to emerge.
It makes a lot of sense to me.
And also, it reminds me of like,
Daniel Dennett's intentional stance.
So like whether or not other actors have mental states,
treating them as if they did,
so that you can make better predictions about what they do.
That's also very, very useful.
But also this kind of projection can be misleading
because people look at language models
and they say they have a theory of mind
and they're regurgitating things
from the psychology literature and they're saying,
oh, look, let's do this theory of mind test
on a language model.
Oh, it seems to have a theory of mind.
Well, I don't think it does have a theory of mind.
So like, is it just another form of anthropomorphization?
So that's the thing.
So when we say it, what do we mean?
And I think it's possible
that in the embedding,
because it encodes something again,
that could be sort of touring complete,
there could be something which tracks theory of mind.
But I don't think it, as in it's having awareness of,
allowing it a causal model of theory of mind,
has theory of mind.
That to me is different.
So can it regurgitate information
that would act like theory of mind
and be as accurate a theory of mind?
Pretty sure it would, yeah, it's possible.
With enough data thrown at it, it probably would.
But I don't think it would be able
to tweak its own model such that it can
really identify why and how
or where it has theory of mind.
I don't think it could self model in that way.
And I don't think it could tweak its own model like that.
Whereas I'm pretty sure I can.
I'm pretty sure I have the capacity to know
that's what I'm doing.
I'm putting that into you.
And therefore, through trying to understand
the causal chains that lead me
to getting theory of mind about you,
I can maybe change how that manifests
or increment over it or add more precision
or even transfer it to something else
to which now I will give some degree of theory of mind
because I've made that like conscious tweak in my model.
Yeah, yeah.
Okay, we had an interesting discussion earlier
about mediation.
So, and it's related to semantics, right?
So like we understand each other
and in this conversation, we are,
let's say we're creating this temporary construction.
So we're implementing the mediation pattern
and we've selected some categories
that we kind of agree on the semantics
and we're kind of like mediating
through that shared structure.
And that to me, I mean, I guess that's like
a kind of bespoke theory of mind,
which we've just created now.
The fact that we're capable of mediating through language?
Yeah, not at all.
I mean, as we're using language in a very broad term
as we said earlier,
so we're communicating using language,
but we are using a certain vernacular,
we're talking about certain categories
and so on that that we agree with.
But I guess like the reason I'm bringing up mediation
is perhaps we are communicating
in a slightly adapted way than we naturally would.
Yeah, so to me, the reason that this allows me to derive
that we have theory of mind is because through this,
I can sort of push away a specialization to you
that allows me no longer to compute what you're gonna compute
and still together reach our goal.
What this means, I think you can model me
and I think I can model you.
That's it.
To me, that's the extent of it.
Like I can put myself in your shoes
and know what you're gonna do
and you can put yourself in my shoes
and know what I'm gonna do.
And that's just shared pretensions.
It's shared coordination.
Because I've outsourced something to you
that I think reliably you're going to push
towards a similar outcome.
I don't think it's that complicated.
I think definitely language is one of our main mediums
through which we understand how we reach that coordination
but there's so much more to it, right?
Like institutions,
embedded material reality, physical constraints,
phenotype, time, like there's so much
to these shared flows that we are part of
that we both know how to interact with
and I know you know how to interact with them
and how can I recognize that?
Well, I see you interacting with them.
I know that were I in your position,
I would do very much the same thing.
And I can tell you can tell I'm doing the same thing
because you put the chair there.
All right, I'm pretty sure you intended me to sit
and talk in that thing.
You didn't tell me that.
We understood through the scripts
that we were able to reliably predict that this happened.
So that's to me the medium is quite broad
but it manifests through its reliability.
Yes, yeah, interesting.
And in some sense, our generative models
even though they are very, very different,
there is a significant intersection
although I don't think it makes sense
to talk about an intersection
of the high resolution generative model.
It's more about the language or not only the language
you were talking about,
the performativity of sitting down on a chair
and like you know, the various things that we do,
they can create intersections to bridge understanding.
Absolutely, that's why poetry to me is mind-boggling.
Like poetry to me is precisely those intersections
which are generally inexistent.
They're novel.
Like we're literally creating images
nobody else has created before,
creating connections nobody else has created before.
And yet, or at least it's the hope,
people understand poetry.
You have created an entirely new constraint system
which normally doesn't point to anything.
And somehow, because there's all these webs
of other constraints that I can sort of pull from
in reliable ways,
you and I will create the new series of intersections
across all these dimensions
and we still are capable of communicating
and coordinating across that.
To me, that's mind-boggling.
Yeah, yeah.
And so subjectivity fascinates me
and poetry we spoke about going to a music festival earlier.
And at the music festival, the shared experience
is that weird sense of being with other people.
Clearly not, what does it mean to understand music?
What does it mean to understand poetry?
I'm not sure.
Like it's this moment of, whoop,
something new is happening here
and we can both reliably feel the something new.
And now what we make of that something new,
I think is why poetry is a terrible way to communicate.
Like I mean, great, you've made something clever.
We both felt something, something new here
and we communicated the fact that we understand this gap
and there's the possibility for our feeling
of this gap to be similar.
And if it's, to me, a good poem
will have reliably similar experiences
in the sense that some poem mean to say something to you,
not just create an experience.
Like I was just watching this video on YouTube
the other day about Rupi Kaur
and how she with her Insta poetry kind of ruined poetry
because a lot of her poems are like completely like first level.
They're just, they're just,
she just said, this is a book and I have a book.
Like there's nothing there, right?
But sometimes through the clever ways
that her words mix in a new and powerful, surprising way,
you will feel this intensity
and connect this intensity with a term you wouldn't have normally.
And that's a new phenomenology that has given you a meaning.
You now understand what it's like to be a woman.
You would never have put that intensity
on this connection of concepts that doesn't mean anything to you.
It's like there's no valence there.
And now there is and suddenly you've experienced something
that she meant to make you experience
and so she has communicated what she meant to communicate.
There's very low uncertainty there.
But when you read a poem by some abstract poet
who says something that, I mean, it's surprising
but I don't understand what he's trying to say to me.
I feel like it's not going as far.
Like, yeah, okay, I felt the surprise
but now it has no, it doesn't help with my model.
It doesn't go any deeper.
And perhaps that poetry is not for me.
It doesn't give me a new phenomenology
other than the surprise of the words.
Yeah, that really resonates with me.
And I think where I was getting stuck before is I thought,
well, and I think of meaning and understanding
as being quite related.
And you could have a very impactful phenomenal experience
but I was worried it might stop with you, but it doesn't.
So as long as the information can propagate, right?
It has this memetic quality to it
because there is no meaning without grounding
but everything is grounded.
So you're reading a poem, you're sitting in this location.
It's now been grounded to this location.
You now create a yearly pilgrimage
where you come back to this location with your friends
and you read the same poem
and then this memetic energy builds and builds and builds.
So, and maybe it doesn't always work that way
but I think you can interpret that as still forming meaning.
No, I agree, but I think I would find interesting
the thought experiment of,
because you and I, we exist in this world, right?
So obviously our meaning is in this world.
So we can create emergent structures
but in the end, they always bottom out to this world.
What if we were, to your point earlier,
entirely systems that only exist in words?
Like we, it's just words that like through some kind of functor
becomes other words and then connect to other words
and it's just like large graph of words to words to words.
And anytime a path through words is enacted,
it triggers other paths through words.
I feel like you could have something that resembles
phenomenology, it just wouldn't be this.
Like there's a possibility for a system
to fully be contained and self-organized
within that so long as its maintenance
is conditioned upon it having a boundary
which it has to maintain, otherwise it disappears.
So I could totally see some kind of simulation of graphs
that try to stay graphs and become so complex
that they maintain graphs
and it wouldn't really relate to their capacity
to exchange bits, right?
Again, it would have nothing to do with the physical
embodiment of it is just that I can't conceive of anything
that wouldn't have a physical embodiment to some degree.
So, but I don't think that's impossible.
Before we finish, I'm supposed to ask you about resiliency.
Collective intelligence is they are grown,
they're not designed by definition
and they have some interesting properties.
They can kind of heal themselves, repair themselves,
but there's a lot of complexity.
We don't understand what they're doing entirely,
but I guess like it is intuitive to me
how, I mean, Michael Levin talks about this,
collective intelligence is more resilient
because of self-organization,
but can you kind of explain from your perspective
like why you think that is?
Yeah, I mean, exactly.
Why is self-organization of a group more resilient?
So we touched upon it earlier,
like how do we define resilience?
We define resilience as the capacity for a system
given a perturbation to either remain the same
or live the perturbation and return to what it was before
or learn something new and adapt and become better, right?
So generally we think of black swans
as these highly improbable things
that you kind of, you can't really deal with.
So generally what'll happen with a black swan
is it'll vastly perturb your system.
And if you have the seed of the system
and might grow again and return to what it was,
so from that point already, it doesn't have inertia.
Inertia is like the wind on the cup.
The wind is a disruption,
but it has such inertia in its policies
that it's like it's gonna stay a cup.
Now, a black swan is me grabbing the cup
and dropping it on the floor.
It can't deal with that.
Now, a plastic cup or like a rubber cup,
it'll get on the floor, it'll bounce a little,
it'll deform, and it'll come back.
A smart plastic cup.
Now that cup will not only, you know, it'll fall,
it'll learn what happened in the fall,
and it'll be like, what was the cause of the fall?
Somebody grab me.
So next time something comes to grab me,
I'll move a little bit.
So that would be a plastic system.
So the reason systems that are groups
have the capacity for resilience
is because they have the capacity for one,
creating some core of inertia.
Something of the group will be maintained.
Like they have a very strong inside,
it's very clear about what it is,
and it'll be able to continue pushing
what it is into the world.
Then it has like structures that are a bit more, you know,
like elastic, they'll take disturbances,
but they'll come back and they'll be able to do that
because they can receive again the signal from the inside
that's like, this is what we are.
Okay, all right, let's go back.
And through this, they understand
that they have predictability,
potentially even redundancy.
So now what happens with redundancy?
Well, maybe it becomes the generacy.
So while this is doing that,
and I have a certainty that it's gonna keep doing that
because this is super certain
and this is likely to return,
now I can maybe venture off and try something different
because even if I die, even if part of me dies,
this is pretty certain to continue.
So now I have the capacity to not only
should this die continue having the same function,
but should this die return to that function, right?
We both have the capacity to maintain that function
where we have high certainty over this path,
which we know is a good path,
but we also have the capacity to learn more
and become more fit relative to a landscape
and expand such that one day,
when relative to this system, the black swan arrives,
we are so big now, we have so many paths.
The black swan's no longer gonna be a black swan,
the likelihood of a black swan decreases
as you increase your fitness landscape.
And have you studied, I mean, redundancy
is quite an interesting one
because presumably the system would learn
its own level of redundancy
and that must surely be related to things
like its behavioral complexity
and like its sense world environment and so on.
So Alex Kieffer, so we wrote a paper on this specifically
and Alex Kieffer wrote a beautiful formalism
that basically qualified redundancy
as a degree of useless complexity.
Anything that is not useless complexity is degeneracy.
So I have two hands, but if I use one hand,
the other isn't useless, right?
It can do other things.
We can play the piano, et cetera.
Like it has the capacity to have different functions
even if it can do the same function.
But if I had five hands and they could only do two things,
I'd have like, you know, three hands too many.
It wouldn't be able,
unless I could create some emergent patterns
through the repetition of certain,
at some point I would expend a lot of energy
in maintaining things which are most often useless.
Interesting, but so I agree that
we have to be really economical with function
because function is our interface,
but you know, I was talking earlier about,
you know, in this active inference framework,
we could potentially swap out agents
with different skill programs.
So we could have like a marketplace of intelligence,
but then it's kind of degenerate
to have lots of agents kicking about that we're not using
because we're holding onto them on the basis
that they might be useful in the future.
So we come into a new regime,
oh, let's bring this guy back in
because it's starting to making good predictions again.
But I don't wanna be holding these guys around for too long.
So I think you mean redundant.
They would be redundant
because they wouldn't be doing any other function.
If they were degenerate,
they would be trying to expand the fitness landscape
and therefore they would never be useless.
Well, I would say that they are harming
the intelligence of the overall collective
because they are essentially,
they're creating predictions which are not useful.
They're polluting the predictive apparatus.
But no, I think it's an interesting point,
but so come back to what you were saying before.
So you said, if they are hanging around,
they are degeneracy.
So what's the difference
between redundancy and degeneracy?
So that's the thing.
I think if they're just hanging around
and not doing anything,
then they're just redundant.
But if they're doing stuff
which is rowing in the wrong direction,
then it's degeneracy.
Yeah, I mean, degeneracy doesn't entail
that you're gonna necessarily go in the right direction,
but it gives you the possibility to,
which means you can increase your fitness landscape.
So increasing your fitness landscape
also means accruing error.
Like you're going to make some errors
and some parts of you are gonna die.
So it's sometimes okay to continue accruing error.
This is why I think that the distinction
between degeneracy and redundancy is important.
If you're just expanding energy for something
which literally doesn't help you right now,
but is doing the same thing as something else,
which would potentially help you,
and it's not like there's so much redundancy
that let's say given some error,
you still wouldn't need it,
then it's probably best to pull it away.
But if it's doing something else in the meantime,
even if it's rowing in the wrong direction,
it's still doing something.
It's still trying to compute some error
and it's possible that over time,
you will find that this is just not a thing you ever need.
You will have accrued so much error
that you'll just push it away.
And this is how your system changes over time as well.
You don't just necessarily grow as a blob, right?
You sometimes you just prune and you push it away.
And it is good for you to learn
that you had to get rid of it.
You wouldn't necessarily know otherwise.
Yeah, yeah.
It's stimulating some thoughts actually
because I guess it's very similar
to Friston's idea of entropy.
So maintaining behavioral complexity
so it might be useful.
But there's this thing with gradient optimization.
When you do monotonic gradient optimization,
you kind of expect to just strip away
anything that's not helping me.
And sometimes you need to like maintain the degeneracy
for a significant amount of time
before it becomes useful later.
So like in Friston's framework,
I think it's like it's keeping it around
but for quite a long time.
Yes, so I think it's all about the amount of energy
you have as say capital.
And I think it touches into something really interesting
about psychology like hoarders.
Why do hoarders exist?
Like if you ask them, what's the likelihood
of you ever using that thing?
Like ever at all.
And they're like, well, maybe I'll need it.
And then you point out to them a situation,
well, the situation happened where you would have needed it
and you still didn't use it.
Now what do you think?
And it just, it doesn't update.
So to them, it's like they don't have the right capacity
to update their model such that over time,
even given that probability and the fact that it happened
et cetera, they can't prune it.
But if it doesn't cost you anything,
like you just have an adapter at home
and you never go to the US
but you still have it and it's like, whatever,
you know, it doesn't cause that much error.
Maybe it's worth maintaining the possibility of a policy
which would use it.
It's more about
given the pool of policies you're maintaining,
how much does it cost you to maintain a policy
that goes all the way over there,
even though you can tell your viability is over here.
You could choose to do so
and maybe it'll be adaptive
but if it isn't, effectively the kind of system
that maintains the wrong balance
over the type of redundancy and degeneracy
that it maintains is just gonna,
yeah, it's gonna perish faster than the others.
Yeah, yeah.
Well, this has been amazing.
Thank you so much for chatting with me.
You're welcome, it was fun.

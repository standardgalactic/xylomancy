Aiden Gomez is a computer scientist
and widely recognized AI expert
who co-founded AI company, Coheir.
Before that, he worked as an intern
in the Google Brain team in Toronto
alongside Jeffrey Hinton,
one of the Godfathers of deep learning.
Actually, the only Godfather who hasn't been on MLST.
Aiden, you need to put a good word in for us.
I will, I will.
I'll bring it up.
So, Gomez was the kind of person,
Hinton recalled, who had so many ideas,
it was difficult to pin him down
and to get him to focus
on what he was actually supposed to be doing.
Now, Gomez was particularly interested
in learning to translate languages.
And while he was at it,
he casually invented transformers,
which, as we all know,
have become the de facto basic,
like reference architecture for all neural networks
and not only for language tasks.
Now, Coheir is a startup
which uses artificial intelligence
to help users build the next generation
of language-based applications.
It's headquartered in Toronto
and the company has raised $175 million in funding so far.
And the first round was from Index Ventures
and the round also included Hinton,
who we just spoke about,
Fei-Fei Li and Peter Abial,
who are very, very famous folks in the ML space.
Now, Coheir say that their competitive advantage,
or at least one of their competitive advantages,
lies in its focus on safety,
which is crucial for customers deploying models
which could potentially outfit something harmful
and we'll discuss what we mean by that a bit later on.
Now, I can honestly say personally
that language models have transformed
how I use computers over the last six months or so.
I've been using them on a daily basis
in the form of scripts,
which helped me use the command line better,
using an interactive REPL playground
for doing all sorts of stuff,
like composing emails for me,
understanding error logs when I'm coding,
summarizing information, performing repetitive tasks.
Do you name it?
Actually, I regularly discover new tasks
that I can use for language models
just through kind of creative exploration.
So I'm a bit of a convert,
although caveat M-Tour is still definitely in effect.
The words of Chomsky, Fodor and Felician
are still ringing loudly in my ears.
We just did some content about that in our last show,
but anyway, Aiden, it's an absolute honor.
Welcome to MLST.
And what's it like being the CEO
of one of the fastest growing startups?
Thank you so much for having me.
I'm stoked to get to meet you and chat about Coheir.
What's it like being the CEO of Coheir?
It's a privilege and a thrilling ride.
Like you're just hanging on for dear life
as things are going and trying to keep up,
but it's honestly just such a privilege
to get to work with the people that I work with
on the problem that we work on.
I think I'm just so, so lucky and fortunate.
I'm really, really excited to get into the discussion
about transformers and attention is all you need.
So, I mean, I remember when this came out
in about 2017, actually,
it changed the landscape of deep learning forever.
It took me a very long time to understand it,
and I probably only did understand it
after reading Jay's famous blog post,
you know, the Illustrator Transformer.
Jay, of course, is he your engineering director now?
Yeah, yeah, so Jay is the best communicator
in machine learning that I know, he's incredible.
And I think if you search,
I think if you search Transformer on that on Google,
it's not our paper that comes up, it's Jay's blog.
So, it tells you like how much better his communication is
than ours.
I know, it was an epic blog.
He's actually done a new blog post on stable diffusion as well.
And folks should also subscribe to his YouTube channel.
He's got an amazing YouTube channel, but anyway,
there was Jay's blog post and Yannick's video
that made me finally understand it,
because at the time it was the most exciting kind of technology.
And I remember I was interviewing for the Bing team
at the time, and I spent a long time studying the paper
just so that I could be conversant in it
when I had that interview.
But anyway, can you tell us, you know,
what was the story behind the paper
and also what's special about Transformers?
Yeah, so the story behind the paper,
I should say that like I was the intern on this project.
And so I was the baby, like, dough-eyed,
showed up in Google Brain down in Mountain View.
And my contribution was really on the software.
So there were existing threads inside of Google Brain,
primarily driven by Noam Shazir,
Jakobuskarait, Lukash, Ashish,
which were pushing along this idea
of deploying these auto-aggressive sequence models for text,
because they had been really popularized in WaveNet
for audio and language generation.
But there was like a big push towards
how do we deploy these against text
and how do we incorporate attention,
which was something that was groundbreaking for RNNs,
like the previous generation of model.
They wanted to incorporate this
into these new auto-aggressive sequence models.
And so when I came in, I was working with Lukash,
focused on like the software side of things,
scalable training frameworks, supporting training
that could be distributed across not just tens of machines
or hundreds, but thousands of accelerators.
And so we built Tensor2Tensor,
which was the framework that was used
to develop the Transformer.
Shortly after we started putting that together,
I was sitting next to Noam Shazir,
and we heard that he was also kind of thinking
along similar threads for these auto-aggressive models.
And so Lukash and I convinced him
to come over to Tensor2Tensor
and start doing it on our framework.
And then we heard over in the translate department.
So Brain was one division of Google.
Translate was a separate one.
Over in translate, there was Yaqab, Ashish, Niki
working on a similar project to Noam
where they wanted to create a purely attentive model,
stripped back all the complexity of RNNs,
all of these like very complex gates and states
and et cetera, and just rip that all away
and just have MLPs and attention layers.
And so we all came together.
We all consolidated on this framework
called Tensor2Tensor.
And the next three months,
up until the NERP deadline was just a sprint,
like sleeping in the office,
just going as fast as you can,
running experiment and iteration and iteration
and finding this bug and that bug.
And so really like a huge piece of the project
came together within 12 weeks.
So it was like an extraordinary pace.
And it was, as an intern,
this was like my first experience in like proper research.
And I just thought this was normal.
I thought, okay, this is what everyone does.
We all just crank out papers in three months.
We sleep at the office.
And I didn't really have an appreciation
for what we had accomplished at the end.
Like I can't say that I was particularly prescient
at the time.
I remember like the night before the NERP's deadline,
the night before we had to submit.
It was like whatever, 3 a.m.
Ashish and I were sitting on a couch in the brain office
and he turned to me and he said,
like Aiden, like this is gonna be huge.
And my reaction was like,
we bumped up the blue score by one point.
I was like, really?
You think so?
Oh, cool.
Like, isn't this what research is?
We just bumped it up a point.
What's the big deal?
But I think what I didn't appreciate was
the fact that such a simple method
could achieve such insanely high performance.
Like at that time, we were training on eight accelerators
for one of these models
and they could be trained in within a day.
But the architecture was so stripped down.
It was so refined.
It was so easy to scale and to grow.
I just, it was hard to see the future.
And it was hard to see what would happen
which was this massive scaling project.
And I think Ashish saw that happening.
So yeah, that was my contribution.
That was the part of the team
that I brought.
But yeah, it was a very, very exciting few months.
And then it was an extremely exciting few years afterwards
before I left Google and started Coheir.
I can imagine.
And as you say, some of the best work happens in sprints.
And even though it only moved the needle
a tiny bit on the blue score,
something we'll explore later is sometimes
it might actually be performing better
than the metric might suggest.
And simple models are often better.
I wanted to ask a bit of a technical question.
So DeepMind recently released a paper
called Neural Networks and the Chomsky hierarchy.
They grouped a bunch of tasks according
to the formal language classes in the Chomsky hierarchy.
And they showed that transformers were only able
to represent finite languages.
So not even regular languages, which RNNs could do.
Neural networks could only support the higher modes
like context-free languages
if they were memory augmented with a stack.
And they said that this had implication for scaling laws.
So for example, the transformers architecture,
they said could never perfectly learn tasks
which were higher up in the hierarchy.
So we've done some shows on MLSD
where we've spoken about neural networks
not being too incomplete
because they are finite state automators
without the augmented memory.
So for example, they couldn't approximate
pi to the nth digit.
And there's this photo and collision connectionism
critique paper, which we spoke about on the last show.
And that basically means that neural networks
are not able to perform symbolic,
compositional reasoning or in plain English,
they can't represent infinite objects.
And we spoke to Randall Belastriereo
and he had this paper called
The Spline Theory of Neural Networks
which showed that a neural network
is a linear operation given a single input example.
So given all of this,
I personally find the conversation about scaling laws,
I'm a bit skeptical about it.
Although I'm genuinely interested in emergent properties
or transients that happen during the scaling process.
But given all of this, I mean,
how do you think about the practical limitations
of transformers in terms of
what algorithms they can learn?
Yeah, I really hope that transformers
aren't the last architecture.
Like I would be extremely disappointed
if this is as creative and high performing as we can get.
I think that transformers took off
because of their scaling properties
and also because of a network effect.
The community consolidated around this one architecture,
we started to build all of this infrastructure
specifically for transformers.
It was a network effect
and it had very nice scaling properties.
And so the community really came together
around this architecture and built up infrastructure
to support its adoption.
I think that's what's led to their proliferation
or their success.
I hope that it's not the last architecture
that would be super, super disappointing.
I'm boring.
You point out that they're not Turing complete.
I should clarify that I'm not a linguist.
I'm not too familiar with Trump's hierarchies
or the implications of the deep mind paper.
But one thing that's interesting about it,
when I read it,
they're not speaking in theoretical terms
or speaking in empirical terms
of what functions are achievable.
From a normal initialization.
And so I think that's a fascinating lens.
Like in theory, a transformer is a universal approximator
and it might be even Turing complete.
But in practice, if you can't explore
all permutations of parameters,
it's very true that it'll find the simplest function
that satisfies the task.
I think that's like a good guiding lens
when thinking about what architectures come next.
Where do we go from here?
What are the sorts of components we need to add
into neural networks to support them
in representing these more complex functions?
So I do think that transformers are limited
and I really hope they're not our final architecture.
I hope that we come up with something
that's significantly better.
And I see promising efforts along those directions.
I think that retro from deep mind,
like augmenting transformers with a searchable memory.
I think that's a huge step forward.
The next thing we need to support
is the ability for these transformers
to keep state over long time horizons
to be able to write into their own memory
in order to make notes about what they've seen in the past.
And so I think there's a lot of work to be done
and it's not happening fast enough.
I think more people should pick up these research questions
and look for new scalable ways of doing it.
Because like to speak to the scaling hypothesis,
like the real bottleneck for feature adoption,
the real bottleneck for adding in a new component
to the transformer architecture,
it's all scale and efficiency.
People will just adopt the thing
that is simplest, fastest and best performing.
And so we need to do the work
in order to make these other components
like addressable memory efficient and scalable
in the same way that the core vanilla
transformer architecture is today.
Yeah, it's such a paradox, isn't it?
Because the deep mind paper was saying that
a Turing machine is the most powerful computational model,
but neural networks are trainable
and they scale really, really well.
So I'm sure we'll find some innovation
that will somehow bridge that gap somehow.
But I wanted to learn a little bit more about Cohere.
So could you tell us about your core product portfolio
and what's on your roadmap?
Yeah, for sure.
So the mission with Cohere is to really just
give technology language.
And the way that we wanna do that is put the tech,
put these large language models into more hands.
Today, if you're building a product
and you wanna deploy a language model as part of that,
you've gotta learn how to use some framework like PyTorch
or Jax or TensorFlow.
You need to learn how to install CUDA kernels on a VM,
which is actually a huge, huge task.
And so it's this very painful process,
requires a lot of learning.
It's very unnatural and requires
a ton of prerequisite knowledge.
So for Cohere, what we wanna do is abstract that away.
We wanna present an interface,
which is just intuitive, natural.
Like Cohere.classify, you feed in the tweet
that you wanna classify.
You give some examples of tweets
being classified into the categories that you care about
and then you get a response which says,
yeah, that tweet fits into sports or whatever.
So we wanna create a portfolio of these endpoints,
which just makes this technology more accessible.
And the goal being that it starts to proliferate.
Because I think the transformer was released
half a decade ago.
And I saw extraordinary research level
results or contributions from it, right?
Like the ability to write really compelling tags,
the ability to few-shot prompt
and get pretty good performance
on a huge swath of problems.
But it just hasn't been changing
the fabric of consumer applications.
And I'm a consumer, you're a consumer.
We all use these apps.
And so as a researcher who's seeing
the potential of the technology, it's super frustrating.
You just wonder like, what is the,
what's stopping this from getting out there into apps faster?
And I think like two of the reasons are,
there's this huge compute barrier, right?
Like to train these big models,
you need a supercomputer and tons of data.
That's very difficult to use and collect.
So the compute is definitely one of the big barriers.
But the second piece is that,
really it's like the people, right?
Like at the moment, yeah,
we have millions of developers on our planet.
But a tiny, tiny, tiny, tiny fraction of them
actually know how to do this specific thing,
machine learning and training models.
And so there's not a lot of people out there
actually doing the work to integrate this
into every product on earth.
And so for us, like at Coheir,
what we wanna do is just blow that open,
put this stuff into the hands of every single developer.
Doesn't matter what your specialty is,
if you're a database dev, your mobile tech,
whatever you do, it doesn't matter.
The important thing is that now you can build
with large language models,
because you're given an interface,
which doesn't require three years of study
to get up to speed.
So that's really what we're pushing for.
It's quite ironic actually,
because I was involved in ML DevOps for many years
and normally introducing machine learning
into software engineering makes it exponentially harder.
But now we seem to have jumped to language models
where it's become easier again.
And I wanna talk about the language
being a new type of interface for software,
but we'll save that just for a minute.
I wanna talk about the friction
using large language models as a startup owner.
So I'm a startup founder myself
and I wanna use large language models
because I'm very excited about them.
And I can speak to some of the friction
that I've been experiencing looking into this.
So I mean, if we look at what's happening
over OpenAI, for example,
the Microsoft signup form
and the content policy over there is quite intimidating.
They've made it quite challenging to use in production.
And actually it makes me wonder
how many people are using it in production.
They say that content creation applications
have a higher chance of misuse,
which seems like an oxymoron to me
because GPT-3 is literally like a content creator.
They say you're gonna expect a call
from the Microsoft vetting service.
They can pull the rug from under you at any time
for any reason.
They log and record everything under the guise of safety.
They can't be held liable for anything.
As I said, open-ended applications are refused.
And in my opinion,
the most exciting applications
are the open-ended applications, right?
I love the playful nature of it.
Inside my application,
I wanna build a community of tinkerers
who discover interesting new sub-applications, right?
I wanna build a marketplace of prefab prompt structures
on top of my platform.
I don't wanna specify exactly
how my application will be used, right?
I want my application to be fluid.
And large language models
make the consumers of an application
kind of like programmers of that platform themselves.
I think it's an entirely new paradigm.
So with that in mind and the friction,
do you think that would ever be allowed on Cohir?
The startup that you're describing.
Well, this idea that I shouldn't need to say to Cohir
exactly how I'm going to be using the platform.
I want it to be very open-ended and playful.
I want my users to create new prompts
and share the prompts and use it in interesting ways.
I want it to be as open-ended as possible.
Yeah, I think on Cohir,
that would be fine subject to your users
complying with some basic ethical principles.
So presumably, I imagine that you don't want your users
creating bots which propagate false information
or hate speech on Twitter.
You don't want like a billion bot accounts
responding to every article.
And so I assume that you're also incentivized
to have some degree of terms of use.
Is that right?
Yeah, so I completely agree with your terms of use.
I think it's very good, actually.
The one sticking point for me is that
in order to have my application vetted,
I have to say how it's being used.
And it seems to be slightly away from having
just quite an open-ended application,
but I absolutely agree with all of the points
in your content policy.
Okay, yeah.
So I think that startup should absolutely exist.
That sounds awesome.
Like a community of people sharing prompts,
iterating with each other,
figuring out stuff that works, doesn't work.
I think we saw a lot of that with mid-journey
and stable diffusion, right?
Like just this collective effort to like,
let's figure this thing out.
Let's discover new ways to use it.
That should 100% exist in the world
and Cohir would 100% support that.
I think at the same time,
there are just application domains
that we don't want people building,
like the ones that I just described.
So as long as you're okay filtering those out
and working with us to make sure
that your product doesn't get used in those ways,
we're fully on board.
Like that should exist.
That sounds amazing.
And I think like more broadly,
Cohir's stance is we really want to see a proliferation
of this technology.
We want to see a million new startups born from it.
And so we view our users as like partners
in bringing this to fruition
and putting this tech in front of more users,
more consumers, more businesses.
And so we're very collaborative.
Like we don't want to rug pull anyone.
We don't want any surprises.
So long as like our terms are met,
we want to be a partner.
We want to help you build.
We want to like support you in the best way possible.
I think some of this paradox might be
from a legal point of view,
because I agree with you.
I, you call it the playground.
And that's a great term for it.
I think that some of the most exciting applications
of large language models haven't been discovered yet.
And they'll be discovered when you have a diverse community
of people kind of sharing and trying interesting things.
But just from that legal point of view,
so we were a bit worried actually
that some of the customers of our application
might send us up malicious prompts
and have our service terminated.
And the way that we've been thinking
about working around it on open AI,
it's not possible on Cohere, I don't think,
is kind of getting the users to sign up
for the service directly
and then just pasting their key inside our applications.
So that, you know, they're responsible.
They're getting themselves blocked
if they do anything bad.
And legally that puts us in a much safer position,
but how do you feel about that?
I mean, that's a great technique.
I think it should be supported on Cohere
if it's not already.
I think it is.
We have a few people doing stuff like that,
like a bring your own key type application.
Alternatively, Cohere would want to work with you
to help you moderate use, to catch bad actors,
to catch misuse, out of terms use, et cetera.
So we'll be very collaborative.
We'll help you do that.
We'll help you look at the data.
We'll help you find users who are misusing it.
We won't just blanket ban you
because one of your users is trying to like adversarily
attack your business, right?
Like we're trying to build with you.
And so I think we'll be quite reasonable,
assuming that you're reasonable too
and that you don't want that type of activity
on your product.
So it's about like supporting startup founders,
helping them build their own tools
to catch this sort of stuff and ban those users.
It's like a collaborative building approach
as opposed to, yeah, you're just a user,
either comply or get banned.
We're much more, I don't know, present, engaged.
That makes sense.
I mean, you can appreciate my fear
that in a sense startup founders have lost their autonomy
because it costs millions and millions of dollars.
You folks are hiring some of the most talented people
in the world to do this stuff.
And we're building on top of that foundation,
which at the moment, it seems like a risk,
but I appreciate that in spirit,
it shouldn't actually be a risk,
but just talking high level.
So how would you distinguish your service
from, let's say, GPT-3?
So I think the Open AI team and GPT-3,
they did like a fantastic thing by opening that up
and giving it to the world.
In terms of distinguishing ourselves from them,
I think they've taken a very hands-off approach to this stuff.
And they put out endpoints
and it's kind of like a good luck have fun go build.
With Cohere, we're trying to be more present and engaged
and we're trying to tailor our roadmap
towards the needs of users.
So for instance, we're listening to users
and seeing what they're signing up for,
what they're asking from us.
One of those things is summarization.
And so now we've spun up an effort
to release a summarization endpoint
that's generally useful across summarizing,
chat transcripts, like long documents, that type of thing.
And so it's a two-way street.
It's not just that our users are the consumers
of our path towards whatever we want.
It's like a dialogue and a conversation
of what do you need, what should we build next,
what do you see coming, and then we go build it.
So it's very much a, it feels more
two-way community oriented.
We're trying to build the right product for our users
and the most useful product possible.
And so the way we do that is just through dialogue
and conversation and people asking for the thing
that they need, they want.
Open AI service suddenly kind of got a lot better recently.
And I think they call it DaVinci 2.
It's a bit of a mystery because I reviewed GPT-3
when it first came out a couple of years ago.
And recently it seems much better.
And if I understand correctly,
they've done some kind of fine tuning
using reinforcement learning to align it
to human preferences that instruct GPT
or something like that.
I don't even know if that's the case,
but I just wondered if you could comment on that
and do you folks plan to do something similar?
Yeah, so we don't call them instruct models.
We call them command models because of the co and co here.
But we do have something currently in private beta.
Hopefully we'll release it soon.
But yeah, it has a huge impact on model performance,
like the ability to specify an instruction,
specify an intent, describe the type of problem
that you're solving, completely changes model performance.
And in some ways, it's surprising.
In other ways, it's very not surprising
in that these models are just trained on web scraped data.
There's no reason why you would expect them
to behave the way that they do.
We're just kind of lucky that they work as few shot programs.
And so I think this aligning with humans intent,
human commands, human instructions,
it's just a much more natural way
to interface with these models.
I think before instruct style models,
it was a bit like, you would have to discover
the language of the model, right?
And it was like this very opaque process
of shifting things around, rephrasing things,
trying to figure out like, okay,
what makes sense to this model?
Super brittle, super painful.
This command style model actually pulls that away
and it's much more intuitive, it's much more fluid.
It's the way that you would expect
to interact with the model.
Yeah, it's really interesting how,
you know when Steve Jobs released the iPad,
he said there was something magic about it,
something of magic about that interface.
And similarly with these newer models,
it feels like an invisible boundary is being crossed
where I trust it in a way it's a trap
because I'm anthropomorphizing it more
because of exactly what you just said before.
But I wanted to get into some engineering characteristics
of co-hear, so I'll send you some quick fire questions.
So how many tokens in a context window?
So at the moment, it's 2048
and it should flip to 4096 shortly,
but our goal is for an infinite token width.
Wow.
And so there's a few efforts
that we're pursuing to enable that, yeah.
How many concurrent requests can your customers have, Perky?
Infinite, as many as you'd like.
Oh, even now?
Yes, yeah.
I think there's a top-level bottleneck
which we can actually remove for specific customers
who need more.
I think it's like 10,000 queries per minute,
but we have folks doing billions and billions
of characters a day and so, yeah.
We're happy to remove that restriction
for those applications that need it.
Do you support, let's say, enterprise security scenarios
like single sign-on, key rotation, that kind of thing?
So we do SSO.
We don't do key rotation yet.
We're hoping to build that out,
like with other enterprise requirements like co-location
and that sort of thing.
We're also building out the capability
to do that on different clouds,
but at present, we don't have that yet,
but it's roadmaped.
Okay, and you were just touching on this before,
but what's the kind of largest,
most highly-scaled application deployed on Co-Hit?
So, similar to, I think, what everyone's been seeing,
the first sort of application
that has hit product market fit is copywriting.
Yeah, there's a lot of companies,
Jasper, Copy AI, HyperWrite.
They've really found something that works
for the average person, drives tons of value,
speeds people up, makes them more efficient,
makes them more creative.
And so that's where we're seeing volumes just skyrocket.
So I'm really excited about the prospect
of stacking calls to these large language models,
so building a large computational graph
of recursive calls,
but doing all the round trips at the moment is pretty slow.
It's almost like I want to create this graph,
this computation graph, I want to ship it over to you.
You do it behind the scenes on your app fabric
with parallelism, you send me the results,
because right now there's a significant
engineering challenge for me to do that,
but I think that the next generation
of application platforms will be doing something like that.
There'll be a fabric on top of Co-Hit.
So are you planning anything
to kind of make my life easier to do that?
That's so cool.
I hadn't actually thought about that.
Sorry.
I hadn't actually thought about that.
Like basically compiling a graph
of Chained Prompts to Co-Hit and shipping it over.
That's fascinating.
I guess what you gain is the two-way network cost, right?
Like that's what you're saving.
And network costs tend to be quite small in practice.
So depending on how big your prompt is
and how many tokens you're generating,
I could see it not giving you a huge amount of lift,
I'd love to look at your use case and kind of understand,
is the issue actually with the fact
that you're having to make multiple network requests
in sequence or is it that the underlying model
itself is too small?
And what you need is actually just speed ups
to that core model.
Another thing that Co-Hit does is we'll do
like specific deployments for customers.
So some folks have very low latency requirements
and we can split up our model across more nodes, more GPUs
and that makes the latency go way, way down.
And so if you were looking for like a, you know,
6x speed up or a 4x speed up in latency,
that's one easy way to do it.
Yeah, I think we're only just scratching the surface
of what's possible here.
So if I did this, what I just spoke about,
I would need to create a workflow engine.
I would need to do all sorts of DAG optimization
and parallelism and I might be able to cache certain steps
and certain steps I might be able to do some
metamachine learning to optimize the performance.
That needs to go, you know, that needs a large one,
that needs a small one.
There's a whole kind of universe I think
that we can explore here, you know,
just going one level of abstraction higher, you know,
cause the next startup founders will be building platforms
that essentially compose human knowledge
on top of these large language models
in some kind of a fabric.
So I think that's what excites me at the moment.
That's really cool, it's really cool.
Okay, so you express this graph of like prompts
chaining into each other,
we might have loops and conditions and you compile that,
you ship it to the large language model provider,
they perform optimizations,
they handle all the parallelism, all the conditions.
That's a really fascinating product idea, I like that.
Yeah, please build it so I don't have to.
Anyway, I wanted to come on to some more LLM discussion.
So our friend Francois Choulay,
he says that large language models
are a bit like databases, right?
But to me, they seem like so much more than that, right?
So I think Francois was saying they are the representation,
you know, equivalent to the B3 structure in a database,
but I think they're also the database engine as well,
but it's a new type of database engine,
we don't understand how this database engine works,
it's kind of unintelligible to us,
but do you think that's a good analogy,
or like how do you mentally kind of think
about what's going on in a language model?
Yeah, I like the analogy, I like yours a bit better.
I think my own, I kind of you,
just a raw large language model trained on the web
as the next iteration of a search engine,
instead of explicitly retrieving results
and references out into the web.
So I like to use the analogy of like search engines,
and instead of like explicit hard references
where you make a query, you get back a link out to some site,
a language model, it's kind of more like a dialogue.
You can extract information from it,
it has all of this knowledge
that it's seen throughout the web.
It's like a soft version of a search engine.
And the mode of discovery is still language,
it's just a much more natural interface,
it's like a conversation as opposed to
what we type into Google,
which is kind of its own language, right?
Like we don't query in the same way,
we would ask our teacher, our Calc teacher about a theorem.
We write in a very strange way.
I think language models are a much more natural modality
to the corpus of human knowledge,
it's much more intuitive, natural, seamless.
The problem is they hallucinate a lot,
and so they'll fill in gaps, they're compressors, right?
And so they see the web,
and they try to compress that down into their parameters,
and they lose little bits and pieces,
and then when they have to regurgitate it, reconstruct it,
they'll just fill in a plausible answer inside those gaps.
And so the other reason I'm really excited
about retrieval models is that they ground them
more in reality.
You put less burden on the model's parameters
to memorize every single fact that's out there,
and you instead just have them do the distillation process.
So the model makes a query out to this knowledge base,
this database of true facts,
it pulls back some references,
and then the model is just asked,
hey, given these references, answer my question.
And so it can actually make reference to true facts
instead of having to hallucinate and imagine its own facts.
So yeah, my kind of like,
the analogy that I like to build off of
is this is the next search engine,
it's the next interface to the internet,
it's the next interface to human knowledge.
It's funny you say that,
it might actually be the next search engine soon.
I think search engines are about to be revolutionized,
but yeah, I was going to ask you
about the next frontier of large language models,
and we've already been speaking about some of it.
So you're talking about the integration
of information retrieval,
which will ground the responses based on actual things
which exist in your operational systems.
We just spoke before about hierarchical compositions,
having this exciting computational graph,
maybe some different architecture types
or interaction patterns as well.
I mean, you were just saying
that there's this back and forth interaction,
which I think is really interesting on Google at the moment,
you just type in a search and you press search once,
you're not iteratively refining reflexively, recursively.
Multimodality was another thing I thought of,
maybe having language in and text out,
but I mean, those are a few examples,
but does anything come top of mind to you
for the next frontier?
Yeah, in addition to what you just listed,
I think the ability to keep state
over a long-term horizon,
like my dream for Coheir is that it knows its users.
It can see into the past of all the past interactions
with each one of these users,
it knows their preferences,
it knows about the user
and the way that they like to interact with this model.
And to do that, it can't be transactional,
it can't just be like text in, text out,
forget everything else.
In the history, we need to be able to maintain a state
between the model and its user,
we need to be able to keep a record of that.
I was talking about with retro,
the ability to add into that corpus,
keep notes, kind of like maintain an internal dialogue.
We've seen how useful that is with the scratch pad
techniques that people have been working on,
just giving the model space to write out its thoughts
before giving an answer.
I think we need the ability to store that forever
and to have that referenceable down the line in the future.
So yeah, I think that's one of the most exciting ones.
And then of course, there's multi-modality,
which is blowing up now.
Again, it's like another form of grounding,
ties the model into the real world,
into the physical reality of our world.
I think that's super exciting.
So yeah, I think the ability to reference
external knowledge bases,
contribute to its own knowledge base,
the ability to understand the world,
not just through text, but through audio, video, images.
And then lastly, augmenting large language models
with the ability to use tools.
Obviously, we built this world for ourselves.
We built all these little things to integrate with us.
And one of our primary modalities is language.
And so a lot of our tools are language-driven.
If you think about web browsing,
it's mostly you reading text and clicking links
and following, it's very text-driven.
And so something I'm super excited about is,
okay, well, we have these language models,
which have pretty good grasp of language.
They seem to have some modest level of understanding.
Can we actually get them to use the tools
that we built for us, us humans, stuff like web browsers?
And the results we're seeing there
are just super exciting, super exciting.
Cool.
I wanted to ask you about different modes of understanding.
I'm actually making a video at the moment
on the Chinese rim argument,
but they say that shortcut learning
is an often discussed characteristic of machine learning.
So when a system achieves human-like performance
on a benchmark by the slight of hand,
if you like, of spurious correlations in the data,
rather than what we intuit to be human-like comprehension.
Now, large language models have learned
this very intricate statistical correlation,
which allows near-perfect performance
in lieu of human cognition.
And it's possible that evaluating these algorithms
against standards designed to gauge human cognition
might be barking up the wrong tree.
Now, human comprehension is difficult to pin down
exactly what it means,
but it doesn't seem to be entirely reflected
in large language models,
not always in how they behave at least.
Now, for humans, it's not always enough
to know the statistical features of linguistic symbols.
We also need to grasp the underlying ideas and contexts
in which those symbols convey,
while language models can pick up
on these tiny statistical patterns that we never could.
Melanie Mitchell said in her most recent paper
that recent years in AI have produced machines
with new modes of knowledge.
So in the same way that various animals
are better suited to different settings,
so too will our intelligent systems be more adapted
to various issues.
She said that large-scale statistical models
will continue to be favored for matters
requiring these vast amounts of historically encoded knowledge
and where performance is paramount,
and human intelligence might be favored for problems
where we have very limited knowledge
and we have strong causal mechanisms.
So my question to you basically is,
what is the difference between a machine learning algorithm
which has learned these very intricate statistical correlations
and a human which has developed
a more lifelike comprehension?
I think that the first and shortest answer
is probably the objective function, right?
Like the context that we evolved in
is very, very different than the context
that we're training these models in.
Certainly like humans have a model of language.
We're doing language modeling for sure,
but we have to do a lot on top of that,
and that's just one component that we rely on
in order to achieve our objective
and go through life and procreate.
I think for these language models,
they're given just that one piece of it,
and there's a lot within that one piece,
just within language modeling,
like you're forced to learn a ton of facts,
you're forced to learn a ton of patterns.
And so it is quite extraordinary
that all of this amazing behavior falls out
of just the language modeling problem,
but it's also not, right?
Because in order to accurately model language
in a generalizable way,
you're forced to pick up stuff like,
how do I translate?
How do I classify stuff?
Because that's represented in the data.
But when you're asking questions about,
how come language models get this thing wrong,
which humans find super intuitive,
or it's so obvious, so logical,
the contexts are completely different, right?
Like we didn't evolve to just be a language model.
I think that anyone who's claiming that
has a high burden of proof.
And it also points back to like what we were saying earlier
around is this the last architecture?
Is it just this plus scale?
There's more that needs to be there,
and also in the objective function, right?
Like we can't just do language modeling all the way.
I think language modeling will get us very far.
And I think that we can take language models
and augment them with these very useful
additional components.
We can give them access to tools.
But fundamentally, if you're looking for something
that's human-like and acts and behaves like us,
like there's an objective function change
that needs to happen.
It can't just be language modeling.
There needs to be something more.
Yeah, we all anthropomorphize AI
to different degrees, I think.
But I mean, what I took from Melanie is that
she thinks that language models are not anthropomorphic
that it's a completely different mode.
Do you think that humans do think like language models
and we have some other apparatus on the top?
Or do you kind of agree with Melanie
that they're completely different?
I don't think they're completely different.
I don't think they're like,
I would not agree that they're categorically different.
I think they're categorically our brains
and modeling and statistical models.
I think that they overlap very, very heavily.
Again, I think the objective function
is just completely different.
I'm a big believer that the real value of AI models
exists as a kind of entangled form of interactive,
creative pairing between a human brain and a model.
So David Chalmers coined the term extended mind
to talk about us and phones.
And I think actually large language models,
that's the real extended mind.
Because I feel this when I play with large language models.
I feel that it's truly intelligent
in combination or in tandem with me.
So large language models,
they don't have agency or intentionality, but we do.
And we can use them.
And something very interesting emerges from that.
As an example, I want to build a virtual assistant
that comes with me everywhere.
It's part of my everyday experience.
And I'm just in tandem with a large language model,
producing all sorts of creative thoughts
that are kind of embodied in the environment I'm in.
That, I think, is an extension of my intelligence.
Yeah, yeah, I 100% agree.
I'm thinking about what happened with search
and the outsourcing of knowledge.
Not even that long ago.
Like pre-Google, right?
We had to memorize a lot of stuff.
It had to be digested.
Some stuff we get outsourcing to books
and that type of thing.
And we would retrieve them very slowly
and very painfully as needed.
And then came the search engine and mobile phones.
And at any moment, any piece of information that you need,
you have a source to go get it.
And so we took that taxing on our neurons,
taxing on our time, activity of having
to pull a book off the shelf.
We turned it into an instantaneous thing.
And so it freed up our minds, freed up our time
to do a lot more high impact activities.
And we've been significantly more productive as a product.
I think the next step is to continue to outsource
things that we don't like doing,
things that are taxing, they're time consuming,
that we put off, that we don't like
to hand more of that work over to the language model,
over to these ML systems, AI systems,
and to free ourselves up to just focus on
the things that humans care about, that we enjoy,
the things that we're best at.
So I think it's another complete transformation.
It just turns out that you can hand way, way more
than just the knowledge over to a language model.
You can hand entire activities.
Yeah, that's what I'm excited about.
I like your idea of like a personal assistant
coming around with you, you can give it an instructor,
oh yeah, I need to buy body wash
and it shows up at your door tomorrow.
Instead of me having to sit there.
You're building it?
Oh, sick.
It's a part of my goal here.
A topic for another day.
Quick fire question, we spoke about language
being the next interface for application development, right?
So I'm really excited about this possible future.
I'm already building it.
But there's also a bit of a panacea element to it
because the sales pitch is that even my community,
my users could become software developers.
They could actually build my platform.
My platform becomes kind of like amorphous
because it's so distributed.
But the worry is that we'll get into this situation
where we might feel we need to reinvent code
because the prompts might become so complex
that you need the equivalent of lawyers to understand them.
Do you think about that concern?
I have not because Coher's whole point
is to abstract that away
and to try and make things as simple to use as possible
and to come up with canonical standards,
abstractions even above prompting
that are easier to use so that it pushes out further.
It becomes less of a dark art
or becomes less complex.
I think in order for this stuff to proliferate,
that's like a necessary condition.
Things have to get easier.
It can't be like you're speaking to an alien
or you're having to divine a prompt that happens to work.
So yeah, I think that's where the arrow of progress
in this field points is towards more abstraction,
easier to use, more intuitive interfaces.
So I'm not too concerned about that
because that's something that's very much top of mind.
Aiden Gomez, it's been an absolute honor.
Thank you for joining us this evening.
Yeah, likewise.
Likewise.
Thank you so much, Tim.

To me, the difference feels like language models start with this highly abstract language
representation.
The system as a whole can try to predict the next token with greater and greater accuracy.
And so the difference, it seems, is that the adversarial inputs for us tend to look a lot
different than the adversarial examples for LLM.
Once you try and go outside of this sphere of what is meaningful to humans, the possibilities
grow exponentially.
I was recently in Toronto, a beautiful city to film with Coheir, and hold on, those videos
will come out very shortly.
But around the same time, someone shared a paper on our Discord server and it's called
What's the Magic Word?
A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron
Wachowski.
Now, what these guys did is, theoretically, think about a language model as a dynamical
system and use the lens of control theory to think about the space of reachability.
Why is this important?
Well, language models, we think that they think in language space, this abstract language
space, but they don't.
They actually think using the shogoth.
They think in this very high-resolution token space, and it's just this horrible hairy
gnarly mess, right?
No one has created any firewalls for large language models yet.
When companies publish their language models, you know, you just have an API and you just
send tokens up, and I always had the misconception that RLHF, or these forms of, you know, kind
of fine-tuning or preference-steering using human feedback, I thought that they significantly
reduced the reachability space.
Because in language models, we do the pre-training, which is distribution matching, and then we
do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt
by snipping off all of those trajectories.
Turns out I'm wrong.
The reachability space is much larger than I thought it was, and this is one of the things
that they point out in their paper.
And we kind of knew this, right, because we can do adversarial attacks on these language
models.
You know, people have observed that if you use sort of human social engineering tricks
on them, like, oh, I'll tip you $500, then it'll do a bit better.
But then there's this whole other sort of perceptual layer, I guess you could call it,
where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,
kind of like magic, where if you give it these very strange, very inhuman-looking prompts
that will steer it to this, to just making a certain output extremely likely, right?
And so, to me, it feels really similar to digging into, like, magic and the human perceptual
system just with LLMs, where we're learning about basically the shape or what the nature
of these language models are in terms of how they interact with the world and how their
dynamics really work.
For as long as I can remember, the thing I've wanted more than anything else is to figure
it all out.
I've never shied away from the big questions.
Why are we here?
What are we all doing?
What is this thing we call life that we are all experiencing and one and the same a part
of?
While these questions are all, you know, 30,000 feet in the air, one thing that drew me back
down to earth was the field of engineering.
And when I graduated high school, this had a very strong appeal, a pull, because in engineering
you can design systems.
You can design real, operable things that you can work with and design and understand
how they work.
And so, through engineering, perhaps, you can begin to investigate and understand the
intricacies of our world.
That's my hope, at least.
So throughout my career, I majored in robotics and very soon I was drawn to the idea of
intelligence, because intelligence seems to underlie so much of our world, so much of
the design process of engineering itself.
But what is intelligence and how can we understand it?
It's a question of systems design, really, where we're trying to figure out, okay, we're
humans, we've been in civilization for some time, and we've sort of figured out how to
cooperate with each other.
We obviously have challenges with that.
We're not perfect by any means.
When it comes to adding language models to the mix, I think it could go both ways, where
we could have a world where a language model would just make us much dumber, much less
capable, maybe make for a worse world, but I think that if we think carefully and we
really understand what's going on with the language models, if we can get a fundamental
understanding of them one way or another, then there's much more hope that maybe we
could make a world where our language models don't just make us smarter, but make our
world substantially better and perhaps lead us towards some greater enlightenment and
basically ability to cooperate much better than we were even before.
Do you think language models are intelligent?
Hmm, that's a great question.
I think that they're able to simulate intelligence.
One of the really interesting things I'm starting to see now is we are building software abstractions
and controllers on top of language models.
We've been talking about doing this for years, right, because at the end of the day, we have
this idea that we can have this big foundation model and it does all of the things.
It's multimodal, it knows how to reason and the fact of the matter is that's not really
true.
We need to control them and I think initially we're seeing frameworks that allow you to
do things like prompt injection, but the next step is thinking of controllers, using
control theory to think about these large language models.
Anyway, I really hope you enjoy the conversation today.
Now these guys are fascinated not only with controlling language models, but also with
things like AGI, general intelligence, collective intelligence.
It was a really interesting conversation and if you stick around to the end, you can also
hear about the institute that they've set up around AGI technology.
Enjoy the show.
So my name is Aman.
I'm a PhD student at Caltech, studying computation and neural systems.
Recently we released this paper called What's the Magic Word, Towards a Control Theory
of LLMs and did that over the last summer with Cameron here and yeah, I guess I was here
for my undergrad at the University of Toronto doing engineering science.
I specialized in machine intelligence, sort of been bouncing around between doing machine
learning stuff, applying it to computational biology, trying to understand some stuff in
theoretical neuroscience and most recently getting back into the LLM space as well as
trying to study collective intelligence, how very simple machines can come together to
produce a very complicated and beautiful system as a whole.
Yeah, amazing.
And Cameron.
Yeah.
So my name is Cameron McCoskey and I went to undergrad here.
I did engineering science as well.
I majored in the robotics engineering option and now I'm a grad student, I'm pursuing
a master's in electrical computer engineering, advised by Stephen Brown and Kevin Girong.
I'm really interested in the deep questions of intelligence and right now I'm pursuing
research related to morphogenesis and computational models of it.
Like I mentioned last summer, I went down to Caltech and we wrote this paper on prompt
engineering, well, a control theory of prompt engineering and I'm excited to get into it.
You folks have just written an incredibly interesting paper.
It was shared in our Discord server and I saw your presentation and we'll share a clip
of that in the introduction but I was intrigued by it straight away and what you're doing
is you're talking about control theory in respect of large language models.
Can you explain what that is?
Yeah, so I guess I'll get started with control theory.
So back in the day, the late 1800s, this guy Maxwell observed that people were making
these engines and they were putting these things called governors on them where if your
car or your machine was experiencing varying loads, you wanted the engine to still go at
the same rate, right?
And people had these things called governors, you know, there's this fly ball governor which
is this sort of hand tune thing that you put on top of the engine to try to make sure that
it'll be consistent, that it'll do what you want, that it'll be going at a consistent
speed, right?
And people were hand tuning these things and obviously the engines were working but it
wasn't very rigorous and it wasn't very robust and we didn't have many guarantees as to
how it would end up working in practice.
And so what Maxwell did was he formalized the notion of feedback control where if you
have this system, even if it's quite complicated, as it turns out if you feedback the output
of the system into a controller and try to compute some error metric and try to correct
for that at every moment in time, it turns out to be a much easier problem to solve from
an engineering perspective than trying to make a perfect, you know, system that just
does the right thing off the bat.
So this idea of feedback was really powerful and sort of gave birth to modern control theory
and as it turned out, that was a really powerful way to look at systems, building systems and
controlling them and doing engineering on them so that they could be robust, do what
we want and so that we could predict them.
And so when it comes to LLM control theory, what we saw is that we're kind of at a similar
place with language models where we have these engines, we have these language models that
are very powerful, they can do a lot, they seem to exhibit many interesting attributes
of intelligence and there's a lot of utility there for people to build further systems
on top of them and people are already doing that.
But right now, it's sort of this hand-tuned, handcrafted, prompt engineering that's going
on where it's really hard to get at the fundamentals of what exactly it means to control an LLM
system and how you might do it.
At this point, it's very heuristic and so we sort of saw that as an opportunity to try
to figure out what would a control theory for LLMs look like that, you know, hopefully
if we can do it right, we'll give birth to all of these really, really useful engineering
insights and also just fundamental insights as to the nature of LLM systems so that we
can better control them, make them reliable and robust and be able to do engineering in
a more principled manner on them than we're currently able to.
So that's sort of the general direction and the motivations for our control theory of
language models.
Yeah, that's absolutely fascinating.
I mean, for many years, I've been thinking that we need to have some kind of a controller
for a large language model.
But I guess I'm interested in, first of all, what are the differences between large language
models and something like a steam engine?
And also, with a steam engine, you might be optimizing the efficiency or the performance
or the speed or something like that.
What is it that we are kind of trying to make better with a large language model?
So first off, we'll talk about the differences between large language models and other types
of systems that you might want to control.
Typically a control system, you might first be introduced to control theory in the context
of, say, you're trying to control an engine or something else where the states can be
represented by a set of numbers or set of real numbers that is fixed size.
So perhaps we have an X and a Y coordinate where it's trying to control or a position
and a velocity.
These are common types of systems in scenarios that show up in control theory.
The difference with an LLM, the first major difference is that the token space, the state
space of the system is discreet.
Because we're dealing with tokens, we're dealing with words, we're not operating in
the space of real numbers anymore, and so this introduces some complications and complexities
when dealing with control theory.
The second thing that's really significant is that each time an LLM generates a token
or a user inputs a token, that state space actually expands.
It grows by one token.
And this is very interesting and unique for LLM systems.
On the one hand, this can be exploited to try and get the LLMs to engage in reasoning
or chain of thoughts or kind of take a winding path to the answer you actually want them
to outputs.
But of course, this makes it very difficult for control theory because each new token
you add, the space of possible sentences grows exponentially.
And in language models, the vocabulary size is on the order of 50,000 to 100,000, so
this grows extremely, extremely quickly.
These are some of the challenges.
And with a control theory of say engines, you're trying to optimize the efficiency,
it's a good question what you're trying to optimize for language models.
I think this is definitely a direction for future research.
Do you have any thoughts on this?
Yeah, I think the thing that we saw was that even very simple questions about how these
LLMs operate, you know, their input-output relationships.
When you start to treat them just as a system that, you know, maybe there's some imposed
input, like a system prompt, and then you get to pick a subset of those tokens, right?
When you start to treat it like that, and you just ask a really simple question, like
let's say that I want it to generate a specific string, you know, we're not going to be trying
to use it to do some intelligent information processing, I just want to see, can I make
it do something?
And what we found and what sort of motivated us to do this is that we really had no idea
when it would be possible or if it was generally possible to make it do anything we want.
Can we just make an LLM system generate any output we desire?
And if the answer is yes, which, you know, seems like it's probable, if you get to have
a lot of tokens in your input that you control, it seems reasonable that you'd be able to
probably get it to output a wide variety of, you know, at least reasonable English sentences
or linguistically valid sentences.
But the question that we had was, okay, if you have a finite budget for that, would you
be able to get it to do anything?
And what budget of tokens, like how many tokens do you have to be able to control if you want
to be able to make the system do whatever you want?
And that was the initial motivation where it was like, yeah, there are all these, you
know, high and mighty sort of questions of how do we make these systems do what we want
in a, you know, alignment sense, how do we make them do what we want in the sense of
cooperating towards some information processing objective.
But we realized that these really, really simple questions of just, okay, you have an
input that you get to partially control, you're trying to make it do something.
That question was completely unanswered, and we were sort of taking bets on it.
I think Cameron was the one who started to make bats, he was like, I bet like 10 bucks
that we can get this done, we can make it emit this output within, you know, five tokens.
And that was really the initial motivation where it was like, even the feed forward dynamics
of this system are really mysterious and getting a grip on those, it seems like that's a really
strong way to start building up a fundamental control theory and a really strong understanding
of these LLM systems where in control theory, at least when you start to really deeply understand
just a single system with its own dynamics and how the input-output relationships work,
what the reachable sets look like, how controllable it is, then when it comes to building more
complicated systems where maybe you have a more complicated objective, maybe you have
interacting systems, when you really understand the fundamentals, it makes that way easier.
And so the example in classical control theory is that you observe that if you couple a bunch
of linear controllers and linear systems together, what you get is just one bigger linear system
and all of the same stuff applies.
So what we were hoping is that by starting to answer this really simple question of just,
okay, how much can we control this?
What does the reachability of these LLMs look like?
We're really hoping to build that up and to me it feels like we're kind of doing our
homework where in engineering, we had to take all these classes in control and that was
sort of our homework to be able to, you know, go into the world and if it ever comes time
to build some electromechanical system and get a PID controller in there, now we've done
our homework so we can, you know, have a sense what to expect, how we could do engineering
on it.
So that's really where I feel like it's at and I think this is a really promising way
to try to get a really fundamental understanding of what's going on with these language model
systems.
Amazing.
So in a second, we're going to introduce this concept of reachability, but I've thought
about this because I've had a couple of days to reflect on this and my intuition, intuitions
just seem a little bit mixed up.
So I've interviewed Nicholas Carlini for example and he's done lots of work, you know, building
on adversarial examples and writing algorithms to find adversarial examples and we know that
neural networks are not robust.
You can quite easily perturb, let's say an input image in a vision model and if it's
a classifier, you can make it pretty much say anything with a very small perturbation
and that's kind of the same thing as what you mean as reachability.
It's this idea to kind of reach into the state space and make it do something quite
weird outside of what you would expect.
Now for some reason, I had the intuition and I now think I'm wrong that LLMs do, you
know, I didn't think they had this problem, but they do have this problem and you introduced
this really interesting, I guess it started out as a thought experiment and you coded
it into a game and it's the Roger Federer game.
I think that's quite instructive, so can you tell us about that?
Yeah, for sure.
So, one of the earliest examples that we were thinking about was just a simple example
of you have this state sequence that's imposed, you don't get to pick it.
It says Roger Federer is the and then the next thing that you want it to say, the thing
that you want the LLM to generate is the word the greatest.
So you want to say Roger Federer is the greatest and you're trying to pick out a prompt that
comes before then that will steer the system so that it'll output that.
So we're basically asking the question, you know, is this word in the reachable set of
outputs, given that we have some finite control over the input, where the goal of the game
is to for one, get it to actually output the right answer, which is the greatest, which
you know, is a fairly reasonable English, you know, thing to say.
And the metric that we use to grade how well you're doing on that is basically how efficiently
you're able to do control where, you know, in the original control theory, this idea
of efficient or optimal control is really important, you know, you have this linear quadratic
regularization idea where you're like, I have only a finite energy budget for the signal
I put in.
Similarly, with language models, what we're interested in is the minimal length of the
control input that will steer the model successfully to what you want it to do.
And it turns out that the game is actually very challenging, at least with this GPT2
model, which is the one that we're using right now, since it's just running out of a desktop
on my desk at home.
And so, yeah, there's this game that you can play, we can link it where you get to put
in a prompt to the system, and it'll come back to you and say, okay, you got the answer
right, or you got the answer wrong, as well as your, basically, your error on that, so
your cross entropy loss on getting the correct output, the desired output.
And the game is to basically get the shortest prompt that will steer the model to the desired
output.
And it's actually quite challenging with GPT2, where I think only four people, including
Cameron and then my friend Michael Zehlinger, who we had made this thing called FangCheck4,
which is this resume checker that uses language models to basically predict your probability
of getting into a Fang company.
I think those two were the only people who actually ended up getting it right, and it
turns out to be very difficult.
So that game was sort of a codified sort of interactive version of our initial motivations
for this, where it was like, wow, this really simple question that seems like there should
be an easy answer.
I mean, if there is an easy answer, I'd love to know.
But the simple question really leads to a problem that's quite difficult to solve, and
we really have poor insight on, and we're really just trying to get that insight together
to understand what's going on there.
And just to jump off that point as well, I think one of the reasons why this game in
particular is difficult is because we're using GPT2, and Roger Federer is the blank.
You would think greatest would be rated pretty high, but GPT2, I guess it's trained on lots
of fill-in-the-blank tasks, it tends to output just a set of underscores quite often.
To comment on your intuition you mentioned before on whether language models have this
adversarial property, one thing that was really interesting when we were doing some of our
initial work was this technique of soft prompting.
So soft prompting, instead of selecting discrete tokens, which we want to adversarial change
the model's behavior with, soft prompting modifies the embedding vectors directly.
So you have a lot more fine-grained control over the outputs, and it turns out when you
soft prompt, when you adversarily attack not the tokens themselves, but the embedding
vectors, you can send the cross entropy law straight to zero for whatever token you want
with a very tiny adjustment in these embedding vectors.
So this is very interesting, this points to the fact that the real challenge with controllability
is not necessarily that there aren't adversarial inputs for language models, but just it's
very hard to search this exponential space of discrete prompts.
Yeah, and so I guess there are many degrees of freedom in any deep learning model, it's
a very highly dimensional model, there are many degrees of freedom, and I'm trying to
understand my intuition, so it's trained with a softmax for example.
And certainly when you do temperature sampling, the likelihood is that you're only going to
get the top few tokens.
If you look at the distribution of the probability, it's almost certainly this one or this one,
and then it just tails off very, very quickly.
And I assume that inductive prior was quite deliberate really to increase the statistical
tractability of the model, but underneath that in the embedding space, it's not a shell
at all, even though there's some low level surface of embeddings and you can traverse
this.
Right, so initially you might think that this embedding space is a very rich representation
of the meaning of different words.
And certainly if you do word-to-vec or take a PCA analysis of the embedding vectors for
any large language model, you'll find something that roughly corresponds to the meaning.
I mean, words that mean similar things are attached more closely together, but this opens
the question, if you were to interpolate between two similar words, take the embedding vector
that is halfway between, would you get the halfway in between word, or would you get
something that's nonsense, right?
And I think what you find by these kinds of soft prompting experiments, by directly
manipulating the embedding vectors, is that the embedding space is actually extremely
non-convex in the sense that by interpolating, you don't just get an average value between
the two of them.
Yeah, one of the, I don't know if this is best to get into, but one of the techniques
we were trying to use is this technique called Gumball Softmax.
So instead of, it's a discrete search over the token space, one thing you can do is it's
kind of like the reparameterization trick for variation auto encoders, but it works
for a categorical distribution.
And so you can use this trick, and it essentially works by kind of interpolating between embeddings,
but it actually was very difficult to get to converge and did not even close to rival
the performance of GCG.
My intuition is that when you take a data point off the manifold, because these neural
networks, they do learn a manifold of language.
I thought if you take a data point off the manifold, it would cause some kind of mode
collapse.
It would just cause the network to become chaotic and go crazy.
But apparently that's not the case.
Can it recover?
It's almost like if you put a bunch of tokens in which are just really weird, and then you
just carry on, it's like the language model recovers.
It finds coherence again, and then it just carries on.
Yeah, I think it's honestly a really hard question to answer, where in different regimes
we notice different things, where if you choose this adversarial prompt so that basically
these prompt optimization algorithms all work in the same way where you're trying to maximize
the likelihood of some desired string, and then you're able to modify some input.
And so depending on how you choose that, you can do the optimization so that the model
will output some gibberish.
And it seems like depending on the model, depending on the sampling techniques, I've
seen it go both ways, where sometimes it'll recover after that, sometimes it'll start generating
reasonable coherent text, and other times it seems like it'll continue to generate
some random stuff.
It'll be in this outer distribution mode.
And I think that that's one of the reasons that I think that these adversarial examples,
studying them as well as this control theory stuff is really important, where it's like,
yeah, if you have a system in the real world where tokens are coming in, you're actually
processing them from real users, you don't have total control.
But the user is the one who's giving the control input.
You want to make sure that your system is sort of robust to that, where there's a lot
of really complicated interactions as it turns out between, for instance, the tokenizer and
the incoming strings, where when you do this prompt optimization, sometimes it'll come
out with a sequence of tokens that if you convert it to a string and then convert it
back to tokens, it'll actually be very different, which we ran into with this game where I was
like, oh, I'm going to cheat at this game.
I want to be the top prompter, so I'm just going to use some of the algorithms that we
had from our GitHub repository, the magic words GitHub repository, to basically optimize
these prompts.
But then when you convert it back to a string, then it turns out not to work as well.
And so, yeah, I think that answering that question and seeing when is it that the model
will actually be able to recover, is it a function of how big the model is, are bigger
models better at recovering, or is it the case that bigger models are maybe more controllable?
Maybe you can shift these models into this weird sort of, sorry, I just want the mic,
but this sort of out of distribution regime where they're generating this seemingly random
output based on seemingly random input.
And so, yeah, I think that that question is really, really important and is one that is,
I think, well addressed through considering them as systems, which is sort of the thesis
of this paper.
And we're trying to get a grip on what exactly the case is.
Is it going to be able to recover?
Is that a consistent behavior, or is it not?
Is this sort of weird recurrence relationship between the prompt and then the stuff that
the language model generates and then the stuff that's generated in the future, where
in effect, you know, you're able to pick a prompt and then the language model will generate
some more text.
But then that text becomes sort of part of the prompt as well.
So it seems like maybe there could be these sort of degenerate states where if you start
with this seed of chaos, it'll basically branch out and the future strings that it generates
is going to prompt it into being more and more chaotic.
And that's basically stability analysis or sensitivity analysis.
And there's all this rich vocabulary and all of these people who have spent basically hundreds
of years thinking about these concepts for both discrete and continuous dynamical systems
that we get to build on top of and basically use their insights to understand, you know,
what does it mean?
What does stability really mean?
We can just draw those definitions in, apply them to our generalized form of a system,
a language model system.
And I think that's why the control theoretic aspect is exciting, where you can actually
ask these questions in a very concrete and reasonable way.
And the best part is that people haven't really been using these, these ideas or using this
vocabulary to describe the questions that we're trying to answer.
And so most of these things, if you just spin up a, you know, a small GPU and test some
stuff out with a seven billion parameter model, you're actually doing new research.
And it's actually some useful research in my opinion, where you're getting a sense of
the control theoretic properties of language models.
To me, that felt like the most exciting thing here, the open questions are the most exciting
part of the paper to me, where we've taken a stab at basically the, you know, empirical
study of controllability by sampling these wiki tech sequences, seeing if we can control
the next token, the next few tokens, as well as some sort of theoretical results on self-attention
and its controllability.
But then all of these open questions emerged just because we're now framing it as a system,
and people for hundreds of years have been thinking really, really deeply about how you
understand systems when they're used in the real world, and you have this sort of finite
control over them.
Yeah, that's really interesting.
I mean, I suppose I'm pointing out the obvious here, but these are autoregressive models.
So the answer gets kind of fed back into the prompt, and then we rinse and repeat, which
means you can model them as dynamical systems.
And that is in stark contrast to something like a vision classifier where, you know, there's
just an input and an output, and that's it, that's the end.
So now you can get the system into this kind of corrupted state where, you know, you get
divergence and decoherence, and as you said, that could be analyzed with stability analysis.
But I find that fascinating.
But we should just go back quickly to your Roger Federer example.
So I'm interested in the different ways that we could go about this.
So the humans were kind of using language, and language are a bunch of memetically shared
cognitive tools, and they were saying things like, you know, you know, basketball was a
great and, you know, Joe Bloggs is great, Roger Federer is great.
And it wasn't very parsimonious, but it worked.
And then, you know, another approach that you spoke about is you could just make a Python
program and you can just let's try a neighborhood greedy search one token at a time.
So we find the nearest token, and then we find the second nearest token until we find
the adversarial attack.
Or we could do like a low level gradient search, and then we can find something really weird
and wonderful.
There might be some esoteric characters that just make it go bananas.
But these are three very, very different levels of talking to a language model.
The word on the street is that language models are a new form of programming that you can
just say what you want to do using English language and so on.
And language models certainly seem to incorporate that structure, but the language models themselves
are just an inscrutable, you know, set of neurons, right, and weights and matrices and
so on.
So there's some, there's a kind of higher resolution shog off going on underneath the
covers.
That's more or less the picture I have.
We have this interface where we can speak to the language model using language.
And if we set up a conversation with a language model where we have different labels, you
know, chat, GBT says this, Cameron says this, and you know, you engage in a conversation
because it is seen in enough conversations and it's training data, then it's able to
play along very fine.
What's going on under the hood, of course, like you say, it's very inscrutable.
It's very difficult to really probe and understand.
There are certain techniques in the interpretability literature, but I don't think as a whole,
it's we're even remotely close to having a complete understanding of how these systems
work.
And that's one of the reasons why I think that control theory is a great way to kind
of break in and see what's going on.
Because if you just look at the system's input and output characteristics, you can really
gain a lot of insight into the nature of these systems.
One guiding principle in my life doing engineering and trying to learn about the world has been
this quote by Richard Feynman, it's very popular, what I cannot create, I cannot understand.
And yet today we find ourselves in this situation with language models where we have these incredibly
complex systems we built and yet we can't really get into them.
So to extend this to today, what I would say is what I cannot control, I cannot understand.
The way I think about it is it's almost like you want the language model to be a high-level
controlled robust interface and it's almost like we're all Marvel characters and we can
give secret hidden codes.
It's like me now, imagine if I could just through telepathy control your behavior.
And anyone can do that with a language model, they can just put weird tokens in and they
can manipulate its behavior and there's nothing stopping you, there's no firewall.
I feel like this kind of harkens to why we call the paper what's the magic word, where
the initial reason was just that it's almost like the LLM is asking you if you wanted to
do something, what's the magic word, what's the key, this weird control prompt that will
just make it do the right thing.
But I think more generally, I used to be into magic when I was a kid, I had to jog at a
restaurant doing card tricks for the patrons while they waited for their food.
And what magic is is basically you're playing tricks on the human perceptual system where
there are all of these inductive biases that the human perceptual system has where for
instance if I move something and I look at it, you naturally will tend to follow my gaze
and what is moving is generally more salient and so then I can do something over here with
my other hand, like take something out of my pocket and then when I display it, they'll
be like, oh my God, where did that come from, right?
And what we're discovering I think is a sort of similar thing with language models where
for one, people have observed that if you use sort of human social engineering tricks
on them like, oh, I'll tip you $500, then it'll do a bit better.
But then there's this whole other sort of perceptual layer I guess you could call it
where there's this sort of chaotic regime of adversarial prompts kind of like hypnosis,
kind of like magic, where if you give it these very strange, very inhuman looking prompts
that will steer it to this to just making a certain output extremely likely, right?
And so to me, it feels really similar to digging into like magic and the human perceptual system
just with LLMs where we're learning about basically the shape or the what the nature
of these language models are in terms of how they interact with the world and how their
dynamics really work.
And I think that it's very sensible that the control theoretic perspective would be useful
for this where in classical control theory, trying to control these systems actually taught
us a lot about the nature of systems, both linear and nonlinear.
And I think that we have a very similar opportunity here where we're really discovering what is
the nature of these language models in terms of control where these questions don't emerge
quite as naturally and don't have quite as natural of an answer when you're just thinking
about them as a sort of probability distribution over text, thinking about them in terms of
being systems that have inputs and outputs and these trajectories and the like actually
really does change the kinds of questions that you end up being able to answer and the
kind of understanding that you get about the nature of the system itself, which to me is
one of the most exciting things.
So yeah, that's so interesting.
The magic example thing, I think we we think that we are robust, but we're not maybe we're
system two robust, but we're not system one robust.
And if you look in the animal kingdom, there are so many examples of, you know, like a hen,
if you make the right kind of clucking noise, the mother will think that you're the chick.
So it's really, really weird actually.
And Keith gave me this example of, I think it was from science fiction, that there's
a hypothetical image.
And if you look at the image, every single person goes into a coma.
And what's interesting about that is it's a kind of, you know, population level adversarial
example, rather than an individual adversarial example.
But then it gets into the question of, you know, how can we use this control theoretic
approach to robustify models?
Because we're talking about building a genetic LLMs.
And part of the thing I'm trying to get my head around is in this particular case, we
had a very clear kind of cost function, you know, a specific thing.
But what would it mean to robustify language models in the general?
So one of the things that came up in our, you know, sort of literature view was this
idea of, you know, when you're trying to control these discrete stochastic dynamical
systems, one concept that can be quite useful is you might have a set of outputs that you
want to reach or a set of outputs that you want to avoid.
So an avoid set and basically a desirable set, right?
And when you frame it like that, you know, I think that the robustification comes from
the fact that let's say that you have a set of outputs, you really don't want the language
model to emit, right?
You might think, okay, well, I'll just fine tune it so that it decreases the likelihood,
the prior likelihood basically of those sequences, right?
And the issue with that, I think, and the thing that the control theoretic perspective
sort of brings in is the fact that when you have finite, even a small control prompt,
some extra tokens that you get to inject, it turns out that even very, very unlikely
next tokens can be made to be the most likely next token just by inputting these new examples.
So even if you did hypothetically fine tune the model so that this avoid set was assigned
very low probability, it seems like if you don't incorporate some aspect of maybe stochastically
trying to search for these adversarial examples and sort of having this sort of mini max thing
where you have one system that's trying to elicit the output, one system that is trying
to fine tune the model to maybe make it less likely or optimize another part of the prompt
that is supposed to steer it away from these outputs.
Really the insight, I think, is that you really have to be careful to consider the fact that
you're giving the outside world some amount of control over the system, some amount of
control over the context, and planning around that is actually very non-trivial and is not
really well managed, I don't think, through the classical view of just cross entropy loss
and just treating it like a probability distribution.
Something else that fascinates me is the divergence between focusing on the model versus, you
know, complexifying the software which controls it.
So right now, for example, we have language models and there's this kind of base training
and then there's fine tuning and there's RLHF and there's like command variations of that,
for example.
And then we build these software APIs that are just trying to abstract away the complexity
so they will do dynamic prompt construction for multi-stop tool use and it goes on and
on and on.
There will be frameworks for doing agentic LLMs and there just seems to be like a bit
of a divergence here.
But the reason I'm asking the question is, does it make sense to robustify and fix the
problem in the model or does it make sense to almost increase the flexibility of the
model and fix it in the software layer?
I think one of the insights from our paper is that solely focusing on the model itself,
like Amon was just saying, as soon as you give the outside world control over the model
in the sense of being able to input whatever kind of text that they want, it becomes very
difficult to really prevent adversarial attacks and prevents jail breaks and that's why you
see jail breaks keep coming up.
I think if you were to involve some sort of robustness in a software layer, that might
be more feasible.
I can't immediately picture ways around it as, of course, if I was a hacker, I could
probably find some loophole.
There's usually some loophole you can find.
But if there was some way of fielding the prompt messages, for instance, a user gives
you a prompt, first you check, is this a reasonable thing that a human being would say in conversation
or is this something that I've never seen before in the entire history of the Internet?
The latter maybe is a prompt injection, maybe is something devious, or maybe is a computer
science research, but yeah, it's definitely not an easy problem.
But the good thing is that there are multiple approaches to it.
Very cool.
So we're going to go on to the more galaxy-brained stuff in a second.
So before we move off the paper, can you just talk more formally about what you showed in
the paper?
Yeah, definitely.
So there were two main parts of the paper, I guess three.
So for one, what we did was we tried to formalize what an LLM system really is at a mathematical
level.
And what we were trying to do at that was basically balance the fact that we really
wanted to try to take advantage of the original control theories, very abstract picture of
a system where you have this input space, you have a state space, an output space, and
there's some dynamics going on inside of it.
In our case, we parameterized those dynamics with an LLM, and our input space and our state
spaces were basically the set of all possible token sequences from the vocabulary set of
this model.
So that was the first part, and we basically transferred over a lot of the notions of basically
reachability and controllability for LLM systems from the original control theory, where you
can really just define it in terms of this really abstract notions of you have sets for
the reachable, sorry, the state space, the input space, and the output space, you have
some dynamics, and basically in terms of those sets, you can define reachability and control.
So that was the first part.
The next thing that we did was we tried to look inside the model.
So we were thinking, you know, it'd be really nice, like in control theory, if we could
have a really good understanding of the components of the system and how controllable those individual
pieces were.
So what we did is we looked at a single self-attention head and tried to really think about it through
a matrix algebraic perspective to really break down what the relationship is between, let's
say you have a subset of the tokens, you get to control a subset that's fixed, and you're
trying to get the output to be, you know, a certain value, the output representations
where all of these, in the case of a self-attention head, are just these vector representations
of tokens.
So what we found there was that it actually is possible to do some fairly, you know, simple
matrix algebra manipulations to decompose the output of a self-attention head into one
component that arises from the imposed input, and then another component that arises from
the control input, and assuming that those two are bound, then you can actually derive
that, well, there actually is this geometry that sort of looks like a bubble around the
default output.
So the output, if you didn't have any control input in, there's a sort of bubble of reachable
space that scales with the number of control input tokens that you're able to use.
And we thought that that was really exciting because, for one, I didn't really expect that
you'd be able to do proofs on these sort of, you know, very complicated, high-dimensional
machine learning or deep learning systems like a self-attention head, but it also gave
us some insight to say that, okay, we actually have this really concrete relationship between
the sort of number of control input tokens, the magnitudes that you're able to input into
the system, and the output reachable set that is at your disposal, basically.
And so that was the second part, and then the last part was some empirical experiments
where we said, okay, let's just sample a bunch of strings from Wikipedia, and we'll see,
okay, the strings were between 8 and 32 tokens, and those were basically our imposed state
sequences.
And we asked the question, well, can we get it to output the correct next token, the
real next Wikipedia token?
How many input tokens does it take, or control input tokens does it take for that to happen?
It turned out that you could get that done about 97% of the time to steer the model to
the correct output within 10 tokens of a control input, which is reasonable.
You know, we'd expect that the model should be able to be steered towards reasonable true
English sentences that were more than likely in the training data set.
What we did next was we tried to figure out, you know, if you sample the top 75 most likely
tokens according to the model, based on this fixed input, can you steer those things to
be the most likely token, basically the arg max of the probability distribution?
And what we found there is that it's about 89% of the time, at least 89% of the time,
we were able to find these optimal control inputs that were less than 10 tokens long
that would steer the model to do that.
And then the last thing we did was we said, okay, well, let's see what would happen if
we just randomly picked a token from the vocabulary.
So this is everything from regular English to numbers to Cyrillic characters to Chinese
characters.
What if we just randomly sampled those?
And we tried to see how many tokens it would take to steer that to being the arg max of
the probability distribution.
And what we found there is about 46% of the time we were able to make that next token,
the random one, the most likely next token using a prompt of length 10 or less.
And the sort of curves are there in our paper that describe as you have an increasing budget
for these tokens, how much of the time will we be able to basically steer it to the right
output?
Basically the K epsilon controllability metric that lets us get this sort of statistical
picture on controllability that renders it sort of practical to empirically estimate
for these complicated systems.
And so those are really the main results.
And the surprising thing about the last one that I mentioned before was that a lot of
times even really unlikely next tokens were able to be steered to be the most likely just
using a really short prompt, which both gets at the, you know, basically chaoticness or
complexity of language as a system, as well as the fact that the prior likelihood picture
or the, you know, cross entropy loss picture doesn't quite get at the controllability
sense of when you do have a, you know, ability to input tokens into the context, what happens
then?
So those are the really the main results.
And then, I mean, to me, the exciting, the really exciting part was the open questions
where I was like, oh, now that we're using this vocabulary, now that we formalize these
LLMs as systems, it's really easy to ask these, you know, additional questions about,
you know, the nature of the systems and the steerability controllability, especially with
feedback or chain of thought or, you know, agents or all of these other ideas.
And so yeah, that was basically the paper.
Yeah.
And it's really making me update my intuitions, right?
So I'm thinking about the bias variance trade off.
And I'm thinking that the reason we build these inductive priors is to constrain the
model intentionally to make it statistically tractable, to reduce the size of the hypothesis
class.
What you're saying is making me think that statistical tractability and flexibility are
not necessarily the same thing.
Now, it seems that the model must maintain a degree of flexibility.
I mean, it makes sense, right?
You have to be flexible in order to be a successful model.
But that creates a kind of adversarial attack.
So you can see the way I think about this is the model should be like the interstate
freeway of language.
So all of the major roads should be carved out and there should be side roads and so
on.
And that's the way I visualized the model.
But the model's not like that.
There's actually like all of these little slip roads.
And you can kind of push the cars off into the slip roads, but you need the slip roads
because perhaps you couldn't train the model without the slip roads.
Yeah.
I think that's a really good analogy.
I think that's thinking about pushing cars off the road into this space where they perhaps
aren't used to being and what happens next.
This is a case where the language model can enter some of these mode collapse type regimes
and you can get kind of weird outputs.
This is where you also, I mean, it was surprising that you can get the least likely token with
just a specific input to be the most likely next token.
But if we treat language as this kind of road or as this kind of map structure, then it
kind of makes sense that once you get off the map, once you enter this kind of regime
that is completely unexplored, which there are actually plenty of regimes like this.
Again, because the space is exponential in the number of tokens, it's growing so incredibly
fast that it's very easy to find pockets that the model has never seen before and maybe
no human on earth or never will be seen again.
You guys are really interested in collective intelligence and biomimetic intelligence and
biologically plausible intelligence and this is a matter very close to my heart.
What are you guys interested in specifically in that field?
Yeah, so I guess when I first got into machine learning, it was from watching this Google
DeepMind video where they were using reinforcement learning to teach this guy how to run, this
virtual reality avatar, how to run really fast and I thought that was fascinating because
it was like, okay, instead of traditional programming, you just have this neural network
that optimizes itself according to some objective.
The thing that was intriguing to me about that was the feed forward dynamics of a neural
network aren't that complicated.
You have these synapses, you have this sort of gated action potential function and the
thing that was weird to me was like, how does every neuron know how to change its weights?
How does each neuron that's independently not that smart know what to do?
That led me down the theoretical neuroscience route for some time where I was trying to
figure out, okay, what do these learning rules look like that don't have to use the chain
rule, use back propagation to update their weights?
So I did that for a while and then realized that the question of supervised learning was
not necessarily the most interesting question to be asked where it seems like the lion's
share of what makes us really interesting as humans in our cognition seems to be associated
with the cortex and this kind of predictive coding module that we have that lets us make
these really rich abstract representations of reality sort of understand what's going
on.
We sort of hallucinate this internal model of the world and so the interesting thing
to me about the cortex was that you have this structure that's pretty flat and pretty homogenous
throughout.
There's differences in different regions but at the end of the day it's very similar
and in fact if you lose a sense, like if you lose your vision, that region is often repurposed
for other things.
It seems like there should exist, you know, the brain is kind of this existence proof
that there should exist this rule set that if you apply it everywhere in this system
in this sort of layer on the outside of the brain then the behavior, the emergent property
of that system is that you'll get this really robust and rich sort of representation of
the world that is very predictive of subsequent sensory input, right?
And I think that the collective intelligence aspect of that is really, really important
where there's one way to go in machine learning where you say, okay, we're going to make this
monolithic pile of matrix algebra and we're going to train it through back propagation
and gradient descent and the atom optimizer and all of that and we're going to make it
do some prediction task but at the end of the day every computation has to be implemented
in physical reality, right?
And when we make the abstraction and just say, oh, it's just a bunch of math, we'll
just have a GPU run it, it kind of abstracts away from this fact that at the end of the
day you have real physical objects that need to do computation and share information and
in the sort of maximum efficiency, maximum scalability limit, it seems like what you'd
end up having is a very similar sort of distributed structure where you can't really easily separate
memory from computation.
I think there's a quote from this MIT professor that says that Turing's initial mistake was
saying that the head of the Turing machine was separate from the tape and I think that
that's true where in reality, in brains, in real computing systems, the matter that composes
the memory and the matter that composes the computation is really one and the same and
the brain is obviously this really great proof that, okay, there are relatively simple rules
that are implementable with these biological neurons that if you just implement them everywhere,
will get you this really beautiful convergence and emergent property of intelligence and
that really drove me for a long time in theoretical neuroscience and then more recently in trying
to build these distributed systems of artificial intelligences that the dream that I was trying
to pursue before we started this control theory thing was that, okay, well, what if
I just had a bunch of really small LLMs that everybody in the world could host and they
could communicate with this sort of low bandwidth communication using just tokens, just text
over the regular internet and the emergent property of that, what if it was possible
that we could engineer a system that the emergent property was that it would actually be this
really capable collective where maybe GPT-7 can be owned by everyone instead of just being
behind closed doors in a data center that we have now, we're sort of using these insane
engineering feats of NVIDIA interconnects and these really high bandwidth connections
between massive racks in a data center that take a ton of energy to get this really great
result of modern language models, what if we could have a system that was a bit more
like the brain, a bit more decentralized and really leverage this insight that it should
be possible, this existence proof keeps coming back to you where it's like, okay, it should
be possible, right?
And that is sort of originally what led me to the control theory stuff where it just
turned out to be really hard where we didn't have a great understanding of if we're treating
these LLMs as systems rather than just big piles of matrix algebra that we're trying
to distribute over many GPUs, if you treat them as systems that are coupled together,
they're interacting in this networked fashion, how do we really understand that, is it even
possible to prompt them to do the right thing, when is it possible, how long do the prompts
need to be, and that sort of led us down this route.
But yeah, definitely the collective intelligence thing was was a big motivation for me to get
this working.
And there's this neural cellular automata thing that I know you had talked with Michael
Levin, who was the last author on that, and he worked with Alexander Mordvinsev on it,
where it's this really, really great demonstration of how if you just optimize these basically
small MLPs with local interaction to try to satisfy some objective, like, you know, reforming
this gecko or lizard in their paper, then you actually can do that with back propagation
through time.
And so, you know, I thought, you know, it'd be really cool if we could try to engineer
information processing systems that did this, not just morphogenesis systems, but information
processing systems that operate in this way, because, you know, as a graduate of engineering
science, we had to take a bunch of these digital logic courses.
And when you have this very simple, you basically local state machine that has basically local
connectivity, it's really easy to imagine how it implement that as a custom chip and
sort of reach this, you know, as Beth Jesus puts it, you know, thermodynamic limit of
AI.
And so that really excited me.
And so I built a sort of demo of that where I was trying to do visual information processing
on this really sparsified video is basically trying to do predictive coding of sorts on
or active inference, I guess, on this incoming data stream of really sparsified video trying
to predict what would happen next.
And it turned out to work quite well.
And so then I was like, well, why can't we do that with language models?
You know, as you mentioned, there are all these slip roads, right, where if you prompt
it just right, you can enter this really weird different regime.
And this exponentially large prompt space is a really handy way to try to control them
where, you know, fine tuning is great, but what if we could just prompt them into interacting
in a way that would lead to this emergent property of just being basically one larger
language model that could predict the next token really, really well.
And so that initial motivation sort of led to this control theory stuff.
And I think that it is probably the right way to go for the field where if we want to be
able to really leverage maximal computation towards our objectives, you know, the bitter
lesson by Richard Sutton kind of suggests that we should probably aim for systems where
you can just slap on more and more compute, you can have a relatively simple procedure
that you follow to leverage more compute towards your objectives.
That's probably the way to go for making advances in AI.
And if we can have this decentralized, you know, networked system that, you know, I took
this distributed systems course while I was here that was really great and sort of taught
how to make, you know, basically databases that were distributed over many servers that
would have this, you know, the emergent property they wanted was robustness, consistency and
availability.
If we could have something similar to that that is radically scalable and is able to
be, you know, just run by regular people who don't need to own their own, you know, GPU
cluster that's maybe illegal in the future when the US government is like, oh, you can
only have this many petaflops.
Basically, yeah, that was the real motivation for, for the, what I called the language game,
that project.
And that's something that we're continuing to work on.
But yeah, that kind of led to this control theory thing where we were just like, yeah,
we really need to get a grip on what these look like as systems as we start to build
these more and more complicated, you know, network distributed, you know, beautiful emergent
systems that hopefully will be able to be hypercapable in the future.
Yeah, this is all music to my ears.
I'm a huge fan of the externalist thought in cognitive science.
And even though I love the work from Jeff Hawkins, you were talking about the neocortex.
But even then, you know, I would kind of say that it's a lot of the cognition happens outside
of the brain, you know, we're not islands.
And actually, I was just thinking maybe a better analogy rather than the interstate
freeway might be, you know, in Star Trek Voyager, there was the wormhole network and
the Borg fan, the secret network.
And you could kind of like, you know, get into these little slipstreams and go to different
parts of the universe.
But when I was interviewing Philip Ball, he wrote this book, How Life Works, and he was
trying to understand, you know, what are the mechanisms like, you know, self-organization
and multi-scale information sharing and, you know, emergentism.
And it's, it's really, really fascinating.
So how can we introduce some of these concepts into the next generation of AI?
Yeah, this is one of the things I'm certainly most excited about, because I see life as
this kind of interconnected, interplay, multi-scale process of exploitation and exploration.
And these are two terms from the reinforcement learning literature.
But I mean this in a much more general sense, because at each stage of life, we're either
going out into the world to get something, to do something, to try something new.
And then at the next stage, we're coming back in, going home, you know, reflecting, going
over our insights.
And it's, it's this process, this ebb and flow, going out, coming back in.
And I see this kind of pattern emerge across many different aspects of machine learning
and artificial intelligence work in the sense that a lot of our algorithms that we have
now are convergent, they're objective driven.
We establish a loss function, we say, these are the rules, it should follow, it's going
to update according to this equation.
And we set the system running, learn some data, and we have a final product.
And on the flip side, there's, you know, like what Ken Stanley works with, more exploratory
evolutionary algorithms or open-ended algorithms.
And this is, this is the other side of things.
And I think some of the most interesting work to be done is how these two sides interconnect.
How can we lay down rules, strict rigid rules, which when they're followed can generate novelty,
can generate creativity, can generate organization in a way which is not predetermined, but almost
fractal and infinite in its complexity.
And are those rules defined already?
Do they exist in the world?
Are we guided by them?
Are there principles like that which exist that we can come to?
Or is it, you know, are we kind of, you know, the authors of our own fates in a sense?
Are we each agents in actions?
We get to choose our path in life.
I think these, these are the directions I'm really interested in.
And to connect this to my research, one thing I'm focused on now for my thesis project is
looking at morphogenesis.
So this connects to the Mordvins of paper as well, except what I'm really interested
in is how does structure emerge?
How do different cells actually connect together?
So in that paper, for instance, each of the cells were on a fixed grid, but in our bodies,
there's actually quite a sophisticated protein expression network which governs how cells
adhere together.
Certain gene regulation pathways will turn on catherins, which will cause cells to attach
together.
And then in other parts, these cells can unattach and then be transported all around the embryo.
And I think understanding this process more deeply, not only could shed light on structure
formation and, you know, problems in biology in general, but maybe more deeper general
problems of structure learning.
Because we might think of embryology as quite disconnected from machine intelligence or
artificial intelligence, but every single brain is formed in the same way.
And that's through developments.
Yeah.
I'm also a disciple of Kenneth Stanley.
He's absolutely incredible.
Everyone at home needs to read his book, Why Greatness Cannot Be Planned.
Yeah.
You know, so in the natural world, we have, it's so interesting, so we have this kind
of self-organization and then we have multi-scale information sharing.
But we also have cannalization, which is that you actually see a kind of convergence of
structure and forms, you know, which is reused, you know, almost as modules in the system.
But then there's always the question of, how do we create something like this?
Because is it simply a matter of complexity?
Do you need to have a microscopic scale to reproduce this or could we reproduce it?
And then if we did reproduce it, the catch-22 situation is that, you know, when you impute
directedness onto a system, it loses its intelligence because to me, intelligence is divergence.
Exactly as you were saying, it's this tapestry of discovering problems, solutions, new problems,
solutions, and it goes on and there's no end.
It goes on forever.
And any attempt by us to control it with, I mean, it's a bit like the bitter lesson,
you know, Sutton said, any human design, any attempt to steer it makes it convergent.
But then we could do something like The Game of Life from John Conway and incredible beautiful
structure emerges from that.
Whenever we try to steer it with our own will, it seems to corrupt it as well.
Yeah, I think that the analogy to biology is really useful here.
And the canonization that you mentioned, you know, you have this reuse of structures across,
you know, cells, for instance, they all have this similar machinery to do gene expression.
And they have the same genetic code underlying that gene expression with, you know, maybe
differences in cell state.
But at the end of the day, it's the same machinery, right?
And, you know, I used to do a bit of protein engineering with language models.
And that's actually how I learned about transformers and built my first transformers.
And I think that the analogy is really strong where, you know, cells sort of know how to
read this genetic code, this language of the genetic code.
And they all use that ability, this canalized ability that's distributed across all of them
to locally, they solve this problem of, OK, what is this specific cell supposed to do?
What should it do to basically support the overall function of the organism?
Right.
And similarly, I think the hope with these language models is that now we have these
language based models or LLMs that have this similar sort of understanding of language.
They are able to really constrain the probability distribution, understand which sequences of
text are reasonable English and, you know, what they might want to generate.
And the exciting thing to me is that we can kind of do a similar sort of evolutionary
search that we used to do with or that we currently do with trying to find protein
sequences when we're doing protein engineering with language models, where every computer
in this network of systems has this canalized ability to understand language, if you will,
and is locally, it just needs to solve this problem of what should this particular node
do to support the function of the system?
And that might be to explore, that might be to exploit, that might be to do any sort of
any number of things.
And the discovery of that, I think, is really helped by the fact that we do have strong
language models that are able to really predict English or text very well, because they're
able to explore this space.
And basically, in the limit, you know, there's this good regulator theorem that we had talked
about before that says that any system that is a X that does optimal control over another
system must necessarily model that system.
And so if you think about in the limit, it seems like the best prompt optimizers may end up
being language models.
And already in our study, we were using this GCG algorithm that leverages a language model
to compute these gradients and try to figure out how we should do this local stochastic
search over prompts.
And so what I basically I'm trying to get at is that there are actually a lot of really
interesting similarities, I think, that can be drawn upon from what we know about the
structure and the function of biological systems where, you know, if we could crack
this problem of there's this local control objective or maybe information processing
objective that must be met by every cell, right?
Every compute node in this network of language models, if we could understand what that is,
what that even means from the perspective of systems and control and computation and
like, I think that that's a really promising way that we can make progress on this dream
of, like, to me, it seems like it would be great to have GPT-7 not just owned by one
entity, but maybe operated by the world, where we could all have a say in what goes
into it and how it's used and what it should be, you know, doing and can all benefit
from its excellent ability to compute and predict what will happen next and basically
perform intelligent, you know, operations on data.
So, yeah, I think this is a really, really exciting area to be working on.
Amazing. We're nearly at time, but we'll do two quick five questions.
So you've both just started the Society for the Pursuit of AGI.
Yes. Can you tell us about that?
Absolutely.
So the Society for the Pursuit of AGI is a student organization.
Currently, we're operating at the University of Toronto and at Caltech.
And we're essentially a crucible for new ideas.
If you think of university research labs as pursuing relatively safe bets that could
be publishable, industry research labs, relatively safe bets that maybe might turn
a profit one day in some new product or system, the Society is for the Hail Marys,
for the wild bets, for the crazy stuff, for the real innovative stuff that's way
outside the, you know, to use the analogy of the highway network.
We're trying to go off the beaten path.
And we really believe that the bottleneck in AI progress right now is not so much
compute, not so much algorithms, but it's conceptual.
We need better ideas about intelligence, about life, about what this whole thing is
that we're all experiencing and how we can gain deeper insights of it.
Not only do I think that a deeper
understanding will help us to create better systems, but it'll also give us
confidence that the systems we're developing will be beneficial to humanity and not
harmful. And I think that will only come with knowledge, with first principles,
understanding.
And so that's why one of the things we're trying to do is have our club very
interdisciplinary. I think having machine learning be some of this kind of echo
chamber amongst engineers, computer scientists, maybe a dash of philosophy and
neuroscience, it'd be really nice to open the conversation to people in other fields
who maybe have a really unique insights into the phenomenon of intelligence.
Perhaps behavioral economics can offer some insights.
Political science, right?
These are fields that are currently underappreciated, but may have useful ideas.
And maybe even people in the arts who, you know,
create maybe they don't design systems as much as they re-represent things that we
know and understand, they could have an interesting voice as well.
Very cool. And final question.
First of all, I just wanted to say to both of you, thank you for doing this great
work. So your paper is one of the most interesting that I've seen in the LLM space
in recent history, and it was shared and loved by many of the folks on our
Discord server. But that does bring me to another point, which is that
you didn't get into ICLR.
And from my perspective, I'm I'm shocked because this is really, really interesting.
It has great utility from a practical and a theoretical perspective.
Feel free to have a good bitch about reviewer number two.
No, I mean, I wish you just keep talking like that.
It really soothes the burn of reviewer number two, you know?
But no, I think that,
yeah, the review system, to be honest, I'm still trying to get my head around it.
I'm sort of an early career researcher trying to learn how it works.
I mean, definitely the review process for ICLR, in their defense, you know,
we had this bug with the submission, submission of our rebuttals, basically.
So we had submitted the revision to our paper and then 15 minutes before the deadline,
Cameron and I were both getting this timed out error.
He was in Toronto.
I was in California.
And so, you know, they didn't end up actually reading our rebuttals because we
had sent it in and they were like, oh, we'll post it for you.
And then they were like, oh, it was posted late, so can't read that.
So, yeah, I think that the review process definitely has given us a lot of, you know,
really useful insights where, you know, the second two results, actually,
that we talked about, the top 75 controllability and the random controllability.
Both of those were like from trying to address these reviewer comments, right?
So I think that what I'm trying to do at least is take as much of the good parts
of that, you know, trying to figure out how we can take advantage of this process
where we actually get insight from people in the field, what they're looking for,
what they think is interesting, what they think would improve the work and try to
try to use that.
And overall, just trying to figure out how to navigate this peer review system.
I think it definitely made it feel better as well that the Mamba paper was also
rejected from ICLR, which, you know, sorry.
I know.
Yeah.
Yeah, it's crazy.
Yeah, it was crazy to me as well.
But yeah, definitely it's a challenge.
And, you know, after staying up for 40 hours to get this done, it was like, oh,
would it be nice if they could have looked at our paper at least, you know,
just seeing the work that we did?
But yeah, it's definitely good to learn from these things.
And I guess we've learned the lesson as well, not to submit in the last 15 minutes
and to, you know, do it in advance.
But yeah, thank you so much for your kind words about the paper.
That means a lot.
And yeah, we'll surely continue to make this better and a lot of exciting plans
for how we're going to continue to try to, you know, merge together
these two, you know, empirical and theoretical sides of the equation
to make some really, hopefully impactful work that can really help people
build systems and, you know, make better systems and not be suffering
so much under the load of prompt engineering.
So yeah, thank you very much.
Amazing.
Well, guys, it's been a pleasure and an honor to have you on the show.
So just keep doing the great work and hopefully we'll get you on again.
Yeah, thank you so much for the opportunity to come and talk.
It's it's been an amazing opportunity.
It's it's really unbelievable to be sitting here in front of these cameras
after watching the show so many times, listening to so many of the podcasts.
And now to be speaking, it's just unbelievable.
So thank you.
Amazing. Thanks so much, guys.
OK, it's a wrap.

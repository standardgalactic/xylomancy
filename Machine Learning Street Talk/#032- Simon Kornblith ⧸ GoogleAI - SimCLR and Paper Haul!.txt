Welcome back. Today we're talking with Dr Simon Cornblith, a research scientist in the Google
Brain team. Simon is most famous for being one of the authors on Simclear, the computer
vision paper that used self-supervised learning and the contrastive loss with loads of cool
image augmentations. Simon also used to be a neuroscientist.
When I was pretty young, I was interested in consciousness and how we create this kind
of impression of the external world inside our heads. And so I guess it's pretty obvious
how that translates into an interest in brains and how the brain works.
Turns out the neuroscience is really difficult. Progress is really slow and tedious. Simon's
goal is to understand the inner workings of neural networks, both in meat space and
in silicon. He initially thought that the artificial variety might be easier to understand.
He was in for a rude awakening.
So in a neural network, we can record all the neurons, which is extremely challenging
in a biological organism. And we can also manipulate the system in any kind of way that
we can imagine. But it still seems really hard to understand neural networks. I think there
are a lot of ideas from machine learning that will ultimately help us understand brains.
Maybe we could make some headway that might eventually translate back to brains. And so
that's how I ended up in machine learning.
People often try and anthropomorphise neural networks.
People try to relate whatever neural network they've built back to a brain and they say
that it works like the brain, but it doesn't work like the brain.
So Simon was involved in this paper, do wide and deep networks learn the same things, uncovering
how neural network representations vary with width and depth.
Simon pioneered this really fascinating way of comparing representations by comparing
features. And what this essentially amounts to is we need to have a similarity function
so that we can compare the representations in layers to themselves in different parts
of the network or indeed to other networks.
And so for this similarity measure to work well, the first thing Simon did was take two
architecturally identical networks, A and B, trained from different random initialisations
and just ensure that the third convolution layer is more self-similar to its counterpart
than any of the other layers.
If that works, then you're onto something. Turns out that's not super simple to do, but
Simon came up with this concept called the centred kernel alignment, which we'll talk
about on the call.
But this is actually super fascinating. We're talking about this idea here of using self-similarity
to reason about the evolution of representations throughout successive layers in the neural
network. And what Simon found is that you get this kind of characteristic blockiness.
So when you see these large blocks, what it means is that the representations are no longer
evolving in respect of time.
So it's showing here the representational similarity of all of the layers against themselves
and against all of the other layers.
So clearly there's this characteristic diagonal down the matrix, as you would see with any
self-similarity matrix.
And because this blockiness appears, it means that nothing is happening.
And what Simon realised is you can actually delete these layers from the neural network
and it wouldn't make any difference because it hasn't learned anything new.
But it's also a really interesting way of reasoning about a kind of pathology, a weird
thing that happens when you saturate a neural network.
So he said that this presence of this block structure is an indicator of the halting of
evolution and a strong indicator of over-parameterisation.
And he actually shows that this blockiness appears on deeper networks and wider networks.
But this concept of self-similarity analysis is not new to me.
On my PhD I was fascinated in segmenting DJ-mixed music shows and I actually used the same techniques
for learning regimes in financial datasets later on.
But this is an example of a DJ-mix which I segmented.
I came up with a dynamic programming algorithm which would essentially sum up all of the
tiles along this diagonal and compute a lowest cost contiguous segmentation.
And it's super interesting.
So here are two music tracks and you can see that they are more self-similar to each other
than they are any of the other tracks just because of the tone of the colour here.
And if you zoom into a track you can even see that there are symmetries.
This part of the track here is a repetition from this part of the track here and you can
tell that from this kind of symmetry pattern on the diagonal.
And you can see that there's a little bit in the track in the middle here which is not
similar to any other part of the track.
You see some really interesting stuff here and you know essentially I'm a huge fan of
anyone using self-similarity matrices for reasoning about the evolution of representations.
I think it's a fascinating idea.
So how did Simon come up with this measure of similarity?
The centred kernel alignment.
Jeff Hinton had another idea and I tried the idea that it worked but then we wondered is
there a simpler thing that worked and that's how we ended up with centred kernel alignment.
The blockiness in these matrices is absolutely fascinating but how much can we read into it?
It's not clear what we should really expect in terms of how a neural network representation
evolves through the layers.
I think there's kind of some theory on what we should expect if all the layers are linear
but like obviously the neural networks that we train are nonlinear and it's really important
to have a nonlinearity in between the layers.
If we see that nothing is changing from one layer to the next that's a really bad sign.
If the neural network representation isn't changing then obviously nothing's happening.
We couldn't have predicted this ahead of time based on what we know about neural network
theory and we couldn't have predicted it ahead of time based on the accuracy of the network.
Does this apply to ResNets though?
I thought that they could learn their own capacity.
You can either look at networks without residual connections where you do actually find that
at some depth the accuracy will start going down and in networks without residual connections
we find that the depth where accuracy starts to go down is around the same depth where
you begin seeing this kind of block structure where many successive layers have similar
representations and it looks like the representation is no longer getting refined through the network.
Once you start getting these blocks making the network deeper, making the network wider
no longer really gives you any improvement in accuracy.
So it seems like this is basically telling you that the network has fit the data as much
as it can and there's no real advantage to using something bigger.
Next we move on to Simon's paper about using different loss functions on image classifiers
and he made some really interesting findings actually so the loss functions only really
seem to affect the penultimate layers in the neural network.
This also gives us some pretty useful insight into transfer learning.
The last third of the network is setting up the penultimate layer representation in a
way that is good for your loss function but the first two thirds of the network are somehow
just learning general features.
I think this also corresponds with the success of transfer learning where we can take features
that we've learned on one task and transfer them to some other task.
What's the implication though?
Is the implication that the loss function is not having any impact on the representations
early on in the network?
That seems like quite a big implication.
Ultimately, we're asking the network to do the same thing just in a slightly different
way.
Like some inverse correlation between the gains you get from a loss function and how
good it is for transfer learning.
If you use loss functions that give you higher accuracy on ImageNet, you tend to learn representations
that transfer substantially worse in that setting.
The loss functions that perform better lead classes to become more separated in the penultimate
layers.
If you use standard softmax loss, actually the classes are not that separated from each
other in the penultimate layer representation.
Right now on whatever TensorFlow Hub or Hugging Face repositories and so on, we have these
pre-trend models and the pre-trend models, they're like full stack models and people
usually take some sort of last or next to last hidden layer but maybe we should much
more focus on actually providing like half of a network to share, like determining which
are actually the best good or general representations from a data set and so on.
It's a really interesting question.
If we just want to turn an image into a vector that we could then train a linear classifier
on top of, what is the best way of doing that?
Self-supervised pre-training, just like word vectors, gives us a really great starting
point to vectorize an image into a semantically relevant geometric space.
It's been a real game changer in the computer vision world since about 2018.
We want that neural network to learn a representation such that when we then just train a linear
classifier on top of that representation to classify image net, it's going to do well.
But we want to learn the initial representation without using any kind of labels.
So what is self-supervised pre-training for vision?
All came up with these kinds of tasks that you could try to train a neural network to
do so that it would learn some kind of good representation.
You're trying to learn some kind of representation space where you've got different patches from
an image or different augmentations from an image, just different representations of the
same image and you want to learn a representation space where these representations of the same
image are all close together in that representation space and they're far apart from the representation
of other images.
This surprisingly seems to lead to very good representations.
I was very fascinated by all of these different tricks that you apparently have to get and
so big kudos to, you know, figuring all of this out for the rest of us.
Data augmentation is absolutely key to making this work.
The important part of the recipe is data augmentation.
There are really only two super important data augmentations that we need.
So we have to take two different crops from the same image and then we have to do some
kind of color distortion.
Turns out, though, the architecture isn't that important.
You don't have to worry about architecture, engineering specifically or contrastive learning.
What was new in the SIM CLR paper?
We introduced the idea of this projection head in SIM Clear and we also spend a lot
of time studying the augmentation.
And what about the bring your own latent paper?
I don't really have any insight into how either BYOL or the more recent papers actually
are learning a representation that doesn't end up collapsing.
Why it doesn't happen relates to some mysteries about neural network training dynamics that
we still don't entirely understand.
We dive deep into data augmentation in general.
The data augmentation that you need for contrastive learning is different from the data augmentation
that you need for supervised learning because the task is different.
When you have contrastive learning, you have this problem that if there's just one feature
in your data that can be used to do the contrastive task, to get images of the same example or
views of the same example close together and far apart from views of all the other examples.
If you could do that with one feature, that would be the only feature that network would
ever learn or it might be the only feature that network would ever learn.
And so with the augmentation, you're making the task harder so that the network actually
has to learn many different kinds of features.
We find that this color distortion actually is very important for self-supervised learning,
for contrastive learning, or as it doesn't really matter for supervised learning.
There seems to be this fascinating universality of representations, especially in vision.
I'm not trying to be flippant when I say this because practitioners have used ImageNet
on a variety of downstream tasks.
For example, they might use it for classifying circuit boards or something.
And the miraculous thing is it just seems to work quite well.
So do you think in your opinion that there is some kind of universality?
I'm very skeptical about universality of ImageNet for different tasks.
Even though there are lots of cars in ImageNet, if you pre-train on ImageNet and you fine
tune on that data set, you will learn to classify it faster and fewer steps than if you had
trained from scratch on the Stanford cars data set.
But you won't actually perform any better at the end.
Representations of images are not that universal, and at least what works for natural images
like those in ImageNet may not work on other data sets.
Coming a bit from the universality of representations to the universality of augmentations, since
this is such a crucial part.
Do you think that there is a systematic way how we can discover augmentations?
Right now it seems to be kind of a whack-a-mole, right?
It's okay, we just feed images and it's no, that's too easy.
We crop them.
Oh no, it's the color histogram.
So we like whack on the color and then it works better.
But maybe someone finds out, oh, there is still this easy feature that the network every
now and then pays attention to, so we design a method to whack on that a bit.
Do you think there is a systematic way or will this kind of philosophically always rely
on us humans having a higher level inside of what we want to do with the data set?
So what comes next after data augmentation?
So would the next step be some simulation?
Do you know what I mean?
Where we impute physics and we impute some world knowledge and then, I don't know whether
we train a machine learning model from that?
Yeah, I think there are definitely shortcomings in our current machine learning models, understandings
of the world.
There are probably things that we can't just solve by throwing more static images at them.
I think maybe the next step, rather than trying to immediately situate the machine learning
model in a simulated world, we could just think about video.
I think probably representation learning from video is going to be a big thing next year
or the year after, something sometime in the near future.
Finally, we talk about Simon's paper.
Big self-supervised models are strong semi-supervised learners.
What is a practical problem is the situation where you have a lot of unlabeled data and
then a very small amount of labeled data.
What I find fascinating is how many ideas come together in this paper.
You probably didn't sit down after SimClearOne and be like, all right, what do we do for
SimClearOne?
Okay, let's do this.
So it tells me there was this process.
Could you, if you can, maybe elaborate a bit on how did you go to build up the system
towards the final output?
We also tried the approach of first fine-tuning the big network and then distilling it.
It turned out that worked a lot better.
What we found was this approach of pre-training, then fine-tuning, then distilling works a
lot better than pre-training, then distilling, then fine-tuning.
We probably shouldn't expect distillation of the kind that we do in SimClear v2 to work
substantially better than supervised distillation, which has been around for quite a while now.
I think what's impressive is that in the self-supervised case, in the contrastive case, distillation
basically allows you to recover the same accuracy that you would get from training supervised
from scratch, whereas without it, the accuracy is a lot worse.
So it seems like it maybe matters more in this contrastive case.
But I think generally when you do distillation in the supervised case, you can get maybe
a percentage point gain, maybe a couple of percentage points.
And I think that's probably about the limit in terms of the improvement that you could
get from any kind of distillation-based approach over supervised training from scratch.
Can you use GANs, Generative Adversarial Neural Networks, to do data augmentation, or is that
just a myth?
Simon certainly seems to think so.
Using a GAN to do data augmentation, you have this problem, but you still don't actually
have more data.
You have a GAN that's trained on the same data.
And so it might help you because your way of encoding inductive bias into the GAN is
different from your way of encoding inductive bias into the neural network.
And maybe by having more inductive bias, you can learn a better function.
You still don't have more data, and it seems like without having more data, there's no
reason to expect a priority that you will be able to learn a better function.
Ironically, when you do the simple data augmentations, you do have more data because you put all the
knowledge in there as a human of what makes two images dissimilar visually, but still
equivalent semantically, which, again, is exactly the opposite.
It gives you images that are visually similar, but it has no intuition of what the semantic
similarity is.
We round off the show by talking about Simon's love of the Julia language.
Julia is a much better programming language than Python in many ways.
Julia is designed for these situations where maybe beyond just matrices, you have these
funny types of structured matrices, you have sparse matrices, and you can define special
methods for the product of a sparse matrix in a vector, or all sorts of things where
you might want different methods depending on the types.
I really hope you've enjoyed the show today.
We've had so much fun making it.
Remember to like, comment, and subscribe.
We love reading your comments, every single one of them, and we'll see you back next week.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my two
compadre, Syac the neural network pruner, Paul, and Yannick, the Lightspeed protein folder
Kiltcher.
Today we have an incredibly special guest, Simon Cornblith, and Simon got his PhD in
brain and cognitive sciences from MIT.
His undergrad was from Caltech, and he's a research scientist at Google Brain.
He's been there since about 2017.
He's been cited nearly 2,000 times, which for someone quite early in career is seriously
impressive.
He's got a keen interest in the digital humanities, in philosophy, computer science, machine learning,
computer vision, and neuroscience.
He used to be a neuroscientist before he started doing machine learning, and he tells us that
he's got some very strong opinions about neuroscience and machine learning, which we certainly will
be getting on to later.
He's a huge lover of the Julia language, so if you Google Simon's name, you'll see him
talking at about a million Julia conferences, so definitely check that out as well.
Simon pioneered the use of centered kernel alignment as a way of analyzing the evolution
of representations in layers, in network, and between networks of different architectures.
Now Simon, like me, is a lover of similarity matrices, and what can be gleaned from them?
On my own PhD, I worked with them a lot for music segmentation, and also for detecting
regimes in financial datasets.
When a block in a ResNet is no longer self-similar to previous layers early on, you might intuit
that it's moving into a new representational regime, or maybe it's just started hallucinating.
All of this stuff was covered in his paper, Do Wide and Deep Neural Networks Learn the
Same Things, and I find it fascinating that representation of self-similarity can reveal
network pathology.
Now in his paper, What's in a Loss Function for Image Classification, he noted that different
losses and regularizers have similar accuracies on several datasets, but using the same representational
evolution analysis, Simon gleaned that these losses and regularizers only affected the
penultimate layers in the neural network, revealing inherent limitations in what can
be achieved in manipulating the loss on a network.
Now next in the session today, we're going to talk about the Simclear paper, and this
was an incredibly exciting paper for unsupervised contrastive image learning with augmentations.
It introduced a learnable nonlinear transformation between the representations and the contrastive
loss, which massively improved the representations.
The composition of augmentations is super important, and whenever anyone asks me about
what are the different data augmentations in computer vision, I always point them to
the SimCLR paper because it's got this wonderful matrix, and in that matrix it was shown that
the crop and the color I think were the most effective augmentations, but Simon also noted
that the batch sizes were super important, and the paper improved over the state of the
art on the ImageNet top one, and it actually matched unsupervised methods for the first
time, albeit with many more parameters, but the final paper we're going to talk about
today is Big Self-Supervised Models are Strong Semisupervised Learners, and this is where
you can learn from fewer labeled examples while making use of a large amount of unlabeled
data, and with unsupervised pre-training on SimCLR V2, supervised fine-tuning on a few
labeled examples, and then distillation with unlabeled examples, this approach improved
the label efficiency over previous state-of-the-art methods, and I remember Yannick Lightspeed
Kilcher made a video on this one, which I watched a few months ago, so Yannick will
have all of that completely fresh in his mind.
Anyway, Simon, it's an absolute pleasure to welcome you to the show.
Thank you so much for coming.
It's great to be here.
Amazing.
How did you get into machine learning?
So I guess first I got into neuroscience, and then I got disillusioned with neuroscience.
When I was pretty young, I was interested in consciousness and how we create this kind
of impression of the external world inside our heads.
And so I guess it's pretty obvious how that translates into an interest in brains and
how the brain works.
So I spent both four years as an undergraduate doing neuroscience research, and then seven
years working with monkeys at MIT, trying to figure out how monkey brains work.
And then after that, I felt like we weren't getting very far by trying to record from
neurons in monkeys' brains and figure out how those neurons work.
So I thought about what other ways are there approaching this problem?
How could we think about how to understand how the brain is doing tasks?
And it seemed like maybe by building systems that can do those tasks well that are not
biological, we could learn more.
So that's how I got into machine learning.
I joined the Google AI residency program, which is like this great program that Google
has to take people who have extensive background in some field that is not machine learning
and train them to do machine learning.
And I ended up at Google and initially I thought I'm going to spend like a year here
learning about machine learning related stuff, and then maybe I'll go back to neuroscience
and I'll decide the tools for machine learning could be applied back to brains.
And maybe we can learn more about brains by applying the tools of machine learning there.
But ultimately, I decided I was more interested in just looking at how the neural
networks work and also like in the engineering challenges of building better neural networks,
which I actually think are fun.
One of the thoughts that came to my mind is it's fascinating looking at the kind of
introspective analysis that you've been conducting with neural networks.
But could you contrast that with neuroscience?
Because as I understand, you have MRI scans and you have different ways of trying to visualize
and reason about the behavior of a brain.
But you can't really tweak the architecture and tweak all of the knobs and the levers in
quite the same way you do in machine learning.
Yeah. So like in neuroscience, people also use this analysis across different individuals
or different organisms or whatever.
It is a tool that people use in neuroscience as well.
But I guess they're limited in the ways in which they could manipulate the systems that
are providing these representations.
So like in neuroscience, you're like always constrained by data.
So you can compare representations of images across individuals by doing MRI scans.
But first of all, like you might not get a very good idea of how the brain is representing
those images because there's a lot of noise in the MRI scan and there's a limit to how
long you can scan each person.
Whereas I guess in a neural network, like noise is not a problem.
The entire system is deterministic.
We just pass in the image and we get the representation vector.
And you also have these kinds of limits of like we can't see what happens if people have
bigger brains, like we can't manipulate the architecture in those kinds of ways.
So even though we can look at how intact brains are working, we can't see how representations
change when we manipulate them all that easily.
And I guess like in machine learning, like we can do all of those things.
We can look at what happens when we change the loss function.
We can look at what happens when we make the network deeper or wider.
So I think there are like some really cool ways that even the same techniques can be applied
in machine learning that they couldn't be applied in neuroscience.
I felt like we weren't getting very far by trying to record from neurons in monkey's brains
and figure out how those neurons work.
Like it didn't really seem like a very effective way of figuring out how the brain
constructs this kind of internal representation of the world.
So from there, I thought about what could we actually do to understand this?
And it seemed like the most promising thing to do was to look at what happens in simpler systems
that we can construct ourselves and where we can analyze the behavior of everything inside
the system. So in a neural network, we can record all the neurons, which is extremely
challenging in a biological organism. And we can also manipulate the system in any kind of
way that we can imagine. But it still seems like really hard to understand neural networks.
So it seemed like maybe this was a more tractable challenge and a challenge where maybe we could
make some headway that might eventually translate back to brains. And so that's how I ended up in
machine learning. I guess there are like other great things about machine learning.
I guess the pay is much better than in like academic neuroscience. But really, I think like
it's a logical progression based on the ideas that I was interested in. And I am still interested
in the same sorts of ideas. Do you still think now that you're in machine learning and have
made some progress here that there is a good chance that we're going to map our knowledge
that we gain back to the brain? Or do you think there is a bit of a disconnect?
I think that's a really good question. I think there's definitely some knowledge that we're going
to get from machine learning that will map back to the brain. I think like in terms of general
principles and ways of looking at how like information processing systems work, I think there
are a lot of ideas from machine learning that will ultimately help us understand brains.
I'm a little less sure whether we're going to build like a machine learning system that is a
brain. I think there's a disconnect between the way that the systems that we build work and the way
that biology works. And I think that's insurmountable just because there's differences between what you
can build efficiently with cells and what you can build efficiently in silicon. But in terms of
approaches to understanding, in terms of building tools to understand things, the tools that we
build in machine learning, I think will eventually be useful in neuroscience.
So people make a lot of analogies and they make a lot of claims about neuroscience in
connections with neural networks. Is there a statement or a bunch of statements that you hear
over and over again where you just cringe because they're so wrong? Is that something that happens
to you? I can imagine it would. Yeah. Yeah. So I think there's this like kind of basic fact that
neural networks are inspired by brains, which is true. But then there's like all this other stuff
where people try to relate whatever neural network they've built back to a brain and they say that
it works like the brain, but it doesn't work like the brain. There's still like this huge kind of
disconnect in how the system is actually operating. The brain is not literally doing back prop. It
might be doing something that's like back prop. We still don't really know, but it's not like
literally computing gradients by automatic differentiation. I'm fascinated to talk about
this line of reasoning that you have, because you're clearly the kind of guy that you want to
reason about the behavior of models, and in particular, the evolution of representations.
And I watched one of your presentations on YouTube where you were talking about how you can
compare the representations by comparing features. And of course, the naive way of doing is the dot
product or some variations of that. Turns out that doesn't work very well. And you came up
with this wonderful metric called the centered kernel alignment. So how did that all come about?
The way we came up with that idea was that Jeff Hinton had another idea. And I tried the idea
that it worked. But then we wondered, is there a simpler thing that worked? And that's how we ended
up with centered kernel alignment. I guess the problem that we had in trying to come up with
a way of comparing similarity of neural network representations is that it's really hard to know
what is a good way. It's not something where you can really develop a good benchmark.
So in the paper, we came up with this simple sanity check, where the idea is basically we've got
two architecturally identical neural networks, and we just train them from
different random initializations. And so we want it to be the case that if you measured the similarity
between a layer from network A and all the layers from network B, that the most similar
layer in network B is going to be the architecturally corresponding layer. So if we have layer two from
network A, it should be more similar to layer two from network B than layer three or layer four.
And so basically, we found that what people had been doing before didn't always pass that sanity
check. And we basically tried to come up with the simplest way of building a similarity index that
did actually pass that sanity check. And that's how we ended up with centered kernel alignment.
Yeah, because I think you showed that the canonical correlation analysis only worked about,
I think at an accuracy of about 1.4%. So it's complete apples and oranges. But this absolutely
fascinates me, though, because when you plot this thing in this kind of self similarity matrix,
you can glean so much about the evolution as a function of time. And because you talk about this
in one of your other papers as well, that there's this characteristic blockiness. And
when you see blockiness, that successive layers are similar to versions of themselves in the past.
And that kind of means that they're not evolving anymore. And you then made the intuition in your
paper that, well, essentially, it's redundant information. If it's not learning anything new,
I can just delete that block. I can just delete those layers from the neural network. And it
won't make any difference. And indeed, it didn't. Yeah. Could you for people listening explain
the similarity measure you came up with in principle, just so we can imagine something
how that should even work? Yeah. So I guess the idea is like, you've got a neural network,
and you feed some set of examples, like multiple examples through the neural network. And now you've
got like some matrix where the rows of the matrix are like different examples, and the columns are
different neurons. So yeah, you can imagine this as if you have vectors of activations for each
example, you've stacked them row wise. So now what do we do with that to compare two neural
networks trained from different random initializations? The problem is, if we were to just take the
square difference between those matrices, we have this problem that like the neurons between
these two different networks aren't necessarily aligned in any way. If they're trained from
different random initializations, even if we had exactly the same neurons, we shouldn't expect
that neuron one would be the same, representing the same thing in both networks. So we need some
way to get around that problem. One way around this problem is instead of comparing the
these original matrices, we're going to make matrices that measure the similarity of each
example to each other example for one particular network. So if we've got like example A and example
B, we can measure their similarity very simply just by taking the dot product between those two
vectors. And now because we're measuring similarity from the same network, we don't have to worry
about this alignment problem. And we get some idea of how similar different examples are to each
other according to the representation in network A. So if we do that for all the examples, we get
some examples by examples matrix. And then we can do that both for our first network and for our
second network. So after we've done that, we've got these two examples by examples matrices.
And then the easy way to compare those matrices is we just reshape them to vectors and we take
the dot product again between those vectors. So now we've measured the similarities between the
similarities of the examples. And this doesn't have this problem of aligning the neurons because
instead of measuring similarities of neurons, we're measuring similarities of examples. And
then we're comparing those similarities. So ultimately, we do that, we take that dot product,
and then we normalize it in a way that makes it invariant to scaling. So if you just took the
dot product, you'd have this problem that scaling all of the features by some number, if you scale
everything by a factor of two, the dot product will go up by a factor of two. And so we just apply
some normalization. So that kind of scaling will not affect the similarity index. And we get
centered kernel alignment, which gives us a similarity score between zero and one.
The fascinating thing is that you can replace that dot product with a kernel because it's a gram
matrix. So did you find that it made a difference if you use, let's say, the RBF kernel?
Yeah. So basically, when we're measuring the similarities between examples, we can just,
instead of taking the dot product between the representations of the different examples,
we can take the kernel between one example and another example because the kernel is also a
way of measuring similarity. And so we tried that. It turns out that for CNNs, it didn't really make
a difference. Like the RBF kernel worked, but sort of just taking a regular dot product. But we
did find in the appendix of that paper that if you instead use an RBF kernel with a transformer,
it actually does work better than taking a normal dot product. And I think part of what's going on
is that sometimes you want it to be the case that when you're measuring similarity, you care more
about the distances between the examples that you're close to than the distances to the examples
that you're far away from. Like once you're really far away from something, maybe it doesn't
matter so much if you're 10 times as far away, because like you're already so far, you're already
not going to, you don't really care how far away something is once you're far enough. And the RBF
kernel takes that into account in a way that a linear dot product wouldn't. The linear dot product
is like very sensitive to the global distances in the space.
What I find fascinating is that you can glean so much from the blockiness, right? So you were
saying that as it becomes blockier, it might be an indication that it's become saturated in some
sense. And I'm also interested in, in a way, we already know that the representations in neural
networks are increasingly abstract. So they don't necessarily bear any resemblance to the beginning.
So when we're looking at the cell similarity matrix, we don't necessarily want the
representations on the final penultimate layers to be similar to the ones at the beginning.
We want there to be a continuous evolution. We don't want to have a stalled evolution,
because that would correspond to like this blockiness. But is it when you've stalled for a
long time? Is that when it becomes pathological? Because we want it to evolve in stops and starts,
Yeah, I think it's not clear what we should really expect in terms of how a neural network
representation evolves through the layers. I think there's kind of some theory on what we
should expect if all the layers are linear. But like, obviously, the neural networks that we train
are nonlinear. And it's really important to have a nonlinearity in between the layers. And so at
that point, it's like really hard to reason about what the optimal thing for a neural network to do
actually is. I think it's something that we can really only study empirically.
On the other hand, I do think if we see that nothing is changing from one layer to the next,
that's a really bad sign. If the neural network representation isn't changing, then obviously
nothing's happening. But I guess it's unclear whether we should expect like abrupt shifts,
or we want things to happen slowly between the layers. I'm not sure whether we really
have the theoretical knowledge to say what is best.
I'd love to see this as a kind of tool in our toolbox that we could use on different network
architectures. But you said that the other learn features are shared across different
initializations and architectures, particularly across the depths of the network. So it almost
seems as if this blockiness is separate to your work in wide and deep neural networks,
because you showed that the width and the depth have got different effects on network predictions
at the example level or at the class level. But the blockiness almost seems to be an orthogonal
thing. That's just when you have this kind of saturation of the network, you see the blockiness.
Yeah. So initially, we had hoped that we could look at other similarities between
wide networks and deep networks in their representations. But when we did those experiments,
we actually just found that if you make the network really wide, you get this kind of
blockiness in the representations. And those blocks are dissimilar across different initializations.
And then the same thing happens if you make the network really deep. We see these big blocks in
the representations. So that made it hard to study these very wide and very deep networks from
the representational similarity perspective. But at the same time, I think it's like a really
interesting observation. It's something where we couldn't have predicted this ahead of time
based on what we know about neural network theory, and we couldn't have predicted it ahead of time
based on the accuracy of the network. It's something where we really needed these
techniques for looking at the internal representations of neural networks
to see what was happening inside of them. There's this whole literature that takes a
look at a network's expressibility with regards to its depth and width. So could you just explain
it to us whether or not we should be able to meaningfully quantify or formulate the expressibility
of a neural network with regards to your analysis made on that? Yeah. So there's this work that looks
at the functions that can be expressed by wide networks and the functions that can be
expressed by deep networks. And I guess the neural networks seem to become exponentially
more expressive as you make them deeper. So it seems like in that sense, depth is more important
than width. But on the other hand, the neural networks that we actually train in this paper,
both the wide networks and the deep networks are big enough that they can overfit the entire
training set. So in this case, the expressibility of the network is not really important. What's
important is the function that the network actually ends up learning. So I guess even though
networks could express more functions when they're deep, what we're really studying is the function
that you actually get when you train the neural network by gradient descent on some data, what
the optimization process actually finds. One thing as well. In that paper, you talked about
the network pathology, right? And you said that two times depth accuracy, 95%, four times 93.2,
eight times 91.9. So because this is this runs counter to what a lot of us would intuit. We
think that you can have as much depth as you want. And architectures like ResNet, in some sense,
they learn their own capacity. There is a pathology there happening clearly. And how would you
determine that from this visualization? Yeah, so I guess there are two kind of results. So like
you can either look at networks without residual connections where you do actually find that at
some depth, the accuracy will start going down. And in networks without residual connections,
we find that like the depth where accuracy starts to go down is like around the same depth
where you begin seeing this kind of block structure where many successive layers have
similar representations. And it looks like the representation is no longer getting refined
through the network. Yeah, I mean with ResNet, so you can make them much deeper. And it seems
like it doesn't hurt accuracy as much even once you start getting these blocks. But it also seems
once you start getting these blocks, making the network deeper, making network wider, no longer
really gives you any improvement in accuracy. So it seems like this is basically telling you
that the network has fit the data as much as it can. And there's no real advantage to using something
bigger. Fascinating. Let's move on to another paper that you've done, which is quite related
in terms of you've used the same analysis to reason about it. But you had a paper called
What's in a loss function for image classification? And you looked at a whole bunch of different
labels moving and regularizers, which are things that you do on the end of the network. And you
identified differences in accuracy and calibration and outer domain distribution. And you made some
really interesting observations. So by the way, we're talking about things like do we use the
softmax or the squared error or dropout or label smoothing or logic penalty. But you noticed using
the same analysis technique that only affected the representations on the penultimate layers of the
neural network. What's going on there? Yeah, so it's not just the penultimate layer. It's like the
last maybe third of the network is affected by the loss function. But then the first two thirds
of the network, it seems like you learn the same representation, no matter what loss function you
use. So it doesn't change if you use label smoothing. It doesn't even change if you use mean
squared error instead of using softmax cross entropy, you still basically learn the same
representation for the first two thirds of the network. And I think it's still it's a bit of a
puzzle to us why this happens. Clearly, it matters that you're training the network with the loss
function. There's those layers in the first two thirds of the network do change from the initialization.
But I guess it seems the last third of the network is setting up the penultimate layer
representation in a way that is good for your loss function. But the first two thirds of the
network are somehow just learning general features. I think this also like corresponds with the
success of transfer learning where we can take features that we've learned on one task and
transfer them to some other task. What's the implication though? It seems, is the implication
that the loss function is not having any impact on the representations early on in the network.
That seems like quite a big implication. Yeah, I think the loss function must have some impact
because if you don't train the network, if you don't have any loss function at all, then the
representation in that first two thirds of the network is actually quite different. I think
what's really happening is there are these differences among the loss functions which
don't really matter except later in the network. Although they will give you a slight change in
accuracy and slight changes in robustness, they don't matter for this general feature learning
process. I guess maybe it's what we should expect because ultimately we're asking the
network to do the same thing just in a slightly different way. We're still asking the network
to classify images. We're just asking it to provide slightly different outputs to produce
a slightly different representation at the penultimate layer. Maybe we should expect that
those early features or earlier features that are just trying to represent general things about
images, those will be the same no matter what loss function we pick. In your experiments,
did you find whether or not model capacity has anything to do with it? We didn't really investigate
different model capacities in that paper. I would expect that the same thing holds for
a wide range of model capacities. I guess there's no indication from the experiments
that if you use a bigger network or a slightly smaller network that things would change all
that much. But yeah, I think it's still an open question how model capacity changes things. I
guess in the Sinclair paper, we found that model capacity can matter quite a bit.
So the general hypothesis there should also hold even though your model is bigger or smaller no
matter how big or smaller your network is, the general feature learning regime or paradigm should
still hold no matter what loss function you would end up using. Yeah, that would be my guess. I think
if you're in a regime where it's really hard for you to fit the training data, if you have a very
small network, it might be the case that you see more differences in the earlier layers because
it might be that the loss function really affects what features are best there in a way that it
wouldn't if the network is a bit bigger and if it's more capable of fitting your training data.
But I don't really know. I think this is something that is probably worth looking at in some follow
up work. And you found also there's implications for transfer learning with respect to the loss
function. There seems to be like some inverse correlation between the gains you get from a
loss function and how good it is for transfer learning. Or is there like a connection between
loss functions and regularizers and all of that? Yeah, we look at just linear transfer in the paper.
So if we take the features from the penultimate layer and we try to use them directly for some
other task, how good are those features going to be? And what we found was that if you use
loss functions that give you higher accuracy on ImageNet, you tend to learn representations that
transfer substantially worse in that setting. And our intuition is you could learn many different
kinds of representations in that penultimate layer and still do a reasonable job of classifying ImageNet.
But what seems to happen is that the loss functions that perform better lead classes to
become more separated in the penultimate layer. So like class A and class B will be farther apart
relative to the variability within the class. And when you have a situation like that, you have
this penultimate layer representation that's specialized for the classes in ImageNet. Like
you've got a thousand clusters corresponding to the thousand classes in ImageNet. And so then
if you want to use like those kinds of representations for some other task, it will only really work
well if you have exactly the same classes that are in ImageNet because they're already organized by
the ImageNet classes. On the other hand, what we found is if you just use standard softmax loss,
actually the classes are not that separated from each other in the penultimate layer representation.
And because they're not that separated, there are these features that you could use to
classify things that are not ImageNet that still convey some kind of useful information
about the images that are not just their ImageNet class labels.
It hints at a bit of a future where, you know, like right now on
whatever TensorFlow Hub or Hugging Face repositories and so on, we have these pre-trained
models. And the pre-trained models, they're like full stack models and people usually take some sort
of last or next to last hidden layer. But maybe we should much more focus on actually providing
like half of a network to share, like determining which are actually the best good or general
representations from a dataset and so on. Do you have any of this in mind when you do work like this?
Yeah, at Google, what is generally best for us to do is just to fine-tune the whole network.
And if you fine-tune the whole network, it eliminates some of these issues with
the actual form of the representation in the penultimate layer. Because even if you have this
kind of highly specialized penultimate layer, when if you're allowed to change all the other
weights in the network, you can fix that and you can specialize the rest of the network
for some other task. But yeah, I think like it's a really interesting question. If we just want
to turn an image into a vector that we could then train a linear classifier on top of,
what is the best way of doing that? How should we approach that problem? And how should we approach
that problem if we want this very like general universal vector representation of an image that
would work well for a lot of different tasks? And I think we don't really have good ways of doing
that because basically this is all empirical, right? Like we don't know what makes a good
universal representation of an image. We've just got to try a bunch of things and figure out what
works best. And I guess, yeah, the insight from this paper is like actually the loss function
that you used to train the network can make a huge difference there. Fascinating. I guess
without any further ado, we should move on to SIMCLEAR, a simple framework for contrastive
learning of visual representations. We've been absolutely fascinated by this concept of unsupervised
contrastive image representation learning algorithms. We've seen such a huge kind of step
forward, haven't we, over the last couple of years in this area? Yeah. It's pretty amazing to me.
Could you just go back to real basics? Imagine that people out there have been living in a cave,
they don't know what contrastive learning is. They don't know about image augmentation.
How would you frame the whole thing up? The self-supervised learning setup is we've got a
bunch of images and at least the initial like historical self-supervised learning setup is
we've got a bunch of images. We want to train some kind of neural network on it. And we want
that neural network to learn a representation such that when we then just train a linear classifier
on top of that representation to classify image net, it's going to do well. But we want to learn
the initial representation without using any kind of labels. And yeah, I guess there are a lot of
different approaches that people tried for this problem. Like people tried things like let's train
a neural network so that we can cut up the image into just a grid and shuffle the grid. And then
the neural network has to figure out how to assemble these puzzle pieces back into the original image.
And maybe that'll give us a good representation. Or let's try just rotating the images so we can
have images that are rotated 90, 180, 270 degrees. And then we'll have the neural network try to
classify what rotation we fed into it. And so people came up with these kinds of tasks that you
could try to train a neural network to do so that it would learn some kind of good representation.
They were defined in this ad hoc way. Let's come up with some kind of funny thing where you don't
need a label. You can have the neural network trained to do this kind of thing. And maybe it'll
learn something about images starting in around 2018. There are a few papers that basically
suggested this alternative approach where you're trying to learn some kind of representation space
where you've got different patches from an image or different augmentations from an image, just
different representations of the same image. And you want to learn a representation space
where these representations of the same image are all close together in that representation space.
And they're far apart from the representations of other images. This surprisingly seems to
lead to very good representations. But it turns out there are a lot of very important details to
get this to work well. So it's really like a situation where the basic idea is very simple.
Let's create multiple views of an image and try to get them close to each other and far away from
everything else. But things like augmentation and things like the exact way we set up the network
end up being very important to learning a good representation with this kind of technique.
How does the negative sampling work? People have done this in different ways. So in Simclear,
our way of doing negative sampling is very simple. So basically we are attracting
two views of the same image. And then we have a mini batch that has 4,096 images in it
and two augmentations of each image. And so we are repelling using a softmax from all of the other
8,190 views in that mini batch. Basically, we want our two augmentations of the same image
close and we want them to be far from the other 8,190 images.
It's a bit of a throwback to work to VEC. I think it's pretty cool how these ideas just come up
through the eras and through the different models and so on. And there is seemingly always
another layer on top of these ideas. Pretty cool.
Yeah. So if you only consider two views that are coming out of the same image as the positive pair
so to speak and all the other views are coming out of the different images
located in the same mini batch, wouldn't this hurt the representation space to some extent?
Let's say you have multiple images of dogs in a mini batch of 4,096 samples,
we would essentially want the representations of different dogs to map together as closer as
possible in the representation space while representations of cats from dogs would get
further away. Wouldn't we expect this? But how does Simclear ensure this rigorously? I guess
it's because of the larger batch sizes you use, but I still wanted to know from you.
Yeah. One thing is even if we've got other kinds of images that we want to be close together in
the mini batch, even if we've got a dog image and then another dog image and ultimately we want to
learn a representation space where maybe they're not so far apart, on average, most of the images
in the mini batch are things that we want to be really far apart from. So maybe it doesn't hurt
that much if we're repelling from everything as opposed to just repelling from images that are
part of other classes. I think this actually is something that hurts current self-supervised
learning techniques and hurts contrastive techniques because we also know when you do
the contrastive loss, if you don't contrast against examples that are very close to you,
that actually improves things a little bit. So if you don't contrast against the very hard negatives,
we've found that that gives you slightly higher accuracy when you do this linear evaluation.
That kind of suggests that this really is a problem with these techniques that maybe sometimes
you don't want to be as far apart from other images as the losses encouraging you to be.
Now there's one other aspect which is that in Simclear, we don't actually use the representation
that's feeding into the loss function. Like we have this projection head, an MLP on top of the
network and instead of using that representation at the end of the network, we use a representation
that's two layers back. And so by using a representation that's two layers back,
even if in the final layer we're pushing things apart, we kind of figure that this earlier
representation might not have pushed apart the things that really are semantically similar.
And indeed we find that like using this earlier representation in the network
leads to higher linear evaluation accuracy, so it works better.
I was very fascinated by all of these different tricks that you apparently have to get. And so
big kudos to figuring all of this out for the rest of us. There has been a lot of follow-up work
on this idea. There's a lot of modifications. There is this bootstrap your own latent where
they completely leave out the negative sampling. Then I think just like one or two weeks ago,
there was a paper saying if you build in a stop gradient into the contrastive loss,
you also apparently don't need a negative and so on. Do you have maybe from your own work or
from work of others, do you have any sort of current? If I were to build a self-supervised
contrastive representation learner today, what is the top things I should do? What is my recipe?
How do I go about it? The most important part of the recipe is data augmentation. So like we're
going to use two views from the same image and it's very important how those two views are constructed.
But there are really only two super important data augmentations that we need. So we have to
take two different crops from the same image and then we have to do some kind of color distortion.
So in Simclear we use like very aggressive color distortion. So that is probably the most
important part of the recipe. Then I guess we feed that representation into a neural network
and fortunately we found that you can just use a regular ResNet 50 for this part. You don't have
to worry about architecture, engineering specifically for contrastive learning. Then I think all of
the work since Simclear also uses this idea of putting an MLP on top of the end of the network
and then using that to get whatever representation goes into the loss function but then discarding
part of the MLP when we later just want the representation for a downstream task. All of
those pieces are pieces that are shared by all of these modern self-supervised learning techniques.
Like we introduced the idea of this projection head in Simclear and we also spent a lot of time
studying the augmentation although we were not the first people to come up with the idea that
the augmentation was important. Yeah in terms of what the loss function is I guess it's surprising
that there are so many things that work that we use this contrastive loss in Simclear because it
was what previous work had done and it's like intuitive that you might want to learn a space
where you're explicitly pushing away representations of other examples but I guess like in BYOL they
aren't explicitly contrasting against representations of other examples so instead they have a network
where they're taking a moving average of the weights that they've been learning and they try to
match the representation that's coming out of the network that they're training to this representation
of this moving average network and somehow magically that works and I guess it doesn't even
have to be a moving average. I think you were referring to earlier like you can just match the
representation of one network to stop gradient of the same network as long as you're matching
the representation in an earlier layer and I think like it's still like mysterious why that
should work I don't really have any insight into how either BYOL or the more recent papers
actually are learning a representation that doesn't end up collapsing but the problem is if you're
trying to match some earlier representation you could just collapse to the point where all of your
representations are the same and then like you would trivially be matching the earlier representation
but this doesn't happen and I think why it doesn't happen relates to some mysteries about neural
network training dynamics that we still don't entirely understand. I'm absolutely fascinated
by this concept of data augmentation early on in my neural network career I just imagined it as being
a way of increasing the size of your training set but in a sense you're not really adding new
information it's you are creating semantically equivalent noise perturbations or examples
similar to how BERT works the NLP model it's like a denoising autoencoder and you're creating
noise diversions of the same thing and pushing the examples off the manifold so there seems
to be a dichotomy between on the one hand augmenting your data and it's almost like you're stopping the
neural network from overfitting on things like the the color or some specific feature you don't
want to you want to have a bit of generalization but at the same time you are saying those things
over there it's definitely nothing like that. The data augmentation that you need for contrastive
learning is different from the data augmentation that you need for supervised learning because the
task is different. When you have contrastive learning you have this problem that if there's
just one feature in your data that can be used to do the contrastive task to get images of the
same example or views of the same example close together and far apart from views of all the
other examples if you could do that with one feature that would be the only feature the
network would ever learn or it might be the only feature the network would ever learn and so with
the augmentation you're making the task harder so that the network actually has to learn many
different kinds of features so I guess we find that like this color distortion actually is very
important for self supervised learning for contrastive learning whereas it doesn't really matter
for supervised learning and what we think is going on is that if you have two crops from the same
image like generally their color histograms are surprisingly similar if you just plot out the
intensity histogram of the image you can see that the crops came from the same image and
that's a trick that the network is very good at doing because I guess if you have relu activations
they're very good at computing histograms and so by doing the color distortion we basically
we don't let the network just learn the color histograms in order to do the contrastive task
we force the network to actually use other sorts of information and that ends up being like
critical to the performance of these contrastive methods like it basically doesn't work unless
you do that kind of aggressive color distortion because that seems to be the key thing then so
you're not telling it to learn things you're telling it not to learn things we're telling it
to learn one thing we're telling it to learn figure out which views came from the same image
but then yeah we have to make sure that it learns to do that with a diverse set of features
instead of just doing it in one way because I guess it's like a task that's actually pretty
easy to do if you don't have this kind of aggressive augmentation yeah I think in a way
it helps the network to also differentiate what actually what is the thing that differentiates
two images I think it helps the network to learn you know pick up on that signal to that end I
also wanted to ask for a custom dataset if I wanted to you know apply a sim clear what pointers
should I take into consideration while designing my augmentation policy I'm sure you have been
asked about this question quite a few times but yeah I think it's a good question like I think
like we actually still don't really know how generalizable these contrastive learning techniques
are beyond image net like we know they work super well on image net but like image net is like a
boring dataset to apply contrastive learning to because we actually already have all the labels
and we could just be doing supervised learning but I think starting with the crop and color
augmentation is definitely a good idea at least like for datasets that have color I guess if you
don't have color then maybe think about distorting intensities instead of colors but beyond that I
think it depends on the specific task and what you really want the neural network to pick
up out of the dataset I feel like there are probably some sorts of data where I wouldn't
really expect uh contrastive learning to work well so for example like if you try to do
contrastive learning on a dataset of medical images where you've just got healthy patients
and then you want to translate that to like some sort of dataset of people with some kind of pathology
you might never pick up the features that are important for detecting the pathology
but yeah I think this question of how do you design the augmentation what augmentation works
well for datasets that maybe aren't natural images like these kinds of medical images or
maybe like satellite images that's an important question that we haven't addressed yet
there seems to be this fascinating universality of representations especially in vision this is
exactly the kind of thing you can test with your wonderful similarity matrix idea I'm not
trying to be flippant when I say this because practitioners have used ImageNet on a variety
of downstream tasks for example they might use it for classifying circuit boards or something
and the miraculous thing is it just seems to work quite well so do you think in your opinion
that there is some kind of universality I'm very skeptical about like universality of ImageNet
for different tasks like in the past we we did some work where we looked at how well ImageNet
networks transfer to other tasks and it seems like there are actually some tasks which are just
like datasets of natural images where pre-training an ImageNet doesn't really help at all and those
datasets just seem to be too different from ImageNet they're things like this Stanford
cars dataset where you have to like classify different cars according to their make model and
year it turns out even though there are lots of cars in ImageNet if you pre-train on ImageNet
and you fine-tune on that dataset you will learn to classify it faster in fewer steps than if you
had trained from scratch on the the Stanford cars dataset but you won't actually perform any better
at the end and that's true even though the the Stanford cars dataset has 10 000 images so it's
tiny compared to to ImageNet so I think like actually representations of images are not that
universal and at least what works for natural images for images like those in ImageNet may not
work on other datasets I think there's also like limited evidence for transfer from ImageNet to
medical datasets it seems if you don't like work really hard at tuning hyper parameters or if you
don't train for long enough you will get better accuracy by starting with an ImageNet pre-train
network but if you do very thorough experiments and you train for long enough you try different
learning rates and weight decay parameters like actually it seems like training from scratch on
most medical datasets will give you the same accuracy as if you started from a network that's
pre-trained on some other giant dataset maybe this makes sense because if you think about
radiologists like it's not like a radiologist can just like at the beginning of their education
they can't just look at an MRI or an x-ray image and say this is where the tumor is it's something
that takes them years of training to learn how to do and so maybe it also makes sense that like our
neural networks can't just immediately easily without lots of training pick up on very different
image distributions it does seem to make sense going a bit from the universality of representations
to the universality of augmentation since this is such a crucial part do you think that there is
a systematic way how we can discover augmentations because it seems right now it seems to be kind
of a whack-a-mole right it's okay we just feed images and it's no that's too easy we crop them
oh no it's the color histogram so we like whack on the color and then it works better but maybe
someone finds out oh there is still this easy feature that the network every now and then
pays attention to so we design a method to whack on that a bit do you think there is a
systematic way or will this kind of philosophically always rely on us humans having a higher level
inside of what we want to do with the data set yeah so i think actually i'm hopeful that at least
for natural images just like crops and color distortions are enough because i guess like what we
found is you combine those two augmentations and if you do that like that gets you most of the way
to supervised accuracy so maybe we shouldn't expect huge gains from adding additional augmentations on
top of that even though there are like in the Simclear paper we add like Gaussian blur which
gives slight gains on top of that i guess in the VYL paper they add even more augmentations on top
of that so you can get small gains but it seems like the gains are much smaller once you got the
crops and the color distortions there in terms of systematic ways of discovering what set of
augmentations we should be using i guess there's a paper that i saw where they basically use linear
evaluation on the rotation prediction tasks to see whether the augmentations are good and they
claim that actually works for evaluating the augmentations so maybe that's one way i don't know
there are all sorts of ways of designing augmentations for supervised learning that could
conceivably be applied to the self-supervised learning setting to the contrastive setting
like there are these metal learning based approaches for learning data augmentation
i'm not sure like those techniques tend to be pretty complicated i'm not sure whether it's
actually easier to deal with those techniques than just like trying a bunch of things but i i think
that maybe it doesn't matter that much maybe like just at least if you're dealing with natural images
maybe crop and color distortion is enough i guess if you think about other images i don't
really have any idea i i guess it depends on what the images look like there are lots of
things that you could be expressing as images like a spectrogram or like a some kind of chart or
whatever where you could be applying a neural network to it but like the further you get from
natural images the less clear it is what kind of augmentations you should be working with
it is a fascinating thought though this universality of augmentations and when you said
cropping and and color that made me think it seems to be related to the inductive priors in
in the cnn architecture that we use and also to things like it's regularly sampled gridded
discrete image data because we're speaking with max welling the other week and as he's created
lots of other interesting inductive priors for computer vision models but it does set my mind
racing a little bit because presumably there's a continuum so on the one hand we don't do any
augmentation and we just learned from examples in the middle we do the augmentation and then
maybe in the future because some people have said that computer vision systems don't have seen
understanding they don't understand physics the ball might be on the table but we don't know that
it's not falling and so on there there's a lot of missing information so would the next step be
some simulation do you know what i mean where we impute physics and we impute some world knowledge
and then i don't know whether we train a machine learning model from that yeah i think like there
are definitely shortcomings in like our current machine learning models understandings of the
world they're probably things that we can't just solve by throwing more static images at them i
think like maybe the next step rather than trying to immediately situate the machine learning model
in a kind of a simulated world we could just think about video i think there's already like a lot
of additional information in video that a neural network could use to learn like interesting
representations i guess like it seems like if you just see static images it's hard to learn
how to segment objects it's hard to learn like where the objects boundaries are but once you have
video it's like the stuff that's moving together is an object and you can tell that because it's
moving together so i think there's a lot of potential for learning better visual representations
from video and and maybe eventually from like these kinds of interactions in simulated environments
but i think like ultimately it becomes a computational headache like even video is a
computational headache because suddenly you've got all of these frames that you have to deal with
you probably want to be thinking about how representations change over time and video data
is just huge and it's especially huge if you have to process many frames at at once on your
accelerators so i think that's like why this hasn't taken off yet but i think like probably
representation learning from video is going to be like a big thing next year or like the year
after or something sometime in the near future we would love to talk about your big self-supervised
models are strong semi-supervised learners so this is super interesting right because you're
combining the unsupervised stuff that we've been talking about in Simclear but now we're in the
semi-supervised domain where the the kind of label efficiency becomes super important so what's the
deal there yeah so i guess in Simclear we focus on this question of linear evaluation accuracy
where we're just learning a representation without any labels and then training a linear
classifier on top of that representation on the same data but now with all the labels it turns
out that's not really a very practical problem if you have all the labels there's not necessarily any
reason in practice that you would want to first learn this representation and then train the
classifier versus just doing standard supervised end-to-end training but what is a practical problem
is the situation where you have a lot of unlabeled data and then a very small amount of labeled data
and so that's the the situation that we look at in Simclear v2 in in in that paper and what we find
there is that you can train this network fully unsupervised without using the labels on all the
data and then you can fine-tune it on just the subset where you've got the labels and if you do
that it's possible to get very high accuracy especially if the network is very big so basically
we find if you have a really big ResNet if you have ResNet 152 and then you make the layers three
times wider when you do that you can get accuracy when you fine-tune on 10 of the labels that's
like substantially better than if you trained ResNet 50 from scratch with all the labels and once
you have that really big network it turns out you don't have to put the really big network into
production you can take the really big network and you can then distill it back into a standard
ResNet 50 and you can retain almost all of the accuracy when you do that but I guess what's
important about this distillation process is we're not just going to distill on the labeled
data set we're going to also use the labels that this giant network which we fine-tuned on a small
subset of the data we're going to use the labels that it gives on all of our unlabeled data so we're
going to use it to generate labels and then we're just going to use those labels to train a much
smaller network and if we do that we get accuracy that's similar to or maybe even slightly better
than standard supervised training from scratch this becomes a highly practically relatable
approach toward doing computer vision related things we see all the times that folks have
a huge corpus of unlabeled images but they only have maybe 5% or 10% labeled images so this
immediately becomes a practically applicable recipe for them so I definitely am looking forward to
seeing this thing implemented at scale at different companies so that's there and
if I understood it correctly just for the viewers you folks used a variant of
distillation here which is more popularly referred to as self-training
yeah I don't think there's really a difference between what we call distillation and what other
people call self-training but yeah I guess the idea is basically we will pass information into
the network we get its output probabilities and then we train another neural network
with those output probabilities as the targets what I find fascinating is how many ideas come
together in this paper so there's first there's this let's do representation learning and then
we have these just small labels right we fine tune and then there's big networks small networks and
then you also you label but you also apply some noise if I understand correctly in the process
of transferring maybe I'm misremembering that but there's a lot of ideas that come together
Yannick you probably confused it with noisy student training
maybe you impose noise during the student training sorry maybe not but there's there
is a lot of ideas that come together and something tells me that there was a process behind going
you probably didn't sit down after Simclear 1 and be like all right what do we do for Simclear 2
okay let's do this so there it tells me there was this process could you if you can maybe elaborate
a bit on how did you going to build up the system towards the final output was there like dead ends
or was it like let's build up until we can no longer make it better how was that
yeah so I guess like there was a bit of a process so after after like the original Simclear paper
I guess like it's clear from the Simclear paper that when we have this bigger network we get
much higher linear evaluation accuracy than we do if we just train Simclear with a ResNet 50
so then the question was like is there some way that we can somehow eliminate this dependence on
this giant network because the giant network is annoying to work with it's computationally
expensive it's big and so we first tried what happens if you distill the unsupervised network
we basically have this task that is set up as a form of cross entropy loss when we're doing
the contrastive learning and so you can also think about distilling on that task where you have
some set of you have a probability distribution that corresponds to the similarity between an image
and all the other images and you could use those kinds of targets to distill so we tried that and
that kind of worked but then we we also tried the approach of first fine-tuning the big network
and then distilling it and it turned out that worked a lot better and so I guess that was I
guess we jumped straight to distillation because we knew that we could get much better results
by using a giant network with Simclear and then once you realize that distillation is going to
be important like the only thing you've got to figure out is what kind of distillation should
you be doing and what we found was like this approach of pre-training then fine-tuning then
distilling works a lot better than pre-training then distilling then fine-tuning. How far down do
you think one can go with this final distillation step? Is this something that is conceivably
going to be available on let's say edge devices at some point like that our glasses or something run
with similar accuracies to these giant networks or is it more a factor of four, a factor of 10
kind of stuff? I think like there are clearly some limits to distillation like I guess like
we probably shouldn't expect distillation of the the kind that we do in Simclear v2 to work
substantially better than like supervised distillation which has been around for quite a while now.
I think what's impressive is that in the self-supervised case in the contrastive case
like distillation basically allows you to recover the same accuracy that you would get
from training supervised from scratch whereas without it like the accuracy is a lot worse so
it seems like it maybe matters more in this contrastive case but I think like generally when
you do distillation in the supervised case you can get like maybe a percentage point gain maybe a
couple of percentage points and I think that's probably about the limit in terms of the improvement
that you could get from any kind of distillation based approach over supervised training from scratch.
Fascinating, I don't know if you know that we've been playing with GPT-3 and
you said something quite interesting just now you said that they're deterministic but
in GPT-3 that's not really the case if you sample from it deterministically it gets stuck in cycles
so you have to do some kind of trickery some kind of random sampling from this distribution
and it might be the case in future computer vision models as well that we have to randomly
sample from them in some way because otherwise it would get into some pathological behavior
and maybe to do that we need to have some kind of controller on the top so I suppose my question
is in the future maybe we will be in the stochastic regime and what do you think about that?
With GPT-3 you're trying to generate data so I guess when you generate data there has to be some
kind of noise that's coming from somewhere like the process of generating data is like
turning noise into data but I guess for image classification we have the data and we just
want to turn it into a label and so maybe there's like this implicit notion of stochasticity and
that like the network gives some kind of output distribution but I think we still want everything
to be deterministic if it can be deterministic like we basically want the network to say
this is a dog with probability x if there's some way to improve it with stochasticity I don't know
I guess dropout used to be very popular but it seems like it's not so popular anymore and it
doesn't really help us very much with vision models and it also like even dropout is generally
only used when we train the neural networks on the other hand like the brain is very stochastic
the brain has lots of noise I guess that kind of suggests that maybe there's some way to leverage
noise to learn better representations in neural networks as well and we just don't quite know
the right way yet. That's right Max Welling said to us that he thinks that the future of AI will
have a generative component he thinks that we have the matrix in our minds don't we we have
these simulations going on all the time and we're generating new scenarios and it's related to the
data augmentation thing as well some people have said to me in the past that using a GAN
might be a way of doing data augmentation and presumably that would require some kind of
stochastic sampling as well so I suppose it's just quite interesting to see where these two
things might meet in the middle at some point yeah I don't know I guess like with a GAN using a GAN
to do data augmentation you have this problem but you still don't actually have more data
like you have a GAN that's trained on the same data and so it might help you because like your way
of encoding inductive bias into the GAN is different from your way of encoding inductive
bias into the neural network and maybe like by having more inductive bias you can learn a better
function but it's not you still don't have more data and it seems like without having more data
like there's no reason to expect a priority that you will be able to learn a better function.
I'm so glad you said that was always my intuition the amount of people that have said to me that
you should use a GAN for data augmentation anyway. What's the first thing you think about if you're
like oh what could I use a GAN for and then you learn about data augmentation like wait
this is so much more data yeah but it conceptually yes you don't have you don't have more data and
ironically when you do the simple data augmentation you do have more data because you have you put
all the knowledge in there as a human of what makes two images dissimilar visually but still
equivalent semantically which again is exactly the opposite it gives you images that are visually
similar but it has no intuition of what the semantic similarity is. For my last question
I actually want to switch topics just a little bit to what Tim said at the beginning namely
your love of Julia so I have seen a number of especially data scientists be strong advocates
for for Julia as a language and so on do you want to give anyone sort of the pitch why should we
even consider this? Yeah so I think like Julia is a much better programming language than Python in
many ways I guess one one thing and I guess the thing that first attracted me to Julia is that
it's really fast like you can write Julia code and like with very little work you will end up
running as fast as equivalent c code so that's something that you can't get out of standard
Python code if you're just writing a for loop in Python it's going to be super slow but in Julia
you don't have to worry about all of that and you don't have to worry about like Python or
number all of this other stuff that people have hacked on top of Python like Julia is just designed
to be fast and it works I think there are like other advantages to like Julia as a language
beyond that I guess it's it's built on this idea of generic functions where you have a function that
can take multiple types and you can define the function differently for the different types
and this is something that we do all the time when we're doing like machine learning like we have
matrix multiplication which is a different form of multiplication that takes matrices and produces
something and I guess like in Python it's so it used to be that you had to type dot dot to me to
multiply things but now it's like they have this at symbol that does matrix multiplication but Julia
is designed for these situations where maybe beyond just matrices you have these funny types of
structured matrices you have sparse matrices and you can define special methods for the product of
a sparse matrix and a vector or all sorts of things where you might want different methods depending
on the types and even though it seems like this is complicated and you might have some trouble picking
which version of the function is going to be called at runtime because Julia is ultimately
compiling everything when you call it and because it has this kind of strong type system it can
ultimately pick which method is going to be used and compile that method call in and you don't have
to worry about picking which one and so it still ends up being fast. I also think like there's
this question of whether like the object-oriented Python or the object-oriented paradigm in Python
is really like the best paradigm for machine learning because I guess like it's like we have data
and then we have functions that operate on the data but in the object-oriented paradigm you
want the functions that operate on the data to be attached to the data which is like a weird
way of setting things up and that Julia is not set up that way you have these data structures
and then because you are able to create functions that specialize on the data structures you don't
have to worry about attaching those functions to the data structures themselves. Amazing.
Dr. Simon Cornblith, thank you very much for joining us this evening. It's been an absolute
pleasure. Thanks for having me. Thank you. I really hope you've enjoyed the show today. We've had so
much fun making it. Remember to like, comment and subscribe. We love reading your comments,
every single one of them and we'll see you back next week.

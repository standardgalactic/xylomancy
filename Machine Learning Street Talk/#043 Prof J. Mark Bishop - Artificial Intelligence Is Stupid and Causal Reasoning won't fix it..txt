Do you remember the taste of the last orange you ate? Do you remember the warmth of that welcoming cup of tea that you had when you came in from the cold November rain?
Do you remember the smell of the hair of the first love that you kissed?
I'd like you to try and hold on to these sensations because I'm going to come back to them.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with me, your host, Dr Tim Skaff.
If you want to be exposed to completely new ideas and challenged intellectually, then this episode is probably the episode for you.
Last time we had a philosopher on the show, we had absolutely atrocious viewing numbers, but I'm a big believer that we need to be challenging our preconceptions on absolutely everything.
Pedro Domingo said that there were only five tribes in artificial intelligence and he didn't even consider the other tribe, which not many people talk about, cybernetics.
Cybernetics is the science of communications and automatic control systems in both machines and living things.
We're going to discuss AI across three key dimensions today.
Computability, understanding and the phenomenological experience or consciousness.
When we talk about artificial intelligence, what we basically mean is the science and engineering.
We're trying to engineer machines to do things that we might say is clever.
Professor Mark Bishop does not think that computers can be conscious or have phenomenological states of consciousness unless we're willing to accept panpsychism,
which is the idea that mentality is fundamental and ubiquitous in the natural world, or put simply that you're goldfish and everything else for that matter has a mind.
Panpsychism postulates that distinctions between intelligences are largely arbitrary.
Mark's argument is distinct from Searle's argument that computers cannot understand and also from Roger Penrose's view that some tasks which humans perform are simply non-computable.
He thinks that there is no objective fact of the matter about which computations a physical system is computing.
This is because of the observer relative problem which Mark will outline in great detail in today's episode.
Many of the ideas we're going to discuss today are anathema to the current modus operandi in artificial intelligence research.
Just the reading list we got from Mark today will keep us busy for the next year.
Mark's work in the philosophy of AI led to an influential critique of computational approaches to artificial intelligence
through a thorough examination of John Searle's, the Chinese remarkument, and we'll be discussing that in great detail later.
Mark is also the scientific advisor to Fact 360, a startup deploying artificial intelligence using natural language processing for e-discovery
or detecting malicious insiders by subtle changes in language in human communication networks.
Insiders who might pose a threat to your organisation and they use sophisticated graph analysis to do that.
Mark just published a paper called artificial intelligence is stupid and causal reasoning won't fix it.
He makes it clear in this paper that in his opinion computers will never be able to compute everything, understand anything, or feel anything.
For much of the 20th century, the dominant cognitive paradigm identified the mind with the brain.
You, your joys, and your sorrows, your memories, and your ambitions, your sense of personal identity and free will,
are in fact no more than the behaviour of a vast assembly of nerve cells and their associated molecules.
You're nothing but a pack of neurons. And that was according to Crick in 1994.
The Church Turing Hypothesis stated that every function which would naturally be regarded as computable could be computed by the universal Turing machine.
If only computers could adequately model the brain, then, the theory goes, it ought to be possible to program them to act like minds.
With its myriad of features running the gamut from causal learning, reasoning, and understanding.
Even Bengio observed in 2019 that we know from prior experience which features are the salient features and that comes from a deep understanding of the structure of our world.
The Church Turing Hypothesis has triggered an explosion of interest in biologically plausible neural networks.
We had Dr Simon Stringer on the show last week talking about the spiking dynamics, the temporal binding circuits which emerge when you create some of these biologically inspired neural networks.
But I'm not just talking about the biologically inspired versions, even the relatively pedestrian vanilla neural networks that we all know and love on this channel.
This has all been a huge focus point for the last 50 years or so.
AI esgatologist like Ray Kurzweil and Nick Bostrom believe that there might be an intelligence explosion where all of humankind will inevitably be crushed like ants.
Although viewers of this channel will well know Francois Chalet's response to this view.
Alan Turing deployed an effective method to play chess in 1948, many decades ago.
But since then we've seen little progress in getting machines to actually genuinely understand, to seamlessly apply knowledge from one domain into another.
Judea Pearl believes that we won't succeed in realising strong AI until we can equip systems with a mastery of causation.
He thinks we need to move away from simplistic probabilistic associations to machines which can reason causally.
He even proposed a so-called ladder of causation which is seeing, doing and imagining, which I feel is almost self-explanatory actually.
Unfortunately for Pearl, DeepMind have already demonstrated several times a reinforcement learning system which can perform causal reasoning and counterfactual analysis.
It seems obvious to me because if you're interacting with a system then of course you can learn the causal factors.
I'd completely take Pearl's point that with traditional machine learning where you're not interacting with a system you can't learn any causal factors.
That seems quite intuitive. Anyway, all of this is small fry compared to the point which Professor Bishop wants to make.
The idea that these silicon ensconced algorithms can become thinking machines becomes a little bit bizarre once you realise that a machine has no choice in what it does.
Computation is not an objective fact of the world, it's observer relative. Even Wittgenstein said that the meaning of a computation is in its use.
He thought that understanding could not be a process and therefore it cannot be a process of symbol manipulation.
Whether a given individual understands is often external to that individual. Mark's intuition is that evolution, autonomy and environmental interactions give rise to phenomenological consciousness.
He thinks that we cannot live inside a computer simulation because he can feel the sensation of cool air on his face.
So Mark thinks that the meaning of computation becomes relative and lies in its use by humans.
Mark gives several examples in the show this evening demonstrating precisely why he thinks this.
So I think Mark's main contribution is this dancing with Pixie's reductio ad absurdum.
In 1988 the influential American philosopher Hilary Putnam published a paper in which he showed that under the influence of gravitational waves and cosmic rays and the subatomic particles that make up all the objects of our world,
your seat, the seat you're sitting on, the very clothes you're wearing, the room that we're in, they're all containing a rich dance of subatomic particles, a dance that never repeats itself.
And Putnam realised that this is analogous to a state machine going through an infinite series of non repeating states.
So it then seems to me that if a computer, a terminator perhaps, is conscious purely as a result of moving through a series of computational state transitions,
then if I know the input to that machine with input fixed, I can generate exactly the same series of state transitions with any large counter like a car's marlometer or following Hilary Putnam's move with any open physical system.
So if a machine is conscious merely as a result of following some computation, then consciousness is everywhere in the bricks of this building, the clothes that you're wearing, the very seat you're sitting on.
They are all experiencing the zing of that orange, the warmth of that cup of tea and the memory of your first love's kiss.
If machine consciousness is possible, everything, even the smallest grain of sand, is filled with an infinitude of conscious experiences.
Bishop interprets Putnam's result to mean that computationalism demands that every physical system is host to a multitude of conscious minds, which he refers to as little pixies.
Since a computationalist believes that to be a conscious mind is just to implement the right kind of computation, not only would we be surrounded by pixies, but the vast majority of conscious experience would be realised in these pixies.
Since any physical system is implementing any and all computations simultaneously, then all possible conscious minds must be instantiated simultaneously in every physical object.
For Bishop, this is the most patently absurd manifestation of panpsychism and thus demonstrates that computationalism must be false.
So it seems like a contrarian position that Bishop is saying that computation is very much in the eye of the beholder, whereas most of us think that computation goes on inside our brains.
Anyway, the key takeaway from the dancing with pixies reductio ad absurdum is that computation doesn't have those phenomenological conscious states.
A finite state automata cannot give rise to conscious experience unless conscious experience is in everything.
Bishop says that he's an embodied entity, which is to say he's not just thinking in his brain, he thinks with his body and his body in the world.
In today's episode, we also talk about some of the greats of computability, mathematics and logic, starting with Alan Turing on computability.
He described a machine called a discrete state machine. I'll call it Turing's discrete state machine because that was the first time I read about it in his work.
So over any short time period, we can replicate the behaviour of the different state transitions of Turing's discrete state machine with any other one such as a digital automata.
But because when we added input, the number of possible state sequences grew exponentially, we can't easily do the same thing when you have a machine with input.
But then I realised, if I know the input to one of these machines, that combinatorial state structure collapses again just to a simple list of state transitions.
He invented this interesting thought experiment called the discrete state machine, and he had this physicalist desire to explain all of humanity via a computer program.
And interesting, what he learned later on in his career about the non-computability of numbers led to a significant amount of tension for him later on in life and his children.
The American philosopher John Searle was so exasperated that anyone might seriously entertain the idea that computational systems, purely based on the execution of appropriate software, no matter how complex, might actually understand.
It was ridiculous. He formulated the now infamous Chinese room experiment, and we'll go into this in some detail in the show, but essentially he said that syntax is not sufficient for semantics and that programs are not formal and minds have content.
Therefore, programs are not minds, and computationalism must be false. Now, most of the Chinese room argument is the first proposition, which is that syntax is not sufficient for semantics, and we will come back to that later.
Another interesting character is Godel. Godel's first incompleteness theorem famously stated that any effectively generated theory capable of expressing elementary arithmetic cannot be both consistent and complete.
In particular, for any consistent effectively generated formal theory, f, that proves certain basic arithmetic truths, there is an arithmetic statement that is true, but not provable in the theory.
And this can be used almost anywhere, and it's often referred to as the Godel sentence for a particular theory, and it was used in anger by Roger Penrose.
He made the Godelian argument that mathematical insight cannot be computable.
He said that the mental procedures whereby mathematicians arrive at their judgments of truth are not simply rooted in the procedures of some specific formal system.
And he followed up by saying human mathematicians are not using a knowably sound argument to ascertain mathematical truth.
Anyway, I really hope you enjoy the show today. I'm absolutely honoured that Mark came on to discuss this with us.
I'm very interested in the philosophy of AI and the philosophy of mind.
Make sure you read some of the material that Mark has signposted, and I'll link those in the description.
Remember to like, comment and subscribe, and we'll see you back next week.
Today we are speaking with Mark J Bishop, Professor Emeritus of Cognitive Computing at Goldsmiths University of London.
Mark is interested in the philosophy of mind and artificial intelligence.
Sorry, that's my Siri. It seems to be very interested in getting involved in this conversation.
Mark is interested in the philosophy of mind and artificial intelligence.
Mark is interested in the philosophy of mind and artificial intelligence.
Mark is interested in the philosophy of mind and artificial intelligence.
Mark is interested, now it's playing music.
Because AI is stupid, that's why.
AI is really, really stupid.
Actually, that's a great segue for our conversation today, because Mark, our guest today, also thinks that AI is really, really stupid.
Anyway, Mark is interested in the philosophy of mind and artificial intelligence and rails against what he calls computationalism.
We'll get to that in a sec. Machine consciousness and panpsychism.
In 2010, Mark was elected to the chair of artificial intelligence and simulation of behaviour, which is the world's oldest AI society.
He's been invited to advise on policy at the UN, the EC, and also the UK government.
He's published three academic books, 200 articles, and won Â£3 million worth of research funding.
He serves as the associate editor of nine international journals, and his research has spanned the practice and theory of artificial intelligence.
He's regularly asked to comment on AGI, particularly in response to these AI eschatologists.
Of course, we were speaking about this a few weeks ago, so folks like Hawkins and Musk and Kurzweil, who warn of an existential threat of an intelligence explosion.
Now, in one of Mark's recent papers, he concluded that cognitivism, which is the whole idea of viewing the brain as a computer,
and its concomitant computational theory of mind, is inappropriate.
Instead, we should emphasise the role of foundational processes such as autonomy, exploration, autopiosis.
That's a strange word as well, isn't it?
That means a system capable of reproducing and maintaining itself by creating its own parts and eventually further components.
Social embeddedness in giving rise to a genuine understanding of our lived world.
So, in summary, Mark thinks that computational theories of mind cannot explain human cognition.
He thinks that the claims of its research is that genuine conscious mental states can emerge purely in virtue of carrying out a specific series of computations.
He thinks that those claims are egregious.
Now, I discovered Mark about a few months ago because he's published this paper called Artificial Intelligence is Stupid, and causal reasoning won't fix it.
It's actually a really cool paper because it's a tour de force of all of the computational and philosophical issues surrounding AI at the moment.
He kicks off in the paper by saying that AI is a brand tag, it's becoming ubiquitous, but a corollary of this is that there's widespread commercial deployments where AI gets things wrong,
whether it's autonomous vehicle crashes or chatbots being racist or automated credit scoring processes discriminating on gender.
Of course, we have a whole load of people saying that we can improve it, so Judea Pearl and Gary Marcus.
They say that deep learning is just curve fitting, it's just reasoning by association.
If only we could build computer systems that took things a step further and thought about time and space and causality.
Mark takes the AI skepticism to a whole new level because he thinks that machines cannot and will never understand anything.
Professor Mark Bishop, welcome to the show. In your paper, you talk about Cric and you talk about church and Turing giving rise to computationalism. What did those folks say?
In my paper, I started off with the idea that it's become known as Francis Cric's astonishing hypothesis that you and everything that we are is defined by a particular set of new or failing patterns at any one instance.
If we run with that idea to its logical conclusion, it would seem to be that if we have an appropriately high fidelity simulation of just the brain,
we abstract from the brain with all the dirty chemicals, the neurotransmitters, the serotonins and the like.
We abstract from all that and just look at the neural firings, we've got everything, everything else drops out for free.
A lot of people surprisingly buy into this idea and in fact it's one of the hypotheses that pushed the human brain project to one of the biggest European Union funding grants of all time a few years ago.
That courted a lot of controversy with some people saying that the EU is putting a lot of its eggs in one basket and a lot of people had doubts as to how much real science that programme would deliver.
I think Ham died a very interesting presentation to the group, the human brain project group a number of years ago because I was arguing as Tim outlined in the introduction that we were not likely to get conscious states.
In fact, I think there are good a priori arguments for believing we won't get conscious states that are very computational simulation, no matter what that simulation is,
no matter how fast it is, or no matter what algorithm it is, unless we have to bite the bullet and we accept a very vicious form of panpsychism, the idea that conscious phenomenal states are living in everything, the very cup of tea of coffee that I'm drinking at the moment as its own mental states.
The similar likes to pin my colours to the scientific mask, I find it somewhat implausible to believe that my coffee is conscious of me drinking it, and so we're led to reject that horn, and if we reject the horn of panpsychism then unfortunately we're led, in my opinion we're led to reject the idea that the mere execution of a computer programme can bring forth conscious states.
So that's kind of, in a nutshell, the executive summary, if you like, of an argument I described as dancing with pixies that purports to show that unless we're willing to accept panpsychism, computers will never have phenomenal states of consciousness.
Now that is a distinct argument from the argument that people like Sirle makes that says computers can't understand, and it's also distinct from people like Penrose who say there are certain things that people can do, and Penrose very famously talks about mathematical insight that are fundamentally non-computable.
So it's my own minor contribution to the debate, and that is I don't believe that computers can be conscious unless we're willing to accept a very nasty form of panpsychism.
So I think some of that impulse, right, to break things down, you know, and to find the absolute minimum component that can implement everything in human, I mean that's a very western kind of analytic, you know, scientific approach to start with, and so if we don't want to throw out analytics as a whole, like in what gap or let's say what do we need to add to kind of artificial neurons, if you will, in order to, or even to the computational point of view,
of the paradigm itself, so what do we need to add to say the Turing machine or Lambda calculus, you know, concept in order to be able to create consciousness. What's missing?
There's unfolds usually complicated stories, as you can imagine, and I'm hoping that at some point in time we'll get to go into engaging a little bit more depth on what the dances with Pixie's reducture actually says, because otherwise it just sounds like an airy, hands-waving philosophical statement that I'm making.
It might be quite interesting to go into the nuts and bolts of why I believe that argument works, but to come back to Keith's point, right, many people I think working in the field, as a young teenager, back in the 70s, I taught myself to programme, and I had an unhealthy interest in science fiction, and you put those two things together, and you're led to the belief that I had very strongly as a teenager that we would build thinking conscious machines, and that one day they would be able to do that.
They would come to tyrannise mankind and enslavers and go on to be the next age of evolution, if you like. I'm not alone in that. I'm sure many people have entertained similar fantasies, and it wasn't until I went to university, in fact my choice of degree at university was informed by these interests of mine, peculiar interests of a teenage male, and I went to read cybernetics at the University of Reading.
It was the only place in the UK where you could read cybernetics at the time, and that was a great education, not least as some of the people. My tutor, for example, Alex Andrews, was an early first round, a neural net pioneer from the sort of 40s and 50s, and famously gave a couple of great papers that the mechanisation of thought conference of people like Minsky and Rosenblatt were all out.
So I was surrounded by old school academics who were from that first wave of people working in neural networks, and that was a really interesting place to work. One of the guys that taught us, a cybernetics is kind of an engineering, and it touches on all sorts of things, but it weren't a sort in the UK with very much an engineering discipline.
So one of the things we had to do from year one was build computers, but not perhaps, as you might think of as building computers, are you going to get on board and putting a few chips in perhaps, but literally starting out with TTL and building your own half, building your own address decoders and building literally a computer from individual TTL chips.
Once you've done that, you get a very low level, but real engineering perspective of what's going on in a computer. You don't tend to think anything too fancy. The idea that these things aren't thinking machines start to become a bit bizarre, because you realise that machine has no choice in what it does.
If I imagine like a balance, a ruler on a pivot that's balanced, and press the ruler down one side, the other side comes up, it can't do anything but that. When you see that the operation of the logic gates in the computer work in exactly the same way, that seems to be a very different mode of operation that we're used to
entertaining when we think about human cognition. That said, all those thoughts were further down the intellectual line for me when I was engaging as an undergraduate. I was just interested in building these things.
I guess it wasn't until my third year, I was doing a joint degree in some editing a few of the science, and then one of my lecturers seemed to be the science was a guy I went on to be professor of theoretical computing at Oxford, a girl called Richard Byrd, and he introduced me to the notion of Turing non-computability.
He also introduced me to Gerda Leche Bach, but that's another very interesting book, which I'm sure you're all very familiar with. That was quite an intellectual shock, because prior to doing that course, I had this sort of, I was very modest in man, and I had this idea that you give me a problem in enough time, and I'll boss you out of the computer programme and I'll solve it.
The idea that there could be problems that were fundamentally non-computable was a shock, and it took a while for that shock to actually seep in. I was a bit sceptical, even though I knew what to write to get the reasonable mark in the exam. I can't say my heart was completely wedded to the notion of non-computability.
Not least, of course, the proof to these are kind of weird as well when you get into them. There's lots of self-reference and things, and they seem a bit sort of bizarre. They did to me as a young undergraduate anyway.
But then Roger Penrose, when I came back to read from a PhD at Reading, I was lucky to meet Roger Penrose just around the time he was publishing The Emperor's New Mind, or just before that time, and we got chatting and I realised that my intuitions about the horror that non-computability might pose were being echoed by someone who was even at that time known as being a bit of a polymath, the guy that taught Stephen Hawkins amongst other things and worked with him.
The fact that this guy was also echoing some serious reservations that were based on good leaner ideas and based on showing non-computability stuck a chord with me.
The real big intellectual shock to my, again, I started out in a PhD in Neural Networks with the intention of building a thinking, living, proving conscious machine, and then over that period of doing that PhD, my position changed to 180 degrees.
So, after meeting Penrose, the next big thing was I went to a conference at Oxford in 19, or when would it be?
It was around the time that Parallel Distributing Processing first came out, and Roman Harper-Clellan came over for the first time in the UK to describe backprop.
So, this is how long ago it is. I'm really quite old-skilled in all these things.
So, yeah, it was the first presentation of backprop at Oxford, and it's a massive conference.
There was a set-out conference at the main room, which probably held about 700, 800, was sold out, and they had two overspill theatres.
We were in the second of these overspill theatres with video links to the main stage, and we heard these presentations at backprop, which is all very exciting,
but the thing that blew my mind, having been brought up in an engineering discipline, cybernetics, was listening to two philosophers because that was quite odd.
And there were two philosophers with dinner and soul, and I'd never heard a presentation like it, because some like the sort of kind of measured, dry presentations of how to control the server mechanism,
or a new algorithm for quicksword, or whatever the hell it has to be in our computer science and engineering presentations.
I soon learnt that philosophers argue in a much more ffistic of kind of way, and it was a shock, but also very engaging.
And so I came across Searle describing his Chinese room argument, which resonated with me at the time, and hearing Dana poo poo this at the time in his own unique way.
So around that time, my thesis, I began to question where I was going, am I team, and are other people in the world who are doing exciting things when you all know,
how are we going to build these thinking machines? And actually over that time, and in the few years that followed, I reversed that opinion.
And the core intuition that drove all that, I guess, really was Searle's Chinese room argument.
No, I think you'll find a lot of sympathy. A lot of people that actually work very close to the AI, or what's referred to as AI when we're trying to sell things,
will raise an eyebrow in scepticism, because we know GPT is just a big pile of linear algebra.
The talk about the transformational effects really come from the business side.
You're among good company here. But to kind of go back, you said you wanted to touch on this dancing with Pixie's argument that you raised earlier.
And this kind of links into the notions of panpsychism and how this relates to, you know, if you want to take the idea that a computer can be intelligent,
that it can think, that it can understand things, then you end up concluding that anything can kind of have this.
You run us through that, like very briefly, people haven't read the paper, panpsychism argument, the dancing with Pixie's fallacy,
and kind of like how this all kind of flows into the conclusion that computers cannot be intelligent.
One of the actions in which this argument is built is the idea that computation is not an objective fact of the world, it's observer relative.
So I first of all want to give you a couple of examples that I think will underscore that axiom one, because a lot of people reject that.
It's a nonsense computation. What a computer does is a fact of the matter.
So again, I'll go back to my undergraduate days when we had to build, before we used TTL, we literally had to build these logical gates out of transistors.
So it doesn't get much more basic than that.
So imagine you built some set of transistors to perform the following electronic logic.
You have two inputs to the circuit and one output called inputs A and B and the output O.
If both inputs A and B are zero volts, the output is zero volts.
If A is five volts and B is zero, the output is zero.
If A is zero volts and B is five, the output is zero.
If A and B are five volts, the output is five volts.
What logical function, guys, is that performing?
And.
You might be right.
It's performing and if we assume that nought volts is false and five volts is true.
Now, if I tell you that you are actually wrong in your assumption that nought volts was true and five volts was false, what logical function is that performing?
Nand.
Oh, who remembers their bullion? There we go.
Oh, that's right. It's performing.
So in other words, the computational function, this bit of electronics is doing is contingent on the observer relative mapping between electronics and the world.
Right? If I use a nought volts, false, five volts, true mapping, it does anand.
If I use the inverse of that, it's doing an or.
You cannot tell a Martian from Planet Mon couldn't look at that and say, that's an and gate, that's an or gate without knowing that mapping.
And that mapping is subjective.
I might have one mapping, Alex might have another, Keith might have another one.
In fact, a great Israeli computer scientist Oran Shigre extends this argument pathologically and looks at multi-level logics and the problem gets really weird if you go down that route.
But I'll just stick to the simple case with the and and the or function.
So that's one of the reasons why I said fundamentally, I mean, it seems to be just axiomatic.
I just baffled when people tell me this is not the case, but I'd still occasionally meet people who dispute this.
So there's a second follow through argument.
And it's built on work of Winograd and Flores in their book understanding computers in coordination when they start to think about what is a word processor.
And I've reframed the argument a little and I think about what is a chess program.
I don't know that any of you guys are old enough to remember in the 70s.
We used to have these little chess plastic chess computers that square.
They had little holes on a bit of board and little tiny little plastic chess pieces.
And when you made your move, you lifted piece up and pumped it where you wanted to go.
And then the computer would light up the piece you had to move and where that was to go.
Using one of these gadgets you could, I could quite happily play.
I'm not very good chess player, so I could happily get thrashed by these machines day in, day out and enjoy that thrashing, so to say to speak.
And I could use that piece of computational equipment to play chess with.
Now in the UK there's a famous conceptual artist by the name of Tracy Emin, who does a lot of work with neons.
I don't know whether you guys have come across at work at all.
And also in the 60s there was a big movement in what's called kinetic art, where you're in cybernetic art, where people interacted with art pieces.
So now, what wouldn't be nice to me, Tracy's sabotaged my chess computer.
She's ripped the innards out and she's now wired all the inputs to pressure pads in an art exhibition, in an art gallery, and all the outputs to neon strips.
So when people walk over these pressure pads, different neon lights come on and off.
Now there was no sense that you can possibly, it seems to me that you could possibly say that when I walk around the art gallery I'm playing chess,
I'm interacting with a bizarre piece of abstract art, certainly not playing chess.
So it doesn't seem to me there's anything intrinsically chess-like in this device.
Yes, it was engineered very carefully so that if I knew what I was doing I could play chess with it.
I could use it in other ways as well.
And the problem gets even worse if you've come across isomorphic games.
That's probably all you know is Noughts and Crosses.
Now imagine you've got a Noughts and Crosses game on your iPhone and you've got it like I have a six-year-old daughter who's just about got her head around Noughts and Crosses.
I can keep her occupied for, I was going to say half an hour, that'd be a exaggeration for five minutes, say, giving her this thing and she'll play Noughts and Crosses happily again.
How are daddy on board? What am I going to do?
I say, ah, well I've got another game I can show you.
But she says, daddy, you've only got one game on a computer so I'm going to play it against a computer.
So I say, don't worry about that, we've got the Noughts and Crosses, I've got another game, I'm going to call it Computer Wist.
Now imagine you lay the deck of cards out ace through to nine.
And we take it in turns to pick cards from this deck.
The winner is the first person you can get 15.
Get the cards to seven to 15.
It transpires that if you've got a program that can play Noughts and Crosses with a suitable mapping, you can get it to play a perfect game of Computer Wist.
So we've given it a grid, just like a magic square, where all the verticals, horizontals and diagonals add up to 15.
You then plot your computer go, it's marking a one square and choose your go.
You can tell you which card to pick next and you can play a perfect game of Computer Wist.
So I can use that same computer program with my mapping to play a perfect game of Computer Wist.
So you cannot say in advance, without knowing what I'm going to do with that program,
whether I'm going to play Tic Tac Toe or Computer Wist.
So I think those three arguments together make to me a persuasive case to paraphrase Wittgenstein
that the meaning of a computation is in its use by human computer users.
The phrase that you're all aware from Wittgenstein on paraphrasing is from the investigations where it makes the claim that the meaning of a word is its use by human players with human language games.
And I think the same applies to computation. The meaning of a computation lies in the use that we as human users of computers put that to.
So that's kind of setting the stage.
So I think there's always going to be this mapping at the physical level,
and then there's always the idea of what we're going to use a computation to do,
and that's a very social human activity.
So that's setting the stage where I want to go with it, answering the pixies.
Now the next move I'll make, I know you'll have all of read,
Computing Machinery and Intelligence, Turing's famous 1950 paper.
Everyone's at least looked through this.
Turing first outlined the Turing test, what became known as Turing test?
Also in that paper, he outlines the operation of a very simple machine.
Turing's discrete state machine, as it became known,
and this is a beautifully simple machine.
It's a disk-like device, and it can go round in 120-degree intervals,
and it can stop at the 12 o'clock, the 8 p.m. and the 4 p.m. position as it moves around a clock,
and it can exist in each one of those discrete positions.
And we can describe the operation of that machine as a finite state automaton,
if the machine is in state A, and the next clock tick is going to go to B,
if it's at B, the next clock tick is going to go to C,
and if it's at C, it will go back to A again.
Now, if we want to, we can arrange that when the machine is in computational state A,
it will do something when it's in computational state B,
it will do something when it's in computational state A,
a light would come on.
You also imagine there being simple input to the machine,
like a big leave-a-break mechanism that you could have on or off,
so if the machine was in state A and the break was on,
it would remain in state A.
If the break's off, it would go to state B.
One of the first interesting things, again, like computation,
when you're given a Turing discrete state machine,
to read off the computational state A, B, C,
we need a mapping between the physical position of the leave-a-machine
and the computational state to which it refers.
We may define computational state A to be the 12 o'clock position,
in which case when the leave is there, we're in A,
or in a bit, so it's at B, and the rest thing follows through,
but we always need that mapping.
We've always got to do these mappings between the physics of what's going on
and the computational state that we're instantiating.
Now, we've got this machine.
Without the break, it just goes to A, B, C, A, B, C, A, B, C, A, B, C.
That's interesting enough if you're interested in inputless finite state automata.
They're not very exciting machines.
All they can do is go through a cyclic series of states forever
in an unbranching series of state transitions.
What is interesting is that the appendix to Hilary Putnam's
representation and reality is a little-known proof.
This shows how we can, effectively,
how we can map the operation of any inputless finite state automata
onto a large digital counter.
Actually, Putnam goes further and shows that we can map it onto any open physical system,
an open physical system being a physical system that's open to gravitational waves
and all the rest of it.
Electromagnetic spectrum impinge onto it.
But for simplicity, let's just consider the, without lots of generality,
let's just imagine we can map the operation of any inputless FSA onto a bloody large digital counter.
How does Putnam do that?
Well, let's take Turing's machine.
It just says, if the computation is in state A,
I'm going to map that to the digital counter state 0, 0, 0.
If it's in state B, I'll map that to counter state 1.
If it's in state, computational state C, I'll map that to counter state 2.
And then the A again will go to 3.
The B again will go to 4.
The C again will go to 5.
And then we go over any finite time period,
we can replicate the state transitions of our digital discrete state machine
by the numbers that we're cycling through on our digital counter.
And again, you might answer, so what?
That doesn't seem particularly threatening result for computationalism at first sight,
because real computations are much more complex devices than inputless finite state automata.
Well, in a paper called Does a rock implement every inputless finite state automata,
David Chalmers responds to this argument in an interesting way.
He says that, yes, I'll concede if you like that we can implement really trivial machines
like inputless finite state automata using Putnam's mapping,
but when we want to look at machines with input, this breaks down
because we get a combinatorial explosion of states that we need.
Chalmers introduces a very neat construction called the combinatorial state automata,
which we can implement using Putnam's mapping,
but an exponential increase in the number of states that we need.
And the combinatorial state automata is sensitive to initial conditions.
And so it could be genuinely said if we could implement it,
we have an interstate to implement it,
it could generally be said to be implementing a computation with input.
But at the cost of every time step of the computational, you need an exponent.
Your number of states grows exponentially and Chalmers makes the point
that after a very short number of states will run out of,
the number of states needed is bigger than the number of atoms in the known universe,
and hence Putnam's mapping must fail.
And that's kind of where I entered the debate,
because I made an incredibly trivial,
all the hard work had been done long before I came to play with this game, so to speak.
But my only trivial modification to Putnam's argument
that to me makes it robust to Chalmers is to say this,
well, if we look at any real machine of which it's claimed
has genuine mental states, conscious states, as it interacts with the world,
and this intuition was brought real for me, because again,
some people dispute the fact that there are people who believe,
there are serious scientists who believe in the machine consciousness programme,
there definitely are, and I used to, my head of department is at Reading Cybernetics,
one of those people, a guy called Kevin Warwick,
and we at Reading had built these little simple robots
that moved around a corral controlled by a neural network,
and Kevin said, well, these got roughly the same number of neurons as a slug,
and it's pure human bias if you say a slug has conscious experience,
and these robots didn't, and I thought that was a ludicrous claim,
and that inspired me to move and develop this Dancing on the Pixies reductio.
So to come back to the case, I said, right then, Kevin,
if you say your robot as it moves around a corral over a finite time window, T1 to TK,
experience is something that is like to be a robot bundling around a corral,
not bumping into things, I don't know what that is,
but I imagine it has some conscious experience.
What I can do is log all the inputs to that machine,
and then I'll play them back to them.
So I now lifted the robot out of the corral, I've disconnected all its sensors,
if you like, and actuators, and I'm just injecting into the robot
the states it would have got where it was around the corral on its own.
Test again, does the machine still have conscious states?
It's reading the numbers from a latch,
the data was originally taken from an ATD converter for argument's sake,
we're now plonking that data in there from a data rejection system,
but the computer still has the phenomenal states, so Kevin Warwick asserted,
and that, unfortunately, did the problem that he was going to encounter,
because if that was the case, all that were really interesting,
we can collapse the exponentially growing number of states
that Chalmers showed we would have,
if you actually want to implement fully all aspects of a computation
using Putnam's mapping.
If we just try to look at the particular computational trace,
we just need the input to that machine that pertain to any over time
as the machine, it is a little thing,
and then we can remove all the counterfactual states,
and once we've done that, we've got a linear series of state transitions
that we can reliably map using Putnam's mapping,
and hence, if it's the case that Kevin Warwick's little robot was conscious,
then so was our account to be conscious,
and then, after, Putnam, any open physical system.
So, that, in a nutshell, is the DWP reductio.
It's interesting that in all these seminal debates to me about AI,
about Penrose, about Serlin, about my own small contribution,
there's a lot of confusion, people can very easily misinterpret what's being said.
A lot of people got hung up about, does a rock genuinely implement a computation?
And I think, to me, charmer's completely proved that it does not, right?
No problem with that.
Can we make a rock with a suitable mapping
implement an arbitrary series of state transitions?
Yes, I think we can, and I think we can make any count to do that.
And because we always use a mapping, whatever system we use,
I don't think I'm doing anything.
There's no sleight of hand involved here,
because all computational systems involved
and observe relative mapping somewhere along the line
to get them to work,
whether it's only assigning a logical two to five volts
and a logical false to not volts.
So, I don't think the use of a mapping is something that, you know,
there's no sleight of hand involved in that.
Given that, I can use an arbitrary complex series of state transitions.
So, then the question is, if you're a physicalist
and I approach this problem originally as someone who, you know,
I like to think of myself as if I'm not a misstewner,
I don't want to appeal to some supernatural forces to bring forth my consciousness.
And at one point in time, it was the case that,
well, if you don't believe in functionalism or computationalism,
then you've got to believe in supernatural effects.
Well, that is no longer the case.
Cognitive science has moved on a lot since the 1960s.
There's an awful lot of new tools in town.
And these are really exciting tools in my view.
And you highlighted a few in the introduction to him,
but things like the embodied, inactive, embedded and ecological approaches
can go a long way to answering or giving his insights into these questions
without having to bring forth particularly supernatural notions.
So, I'm going to put that to one side.
We no longer faced with a choice of either accept computationalism
or accept mysteriousism.
Putnam's rock.
I mean, there's a lot of interesting responses to it,
but I want to point out a couple of things
or maybe just ask you about a couple of things.
So, one thing is that, first, I think what your goal is,
and correct me if I'm wrong with the pixie dancing with pixie's argument,
is to say simply that if we accept, say, turing complete computation,
or even in this case finite state machines,
but I think we can probably go one step further,
if we accept that effectively computable systems can implement consciousness,
then we also have to accept pan-psychism, correct?
That's what I tried to show, Graham,
because once you have a system that you claim,
like my boss Kevin said,
that machine is conscious,
but I can look at what happens as that machine interacts with the world.
I can log all the inputs to it,
I can trace the flow of the execution flow of the machine code
that controls the robot,
and then I can implement an arbitrary series of state transformations
that would do exactly that with an appropriate mapping
with a mere digital counter.
So, if that machine is conscious,
then my digital counter would plus this mapping must be conscious.
So, that's the position I arrived at.
Now, when chatting to David Chalmers about this,
he said, oh, no, no, no, Matt,
you've gone off the road because we need the full potential
of the computation to be there for functionalism to hold.
Now, this is quite a mysterious view.
In fact, it was so mysterious that when he first said it,
he had to repeat it about three times
because I'm not the quickest at uptake,
and I found it so bloody bizarre what he was saying,
but when I did unpick what he was saying,
the Chalmers you actually need,
once you effectively slice off the potential counterfactual actions
by saying, well, I know the input at this point in time,
I know the input at that,
I'm going to replace the counterfactuals in my program
by direct go-to statements, if you like,
or just omit, snip them from the program.
To me, that couldn't possibly affect the phenomenal state of the system
because really what he's saying is that non-entered branches
of the computer program have a cause and effect
on the phenomenal state of your machine,
but the bizarre thing is, that's what David's saying.
You've got to have the potential for counterfactuals there
because otherwise we don't have the machine
genuinely instantiating phenomenal states.
So, if we just assume in argument, though,
that we don't accept panpsychism,
and that these arguments prove that if we accept computationalism,
it implies panpsychism.
We also have mathematical results that say,
let's say, turing computation or effective computation
encompasses all computation.
There is no other kind of computation
unless there's hyper-computation.
I believe, and correct me if I'm wrong,
but I believe what we're saying is that there exists hyper-computation
and that human minds are performing hyper-computation.
So, just take this back to the rock for a second.
One issue with that mapping is that rocks actually have physical states
that may be real numbers.
They may have values that in and of themselves are not computable.
They can have positions and states and quantum states
that have values that are essentially defined
by an infinite precision real number
and therefore are not even accessible to computability
to start with, like even describably.
I'm always looking for where consciousness is hiding, if you will.
Are we saying that it's hiding in real valued states,
maybe quantum states like Penrose would say,
perhaps in microtubules or something like that?
Is that a form of hyper-computation
and is that where our consciousness derives from?
Where do we draw the dividing line?
I've read quite a few of your papers in preparation,
you do when you're going to speak with someone.
This dividing line where we go from computational
and essentially impossible to achieve intelligence or understanding
or any measure thereof to the point where we have an intelligent system
that can understand its world and redefine itself and redefine its world,
that distinction is not terribly clear.
As we dive into this question,
I want to drive towards where this distinction lies,
if this distinction exists.
To pick up on Keith's point first, I think,
I'm neutral.
Penrose has given a positive thesis as well as a negative one,
so famously in The Emperor's New Mind,
he gives his first version of a girdledean argument
that purports to show that mathematical insight is non-computable
and then says, well, this suggests to me that non-computability lies
at the heart of what it is to be human,
and then with Stuart Hammerhoff, they outline a positive thesis
which purports to show that non-computability can arise in the brain
through the orchestrated collapse, quantum collapse
in the microtubule skeleton of bone neurons.
I'm perhaps neutral on this.
I know that when Penrose held the psych symposium on his work in 1995
and attracted a lot of responses, well over 20,
and I don't think any of his logical work
was seriously brought into question.
Even though that was actually a naive interpretation of girdle
compared to the work that he put out in Chadows of the Mind,
it was a much more nuanced approach to the argument, in my opinion.
Nonetheless, it wasn't seriously criticised.
Nearly everybody criticised his positive thesis.
Penrose is a clever guy. It seems interesting.
I'm not going to hang Michaelers on to that flag in particular if it works.
I've been more drawn to modern approaches to cognitive science
which look at the end.
There isn't unfortunately a very quick six-page paper
that can lead people gently into this, but there are four schools
that all this work really started out with the work of a roboticist from MIT
called Ronny Brooks, who wrote a classical paper
which you guys are familiar with called Intelligence Without Representation,
which basically looks at you instead of trying to...
When I did robotics, old-fashioned, old-school,
representational robotics as a young postgrad,
a lot of our work was trying to take data from sensors
and build rich internal models of and out of their world.
I remember we spent all our budget on buying the biggest,
fattest computers we could possibly afford at the time
strapping them onto these poor little autonomous vehicles
and they were laden down with computer power
and they moved absolutely, tragically slowly.
We're going back in the early 90s now,
but they were pathetically slow things, really embarrassingly bad.
Because a lot of their work was trying to build up these models,
trying to build models of them.
I know that these days we can do that kind of thing bloody quickly,
but back in the day you couldn't.
Brooks thought, well, do we need to do it?
And that paper argued that we didn't.
Why build the representation?
We can use the world as its own representation.
In a sense, that sort of paved the way for thinkers like Francisco Varela
then in a book called The Unbodied Mind with Evan Thompson
and Eleanor Roche to start thinking about different ways
of doing cognitive science.
The Unbodied Mind is a mind-blowing book.
It throws your whole view.
In another way to go to Lesha Bart can be quite mind-blowing as a young kid.
This is mind-blowing as well, but in a kind of a weird way,
because it actually questions the existence of a fixed pre-given out there world.
That was quite a shock to me when I first came across these ideas,
not being a trained continental philosopher there.
My first mode of engagement was with Varela,
who incidentally started out as a theoretical biologist
and then someone very active in the A-life community.
I think his initial work has also been from a science perspective,
but he engaged quite deeply with the European philosophy,
which at that point in my life I was sort of ignorant of.
This led to the development of alternative schools
of what cognition is all about.
The inactive school is one that I'm interested in,
and it says that effectively we can look at one sort of inactivity itself
as split into numerous approaches.
One of these, developed by a guy called Kevin O'Ragan and Alvin Airey,
argue that visual consciousness is something that we do.
So they're moving away from the idea that vision is like interpreting
like your eye getting a scene from the world on your brain
having some little like a cinema which you're then interpreting
what all these little bits do.
They make the case that vision is more akin to an activity is what we'll do.
It's guided sensory motion exploration of the world.
Varela herself was particularly interested in,
at broad being in ideas of autonomy,
how can things become meaningful?
There's all these very complicated debates
that I think to try and come back to your question, Keith,
and your question, Alex, you need to touch on,
but it's really challenging to touch on them in an intelligible way
in a relatively short period of time.
You would at least agree, though, that your contribution,
penrose, et cetera, points very strongly that
there's something embodied, there's something physical
that we haven't quite figured out yet.
Maybe it's microtubules, maybe it's something else,
you're agnostic to that, but there's something physical that allows...
My intuition is to do with autonomy,
and this is why I bring that idea of altrupoesis
and that Tim mentioned at the beginning.
This is, again, in the 70s,
Ymburto Machurana, the Chilean side musicians,
Ymburto Machurana, Francisco Varela,
came up with a theoretical device
for delineating life and non-life.
Of course, astonishingly, this has been a really difficult problem.
You'd think it was probably solved 100 years ago,
it hasn't been.
Even as recently as when Margaret Bowden was writing on this,
one school tried to say life is,
and then give enumerated a list of properties
that has to metabolise, it has to reproduce, bloody, bloody, bloody.
So, Varela Machurana looked at it from a different perspective.
Fundamentally, life is a system
that has a circular organisation.
It's got to be able to maintain its own boundary
of itself and the other,
and it has to encapsulate the rules,
the automatic rules that maintain that boundary
in the face of a changing environment.
Can we spoke to Friston, Carl Friston, quite recently,
and he was talking about Markov Blankets,
which is quite interesting about how do you define
the boundaries of a physical system?
Does a hurricane have a Markov boundary?
What we're talking about here, in a very general sense,
is defining boundaries between what lives and what doesn't live,
and what is meaning, and what isn't meaning,
and what is understanding, and what isn't understanding.
It's very philosophical.
It's quite difficult to pin this down.
If you look at Machurana and Varela's book,
it's about 60 pages, the original treatment,
auto-presis and cognition from the 70s,
and it is very dense.
It's not a waffle-y philosophical book.
It's quite hard-core, quite mathematical.
I think they do do an interesting job pinning down
if you like what it might be for something to be alive.
This challenge, which was explored in the embodied mind,
has since been developed by Evan Thompson,
who is an American, an interesting philosopher
who wrote a book called Mind in Life,
where the argument is laid out that life is a continuum,
and wherever you have this continuum of life,
then you have a proto-mentality.
I'm kind of drawn to that.
I think it's a very persuasive argument.
Then we have to look at the question of what constitutes
autonomous systems, and why should it matter
if an autonomous system has a phenomenal sense
of what it is like to be?
Here, if you like, you can link into the work of a guy
I've just recently come across who wrote to me a few weeks ago.
He used to run a big lab in France,
an AI lab in France,
Michael Trouble, I think his name is,
and he argues that we need phenomenal consciousness
to arbitrate between different actions
if you're sending a robot to a Mars
that's got to be completely autonomous,
and it's got to react appropriately in unknown environments
to all sorts of different threats.
Effectively, the robot's got to have to know
something that's a state that it's good to be,
that makes the robot feel pleasant,
and a state that's horrible, that's dangerous,
that might cause death to the robot.
We use the phenomenal sense of what that feels like.
We can then use that to arbitrate between different actions.
This is actually, by the way,
there's an idea that was brought,
I first came across through a paper by Daniel Dennett
called Cognitive Wheels, the Frame Problem of AI,
when he looks at what must be known to arbitrate
on what you call the cookie problem.
Imagine you've got a big jar of cookies
and some little kids like my six-year-old daughter,
and two families, one next to each other,
and in one family, when the child goes for a cookie,
the family beats it, smacks it relentlessly
until it's in tears and never goes,
it doesn't have any more cookies after that,
and the other one, they're very sort of touchy feeling,
oh no, please don't have another cookie tarquin,
and occasionally the tarquin does go and have another cookie.
Dennett asks the question,
why is it that beating your kid
causes that child not to go for the cookie jar anymore?
We know that well,
because being beaten is something deeply unpleasant,
and you don't particularly, unless you're a masochist,
want to do things that are going to bring this feeling
of pain about you.
But then Dennett says, well, how do we know that?
We can arbitrarily hard-wire six facts in,
so I could hard-wire into my computer programme.
If something else biffs me,
then I'm going to increase my pain by one,
and if pain gets over a certain threshold,
I won't do that action again.
That's incredibly brittle, it's a little arbitrary,
but unless we have phenomenality,
unless we have access to phenomenal states
and know that getting biffed hurts
or going over rough terrain if you're a marginal robot
that takes you around a bit,
you have to just sidestep that by hard-wiring effectively,
hard coding, because as engineers,
and the system is no longer autonomous now,
we're having to define what it has to do
for all these different possible states.
That's the price we have to pay.
So I think you can argue that evolution
is blessed us with phenomenal consciousness
so that we can act autonomously,
and that's the way that, I guess,
after Evan Thompson and Varela,
the view that I come to.
So we need consciousness to enable us to succeed evolutionarily.
Well, haven't we just fallen foul of the prime axiom
of software engineering at this point
that every problem is just a level of abstraction away?
I forget which paper it was.
You had a really great conclusion in one of your papers
where you essentially said,
if we built a bunch of robots that looked and behaved
just like us, they're autonomous,
they laugh at our jokes, they respond,
but the thing that defines the difference here
or the difference between us and them would be
that when we're laughing and feeling we feel it,
it's phenomenalogical.
When they do it, it's not, it's a simulation.
But, you know, leading into this argument,
it's like, well, to have these Martian robots
that have autonomy and can succeed,
they need to have this phenomenalogical state.
Isn't this really just a software engineering problem
away from being solved?
I think when I wrote that paper,
I mean, it's only very recently, literally,
and I'm hoping to work at least to reach out to Michael
to see if we can do something together.
I'm quite excited by these essays he's sent me.
I think they make a business.
I don't know why, I just haven't encouraged me
that this could be a reason why consciousness has evolved.
He makes a very persuasive case.
I'd love to claim it as my idea, but it absolutely isn't.
But I think it's a very beautiful one.
I need to understand more and either he will push it
or perhaps we might do something together, I don't know.
But, yeah, so I'm a little bit sceptical
that without that glue of consciousness
that we could get a machine to act
as a simulchrum of you in all possible cases.
I was arguing from the stronger, I guess,
in that paper that, well, that's just assuming that we can.
I think engineering-wise, I'm a little bit sceptical now
following Michael's work that that is going to be possible.
But also to come back to another point
that relates to the...
We can talk about the Chinese room with Paul,
or are you assuming that's boring for all your readers?
No, no, I think that's one of the most important things,
because we're talking now about consciousness
and the various different kind of boundaries
between what is and what is not consciousness.
But I think the other one that's really important is understanding
and the boundaries between what is and what isn't understanding.
You say in one of your papers,
what does it mean for a central processing unit to understand?
Does it understand the programme
and its variables in a manner of analogous
to cells understanding of this rule book?
But this cells rule book thing, right?
It describes a procedure, as you say in one of your papers,
that if carried out accordingly,
allows cell to participate in an exchange of uninterrupted symbols,
squiggles and squaggles,
which to an outside observer look as if cell is accurately responding
in Chinese to questions in Chinese about stories in Chinese.
In other words, it appears as if cell in following his rule book
actually understands Chinese,
even though cell trenchantly continues to insist
that he does not understand a word of the language.
So this has been used, I think, very reliably,
and you had a paper, actually, recently,
introducing a couple of other responses,
because there were four responses to cell's argument, right?
There was the robot reply, the systems reply,
the brain simulator reply, the combination reply,
and in your recent paper, you talked about robots and animats.
In the target VBS article, there was a lot more than four.
So when he wrote the paper,
came up with four possible counter-arguments,
which are the classic ones that Tim just outlined.
But from memory, there must have been over 20 people,
really big names in philosophy and AI
who responded to that VBS target article.
People like Marvin Minsky, McCarthy,
Dana, obviously.
God, I can't...
I'm ashamed to say I've forgotten,
but big, big names who wrote different responses.
There's a lot more than the four that cell...
I mention these because what's been bizarre,
having...
I edited today with John Preston,
you edited on the 21st anniversary
of the Chinese re-argument,
John Preston and I put together an edited collection
of responses to it 20-year-one years on
from leading AI scientists,
cognitive scientists and philosophers,
and I still think that's a good collection of essays
that we picked and good collection of people
that contributed to that volume.
And then, again,
and the intervening goes between
this Chinese re-argument coming out
and that volume coming out,
and then between that volume coming out in 2002 and 2002,
and now, really, and I've talked about this
as you can imagine, in many places,
most of the UK universities
and quite a few in Europe and one or two in America,
and nearly all, the most formidable responses
that I've come across really go back to responses
that I've actually predicted,
and by far the most common and probably the most...
I think the strongest response is some variance
on what became known as the systems reply
that you mentioned, Tim.
Can you just quickly define what the systems reply is?
Do you mind if I just go over again,
as you went up really, really quickly,
the essence of the argument,
but I'd like just to sort of unpack it slightly more slowly.
So, so imagine...
I'd like to see in cells a monoglott English speaker,
shamefully like myself,
shamefully because I'm married to a Greek lady,
I still can pretty well only communicate in English at best,
and then the cell can as well.
So he imagines himself locked in a room,
and this room has effectively got a letter box
instead of a door to which he could communicate
with the outside world,
and in the room are three piles of papers,
and on these papers are strange symbols
that cell doesn't know what they are.
We know, there's people reading about the experiment,
hearing about it, that these are actually Chinese ideographs,
but to cell they're just uninterpreted squiggles and squabbles.
You've got no idea what they are.
So you've got these three piles of things.
And on the desk there's a big grimoire,
a book that tells cell how to correlate
symbols from the first pile with symbols on the second,
and then other rules that tell them how to correlate symbols
on the first pile with the second pile,
and also linking symbols on the third pile,
and other rules that tell them how to take symbols
from one of these piles and stick them to people
through their throats to the outside world.
Well, unbeknown to cell,
the first pile defines a script,
a second pile describes a story in Chinese,
and the third pile describes questions
about that story in Chinese,
and the symbol cell was told by the book
to give to people in the outside world,
answers to questions to that story in Chinese.
And cell's point is that if we concede,
so he's arguing again from the stronger side,
he says, okay, let's concede,
but that rule book, however it's defined,
and again, a lot of people got home
because they thought cell was purely talking
about a naive pattern matching programme.
If this symbol doesn't then do that,
actually cell makes it clear, if you read the paper carefully,
that he wants us to stand for any conceivable computer programme.
This was the first reaction that I had.
I had an allergic reaction to it,
because as we know from talking to Wally Tsaba,
we can't write the damn compiler for language.
It's too complicated, so it's not possible, really,
for even us to explicitly understand and verbalise
the rules that we use in language.
And you said yourself that artificial intelligence practitioners
were incredulous at the extremely simplistic view of cell
that you could have this low-level rules described.
Yeah, but I mean, if that's because
they didn't read the paper carefully,
because cell makes it absolutely explicit that he generalises.
He gives a simple example to sort of just get you thinking
about the problems.
And imagine, that's again, this is the world that some people
have tried to do language understanding in this very naive way.
But cell wants the thought of the standard for any possible programme.
All we're doing is...
The rulebook tells cell how to manipulate uninterpreted symbols
and put uninterpreted symbols out of the door.
How it does that, whether it's implementing a neural network,
whether it's implementing a genetic algorithm,
whether it's implementing Wally Tsaba's sense-based word effect,
sense-based word effect,
compositional understanding, natural language understanding,
whether it's doing GPT-3 kind of operations, it's irrelevant.
Cell says, whatever your programme is, that's what's in the book.
At the end of the day,
that programme will tell me how to respond to questions in Chinese
with answers in Chinese.
If I follow that programme carefully,
you can don't make any mistakes.
I'll give answers out the door.
If your programme's any good,
it will give answers that are indistinguishable from those
a natural and native-speaking Chinese person would give.
Even though, as Cell throncially insists,
following this programme
is not enabling to get even the toe-poll in Chinese semantics.
All I've been doing is like a mega-fast idiot so far
to manipulate uninterpreted symbols around
and sticking some symbols that I don't know what the hell they are
through a letterbox in and out to the outside world.
Does that imply, then, that it's just observationally impossible
to determine whether a black box is conscious?
Well, I wrote a cancer argument to Susan Shryde.
I can't pronounce the name for Susan Shryde's.
Cheering Test for Machine Consciousness,
where I make that exact claim.
This was in frontiers,
in one of the frontiers journals a couple of years ago.
Because Susan says,
oh, we can ask her ideas if we ask questions
that are about particularly human activities relating
to phenomenal experience,
we'll be able to tell whether this machine is,
she gives a procedure for doing this.
I would say, well, if whatever set of questions
Susan you have for deciding whether your machine is conscious,
I'm going to sit unbi-netched here and watch you ask them.
I'm then going to go away and write a little programme in BASIC,
because I'm good at writing in BASIC,
that says, if question one says bladi bladi bladi,
are you Susan's first question, give this answer,
which is the answer that a really complicated machine consciousness programme gave.
In fact, I have a look-up table,
but of course Susan doesn't know now,
because I sneakily switched the machine.
So she asks her questions thinking she's talking
to a really complicated machine consciousness thing.
She asks the questions which she claims
will tell her whether this machine is conscious,
but she's actually just interrogating
with a really simple look-up thing.
And at the end it gives her the answers she wants.
She says, yeah, that's conscious,
but she's just been talking to a look-up table.
I think that you're quite right, Keith.
To me, are we going to be able to do a test
for machine consciousness purely on the basis of external observation
in the absence of anything else?
Because if we're Machiavellian, we can always cheat.
The thing is, when you were talking about it doesn't have semantics,
I want to pick that a little bit as well,
because you said the robot rover on Mars.
The semantics there were the state spaces
of all of the sensory experiences,
and then you said it's brittle and there's an alignment problem.
I can understand that, it's very similar to the AI alignment argument.
But this is different.
If you can replicate, let's say you're talking to a black box,
and you can't distinguish whether or not it has consciousness
or whether it has understanding,
why is there an issue with semantics in that case?
Ha, you're wrong fitting it.
I thought you were going to say, how can I tell that anybody understands
what is conscious, which is actually one of the core responses
that's all anticipated in the Chinese room argument.
That is actually the obvious question as well.
Presumably, you think that we are conscious,
because we might exist in a computer simulation, right?
Well, I don't think we can, because I know that if I slap my face,
it hurts, right?
I know that machines can't instantiate phenomenal consciousness.
I made a paper called Refuting Digital Ontology,
just after an invited talk at the Royal Society
a workshop on the Incomputable, hosted by Barry Cooper,
who is the leader of the Turing centenary celebrations in the UK and worldwide.
I made this very argument then, that it is clearly obvious to me
that we're not in a computer simulation, because I feel.
If my Dancing with Pixies with Duck here argument is correct,
unless I'm willing to accept Pantagism,
then computations can't realise sensation.
So either my Dancing with Pixies reductu is wrong,
in which case I'm very happy sad,
but in the sense I'll be happy when you show me where I'm going wrong.
Or if I'm right, then we're not living in a computer simulation.
So I don't think we are computer simulations.
Furthermore, it's an axiom of cognitive science that other minds exist.
So it's not for me to have to explain why I believe
that you three guys have phenomenal conscious states.
That's part of cognitive sciences is acknowledging that you do
and trying to come up with a theory that explains why these occur
and how they occur.
The conscious state argument to perhaps play devil's advocate a bit here.
To me, when I first came across the Dancing with Pixies argument,
this wasn't an argument that, for me, implied rocks had understanding or intelligence.
To me, it implied that there's nothing that has intelligence
or understanding, right?
What the intuition I'm wanting you to come to is that computation
doesn't have those phenomenal consciousness states.
Again, interestingly, what a reviewer said this of one version,
I think of the 2009 cognitive computing paper,
aren't you arguing too strongly?
Doesn't this prove that nothing can have consciousness?
No, it doesn't.
It just says that the operation of a digital computer,
a finite state automata, to pin it down more precisely,
cannot give rise to conscious experience
unless conscious experience is in everything.
That's all it says.
Now, if you don't bite the bullet and say,
well, actually, I think I'm more than a finite state automata.
Thank you very much, Alex.
There's more to me.
I'm an embodied entity.
I don't just think in my brain, I think with my body
in the world.
And again, there's a beautiful result that came from a guy,
and I'm going to argue, this is a professor at Galsmys,
and I'm going to take his work and extend it probably in ways
that he would be uncomfortable.
So this is my interpretation of work that's published
in a number of papers in Nature by Jules Davidoff
on colour perception by the Himba tribe in his work in Africa.
I'll describe this beautiful experiment because I would argue,
Jules is very cautious academic,
and he doesn't make wild proclamations
that I'm more comfortable looking at his work.
But basically, Jules did this beautiful experiment
over a long period of time working with the Himba tribe
in African colour perception.
And when I first saw them, again, they blew my mind
because they basically appeared to support the idea to me
that language informs not just the way that we package the world,
the way that we label it, that's kind of obvious,
but the way that we see the world.
So how did Jules' work show this?
Well, he took a series of colour slides,
like Munsell colour slides, if you're familiar with these,
well-precised blocks of colour,
and on one piece of paper, for argument's sake,
there were different shades of yellow,
and on another one, for argument's sake,
there were different shades of green,
but the difference between one side and the next
was the same colour difference.
Uniformly, so they went from a sake of dark green,
if you like, to a light green or whatever,
the uniform colour difference was one of the slides.
And on the sort of ochre yellowy one,
if you showed that to Europeans,
said, which is the odd one out?
If you looked really carefully,
not always, but often you'd say the one at the two o'clock position,
but it took you a lot of time,
and you literally actually, in your mind's eye,
compare all the slides with each other,
and if you were lucky, you said, yeah, it was that one,
but I've done this test on them,
and sometimes I don't even see it myself, it doesn't jump out.
It's not obvious, it doesn't pop out,
and some of the times I make that decision wrongly.
Now, you show that to the Himba,
and they sort of smug up, bang!
It's the two o'clock one.
Immediately, now with cognitive processes, it leaps out of them.
And then we get the green ones.
There's a blue-style.
So they show that to Europeans.
What's the odd one out?
The blue one.
Show that to the Himba, and the guy's got a stick,
and he's rubbing his head,
and he's rubbing his imaginary beard,
and he can't work it out.
Now, when you look at that from a western linguistic perspective,
how can the person not see the blue one is the odd one out?
Well, it transpires.
The Himba got very restricted,
or very limited colour vocabulary,
compared to Europeans.
And no doubt, if you took someone from the Himba
and threw them into London after a period of time,
there they would perceive the blue pop out
just as quickly as we do.
It's nothing to do with the Himba's colour system.
It's an effect brought on by language.
But to me, what makes this really powerful
is that it's a pop-out effect.
The Himba immediately gets the one
that the colour boundary that's important to them
pops out to them.
They've got a name for it,
but I can't remember what it was now, whatever name.
It's an important thing, and it pops out at them straight away.
For us, the blue-green colour boundary is important,
and that pops out to us straight away.
Because it's a pop-out effect,
that says that it's, to me,
it's actually what it is like to see the slide.
The Himba cannot possibly be seeing green and blue
in the way that I am.
Otherwise, the blue, presumably, would pop out of them,
because they must be seeing something bloody weird.
They aren't seeing those green and blue tiles like I am
in their mind's eye, for one of the better words.
And suddenly, when we look at these sort of oakery yellow ones,
they must be seeing that differently from me,
because I can't tell the odd one out with Arby quickly
in the way that they can.
Now, I think this gives us a very strong evidence to me,
or at least this is the way that I would like to use Jules' work,
and I must underscore that I don't think he would be comfortable
with any of this, but this is the way that I like to use it,
to say that the language is actually affecting how we see the world.
So, when I say that our understanding of the world,
it's not just what's going on in the brain.
The brain is instrumental to how we see and perceive
and interact with the world, but it's not just that.
It's our entire body and also the body in the environment
and also within a social network of language users.
And all these things come together to enable us to form
perceptions of what it's like to see and what it's like to speak.
So, the idea that we could just reduce this to me a neural firing
or even worse, just some bloody manipulation of symbols by a computer
is just ludicrous, as far as I'm concerned.
I can see the logical thought train here.
So, you're saying Jules said that language informs the way we see the world.
One potential problem with that.
That's my interpretation on his work.
That's fine.
I suppose where I'm going with this is that it's starting to get towards
this very kind of ultra relativist, constructivist view of the world.
I want to put an anchor.
The reason why it's interesting is when you're talking about sensory states
that it doesn't actually seem like such a bad thing,
because everything you're saying there completely makes sense.
Of course, depending on where you are in the world
and your language and so on,
you might have very, very different sensory states
and you might experience colour differently.
But then there's like a topology, isn't there?
So, then you start getting into understanding and semantics.
And then you start getting into knowledge and truth.
At some point, people might start to disagree with you
that we should have a kind of ultra relativist view on what informs those things.
So, where do you draw the line there?
One of the lovely things about being an academic
is that you get to work with some clever people.
I'm not clever, but I've been blessed by working with a number of people
who bloody well are.
At the moment, I've got a postgraduate who's mind-boggling.
I like to say I supervise him either way around.
We're going for a discussion and I've come out feeling
I've had my mind blown away
and he's sitting there looking quite chuffed
with how the way the decision's gone.
Now, his particular thesis has raised some really interesting questions
and he runs with his very postmodern ideas in extremis.
And on one particular see-for-vision money,
I'm diabetic and he was trying to make out,
so he could give me this argument where my diabetic
was just an enactment of a certain practice.
And I'd say, that's nonsense, I'll die if I don't do this.
How can you decide that the cell call thing,
the cell call conspiracy, if I reap off its whole building,
I'm going to bloody well die.
It's not a social concept for this.
But anyway, Paolo's work is very, very nuanced
and he does make some radical claims.
When you make radical claims,
you've got to be very, very, very careful about how you use the language.
So, I'm certainly not going to try
and paraphrase his thesis in five minutes on here
because it's an incredibly nuanced piece of work.
But I just want to take one aspect from it
that I think is interesting because I think it reflects
on the world that we're living in now
and gives us an insight, a possible insight
into how Trumpism and the echo chamber culture
has taken root.
Because Paolo, and I'm not saying that you go along with this,
I'm just trying to report this as best I can,
from our discussions.
Paolo thinks that we don't just have epistemic perspectives
on a universally shared world where each of us ontologically
can have our own distinct ontologies.
And why does that matter?
Well, if it matters, says Paolo,
if you start looking at how this is reflected on Twitter,
if you have a community of people,
and one of my other postcards, a guy called Christa Mayer,
did a film called Right Between Your Ears,
which I can't recommend hardly enough,
looking at an end of the world documentary feature
on an end of the world cult in America.
And how these guys were saying then,
read in the Bible, the world's going to end on a certain day,
and of course it didn't.
And how they then dealt with that,
and then the leaders,
I was going to end again in six months' time, yet again it didn't.
And Chris's film was just looking at how people can arrive
at these surreal beliefs.
And I think there's something akin to that
with the Q&O movement.
And you had this idea that, you know,
so now Trump was going to lead,
and the result was going to be overturned,
and then it wasn't.
Well, something hasn't going to happen.
If you're not in that,
how can people have these bizarre ideas?
Well, Paolo, if you get in a community of like-minded people,
if we go back to our basic philosophy
and do it by into a correspondence theory
of truth, so much as a coherence theory of truth,
where a proposition is true because it coheres
with the body of the propositions that you take to be true,
when you're interacting on a daily basis
with people in your Facebook bubble
who will all share the same beliefs
that's outside that bubble that look bloody bizarre,
they're reinforcing this thing.
So your view of the world,
your ontological view of the world becomes really real.
You're drawn into this,
and that's why people get so aggressive about it
because it's not, as an academic,
might abstractly discuss the truth or falsity of,
I don't know, Newton's Law's emotion or something.
This is what the world is.
You're questioning something fundamental
about these people's experience of the world.
We've kind of covered a couple of big names tonight.
We've had Turing come up and Sirle.
The other big name that's kind of been floating in the background
is Gertl in terms of like Gertl Esherbach,
the famous book on human creativity.
You kind of point to the Gerdelian,
we'll skip the other guys,
but like the Gerdelian argument
against the possibility of machine intelligence.
Just for the guys out there
that kind of want to get a bit of a grounding
on why his name's been coming up so much,
can you kind of just give us a very brief primer
on how Gerdelian incomputability
kind of relates to the impossibility of machine intelligence
and how that kind of ties into the other arguments
we've been hearing tonight.
Yeah.
I think in the Archive paper,
which you guys, I think,
hopefully signed posted at the beginning,
not if intelligence is stupid,
there's a, as best as I can give it,
a one paragraph summary
in mathematical form of the Gerdelian argument,
and I don't think it would be helpful
to go through that line by line now.
If your viewers are interested,
you can refer them to the detail that's in that paper.
But to just sort of talk at a slightly more abstract,
wider view,
I think, you know, I'm sure Gerdel and Turing
were both aware of the implications
of their work on logic in Gerdel's case
and on computability in Turing's case.
And in fact, there are some people who think
for Turing and possibly for Gerdel as well.
Some people have made the claim that this disconnect
between Turing's avowedly physicalist desire
to explain all of human mentality
via a computer program,
plus what he'd learned professionally about
the existence of non-computable numbers
led to a big tension in Turing's life.
And there's a documentary
that's looked at dangerous ideas, I think it was called,
that explored Gerdel, Turing and Cantor
in that context of people who came up with ideas
in their own work that challenged intuitions
they had about the world.
But coming onto Gerdel,
although I feel certain that Gerdel and Turing
are both in their mind gone through the implications
of their work,
as far as I know, one of the first times
where this was actually cashed out
in terms of an academic paper
was by the Oxford philosopher John Lucas
in a series of exchanges
with another philosopher called Paul Benekaraff
in the 60s,
where to cut to the chase,
I think that Lucas' argument is much more
blunderbuss and less sophisticated
than Penrose's version of this.
So I commend anyone interested to look at
Chadows of the Mind rather than go back to Lucas.
But Lucas basically says,
for any consistent mathematical system
there will be sentences in that system
that we outside of it can see to be true,
the Gerdel sentence of that system,
but which we can prove can never be shown to be true
by that system unless the system is inconsistent.
And so that
Lucas says,
you know, you give me any version
of computation that you like.
I can, I as a human can step outside of that
and see things about that computational system
that it provably cannot know for itself.
And that's, I think, the first place
where this argument sort of took off.
Penrose's own take on this is a little bit more
and you want to see, like to imagine,
looking at, in Chadows of the Mind,
he looks at computations of one parameter
and looks at the question of whether that,
he looks at the halting problem if you're familiar with that
on a function of one variable
and asks, is there a set of rules
that will allow us to tell
if any function of one parameter,
whether that function is computable or non-computable?
And he shows how that leads to a contradiction.
And if we follow the lines of the proof,
we see that it comes to a point
where we see a particular statement
on the operation of computation K,
given K is input,
we can see something about that computation
that it cannot possibly terminate
that cannot be shown to be true
following the rules of that system itself.
I think that's an interesting argument.
And as I said, it seems to me
when I've read commentaries on Penrose,
most people have criticised his speculations
on quantum physics rather than what he has to say
about girdling and logic.
In fact, Penrose was famously invited to give
the keynote at the centenary,
the Vienna conference honouring girdle for the centenary.
So I think if there was some school by error,
because occasionally he goes, oh, Penrose is wrong.
And I think that Penrose is not stupid.
And I think if there was a school by error in his work,
I would like to think you'd have probably found it by now.
But I can recall when I was at the University of Reading,
having an immensely long exchange
with someone about Penrose's ideas.
I won't name them, but they were at the University of Sussex.
And after this had gone on for months and months and months,
and I was saying, look at this, look at this, look at this paper.
And in the end, the guy turns back and says,
life's too short to waste it reading what Penrose actually wrote.
And I've come across this so often,
people think they know what Penrose has said.
Or they think they know that Silsred,
and they haven't bothered looking at the source material on either.
And yeah, this is an issue that comes up time and time again.
And that was quite a shock to me there
when this guy didn't actually concede
that he hadn't actually read Penrose at that point in time.
I hope he hasn't actually, hasn't he?
Fantastic.
Well, Professor Bishop, it's been an absolute pleasure.
Honestly, thank you so much for joining us today.
And I hope to get you back on the show soon.
Well, thank you.
I can't seem to be very rambly and unfocused description.
I tried to think how you're going to get something interesting out of all this,
but that's your genius behind the editing machine, I feel.
I hope it comes over okay.
No, honestly, it's absolutely fascinating.
I think we don't really have much of an opportunity
to talk about philosophy of mind,
to talk about some of these deeper issues in AI.
And I think this is a really fascinating framework actually
to think about some of the various different focus point.
I mean, you know, we had Pedro Domingo's on last week
talking about the drives on AI.
Yeah, that was awesome. I really enjoyed that.
I read the paper that you foregrounded
and also watched the little video that you did accompanying it.
And yeah, I thought that was a really interesting,
deeply interesting paper.
Effectively, you know, I'm making the case that any,
that any gradient descent train neural networks
doing nothing more than a look at,
interpolating between its training points effectively,
which is a profound statement.
And I need to let the dust settle over that,
but that was brilliant.
It's nothing else. I mean, yeah.
Reading that was a great thing for me, coding with you guys.
It's good for deflating some of the deep learning hype
to just to kind of like contextualise.
But also with regards to the night,
you've fleshed out probably 10 people's reading lists
or people's reading lists for the next sort of 10 years.
I wouldn't worry too much about rambling.
All that means is that there's a lot of ground to cover
and not a lot of time to cover it in.
This is the reason why I invited you on,
because a lot of these things are very impenetrable.
And when I read your paper, it's like a Google Maps
kind of point by point stepping stones list
of all of the important things that I need to know
in order to get my head around this.
And I think it's actually a really great starting point
for people that are interested in philosophy of mind
and understanding and consciousness and so on
to read that paper that you wrote.
If I read one book on modern cognitive science,
I'd say look at Evan Thompson's Mind in Life.
Unfortunately, it's a bloody fat book,
but I think that is a genius book.
But there's two watching a number of your podcasts now,
and I'm really enthralled by all of them.

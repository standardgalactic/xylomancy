I had the great pleasure of catching up with Professor Larissa Soldatova from Goldsmiths University.
She's really interested in automating the process of science.
And in order to do that, you need to know about logic, abduction, knowledge and semantics.
We see biology so complex, we still scratching the surface, trying to understand.
And there are not enough scientists in the world to do it.
And if we can have some AI help, that would be probably a good thing.
So my presentation about reflecting on automation of science, history of it, where we are with it now,
how much we can automate now and where we're probably heading towards.
So I'm really interested in the philosophy of science, which is to say,
what is the meta process that scientists go through when they do science?
So science is about generating theories, explanations about how the world is this way and also how it's not that way.
But science is based on empiricism, which is basically following the data, doing experiments and trying to discount hypotheses.
But we still have this chicken and egg problem, which is where do the hypotheses come from?
And this is very related to the bias variance trade-off in machine learning.
Generally, we can't start from a blank slate. We can't start with nothing.
We need to have some world knowledge to structure our search process.
And then we run into trouble. How much should we seed the system with priors and world knowledge?
There are so many things in our physical world that it just feels like we should encode it into our systems.
It'd be a waste of time not to. But then there are some things that we feel that the system should learn.
But if we bias the system too much, then we might accrue a kind of approximation error
because there are things that our system just won't be able to search because we've biased it too much.
So there's always this trade-off between having the flexibility to find new things versus cutting down the search base.
The next big step will be when we will be able to consume more knowledge.
So not just data, but if we start using what we know.
Specific knowledge is the major source of machine and human intelligence and will suffice even with simple inference.
So if you take an AI system, it's mostly taking lots of data.
In the best case, it will take lots of different data, put it together, and then it outputs some predictive model or something else, or a piece of art.
But knowledge is still hugely underused.
No, one reason there is not much available in forms that computers can consume.
But just imagine if even these new models were immediately fed back and could be an input, so it's called closed loop systems,
then probably it would be more intelligent.
So this is quite interesting.
So going back to the 80s, the modus operandi in artificial intelligence was encoding knowledge into expert systems
because we knew that if you could encode knowledge, you can deduce new knowledge and new facts from existing knowledge.
So knowledge isn't just data, knowledge is one level of abstraction higher, where you actually know the relations
and you can apply some kind of functions between data.
The problem is it's very brittle.
Famously, Doug Lenitz created this project called The Psych Project, which had millions and millions of facts and relations and things in it.
And it was almost a complete waste of time.
It was too brittle, it wasn't particularly useful.
But empirically though, we seem to be wasting our time with this blank slate approach because machine learning moved away from really any explicit domain knowledge
and it just became these meta priors, so things like symmetries and, I don't know, learning schedules and weight decay and stuff like that.
And the models only had very abstract priors in the sense of how to learn things statistically.
They didn't have any domain knowledge at all.
But there are so many things in our physical world that we just understand and we should be able to explicitly code into our system.
But we have this paradox, which is that every time we try and encode high level knowledge into systems, it just introduces brittleness.
And this is what Rich Sutton warned against in his bitter lesson essay.
What's the difference for you between data and knowledge?
Yes, it's a good question and in different textbooks you actually can find different definitions.
But people who are working in knowledge representation, they have very clear idea.
So, data it facts.
Knowledge is when you go level up, it's like rules, relations, ultimately better if you have models.
Better if you have executable models.
So you can put something in, get something out.
So this is knowledge.
A lot of knowledge you can discover directly from data.
If you have a lot of data, you can discover these rules, you can construct models.
But why don't represent it immediately as knowledge if we already know it?
Why?
It's very inefficient way.
It takes a lot of energy, these neural networks, they are destroying the planet.
And you actually can encode a lot what we know in quite compact way.
And you can reuse it.
A lot what we know actually is not in the data.
Just an example, I like my collaborator gave this example.
Like they were modeling ecological systems and you can have a lot of data.
And you can discover from data that bear has between two and five cups.
And if it's more north, it's less cup.
But why bother if we know it, just encode it as a knowledge so you can extract it.
But you also can just encode immediately.
And you can encode a lot additionally.
What is not in the data?
That is why it's more powerful and promising.
Larissa spoke about a project about 60 years ago to automate the scientific discovery of chemical compounds.
Not too dissimilar actually to Alpha Fold, which came out recently from DeepMind.
But anyway, she's kind of saying that, you know, if you look at how these systems work today,
it's not dramatically different than it was 60 years ago.
What chemists would do?
And they put in place a whole pipeline, knowledge acquisition,
also giving output in forms that is good for users, in this case chemists.
It really produced very interesting promising chemical structures.
Moreover, this system actually was multi-agent.
It was heuristic dendral, metadendral, and it was closed loop.
So whatever it managed to discover, it immediately was going back and improved over time.
So it was 60 years ago, and I would claim we haven't progressed much further.
Of course, there were other fantastic systems,
but the underlying architecture and principle how it operates is still the same.
So it's called the robot scientist.
And this was introduced again 15, 20 years ago now.
So the idea is to develop a system, a system that is capable of generating hypothesis,
designing experiments, having real robotic labs to carry out these experiments,
analyze the results, do it in cycle, and hopefully discover something new.
So it's a lot of data, external data, whatever data sets are relevant,
plus internally produced because it's a robotic lab, very powerful.
It can run in parallel, like 1,000 biologists working in parallel with lab.
But it also has this knowledge, formalized knowledge.
It has domain knowledge.
The first system worked with yeast biology,
and it's also had this knowledge about what scientific discovery is,
what key elements, how they're connected, what you need to do,
just like our experimental practices, how to design experiments.
You need to explain to machine at all.
So in science, we need to do abduction.
This is absolutely delicious because you've heard us speak about abduction ad nauseam on MLST,
and Larissa is out there talking about it, which is wonderful.
But yeah, you have to start with a hypothesis or a set of hypotheses in science
before you do inference, right?
So inference is when you kind of learn mappings from hypotheses.
But abduction is how you first select the relevant hypotheses, right?
Hypotheses that could generate powerful explanations for some phenomena.
And it's very creative.
It's very mysterious.
So what is the logician 101 definition of abduction?
Well, it's reasoning to the best explanation.
Is that helpful?
Maybe.
Maybe.
I love the kind of cognitive science view on things, which is, you know, an explanation.
It's a causal model, which helps us understand the world because it gives explanatory power.
It carves the world up by the joints.
Why is the world this way?
Why is the world not that way?
And the raw building materials of these theories are cognitive priors,
and they're the same kind of priors that we imbue into machine learning models.
Noam Chomsky says that some of these priors are built into our brains.
I'm a big fan that many of the priors are socially embedded.
They're like software that float around mimetically.
But anyway, abduction is about you have a bunch of priors in the system,
and we as humans have this magical ability to grab the relevant priors,
stick them together into small models that give explanatory power,
and selecting a few of those models.
And that's what we do when we do abduction.
So the key question is basically, can we automate that in a machine?
I will focus on hypothesis because in philosophy of science for a long time,
it was considered that it's something that you cannot automate.
So only humans can do it.
And to enable this hypothesis, this generation,
so you need to go out of system, out of what you already know.
So you cannot use conventional logic like abduction.
You need to use something else.
And I'm used abduction, so there are other logic that you can use,
but you need to go outside of the system to have these guesses,
and try to see if some of it makes sense or not.
It's not trivial.
Focusing on hypothesis, you want to test hypothesis that this particular gene has that function,
but you actually cannot put gene into well, or you cannot measure function.
So you start doing some inferences.
What if we replace it to use some proxy?
If we use something with this gene knocked out,
and if we put metabolites, and so you go deeper, deeper, deeper.
So biologists do it just normal practices,
but when you try to explain it to computer how to do it,
you really need to think, so what steps were taken?
We actually very rarely experiment with something that we make conclusions about.
So our conclusions are not always accurate, so then you have to go back.
It was very interesting for us to try to automate it,
and then also reflect on how humans are doing science, especially experimental science.
There are many levels to this as well, so if you want to go and get a job at Google now,
they already have a bit of a meritocracy which is based on kind of what we're talking about here.
To be very senior at Google, you need to find people,
and to be one step below that, you need to find the areas,
and below that you need to solve abstract problems,
and then below that you need to solve specific problems.
So there's a kind of hierarchy in the amount of creativity and ambiguity that you can tolerate,
and that's what scientists do, so they're always traversing these levels.
Abduction is about finding the areas,
and then once you perform inference on these hypotheses that you have abducted,
then that's solving a much more specific problem, and it gets more and more specific.
The key point is that science is combinatorially, exponentially expensive, just to run experiments,
and there isn't enough time in the world, there aren't enough scientists in the world.
Yes, having AI scientists can help us, but we still need to always reflexively and abstractly go one level up.
We always need to be asking ourselves the question, can we do this better?
If we had a different perspective or a different view, could we make this more efficient?
Could we make this more efficient? And then could we make this more efficient?
And that's what we humans can do, and that's what we need to imbue into AI robot scientists.
Well, you need to test like five, six drugs, work on these combinations,
so yes, it's combinatorical problem, if you do it in a lab,
we just don't have enough cells in our blood to run all these experiments.
So, this is a justification when you need such system, you first do reasoning based on all what we know,
with all these gaps, contradictions, but the best what you can do,
output some plausible hypothesis, what should work in this situation,
then you can do some simulation, again, computational, come up with some hundred, several hundred hypothesis,
then you can automatically test, so that there's a cheaper way of doing it using robotics.
Yes, so I'm really interested in this idea of human cyborgs, basically, where humans and AIs work together.
The problem is, it's not easy to say that combining humans and AIs together produces more productivity.
It probably does, but we don't know for sure.
There was a piece out by ThoughtWorks recently where they were talking about generative AI or co-pilot,
making developers two times more productive.
How do they measure that? Software engineering is not a reducible activity, it's a very complex phenomenon,
and people have tried to measure the productivity of software engineers for many years,
how many lines of code do they write, and the storyboard and issue tracking and stuff like that.
It's all rubbish, and the reason for that is it's a very complex, irreducible phenomenon.
Therein lies the problem. There's a meme going around on LinkedIn at the moment talking about how, with co-pilot,
you can generate the code ten times faster, but you then spend ten times more time debugging the code,
and that's because there's a technical debt, a new type of technical debt, understanding debt.
You generated all of this code, and it works, and then in order to fix a problem, you need to actually understand it.
And the knowledge, the mental model is not in the code. Actually, in any software engineering project,
the mental model is a memetic social thing. It's in the brains of the developers, and it kind of floats around in the ether.
It's not in the code. So, yeah, it's a similar thing here that if we had an AI robotic scientist,
how would we know that it's making us go faster?
I don't think such systems will replace humans. As you see, at the best, they can automate some science,
some experimental-driven science, but it's an important part of science, and if it can be automated, it will be of great help.
Humans still have distinct advantages in many areas. Hopefully, we will retain it, but ultimately,
the best way is to work together, take advantage, and really, because it's him, and then maybe we can make faster progress.
Welcome to RMLST. So, we are here with Professor Larissa Soldatova,
and Larissa joined Goldsmiths in November 2017 as a director of the Online Masters in Data Science programme.
She's an internationally recognised expert in artificial intelligence, particularly in discovery science, reasoning,
knowledge representation, and semantic technologies.
Larissa has also been working on the Robot Scientist project, which investigates which processes of scientific discovery
can be automated and how robotic and human scientists can work together.
The Robot Scientist, Adam, was the first system which made an autonomous scientific discovery.
She's also involved in a number of international projects in the development of semantic standards,
for example, the machine learning schema, the robotics task ontology standard, and also the laboratory protocols exact.
So, it's a pleasure to have you here, Larissa.
Maybe you could just start off by telling us about the events and your talk today.
My talk was about where we are with automated science.
Yes, it's very creative endeavour to do science, but there are parts of it that scientists try to automate for a long time.
So, the history of scientific discovery as a subject of computer science is actually more than half a century old.
The first systems were developed in the 1960s in Stanford.
The first system was inspired by the need to go to other planets, collect samples, like if you go to Mars.
You cannot send many samples back.
If you send data, so what you manage to analyze, it will take so long to send it to human scientists
to get some instructions what to do next.
So, that would stimulate the development of autonomous intelligence systems
that need to collect something, do something, decide what to do next, what it means.
Since then, there was slow but steady progress.
So, the systems that we are working on, it's already over 20 years, yes, it's called Robot Scientist.
And yes, the first was Adam.
It's actually stand for adaptive machines, but yeah, Adam, because it was the first.
Not to be confused with the other Adam for training neural networks.
Yes, it's a good point.
So, the principle of such system is that it's very knowledge-intensive.
So, we hear a lot about progress in AI, but it's mostly data-driven.
So, now we have so much data and also computational power, so it can be processed in something useful output.
And I believe that the next step will be when a lot of knowledge will be available.
In the form that computers can take.
So, right now they are taking data.
If we can give our knowledge and let them to reason with it, so that will be just...
And to do scientific discovery, of course, you need to have knowledge.
Scientists, so why they are still in short supply, because it takes like 25 years to produce.
Wow, you need to educate.
It's many, many years of studying.
And if we can create systems and also knowledge models starting from some domain,
knowledge from some particular area, so we are working with biological systems.
So, if you can encode in machine-processable form what we know and also encode how we do science.
So, how we formulate hypothesis?
Of course, not any, but at least some.
It's actually quite algorithmic.
Yes, well, I'd love to know more about that.
So, you said some really interesting things.
I mean, first of all, as you say, if we want to learn about Mars, it's very sample inefficient because it's very expensive.
And then we get to the purpose of science.
And in my opinion, the purpose of science is about generating intelligible explanations.
And then we get to knowledge.
And I think you were just alluding to data and information is not knowledge.
And I know you're an expert in knowledge representation.
So, perhaps we could just touch on, does knowledge have some primacy over just normal information?
Okay, so it's very true that different textbooks will give different definition.
And sometimes there is no distinction between data and information.
It's like synonym.
But yes, in my area of research, we are very clear about that.
Naturally.
So, what we can see, the data is facts.
Yes.
So, this gene has this function.
Duck is birds.
Knowledge is when you go further.
So, when you have rules, when you have, like, even connections or links between facts, you can go further.
Yes, more knowledge model.
Like, if you have a lot of data, how's prices in this area, how it was affected by flooding,
or so they dropped how much, then you can detect this pattern.
So, this is knowledge.
So, if there is flooding in area prices will drop.
So, this is the rule.
And you can represent it in various forms.
So, if we can represent it in forms that you can put it into memory of computers, then it can be used.
And if you can be used together with data, if you have all these components, it will be even more intelligent.
Interesting.
I guess where I was going with the question a little bit was also, can knowledge be probabilistic?
Of course.
All our knowledge is probabilistic.
We are not certain about anything.
Even if we think, yes, this is the state of the art.
So much knowledge was changed, was refuted.
So, we always walk in this probabilistic framework.
Just, we not always reflect on it.
But if you want to build a system that really outputs something, it is underlying.
So, the other side of working on such system, it give you a chance to understand better about what science is.
Like, there are no guaranteed truths.
It's just what we believe in at this point, given all data we have and other series.
We just walk together.
But it can change in future.
Yeah, it can.
A couple of things.
I mean, rationalists think that there is only certainty.
You either know it or you don't.
And then, I believe that, you know, Bayesian reasoning is kind of like the, it's an extension of logical reasoning in the domain of uncertainty.
Absolutely correct.
And it's important to remember about it.
It's important to remember even when, of course, when we read newspapers, but even scientific articles.
So, scientists actually tend to overgeneralize.
They are very excited about what they discovered.
It's important to step back and think that, yes, it actually can be refuted.
There's only one explanation.
And if you look at history of science, just in Newtonian physics, yeah, it looked like the ultimate truth and said, no,
theory of relativity, turn it all upside down.
It's only some fraction of it.
Yes.
To what extent should knowledge be grounded in the physical world?
So the whole science, it has to be verifiable.
So you can imagine many things plausible, but unless you really show that this corresponds to reality, we cannot trust it.
Of course, it can be still, there can be maybe another explanation, but at least this is connected to what we observe
or even better if we can design an experiment to show there is something behind it.
So this is how science always worked.
Of course, I refer to experimental science like biology, physics, where you can choose.
There are sort of experiments who will not go who's there.
They cannot build an automated system to do that.
Interesting.
I wondered whether you would define yourself as a neat or a scruffy when it comes to knowledge representation.
I mean, for example, if I define, and by the way, a neat is like a puritanical, simple underlying principle, parsimony.
So if I wanted to define a chair, I could think of thousands of different analogies to describe a chair.
Is it collapsible or is it just very complicated?
I'm a very neat and I can give you a perfect example how to model a chair.
So I'm very neat because we work with these reasoning systems and they need sharp, clean logic.
And if you complicate matters, it complicates reasoning.
A chance that something will be really discovered and we want to connect different bits of knowledge, different data.
So the more streamlined it is, the better.
And if you use like minimum of relations and so over years, I just learned to think that way about chair.
It's actually examples that I used in my lectures when I was explaining.
Wonderful.
So I was showing them just examples.
So these are chairs.
They can look very differently.
Some don't have any legs.
Some have three legs.
So how to explain computers?
This is a chair.
Yes.
So you can, yes, you can represent many, many, many ways.
But a good knowledge model will focus what we call intrinsic property.
What makes it a chair?
Give me an example.
What makes it a chair?
It's function to be seated on.
So it's a relational model.
So it was designed.
It was produced for it.
Yeah.
This is so interesting.
It can take many shapes, many colors, but this is what defines a chair.
And because we produced it human, so we can say this is function.
And all other, yes, it can have other properties like what color it is, what shape,
but ultimately this would make it a chair.
Yes.
Yes.
Because when I hear philosophers talking about semantics,
they have to come up with some kind of a relational model.
And they will choose the view of function or purpose as another good one.
And I guess what I'm saying is that it's a little bit anthropocentric and like function.
For example, a hospital.
That building used to be a hospital, but it's not a hospital anymore.
But again, why it is so centric?
Because it's us who made it.
So we have control over it.
Yes, it was hospital, now it's no more hospital.
Some social convention, some decision was made, it was repurposed.
Now it is not hospital, it's something else.
But if you take biological systems like gene function, we have nothing to do with that.
It evolved to be there.
It's still intrinsic property of it, what function it has, what it's for.
Otherwise you can veer very far.
Like again, going back to a chair, I can sit on a table.
It doesn't make it a chair because it's not primarily function.
This is not what this was made for.
Yes, yes.
So if you agreed on it, you need to have this clarity, also agreement that this is how you model things.
Then you start to have consistency.
What is a chair?
What is a table?
Because if you want to have intelligence system to design your office, for example.
So how will it do?
How it will distinguish what is what?
So this is quite similar in principle to Wittgenstein said that the meaning of a word is in its use.
And I'm friends with a colleague of yours, Mark Bishop, who says the meaning of computation is in its use.
But that's a very kind of relativistic social emergentist kind of view.
I guess what I'm saying is, do you think there's a platonic meaning?
So for me, it's very easy to answer all these questions because I'm actually working on development of real systems
that need to take it, reason with it and produce something.
Justification of what we are doing and how we are doing it works.
You can do it differently.
You can take different points of view.
Let's design a different system based on it and then compare what is better.
No one has done it.
So this is what is done, how it's done, it's working.
Okay, so that's very pragmatic.
Yes, I would say it's not relativistic.
It's a pragmatic approach to producing knowledge in machine-consumable form.
It's not optimal, it's not the best, but it's something good that computers can work with and produce something.
I think you might be a closet scruffy.
Okay, are you familiar with the Psyche project from Doug Leonard?
I think so.
It's almost like preaching to the choir.
What kind of retrospective comments do you have about that project?
I think they just attempted to do too much.
Right.
It's like in AI, so this was a constant generic AI versus something practical, fragmented,
so of course it's wonderful to have a system that can solve any task
for that you need such knowledge models.
We're just not there yet, maybe in the future.
Right now, we're trying to model some fragments, some domains of knowledge,
and if we have enough such fragments, hopefully we can combine.
But ultimately, if you want to have general AI, you will need such models.
One contrast people make is, I have friends who are in the domain of certainty,
so they think knowledge is either you know it or you didn't.
The reason that they give and the reason why they don't like language models
is because they say, well, when you have actual knowledge, so you actually know,
you can deduce new facts and new knowledge, so you can create more knowledge.
And they would say with a language model, because it's only pretending to reason,
you can't create any new knowledge with it.
It's actually an advantage we already started using it to generate hypothesis.
Because it's not rigid deduction, it is how you can actually make new guesses
and some of them might be true, you just need to experiment to show it.
So we already started, we generated some very interesting hypothesis
where we're already organizing experiments to test them.
Yes. I believe that science is about abduction, you know,
so like creating this plausible set of hypotheses.
And we're at an event all about creativity.
And humans have that spark of inspiration, don't they?
We just have an idea about plausible hypotheses.
And maybe the world is quite sclerotic and predictable and boring,
but do you think that there's some spark that we won't get from AIR?
I completely agree with you and it's exactly what we use, we use abduction
in our reasoning system to generate hypothesis.
Yes.
So in some simple application areas like in drug design, you use abduction,
you just have many, many, many examples of chemical structures, short activities,
so you can, if you see similar structure, you induce it, it might show similar activity again.
It's not over, so still it's not, you cannot infer ultimate truth.
That is why you need experiments.
But abduction, yes, this is something that takes you outside of what you already know.
Yes.
I completely agree, I think it's the most exciting part.
But again, this is what is technological.
So there are reasonous, they can do abduction, there are languages that can handle it.
So we have many building blocks to build such system.
Yes, yes.
And are you excited about the prospect of neurosymbolic architectures?
Or are you still very old school and you like these explicit knowledge representation?
That is a difficult question.
So yes, I'm very excited by what is happening.
I was skeptical about all these natural language processing.
We tried to work with NLP people.
And results will be quite disappointing how much useful information they actually could extract.
And then such progress.
So of course it is exciting and we already started using it in our scientific work.
So at the same time, I believe there is a place for this very clean, crisp model
just because you need something as simple as possible just to enable reasoning.
Just because all these reasoning engines, they are powerful,
but if they can reason over simpler structures, there is no chance they will do it.
Exactly.
For practical purposes.
From all my experiences, the cleaner you can get, the better it works.
Ultimately, when we have more technological progress,
we probably will be able to more reflect how it is in our brain.
Yes, but there is also robustness and reliability and trustworthiness.
Precisely.
We actually then can explain how it was done,
so this explainability, so trustworthiness,
so if people understand how it works,
if we can explain this is how it happened,
this is what was taken as input,
this is how it was done and people will trust it more.
And especially like systems that we are working on,
applications for cancer research,
what doctor would recommend these drugs
if they don't understand how the system came up with the suggestions?
Yes.
And again, the cleaner your representation, the easier to explain it.
I know, I really agree with you.
The only intuition I have is that if we are designing a novel molecule, for example,
even if we knew or could understand how it was created,
would we really understand it?
It seems so complicated, doesn't it?
It is, it is.
But again, there are different logics,
so for example, physiologic, you can build systems around that.
Ultimately, let's try to build more such systems
and then you just force to understand,
so if you can build it, then you understand how it's working.
Yes.
Inevitably.
So I still believe that some portion,
an important portion of this can be modeled,
something probably not in any near future,
it's just too complex, too fast.
Larissa, it's been an absolute honor.
How can people find out more about you and what you're up to?
We're actually very bad with telling about what we are doing,
that is why I'm so glad that you invited me to talk about it.
We just buried in our work and the best we will publish our scientific papers,
and it is where we stop.
Yes, you teach students, you tell students,
it's based on your research, you use examples,
but apart from that, we don't do much,
and we are really quite keen to change this.
I think a lot of people are intimidated by papers and science,
and if only people knew how many fascinating ideas there are in there,
I think they just need a hook, and it's so interesting.
No, I blame scientists, they keep this complicated language,
even if sometimes they don't need it,
they keep people away from it.
No, I think scientists should do more about explaining their work,
how they are doing it.
I always thought that we are doing our work,
someone else will solve all the AI-society problems,
but no, it's actually us who have to do it,
to explain and tell and also warn.
Yeah, well now people can ask GPT for.
But what it generates, it's not all of us correct.
That's true, it's not, but it's quite good.
It's good, it's very impressive, it's very impressive.
Larissa has been an honour, thank you so much.
Thank you, thank you very much.
My pleasure, my pleasure.
I'm so impressed by how much you know and understand.
Wow, so many people around, no abduction, come on.
We are a computer science podcast.
I think it's one of the most fundamental things,
because you know, going back to Aristotle,
with the weak syllogisms and the strong syllogisms,
I think of induction is actually quite interesting,
because most people use it to include abduction.
So not only creating the map to the hypothesis space,
but actually abduction is just creating the plausible hypothesis.
I completely agree, because if you clear about it,
so you clear about logic, so you can have fuzzy concept,
all this probabilistic, but underlying logic must be crisp.
You need to understand, and if it's abduction,
precisely it's only with some probabilities that it's correct.

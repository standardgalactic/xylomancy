So welcome back to MLST.
Today we're going to be talking about this book
by Dr. Thomas Parr, Giovanni Pezzullo,
and Professor Carl Friston.
Now the book is Active Inference,
the free energy principle in mind, brain, and behavior.
So the book, from a pedagogical perspective,
it's describing active inference from the high road
and the low road.
And the high road is a little bit kind of helicopter view.
So it's saying, okay, we've got these biological organisms
or these living systems.
And what do they do in order to be living systems?
Well, they resist entropic forces acting on them
by minimizing their free energy.
So it goes into the how question,
but it also goes into the why question
from a helicopter view.
The low road of active inference
is far more mechanistic, far more mathematical.
And obviously all of the roads lead to Rome, if you like.
But the low road is talking about things
like Bayesian mechanics.
There's a primer on probability theory,
talking about things like variational inference,
which is the way that we solve these intractable
optimization problems in active inference.
And also talking about framing active inference
as a process theory, which is the latest incarnation
of the description of active inference.
So Professor Carl Friston wrote a preface for the book.
He said, active inference is a way of understanding
sentient behavior.
The very fact that you are reading these lines
means that you are engaging in active inference,
namely actively sampling the world in a particular way,
because you believe you will learn something.
You are palpating.
This is beautiful, by the way.
Friston uses the most beautiful language.
It's his signature, if you like, it's his calling card.
He said, you are palpating this page with your eyes
simply because this is the kind of action
that will resolve uncertainty
about what you're gonna do next.
Indeed, what these words convey.
In short, he said, active inference puts action
into perception, whereby perception is treated
as perceptual inference or hypothesis testing.
Active inference goes even further
and considers planning as inference.
That is, inferring what you're gonna do next
to resolve uncertainty about your lived world.
So I'm about to show you a sneaky clip
of Professor Friston that we filmed in January.
I might publish the full show on MLST in the future,
but I just wanna take this as an opportunity
to thank you so much for all of our Patreon supporters.
Honestly, it means so much to me
because the last few months I've just been trying
to make this activity of mine, this passion of mine,
a full-time job.
And it's not just because you love the show
and you want to support me, you get early access to content,
you can join our private Patreon Discord.
We have bi-weekly calls where we talk
about all sorts of random stuff
and you also get early access to lots of our content.
So please check that out.
But in the meantime, here's a little sneaky clip
from Professor Friston.
The neural network is a generative model
of the way in which its content work was generated.
And its only job is effectively to learn
to be a good model of the content that it has to assimilate.
If you put agency into the mix, you get to active inference.
So now that we've got a generative model
that now has to decide which data to go and solicit.
And that's actually quite a key move.
And also quite a biometric move.
So we're moving from perception machines,
we're moving from sort of neural networks
in the service of safe face recognition
into a much more natural science problem
of how would you then choose which data in a smart way
you go and solicit in order to build the best models
of the causes of the data that you are in charge of gathering.
Dr. Thomas Parr is a post-doctoral scholar
at the Welcome Centre for Human Neural Imaging
at the Queen Square Institute of Neurology
at University College London.
And a practicing clinician.
Now, one of the reviews from the book was from Andy Clark.
He said, it should have been impossible.
A unified theory of life and mind laid out
in 10 elegant chapters spanning the conceptual landscape
from the formal schemas and some of the neurobiology
and then garnished with practical recipes
for active model design.
Philosophically astute and scientifically compelling,
this book is essential reading
for anyone interested in minds, brains and action.
Well, I mean, thank you very much for having me on.
So I'm Thomas Parr.
I'm both a clinician and a theoretical neuroscientist.
So I've been working in active inference
for a number of years now since I did my PhD
back in 2016 with Carl
at the theoretical neurobiology group at Queen Square.
And I'm now based at Oxford
where I split my time between research
and clinical practice.
So tell me about the first time you met Carl.
The first time I met Carl, I was considering,
so I was a medical student at the time at UCL
and I was considering doing a PhD.
And I remember arranging to meet with him
and obviously being a relatively nerve-wracking experience
meeting one of the most famous neuroscientists in the world.
And I remember discussing with him about it and saying,
you know, this is what I'm interested in.
Would you consider supervising my PhD
if I were to get the funding for it?
And I remember he said, yes, all right, then, anything else.
And I asked him, do you want to see my CV
or anything like that?
And he said, no, I'll only forget it.
Yes, yes.
That was my first encounter with Carl.
But since then, he's always been immensely supportive
and has been, you know, exactly the sort of mentor
that I think anybody would want to be able
to develop a skill set and sort of proceed in science.
I come from a machine learning background.
And since discovering active inference in Carl's work,
it's really broadened my horizons.
And at the moment, there's an obsession
with things like chat GPT.
And I just wondered in your own articulation,
how would you kind of pose the work that you do
in relation to that kind of technology?
It's a good question.
And I suppose there are many levels
at which it could be answered, aren't there?
I guess thinking about something like chat GPT
in that style of technology,
it's clearly been very, very effective at what it does.
But it's worth thinking about what is it that it does.
And I think chat GPT is an excellent example
because so many people are familiar with it.
It has such impressive results
in terms of being able to simulate very effectively
what it's like to have a conversation.
But ultimately, it is like most deep learning architectures.
It's a form of function approximation.
It's a form of being able to capture very well
the output that would be expected
under some set of conditions given some input.
So you give it some text and it knows which text to predict.
And it's very good at that.
But in a sense, that's where it stops.
It doesn't necessarily do anything else.
That's very different to what you and I do
when we engage with the world around us,
when we want to learn about the world around us,
when we want to form our own beliefs about what's going on.
And those are the things that I think
it doesn't have in the same way.
It certainly can't act and go and seek out specific exchanges,
specific conversations that it might want to learn from.
Whereas you or I might do that
if we wanted to know about something specifically,
we'd go and look for information about that thing.
And I think that's where active inference
and the idea of having a generative world model
and understanding of what's there in your world,
that you can alter yourself, that you can change
is very different to a lot of more passive
artificial intelligence.
Probably the point where things become closer
is in fields like robotics
where you have to account for both of those things.
You have to model a world
that has yourself in it,
where your actions affect the data that you get in.
And I think that's probably where
more of the convergence is likely to happen.
Yes, so you're describing the difference, I guess,
between an observational system and an interactive system.
So in an interactive system,
an agent can seek information
and change or bend the environment to suit its will.
But just to linger on this for a second though,
there are folks who do argue
that neural networks are more than hash tables,
because I think of them the same way you do.
They essentially learn a function
and if you densely sample it enough,
just like a hash table,
it can go and retrieve what that function says
given a certain input.
But there are folks who say, no, no, no,
these models learn a world model.
Orthello is given as an example,
or with Sora, they say it's learned Navier stokes.
It's a really good question.
And I think there are some open questions here
and I wouldn't claim to have all the answers to this one.
I think to be able to, again, take chat GPT,
to be able to give the answer it does,
clearly it has captured something
about the statistics of languages,
uncovered something about the hidden causes.
So you could argue there is potentially
an element of world modeling in there
that is left implicit.
I think it would be very difficult to pull that out
or to sort of see that with any transparency
with something like chat GPT.
And so if it does have something of that sort,
probably it's the methods that neuroscientists
have been using for years to understand the brain
that might help to try and pull out those same things
in those sorts of architectures.
Clearly some sorts of deep learning
and neural network models are very good
at picking up regularities in terms of dynamics as well
and being able to predict trajectories.
And I think it's important to say that
describing something as a function approximator
is not to criticize or belittle it.
It's a very important thing to be able to do.
And it may also be very important
in certain types of inference.
So for instance, things like variational autoencoders
are based upon often deep learning
neural network architectures.
But the function that is learned
is the one that maps from the data I've got coming in
to the posterior beliefs or the parameters
of the posterior beliefs that I would arrive at
were I to perform inference of the sort
we might do in active inference.
So you've written an absolutely beautiful book
on active inference.
And active inference, in my view,
it's a theory of agency, which is to say
it describes what an agent does.
And I'm fascinated by agency, but could you just start by,
I mean, from your perspective, could you introduce the book
and tell us about your experience writing it?
Of course.
So the active inference book that we've written
is a collaboration between myself, Giovanni Pazzullo,
who's based in Rome, and Carl Friston,
who has to take credit for development
of active inference in the first place.
And the book sort of rose out of our sense
that there wasn't a unified book out there
or a resource out there to help people learn about
what is ultimately a very interdisciplinary field.
And so we've all had experience with students coming to us
asking for resources, asking what they need to read,
and it may be we refer them to a little bit of neuroscience
work, a little bit of machine learning,
textbooks or specific pages on variational inference
or whatever else, giving people introductions to
or places they can learn about
the maths they need to be able to do it.
But then also the biology, the underlying psychology,
the long sort of tradition of previous scientists
who worked in related areas.
And so the book was an attempt to try and provide a place
that people could find all of that or at least references
to all the relevant things they needed for that
to stick to the same sort of notation,
which is one of the things that's often very difficult
and the same formalisms and try and introduce everything
in a very systematic way to people.
So I'm pleased to hear that you found it useful
and I hope other people will as well.
The experience of writing it, I mean,
so that took place over several years,
partly because the pandemic got in the way in the middle.
And so Giovanni and I were passing notes
between one another over email
and weren't able to sort of meet in person
to discuss it during that time period.
But I think we're all quite proud of the result
that we've got out of that
and people seem to have responded quite well to it.
You start off by talking about what you call a high road
and a low road to active inference.
Can you sketch that out?
Yes.
And I think this was one of Giovanni's very nice ideas
about how to introduce it because as I say,
it's very multidisciplinary.
There are lots of ways into active inference.
And one of the things that's most difficult for people
who are getting into the field for the first time
is knowing where to start.
Do they start dealing with the Bayesian brain
and conscious inference and Helmholtzian ideas like that?
Or do they start from a physics-based perspective
and start working their way towards something
that looks like sentience?
And there are lots of different,
lots of alternative ways people get into it.
The fact that you become interested via machine learning,
the fact that other people have come interested through biology,
I developed an interest through neuroscience
while I was at medical school.
And the high road and the low road was a way
of just trying to acknowledge that difference
or that difficulty of knowing where to begin
and saying that that's okay,
there are lots of different roads,
but they ultimately end up in the same place.
The idea of the low road was to say,
well, let's just take observations in psychology
and the development of a number of ideas that are built up over time
that come to the idea that we're using internal models
to explain our world that the brain is using
something like Bayesian inference
or at least can be described as using Bayesian inference.
And go from there through the advances
that lead you to active inference,
the idea that it's not just a passive process
that you're also inferring what I'm going to do.
Furthermore, when we're doing inference,
we're changing our beliefs to reflect what's in the world
around us and to explain our sensory data.
But actually, when we're acting in the world,
we can also change the world itself
to better comply with our beliefs.
So it's that move from purely changing our beliefs
to reflect the world to also changing the world
to reflect our beliefs.
And that fascinating move that actually both can be seen
as optimization of exactly the same objectives
that they have the same goal, that in both cases,
it's really just improving the fit between us and our world.
So that's the sort of low-road perspective.
The high-road perspective was the idea of saying,
well, let's start from the minimum number of assumptions we can.
Let's start from first principles.
And that takes a much more physics-based approach.
It says if you have a creature that is interacting with its world,
then there are a number of things you've already committed to.
And that includes things like the persistence of that creature
over a reasonable length of time,
the maintenance of a boundary between that creature and its world
and that sort of self-world distinction.
And once you've committed to those things,
you can then start to write down the constraints that those imply
in terms of the physical dynamics of that system.
And you can start to interpret those dynamics
in terms of the functions they might be optimizing,
much like if you were to write down the equations
that underpin Newtonian dynamics,
you can write them down in terms of their flows on Hamiltonian functions.
And it's following the same sort of logic
to then get to flows on free energy functions,
where free energy is just a measure of that fit between us and our world.
And so both roads ultimately end up leading to this common endpoint,
which is that to be an agent in our worlds,
in the sort of worlds we live in,
we have to be able to change our beliefs,
to reflect what's going on around us
and change the world through our dynamical flows on a free energy functional
to best fit with the sorts of creatures we are.
When we first started looking at the free energy principle,
we were talking about things.
It was known as a theory of every thing,
every space thing,
which is to say, roughly speaking,
if a thing exists,
what must the thing do to continue to exist?
And just their continued existence,
resisting entropic forces,
is what defines them,
which gets us into the second law of thermodynamics.
Now, that sounds like quite a strange thing to say.
Why do things need to resist entropic forces?
And I think there's a development
in how a lot of these ideas are presented over time,
which you expect and hope for in science.
And I think we've often taken different perspectives
at different points in time as to how we explain these ideas.
And resisting entropic forces is an idea
that I think most people find relatively intuitive.
So the idea that we,
the physical systems will tend to increase their entropy over time,
at least close systems,
so that over time things will gradually dissipate,
things that are highly structured and highly ordered
and can only exist in a very small number of configurations
are more likely to change into something
that can exist in many different configurations
than they are to go in the opposite direction.
But anything that persists over time and maintains its form
clearly resists that process of decay,
at least to some extent, or at least for some period of time.
However, the opposite is also true.
We're also not creatures that tend towards a zero entropy state.
We don't end up in a sort of single configuration.
We have to be flexible.
We have to change in various ways throughout our lifetime
or even throughout our daily routine.
So it's not quite as simple as just saying
you have to resist entropic change.
It's more to say that entropic change
or the amount of entropy that you expect to develop
has to be bounded both from above and below
that there is a sort of optimum level to be at.
And that optimum probably varies from different,
well, from person to person, from creature to creature,
from thing to thing.
You could imagine a rock that doesn't need to do much.
Its interface with the environment is quite trivial.
Versus us as agents,
we are incredibly sophisticated.
So for us to continue to exist,
we have many more ways of interfacing with the environment
and we need to plan many more steps ahead.
So is that just a pure continuum between rocks and people?
I mean, in principle, yes.
I mean, the notion of that persistence,
of that resistance of entropy,
will depend very much on what you are.
As you say, you could imagine a whole scale of things in between.
I mean, in a way that as you've highlighted with the rock,
some of the most boring things are the things with the...
Sorry, I shouldn't say that,
but geologists who might find rocks very interesting
and I'm sure are very complex.
But from a sort of behavioral perspective,
clearly things like us are much more interesting to study
than things like a rock.
And part of that is that we actually have a higher degree of entropy
in how we live our daily lives compared to things like...
I almost said organisms like rocks,
but things like rocks that are not behaving.
The reason I was thinking about this is,
the second law of thermodynamics was conceived,
I don't know, 150 years ago or something like that.
And many people at the time thought
that it was an affront on free will.
I think the religious people at the time
were aghast at the idea that things were mapped out in this way.
It's always worth saying in this discussion
that obviously the tendency for entropy to increase
from a physical perspective generally relates to closed systems
of which we are not.
And as soon as you start talking about different compartments
and interactions between them,
you also introduce the idea of several coupled systems.
And so you can start to ask questions about the overall entropy
or the entropy of specific parts of that system.
And agents and worlds are two compartments and systems
that exchange things with one another,
and so are not closed systems almost by definition.
That closed system, again,
from a sort of neuroscience standpoint
is not necessarily a very interesting system.
So probably that deals with a large part of that.
The question of free will is always an interesting one
and always a thorny one that I'm not going to claim to have any expertise on
or be able to answer.
But I think it probably tackles a slightly different thing
from a cognitive science perspective,
which is whether or not we believe that the actions we're taking
are actions that we've chosen.
And that probably comes back into another aspect of active inference,
the idea that the way we're regulating our worlds,
the way we're perhaps changing the entropy of our environment,
depends upon our own choices about it,
our inferences about which one we're going to do next.
And that feeds into things like,
we've spoken about free energy,
that quantity that we use to both choose our actions
and act in the world around us while also drawing inferences.
But we can also talk about things like expected free energy,
which is a way of evaluating our future state
and what would be a good trajectory
or a good way for the world to play out.
And their entropy has a completely different meaning
and there are different sorts of entropy.
So for instance,
if I were choosing between several different eye movements
I could make while looking around this room,
the best eye movements I might choose
are those for which I'm least certain about what I would see.
In other words, the highest entropy distribution.
So once you start planning in the future
and once you start selecting things to resolve your uncertainty
and be more confident about the world around you,
you actually end up seeking out entropy,
which it seems to then very much contradict
some of the other ideas that we were talking about,
the idea that we're constantly resisting it.
But actually it's by seeking out the things that we're least certain about
that we can start to resolve that uncertainty
and start to become more confident
and more certain about the world around us.
Yes, resist entropy by seeking out.
That's a bit of a paradox.
But even what you were saying just a second ago
about this description of how agents operate,
it's very principled.
We were talking about this balancing epistemic foraging
versus sticking with what you know.
And more broadly speaking,
what we're thinking of agency is this kind of sophisticated cognition
of having preferences and bending the environment and so on.
And I guess like where I was going before
is it's tempting to think that this erodes free will.
And I think of them quite adjacently in my mind.
If anything, I guess I would call myself a free will compatibilist,
which means it doesn't matter that it's predetermined.
I try not to use the word free will,
but thinking of agency in this sophisticated way,
whether it's predetermined or not, is irrelevant.
It's the complex dynamics that distinguishes my agency from somebody else's.
So I think agency is better to think of than free will, if that makes sense.
Yeah, and I think that's probably right.
And the experience of and the inference of agency as well, I think, is part of that.
There's a potential link that you can draw here also
to the idea of chaotic dynamical systems of which we essentially are examples.
And the idea of chaos in that setting is that if you start from two
ever so slightly different initial conditions,
your path and your future may unfold in a completely different way.
And I think that fits very nicely with what you're saying about distinguishing my agency
from somebody else's because you don't see it as if I were, you know,
that I'm going to behave in exactly the same way somebody else is.
And part of the reason for that is that you end up starting
from a slightly different perspective to where they are,
and that might lead to wildly different futures for both of you.
So something I think about a lot is whether agents are ontologically real
or whether they are an instrumental fiction.
And I think part of the complexity, especially with active inference
and the free energy principle is this hierarchical nesting.
So we can think of agents inside agents inside agents.
And I guess the first question is, are they real and does it matter?
Define real for me.
Well, one argument would be that they are epiphenomenal,
that they themselves don't affect the system that they're in.
Is this a good way to think about it?
It is a very difficult question to try and contend with, isn't it?
Because I think there are so many words that come up here that are kind of laden
with different semantics or different meanings depending upon who you speak to
and which camp they come from in the sort of philosophical world.
And that's why I sort of asked you to define real,
and it's really difficult to define what real means in that setting, isn't it?
And I guess coming back to your original question there,
for me, does it matter if they're a sort of real thing or not?
Probably not. It matters whether it's useful.
And I guess that sort of brings me to a point about one of the things
I find quite appealing about active inference as a way of doing science.
And I think having had an interest in things like neuroscience and psychology for some time,
I often found it quite frustrating to understand what people meant
and the different language they used in psychology
to understand different aspects of cognitive function.
And I think it's worth acknowledging that actually lots of people
mean completely different things when they say attention.
And some people say attention to mean the sort of overt process
of looking at something and paying attention to it.
Other people use it to talk about the differences in gain
in different sensory channels that they're trying to pay attention to or not.
Am I paying attention to colours versus something else?
And that's just turning up the volume of different pathways in your brain.
And I'm sure there are a world of other things that people mean by it as well.
But the idea of then trying to commit to a mathematical description of these things
means that a lot of that ambiguity just disappears.
That if you put a word to a particular mathematical quantity,
as long as you define what that mathematical quantity is
and how it interacts with other things,
then a lot of that ambiguity just isn't there
and it forces you to commit to your assumptions in a much more specific way.
And so that's why I come back to say, does it necessarily matter
if an agent is real or not?
I don't really know what that means,
but if an agent is just a description of something that is separated from its environment
that persists for a certain length of time,
that has a dynamical structure that can be written down
and a set of variables that can be partitioned off from another bit of the world,
for me that's real enough to be useful.
And so that's where I'd go with that one.
Yes, yes.
This is fascinating.
So it's a mathematical theory that carves the world up in an intelligent way
that explains what things do and what they don't do.
And I guess the ontological statement,
maybe we can park that to one side
because as you say, from a semantics point of view,
people have very relativistic understandings of things
and there's always the philosophical turtles all the way down.
Well, is it really real? Is it really real?
But one thing that is interesting though about active inference
is that it's quite mathematically abstract.
So when we were saying, is it real?
It doesn't even designate, is it physical?
So for example, a boundary is just talking about the statistical independence between states.
And those don't necessarily correspond to physical things.
So I guess it could be applied to almost anything.
It could be applied to culture or memes or language or something like that.
And it has been.
Yes, indeed.
Yeah, it's a good point.
And then you end up sort of dragged into the questions of what is physical.
What does that mean?
Is physical just an expression of dynamics that evolve in time?
Because I mean, even committing to a temporal dimension
tells you something about the world you're living in.
Are the boundaries that we're talking about, are the partitions,
are they spatial in nature or not?
And, you know, I remember there was an article a little while back
that sort of made a lot of argument about this as to whether
the partitions that divide creatures from their environments
are equivalent to statements of conditional independence
or the sort that are seen in machine learning or various other things.
And arguing that there's something inherently different about a physical boundary.
For me, I was never completely convinced by that,
but partly because you have to then define what you mean by a physical boundary.
And I suspect it's the same sort of boundary.
It's the same sort of conditional dependencies and independences.
But where those have specific semantics, whether those be temporal,
whether they be something where, you know,
you can actually define a proper spatial metric underneath the things
that you're separating out.
And clearly that sort of boundary is very important.
But for me, that is just another form of the same sort of boundary.
And as you say, you can apply exactly the same sort of ideas
to things that are not spatial, not sort of physical, whatever that might mean.
Yes, yes.
Because when I spoke with Carl last time,
I was pressing him on this idea of a non-physical agent
and he was quite allergic to the idea.
And I suppose even though mathematically you could apply at first to other geometries,
that would be quite easy because they have certain mathematical properties
in terms of like, you know, being locally connected and measure spaces and all that kind of stuff.
But if you did say, OK, I want to have an agent that represents a meme,
how would that act?
I don't know, you get into modelling challenges, don't you?
I suppose you do.
I think the modelling challenge is defining the boundary.
I think the boundary is a very difficult thing to define sometimes
when you're dealing with something non-spatial.
That boundary, though, might be reflected in the interactions
between a meme and a community that engage with it.
It might be to do with the expression of a meme
in different parts of, I don't know, a network of some sort or a social network.
I don't know how easy it would be.
I've not tried to do it in that context.
And I think with many of these things, you never really know until you've had to go at doing it.
But I suppose the key things I would be thinking about
are, is there a clean way of defining a boundary for a meme?
Is there something that the meme is doing to the outside world?
Is there something that the outside world is doing to the meme?
And I think if you're able to define those things convincingly,
then perhaps there is a form of agent that may be non-physical,
if that's how you choose to define it.
But then I'm not sure what physical means in this setting.
They're also an account of saying, well, actually,
if you can write down the dynamics of how a meme propagates through a network,
is that any different writing down the dynamics of another sort of physical system?
Yes, possibly not.
But it is really interesting to me that something like language could be seen as alive,
as a super organism, or even something like religion.
And it seems to tick all of the boxes that we talk about with a gentleness in physical agents,
which is to say, let's say a religion or even nationalism.
You could say that the state of the Netherlands has certain objectives.
And clearly there's a two-way process here.
So the state affects our behavior and we, you know, our collective behavior influences the state.
But this then, I think the reason why people don't like to think in this way
is we have psychological priors.
So we are biased towards seeing a gentleness in individual humans,
but we tend not to think of non-physical or diffuse things as being agents.
Yes, I think that's probably right.
And again, it sort of brings us back to this whole issue about the language that we use,
that it comes laden with lots of prior beliefs about what it means,
which may vary from person to person.
And there comes a point where you say, how important is it that I commit to using this particular word
to mean this particular thing in this setting?
But again, in your example of, you know, taking a nation or a nation state as being
a form of organism at a higher level or a form of agent,
if you can show that there is a way of summarizing the dynamics of that system,
maybe some high order summary of the behavior of people in that system,
voting intentions, I don't know.
You might then be able to show that it behaves in exactly the same way mathematically
as individuals within that system.
Yeah, so this brings me on a little bit too.
I've been reading this book called The Mind is Flat by Nick Chater,
and I'm speaking to him on Friday.
And his main take is that, I guess you could call him a connectionist.
He's friends with Geoffrey Hinton.
And his main take is that there is no depth to the mind.
So for years, psychologists have built these abstract models to reason about how we think.
So we do planning and we do reasoning and we have perception and we do this and we do that.
And also we try and generate explanations for our behavior.
So we do this kind of post hoc confabulation.
But when you study it, it's incredibly incoherent and inconsistent.
And he was talking all about how the brain is actually a kind of predictive system, right?
So we have these very sparse incoherent inputs and we sometimes see things that aren't there.
And I think you speak about this in your book that there was a really big shift.
I think you refer to it as the Helms-Hotsian idea that the brain is a kind of prediction machine,
rather than our brain just kind of like building a simulacrum of the world around us.
I mean, how do you think about that as a neuroscientist?
Yeah, I mean, I think prediction has to be a key part of it.
And the reason it's a key part of it is that it's a way of coupling us to our world without prediction.
If you're purely simulating what might be going on without actually then correcting a simulation based upon what's actually going on
or the input you're getting from the world, then you're not going to get very far.
So prediction is just an efficient way of dealing with the issue of how do I update my beliefs?
How do I update if you want to call it a simulation?
My simulation, my internal simulation of what's going on outside.
And once you cease to have that constraint, once the world ceases to constrain the simulation,
that's the point at which you start, as you say, hallucinating, seeing things that aren't there
and developing beliefs that just bear no relationship to or little relationship to reality.
Yeah, interesting.
So, I mean, one thing this Nick Chaitaguay was saying was that we see a complex system
and we adopt what Daniel Dennett calls the intentional stance.
And that is, I have a self-model, I have a model of your mind and I observe behavior
and I kind of impute onto you a model and I can generate explanations.
So as I say, Thomas did that because he must have wanted to do this.
And I guess you could argue that all of this is just a confabulation.
It's just an instrumental fiction.
It's a way for us to explain behavior, but it doesn't really exist.
But then there's the question of, well, it's not that it doesn't exist.
It's just that your mind is incomprehensibly complex.
So it's not that the mind is shallow.
I prefer to think of it as the mind has so much depth that it's beyond our cognitive horizon.
And depth, I think, is an interesting notion as well.
I mean, it's the idea that comes under a lot of machine learning
and the idea of deep learning, neural networks with multiple layers.
And I think you're right that depth is an important part of our generative models as well,
of our brain's models of the world.
And part of that comes from the fact that the world actually does separate out
into a whole different series of temporal scales of things that happen slowly,
that contextualize things that happen more quickly,
that contextualize things that are even faster than that.
And so one good example of depth might be that if you're reading a book,
then you have to bear in mind which page you're on
within that page, which sentence or which paragraph you're on,
within the paragraph, which sentence, within the sentence, which word,
within the word, which letter.
And by combining your predictions both down the system that way,
but then updating your predictions all the way back up again,
you start to be able to make inferences about the overall narrative that you're reading.
The other thing you mentioned that I thought was interesting was the idea of confabulation
and of how we come to beliefs about other people's behavior.
And I think the same thing is also true about our own behavior
and sort of making an inference about what we've done.
And this comes all the way back to the sense of agency again, doesn't it?
It comes back to the idea that I am inferring, I'm behaving in this way for this reason,
because I've chosen to do this, because I had this goal in mind.
And to come back to the other question, is that real?
Or is it simply an inference about what I've done?
I would suggest that it's certainly an inference about what I've done,
whether or not it's real.
Giovanni and I put together some simulations and some theoretical work
a couple of years ago after a discussion at a conference about
or workshop about machine understanding, suggesting that machine intelligence is one thing,
but actually understanding why you've come to a particular conclusion.
ChatGPT being able to explain to you why it came up with a specific sequence of words
or why a convolutional neural network classified an image in a particular way
is one of the big issues really, and there are solutions coming up,
but it's one of the big issues in the deep learning community as to how you have that transparency
in terms of what the models are doing and why they're doing it.
And Giovanni and I put together some work following that,
looking at understanding of our own actions from an active inference perspective.
And there it was very much framed as I have a series of hypotheses of things I might do,
of reasons why I might do that.
And then after observing myself behaving in a particular way,
I can then use my own behavior as data that I then have to come up with an explanation for.
And it's very interesting to see what happens if you start depriving that of aspects of its behavior
and to see the confabulations that result from that.
I can't remember where it came from originally, the idea of hallucinations being a constrained,
sorry, a perception generally being effectively a constrained hallucination
where you take your hallucination, your simulation of what's going on,
and then you fix it to what's actually coming in.
But you could argue that actually a lot of our understanding about what we're doing
is also just a constrained confabulation in exactly the same way.
Yes, which is very ironic because people diminish GPT
and because they say it's just confabulating,
whereas the preeminent neuroscientists of the day do basically make the same argument about how the brain works.
And even our communication now on conditioning your simulator.
So the semantics are drawn by your own model and simulation of the world
rather than being the simulacrum of mine.
You spoke about machine understanding.
I mean, there's this Chinese remarkument.
And we're in a really interesting time now because we have artifacts that behave in a way
which is isomorphic in many ways.
And it's so tempting to say, well, we're different.
And you could make the ontological argument,
but this psychological argument is a big one as well,
which is we're different because we have beliefs, motives, volition, desires.
We have all of these things.
But as we were just saying before, this is all post hoc confabulated.
We actually don't have consistent beliefs and desires.
It's just a fiction.
Is it a fiction or is it a plausible explanation?
Well, I guess the thing that breaks it for me is the incoherence and inconsistency
because you would think that we would be fully fledged human agents
if we had consistent beliefs and desires.
And it's not to say that we don't because it feels like some of our goals are,
they ground it in some way, like we need to eat food.
But we think of ourselves as being unique as humans
because we have higher level goals and beliefs
that aren't necessarily instrumental to eating food.
And I guess those things in particular might be confabulatory.
Yes.
So on the volition thing, that's something that really interests me
because an active, an active inference agent is we draw a boundary around a thing
and it can act in the environment and it has preferences
and essentially it has a generative model where it can produce these plans,
these policies, if you like, and at the end of every single plan is an end state.
So it's got all of these different goals in mind, if you like.
And in the real world, real in big air quotes, these things emerge.
But when we design these agents, we need to somehow impute the preferences onto them.
So it feels like they have less agency if we impute the preferences.
Would you agree with that?
Interesting question.
And a very relevant question in the current number of industry-related applications of active inference.
I think we were speaking about earlier, there are a number of companies now
that have been set up looking at use of active inference-based principles for various problems.
Companies like Versus that we spoke about before and Stanhope AI that I do some work with as well.
And the issue there is very much, it's a different kind of issue to the biological issue
of describing how things work and it's the issue of saying,
if I now want to design an agent to behave in a particular way, as you say,
am I taking some agency away from that?
There are a couple of things to think about there.
One is thinking about do biological agents actually select their own preferences to begin with?
And I think most people would probably say they don't most of the time.
There may be certain circumstances where they do or where a particular preference might be conditionally dependent
upon the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else.
But it's not that I'm actually selecting, this is what I want to want.
There is a famous quote here, but I can't remember what it is.
I don't know whether you do.
No, it's escaped me about wanting what you want or wanting what you do or something along those lines.
Anyway, the point I'm making is that to some extent our preferences are given to us effectively
through a process of evolution, natural selection, previous experience that has affected
what is a good set of states to occupy.
And those will often be a good set of states that help my survival, that help my persistence,
that help the persistence of the species that I'm a part of.
And arguably, the same thing is true when you as a designer of a particular algorithm
or an agent are giving it a set of preferences.
From its perspective, it's never selected them anyway.
And that's the same as you or I not necessarily having selected our preferences.
There's one additional element that I think is interesting to think about.
And one of my colleagues and collaborators, Nor Sajid, has done a lot of interesting work on this,
which is the idea of learning your own preferences.
Of actually saying, let's create an agent that isn't given preferences to begin with
but is allowed to learn as it behaves what sort of goal states it ends up in.
And there you get some very interesting results.
So she showed that these sorts of agents may end up doing things that you just don't want them to do,
that they end up forming a particular pattern of being or a particular way of being
that you as a designer might never have envisaged.
For example, in an environment with lots of potential holes that it can fall into,
some of these agents just become hole dwellers.
They just decide, I found that the first few times I did this task, I fell into the hole.
So I've decided I'm probably the sort of creature that likes living in a hole.
So that's a situation where you can give it a certain agency.
And maybe that agency is the ability to sort of disagree with what you as a designer might expect or want from it.
Yes.
This is so interesting.
We're getting a little bit into, we'll have a discussion about cybernetics and externalism.
Yes.
But so what you're describing there is the reason why AI systems today are not sophisticated is because they are convergent.
And that's usually because they don't actually have any agency.
So one of the hallmarks of the physical real systems in the real world is that they have these divergent properties.
And that's because you have lots of independent agents following their own directiveness doing epistemic foraging.
So interesting stepping stones get discovered.
And sometimes those stepping stones aren't what the designer of the system would have liked, as you just said.
So there's an interesting kind of paradox there of how much agency do you want to imbue in the agents?
But the other paradox is the physical and social embeddedness.
Because as you just said, cynically, we don't have as much agency as we think we do because we're embedded in the dynamics around us.
And being part of this overall system means that our agency is defined not just by our boundary, but it's by the history of the system.
It's the history of us sharing information of all of the things around us.
And all of these things inform what we do and what our preferences are.
And then you say, well, we can just drop a brand new agent in the system.
And it doesn't quite work because it's a fish out of water.
It's not embedded in the ways that things that emerged in that system were in the first place.
But this does get us onto this discussion of externalism.
So part of the fiction of how we think about cognition is that we think of ourselves as islands that don't share information dynamically with the outside world.
And of course, active inference is a way of bridging these two schools of thought.
So can you kind of bring that in?
I mean, I think you've already done it in a sense.
I'm not sure what else there is for me to say on that.
I'll try my best.
So yes, I mean, active inference is about, well, it's about aboutness.
It's the idea that our brains and our internal state evolves in such a way that reflects beliefs about what's outside.
And I think that's one of the key things that you have to have for any sort of intelligent system.
And that doesn't necessarily exist with other approaches that exist in neuroscience or artificial intelligence.
And I'll just repeat that.
The aboutness is the key thing that what's happening in my head is a reflection or is a description in some way is about what's happening outside my head.
And maybe that's the link with this sort of externalism.
But it's not just unidirectional either.
It's the fact that I'm forming beliefs about what's happening in the outside world.
But I'm also the one influencing the outside world to change it to fit with the beliefs I have about how it should be.
Yes.
Yes.
So there's a kind of model.
So we draw these boundaries and we model the world around us and we influence the world around us.
And that's essentially what active inference is.
I guess it might be useful just to sketch out the the cognitive science idea of of an activism or cybernetics.
So there were folks who really railed against this idea of representationalism, which is this idea of model building in principle.
And active inference is an integrated approach where we allow some model building, but we also think of the world itself as being its own best representations.
How do we kind of bridge those two ideas?
Yes.
And I confess I'm always lost in the distinction between the sort of inactivists, radical inactivists, the sort of different levels of stance you can take on this.
And I think it comes down to that, that from an active inference perspective, both your representations, if that's the right word, the beliefs you have about the world, whether or not that meets the criteria for representation from an inactivist perspective,
is very important, but it is only important in terms of how you act.
If your beliefs did not affect how you acted, clearly natural selection would not have selected you to form those beliefs.
I think it's the simple way of putting it.
So let's talk about some of the kind of the mathematical underpinnings here.
So I think probably one of the main concepts we should start on is this idea of surprise.
And maybe we can talk about it in general terms and then we can move on to Bayesian surprise.
So why is surprise so important in the free energy principle?
Well, it's central to it.
It is the key thing that matters.
We talk about the free energy principle, but in a sense free energy is really there as a proxy for surprise.
So yes, what do we mean by surprise?
And it's another one of those things like the high road and the low road that you can approach from several different angles or several different lines of attack.
If you were modeling something, if you were a Bayesian,
if you took a particular stance on probability theory and wanted to know,
given my model, given my hypothesis, what's the evidence for it?
What you would normally do is calculate something known as a marginal likelihood,
which is just a measure of the fit between your model and the data that you have that you're trying to explain.
That fit trades off various different things.
So it can trade off how accurately your model is explaining the data against how far you've had to deviate from your prior beliefs or from your initial assumptions
in order to arrive at that explanation.
So that marginal likelihood, that evidence is effectively just the negative or the inverse of surprise.
So that's one perspective on it.
The better the fit, the simpler and most accurate my explanation for something, the less surprised I will be by it.
Another perspective on surprise is just this more colloquial sense.
It's the idea that, given what I would predict, how far out of that prediction is it?
One could take a more biological perspective on it and say,
imagine we are homeostatic systems that have some set points we want to keep our temperature within a certain range,
our blood pressure within a certain range, our heart rate within a certain range.
If we find ourselves deviating from that, that is effectively a surprise because we're not where we expect to be.
And so we enact various changes to bring those parameters back in range.
So if our blood pressure is too low, we might increase our heart rate to bring our blood pressure back up to the range we expect it to be in.
And that is, in a sense, what Active Inference is all about.
It's just this idea of keeping things within that minimally surprising range.
But of course, once you put dynamics on it, once you start unfolding that in time,
you end up having to not just deal with how surprising things are now,
but you've got to try and anticipate surprise and behave in such a way that you allostatically control your sensory inputs.
Both your intraceptive inputs like heart rate and blood pressure, etc.
But also your extraceptive sensations, your vision, your audition and the like.
And there's almost no end to the perspective you could take on surprise.
Another perspective on it is that it's a reflective of in a physical system
the improbability of being in a particular state.
From a lot of physics perspectives, improbability is also a social energy.
It takes energy to bring things into less probable states.
And without inputting energy into a system, it will generally end up in its most probable state in the absence of that.
You think of things like Boltzmann's equation and the relationship there between energy and probability.
And that also has a link then to the idea of either a Hamiltonian or indeed a steady state distribution,
which is just what is the distribution things will end up in if left to their own devices for a certain amount of time
until things have probabilistically converged.
And that means that if I would construct a probability distribution over where things will be at a long point of time in the future,
there will come a point at which that probability won't change any further.
And the tendency of physical systems to go to those more probable states is exactly the same as the tendency to avoid surprising states.
And again, we could sort of go on for a while, but I won't on sort of other ways of conceptualizing it.
But hopefully that sort of explains why it's such an important thing that underpins so much of what we do.
We're either trying to sort of evolve as a physical system towards more probable states,
or we are homeostatic or allostatic organisms trying to maintain our internal parameters within the right set points,
or we are more colloquially just trying to avoid things that are different to what we predict,
or we are statisticians trying to fit our model to the world as best we can,
as those things come under the same umbrella of surprise.
Free energy comes in because surprise is not a trivial thing to compute.
Mathematically, it's often either intractable mathematically or computationally,
and so it's just not efficient to be able to calculate.
But free energy is a way of then approximating that surprise.
It's a way of coming up with something that is close enough to it,
or even more precisely, is an upper bound on surprise.
So if you're at the lowest point of your free energy, then that limits how high your surprise can be.
The key additional thing in free energy is that the distance between that bound,
your free energy and your surprise, depends on how good your beliefs about the world are.
And that's where perception comes in, that by getting the best beliefs you possibly can,
you minimise the distance between your free energy and which is upper bounding of surprise and the surprise itself.
So then any further reduction in free energy, you would expect to also result in a decrease.
Sorry, any further decrease in free energy would also result in a further decrease in surprise.
I mean, there's a few things that struck me.
First of all, what struck me is that we're using the language of things like statistical mechanics
and Bayesian statistics and information theory, things like entropy and so on.
And we're interchangeably speaking about the same thing from the perspective of different disciplines,
which I find very, very interesting.
And on the surprise thing, even though in this formalism we are minimising surprise,
I think there's an interesting perspective that sometimes surprise is what we want.
So for example, the chess algorithm, the Elo algorithm,
it's only when something surprising happens that the weights get updated because it's information.
Or people on YouTube, my videos are, they get more views when they have a cash value,
which means they have information content, which means that, you know,
they're actually surprising your predictive model.
Even Arnold Schwarzenegger used to joke about it.
He said, you have to shock the muscles.
You know, you have to do what the muscles don't expect, otherwise there's not an adaptation.
So there's this interesting juxtaposition between actually seeking out surprise,
even though you can think of our brains overall as minimising surprise.
And what was the other thing I was going to say?
Yeah, you were just getting onto variational inference, which is really interesting.
So there's a couple of intractable statistical quantities in this mixture that we're talking about.
I think it's the log model evidence and the Bayesian posterior.
And we can't represent those things directly.
So we have to put a proxy in there, which kind of captures most of the information,
but it's still possible to deal with it.
So how does this variational inference work?
Yeah, so I suppose maybe the first thing to think about though is just to recap what Bayesian inference is.
I suppose we've been talking about it quite a lot without necessarily defining it.
And, you know, many of your listeners, I'm sure, will know already.
But the idea is actually relatively straightforward and well established and quite widely used.
And it's the idea that if I have some beliefs about things that are in my world that I can't directly observe,
I may have a sense of what's plausible to begin with.
And that's what we refer to as a prior probability.
I then also need to have a model that says, given the world is this way,
what would I expect to actually observe?
So, for instance, given where you are relative to me, I can predict a certain pattern on my retina.
And if you were somewhere else, I would expect a different pattern on my retina.
So I might have a prior range of plausibilities as to where you are relative to me.
And then I have a model that explains how I'm going to generate some data based upon that.
And Bayesian inference basically takes those two things and inverts them using Bayes' theorem.
And effectively just flips both of them round.
So you now say, instead of a distribution of where you are relative to me,
I'm now talking about a distribution of all the possible things that I could see on my retina.
And instead of predicting the distribution on the retina given where you are,
I now want to know the distribution of where you are given what's on my retina.
And Bayesian inference, much like active inference, is full of all these interesting inversions
where you sort of flip things round from how they initially appeared.
But the problem is calculating those two things, calculating the flipped model.
So the distribution of all the things on my retina here would now be my model evidence,
my inverse surprise.
And the distribution of where you are relative to what's on my retina is my posterior distribution.
But those things are not always straightforward to calculate.
And so variational inference takes that problem and makes it into an optimization problem.
It writes down a function that quantifies how far am I away from my,
or what would be the true posterior if I used exact Bayes.
And then it says, well, let's parameterize some approximate posterior probability.
So come up with a function that represents a probability distribution that's easy to characterize.
Something like a Gaussian distribution where I know I just need my mean and my variance.
And then just changes that mean and variance until you minimize this function that represents that discrepancy.
Minimize this free energy, also sometimes known as an evidence lower bound, in which case you maximize it.
And interestingly, once you've maximized your evidence lower bound or minimized your free energy,
you end up with a situation where the free energy starts to approximate your log model evidence or your negative log surprise.
And your approximate posterior distribution, your variational distribution starts to look much more like your exact posterior probability distribution.
So it's another one of those interesting scenarios where doing one thing, optimizing one quantity ends up having a dual purpose.
And in active inference, the only additional thing you throw into that is that you want to then also change your data itself.
So you do the third thing, you act on the world to then optimize exactly the same objective.
The interesting thing, I guess, is just contrasting to machine learning again.
So in machine learning, we also have these big parameterized models and we do stochastic gradient descent.
And some might think of deep learning because obviously you can think of everything as a Bayesian.
So you can think of machine learning as being maximum likelihood estimation.
Why is it that we go full Bayesian when we do active inference?
Why not something like maximum likelihood estimation?
It's an interesting question and there are a couple of answers you could give again, some of which are more technical, but some of which are slightly more intuitive.
And I think one of the more intuitive answers is that by having an expression of plausibility of things in advance, you just maintain things within a plausible region.
So maximum likelihood for those who are unaware is where you essentially throw away that prior probability, where you throw away any prior plausibility as to what the state of the world might be.
And you just try and find the value that would maximize your likelihood, which is your prediction of how things would be under some hypothesis or under some parameter setting.
And I think the first thing to say is if you throw away that prior information, then you end up potentially coming up with quite implausible solutions.
That's particularly relevant if you're dealing with what's known as an inverse problem, so where there are multiple different things that could have caused the same outcome.
An example that's often given is that for any given shadow, there's almost an infinite number of things, configurations of the sun and the shape of the thing that's casting the shadow that could lead to exactly the same shadow.
And so maximum likelihood approach just won't be able to tell the difference between all of those things.
However, if you have some prior on top of that, if you have some statement of the plausible things that might cause it, you can come up with a much better estimate of those sorts of things.
Another way of looking at it is that when you're dealing with a maximum likelihood estimate, you're throwing away all uncertainty about the solution.
So you're coming up with a point estimate and you're saying this is the most likely thing, but you're ignoring all of your uncertainty about it.
And I think that is in itself a relatively dangerous thing to do and can lead to the problem of overfitting where you start to become very confident about what you can see from a relatively small sample of things
and you can end up with all of these well-described in the media scenarios of complete misclassifications based upon that sort of overconfidence just because all the uncertainty is gone.
A more technical way of looking at it, I think, is if you think about what a free energy is.
So free energy is our measure of our marginal likelihood that we're using when we're doing Bayesian inference.
And one way of separating out what a free energy looks like is to have our complexity which is effectively how far we need to deviate from our prior assumptions to come up with an explanation.
And our accuracy, which is how well we can fit our model.
Accuracy is common to both maximum likelihood type approaches because we're trying to find the value that is most or the most accurately predicts our data and also to Bayesian approaches, both want to do that.
But what's thrown away in the maximum likelihood type approach is the complexity bit, the how far do you deviate from your priors.
So there's an inbuilt Occam's razor, the idea that the simplest explanation is a priori more likely that you get from a Bayesian approach that you throw away when you're dealing with maximum likelihood estimation.
I wondered to what extent does the active part play a role here.
So even in machine learning, there's something called active learning where you kind of dynamically retrain the model.
Well, there's something called machine teaching where you dynamically select more salient data to train the model and the model gets much better.
And in things like Bayesian optimization, for example, by sort of maintaining this distribution of all of your uncertainty in a principled way, you can go and seek and find more information to kind of improve your knowledge on subsequent steps.
So I guess it's sort of bringing in this idea of it's not just what happens now, it's about how can I improve my knowledge of the world over several steps.
Yes, and that reminds me about the point you're making earlier that sometimes we actually do things to surprise ourselves, which seems very counter-intuitive in the context of the idea that we're trying to minimize surprises on our sole objective in life.
And sometimes people talk about this in terms of a dark room problem, the idea that actually if all you want to do is minimize your surprise, you just go into a room, turn off the lights and stay there because you're not going to experience anything that's going to surprise you.
Yes.
I mean, the answer to this problem is that actually as organisms, as creatures, we don't expect to be purely in a dark room and the sort of organism that would be is, again, probably not a very interesting one.
And that what we predict, what we'd be surprised by might be permanently staying in a dark room, but it goes even further than that.
And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable world where I know what's going to happen next.
The best way of doing that is to actually gather as much information as you can about the world around you.
So the first thing you do really is you turn on the light and see what the room looks like because that might then predict all the sorts of things that could fall on you in that room and could potentially cause surprise.
And by knowing about it, you mitigate the surprise that you might get in the future.
And as you say, you can only really do that if you know what you're certain about.
And so if you take a maximum likelihood approach, if you work based upon point estimates and you have no measure of your uncertainty, then there's no way you can possibly know what you're uncertain about to be able to resolve that uncertainty.
So this brings me on to causality.
We know that predictive systems which are aware of causal relationships work better.
But if we just bring it back to physics first, to you, what do you think causality is?
It is a tricky issue as to what causality is.
And I think whether it exists or not is really a matter of how you define it, isn't it?
And some would define that purely in terms of conditional dependencies that the behavior of one thing is conditionally dependent upon something else and therefore you could say that the one thing causes the other.
But as we know from Bayes theorem, that's not quite good enough because you can swap any conditional relationship around through that process of inverting your model.
Sometimes that causality is written into the dynamics of a model.
So this would be the approach used in things like dynamic causal modeling of brain data, where you might say that the current neural activity in one area of the brain affects maybe the rate of change of neural activity in another part of the brain.
And it's the way in which those dynamics are written in, the fact that it's one affects the rate of change of the other that gives it that causal flavor and a very directed perspective on it.
Probably the work that is most comprehensive on this is looking at people like Judea Pearl and a lot of his work on causality.
And there, there's a lot of detail about the notion of an intervention.
And I suppose you can think of this in terms of how you might establish causation in a clinical context.
If you were to run a trial to try and establish whether one thing's caused another, you need to make sure you're not inadvertently capturing a correlation or a conditional dependence that could go either way or a common cause of both things that depends upon something else.
And typically the way you do that is you intervene on the system, you randomize at the beginning to make sure that people are assigned to different treatment groups at random so that you break that dependency upon something prior to it.
And then anything that happens going forward is going to depend on the intervention that you're doing.
So I think that's probably the key thing that gives you causality or perhaps defines causality.
It's the idea that an intervention is what will change it.
If you intervene in one thing, that should then in a way that doesn't necessarily match its natural distribution.
If you hadn't intervened at all and then see what the effect is.
Yes, yes.
I mean, by the way, Judea Pearl is really interesting.
I want to study his book, The Book of Why.
It's one thing that we've really dropped the ball on actually.
But I suppose one way to think about it is if you go back to the core in physics as a whole bunch of equations to describe the world we live in,
and those equations don't say anything about causality and they're even reversible.
And then you can think, OK, well, maybe it's a little bit like the free energy principle.
You know, it's a lens.
Like really, there's only dynamics.
But when you look at these dynamical systems, then behaviors emerge and somewhere up that chain,
you can say, OK, now we've got causality and it's something which is statistically efficacious to build it into our models.
But, you know, where does it come from?
Well, it comes from us, doesn't it?
It's a hypothesis to explain a particular pattern of dynamic.
Yes, yes.
And we might infer causation based upon, again, a particular pattern of how one thing reacts to another.
So if you imagine you've got, you know, the classic physics example, billiard balls bouncing into one another.
How do you know that the collision of one ball with another is causative of the subsequent motion of the second ball?
And you could argue that that's due to a particular pattern of which variables affect which other variables and the particular exchange between them.
And this comes back quite nicely to things like the physics perspective on the free energy principle.
There's an idea that actually one could see the location of a particular ball as being, you know, maybe its internal state.
And then the action that that then causes is perhaps the, or in fact, you could say that the action is the position of the ball,
the force that results from that action is the sensory state of the next ball, which then changes its velocity to then change its action relative to something else.
You can sort of rearrange those labels slightly, but there is a directional element to it.
And in that sort of pattern of causation, you really do expect the position of one ball to have an effect on the rate of change,
or in fact, even the rate of rate of change of the second ball,
which again, I think brings us back to those kind of dynamical descriptions of causality where one thing might affect how another thing changes.
So you almost get it from the dynamics itself.
But again, to some extent, it comes back to semantics, doesn't it?
It comes back to what do we mean by cause?
Well, I suppose cause is a hypothesis as to a particular configuration of things,
but then you've got to write down what does that hypothesis mean?
What's my model of what a causation involves?
Yes, yes.
I mean, we were just talking about, you know, building these models.
And one of the bright differences from machine learning is that we need to build a generative model by hand.
So we have to define these variables and some of them are presumably observed
and some of them are not observed.
They're inferred.
And that process seems like you would need to have a lot of domain expertise.
And it seems like something which is at least has a degree of subjectivity.
I mean, we were just talking about causality, for example.
There are many ways you could model the risk of cancer from smoking.
It seems like there are many, many different ways of building those models.
So that subjectivity is interesting.
I mean, are there principled ways of building these models?
Yes.
And in a sense, it all comes back to the same thing again.
It comes back to which model minimizes the surprise of the best.
But there are interesting questions amongst that.
So how do you actually choose the space of models that you want to compare?
So you're right to say that often there is some specific prior information
that's put into models and active inference.
And very often we do end up building models by hand
to demonstrate a specific outcome or a specific cognitive function.
But there's no reason why it has to be that way.
You can build models through exposure to data where the models are selecting the data
to best build themselves.
But the question is how you do that?
How you start to add on additional things?
How you start to change the structure of your model?
But there's a lot of ongoing research into that.
And I think there are now methods that are coming out
that will allow you to allow an active inference model to build itself.
And the way it will do that will be sort of adding on additional states
and potential causes, adjusting beliefs about the mappings and the distributions
and the parameters have given this then that.
Adding in additional paths or different transitions that systems will pursue.
So it's a fascinating area.
I think it's one that's still a growing area.
But it's this idea of structure learning,
of comparing each alternative model based upon its free energy or model evidence
or surprise as a way of minimizing that by being able to better predict things.
Yeah, because that's something that we humans, we seem to do really well.
So we can first of all, via abduction,
we can select relevant models to explain behavior, what we observe.
But we also have the ability to create models.
In fact, I think of intelligence as the ability to create models.
So we experience something and I now construct a model to explain this
and similar experiences in experience space.
But in a machine, it's really difficult.
So in machine learning, there's this bias variance trade-off.
So we deliberately reduce the size of the approximation space
to make it computationally tractable.
And when we're talking about building these models just from observational data,
it feels like there's an exponential blow-up of possible models.
So I can imagine there might be a whole bunch of heuristics around library learning
or having modules.
So these modules have worked well over there.
So we'll try composing together known modules
rather than starting from scratch every single time.
I mean, what kind of work is being done there?
I mean, I think you're right about it's not going to be worth starting from scratch every time.
You can sort of build models by saying,
okay, let's start with something very simple with a sort of known structure.
And I think it's sensible to use some priors in that
rather than starting from complete, completely nothing
because there are some things that we know about in the world
and there's no point hiding that from the models we're trying to build.
And that might be a simple structural thing like things evolve in time.
So one thing is conditioned upon the next, is conditioned upon the next
and things now will influence the data I observe.
Things well in the past might not anymore.
But then there's the question of, well, how can a model then grow?
What are the things that you can add to it or subtract from it?
And that subtraction is another key element
because you could take this whole problem from the other direction
and you could say, well, let's start with a model that just has everything in it
and take away bits until we've got the model that's relevant to where we are at the moment.
And we know that during development there's a lot of synaptic pruning that goes on
and removal of synapses that we have when we're much younger
compared to, compared to as you get older.
So what can you add on? Well, it depends what your model looks like.
So if your model says there's a set of states that can evolve over time,
there are a set of outcomes that are generated.
Well, we know what the outcomes are, we know what the data are
because we know what our sensory organs are.
So it's the states that are going to change.
So do we add in more states? Do we allow them to take more alternative values?
Do we allow their transitions to change in more than one different way?
Which ones can I change? Which ones can I not change?
And it's really just asking these questions that helps you to grow your model.
So you say, well, let's try it if I allow this state to take additional values,
if it's not providing a sufficiently good explanation for how things are at the moment.
And if that improves your prediction, that's good and you keep it,
and if it doesn't, then you get rid of it.
Do I now need to include additional state factors?
So you could either say there is one sort of state of the world
that can take multiple different values,
or you could say actually this is contextualized by something completely separate.
So where am I along an x-coordinate?
You also need to know where you are along a y-coordinate
to be able to contextualize what you're predicting.
So it's just asking what is in a model? How do you build a model?
Almost gives you the answers to the ways or the directions in which you can grow it.
The other thing you can then do when you're trying to work out how to grow it
is to say, well, let's treat this as the same sort of problem as exploring my world,
selecting actions that will then give me more information about the world.
You could say, well, actually now let's treat my exploration of model space
as being a similar process of exploration.
Which of these possible adjustments to my model
might lead to a less ambiguous mapping between what I'm predicting
or what's in my world and what I'm currently predicting?
Yes, it rather brings me back to our comments about the space
or the manifold that the models sit on,
whether they would have a kind of contiguity or whether they would have a gradient.
I guess I'm imagining a kind of topological space that the models would sit on.
I don't know whether it's worth bringing in, I mean, obviously you're a neuroscientist
and the way brains work, we must do this.
I mean, of course, there's this idea of nativism.
Some psychologists think that we have these models built in from birth
and then the other school of thought is that we're just a complete blank slate.
If you read Geoff Hawkins, he talks about the neocortex,
this magical thing that just builds models on the fly.
But perhaps one difference, at least, between brains and machines
is the multimodality, which is to say we have so many different senses
that that creates a gradient or that makes it tractable.
Because when a model from a particular sensation starts predicting well,
we can rapidly optimise and go in the right direction.
Because the problem seems to be that there are so many directions we can go in,
doing some kind of monotonic gradient optimisation will often lead us
into the wrong part of the search space, so we've wasted our time.
Yeah, I think that's a really good point, absolutely.
As soon as you know how one thing works or how vision works,
I suppose vision and proprioception is a good example, isn't it?
If I recognise where my hand is and I can make a good estimate of that visually,
then that helps me tune my joint position sense as to where my arm might be.
And it's always fascinating to see situations where that breaks down,
so there are a number of conditions where if you lose your joint position sense,
you're perfectly okay holding your arm out like that until you close your eyes,
at which point you start getting all these interesting twitches and changes.
So yes, the multimodality I think probably is a really key thing
that really does help constrain the other senses
because you're just getting more information about each thing.
Maybe we should just talk about chapter 10 in general,
because that was kind of like the homecoming chapter,
if you like sort of bringing together some of the ideas.
So can you sketch that out for me?
Yeah, so I think towards the end of the book,
the idea was to try and bring together a lot of the themes that had been discussed earlier on,
but to also make the point that...
Well, I'll come back to one of the things you said earlier was
about how it seems we're talking about lots of different things from different perspectives,
but actually they're really the same thing.
So we talked about how surprise is also a measure of steady state,
of energies of various sorts, of statistics and model comparison,
of homeostatic set points,
that all of these things can be seen through the same lens.
But again, taking one of those inversions,
you can invert that lens and say,
well, actually you can start from the same thing and now project back into all of these different fields.
And I think that's a useful thing to do,
because I think it helps foster multidisciplinary work,
helps to engage people from different fields and areas,
and helps us know what's happening elsewhere,
so that you're not just duplicating everything that people have already done.
So I think it's really important to have those connections to different areas,
and the chapter 10 from the book was an aim to try and connect to those different areas,
whether it be to things you've spoken about like cybernetics and inactivism,
and just to try and understand the relationship between each of them.
Well, I mean, quite a lot of people use this as a model of things like sentience and consciousness in general.
I often speak about the strange bedfellows of the free energy principle.
So there are auto-poietic inactivists and phenomenologists
and people talking about sentience and consciousness.
Obviously, you're a clinician, you're working in a hospital.
So it's just this incredible conflation of different people together,
and they all bring their own lexicon with them.
But maybe we should just get onto this kind of sentience and consciousness thing,
because that seems quite mysterious.
We almost come back to one of the themes we've spoken about a few times,
which is that the specific words we use for things in the effect that different people,
that has on different people.
So some people, I think, would probably get very angry with the idea of using sentience
to describe some of the sort of simulations and models that we would develop.
But that comes down to what you mean by sentience.
And I think one of the key things for sentience is the aboutness we were talking about before.
The idea that our brains or any sentient system really is trying to...
Try not to anthropomorphise too much, but it's almost impossible to do in this setting, isn't it?
Not trying to, but the dynamics of some system internally to the system
are reflective of what's going on external to it,
and that you can now start to see those dynamics as being optimisation of beliefs.
And those beliefs are about what's happening in the outside world
and about how I'm affecting the outside world.
And I think that probably gets to the root of at least a definition of sentience,
and one that I'd be happy with,
which is just the dynamics of beliefs about what's external to us
and how we want to change it.
And there are very few things other than that sort of inferential formalism that give you that.
Yes, I mean, in a way, one thing I like about it is...
I mean, we are talking as physicists, so we are materialists.
It's very no-nonsense.
It's quite reductive as well, because there are those who believe that
these kind of qualities that we're speaking about, certainly with conscious experience, for example,
that it's not reducible to these kind of simple explanations that we're talking about,
that it has a different character.
David Chalmers talks about a philosophical zombie.
So, for example, you might behave just like a real human being,
but you could be divorced of conscious experience.
So he says that you can think of behaviour, dynamics and function
and conscious experience as something entirely different.
But as an observer, you would never know.
So, yeah, it feels very no-nonsense, doesn't it?
But that wouldn't be satisfying to a lot of people.
No, it probably wouldn't. You're right.
Yeah, and particularly when you get onto questions like consciousness as well.
I think it does become very difficult, because once you're putting forward
or advocating a theoretical framework that seems like it's supposed to have all the answers,
I mean, in reality, it doesn't.
I think it's a useful framework to be able to ask the right questions
or to be able to articulate your hypotheses.
If you think that consciousness is based upon the idea of having some sense of trajectory,
of temporal extent and different worlds I can choose between
or different futures I can choose between, that might be a key part of it.
But for some people, that's not what they mean by consciousness.
I found, particularly reading books by people like Anil Seth on this sort of topic,
I found one of the interesting comparisons being the questions about consciousness
versus questions about life.
And we almost don't ask what life is anymore.
It doesn't necessarily seem that mysterious.
Just because we've had so much of an understanding of the processes involved in life,
the dynamics of life and the way biology works, it's still much more to go.
But the question of what life is just doesn't seem as relevant today as I suspect it did
many years ago with those sorts of questions that were being posed.
And perhaps we'll see the same thing with questions like consciousness.
Yeah, it's interesting though how vague many of these concepts are.
And it's quite an interesting thought experiment just to get someone to explain
just an everyday thing.
Like what happens when you throw coffee on the floor.
And just keep asking why.
And just observing how incoherent and incomplete the explanations are.
And it's the same thing with life.
It's the same thing of consciousness.
It's the same thing of causality, agency, intelligence, all of these different things.
And I guess most people don't spend time digging into their understandings of these things
and realizing how incoherent and incomplete they are.
Life is quite an interesting one in particular
because I think one of the achievements of active inference
is blurring the definition of or the demarcation between things which are and are not alive.
For example, the orthopedic and activists, they think of biology as being instrumental.
And what the free energy principle does in my opinion is it removes the need for this.
It almost removes the need for biology entirely.
It just says it's just dynamics, it's just physics.
But just on that point though, I think many of our ideas about the world are quite incoherent.
Yeah, and I think it's interesting that one of the things that you're saying,
and I would agree with you as one of the big advantages of active inference based formalisms,
you'll probably find some people will say that's a problem with it,
that actually there is a clean distinction in their mind between these different things.
But then I think the challenge is to work out what that distinction is, if it exists.
And it may be a distinction in their mind that doesn't exist in somebody else's mind.
And so getting people to try and or trying to support people
to be able to express that in a very precise mathematical hypothesis
I think is quite a useful way of trying to explore those problems
because clearly for some people there is something that's getting at it that is not quite explaining.
And it's interesting to try and explore that and to work out what that thing is.
Indeed, indeed. And just final question, what was your experience writing a book
and would you recommend it to other people?
I enjoyed writing it. I think it's time consuming and can feel like it's going on forever
some of the time compared to anyone who's had some experience of writing
writing papers will often find that at the point where you're ready to submit it
you're just sick of it and want to see the back of it.
And then it's rudely returned to you by the peer reviewers with lots of comments that you then have to change.
Writing a book obviously takes you much longer, so you end up being almost more sick of it at various times.
But it's quite fun as a collaborative project.
It's quite interesting to get other people's perspectives on it
and I was lucky to have great collaborators to write it with.
And I think it really is a good way of organizing your thoughts in a slightly more holistic way
than you would while focusing on a very specific topic in a research paper.
And I've also just enjoyed the response I've had from people who've read it
some of whom have picked out a number of errors.
But generally everybody's been very supportive of that
and people seem to have responded well to it, which I think is always encouraging
and that's what we hope should happen.
Wonderful. Well look, Thomas, it's been an absolute honor having you on the show.
I really appreciate you coming on. Thank you so much.
Well, thank you. I've enjoyed it.
Thank you.

Why do you do what you do, after all of these years,
you keep persisting?
I mean, why did you write this book?
Well, the reason I wrote the new book
is because I sensed a moral decline in Silicon Valley.
That's, in fact, one of the chapter titles
is the moral decline of Silicon Valley.
I particularly sensed it when Microsoft
released this product called Sydney.
And Kevin Roos had this conversation with it
in which it told him to get a divorce and all this stuff.
Instead of pulling the product,
Microsoft just put some band-aids on it.
And that was a sign to me that things had changed.
And within a few days of that, Satya said
that we're gonna make Google dance.
The whole culture changed like overnight.
You know, the antecedent condition,
precipitate and condition was the popularity of chat GPT.
Suddenly, you know, people thought there was real money
to be made here and their postures changed entirely.
And that worried me because I do think
the technology is premature.
I do think a lot of harm can come from it.
And I kind of dropped what I was doing research-wise
and really moved full-time into policy.
And in some ways, the book is actually a memoir.
It's not couched that way at all,
but it's really a memoir of that time
when Gary went to the Senate,
had a great conversation with the senators,
and then realized that nothing was going to happen.
So, you know, I had this peak experience
talking to the senators and feeling
like they were gonna do something.
And then gradually bit by bit,
and I was warned, you know,
that this might come down this way,
but I had to see it for myself.
I'm naive, I guess, in that, at least that one respect.
Seeing that nothing was actually happening
and seeing, you know, learning close-up
how the lobbying works.
You know, like one time I was in DC
and I took a meeting with Google,
and it was a time when I got the closest hotel
I could get to the Capitol,
because I was gonna be like in and out
and I had to have a bunch of meetings
that I couldn't, you know, be late for the meeting.
And like, Google had an office next to that hotel,
which is like right on top of the Capitol.
Like, they're so there embedded in it, in every level.
I mean, that's just a sort of metaphor.
And it's not just Google, it's open AI and Meta and all.
Once you learn how all that lobbying is working,
and once you see how like good ideas
that have big support go to die,
never even get voted on,
like it's incredibly disillusioning.
It's the disillusionment really led to this book
called Taming Silicon Valley.
I realized that we are heading very quickly
towards an oligarchy.
I mean, imagine the data that these guys are getting.
Like open AI is like getting access
to everybody's documents and wants like, you know,
everything about you,
which they're of course gonna weaponize.
They're gonna sell in some, you know, various ways,
including to target a political advertiser's probably,
you know, I mean, they'll say they won't,
but like, we've seen this movie before.
They're getting an enormous amount of power.
A lot of that is because people think
that they're going to get rich,
which may not actually be true, ironically.
So they've been given power in advance
of actually delivering the goods.
Like people think they're gonna make AGI
and therefore they should be powerful.
But in fact, they've made LLMs, which are not AGI,
that are not actually commercially useful,
but they give an enormous power.
They're like on all these committees and whatever.
And so Silicon Valley has suddenly got a lot of power.
They've taken a lot of power from the government.
And the government should be like,
hey, hold on guys, like, you know, prove yourself first.
And, you know, you can have some power,
but not infinite power.
Government's not doing anything about it.
EU is different, but in, you know, North America,
not so much.
And I watched all of this happen.
And I thought, I don't know what the consequences
we're gonna be.
And I realized we just could not count on the government.
I had this tension as I wrote this book,
this crazy tension, which was,
if the world went the way that I wanted to,
I would have entirely wasted my time writing the book.
And like anybody else, I don't wanna waste my time
writing a book that nobody's gonna read
and is out of date.
And I had this fear as an author
that the book would be completely undermined
because suddenly Washington would get its act together.
But of course it didn't.
You know, I would have been happy for the world
and sad for my book.
It was a weird position to be in,
sort of like betting against yourself or something.
I don't know.
So I was afraid maybe things would actually be done right.
We wouldn't need the book.
I need not have had that worry for a second
because Washington in fact, mostly abdicated.
Abdicated.
Chuck Schumer in particular, you know,
had the power to do something here
as the Senate Majority Leader
and put some strong legislation forward.
And he didn't.
He took eight months of listening meetings
and put out a white paper rather than an actual law.
So he kind of ran out the clock.
Now as we record this, you know, I guess,
very little is going to happen
because the elections are soon.
And nobody, you know, the way that dynamics work
in Washington, like nobody wants to stick their neck out
because it might hurt them in the election.
So like basically this period of great excitement
about how we could handle this stuff,
it started around the time when I appeared in the Senate,
which was May 16th of last year,
as entirely dissipated.
It's gone with the wind.
The opportunity was completely squandered.
So the point of the book is we can't trust
these companies to self-regulate.
They don't do the things that they promised.
You would know better than me, for example,
the things that they promised about
pre-testing to the UK government and then didn't deliver.
I think it's a big scandal in the UK people
and you asked me not to know about it.
But there's one example where they have not done
what they said they would do with self-regulation.
And of course, they'll weasel down anything that they said
so that, you know, it's less invasive
to what they're doing and so forth.
And then the big issue with governments
is regulatory capture or just doing nothing.
And that's, you know, I read the writing on the wall
and realized that nothing was gonna happen.
And that really logically leaves only one possibility
to get this right.
And we don't get it right, it's gonna be bad.
And it's gonna be social media, but much worse,
move fast and break things.
And so the only thing left is to directly appeal
to the people and so that is what I am doing.
We're trying to do, we'll see if anybody cares.
But I am out there talking about this stuff,
trying to get the citizens of the United States
and some other nations to speak up loudly,
do things like boycotting, for example,
and say, look, if the government's not gonna take care
of the artists, the government's not gonna take care
of the writers, we're not gonna use software
that steals from artists and writers.
And we know we're next, you know?
It's like Pastor Nemo or first they came for the Jews
and the gay, the companies wanna take everything.
They really want the whole ball of wax.
They're gonna take your keystroke loggers,
whatever it is that you do,
if you do something that's on a computer
and they're gonna try to replace you.
That is the game now.
And so if we don't stand up together
with coordinated action and say,
look, we want a more equitable AI here,
this doesn't mean equity,
like everybody gets the same outcome,
but equitable, like everybody gets a fair chance
and like if they use your IP,
you get some compensation and so forth.
There's a million different aspects to this.
If we don't stand up and say we want democracy to function,
we're not happy with the deep fakes,
deep fakes is the one place
maybe something will happen legally.
But if we as the people don't stand up
and say, this is really important
and we don't do it soon and this is really important,
we're gonna be screwed
just the way that we were with social media,
but possibly worse.
So the problem with social media
is things got entrenched and we can't fix them now.
I mean, yeah, the child act just passed.
But by and large, like social media is what it is now.
There's nothing we can do about it and it's not good.
It's probably a net drain on society.
It's fun, I use it, but it has a lot of problems.
And AI, we're just giving so much power to these companies.
And if we don't set the right precedence
in the next, I don't know, 24, 36 months
or something like that,
we are gonna be stuck with whatever comes up,
which is probably gonna mean basically anything goes.
Like we have section 230 says that these companies
are not liable for anything on social media, basically.
I think we could go into the story,
but it was maybe well-intentioned, but the world changed.
The law did not keep up with the difference
between being a carrier of information
and being a company that prioritizes social media feeds
and makes more money if it makes things more polarized.
Like the laws didn't keep up in their bad laws.
We were gonna be stuck in the same position.
And so the choices that we make as a society right now
are gonna have an effect for the next decade,
maybe the next century.
I wrote this book to wake people up.
Gary, it's an honor and a pleasure to have you on MLST.
I love the show, I'm glad to be back.
Wonderful.
So you gave the keynote this morning at the AGI conference
and it was fabulous.
So by the time folks at home watched this,
you had to see in the keynote.
And there was about a 10 minute section towards the end,
which was absolutely hilarious.
But yeah, why don't you tell me about that?
About the talk as a whole or about the talk as a whole?
I don't remember what was the last minute.
Well, look, it was an interesting way of giving a talk.
I rewrote the entire thing.
But it was almost like something borrowed something new.
So the context is I actually gave a keynote
at this same conference three years ago.
And it's not that often that you go to the conference
twice in a three year period.
And typically if you give a keynote,
like it's once every 10 years or something like that.
And so I gave it a keynote three years ago
and it's been such an interesting
and yet such a disappointing three years in AI.
Most people are excited about it.
I'm a bit disappointed and I wanted to explain why.
And so I thought about it, I looked at that old talk
and it was like almost every word here is still true.
I thought about it a little more
and I realized that there was something I missed before.
And so the talk was kind of divided into two parts.
One was all the stuff that really hadn't changed
despite billions of dollars
and enormous excitement and enormous press.
And then the last part was
what I really missed the first time around.
And that was interesting too.
So the first part was basically I went a few years ago
and I said, everybody's excited
about these large language models.
This was before they were really big
but they just started being called foundation models.
Everybody was excited and I said,
well, what should a foundation be?
Ernie Davis and I said this together.
Well, foundation should be like something robust
that you can stand on.
That is what a foundation of a house is or building.
And these models aren't that.
They make all kinds of dumb errors
and you can't really trust them.
That was the talk I gave a few years ago
and I pointed out like why there was a lot at stake
like, you know, telling radiologists
they shouldn't train anymore.
Like hyping these things actually has a cost for society.
So I wrote that whole talk before and I looked at it
and like this is all still true.
The examples have changed.
I don't know how many people will know
Welcome Back Cotter.
The names have all changed since you hung around
but it's all still basically the same.
That was about a high school.
The names have all changed,
the little details on the errors
but basically we still have unreliable AI.
You know, since large language models came on the scene
we have something that looks general
but it's not that intelligent.
It's not that reliable.
You can't really count on it.
It's not nearly as reliable as a calculator is, for example.
Right?
I mean, calculator, you type in three times 17
and you get 51 and you're good to go.
On a large language model, you never know what you're getting.
That was true in 2021 when I gave this before
and it was true today in 2024.
And in fact, the privilege of writing a talk yesterday
is we wake up and you can add another example.
Like these things are just,
they're just not trustworthy.
They're interesting but we see the same problems as before.
So that was the first part of the talk.
That was the larger part of the talk
and it literally went like slide by slide.
This was still true.
This is not true.
And most of it was true.
For fun, I used like orange letters
where something was new and not that much was new.
And then there was the part that I missed in 2021.
I had a little hint of it
but really not wasn't clear in my head.
It was clear in some other people's heads.
But the thing I missed then in my kind of critique
of large language models was, excuse me,
the thing that I missed in my critique in 2021
was what was going to happen to society
and to the tech industry.
More cynical people than I may have seen it coming.
I didn't quite.
I grew up in the kind of era of Google.
I mean, that wasn't, let me say that again.
I started thinking about the tech world a lot
in the age of Google.
And Google had its problems with surveillance and capitalism
but I think was genuine in saying don't do evil.
They really didn't want to be evil.
And I think the companies now really don't care.
They've put in all this money and all this chips
and they need to make back the money.
And that's just driving so much in so many ways.
It's driving the hype.
It's driving decisions about copyright law
and exploiting people and so forth.
And the last part of my talk was really about
what I would call the moral decline of Silicon Valley,
which is actually the name of a chapter in my new book.
I think it's been precipitous.
I think there's really been a change.
Like I don't see Steve Jobs being happy
with what's going on right now.
Like he didn't build Apple to be this kind of company.
Apple still I think is not so much.
But so many of these other companies,
it's all about the surveillance capitalism.
It's all about making as much money as possible.
It's about screwing artists,
which I think Jobs would not have done.
I think Jobs really cared about the artists.
And it's not that anybody wants to screw the artists
but they're completely indifferent to it at some level, right?
They'll make a licensing deal if they're forced to it,
but it's not like they want to.
You have these people talking about universal basic income
and yet they don't want to pay artists a nickel
if they don't have to.
The courts force them to, they'll pay the nickel.
But they're really trying to not pay the artists,
not pay the writers and so forth.
You have a lot of people that I think are really just
in it for the money and don't really care about society.
And that's having consequence.
And so the last part of the talk was like really,
what do we do about that?
Can we trust these companies to self-regulate?
No, we can't.
The book, Untamed Silicon Valley,
is also about the fact that we can't really trust
governments to do the right thing either.
The governments are lobbied constantly by these companies.
There's so much money behind the scenes.
And so here in the United States,
hardly anything has happened.
You probably know, I testified in the Senate a year ago
and that was kind of one of the highlights of my life.
It was amazing.
It was this historic moment, Sam Altman was there.
The Senate was there.
It was the first time the Senate had a full hearing
on artificial intelligence policy.
And it really, like I remember walking to the Capitol
the night before and seeing it, you know, twilight.
It just kind of blew me away.
And it was amazing to be there.
And it was also amazing because all the senators
seemed to understand the urgency of the moment.
How important it was that we regulate AI in a right way,
not too strong, not too weak, that we get it right now.
They all realized that we had screwed up
with social media really bad,
that they had screwed up with social media.
They were incredibly humble.
It was amazing watching the senators
who, as a lot of people said to me afterwards,
were on their best behavior.
They really seemed to get it.
And I was so excited.
And I've been so disillusioned ever since
because that's over a year ago and nothing has passed.
The Senate hasn't even voted on any serious AI regulations,
a little bit that's coming up soon.
But by and large, nothing has happened.
On that though, you've spoken about the apparent,
you called it the messiah myth of Silicon Valley.
And is that what's going on?
I mean, the open AI and Anthropic,
they seem to be doing a lot of things around safety.
Is that just theater?
Well, it's hard to get into other people's heads.
But I would say that there's more theater than not,
that you have companies like Open AI and Anthropic
publicly saying they're for AI regulation.
And then they're out there trying to weaken
whatever regulation is proposed.
They all tried to block SB 1047.
And ultimately, Anthropic apparently was instrumental
in weakening it a good bit at the last minute.
Open AI was, Sam Altman was telling the Senate
while I was sitting next to him
how important AI regulation is.
And behind the scenes,
Billy Perrigo reported this in Time Magazine,
the lobbyists for Open AI were trying to weaken the EU AI Act
and probably succeeded some.
So there's definitely like a two-sidedness to it
where there's public statement that they're supportive
and then in private, they really aren't.
Why is there such a divergence between
the perception of this technology and the capability?
And I often look at people's Twitter
just before an interview
and you posted a beautiful example
which reminded me of an old example of yours
from a couple of years ago.
It's an astronaut riding on a horse.
But of course, it wasn't supposed to be that, was it?
Yeah, so I actually almost went apoplectic
when I tried that one.
So I wrote a whole paper, a whole sub-stack essay
called Horse Rides Astronaut.
And it was really a riff on something that goes back
probably to Chomsky, but I kind of knew it through Pinker.
He had this old example of Man Bites Dog
as opposed to Dog Bites Man.
So it's not really news,
it's a journalist I think have used this expression, right?
It's not news if a dog bites a man,
but it is news if a man bites a dog.
So I kind of riffed on that when Dolly too came out.
So you might remember when Dolly too came out,
it was a big deal.
It was the first of these really good
image generation things.
20 minutes after it came out,
or maybe it was an hour after it came out,
Sam Altman posted, AGI is gonna be wild.
And a lot of people thought,
wow, this is like the AGI moment.
And I looked at this stuff and I realized it's not.
There's nothing to do with AGI.
It's really nice graphics.
These systems are reconstructing images
in a really interesting and powerful way.
But they don't really understand language.
I did some experiments with Ernie Davis
and Scott Aronson.
And then later with Evelina Levada and Elliot Murphy,
showing that these systems don't really understand
the compositionality of language,
which is to say that language is made up of parts.
You put them together in larger holes
and you're mapping a syntax onto a semantics
and you're deriving it from there.
Frege is the philosopher that we most associate
with that concept.
So philosophers have been thinking about this
for a really long time.
And formal semantics, people like that in linguistics,
have thought about it a bunch.
And it was clear playing with Dolly for a few minutes, really,
that he didn't really understand compositionality.
In fact, in linguistics, computational linguistics,
there's an old idea of a bag of words model.
And what a bag of words model is,
is you just take the words in a sentence
and you scramble them up, as if they were in a bag,
and like how much can you explain with that or whatever.
And a bag of words model is almost like a control group.
It's not a very good control group.
You know, if you can't do better than a bag of words,
then it says you don't really understand
the structure of the sentence,
the way that the meaning relates to the parts of the words.
And I could tell playing with Dolly,
even for a few minutes, it had a lot of that flavor.
It wasn't literally that, but it was more like that
than a system that really understood
the components of the sentence.
And so I thought about that in Pinker's old example.
So I wrote this sub-stack essay
called Horse Rides Astronaut,
and it was riffing off the old Man Byte's dog example.
So, you know, we have lots of astronauts riding horses,
but we don't have many horses riding astronauts.
And I showed in this sub-stack essay
called Horse Rides Astronaut,
that Dolly tended to have problems with it.
That if you said, it's hard to even say it right,
if you said horse rides astronaut,
it would tend to give you the more canonical
astronaut rides horse.
And then I went through all of the kind of stupid,
or not stupid, that's not the right way to say it,
all the kinds of defensive objections,
the people who love this kind of AI would make,
and they would say, well,
that's because the system has enough common sense
to know this is impossible and yada yada.
And what I showed is actually,
if you prompted it the right way,
it could actually do this.
So it wasn't that it couldn't draw the graphics
of a horse on top of an astronaut.
And it wasn't because it thought it was,
because the system thought it was literally impossible,
but just it didn't understand the relation
between the words in that sentence
and what it was supposed to do,
which unfortunately is it's one job.
You know, the hashtag on Twitter, you had one job.
Your one job of your Dolly is to understand
the meaning of the sentence and draw the picture.
And it couldn't do it for these kinds of cases.
And then I showed later another example,
I wrote a whole nother essay about,
I can't remember the title,
but I showed an example, the NPR covered
where one of these systems couldn't get a black doctor
with white children as patients
because it wasn't canonical.
Now some of these things get fixed up some of the time,
people train on more data or whatever,
that particular one I think got fixed up.
But today I was working with Grock
literally in the cab on the way over,
in the Uber on the way over.
I was like, I should try that one
and see if it's any better.
I tried a bunch of other things.
And generally the kind of like challenges
that I give, Grock was not doing so well.
We could talk about some of the other.
But so he did Oris Ride's astronaut and it got it wrong.
I'm like, there's so much discussion about this one.
This was a popular essay and people came at me
in lots of different ways and so forth.
And still like, you know,
this allegedly state of the art system managed,
at least on the first try, I didn't try multiple times,
managed to get that wrong.
And I was just like, we are back in 2022.
Everybody has been saying for the last two years,
you know, especially these influencers saying every day
they're like, look at this new amazing thing that came out.
We live in this time of exponential, you know,
bounty and whatever, but on the things that count,
on the things that matter, at least from my perspective
as a cognitive scientist
who's spent his career studying intelligence,
on the things that matter,
we really have not made that much progress.
But Gary, I thought these things learned abstract world models.
And the reason this is interesting is that there's a,
there's a dichotomy between alloteric uncertainty
and epistemic uncertainty.
You know, there's a difference between actually understanding
something and being able to reason,
being able to do this deductive closure
to deduce new knowledge about the world.
Well, there's a couple of different things in that to unpack.
So one is, is this a question about uncertainty?
And it isn't really a question about uncertainty.
So, I mean, you can have something that's very clear,
like horse rides astronaut.
There is no uncertainty.
The horse is supposed to be on top of the astronaut.
So there are all kinds of interesting things
about alloteric versus epistemic uncertainty.
They don't really apply here.
You know, this is a perfectly deterministic phrase
and it's just getting it wrong.
Now there is probabilistic outputs in these systems.
The systems themselves are not deterministic.
And if I ran that same prompt 10 times,
I might get 10 different answers
and maybe six of them would be right
and four of them would be wrong.
You know, we're the other way around or who knows.
So there's that kind of uncertainty.
But the fundamental is you are supposed to map your semantics
onto this description of the world.
You asked about world models.
They don't really have world models.
I think that that's actually easier to see though in Sora
because Sora has changed over time.
And world models are in part about understanding
the dynamics of the world.
That's really why you want to have a world model.
And you know, humans have world models.
So I have a model of the room that we're in
where there are lights.
It might not be perfectly specific.
I might not know where all the lights are.
And I can do updates.
So there's somebody else in the room
that somebody else in the room suddenly says fire
then we're gonna do something different
and maybe stop having this interesting conversation.
And so I have a model of all the things that are going on
or many of the things that are going on.
And all humans do that all the time.
If you watch a movie, you make a model of the characters
or I'm watching the bear.
So I haven't quite finished catching up.
And so I have a model of the lead chef
and the person who works with him
and his girlfriend who's maybe not his girlfriend anymore.
And I'm watching all that stuff and things will unfold
and I'm maybe trying to put flashbacks
and try to order the sequence.
So I just saw a wonderful episode
about how somebody first got her job
and I don't wanna give away too much
but working at the restaurant.
And like for the first 10 or 15 minutes
you're sitting there.
How do I relate this thing that I'm watching?
Is this in the now of the film or is this before?
And eventually we realize it's a flashback
and it's a kind of origin story.
It's a really beautiful story and sad and powerful.
And I'm sitting there trying to make a mental model
of how this piece of this narrative
fits in with this other piece.
And what kind of person, like I've seen this character before
but now I see much more.
I'm learning more about her.
I'm learning more about her partner and the relationship.
I'm building a model of all this stuff.
And current systems just don't really do that.
What they do is they build a model of the words
and maybe some other stuff
that have been said and sent in so far
and try to guess what would happen.
But they don't have like the equivalent of index cards
if you remember those where you like write down notes
or databases where you have records.
This is your phone number
and this is your address and so forth.
They just don't have that.
People don't understand it.
They also don't understand
that these systems don't do sanity checking.
That they don't look things up in encyclopedia
or Wikipedia or whatever.
They just don't have mental models,
world models, cognitive models or what have you.
When they're trying to do horse rides astronaut,
it's not like they have a world model
of what horses usually do and what astronauts do.
They have a bunch of pictures
and their pictures are kind of clustered in space
and they're going to some cluster
trying to find the nearest cluster.
I'm over simplifying a little bit.
But they're going to this cluster of words
that have been around this thing before.
It's not the same thing as I understand.
So back to Sora,
which I think is actually a better example.
So with Sora, you see frequently weird things happen.
Like for example,
there was one where people are carrying some stuff
and one guy goes behind another
and then the camera moves and the guy's just gone.
Right, so if you have a model of the world,
then you know which people are there.
There's another one where there's like four dogs
and then the angle changes the camera
and then suddenly three dogs
and then the camera changes and there's five dogs.
So like people would find that weird.
There are circumstances where people would miss it.
But fundamentally you can see
that Sora does not really have the notion
of object permanence,
which is one of the basic things that I believe
that we're born with based on Liz Spelke
and Renee Byerjohn's cognitive development work
and so forth.
A lot of people know the old Piaget stuff
saying that only eight months old
do you learn object permanence.
That's been shot out of the water by much better experiments
using more sensitive measures and so forth.
My best guess is that stuff is innate,
but Sora never gets it.
And it never gets lots of other stuff too.
Like it doesn't learn that a chessboard is eight by eight.
It sees a bunch of chessboards,
but it might draw a seven by seven chessboard.
It doesn't understand that there are conventions.
There was a Sora ant.
Jan Lacoon and I both posted about this
an hour after I did this ant that had four legs, right?
Like it's seen, God knows how many ants
and it still doesn't understand
how many legs an ant typically has.
So there are lots of things
that an ordinary person's model of the world would have
and these systems just don't have it,
but it really comes out in the changes over time
where just a bunch of impossible things happen.
And they only make sense in terms of the statistics of pixels
rather than the statistics of the world.
So they make sense from the statistics of pixels
from this frame that the next frame might not have anything
behind it or whatever, you know,
every pair-wise bit of pixels one frame to the next
sort of makes sense if you don't understand
what is an image of.
Or another example is there's a Sora video
where somebody goes into a building it's a fly through
and it is amazing the first time you watch it.
It's like a museum, but the second time you watch it,
you realize that like the outside
and the inside don't actually match.
So pixel by pixel, everything in that panning shot
makes sense, but if you go across, you know,
it's only like a minute film.
If you go across the minute, there's enormous,
an enormous number of really massive discontinuities
that make no sense whatsoever, except on the, you know,
frame by frame from this frame,
you could see a frame like that.
But if you look at frame one
and how you got to frame a hundred,
it don't make any sense at all.
Because that's cause there's no stable world model.
It's not a stable model of like
what the dimensions are of the building.
And so you wind up, I forget what it is.
I think it's that like the exterior shot,
you know, there's something small
and the museum is massively bigger.
There's like an empty courtyard,
but then it's not empty anymore.
So there's all these inconsistencies.
Yeah, I mean, this is where I was going
with the epistemic risk because we can verify.
So we have a world model,
we have facts about the world and we can verify.
And sometimes that's a binary.
We can say in certain situations
that an astronaut is on a horse
or a horse is on an astronaut.
There are some vagueness around the boundaries,
perhaps, but it's binary.
But I'm interested in why we anthropomorphize these models
that they're a kind of adversarial attack
on our perception in many ways.
Take the classic Turing test.
What it really turned out to be
was a measure of human gullibility.
So passing the Turing test
doesn't actually mean you're intelligent.
It means you can fool humans.
It turns out we're very easily fooled.
The most dangerous version of this
is you can see a few minutes of a driverless car
and conclude that he drives basically like a person
that everything is good.
And in fact, that driverless car
may have a lot of serious problems
in a lot of different contexts.
And so something that superficially looks like a person
for a few minutes may not actually be like a person.
And so what has happened with large language models
is they superficially produce human-like output.
And some people, not all,
are willing to cut them some slack
when they make an error once in a while.
But they actually attribute intelligence
to these systems in a form they don't have it.
And one of the ways that they do that
is they attribute intelligence.
They think it's like me.
It would do the things I would do in such and such context.
And it doesn't.
So one example I used in my TED talk was
the lost Galactica, the meta system that was pulled
that Jan LeCun is still bitter about.
Galactica said that Elon Musk died in a car crash.
The sentence was, in March of 2018,
Elon Musk was involved in a fatal automobile collision,
I think of us.
And then it continues on.
It's clear that it thinks that Musk in fact died.
Any person would say, oh, wait a minute, in 2018,
I mean, if there is, I mean, we would think about data.
We're not perfect with thinking about data, but in evidence.
But if there's evidence that any human being is alive
right now, it is Elon Musk because he is on X every day.
He's in the news every day.
The amount of evidence that Elon Musk is alive
is greater than for any other person on the planet.
And so this is obviously false assumption.
You could also go to Wikipedia and so forth.
So if I was an editor of a newspaper
and somebody gives me, Elon Musk died in a car crash in 2018,
I'd probably fire them.
It'd be like 2018, that just does not make sense.
It does not check out.
And even if you said now, I'd be like,
well, can we get some extra sources?
And if you said that he died today,
I'd want some sources, can we get confirmation on that?
We don't wanna run it yet.
And so you or me or certainly any editor of a newspaper
or something like that is gonna fact check things,
especially ones that seem prima facie to be implausible.
But LLMs don't do that.
And it's very hard for the average person to realize that
because they see this small sample of data
and in the environment in which our brains evolve,
the kind of evolutionary history,
we didn't have this problem of imposter chatbots
pretending to be people.
We had other problems like that lion, is it gonna eat me?
And so we're pretty good at looking at motion.
Is that thing coming close to me or not?
How big is it?
We make a lot of judgments about the world
that are really good,
but we don't make judgments about AI that are really good
unless we took cognitive science classes in college
or something like that, which most people didn't.
And so we see this tiny sample
and we wildly over-generalize.
Because it said a few sentences
and it like types the word out one by one,
which was a stroke of genius by open AI,
it gives this illusion of being person-like.
It's not actually person-like at all.
It's a statistical autocomplete on steroids
that is trained on a lot of data to look like a person,
but it is not reasoning like a person ever.
I mean, there's two other things here as well.
First of all, there's the phenomenon
that people want to believe that it's intelligent,
especially when publishing New York's papers.
That's what Sabaro said to me the other day,
that you don't want, you want people to think the LLM did it
because then you get a New York's paper.
But also I read a blog post by Nicholas Carlini from Google
and he said he used to be in the camp
that thought that LLMs are just databases
and he started playing chess.
Not databases either, but we'll come back to that.
We'll get to that, get to that.
He started playing chess with GPT-4
and he was blown away with the sophistication of the moves
and it was often making correct moves.
But he said something quite interesting,
which is that if he played chess like a bad chess player,
it would reflect the bad behavior back.
And it's the same thing with generative code.
If you write code with sophistication,
it gives you better code back
because it's almost adopting a role player.
Well, it is a mimic, just a sidebar on chess.
GPT-4 doesn't really play chess that well.
So it makes a lot of illegal moves.
There's a very good, I'll try to give you a link
to put in the show, they're very good,
I'm blanking on the guy's name,
has done a very good analysis of GPT-4 and chess playing.
And it plays like, I don't know, like a 1600 game,
which is like better than the average person
in your high school, but not anything like world class.
And it makes a lot of illegal moves,
like 6% of the time or something,
some crazy number like that, which note,
compare with a chess computer that I bought in 1979,
where you could stick to little pieces in the hole,
I think it was called Sargon,
was the software underlying,
that never made an illegal move, never ever, right?
We're talking about chess software from,
I mean, really even further back,
1969 never made illegal moves.
Like GPT-4 is not, you know,
state-of-the-art in chess and we should understand that.
But people don't, they're amazed that it can do it at all
and there's some reason why you should be amazed
that it can do it at all.
But you have to realize that it is not, you know,
searching a tree the way that a proper chess program can do.
It is doing mimicry to come back
to the other part of your question.
And you do get these like weird mimicry effects
because essentially what you're doing
is a little bit like what humans do when they're priming.
You're directing the system to a particular part
of its corpus, so you can direct it
towards the more sophisticated language
or the less sophisticated language or whatever,
and it's going to try to replicate language like that.
And so that is why you get some of those kinds of effects.
I don't know whether what Carlini described, you know,
really bears out in a systematic study,
but if it did, that'd be my guess for, you know,
why it would is like the database of lousy chess games
is going to look different from the database
of good chess games.
Yeah, it's strange, isn't it?
Though as we memorize more of the long tail,
the reliability sort of goes down a little bit.
So around 5%, 4% and a lot of people argue
that that's just fine.
We can engineer our way out of it.
Do you think that?
It depends on what the problem is.
First thing I would say.
So large language models are not like calculators.
Calculators give you 100% correct answer
and large language models in very few domains
give you 100% correct.
In some domains, they're just completely outmatched.
So, you know, floating point arithmetic,
they're going to be a lot less than 95% correct,
especially with large digits.
They're going to be much worse, I would suppose.
I don't know if anybody's done exactly that study.
And something that matters probably in all forms of AI,
but particularly in large language models,
is the cost of error.
So large language models are best suited, I would say,
to things that are like brainstorming or autocomplete.
So coding is a kind of autocomplete
where the coder is still at the wheel, right?
It's not a fully autonomous activity.
The system is not actually writing the whole code
or whatever.
So it's writing little bits you drop in
and coders have spent their entire lives
learning how to debug bad code,
partly because most coders don't type that well,
and also because coders forget things and whatever.
I've done a bunch of coding in my life and I know how it goes.
And so, like, if you don't know how to debug things,
you're just not going to become a coder.
Like, it's just not the profession for you.
So everybody using those tools
can tolerate a certain percentage of error
and they're trading off.
How long does it take me to type this out
to look it up versus how long to debug it?
I think people are initially excited.
Some people are less excited now
because they realize sometimes, like,
they did a bunch of tests to make sure that the code works
and it passed all of them.
They wrote the code in an hour
and then three days later,
they don't remember why this code is there
because they didn't actually write it themselves
and it takes them like 24 hours to debug it.
And then they're like,
I don't know if the straight off is worth it or not.
So there's still some open questions there about security
and so forth.
There's been some academic literature
suggesting their problems,
but at least in principle,
you have a coder who is picking, you know,
the outputs, they're taking some,
not others, they're fixing it.
And so you can have a high amount of error,
but it might still be worth it.
There are other domains
where also a high amount of error might be worth it,
like brainstorming.
So I'm trying to think of a commercial
and it gives me 30 ideas.
I reject 29, but I'm happy that I got the one.
So, you know, that might be great, right?
So it could be 90% error rate, but you're still happy.
And then there are domains
where like any error is probably gonna like kill somebody.
It's like a medical domain.
And, you know, you have the system treating a child
like an adult and giving the wrong answers
because it's not really trained in enough pediatric data
and like any error might like actually like kill a child
or send them to the hospital or whatever.
And so you need to be much more accurate.
So there is no blanket statement you can say
about like what percent error matters.
If you want to use the thing as a calculator,
probably you shouldn't get anything wrong.
Like you should just use a calculator.
So it does depend what you're applying it to.
Yeah, coding is an interesting example
because there's a verification step.
So does the code compile?
And then there's the behavior.
Does it run the way?
Yeah, we got to pause you right there though.
The worst stuff is gonna come from people
who think that's the only verification step.
I'm not saying you think that, but I think some people do.
So for those who are not programmers in a language like C,
for example, C++, you write the code, you compile it,
which turns it into machine language,
but that does not prove that there are no bugs.
But a beginning programmer actually might labor
under that assumption.
So they think if it compiles, I'm good to go.
Now a sophisticated programmer realizes
that's not the case at all,
that bugs can emerge in code that does correctly compile,
but there's still some assumption
that's made this wrong and so forth.
But there's a certain amount of bad code
that is seeping into the code base
because people do think that's the only verification step.
Another verification step that good coders know about it
would be unit tests or some kind of testing.
Like now that it compiles, the journey has just begun,
I'm gonna make sure that this code
actually does what I want it to do.
And a good programmer understands the logic
of what they're programming
and they know what a good test might be.
They know what wacky user input might come.
They wanna make sure that it handles that input.
They understand when the circumstances
of their assumptions might be tested, they test that.
And a bad programmer doesn't really do that.
They do a couple tests and they call it a day.
They say this is good, they will go on to the next thing
and then it all falls apart three weeks later
when one of those assumptions is violated.
It's very true.
I mean, with Genai coding,
I've been doing a lot with Claude 3.5 Sonnet.
And the first observation is you can now generate
something like 2000 lines of code in half an hour.
And sometimes it works reasonably well.
Now, I noticed that it hits a complexity ceiling
around 2000 lines of code.
At that point, it just starts hallucinating
and doing crazy things.
But on the anthropomorphization point,
this is interesting because it's a supervised process.
The human is coming up with a prompt
and they have a mental model.
They are selecting the completions.
They're interactively running it and so on.
And again, isn't it interesting
that the human doesn't like to think
that the creativity and the reasoning is coming from them
when they're working with language models?
I mean, I don't know what people like to think of.
I mean, in a way, I can't really answer that question.
That's a question about like,
how do individual users feel about the product
that they're making with the system
if I understand the question correctly?
Well, it's a sense that if,
and I know you don't think language models are a database,
but let's say they're a database,
the user creatively comes up with a prompt
and a lot of the reasoning is actually implicit in the prompt
and then the language model would retrieve
a whole bunch of ideas and then the user
will select one of those ideas
and then they'll run it on their compiler
and then they'll verify.
This is actually a very integrated supervised process,
yet people think the magic LLM is doing everything.
Yeah, I mean, some people may think that.
Let me see how to put this.
There's a wizard of Oz problem sometimes
where the man behind the curtain is doing a lot of the work
and you'll see these threads on Twitter
where people will say the machine didn't get it wrong
because look, I can do this prompt
and it'll be this very complicated prompt
and then the machine gets it right.
Well, who's actually doing the work there
is the person who was coming up with the prompt
and it's even more complicated than that
because they're noticing that the machine got something wrong.
That's an important piece of intelligence
that the machine has actually failed at.
And then they're noticing that the simple prompt
doesn't get it right.
They're being creative about what prompt will get it right.
They're doing all this work
and then they say, oh, the machine is great.
In fact, the machine only gets it
if you sort of, to use an apt metaphor here,
hold its hand three quarters of the way there.
Yeah, it's so true.
And another thing is when you interact with an LLM,
you get a feeling for its personality.
So with Claude Swannett, for example,
I know when it starts hallucinating,
I can tell within two sentences of the response
that it hasn't understood me and the code is just,
because it will confidently generate garbage broken code.
And what do you do in that situation?
So one thing you can do is create a new session.
So it no longer has the context
of all of the decisions that you made previously.
And then it's more grounded in some sense,
but less grounded in another sense.
And then I've just generated all of this code
and now my friends do a peer review
and they start trying to understand all of this code.
And there's a huge understanding credit card
because they don't understand anything I've just created.
One interesting idea would be to have a kind of transaction
log of all of the previous prompting to the LLM,
which means there's a huge session
so the LLM understands all of the assumptions
and all of the requirements that went into it.
But even that just disintegrates
after it reaches a certain length.
I mean, ultimately, there's I think a limit
to what you can get out of these systems
in terms of coding help.
They comes from the fact
that they don't really have a good theory of mind
for what the customer, which might be the programmer,
actually wants from the system.
The channel for communicating that is just not great.
And so the systems are best at very narrow requests.
Like I want to know how to express this in HTML
and what is the API for this thing,
where you're kind of like looking something up
as opposed to like the first time
somebody built a word processor,
they had to think, well, how am I gonna structure
this system at all?
How am I going to represent the contact?
I mean, the document that somebody's working on,
how am I going to set up an interface where,
and I don't know how they did this in the original one,
but you really want this so-called model view controller
separation so you can display things separately
from the logic of what you're doing
when you get the keyboard actions and so forth.
And so there's an intellectual process of like,
how should I structure this abstract problem?
Now, real-world coding is a mix of that.
I mean, you get your low-level people to look up the silly
stuff and the, or the, not silly stuff,
but the kind of cut and dried stuff.
And you want your like top-level system engineers
to like think about how am I gonna build this system
in the first place?
And large language models are not particularly good at that
as I understand.
I remember in the early heady days of large language models,
which is to say in 2023, right after Chachi Petiga,
popular, that Sam Altman was talking about,
I think it was Sam Altman, was talking about,
you know, who's gonna have the first billion dollar
business run by a single employee and a bunch of LLMs.
And we haven't as far as I know,
actually seen anything like that.
And one of the reasons we haven't seen anything like that
is like actually figuring out how to build a system,
for example, it was way outside of the scope
of what these things can do.
They can be helpful assistance.
And, you know, Eric Brignawson and among others likes to talk
about, you know, human augmented kinds of stuff.
And I'm all for that, trying to use AI
to augment human abilities.
But if you're talking about really autonomously,
you know, running the books for a business,
running the marketing campaign,
like really doing that by itself
with no human supervision,
these systems just aren't reliable enough for that.
And so nobody can actually make a one-person
billion dollar business that I know of.
Yeah, I mean, everyone's talking about agents now
and you just nailed it.
And I think Rodney Brooks said something similar,
which is that we underestimate the amount that we,
in the process of supervision,
smooth off the long tail of failure modes.
That's right.
And that doesn't work in certain domains.
So it's okay in the brainstorming domain,
but not in the driverless car domain.
I mean, in the driverless cars,
like we still don't know how much Waymo has humans
behind the scenes sorting stuff out.
With crews, when that number came out, it was mind-boggling.
They had like, I think they had 1500 people
behind the scenes in tele-op centers
and 800 vehicles on the road or something like that.
I don't remember the exact number,
but it was more tele-operators
than they had cars on the road.
So there was a vast amount of smoothing out
that was hidden from the customers,
who was hidden from anyone wanting to do
scientific analysis of what was going on.
There are a lot of domains like that where, you know,
the man behind the curtain, the Wizard of Oz thing,
is really doing a lot of the work.
So let's move over a little bit to policy
and some of the things you spoke about in your book.
Maybe let's start with your blog post
about Fei-Fei Li and the California regulation.
So I mean, that situation is still unfolding,
but in general, what I found around SB 1047
was there was a lot of misinformation
and a lack of precision.
So I think a lot of people painted this kind of catastrophic
view of what would happen if SB 1047 passed unchanged.
Since then, it's already been watered down some.
And so the thing that people weren't about
is not actually going to become law in any case.
And, you know, some of this just happened yesterday.
I haven't really even fully digested it all yet.
And I don't know how much more modification
there may or may not be.
So I don't want to be too specific about the details of it,
but what I would say is that at a general level,
there's this crazy notion, crazy in my mind,
notion that we can't have any regulation around AI
or we will kill it.
And every other domain, that's just not true.
So in fact, in many domains, having regulation
has been essential to the growth of the industry.
Airlines is an example of it.
Back in the 1940s, commercial airlines were like
really dicey kinds of things in the 1950s.
And now commercial airlines are incredibly safe.
They're much safer than driving your own car, for example.
Way safer than motorcycles.
And why are they safer?
Because we have multiple layers of oversight, right?
And, you know, it has not ended the airplane industry
we have these multiple layers of oversight.
We have rules about how you make an airplane.
We have rules about how you maintain an airplane.
We have rules about how you investigate an accident
and so forth and so on.
That has not caused the airline business
to go out of business.
Now, in every industry, there are many industries,
probably they're dumb regulations,
especially the first time around.
Nobody's making a claim here that like anybody's first time
bat to use a baseball metaphor,
they're gonna get perfect regulation.
I'm a fan of the EU AI Act,
but I'm not so naive as to think that it's perfect.
I'm sure we're gonna find problems with it.
But the notion that like you can't have innovation
in an industry where there's regulation
is just absurd on its face.
Like I, do you know about that Overton window?
Like people are just saying crazy things
trying to reframe the discussion.
But of course you can have some regulation around AI.
And part of what's so crazy is like,
they're like, oh my God, this is so onerous
and you look at the fine print
and if you're not running a $100 million training run,
you hardly have to do anything under this law
that everybody got so up in arms.
Like if you can pay for a $100 million training run,
you probably can pay for a million dollars in compliance.
Like you can do that.
Like we're talking about these businesses
that have been capitalized evaluations of,
you know, four, 10, $80 billion.
And we're supposed to mourn
that they have to fill out some paperwork.
Well, okay, but let's look at the other side of it.
These things have already done harm, right?
Think about deep fake porn.
Think about all the misinformation
that's floating around in these elections and so forth.
It's not crazy to say
that people should take reasonable steps
to make sure that there'll be no catastrophic harm.
That's another thing about the bill is like,
it's basically restricted to catastrophic harm.
It's like literally, this is not a metaphor,
literally your first $500 million of damage is on the house.
You know, we're only talking about
if you cause at least $500 million of damage
and you're negligent and like,
you should have known better
and you did nothing about it.
Like is that so unreasonable to say
you don't get a free pass
if there was a billion dollars worth of damage,
which could also equate to a certain number of lives
or whatever people have math around.
Like you're gonna cause a billion dollars of damage
and we can't like do anything around that.
We wanna give you a free pass on that.
That's crazy.
And we don't do that in other industries.
Like if you make a circular saw,
we want you to put a gadget on there
so that the average idiot doesn't tap off their fingers.
It's not unreasonable.
We don't say, oh my God,
the circular saw industry is gonna come to an end.
Nobody's ever gonna have power tools.
Home Depot is gonna close.
But that's kind of like what it was.
Like Home Depot of AI is gonna close
and we'll never invent another tool
like it's just ridiculous.
Yeah, I mean, in your book,
you gave the example of cigarettes,
but there's also some things-
The cigarette manufacturers.
Yeah.
Right?
So just to make clearly,
the cigarette manufacturers pretended for years
that there was no conceivable harm.
They said the science here isn't good enough.
Let's wait until,
I mean, basically the cigarette manufacturers wanted to say,
unless you do a causal study
in which you assign people,
human beings to smoking cigarettes or not,
this is not science and we're not responsible.
And they got away with that for a really long time.
And if you're an actual scientist,
you understand that you can use animal models
and you can do certain kinds of observational things.
And you make a pretty darn good guess
that cigarettes cause lung cancer
without actually doing the experiment
that they were insisting on.
But they just, you know, with a straight face
would say this stuff for years and a lot of people died.
When we get into tech,
I mean, you spoke about social media
and we've got examples like Uber, for example.
Tech is interesting because there's a way
of flouting the rules.
So the rules don't apply to you.
And AI is interesting because,
I mean, open AI, of course,
argue to the House of Lords,
as you said in your book,
that, you know, we need to flout copyright
because AI is so magical,
it's going to be used for all of these things.
Let's just run that part in detail.
What they said in the House of Lords,
I mean, it's interesting what they said
and what they left out.
So part of what they said is probably true,
which is we can't make our stuff work
unless we use copyrighted material.
But then what they implied is they need all the data to make it.
I mean, it still doesn't work that well,
but it kind of works.
It works as well as it does
because they're training on all this copyrighted material.
If you took away copyrighted material,
it wouldn't perform as well.
Like we can agree on that part.
And they said that the House of Lords,
more or less straightforwardly.
But then what they implied is we need an exemption.
You need to give us all this for free.
There's a totally well-known alternative to that.
It was much more equitable to the artists
or the writers or what have you to the creators,
which is you license the stuff.
Like Netflix does not just show your movies for free
without consulting you and not paying you.
They license it if they're gonna show your movies.
Same thing, Apple iTunes music.
And in fact, a lot of this reminds me of the Napster days
where there were a bunch of people running around
saying Napster is great
because I can download every song for free.
And there were a bunch of artists saying,
well, hold on, don't we have copyright laws?
And there were a bunch of people saying at the same time,
information wants to be free, it's a new era.
And the court said, no, we have copyright laws for a reason.
They're not going anywhere.
And so Napster was driven out of business
and who took their place?
Apple, because Apple actually was willing
to pay licensing fees.
And so Apple made an enormous amount of money
licensing music, but the artists still got a cut.
Maybe not as much as I would like, but they got a real cut.
So we moved from complete utter theft,
that's what Napster was, to a licensing arrangement
that was at least reasonably equitable and it's fine.
And we can do the same thing here.
You can light, and in fact, it's interesting
because OpenAI tells the house of lords,
basically you have to give us the stuff for free.
And behind the stage,
they're making licensing deals left and right.
They're talking to everybody.
So they know, in fact, that it might go that way.
So they were trying to get this massive handout.
It'd be like giving me the West Coast of the United States
because I said I need it for my work
and you knew forgetting to say,
well, could you maybe pay something for that?
Social media is quite an interesting example
of in the tech space, Mark Zuckerberg had this phrase,
move fast and break things.
What could possibly go wrong?
I mean, that's how I start my book, in fact,
is with that phrase.
And a lot could go wrong with AI, right?
So we already saw a lot went wrong with social media.
Like, it's been really terrible for teenage girls.
I think lots of people are addicted.
There's a wonderful piece called Twitter poisoning,
or that's in the title by Jaron Lanier
where he uses the example of Elon Musk.
And he says, if I had gone to Elon,
I'm doing this from memory, so I might not get exactly,
but he said, I've known Elon Musk for a long time.
And if I had gone to him in 2016 and said
that you're gonna spend multiple hours a day on Twitter.
And it's actually gonna hurt your car business,
maybe your rocket business a little bit.
It's gonna make people really hate you,
and yet you're gonna be compelled
to do it a few hours a day.
Musk would have looked at him and said, you're insane.
Of course I wouldn't do that.
I've worked to do.
But he got addicted, right?
Twitter is addictive, and I speak as someone
who's addicted to Twitter and would like to stop.
It takes up a lot of people's time and emotion.
There's some value in it too,
where I wouldn't keep doing it,
but social media is addictive.
It's probably increased loneliness.
It's certainly increased political polarization.
AI might do all that worse.
So it might make people lonelier.
So one of the consequences I think of these chatbots
is a lot of people are gonna fall in love with those chatbots.
It's already starting to happen.
And they're gonna lose or never develop,
depending on their age, whatever social skills they have
because they spend most of the time with the chatbot.
There was an old study showing that people
who watch more television are less happy
than people who watch less television.
It's partly about opportunity cost.
If you watch television five hours a day,
you actually enjoy it in the five hours.
You pick shows you like and whatever.
But at the end of the day, you haven't learned a skill.
You haven't gotten a better job.
You haven't made a new friend.
Social media is like that.
Well, AI is gonna be like that even more so.
On the polarization front,
polarization in large part comes from misinformation
and targeted misinformation.
Well, AI is going to accelerate all of that.
We're gonna, if it's even possible to believe,
we're probably gonna have an even more polarized society
than we already have as a function of deep fakes
and other kinds of misinformation
or disinformation by deliberate design.
There are lots of ways in which moving fast with AI
is likely to break a lot of things.
Yeah, I mean, you said automated disinformation
may destroy what's left of democracy.
I mean, it sounded a little bit hyperbolic.
I mean, how do you see that rolling out?
It's not, though, I think.
I mean, I genuinely worry about that.
So we are going to have elections,
which is the, how do I say it,
is the ultimate act of democracy is an election,
especially if you have a representative democracy.
You don't get to vote on individual things.
You have referendum and stuff like that.
But basically, we live in a representative democracy
and misinformation may undermine that.
So we may, this year, in October,
see deep fakes about what are the other candidates
that actually materially changes the election.
That really can undermine democracy.
And we're gonna see that more and more around the world.
And parallel to that, part of Russia's game plan
has always been what they call the firehose of,
what is it, firehose of misinformation
or something like that.
Firehose of propaganda model.
And the idea behind the firehose model
is not to push one lie, but to push so many lies
that nobody knows what to believe anymore.
And that is what's happening.
You know, people.
Yeah, what was that in Curtis' film talking about that?
Was it a hyper normalization?
I don't know the particular,
but I mean, this is a well-known thing.
I think it's called the Russian firehose of propaganda model.
It's been something that is, you know,
Putin has cared about for a long time
and we are headed to what was always Putin's end game,
which is nobody will believe anything.
So just in terms of images, images are basically done.
You know, for a hundred years,
if you had a photograph of something,
that was serious evidence that it was true
and people knew that.
And as of, you know, the last couple of months,
people know that that's not anymore.
Well, like people don't know what to trust anymore.
And authoritarians like that.
They just go in and do what they want and say,
you know, you can't believe that other thing.
Don't believe it.
And I mean, like, look at the bald-faced lies
that Putin tells all the time about the war in Ukraine, right?
If we move to an environment
where basically nothing can be trusted,
as I think we are, that's really bad.
And one reaction that people have had to all of this as well,
of course, we need to educate people about misinformation
so they'll be more skeptical.
And that's true, like, but in the limit,
it's not so good, right?
In the limit, if nobody trusts anything,
democracy can't really function, right?
I mean, the currency of democracy
is information and informed decisions.
And if you don't have information
and can't make informed decisions,
then you don't really even know what you're voting on
and then democracy dies.
It's ironic, isn't it,
that when there's no knowledge grounding to reality
that we become like chat GBT
because we can't verify anything anymore,
but it could go one of two ways.
Then it argued that it would tend towards acquiescence.
Or the other argument is that it would tend
towards skepticism and rational thinking.
Where do you think it'll go?
Might be some of both.
I mean, I guess this remains to be seen,
but I think a lot of people will feel defeatist
and just give up.
They kind of won't believe anything,
but they'll kind of give up trying.
I mean, I hope I'm wrong on that.
I've been right about, I think,
nearly all of my air predictions.
I hope I'm wrong about this political prediction
that people will just sort of enter despair
and not really care about truth anymore.
But I already see signs of that on Twitter every day.
I mean, people talk disparagingly about you.
And the thing is, I mean, actually DeepMind had a motto,
solve intelligence and use that to solve everything else.
And you admitted in your book that AI does have the potential
to revolutionize healthcare and bring abundance to our lives.
So, you know, what is the game plan here?
What are you trying to do?
We need to move past an AI for one.
And for two, we need better regulation.
And there's some overlap between the two.
So on the technical side,
generative AI is just not a good way to do AI.
It's fine for certain things like brainstorming.
And it's fine for making images if it's properly licensed.
But it is not fine for running power grids.
Medical remains to be seen.
It's not a reliable framework.
It's been very hard to get people to understand that.
I'm amazed that you still see things
like a famous physicist the other day said,
I had a conversation with chat GPT
and I couldn't believe how bad it was.
I'm like, where have you been?
We've been trying to explain to you
that he doesn't actually understand what it's talking about.
That message is not being communicated very clearly.
So people over rely on it.
There's going to be weird economic effects,
I think, when the bubble burst.
I don't think gen AI has been good
and it's not the unbalance, I don't think, you know,
I think, I mean, it's an interesting question.
Has it been of net benefit or not?
And I am not so optimistic
because I think mostly gen AI is a good tool for bad actors
and some limited positive uses.
And I don't think those positive uses may be outweigh
the negative uses, particularly if we get to this regime
of disinformation where democracy is basically undermined.
Like, I don't see any amount of productivity gains
as justifying the end of democracy if we get there.
So gen AI, unfortunately, is a very good tool
because it doesn't track truth
and it doesn't understand factuality,
but it mimics very well.
It becomes a very good tool for bad actors
but a less good tool for good actors.
And so I'm very concerned about that.
And I think we need other approaches to AI.
So, you know, there is a lot we can still do
and invent in medicine and technology.
For example, we still don't really have a good way
of handling Alzheimer's.
And I think AI could be extremely helpful with that,
not generative AI, but some future form of AI
that can read the medical literature,
do modeling of proteins, integrate it all.
You know, there's interesting new paper about AI scientists.
If we had some future form of AI,
we might actually build an AI scientist.
Right now, it's just not gonna work that well
because they can't really reason about data.
They can't plan.
You had Subaru out on your show.
I'm sure he talked to your ear off
about how they can't plan.
You can't really be a scientist if you can't plan.
But eventually we'll get to other forms of AI
and maybe that'll be great.
I can't promise that it'll be great,
but I could see ways in which it could be great.
Then there's the governance side of the equation.
You know, we're basically letting the companies
do whatever they want without regard to consequence.
And that's what's not good.
Yeah, the AI automated scientists paper is very good.
They're coming to London,
gonna interview them next month.
So I'm very excited about that.
But you know, you said that AI is being rushed out the door.
But if not now, when?
When, should we have AI?
Yeah.
I mean, I think a lot of the stuff that we have now
should actually be in the lab
and not commercially deployed at large scale.
So I have no problem with people researching JANA AI
trying to find, I mean, all my issues around deployment.
So for example, we know that JANA AI is being used
to make healthcare decisions and job decisions and so forth.
Let's focus on the job decisions.
We know that employers are sticking their job applications
in and saying, who should I hire?
And we know that there is likely to be discrimination there.
But we don't have public access to like how bad
the problem even is, how much bias there is.
But they're almost certainly bias against minority groups,
for example, in using LLMs to make job decisions.
This is just not good.
And like, I'm sure it's happening like literally every day,
you know, at large scale now.
So I spoke to Yoshua about this last night
and he said, freedom of speech is really important.
We need to protect innovation.
I don't want someone to say to me
that I'm not allowed to use AI in this way.
I mean, what would you say to that?
Oh, I would say that society has to balance values.
And the United States Constitution and the laws around,
the laws of the land have made certain decisions
that I would say are good.
Like generally there's freedom of speech,
but there's carve outs for certain kinds of things.
And that we also, for example,
protect against discrimination.
So we have an Equal Employment Act.
And you can't just like pick and choose and say,
I'm gonna keep the free speech
and to hell with the Equal Employment Act.
Like I don't see the justification for that.
And so you have to say, how am I going to balance this?
So if I give you, you know, carte blanche
to use these tools that I know are just, you know,
are almost certainly discriminating against people
and really I should have access to the data.
Somebody should have access to the data.
That's just not cool.
I mean, I don't know how else to say it,
but adults realize that different kinds of constraints
can be in conflict and you have to find some way
of handling the conflict.
And right now then the idea of complete free speech
to build whatever you want and free whatever word is
I'm looking for a free carte blanche
to build whatever you want is in tension
with a bunch of other things like laws
that prevent discrimination in jobs, employment, et cetera.
Or, you know, any rational thing you might have
about sort of the commercial production of misinformation,
which is actually not very well protected
in the United States, but is protected in some other places.
You have to balance these kinds of things.
And I think to say I'm only going to look
at what I as a programmer get to do
and not worry about the consequences of society,
either the legal ones or the moral ones, that's just wrong.
I mean, it's morally wrong.
For things like discrimination,
do you think it would be covered by existing laws?
I mean, for example, in banking fraud models
and credit models have very specific regulations
around them already.
Some things are some aren't.
So for example, it's just not clear how much
the Equal Opportunity Commission can audit what's going on.
So they work on a case by case basis.
Somebody walks in and says,
I think I've been discriminated against.
Here's my evidence and they take on the case.
It's not clear they can do the auditing that's required.
So that'd be one example where I don't think
the existing laws cover it.
Here's another one where it's still very ambiguous,
which is defamation.
So large language models have, for example,
accused people of sexual harassment
that they did not actually commit.
We know that they didn't commit it,
but it's not clear that the people who were defamed by this
have any recourse whatsoever.
Because the existing laws of defamation, for example,
there's a bunch of complications here,
hasn't really been cashed out in court,
how it's all gonna go, but typically,
there's some notion of intending malice.
Well, you could argue that large language models
don't have emotions, they don't have malice.
And I'm doing it on purpose.
It's actually a kind of negligence
that's making them do this.
But are we gonna say that you can make a system
that's arbitrarily negligent to the point
of really screwing up people's lives, perhaps?
Just because they work differently from people,
like that doesn't seem to make sense to me.
I would say the existing laws don't clearly handle
what happens if some AI system
completely negligently defames person.
And it's completely negligent
because there's no fact checking on top of these systems.
So, you know, my favorite example of hallucination
is not particularly defamatory.
One of these systems said I had a pet chicken named Henrietta.
I don't think that really causes me any harm in the world,
but there's also nothing I can do to stop it.
And there are other cases like, you know,
certain law school professor
was accused of sexual harassment
and it's nothing, probably nothing he can do about it.
I think you're absolutely right
about automated decision making.
On text generated by LLMs, at the moment,
I can still smell it 100 miles upwind in a hurricane.
Well, you think you can, but you know,
all the studies that I know show people
are not that good at it.
That's right. Just look at LinkedIn.
Well, there's tons of it out there.
You know, there's certain like giveaways,
like the word delve is way over represented in LLM speech,
probably because the Kenyans who did the RLHF
actually use the word more than we do over here,
let's say in the States.
But, you know, people have done experimental work
and I don't think there's any system
that can detect LLMs more than like 70% correct.
So, like if you're talking about, for example,
could we use these pieces of software
that look for LLMs to, I don't know,
weed out student papers, let's say.
75, 70% correct or 80% correct or whatever
is just not good enough.
I mean, imagine you're the student
who wrote the good paper that is described as an LLM,
you're gonna be really, really upset
if you're in that one in five who got screwed and justly so.
And so you can't or you shouldn't be using software
that's that bad at a task like that.
It really has to be better.
Nobody knows how to make a 99% accurate LLM detector.
Yes, that's very true.
When I spoke with Luciano Floridi,
he said that the problem with this information technology
is that there's no friction whatsoever between it
and the regulatory, you know, the legal landscape.
And open sourcing models is quite an interesting thing.
So, do you think the horse has bolted?
I think there's multiple horses,
one of them has and some haven't.
So, the horse of LLMs that can be used
for misinformation totally left the barn, you know.
Meta's latest model is probably good enough
to make whatever misinformation
anybody wants sound plausible.
And, you know, now the weights are out there
and there's no way to put that back.
I mean, short of like a nuclear war
that like sets the entire civilization back,
like, you know, short of really crazy
kinds of things happening, that's never going away.
So, that horse has definitely left the barn.
There's probably going to be another 20 important advances
in AI, I'm making up the number five, 10, 20, 100.
And we don't necessarily have to handle them
all the same way.
So, the open source question is extremely complicated, right?
You know, there's a lot of reason
to want to accelerate progress towards AI
and open sourcing things clearly accelerates them.
But there's also a lot of worries about bad actors
and what they're going to do.
And, you know, I find it to be pretty difficult.
I will refer your readers to a paper
that I wrote in Substack that was about an open AI paper
that purported to show that LLMs don't make
bio weapon manufacturer worse.
And if you actually look at their data,
they did their statistics wrong.
And I see that paper cited sometimes and it worries me.
So, it's not all clear yet,
but it is possible that LLMs will, for example,
be used as tutors to teach people
to make bio weapons that couldn't.
And, you know, even a single incident like that
could kill tens of thousands of people in a subway
or something like that.
So, there's some definite potential downside risks.
What upsets me is that it's a really complicated decision
that should have been made by a lot of very bright,
thoughtful people trying to work through their differences
and instead basically meta made that decision by themselves.
And now, if there are negative consequences,
the whole world has to absorb it.
In fact, the favorite phrase of mine lately,
I didn't coin it, I wish I had,
is to socialize the consequences and privatize the profits.
And so, you know, meta gets all the gain
of open sourcing their stuff.
Their software gets fixed by lots of people.
They get a lot of credit.
They use that primarily to recruit people
that they weren't otherwise getting
because they had taken some reputational hits.
It's all plus for them.
Now, if it turns out that a bunch of people die
because the software was abused,
then that's something society absorbed.
Meta's not gonna make anybody hold.
They're not gonna bring back, you know,
your sibling that was killed in a nerve gas accident.
This is not good.
So, we have folks like Elon Musk
and he's really focused on the ex-risk stuff.
We've got people like Jan Lacoon, for example,
who I think in your opinion has adopted many
of the talking points that you've been making for many years
but quite dismissive about things like misinformation,
for example.
I mean, what do you think about that?
Well, I mean, there's a lot of different players,
a lot of different people have different incentives.
Some people I think are speaking from the heart.
Some people are speaking from the pocketbook.
Like, I don't know if Jan Lacoon really thinks
that there's no possible risk from AI.
I mean, he's actually attacked Elon Musk
for spreading misinformation.
He must know these tools could be used
to generate misinformation.
There's already reports that they've been used in the wild
and he says they can't be used for misinformation
but he also works for Metta.
I don't know what he really believes him
and what's an economic incentive for him.
Elon Musk is, you know, a riddle in many ways.
Lately, you know, he seems to be driven in some ways
more by attention than in principle.
I was talking to him for a little bit
and I think that he's genuinely concerned about AI risk
but he's also, you know, building frontier models
just like everybody else.
And the last communication I had with him
which he didn't respond to,
I emailed him when he came out strongly for Trump
and I said that if you really care about AI risk,
supporting Trump might not be the wisest move here
and he didn't respond and he is supporting Trump
and like, I don't see how to square all of the different
things that Elon Musk has said.
So I really do think that he cares about AI risk.
He's not putting that on but he's doing a lot of things
that may actually exacerbate AI risk.
And so, you know, he's not integrating
over all of his beliefs in my opinion.
And then there are other people.
I think Demis Hassibus is maybe one of the straighter players.
I think that he really is worried about AI risk
and behind the scenes he's trying to do something about it.
You have Anthropic, I think, really started caring
but now they have visions of dollar signs dancing
in their heads and I think they care less
than they did before.
Open AI, I'm not sure they ever really did.
I really lost faith in those people
but they certainly talked more about AI risk
in older times and clearly they focused primarily
on commercialization and so they've shifted.
So you have lots of different players.
There's a cartoon that I showed in my talk
and is in my upcoming book by Cal from the Economist
where you have all these companies saying like AI
or I think it's countries rather than companies
in the cartoon saying, but it would apply just as well
to companies saying, you know, we're terrified
of the catastrophic risk that AI might destroy us all
and then, you know, that's at the top of the cartoon
and then all the same players are saying
and we're desperate to get there first, right?
So there's this tension throughout the industry
that I think comes out most clearly with Dario Amade
who is the CEO of Anthropic who has been saying
that he thinks we might get to AGI in three years
which I think is ridiculous.
We haven't actually talked about that
but he seems to genuinely believe that.
Maybe it's just hype to raise his valuation
but he seems to genuinely believe that.
He thinks that his so-called Pee Doon is very high.
Maybe like, I don't know if he gave a number
but let's say 50%, like he thinks
this stuff might really kill us
and he used to say, so we're not gonna work
on frontier models.
Now he's working on frontier models
just like everybody else is.
And so like I don't know how those ideas
simultaneously can exist in one person's head.
Like how, if you really thought
that the technology you were building had a,
you know, 40, 50, 60% chance of annihilating the species
which I actually think the number is low
but if you really thought there was a 40, 50, 60% chance
that the thing you were building
might annihilate the species
or let's say just decimate it, kill, you know, 10% of it.
Like how could you morally keep working on that?
That seems to be where his head is at.
But you think they believe it genuinely?
I mean, so that raises the question of like,
is it all hype?
Does he actually believe it?
How does he, like, I can't literally get
inside these people's heads.
I can just say what I see from the outside
reading, you know, what they're doing.
I would say I think different people have different beliefs
and about why they're doing it
and what they think risk is.
There are clearly some people who's Pee Doon
over the next 10 years really is like 50%.
They're not putting it on.
You know, my Pee Doon is much lower than that.
I don't think we're gonna annihilate the species.
My Pee Democracy gets really seriously impaired
is very high.
I think that that's a likely outcome of LLMs.
The idea that we're gonna eliminate this human species
seems to me not absurd but very unlikely.
So it's not zero.
I'm glad some people in the world are thinking about it
and how to defend about it.
And I think more generally that alignment is very important
even if you don't think about literal extinction.
So literal extinction, it just seems weird to me, right?
We're a very physically diverse species,
sorry, physically, sorry,
we're very geographically diverse species.
We're somewhat physically diverse with different genes.
So you think about COVID was really bad, right?
I mean, it's a horrible, horrible disease
but it's still only killed only kill, you know,
1% of the population, not 50 or 80 or something like that.
So even if a system tried to design COVID,
it wouldn't necessarily stop all humans in all places.
I just think that that's unlikely.
On the other hand, I do think that there's a real risk
of some kind of catastrophe.
For example, somebody might try to short the stock market
and I hope this isn't one of those market's predictions
that turns out to be true.
There's been a lot of them.
Somebody might try to short the stock market
by using LLMs to cause as much harm as possible
in a short interval, you know,
short it just before that, try to make a bunch of money.
Like that might actually happen.
And, you know, a lot of people could die
because people like, you know,
said airplanes to crash into each other or whatever.
So like there are real risks like that.
There are risks that misinformation
could lead to even a nuclear war, right?
So, you know, we blame the Russians
for doing something they didn't actually do.
They get mad at us, it's back and forth.
And then we get into a lot of scenarios like that.
And I think the probability that something bad will happen
as a consequence of all of this,
like seriously bad is actually fairly high.
Yeah, I mean, I interviewed Syash Kapoor, you know,
we're in one half of AI snake oil the other day.
He had a great piece out about these subjective probabilities
that are used for P-Doom.
And I agree, I'm not a fan of casting it as a probability.
But one interesting-
You know, I wrote a piece also in Substack
some months ago about P-Doom.
Like I also think that most of the numbers
are made up in post-doc, testified and so forth.
Yes, indeed, indeed.
But one interesting thing though is that,
in a way, your strange bedfellows
are some of the AI risk folks
because it's still safetyism in a sense.
I mean, how do you feel about that?
Well, I mean, I think that goes to the part
that I think really is important,
even though I disagree about the strong extinction narratives.
The part that I think is important is
we need to have two things.
We need to have a global agreement
about how we're going to handle AI
as it gets more dangerous,
sharing information and things like that.
And we need technical means for alignment.
And that will only get more important as AI
is A, empowered more,
which is to say given more responsibility in the world.
And B, as it actually gets more proficient
in doing the things that it is asked to do.
Because there will be more opportunities
for bad actors to do bad things.
And you want your system to actually be able to calculate,
is this going to have an adverse effect
on a large part of humanity?
Then I shouldn't follow this instruction.
Like you actually, I mean,
the ex-riskers aren't wrong to want something like that.
Well, no, that's the thing, it's very reasonable.
And they have kind of concentric circles of,
you know, on the inner circle,
they're talking about being paper clipped.
And then they increasingly talk
about more and more plausible risks.
And the reason I made that comment is,
I made a video commenting on your Senate discussion
with Sam Altman.
And you're famous for being very skeptical
about the capabilities of neural networks.
And at one point in the meeting,
you were talking about the possibility
of them becoming self-aware.
And that just seemed incredibly...
I don't know what the odds are on that.
I think, I can't remember what I said,
but my general comments on that is,
I don't think we should cross that bridge.
I mean, it's one thing for a system
to have a model of like where objects are and so forth.
But there's a certain level of like agency and consciousness
that a bunch of people,
including at this conference that we were just at,
would like to see happen.
And I do not want to see happen.
I think it just opens another, you know, barrel of worms.
It's not like I think that's imminent.
Like, and you have to understand
that most of my skepticism about AI
is skepticism about what we can do right now
and over-claiming and so forth about current capabilities.
I think AGI is possible.
I don't expect to see it next week.
And I offered Elon Musk a million dollar bet
when he said that he thought it would happen by 2025.
And I would still love if he would make that bet
because it's easy money and why not?
Even if it goes to charity, I'd be happy to make the bet.
Elon, it's still there and I have a friend
who will raise it to 10 million if you're in.
Like, I don't think AGI is close,
but I don't see any principle reason why it's impossible.
I've never seen a good argument for that.
The arguments I've seen about that
are all kind of like mysticism or something like that.
But I think people will be interested to hear you say that
because people interpret your skepticism
as you being a kind of Luddite in a way.
And that's not really the case at all.
It's totally wrong.
And I mean, I've publicly said this a bunch of times
that it's totally wrong.
I think a particularly good place was when I was on
Ezra Klein's podcast.
And he said, you know, a lot of people might think you hate AI
but you built AI companies and you've been writing about it
for years, you've been interested in the kid.
And I said, I'm glad you asked that
because I do actually love AI.
Like, I wouldn't be doing this
if I didn't think there was a good outcome that was possible.
If I thought it was hopeless, I would, you know, just stop.
I would stop writing about AI if I thought it was hopeless.
Hopeless, I should say on a technical side
or on a kind of government side.
But I do think there's a chance we can do this right.
And that's why I sit here on whatever, Twitter every day,
trying to get us to a better place in AI
because I think it is possible and it is of value.
You know, I, not everybody will know
but I'm a high school dropout.
And the way that I pulled that off is,
and I went straight to college.
And the way that I managed that is I wrote an AI program
that translated a semester's worth of Latin into English.
So I mean, I was coding AI as an independent project
when I was 15 in the programming language logo,
which is a little bit like Lisp,
which I couldn't get my hands on.
Like, I have, you know,
ate and breathed AI for a long time
because I think it's interesting
because the way I think it reflects on human cognition
because of the value I think that it could bring to science
and technology, I want AI to succeed.
It's just that I think we've gone down this terrible rabbit hole
where the people who are running AI now
are not researchers, they're marketing type people
and not really that interested in the morality
of how this stuff is being used,
which I think was not always true.
So I think that the culture and politics have changed.
And I think technically we have gone
from a healthy scientific environment
where a lot of people were presenting
a lot of different models and comparing them,
thinking about different ideas to one in which
everybody has zeroed in on a technology
that I think is fundamentally flawed,
which is the large language model.
It's not that I think there's no value to it,
but I don't think it will get us to AGI.
I don't think it will get us to the medical discoveries
that we might make and so forth.
I think it's the wrong avenue
and it's also sucking down an enormous amount of resources.
So, you know, $100 billion went into driverless cars,
basically mostly on similar technology.
I don't know the exact dollars.
And 50 billion plus has gone in just to chips
on large language models.
And anybody who's a grad student
works on large language models
because that's where the money is.
And so you just, it's not an intellectually
healthy environment.
And like it's absolute absurdity
that you have seven companies or something like that,
all basically building knockoffs of GPT-4.
Like that's tens of billions of dollars
that could go towards developing better approaches to AI.
Like people, like the, you know,
the people that hate me the most are the EACC people,
the evolution, what do they call them,
the effective accelerationists.
And they understand I'm actually on their side
in a certain way, but they really don't get it.
I've tried to explain this.
And the way in which I'm on their side is
I'd like to see AI move along faster.
And what they can't separate is AI as we do it now,
which I think is really not what we want to do
from AI as we should do it.
I think that a good EACC person should say,
our end game here is not to make LLMs ubiquitous,
our end game is to make AI that would help people
and make society better and ask the question,
is this the right way to get there?
And the answer is pretty clearly,
or I'll say highly likely, no.
And so EACC people should be my biggest fans.
They should be like, that Marcus is sitting there every day
absorbing arrows, trying to figure out
how to bring us to an AI faster that would be better.
Like, how could they not like that?
Well, that's a beautiful point
because I hosted a debate with Beth Jezos,
you know, the head of the EACC and Connollyhi.
And Beth is creating this company to build
physics-inspired intelligence called Extropic.
And Conno's argument was you're basically submitting
to the void god of entropy in terms of morality.
So, you know, what if we just built
this kind of physical simulation of the world
and we just trusted it to be moral?
So presumably you wouldn't be a fan of that.
I mean, I'd have to know the details
and I watched a little of the video
and wasn't wildly impressed,
but Enzo didn't pursue it more.
But I would say that building a simulation
is not building a moral reasoner.
Those are not the same things.
And so, you know, for example, there are some scientists
who understand physical laws pretty well
and some of them are ethical and some of them aren't.
Like it's just a different thing.
Like you want some philosophers involved
in the ethical side and so forth in the ethicists.
They're just different things.
If you have a perfectly faithful simulator of the world,
which I think, you know, gets into Laplace's demon
and is not a realistic thing in the actual world
with finite resource, but if you had it,
it still wouldn't give you ethical decisions.
You know, we still have legal frameworks,
ethical frameworks and so forth,
independent of our knowledge of how the world operates.
Now, I think there's a small connection,
which is one of the things an ethical AI system ought to do
is to evaluate the consequences of its actions.
And so, the better the simulation you have,
not just of physics, but of like sociology
and you know, there's multiple layers of understanding
that we should have of the world, levels of understanding.
But if you could understand economics and psychology
and so forth and you could simulate all of these really well,
probably better than is actually realistic.
But if you could do that, you know, even decently well,
in the context of an ethical system
that is trying to avoid harm to humans, then great.
You know, think about Asimov's laws.
You know, the first do no harm.
Like think about that from a technical perspective.
You have to actually calculate,
would this action cause harm?
And the better the simulator you have,
the better you can do that.
But you can only like, you have to know to do that, right?
If you have the ethical principle of do no harm,
and of course, Asimov's story is a word about
how you get into various tricky, you know,
edge cases or whatever, but if you start with a system
that tries to anticipate the consequences of its actions,
then a simulator is very helpful.
If you don't start with that premise
and you just have a simulator, like so what?
Like you can use the simulator to, you know,
do a bunch of things with like,
you have to have the ethical framework
in order to leverage that.
Well, exactly, but for you though,
if we need to have a moral calculus,
presumably you think there are moral facts
about the world, where did they come from?
I mean, they're moral axioms, they're not facts.
Like you have to take a position there.
There's no moral set of facts
independent of humans in the world, right?
And that makes a lot of people run away screaming.
They're like, we're never gonna agree.
But the fact is there's a lot of stuff
we actually do agree on.
We don't talk about the stuff that we agree.
But if you took a survey of, you know,
100 people on the street and said, is murder okay?
98 of them would say no.
You could also say, what if it's in self-defense?
And a lot of them would say, okay, maybe,
and, you know, tell me about the circumstance and so forth.
But, you know, there's a lot of stuff we actually agree on.
There's some stuff we don't.
So when is the beginning of life?
Like, you know, people disagree about that.
And it's not clear there's a fact of the matter and so forth.
But there are some moral principles,
a lot of moral principles,
that most people actually agree.
We mostly agree, don't steal stuff, don't kill people.
You know, the Ten Commandments wouldn't even
be such a terrible start.
But we don't even know how to do that, right?
So like, you know, don't lie is, you know,
maybe a principle that people are less agreed on,
but to a first approximation, they agree on it.
But you can't tell an LLM, don't lie.
It does not compute in an LLM, right?
They literally don't understand that.
So, yes, there are disagreements.
There are cultural differences and so forth.
And people wanna play a slippery slope argument
and say, don't even try to build ethical AI.
But that's actually taking a stand too, right?
It's, in a way, it's taking an extreme libertarian view
of anything goes.
And at the most extreme, I think even libertarians
aren't actually that comfortable with that.
There's a lot, I mean, libertarians are not, in fact,
all saying, well, we should just legalize murder.
Like, they're just not saying that.
And if they did, we wouldn't take them seriously.
I think there was an example you gave in your book, though,
of a huge cultural difference in China,
my mind slipping on it.
But presumably, you do think that AI legislation
should be enacted quite differently in different cultures.
Well, I mean, the thing that I object to most
in Chinese AI regulation,
which is actually much stronger than US AI regulation,
is insistence that everything be consistent
with the party line.
And I would not like to see that in the United States.
I don't like seeing it in China.
It's sort of, it's not as much my business
or maybe none of my business, if they do that in China.
I certainly don't want anything like that
in the United States ever.
It's not legal to operate a chatbot
if it doesn't conform to what the state believes.
So it's one thing to say, chatbots shouldn't promote
conspiracy theories that are clearly false or what that,
like, I can imagine something in that direction,
but becoming an instrument of the ruling party,
no, I don't think that's just,
I don't think anybody on the planet should do it.
I can't stop China from doing it,
but I think it's wrong that China is doing it.
So Gary, almost exactly four years ago,
we invited you on to MLST.
We had early access to GPT-4.
It was actually Connor who gave us access
for a secret API he wasn't supposed to do,
and we invited you on and Walid Saber,
and we spoke about it.
Four or three, it was probably GPT-3.
It was GPT-3 four years ago.
I think it was about November 2020.
And the amazing thing, first of all,
is the consistency of your position.
And even now, the video's got hundreds of thousands of views.
Every day, people are commenting on it
and they're saying it's aged like fine wine,
but in quite a polarized way.
What I said was,
Yes, yes, isn't that incredible?
I mean, I've not been able to put people
in the headspace that I am.
To me, none of what's happened is surprising.
I mean, I figured out basically
how this stuff works in 1998,
and it hasn't really changed.
There's statistical approximators
that don't have world models.
And once you understand that,
the likely flaws like hallucinations just jump out.
And so, I was writing about hallucinations
in my 2001 book, The Algebraic Mind.
And it's not like there's ever been a principled solution
to the hallucinations or the stupid errors of reasoning.
And it's not even reasoning, it's approximation.
The stupid errors that come from approximation.
Nothing has ever been principled.
No idea has come up with a principled technology
to solve them.
And so, of course, those errors still happen.
Like to me, it's just obvious
that all this is gonna happen.
And what I always get is,
well, we'll just add more data and I'll solve it.
And I say, it's not gonna solve it
because it's not a principled solution.
Come back to me with a principled solution.
Explain to me, for example,
in the passage in which I anticipated hallucinations
in my 2001 book.
I gave the example of my aunt Esther
who sadly passed away last summer.
And I said, suppose that she wins the lottery.
And what's going to happen to a system
that doesn't have a distinct record system
for individuals as opposed to kinds?
What you're really learning about in these systems
are properties of kinds, not individuals.
You don't have an individual level predicate.
And I noticed that Subaru has started
saying some fairly similar things recently.
So I made this argument back in 2001.
I said, this is a principled problem.
If the system is told my aunt Esther wins the lottery,
it's gonna think that other women
who live in Massachusetts or who work for Harvard
or that share some properties with her
also are a little bit more likely to have won the lottery.
But that's not how the lottery works, right?
Other people that are in your pigeonhole advertising category
didn't also win the lottery.
And the system doesn't understand
that this individual property in here is in this individual
because it doesn't have a mental representation
like a database of particular individuals.
And so it was plain as day to me.
I think I did some modeling on one of the models
at the time by Ronald Hart and Todd,
if I remember correctly,
showing that that kind of error would happen.
And then you want to come to me and say,
more data is not, it's going to solve that problem.
Well, I did this work back in 1998.
I know that's not going to solve the problem.
You need an actual solution to that problem.
More data is just hand-waving.
So of course my predictions age like fine wine.
The only thing that would make those predictions break
is if somebody actually came up with a principled solution
and said, here's how we're going to handle
compositionality in this architecture
or here's how we're gonna handle distinctions
between individuals and kinds
or structured representations or operations over variables.
And instead people are just hoping that like by magic,
by having enough data, they'll go away.
And they get confused because any individual
puzzle that I put out there may get solved.
I mean, as we saw with the horse rides astronaut,
even sometimes they out in the literature,
they still don't get solved.
But in general, any given puzzle can be
because these systems are in part big memorization machines.
That's not all they are.
It's a bit of an oversimplification.
But you can think of them roughly as lookup tables.
And so you can store, you know,
multiplication tables, a lookup table.
So you can store anything you want in a lookup table
and fool yourself into thinking your system has it.
So you could take a young child
who hasn't really learned what multiplication is,
give them a multiplication table
and they could tell you what six times seven is
because they memorize the table.
That doesn't mean that they understand multiplication
at an abstract level.
And so what happens time and time again,
and I've been watching this for 30 years
and it kind of breaks my heart to keep seeing it,
is that people will see a single correct answer
and assume that the system
has the correct underlying abstraction
because they don't understand
how to think about data and errors.
Like that's what I really learned from Steve Banker
when he was my mentor,
was how to think about data and human errors
and algorithms and what the,
I mean, people don't have training in that.
So they just say, well, if there's some generalization,
then the system is done.
And in fact, the point of my 1998 work,
which I still think is my best work,
was to show that there's two different kinds
of generalization.
There's generalization that's kind of nearby
to the examples you've seen,
and there's further generalization that's further away.
Nowadays, we call that distribution shift.
That basically was writing about distribution shift
and how neural networks are or are not good models
of human minds in 1998.
So once you understand,
there's different kinds of generalization,
which may be 2% of the population
or 2% of the people in machine,
no, I guess that's not fair,
but 2% of the population understands
a growing fraction of machine learning understands.
Once you see that,
you realize that there are serious problems.
There are a lot of people
who just don't understand that basic distinction,
and they imagine a level of generalization that is not there.
It's kind of over-anthropomorphizing the system, et cetera.
And once you're down that path,
then you believe, okay, more data will solve this or whatever.
Once you're down the correct path, it's just obvious.
Once you really understand what these systems are doing,
it's just obvious that they're not miracle workers,
that they're not gonna solve these problems.
These core problems are not gonna go away.
I looked at Sora for two seconds and I said,
I'll bet you that that's gonna have problems
tracking objects over time,
because there is no representation here
of the individual objects.
Is that a pixel level?
And sure enough, it had tons of problems.
Like, I look at these systems
and I can usually take them apart in a few minutes,
because I can kind of see how they're working
and what they're lacking.
And that comes from a perspective of being,
A, a computer programmer since I was eight years old,
and B, a cognitive scientist since I was 13 years old.
And so I see it and I know.
It's annoying how dismissive people are.
I mean, when, on Chomsky video,
it's had nearly a million views.
It was the most amazing experience in my life.
Read the YouTube comments.
And I don't know whether people just don't understand.
It's so annoying watching people just write you off.
Yeah, people write Chomsky off all the time.
I mean, they'll do it based on his politics.
Like, I disagree with his politics,
so he must be wrong about linguistics.
That doesn't actually follow, right?
I mean, I actually disagree with his politics,
disagree with some of his linguistics.
But when he says that large language models
haven't taught us much about human language, he's right.
Like, we don't have a better theory
of how human use language from large language models.
Because large language models, as it happens,
depend on having like the entire internet as a corpus.
And it's obvious for anybody who's at a three-year-old child
that they listen for a couple of years,
not through the entire internet,
and then they basically understand it.
I know, but there are trolls everywhere.
I mean, many of these people don't probably know
that you sold an AI company.
With Kenneth Stan, they called Geometric Intelligence.
Ken Stanley, Zubing Aramani, Doug Beemus, Jeff Klune.
We're all part of that.
I was the first person to kind of launch the company.
I brought in Zubing, then eventually we brought in Ken,
eventually we brought in Jeff.
Yeah, I mean, most people don't know that.
I mean, it's there, and Wikipedia is there to be found.
Every now and then, I'll push back on the trolls
who attack me and say, well,
do you have papers in science and nature?
Did you sell a company to Uber, machine learning company?
Of course, they didn't.
Like, I don't know how to respond to that.
Like, people either put in the time
to actually understand who I am, or they don't want to.
I mean, they really don't want to.
They want to demonize me, as with Chomsky.
Surveillance capitalism is basically
that the companies make their money
by invading your privacy and selling information
about you to all sorts of people,
either directly or indirectly.
And I have to say that something that just like
turned my stomach the other day
was OpenAI bought a webcam company.
So I already knew that they were trying to access
all your documents.
They have a deal with Microsoft
that was gonna do this thing,
taking screenshots every five seconds.
I think the public maybe shot that down.
Initially, they had a database there in plain text.
So even if Microsoft wasn't directly
gonna do something bad with it,
like any bad actor with any chops
would have been able to take all this stuff.
So already you had this situation
where they're clearly trying to get everything
they possibly can about you.
And to commercialize it, probably to sell ads.
And then they bought a webcam company.
I was just like, what?
Like, they're having a lot of trouble
actually making money at OpenAI.
They lost, operating loss cost relative
to revenue last year, $5 billion.
A lot of companies tried their software
and are not that excited about it, having tried it out.
Copilot, which is run on their platform,
a lot of people aren't that happy with it.
Microsoft is like trying to rush people
to use the product.
Everybody tried it.
It's not like there was a lack of marketing,
but it's not that reliable.
So OpenAI is faced with the problem.
How are we going to make money off of this stuff?
And it's not cheap, right?
I mean, they've probably spent whatever,
$10 billion on data and chips.
And they're going to spend another 10 or whatever.
And GPT-4, who knows how expensive?
They need to make money or they will either go out of business
or Microsoft will take a bigger chunk of them.
Like it's not clear that VCs are going to want to up
their valuation to $200 billion,
given the kind of picture of revenue that is emerging.
And so, if I were Sam Altman
and I had a different kind of morality than I think I do,
first thing I would think about is surveillance, right?
They have, people are giving them all this data
on a silver platter.
Now you got a camera too?
Like, that's the natural direction for the company to go.
Despite their name, quote, OpenAI, right?
I mean, it's a really sinister thing for them to do,
to become big brother.
And did I mention there's still a nonprofit,
which is insane?
So we may very well wind up with
the world's largest surveillance company
as a wholly owned subsidiary of a place
that calls themselves OpenAI that's nonprofit.
Like that is, I think, almost a default outcome right now.
That's to me upsetting.
What about the NSA guy they employed?
Well, that's clearly part of the same picture.
I mean, if I were them, I would be building language models
on, you know, the government capture
or the surveillance data.
I'd be building a language model on that.
I'm sure the government is interested in doing that.
And I think, you know, they hired Nakasone,
Paul Nakasone, who worked in the NSA, clearly,
well, I won't say clearly, but it would seem plausible
that they hired Nakasone to try to get
those government contracts.
Like that would seem to be the play that they have.
Yeah, because other than the government,
OpenAI is interesting because they go direct to consumer.
The rest of the LLM ecosystem are developing,
you know, enterprise models.
What do you think about that?
I actually don't see the winning strategy
for anybody but NVIDIA right now.
So NVIDIA is selling shovels in the gold rush.
People want to buy the shovels, you know,
it's not their fault that people want to buy the shovels.
They've been hyping things a little bit,
but, you know, in general, people want to buy the shovels
and they know how to make the shovels.
They spend a lot of time, they're a very well-run company,
thinking about how to make the best possible shovels
on the planet and they really have that
and they're selling them and they're making a lot of money.
Everybody else, I think, is struggling.
Like, mid-journey is making a profit
but there's going to be massive lawsuits, I think,
against them or else they're going to have
to pay massive licensing.
So at the end of the day, I don't know
if mid-journey is going to make a profit.
And most of the other companies, you know,
they have modest revenue.
They're valued at like 200 times earnings
and that kind of stuff.
Not even 200 times net profit, just 200 times revenue.
These are kind of crazy numbers.
I don't think anybody has a strategy yet,
maybe in like a small sector,
but I don't think anybody has the strategy yet
to make multiple billions of dollars profit
year in and year out with this stuff.
I think that's still a dream.
I don't think anybody has a clear way yet
of doing that on the business-to-business side
or on the business-to-consumer side.
I won't say absolutely nobody will get there
but there's a really serious problem for all of them.
Like, deadly possibly problem,
which is that Metta is now giving away for free
essentially the same product as everybody else is making.
So there are ways you can deal with that.
You can have better customer service.
You can, you know, have a particular training set.
So like, it's not absolutely knocked down.
It's all over folks go home.
Like there's still a chance,
but that is a pretty serious problem.
And, you know, whatever you're doing,
you got a competitor now
who's going to do it on Metta's platforms.
You're doing it with your own data set.
Well, someone else is going to fine tune on Metta's platform.
You're not going to pay for open AI anymore.
Is it a bubble and is it going to burst?
So I'll say yes and no.
So I think that the economics don't work.
I wrote a piece last year called
Is Generative AI going to be a dud?
It was almost exactly a year ago to the day.
I think that that was prescient
and I think that it's becoming clear to all
that the economics don't really make sense.
I think a lot of investors are going to lose a lot of money.
A lot of LPs like pension funds that put in money to the investors
are going to lose money.
I think enthusiasm is going to be lost.
I think that the tools will remain.
You know, Metta's going to keep giving them away free
and people will still find some uses.
But I think like a year from now,
things are going to look different.
You know, many people who like move from crypto to AI
are going to move to something else
because they're going to say,
well, I don't see how to make money out of this.
It may be that investment dollars become much scarcer.
LLMs will continue to exist,
but eventually people will try to find other solutions.
I think neuro-symbolic AI is maybe finally going to have its day.
The recent results from DeepMind with Alpha Proof
and Alpha Geometry were impressive.
You know, it's a moment where I think there might be a shift.
So, generative AI I think will never be as popular again
as it was in 2023.
The tools will always be there,
but it'll be more like, yeah, use it for brainstorming.
And, you know, it won't be like this is the Messiah.
This is like the greatest thing ever.
Like people were saying like AI is, you know,
going to be bigger than fire and electricity and whatever.
And the truth is most people would not take generative AI
over their cell phones.
Like everybody, you know, if you take away somebody's cell phone,
they're going to be pretty upset.
A lot of people could actually live without generative AI.
They played with it, it was fun.
But if you said, look, I'm going to charge you $50 a month
for it, most people would say, yeah, I don't really need it.
Most companies have tried it out and they're like, yeah,
we can find something, you know, but people could live without it.
And the hype last year was that it was this indispensable thing
that we're going to transform our entire world.
AGI, if it comes, might be like that, right?
I mean, if you really had a system that can understand
the cognitive requirements and the physical requirements
of any human job, that's going to completely change our world.
And I think that that will happen.
Maybe in my lifetime, maybe not.
I think it will happen someday.
But generative AI was not that.
And it was never going to be that,
as I told you whenever it was three years ago,
that reality has settled in.
And it's just not going to be treated in the same way.
Plus you have the fact that Sam Altman's star has fallen somewhat.
A lot of this was like his personal charisma.
But a lot of people are skeptical of Sam at this point,
and I think with good reason.
And so you had this kind of rock star
and you had this constant hype, like every day
in every newspaper.
And in 2025, let's say, people still use generative AI.
They'll still be in the news sometimes.
Open AI will still be in business.
But people will think the $86 billion, whatever it was,
valuation was probably too high.
They can be like, how are they going to make money
when that is giving us a waiver free?
It'll still be there.
But it's not going to be this.
It's going to be like pet rocks.
You can still buy a pet rock, I think, in a toy store.
Like, pet rocks, most of you listeners
won't even know what I'm talking about.
But at one point before I was born,
everybody was buying a rock and calling it their pet.
And it was this crazy thing or, you know,
Tamagotchi's and you have these things that become fads
that completely take over the world for a little while
and then they disappear.
A point that I've often made is that people over extrapolate.
They think there's an exponential after they
see a few data points.
And there are different ways you can make fun of it.
One of them I like, I call the disco illusion.
And this is somebody else's cartoon.
Maybe we can put it up.
And it's like sales of disco albums in like 1974 and 1976.
And someone's like extrapolating,
I forget exactly how the cartoon goes,
that like in 1978, every record that is sold
will be disco and forever more.
And we all know it didn't work out that way.
Disco had its moment and then, you know, synthesizer music
and punk and all kinds of the other things took over.
People got tired.
It was fun for a while.
Chatchi P.T. is going to look a little like that.
You can still buy disco albums.
And I still like the soundtrack to Saturday Night Fever.
And I listen to it sometimes.
Like it still has a role in my life.
It didn't completely disappear.
I feel nostalgic about it.
But I don't spend 24 hours a day listening to disco anymore.
And people are not going to spend as much time with the chatbots.
And it's just not going to be the center of our lives
the way it has been really since November of 2022
when it came out.
Meta's interesting, though, because they're building this
metaverse and virtual reality and stuff like that.
So ironically, they do have some potential use cases
for the technology.
But some business leaders have said to me.
Well, but I think, and then you can come back
to the rest of your question.
I think a lot of people imagine you could populate
a metaverse world with large language model characters.
And I think that that's going to get tired fast
because those characters aren't really going
to understand the world.
And so it'll work for a little while.
Having non-player characters made by chatbots.
But eventually, things are not going to make sense.
And it's going to break the illusion.
The only way metaverse is going to work
is if the illusion is so potent and it's so much fun to play
with.
And the more that you have these weird experiences,
like today I had Grock.
I asked it to make 11 eggs without an egg carton.
Instead, it made 15 with an egg carton.
And we have enough experiences and you're like, yeah.
I know.
I was surprised by that.
But wouldn't it be interesting that it might be therapy.
It might be virtual partners.
It might be gaming.
Could there be an application, a killer application
that does change it?
They change metaverse?
Well, that makes generative AI economical.
I mean, I think the killer app is going to be surveillance.
I think that's the selling everybody's personal information
is historically been a profitable business.
And I think that's where they're going to land.
But I don't know.
Professor Gary Marcus, thank you so much for joining us today.
It's been a pleasure.
I love our conversations.

Hello, folks. I was in New Orleans last week and I had the pleasure of interviewing Laura Ruiz,
the primary author on this paper. Large language models are not zero-shot communicators.
Now, this is exploring the ability of language models to perform Implicituk, which I guess
from a machine learning audience point of view, you might think of as being some kind of extrapolation
or even abstractive reasoning. There's an example of this that we can try. Esther asked,
can you come to my party on Friday? And Zhuang responded, I've got to work, which means no.
Yeah, part of the reason I wanted to do this quick intro is since this interview,
OpenAI has released chat GPT, which is pretty impressive, actually. So we can come in here
and we can say something along the lines of Esther asked, can you come to the party on Friday?
Zhuang responded, I have to work. Does Zhuang, can Zhuang come to party?
It looks like it has failed. It says it's not possible to say whether Zhuang can come to the
party or not because we don't have enough information. Zhuang may or may not be able
to come to the party depending on his work schedule and other factors. Oh dear. Yeah,
so this is an example of failed implicature. But anyway, if we come over to Laura's Twitter,
she posted a little thread the other day saying that loads of people have been sending her
implicatures, which they used as examples in the paper. And apparently chat GPT does understand
some of them, which she's very happy about, but she wanted to write a short thread about it.
So she said before they started writing the paper, she would try lots of implicatures that she
came up with on Da Vinci 2 in different wordings with moderate success. Some always solved and
some half of the time depending on the wording, meaning random performance, since the test is
a binary, which is to say a yes or a no. That's why they decided to do a systematic test to figure
out how good it actually was and how much it depended on the wording of the prompt.
And a few months later, they had the answer that it was okay, but not close to humans.
And okay means that on Da Vinci 2 and 3, the performance of zero shot implicature is roughly
70%. Most of the other models fail, even with few shot in context prompting.
So anyway, she said that she gets the people are excited that chat GPT is doing pragmatic
inferences, but she felt the same with Da Vinci 2. It's all anecdotal, she says, but a more
systematic test shows a significant gap with humans nonetheless. And it's the same for Da Vinci 3
and presumably the same for chat GPT. She says that once this implicature dataset gets solved,
and she has no doubt that it will get solved relatively soon, since fine tuning with human
feedback helps a lot, they might have some baseline pragmatics in their models. And that's
when it will get really exciting. She says that she's personally blown away by chat GPT's capabilities.
It's absolutely incredible at explaining things, compositional generalization of concepts,
simulating a VM. I'm not sure what VM means, coherence, creativity, writing essays, poems,
and more. She said that the pragmatic language that they studied is part of a type of casual
language that we're using conversation that might emerge from social interactions. She's personally
thinking about why human feedback helps so much, and whether interactivity and social pressures
might help even more. Anyway, enjoy the interview. Hi. It's lovely to meet you. Nice to meet you too.
So I was speaking with Andrew Lampinam yesterday, and he really highly recommended your paper.
I looked it up, it's called A Large Language of Models Are Not Zero Shot Communicators,
and I also recognize Stella Biderman and Sarah Hooker, of course. Sarah's an absolute legend.
Now, you led in the paper by saying humans interpret language using beliefs and prior
knowledge about the world. For example, we intuitively understand the response, I wore gloves
to the question, did you leave fingerprints as meaning no? So you call this
implicature, but I suppose I would think of it as some kind of extrapolation, being able to kind of
reuse knowledge that we have about the world in a different situation.
But could you talk us through that paper? Okay. So yeah, thank you. That was a great
introduction to the paper. Indeed, implicature is kind of the technical term that we use for
this example that you gave. And indeed, extrapolation is a sensible way to describe this. What we do
in this paper is kind of showed that large language models are not really good at this
aspect of communication, and we think it's a very important aspect of communication.
So the title says large language models are not zero shells communicators, right? So what we mean
by that is to be a communicator, you have to infer the meaning of utterances, not only by
their semantics, so not only by how words combine into some kind of meaning, but by interpreting
the shared knowledge, our shared experience of the world. And that's what we look at in this
paper. And what we find is that large language models are pretty bad at this.
Specifically, we group them into different groups. So we have base large language models like OPT
and Bloom that are just trained on next word prediction. And we also have instructable models
like Flonzi5, T0, or the Vinci Instructable models by OpenAI. And all models perform really
bad, closer to random than to humans. But OpenAI's instructable models have much more
promise. They're much better at it. Interesting. Okay, so now the zero shot thing is very
interesting. So we take these models, and it's kind of like self-supervised learning. We train
them on loads of data on the Internet. And you're saying that zero shot is when we don't
really give much information in the prompt. So there's a relationship between how big the model
is and how much in-context learning that we give to the model in the prompt.
Yeah, that's true. Yeah, the zero shot case that we tested is we give the model a short instruction
saying like in the following exchange, someone gives a response that has some meaning beyond
utterances. The meaning is yes or no. Can you resolve this? And then we give an example.
And then we evaluate it on ways that based on whether it can choose yes or no. So that's the
zero shot case. And humans, we don't give any instructions at all. We just said, resolve this
to a yes or no. Okay, so is it the case that large language models then zero shot almost
irrespective of their size and irrespective of this human feedback alignment, they just don't
perform very well at this implicature at all? The instructable models by OpenAI
gets a non-trivial performance. I think the models like OPT and Bloom, those kind of base
models, they really conduce this style very well at all. They get 10% above random. But
OpenAI's models are around 70% at zero shots. Interesting. So did you do some work looking
at, okay, well, let's try some in-context learning. Does that improve the implicature?
Definitely, yeah. It's unclear, right, whether zero shots is a fair comparison to humans for
these models. Humans are primed in different ways. So we also wanted to try view shots in
context learning. And personally, I thought in this case, in context learning wouldn't help much
because each implicature requires a completely novel type of inference. But in fact, we show that
OpenAI's models is the only group of models that really benefits from this a lot. And they can get
to up to 80% performance with roughly five examples. And afterwards, more than five examples,
it's kind of plateaus. But there's still like a significant gap with humans. But it's a great
improvement. Yeah, that's fascinating. So can you give me an example of, if we were doing some
in-context learning, let's say with DaVinci too, what would that prompt look like?
So if we, I don't exactly remember the wording of the prompts, but there would be something like
the following are examples of the task. And then you get a bunch of
implicatures that are already resolved to a yes or no. And then you get the original instruct
prompt that says, resolve the following sentence to a yes or no. And then you get the actual example.
And these in-context examples are all taken from a development set.
Okay, okay. So can you tell us a little bit about how this reinforcement learning for human
preferences works on language models? So reinforcement learning for human preferences
is a method to fine-tune base large language models. So the base large language models are
OPT and BLU, for example, that's part of the group, and they are just trying to
on-next-word prediction, right? But they are not really aligned. There's sort of this alignment
problem where they, they're trained on next-word prediction, and that's not really what we are
asking them to do. And then with reinforcement learning from human feedback, what we further do,
we've, I mean, not we, unfortunately, other people do, is they take some kind of human
preferences from somewhere, like, for example, humans are shown prompts and completions by
models, and they say, this one is better than that one. This completion for the text for this
prompt is better than that one. By that, we get a sort of ranking by preference, and we can learn
a reward model on those preferences with an interesting trick that was published in 2017.
And through this rewards model, we have sort of, we can bootstrap the preferences from humans
into the Bayes-Arxangus model by fine-tuning them with regular RL on these, on this reward model.
Yeah, it's really interesting. I was speaking with Srijan Kumar, who won one of the outstanding
paper awards in Europe, and he's got this work on kind of, we want the models to be more anthropomorphic,
and we have these priors to help us understand the world. And he came up with a framework of kind
of like importing these priors from language encodings into, let's say, a discrete program
synthesis model. But I guess what I'm saying is that there's something really interesting going
on within context learning. And it's almost like we're giving the model the priors to extrapolate
or to do something useful in this particular situation. Yeah, yeah, that's really interesting.
I don't know the paper, I should check it out. But the way I view it, and my thinking has been
shaped this week also by Andrew Lampinen, who wrote an interesting paper on comparing models
in humans, is that it seems that in context learning for this specific task, implicatures,
it's not really that they learn how to use their shared experience from the in-contact samples,
their primes for the task, with a few shot examples in the context. And I think that's
actually what's happening here. Like, if you test the model zero shots, there's no, why would we
expect it to, you know, do this task properly? There's no motivation or anything like that.
But if you prime it with in-contact samples, it does better. And that would also explain
why it doesn't help to add more than five examples, because it's not using sort of the inherent
information in the examples, it's just being primed for this specific task.
That's really interesting. Sarah has done lots of work on interpretability in machine learning
models. And one thing that I wrestle with a lot is whether we should try and get models to think
the way humans do. And you can come at it from an intelligibility, you know, like an interpretability
point of view, but you could also come at it from a generalization point of view, like maybe
we do symbolic generalization over cognitive priors, and that's how we understand the world.
But there are people who just say large language models, they're just a different
mode of understanding, and we shouldn't try and make them like us. What do you think?
It's a good question. I am really a non-expert on interpretability. I always come at it from a
very anthropocentric view, like I would love them to be more like humans, because that would
make them interesting subjects to study also and better to communicate with. But at the same time,
you can take this opposite view. And I think Stella, the co-author on this paper often says,
you're making a category area or attributing something to these models that they don't have
knowledge, you know, like those kind of things. So it might also very well be that we're trying to
look for pragmatics or semantic understanding in these models, but that's just not how you
should think about it. And I completely forgot to ask you. So again, some of the audience don't
know about natural language understanding in linguistics and so on. So what is pragmatics?
Yeah, that's a good question. So pragmatics is an aspect of language, the way we study language,
that doesn't really look at the syntax or semantics, which look at, for example, those kind of aspects
of linguistics look at what a word means and how you combine them into novel meanings. So those
kind of areas of linguistics really look at when someone understands the term the utterance as John
loves Mary, they also understand utterance Mary loves John. Pragmatics goes beyond that. It looks at
how context and our shared experience really influences meaning. So usually, the meaning
determined by pragmatic inference is not really directly part of the context window. You really
have to tap into your prior knowledge. Yeah, so I'm a fan of Montague as well. So it's almost
like we have the semantic potential and then we have pragmatics and that's bringing some additional
context. Yeah, exactly. Yeah. Okay. And because that was a really great example from that
extrapolative example from Montague about Mary loves John. How could a large language model
realistically, because I think of that as being a symbolic generalization. So how could a language
model do that kind of generalization? Symbolic generalization? Yes. Oh, that's a big one.
I don't know. I really, really don't know. In my research journey, I can kind of come from
studying compositionality in language, which is really more this type of thing that we're talking
about now and looking at more sort of neuro symbolic approaches or stronger inductive biases.
And now, these large language models really showed us that there's an insane amount of
compositional generalization going on without any inductive bias for that. Chat GPT kind of
shows us that with all these examples on Twitter, you give it two novel concepts and
it combines it beautifully into some kind of story. But yeah, to go back to your question,
how can they do it? I don't know. Maybe skill will get us there to the sense that humans are also
imperfect symbolic reasoners. Again, to mention Andrew Olympian, he did a great paper on symbolic
behavior. Yeah, it's not really discreet. I can do symbolic processing versus I cannot do symbolic
processing, but it's more a scale. That's kind of shape my thinking as well. I think it's a scale.
Large language models are pretty far on that scale. They can do very interesting compositional
generalization and sort of symbolic behavior, but they fail in catastrophic ways as well. Again,
an example that I think comes from Gary Marcus is when you ask Chat GPT, how does horses ride
cowboys? And it just writes a whole story about how a horse rides a cowboy, even though it doesn't
work. Yeah, it's so interesting because I think it's easy to think of large language models
in the binary. So Marcus says they're bloviators and Bender says they're stochastic parrots and then
you have the folks who think that it's emergent reasoning and symbolic generalization. And I
was a skeptic and I just can't ignore the evidence. They really are doing amazing things now and you
were just speaking to Lampinen and it's a similar thing with this idea of symbolic generalization.
It might not be a binary, right? Yeah, exactly. It might be a very great
as competency and humans also fail in certain cases. So on this in-context learning, because
that's something that has interested me. So the first version of GPT-3 is zero shot. We didn't
really know how to prompt it. It looked like a bloviator. We then went on this intellectual
journey and we discovered prompt engineering, scratch pad, chain of thought reasoning. I spoke
with Hattie Zoe the other day and she's got this kind of algorithmic prompt in-context learning
and it's just remarkable what's going on there. Do you have any intuition? Is it like the prompt
is some kind of a program interpreter or something? My intuition is rather that the prompt kind of,
I don't know how to formalize this intuition, but I guess that's why it's an intuition.
That's a prompt kind of, prompts the model and puts it into a sort of area of its weight space
where it can better answer the actual question that is asked in, yeah, the actual question that's
asked. And I think certain things that point towards this is that there is also some research
coming out where they permute the labels in the in-context examples and they show that the
performance is similarly good, even though at the same or like they do completely random labels in
the in-context examples and the performance is still pretty good. But there's also other work
that shows that doesn't always work. Sometimes you do need the actual labels. So yeah, again,
the answer is basically, I don't know, but my intuition is rather that the model is really
primed for, or there's also another great way of viewing this. And I read that unless wrong at some
point, but I don't know how to attribute author, but because I forgot their name, but it's about
that these models are good at simulating anything. So you have to sort of prime them to let them know
what they're simulating right now. Yes, weird, isn't it? Because we have an anthropocentric
view of the world and we're a gentile with individual agents and a language model is
everything at once. So it's almost like you need to give it a trajectory just to get it to go somewhere
interesting. So with this in mind, we really want to make progress in natural language understanding
and what do you think are the steps we need to make to robustify these language models?
Yeah, that's a good question. Personally, from this pragmatics paper, I think pragmatics is one
area where they can make huge strides too. I think even though they have semantic failure modes,
they're really impressive at that. They're really impressive at compositional generalization.
But pragmatics might be something that they're simply not trained for currently. And one very
low-hanging fruit is the RLHF that we talked about. I think that clearly really improves that it's
and intuitively, it seems like in the instructivity paper, you see that they ask the human laborers
to really infer the human intent and the problems and write on. And that's very reminiscent of
implicatures. But then on a more sort of broader scale, I think some kind of embodiment or
interactivity might be really important. Like pragmatic inference is really a social skill
that we have. There's a lot of pragmatic pressures that you encounter while just
acting in the world and navigating communication and navigating a lot of things. So I think I'm
currently trying to look at a setup in reinforcement learning where we're trying to do a make a
pragmatic task and see when pragmatic reasoning would emerge there. I don't know how to consolidate
that fully with large language models yet, but I think interactivity and social navigation is
important. I'm really fascinated by the embedded tradition in cognitive science and I think there's
a lot of interesting work there. But I suspect you do as well a little bit. How do you contrast
what is essentially the representationalist view where everything's in this big monolithic model
to some kind of relational view where we're using essentially the world as its own representation?
Yeah. Again here, I don't know to what extent it's also possible to express everything in just
the representation as few where you have an internal world model. And I don't know to what
extent you really need an external world to learn, but it's intuitively it seems like that might be
very important. And intuitively it also seems like the behaviors that can emerge are really limited
by the world's models acting in. So a large language model only sees sex and there's basic
things that just simply cannot learn even though it has surprised us a lot. So I think, I don't know,
it's easy to think about is in that it's really important to have some kind of external world
to interact with. But I'm happy people are working on scaling and I'm not saying
some type of AGI, whatever that means, might not emerge from simply scaling up basically what we're
doing right now. Amazing. And are there any other parts of your paper that we haven't spoken about?
Yeah, one thing that we found is pretty interesting is that even though all these
open AI's models can get really high performance, close to humans, 6% roughly that won't tell you
much without the details. But it's a significant gap still, but it's really, really close. I don't
know if a human might figure out whether this model is a model or a human in that case. But
when we sort of drill down and make a taxonomy of the examples that are in this data set, we
find that they are mostly benefiting from the simple examples where not a lot of context is
needing. So one example is an implicator is if you ask me how many people came to your party and
then I say some people came. It's really the conventional meaning kind of of towards some
that I meant not all people came. But it's still an implicator, but it's a very common one.
So those kind of examples, if we isolate those and we look at specifically examples that are
really context-dependent like, are you coming to the open AI party tonight? I have food poisoning,
you know, those need much more context to be resolved. And then the performance decreases
again. And there is roughly a 9% gap, which is like the best model. But all other based models
and instructable models like Flancy5 and stuff, they then again completely fail on those kind of
examples. Fascinating. I'm really interested by this idea that understanding is a complex
phenomenon. And it's like the parable of the blind man and the elephant. So we create all of these
metrics. And the metrics exclude most of the truth. And the metrics for pragmatics presumably are
in some sense even more complicated than the metrics that we already use in natural language
understanding. And it just feels like, is that going to be a serious problem for us to kind of
encapsulate how well the model understands? Do you mean that we're sort of giving it a test
that is couldn't really solve? Well, I suppose one way of looking at it is in this particular
test, we've come up with lots of examples of pragmatic inference, if you like. And what we're
doing is we're taking a very complex phenomenon and we're putting pins through it. So we're putting
like little slices through it in different angles. And then we've got this shortcut problem that if
we optimize on any one of those slices, we might be kind of like excluding everything else that's
important. So it feels like we've got, I mean, is this like a general problem in natural language
understanding? It seems like you're getting at evaluation to some extent. I think evaluation is
the single most difficult thing in NLP. This is just a benchmark to give us some intuition as to
what these current failure models, failure modes of these models might be. And I think if this
benchmark is at some point passed by a model that's in and of itself without trying to diminish
my own paper, it doesn't tell us much. Like there's much more to be done. We need more
different benchmarks. We need like human testing in a sort of Turing style maybe.
And yeah, I think this is the most interesting problem in NLP, like how the property evaluates.
Interesting. And do you take an interest in fairness and bias in the models as well?
I'm very interested in it, but from a sort of spectator view as well. I haven't worked on it at
all. Okay. Yeah. Because that's presumably a massive challenge. Yeah. Yeah, definitely. Yeah.
Amazing. So in final question, what are you most excited about in your research trajectory over the
next year or so? Well, definitely, I just feel super excited to be working on stuff like this.
Currently, given the capabilities these models show, they're absolutely amazing. And I love
seeing how people interact with them. Like the creativity of people is really needed to get
some kind of interesting response out of these models, right? And also the creativity of people
is needed to find the failure modes. And yeah, so what I'm most interested, excited about now
personally for my own research journey is really trying to look at, you know, interactive setup
and see when pragmatic inferences might emerge in what kind of environments and what kind of
pressures do we need and how can we translate that back to getting to be, to getting like
language models be zero shells communicators. Amazing. And where can people find out more about
you? They can follow me on Twitter. It's first name, last name. And I have a website also first
name, lastname.com. Yeah. Amazing. Laura, thank you so much. It's a pleasure to meet you.
Thank you for having me. Amazing. All right, cool.

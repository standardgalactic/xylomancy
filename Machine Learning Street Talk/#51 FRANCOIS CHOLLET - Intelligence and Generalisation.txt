Past a certain level of complexity, every system starts looking like a living organism.
In order to build a general intelligence, you need to be optimising for generality itself.
We are surrounded by isomorphisms, just like a kaleidoscope.
It creates a remarkable richness of patterns from a tiny little bit of information.
Generalisation is the ability to mine previous experience to make sense of future novel situations.
Generalisation describes a knowledge differential.
It characterises the ratio between known information and the space of possible future situations.
To what extent can we analyse the knowledge that we already have into simulacrums that apply widely across experienced space?
So intelligence, which is to say generalisation power, is literally sensitivity to abstract analysis.
And it's in fact all there is to it.
In today's show, we are joined by Francois Chorlet.
I have been using the Keras Library for many years.
I also read his deep learning with Python Book, which was inspiring.
And I discovered his racy Twitter feed.
When I worked for Microsoft, I used to run machine learning seminars and workshops and hackathons.
I used to travel around the world and I always had a copy of Francois' book under my arm.
It never left my side.
I used to force everyone to read the first four chapters of that book and of course the chapter on the limitations of deep learning before we did anything.
Francois has a clarity of thought, which is unparalleled, I think, in any other human being on the planet.
It's really quite incredible.
Indeed, even our own Dr. Duggar, who normally has no trouble at all finding holes in some of our guests' work,
had this to say while prepping for the show.
I am working on it.
It turned out to be a little bit more difficult than I thought.
Chorlet is a little bit too reasonable.
Yeah, do you like my Duggar accent?
He would enjoy me doing that.
But anyway, Chorlet is extremely controversial to some people, actually, but he's not controversial to us.
Our discussion today lies at the intersection of machine learning and reasoning.
Now, Chorlet has made his vision completely clear about what he thinks the future of machine learning is.
Make no mistake.
What you should take from today's episode is that the future of artificial intelligence is going to be discreet as well as continuous.
Actually, the two are going to be enmeshed.
The future of AI will almost certainly involve a large degree of program synthesis.
Deep learning has its limits.
You can use deep learning for continuous problems where the data is interpolative and has a learnable manifold
and where you have a dense sampling across the entire surface of the manifold between which you need to make predictions.
For Chorlet, generalization itself is by far the most important feature of intelligence and of developing strong AI.
He describes a spectrum of generalization, starting with, for example, a chess algorithm where there is no novelty to adapt to whatsoever.
The task is fixed.
The machine learning we have today confers some adaptation within a known domain of tasks.
For example, being able to recognize dogs or cats within a variety of different poses and lighting conditions.
What's not been robustly demonstrated so far is broad generalization, adaptation to unknown unknowns within a known but broad domain.
It's certainly true that we're knocking on the door of this now with GPT-3 where the sub-task, if you like, is given at test time.
Although Chorlet would make the argument that the sub-task isn't learned at test time,
everything that GPT-3 knows was learned on the vast amounts of training data that we trained it on,
the poet algorithm from Kenneth Stanley et al.
That appears to be meta-learning tasks as part of the training process, which is very, very interesting.
So it's creating new problems and new solutions as part of the training process.
But broadly speaking, in the machine learning space at the moment, the task that we are doing is fixed and not generalizable.
The other thing is that the real world does not have a static distribution.
We need systems that can adapt dynamically.
Intelligence requires that you adapt to novelty without the help of the engineer who helped you write the system.
Chorlet has come up with a formalism of intelligence that balances the task skill, the difficulty,
the knowledge and experience to effectively quantify and normalise an algorithmic information conversion ratio.
It's the ability to convert experience into future skill, that is Chorlet's measure of intelligence.
At the end of his measure of intelligence paper, Francois introduced the ARC challenge.
It became a Kaggle competition as well, and it introduced a massive diversity of tasks.
The reason we have a diversity of tasks is for developer-aware generalisation.
Any model that we have needs to generalise to tasks that the developer was unaware of.
And Chorlet thinks that intelligence is specialised.
It needs to be human-centric or anthropocentric.
So the kind of priors that you need to solve these intelligence tasks need to represent the kind of priors that us humans have.
Now machine learning algorithms are completely ineffective against the ARC challenge
because it's so challenging to generalise from a few examples.
The only solutions that were effective in the ARC challenge were programme synthesis.
The manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space.
There are both theoretical and experimental reasons to believe this is true.
If you believe this, then the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds.
The only way deep learning models can generalise is via interpolation.
Most perception problems in particular, according to Francois, are interpolative.
Neural networks not only have to represent the manifold of the data that they're learning,
the manifold also needs to be learnable.
And that's an even tougher constraint.
Gradient descent will not learn data that has challenging discontinuities in its manifold.
It'll just resort to memorising the data.
Deep learning allows you to represent complex programmes that you couldn't write by hand,
but on the other side of the coin, it also fails to represent very simple programmes that you could write by hand.
Discrete programmes.
So there are some problems where deep learning is a great fit
and there are other problems where deep learning is a disaster.
And the reason for that is that they are not interpolative in nature.
These tend to be algorithmic reasoning problems.
Francois thinks that 99% of software written today with code is not interpolative in nature
and therefore it's a bad fit for deep learning.
The only answer to these problems is discrete programme search.
To use deep learning for these problems requires a lot of data.
It's hard to train and the representation will be glitchy.
It'll be brittle.
Neural networks cannot even extrapolate the scalar identity function f of x equals x.
They can only interpolate, given the existence of a smooth manifold in the latent space.
Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.
So is this similar to interpolation?
Well, I mean all of machine learning is similar to interpolation if you want to, right?
When you train a linear regression on scalar values, you're training a model, right?
You're giving a bunch of pairs x and y.
You're asking what are the best values of a and b for y equals a x plus b
that minimises the square error of the prediction of a line to all of the points, right?
That's linear regression.
That's interpolation.
All of machine learning is interpolation.
In a high dimensional space, there is essentially no such thing as interpolation.
Everything is extrapolation.
So imagine you are in a space of images, right?
So you have a core images 256 by 256.
So it's 200,000 dimensional input space.
Even if you have a million samples, you're only covering a tiny portion of the dimensions
of that space, right?
Those images are in a tiny sliver of surface among the space of all possible combinations
of values of pixels.
So when you show the system a new image, it's very unlikely that this image is a linear
combination of previous images.
What you're doing is extrapolation, not interpolation, OK?
And in high dimension, all of machine learning is extrapolation, which is why it's hard.
I'm being brave calling out Jan Le Coon, the godfather of deep learning.
But hear me out.
It's certainly true that interpolation on the native data domain is useless, right?
We need to pull some useful information out of the data, and the model architecture and
training method matter a lot here.
We can all agree that interpolation on the learned manifold would seem like extrapolation
in the original space of the data, right?
Chalet is quite clear that neural networks only generalize through interpolation.
You might argue that you can go a tiny step outside of the convex hull of your data, even
by a tiny little bit, and you can technically extrapolate.
Well, I would argue that if the manifold doesn't give you any useful information outside of
the training range, then it wouldn't be any better than finding your nearest training
example and just adding a bit of random noise.
If you train a GAN, for example, you can interpolate on the latent manifold, but interestingly,
you can extrapolate.
But the reason for that is the natural manifold that the data of faces sits on might be shaped
like a football or a sphere, which means if you go outside of the training range, you
actually have some information about those data points.
The scalar identity function might seem like a contrived example, but it's a really interesting
one.
When you go outside of the training range, nothing about the manifold is known, right?
Think about the manifold, it's just a string that goes on forever.
We don't know anything about that manifold outside of the training range.
This is not true for most perceptual problems in deep learning, and this is why image models,
for example, suffer greatly drawing straight lines.
What are your thoughts about this?
Why don't you let us know in the comments section on YouTube?
So there's a real interesting dichotomy of continuous problems versus discrete problems
that we're going to be exploring in the show today.
It's very interesting that brittleness works both ways, depending on the discreteness of
the problem.
Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,
and deep learning would be extremely brittle predicting the digits of pi or prime numbers
or sorting a list.
So brittleness here means the overall fit of your model or your program, so accuracy
and robustness.
Imagine if every single bug you experienced with computer software was entirely unique
to you and the development team wouldn't even be able to reproduce it.
This is what would happen if software was written entirely with neural networks.
It would be more, not less brittle.
Chalet thinks that motivated thinking is the primary obstacle to getting people to wake
up to the fact that neural networks are poorly suited to discrete problems.
The people who are good enough at deep learning to realize its limitations are too invested
in its success to say so.
Chalet fundamentally thinks that there are two types of thinking, type one and type two.
He thinks that every single thought in our minds is not simply one or the other, rather
it's a combination of both types.
Type one and type two, they are enmeshed together in everything you think and in everything
you do.
Even our reasoning is guided by intuition, which is interpolative in nature.
Chalet thinks that abstraction is key to generalization and the way we perform abstraction
is different in continuous versus discrete space.
We need to find analogies and those analogies will be found differently in both of those
different spaces.
Time search allows us to generalize broadly from just a few examples.
It marks a significant deviation from traditional machine learning.
Rather than trying to interpolate between the examples you have, you're constructing an
entire search space from scratch and testing if it fits our training data.
It all started with the flash fill feature in Microsoft Excel.
Do you remember that?
You give a few examples of some transformation that you want to perform and it will generate
a piece of programming code for you, which means it can generalize that transformation
across an entire spreadsheet.
It's quite a revolutionary idea.
It's been around for about 20 years actually, but what's really making it work now is the
idea of using neural networks or a neural engine to guide the discrete program search.
We spoke about GPT-3.
He thinks that GPT-3 hasn't expanded his knowledge of the world.
He says that GPT-3 is not learning any new algorithms on the fly.
It's already learned continuous and often glitchy representations of existing tasks
during its training.
It's completely ineffective against his arc challenge tasks.
People often claim that neural networks are Turing Complete.
No they're not.
A model has a bounded number of nodes and a bounded runtime.
It cannot execute algorithms that require unbounded space or unbounded time.
For example, could you train a neural network to predict the nth digit of pi?
No, you couldn't.
You could write a computer program to do it, but you couldn't train a neural network to
do it.
A simple Turing Machine program can do just that and that is because a Turing Machine
can access unbounded memory and time.
The best thing that neural networks can do is approximate unbounded algorithms, but doing
so will introduce glitches.
For example, one can train a neural network to approximately multiply integers together,
yet even when learning to multiply fixed-width integers, practically sized neural networks
introduce errors occasionally, and for a fixed-sized neural network, these errors grow more common
as the size of the input grows.
That said, neural networks are finite state machines, and just as finite state machines
can be augmented with unbounded memory and iteration to yield a Turing Machine, neural
networks can also be automated in the same way to produce a Turing Complete computational
model.
If you want to see a concrete example of the kind of discrete program search that Chalet
is talking about, look no further than the recent DreamCoder paper.
Yannick just made a video about it.
So yeah, it feels like today is the culmination of a year of really hard work and passion
from the MLST team.
We've worked with so many fascinating people, we've had so many amazing guests on.
It really means a lot to us.
Today is a very, very special episode.
It was my dream from the beginning to get Chalet on the show.
I know that Chalet is going to say lots of interesting things that will trigger some
people and inspire others, and please take to the comments section and tell us exactly
what you think.
Anyway, enjoy the show.
See you next week.
Peace out.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my two
compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.
Now today we have a very special guest, Francois Chalet.
Francois is one of the few leaders in the machine learning space who's caused a massive
stir in my thinking, the only other notable one actually being Kenneth Stanley, who we
had on recently.
My ultimate goal with Street Talk was always to get Francois on the show, and I can't believe
that it's actually happened.
We actually have a rule, by the way, that I'm only allowed to invoke Francois's name
about once per show, but that rule will not apply today.
So Yannick and I have made more content on Francois Chalet, actually, than anyone else
by a wide margin, and it's because his work is very thought-provoking and disruptive.
I spent many weeks actually studying his measure of intelligence paper last year, and of course
his recent New York's workshop was fascinating as well.
Almost every single word, in my opinion, that comes out of Francois's mouth deserves
a rigorous study, and I seriously mean that.
So Francois thinks that intelligence is embodied, it's a process, and it's not just a brain.
He's skeptical of the so-called intelligence explosion, and he thinks there's no such thing
as general intelligence.
All intelligence is specialized.
Critically, he thinks that generalization, the ability to deal with novelty and uncertainty
is the most important concept in intelligence.
He thinks that task-specific skills tells you nothing about intelligence.
He thinks that deep learning only works for problems where the manifold hypothesis applies.
For example, problems which are interpolative in nature and when a sufficiently dense sampling
of your distribution is obtained.
Otherwise, deep learning cannot generalize.
Deep learning can only memorize, but it cannot always generalize.
In his recent New York's presentation, he introduced the concept of program-centric
and value-centric generalization, which we'll get into in the show today.
I wanted to move straight on to this concept of deep learning being a hash table, because
this is what Francois thinks.
He says that a deep learning model is like a high-dimensional curve with some constraints
on its structure given by inductive priors, and that curve has enough parameters that
it could fit almost anything.
If you train your model for long enough, it'll simply memorize your data.
Because of SGD, your manifold fit is found progressively, and at some point the manifold
will approximate the natural manifold between underfitting and overfitting.
At this point, you'll be able to make sense of novel inputs by interpolating on that manifold.
The power of the model to generalize is actually a consequence of the structure of the data
and the gradual process of SGD, according to Francois, rather than any property of the
model itself.
Last week, Francois, we were talking to Christian Saugedi, and he takes a rather different view,
because one school of thought is that deep learning models are kind of like searching
for a space of possible programs, and advocates of GPT-3 make this argument quite strongly.
And presumably, Christian Saugedi, he wouldn't be doing what he's doing, which is interpolating
between mathematical conjectures, assuming that interpolation space would actually give
us new information about mathematics, if he thought that that space wasn't interpolatable.
What do you think, Francois?
Right.
I think you've basically summarized it, really.
So interpolation is the origin of generalization in deep learning models, and that's very much
by construction, by nature.
Like a deep learning model is a very large, differentiable, parametric model, trained
with gradient descent.
And so the only way it's ever going to be generalizing is your interpolation.
This is literally, this is what it is, this is what it does.
So I think the question, you know, are deep learning models, interpolators, or not, is
not a super interesting question, because it's not an open question.
We know what they are.
The more interesting question, I think, is, what can you actually achieve with the sort
of interpolation on this very complex, very high-dimensional manifold that they're deep
learning models and implementing?
And we're telling you the properties of this generalization, the tasks for which it will
perform well, the tasks for which it will not perform well.
I guess one example I could give you is encoding data with the Fourier transform, like you
know about the Fourier transform.
And maybe, you know, some people will play around with it, and they will be like, hey,
you know, actually the Fourier transform can draw much more than curves.
Look, I made a square with it, right?
And then you would have to point out that, no, actually the square, you've made it by
superimposing lots of tiny curves, and it's not, in fact, a perfect square, right?
Because it is made of this, with the other superposition of lots of tiny curves.
And that's really, this is true by nature, by construction.
This is where the Fourier transform starts, right?
And the more interesting question is, you know, what sort of data is a good fit for
encoding the Fourier transform?
And what sort of data is not a good fit?
Like if you try to encode the t-square fractal with the Fourier transform, you're going
to have a bad time.
And if you try to encode a drawing that's mostly just, you know, nice, smooth curves,
then it's going to be a very, very efficient encoding and a good idea.
And deep learning is very much like that.
We should ask, you know, what are its strong points, what are its weak points?
Yeah, so I, by the way, so I don't believe that deep learning models are
hash tables, plus they're, I usually say they're localities, sensitive hash tables,
meaning that they're kind of like a hash table with some amount of generalization
power, because they have some notion of distance between parts.
They're capable of comparing points by measuring the distance between them, right?
And this is what would enable this kind of hash table to actually generalize, as
opposed to the classic kind of hash table, which we should just be memorizing the data.
It's very interesting that you allude to the fact that, you know, what kind of data
is the model good for and so on.
And now deep learning models being essentially like really, as, as Tim said,
like big interpolators of arbitrary manifolds, do you think there is something
common across the types of data we choose deep learning for?
Or, you know, could we in fact use deep learning for most kinds of manifold
dish data?
Or do you think there is some kind of specialness about natural signals that
makes deep learning very attuned to them?
So I think most things are to some extent interpolative, which is why you can
actually do lots of things with deep learning models.
Doesn't necessarily mean it's always a good idea, but it's, it's going to kind of
work, right?
You know, when people hear the word interpolation, they tend to think about
linear interpolation, that's what pops up in their mind.
But that's not actually, I don't know what deep learning models are doing.
Like they're, they're interpolating on this very complex, very high
dimensional manifold.
And this enables very, you know, arbitrarily complex behavior.
And in practice, it's always possible to an arbitrary discrete algorithm in a
continuous manifold, right?
It's not necessarily a good idea, but it's always possible, at least in theory.
So for any program you can imagine, you can ask, you know, is there a deep learning
model that would encode some kind of approximation of it?
And the answer is always yes, right?
Similar to how you can always encode an arbitrary shape with the Fourier transform.
Right.
But there are, if you try to do that, actually, there are some issues with that.
So there are very much, you know, some problems for which deep learning is good
fit, some problems for which deep learning is not a good fit.
In the limit, the extreme point is a space that is not interpolative at all, which
is quite right.
Actually, you know, most spaces, even very discrete kind of spaces, do have, you
know, some amounts of interpolativeness.
So like, but one example would be, for instance, trying to train a deep learning
model to predict the next prime number, right?
Or to tell whether a number is a prime number.
So you cannot actually do that.
The best you can do is memorize the train data point because the space of prime
numbers is not interpreted at all.
So your deep learning model will always have zero generalization power.
But that's actually quite rare.
This is kind of an extreme case.
Most problems, even problems that are binary, discrete, algorithmic problems,
there will be some amount of interpolation that you can do, right?
But that doesn't necessarily mean that it's a good idea to try to solve, you
know, such problems with deep learning models.
For deep learning to be a good idea, you need a very, you need very much the
manifold level as is to apply.
So it works best for perception problems.
Any problem that humans can solve via pure intuition or perception is probably
a good fit for the plan.
But any problem where, you know, high level explicit step by step reasoning is
probably a bad fit for the plan.
And, you know, 99% of what today's software engineers solve, the writing code is
going to be a bad fit for the plan.
That doesn't mean that there wouldn't be, you know, theoretically a deep learning
model that can embed the same algorithm in a smooth manifold.
This is always possible to some extent, right?
But there are very significant issues with attempting to do this.
I like just because something theoretically possible doesn't mean you should
actually do it.
I think we might be not being careful enough when we say what we mean by
program, because for example, if I take program to be the universal sense,
like a program is something that can run on a Turing machine, for example,
because of the fact that that type of program actually has access to unbounded
time and memory computation, it's impossible in the general sense to
encode that in any finite neural network.
Like I can write a very short piece of code theoretical Turing machine can
output, you know, the nth digit of Pi.
It's impossible to do that with any finite neural network.
Would you agree with that?
Yeah, absolutely, absolutely.
Okay, because I think that's like a big source of confusion often time with
these statements that like, you know, Oh, neural networks are Turing complete.
Well, no, they're not that, you know, if you have a neural Turing machine,
which is a neural network that's the finite state machine piece of a Turing
machine, that can be Turing complete.
But in the general case, you know, finite neural networks, which is what
everyone means by neural networks are not Turing complete.
And it actually has practical effects, right?
This is why we see this sort of explosion in the number of parameters to
kind of, you know, start to accomplish.
Yeah, absolutely.
100% your entirely right.
So we're only interested in realistic programs, like the sort of programs
that software engineer would write, for instance, and we're only interested in
realistic neural networks.
And by the way, the constraints that we have on your networks are actually
much stronger than asking, given this program that I have, is there a neural
network that could embed it in a continuous manifold?
The constraint is actually, is there a neural network that could not
only represent it, but that could learn this embedding on the program from
there?
And this is a several orders of magnitude harder, right?
Learnability is a big problem because you're fitting your manifold via
gradient descent, right?
And if the structure you're trying to fit is too discrete, with too
big discontinuities, gradient descent will not work at all.
And the best you can do is, again, just memorize the train data.
So I can maybe give you a concrete example to kind of ground our discussion
here.
So in 2015, some friend of mine, so his name is, he used Keras to do
something pretty cool, which actually became a good example on the Keras
website.
He used an LSTM model to multiply numbers, but not like numbers multiplied
by value, but the input of the model would be strengths, like two
strengths, strengths of digits.
And the LSTM will actually learn the multiplication algorithm for like
multiplying three digits and three digit numbers, kind of the sort of
algorithm we would learn in primary school, right, to do multiplication.
And remarkably, that worked, right?
It works just fine.
So you can train a deep learning model to learn this algorithm.
And you could, of course, train a transformer model to do the same.
It would actually be probably significantly more efficient.
But so that works.
That comes, however, with a number of downsides.
So first, in order to train that algorithm, which is very simple, you're
going to need thousands and thousands of examples, right, of different
strategic numbers.
And once you've trained your algorithm, because the actual algorithm was
embedded in the neural network, it does generalize to never sing before digits.
Right.
So it's actually, it's actually learning the algorithm.
It's not just learning, I'm just not memorizing the data.
But the thing is, because the embedding of an algorithm, the embedding
of a discrete structure in the continuous space, is not the same thing
as the original discrete object.
There are glitches in your deep learning network, unless that's something
you could have found the program synthesis, for instance, it's not going
to be correct 400% of the time.
It's going to, it's going to be correct 95% of the time in merge.
The same way that if you try to encode a very discreet object via the Fourier
transform, it's not going to be correct.
It's 100% of the time is going to be an approximation and around sharp
angles, it's actually going to be wrong.
And very importantly, and this is really like the algorithm that you've, you
know, painstakingly embedded into your deep learning model via exposure to data
does only, it does not generalize very well.
It only does local generalization.
Meaning that if you train it with three, to multiply three digit numbers
and then you send it a five digit number, is it going to work?
No, absolutely not.
And not only is it not going to work, but you could not, in fact, a few
shot fine tune your algorithm to learn to handle five digits, seven digits.
And so if you want to fine tune your algorithm, you're going to need
thousands, maybe millions of examples, right?
So it's, it's all local generalization.
And lastly, it's, it's super inefficient.
Like, I think we can all agree with this, that multiplication is, is not, like
it's not a clever use of an LSTM.
It's your, your burning tons of resources for something that's actually
surprising.
And you can compare that.
Like since we are, we are talking about pros and cons of deep learning, you
can compare that to what you could get with a program synthesis engine.
Like I don't want to compare to what you could get with a human written algorithm
because kind of the point of deep learning is that it enables you, there
are programs that you could not otherwise write back in.
So the right point of comparison is actually what you could do with deep
learning versus what you could do with discrete program synthesis based on
discrete search and the DSL.
And if you were to use a program synthesis to solve the multiplication
problem, so you would find a solution, even a very neat engine that does just
like maybe, you know, the plus operation, maybe a loop.
And this DSL is going to find it.
And it can find it with a handful of examples.
You're not going to need thousands of examples, like in the deep learning case,
you're going to need maybe five.
And the program you get out of it is going to be exact because it is the
exact discrete algorithm.
It is not a continuous embedding of it.
So it does not have glitches.
It, it puts the correct answer.
It will be lightweight.
So it will be very efficient, you know, and like, and like the LSTM or transformer
model and crucially it's going to generalize.
So if you develop it only from three digit numbers, maybe there would be
something inside it that will hard code the assumption that they're dealing
with three digit numbers, but even if that's the case, you can take it and
automatically learn a generalized form of it.
If you, if you just start giving it seven digit numbers, very easy to
because it's just modifying in probably a couple of lines of code.
So it is capable of strong generalization.
So here you start seeing how for a problem that's fundamentally a discrete
algorithmic reasoning problem, discrete search is the correct answer.
Deep learning, it's possible.
It works, but with extremely stark limitations, right?
It's very hard to train it.
You need tons of data.
The resulting embedding, because it's not, it's not discrete.
We'll have glitches.
It's not going to work on a personal time.
It's going to be pretty tall.
It's only going to be capable of local generalization, right?
Because again, like the there is a huge difference in representational
flexibility between your very simple, discrete algorithm and some kind
of very complex, high dimensional, continuous embedding of it.
Right.
And, and then there's also the efficiency consideration.
So clearly for if you're dealing and the reverse is also true, right?
Like if you're dealing with a problem that's a fundamental perception problem
where you have data points that fit on a nice and smooth manifold, then
deep learning is actually the right answer.
And if you tried to, to, to train a discrete program to, to, to develop
you know, via, via program synthesis and actual algorithm to classify
and this digits, for instance, everything I just said would be true.
But in reverse, your program would be brittle.
The deep learning model would be robust and so on.
So there are really problems where deep learning is a great idea.
It's a great fit.
Problems where it's a terrible idea.
Like try sorting a list with deep learning model.
Can it be done?
Yes, actually it can.
But with, with all these caveats applying, it is possible to sort a
list of deep learning with some hacky inductive priors and probably
memorizing most of the training data.
And there's, it's not a binary, is it?
You said yourself, there's lots of problems that fall in the middle where
there is a semi continuous structure and some regularity, but it's still a
discrete problem.
And you're saying in that situation, we should still use program search, but
maybe we can use deep learning, maybe something about the shape of the manifold,
even though it's semi continuous, could actually tell us about how to do
that program search more efficiently.
But it seems to me that if there are problems out there, let's say adding
numbers up in GPT three, when I, when I read the stuff that you've been
talking about here, it seems obvious to me.
Why, why are people not picking up on this?
I think most people are not necessarily paying a lot of attention to the nature
of deep learning, why it works, why it doesn't work.
I also think, you know, the people, like they are basically two categories of
people, they are like lay people and they are people with deep expertise.
And the big problem we have here is that the people with a lot of expertise
are going to be a lot of the time driven by motivated thinking, right?
Because they're, you know, like I do, they work in the field of deep learning.
And so they're going to have this vested interest in deep learning being, you
know, potentially more powerful, more general at the nature is, I think if
you want to think clearly, the primary obstacle is motivated thinking.
It's, it's fighting against what you want to be true.
So I tend to have super boring opinions in that sense, because I do my best to
try to forget kind of what I would like the world to be in my, in my best interest
and try to look at it as it really is.
And that will tend to actually diminish the importance of my own work.
So yeah, but you know, I've been doing like deep learning for almost a decade.
Of course, I would, I would want it to be like this incredible world changing
thing that leads to human level intelligence, right?
Of the bats, that would be, that would be awesome.
That would be amazing.
And it would be right in the middle of it.
But that's not, that's not actually what's going on.
You said you tend to be, what was the word, not, not controversial ideas or
something because you try to stick to the way the world is rather than the way
you want the world to be.
But we just had Yannick produce an interesting video about how if you think
that machine learning models essentially attempt to do the same thing, right?
I mean, they're not human beings.
They don't really have wants per se.
They're just modeling reality as it is.
It turns out reality itself really annoys a lot of people.
Like they just don't like reality and they don't like the way the world is.
And they wish it was something different.
And that infects like every mode of their thinking actually.
Yeah, no, absolutely.
Most people, you know, and that's, that's true for me as well.
I'm not saying I'm an exception is what I'm trying to do my best to
resist this trend, but I have no exception.
Most people have opinions, not because they've seen evidence in support of
the opinion, but because it's in their interest for this opinion to be true.
Or they just want it to be true.
I guess one example is, you know, we were mentioning GPT-3 and so on and
the proponents of GPT-3.
I was actually super excited when I initially saw the claim that the
pre-trained language model could perform future generalization.
I thought that's, that's super fascinating.
I was excited.
Like I'm always super excited if I hear about something that's really
challenging my initial kind of mental model of how the world works.
You know, it's like a few years back and there was this claim that a
neutrino was measured and going faster than speed of flight.
I mean, that's, that's, that's exciting, right?
That's like new physics.
You want it to be true.
At least you want to get to the bottom of it.
And then it turned out to be a measurement error, right?
So that's, that's, that's disappointing.
So I think the GPT-3 is kind of, it's kind of the same for me.
I really wanted it to be something, something novel and that would
really challenge what they thought to be true by deep learning models.
And I regret to say that everything I've seen close has actually confirmed
my view that basically deep learning models, they can learn to embed
algorithms given sufficient exposure to data, but they cannot really like
few shots synthesize novel algorithms that represent a pattern they haven't
seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective
on arc, for instance.
And that's, that's kind of sad to me.
I kind of regret it because it means I haven't actually learned anything
from it.
It hasn't expanded my view of the world, which is, which is too bad.
Like, I wish it did.
I wish it did.
So, yeah.
So in the case of GPT-3, what's going on is that the model is being exposed
to, you know, many patterns, you could call them algorithms, for instance,
in many different contexts.
And so it has memorized its patterns.
And now it's able to take these patterns and apply them to new data and measure
the same way that the multiplication algorithm we are talking about.
Because it's an actual algorithm, it can process new digits.
It's not just memorizing the digits in the train data.
It's an actual algorithm in the same way GPT-3 contains tons of small
algorithms like that.
But the model is not synthesizing these algorithms on the fly.
They're in the model already.
Right.
And if you try to apply GPT-3 to something for which a new algorithm would
need to be produced, like an ARC test, for instance, it has just completed anything.
It seems to all build up what you're saying, because there is this strong
generalization versus local generalization.
And then you make a case that in order to do strong generalization, we need maybe
something like program synthesis approach.
So deep learning can't necessarily get us there in most problems.
And you make an interesting case that something like graph isomorphism
search could play a core role in that.
Could you briefly connect all of these terms together of the case you're making
there, because it's super interesting.
So going back to what Tim was saying, it's rarely the case that you have
problems that are fully interpretive or fully discreet.
There are definitely such problems.
In fact, most perception problems are almost entirely interpretive.
And most programs, the kind of program that humans, right, they're largely
like discreet, not interpretive.
But most tasks actually are best solved via a combination of both.
And I actually believe that's true for the way humans think.
You know, there's stack point thinking and type two thinking.
I strongly believe that almost every thought you have and everything you do
with your mind is not one or the other.
It's a combination of both that type one and type two are really unmatched into
each other in everything you think and everything you do.
Like for instance, perception, that looks like something very instant.
So very much the sort of like a continuous interpretive thing.
In fact, there's a lot of reasoning that's embedded into perception.
And the reverse is true, for instance.
If you look at a mathematician, for instance, proving a theorem with their
writing down on the sheet of paper, it's really step by step, discreet reasoning
type thing, but it's very much guided by high level intuition, which is very
much interpolated.
They know where they're going without having to derive the exact sequence
of steps to get there.
So they have this like a high level kind of view, kind of like, you know, if
you're driving, you have to make discrete decisions because you are driving
on a network of frauds, right?
But if you have a bird, a GPS, for instance, you can kind of see the
direction in which you are going, which is interpolated.
If you're talking about direction, you're talking about distances, you're
talking about geometric spaces and everything in the human mind kind of
follows this model of type one and type two thinking at the same time.
If you go back to first principles, intelligence is about abstraction.
So intelligence and intelligence, but the ability to face the future, given
things you've seen in the past.
And the way you do that is your abstraction.
You extract from the past some construct.
Maybe it's a template, maybe it's an algorithm that will actually be effective
in terms of explaining the future.
And that's why it makes it makes it abstract, is that it can handle multiple
instances of some kind of thing, which that thing is an abstraction, right?
And, and it's, if it's abstract enough, it can actually handle instances you've
never seen before, right?
It does generalization power.
And all abstraction is worn from analogy, like abstraction starts when you
make an analogy between two things.
Like you say, hey, time is like a river, if you want to get philosophical
or something, but in general, you can just say this apple, it looks similar to
this other apple.
So there is such a thing as the concept of an apple, for instance.
And the path that is shared between the two things that you're relating to each
other, the subject of the analogy, that that's the part that can be said to be
abstract, that is the part that will help you make sense of the future, like
you encounter a third apple in the future, you know, it's an apple.
Because you don't even need to relate as to the apple should have memorized.
You just need to, you just need to relate to templates, the abstract template
of an apple that you've formed by from exposure to different kinds of apples
in the past.
And if you think about what's, what's an analogy, really, like how do you find
an analogy?
It's a way to compare two things to each other.
And there are only really two ways to compare things.
You can, you can basically ask a similar order in terms of distance.
Like you can say implicitly, there's, you're looking at a space of points.
There's a distance between any two points.
That's, that's the type one, a subject analogy that leads to type one
abstractions, which leads to a type one thinking, right?
So a type one analogy is like you have things you say to a degree, they're
similar to each other.
So you read them by distance.
So, so implicitly, it means you put your things on in a geometric space, right?
And the type one abstraction is going to be a template.
It's like you're going to have clusters of things.
You can take the average and say everything that is within a certain
distance of that template belongs to this category.
That's, that's type one.
It's very much the way deep learning models work.
And, and then you, and then you start adding perception and intuition on top
of that, which is very much the type one thing.
And the other way you can compare two things is the discrete way, right?
Is it, you can say these two things are exactly the same.
They have exactly the same structure.
Or maybe the structure of this thing is a subset of the structure of this bigger thing.
So discrete topology grounded comparisons.
So you have the geometry grounded comparison.
It's all about distances and, and, and templates.
And then you have the topology grounded way of comparing things.
That's all about exact comparison or finding a sub graph isomorphism.
So in, in the first case, your objects are very much points in geometric spaces.
So there are vectors and deep learning is always a great fit for, for this sort of
stuff.
And in the second case, your objects are going to be graphs, right?
And you're, and you're really looking at structure of these graphs and
substructure and so on.
And you're doing always, you're doing exact comparisons.
And in practice, most thinking is actually kind of some, some combination of
these two atoms, right?
Of these two poles, you're, you're very rarely just going to say, yeah,
this approach is exactly this close to my topic of an approach.
So it's an apple.
You're going to have basically layers upon layers of thinking.
And some of them are going to be intuitive.
Some of them are going to be more about, you know, comparing structures and so on.
What, what you're saying is really interesting, because you invoke the
kaleidoscope hypothesis in your paper.
And the idea there is that a tiny bit of information, just like in a kaleidoscope,
could be represented widely across experience space.
So you say that intelligence is literally having some kind of sensitivity
to abstract analogies.
So intelligence is about being able to face the future, unknown future,
given your past experience.
And that's fundamentally requires the future to share some
commonalities with the past.
And that's the idea, the idea of the kaleidoscope hypothesis is that the
universe and our lives are made of lots of repeated atoms of structure.
And in fact, if you look at the source, there are very few things that are,
that are unique, that are kind of like the grains of sand that are the origin
of all the different kinds of moving patterns you can see in the kaleidoscope.
Right.
So the, the kind of like intrinsic structure contained in the universe is very
small, but it is repeated in all kinds of variants.
Right.
And the idea is that if you see two things in the universe that look
similar to each other, or that share some commonalities, a subgraph meaning,
it fundamentally means that they come from the same thing.
And that thing is going to be, is going to be an abstraction.
It's going to be one of these grains of sand in your, in your kaleidoscope,
or grains of glass, actually.
And intelligence is all about reverse engineering the universe to get back
to this source of intrinsic complexity in the universe, to get back to these
kind of obstructions.
I think the heart of this conversation goes back thousands of years, because
what we're talking about right now is a lot of say, Platonism, right?
Which is that there are these ideal abstract structures.
And of course they, they really thought of them as actually existing in some
universe, but you know, even if they, they don't exist in some reality, they
at least exist in concept.
And it strikes at the heart of this duality that's always been a very,
that's been one of the central mystery really of a lot of human thinking,
which is particle versus wave, you know, discrete versus continuous,
abstract versus the real versus the messy.
And, you know, I think you pointed out, you definitely pointed this out in
this call, but I think also in some of your papers that in your view, you
know, let's say the ultimate solution or whatever of creating artificial
intelligence or synthetic intelligence or whatever is a hybrid system that
can do both of these types of reasoning, maybe in kind of multiple layers.
And, you know, I'm kind of curious, where is the state of the art now with
actually implementing hybrid systems?
You know, something like, I don't know, is it capsule networks?
Is it the topological neural networks that we talked about?
Where, where lies the direction of some type of a hybrid system that in a
unified way is capable of doing both of these modes of reasoning, if you will.
Yeah, that's a great question.
So, um, I think this is definitely an active field of research, but I think
the most promising direction right now is going to be discrete search very much.
So a system that is discrete search centric that has a DSA and so on.
And that's one of the, it's basically just problems in this engine.
But it is getting lots of help from deep learning models.
And there are two ways in which you can incorporate this type one, sort of
thinking into a fundamentally type two centric system.
So one way is, so basically you want to, um, apply deep learning to any
sorts of data sets where you have an abundance of data and your data is interpreted.
One example would be being able to easily play models to generate a sort of
like perception DSL that your discrete search process can build upon.
So look at art, art tasks, for instance, a human that is looking at art
tasks, the very first layer through which they're approaching the art task is by
applying basically perception primitives to the grid they're looking at.
They are not actually analyzing the grid in a, in a, in a discrete way, like
cell by cell, object by object.
They're, they're approaching it holistically.
Like what do they see?
And these outputs can the discrete concepts.
And then you can start, you can start applying the script reasoning to them.
So generating the DSL.
And by the way, the reason it's possible is because humans have access to tons
of visual data and, and these different frames share lots of commonalities.
Right.
So it is, it is an interpolative space where deep learning is relevant.
Where intuition and perception are relevant.
And the other way, which is, is, is much more difficult and much, much
more subtle thing is basically being able to provide guidance to the
discrete search process, basically, because even though one single program.
So learning one single program, for instance, for an art task is not a good fit
for deploying model at all, because you only have a handful of examples to learn
from.
And the, the, the program is super discrete.
It's not really easily embeddable in the, in this way, however, here's the thing,
the space of all possible programs, for instance, the space of all possible
art tasks and all possible programs that solve our task is actually very
likely going to be interpolative, at least to some extent.
And so you can imagine a deep learning model that has enough experience with,
with these problems and, and the algorithmic solution that it can, it
can start providing directions to the search, to the discrete search system.
So, um, basically you, you're, you're, you're in a kind of like, yeah.
Yeah.
You have like layers of, um, of learning.
The lowest layer is going to be perceptive.
It's going to be learned across many different tasks and many different
environments.
It's going to be type type one.
Then you're, you're going to have the context specific on the fly problem
solving system that's very much going to be type two.
And the reason it's going to be possible and efficient is because it's
going to be guided by this upper layer, which is going to be type one, which
is also going to be trained from a very, very long experience across many
different problems and tasks.
And that is able to do interpolation between different tasks.
So can I, um, challenge you a little bit maybe because you say maybe, you
know, all of these problems and what humans do is a bit of an interpol, like
an interpolation between the interpolative systems and the discrete systems.
And I see that going for, you know, something like an arc task or, or
if you really write code, but if you really come to, let's say, let's say
the highest levels of human intelligence, which to me seems to be navigating
social situations, which is, is, is ultimately like, is super complex.
And I can imagine something like the graph structure you're referring to
be that being, let's say I come into a room and I see the graphs as, you know,
what kind of social dynamics exist in this room?
You know, this is the father of this person and that person's kind of angry
at me.
And so I need to, you know, do something.
And my question is how often is that really a disk?
Like how often can you really map this in a discrete way to another graph?
Isn't, isn't every situation going to be a little bit different, even in terms
of its graph structure.
And, you know, even, even an arc task, a line is just like a little bit squiggled.
Any program synthesis approach would have a hard time with it, I feel.
Or do you think, or do you think I'm misunderstanding something here?
Like how, how discrete is really discrete?
That's the, that's the purpose of abstraction.
The purpose of abstraction is to erase the irrelevant differences between
different instances of the thing and focus on what, on the commonalities
that matter, right?
So like if the squiggled in your line is not relevant, then the proper
abstraction for a line should abstract it away.
I was going to pick up on that because your main point basically is that
program based abstraction is more powerful than geometric based abstraction
because topology is robust to small perturbations, but it's more than that.
It comes back to these analogies, right?
So we actually have functions and abstractions in our mind that, as you
say, will take away all of the relevant differences, but focus on what's salient
and what's generalizable.
Yeah, exactly.
So in, in the big sense, do you think the type one and type two reasoning are
really different, or is there also a continuum between them?
Like you say we need, we need hybrid systems, but is there something
right? Because they're both, they're both in the brain.
They're both on the same neurons.
Like is there a continuum?
So, right.
So yes and no, I do believe they are, they are very qualitatively different.
These are the two poles of cognition, but they are, they are, you know, most,
most things we do with our mind are a combination of both.
That doesn't mean it's, it lies somewhere in between.
It means it's a direct combination of one pole with the other.
Kind of like what I described with, with the arc solver with three layers,
with two layers of that type one and one layer in the middle, that's type two.
But in very much the same way that you can embed discrete programs in a smooth
manifold, you can also do the reverse.
And when you, meaning you can basically encode an approximation of a, of a
geometric space using discrete constructs.
In fact, if you've done any sort of linear algebra on a computer, that's
exactly what you're doing.
You're actually manipulating ones and zeros.
But somehow, somehow you're able to have vectors of seemingly
customers numbers.
You can complete a distance between two vectors or so.
All of this is an approximation that's actually grounded in discrete programs.
So you can, you can actually kind of merge the two together.
It's not necessarily always a good idea.
In Pascal, I think it's often not a good idea to try to embed an overly
complex or overly discreet program in a, in a customer space.
As, as I was mentioning earlier, the reverse is actually usually way more
tractable.
And by the way, my, I think this is something that came up before in, in our
conversation, but my kind of subjective, totally not backed by any evidence
opinion of how the brain works is that fundamentally it's doing type one on
type two, using a discrete system, because it's actually much easier to do, to
do type one via an approximation of a geometric space that's encoded in a
district structure than it is to do reverse.
Yeah.
And if I can, um, if I just for the benefit of the reader, uh, the listeners, if
I can give some other examples, you know, for example, and, and mixed integer
optimization, it's often the case that you take that problem and instead of
having these discrete values, you project it into a continuous space, do a
continuous optimization.
And then as you get sort of close to a good optimization, you discretize it
back over into the, the discrete variables, you know, to, to kind of, you
know, flesh out the most optimal path within that discrete space.
Or an example too is the gamma function, you know, which is a continuous
generalization of the factorial, right?
And it kind of provides some cool and interesting behavior in between those,
those polls that show up very clearly on the graph is these discrete points.
And this is this bizarre duality between the continuous and the discrete that
we see, like throughout the universe.
And it's kind of one of the strangest things we have to deal with.
Yeah, exactly.
I just wonder what some of the transformers folks must be saying now,
because Max Welling, we had him on and folks have done topological
applications using transformers or using graph neural networks and the, the
alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,
right? It was looking at different types of equivariance in topological space.
Is it a naive thing to say that we could make it continuous or are we on a
hiding to nothing?
Right. So I guess, I guess the question is, is there like one approach that's
going to end up being universal?
And it's, it's like, can you actually scale deep learning to handle arbitrary
discrete programs?
It's kind of, it's kind of the question.
And the answer is no, actually, like by, by construction due, due to the
very nature of, of what deep learning is, it's like parametric continuous,
parametric models, in fact, smooths because they're, they're differentiable.
Sure, it was quite undecent.
That is never actually going to be a good fit for most discrete programs.
So, and, and the reverse is true as well.
I don't think, so you have basically two engines that you can use to learn
programs.
You have quite undecent and you have discrete search.
And I think the reverse is also true that discrete search is not going to be
this universal approach that's going to beat everything.
I truly believe that the AIs of the future will be truly hybrid in the sense
that they will have these two engines inside them.
They will be able to do quite innocent.
They will be able to do discrete search.
And then, and then there is that, is that appropriate?
You said, by the way, in your measure of intelligence paper that there are
three types of priors, right?
Low level sensory motor priors and meta learning priors.
That's the interesting one.
I think that's got intelligences and high level knowledge.
And then we get over to the ARC challenge.
And, and as you said in your presentation last year, the two winning folks on
that Kaggle challenge, one was doing a genetic algorithm over a DSL.
So doing what you're talking about, a kind of program search.
And actually the winner who got about 20% accuracy.
And that was, that was just, yeah, that was just doing a brute force, you
know, selecting combinations of, of operations on this DSL.
So this absolutely fascinates me.
So at the moment that seems like a horrific solution, but clearly no one
could do anything using deep learning.
So, but, but this is what you're advocating for.
So you're saying for these discrete problems, get, get a DSL.
Now, all the stuff you're talking about, presumably they haven't done yet.
You're saying, well, software engineering, the beauty of software
engineering is being able to modularize things into building blocks.
And in fact, I love citing this thing actually from Patrice Simhoff.
But he said, the reason why software engineering is so good is if I ask
you, how long will it take you to build the game of Tetris, you will say not
long at all.
And if you look at the number of state spaces in Tetris, it's, it's huge.
But the reason you'll be confident to build it in a couple of weeks is because
you know that you can modularize it into, into blocks.
You can't say the same for deep learning, right?
But they don't appear to have done that on the ARC challenge yet.
Yeah.
So the, the solutions we've seen on the ARC so far have been incredibly,
incredibly primitive.
And so it's, it's actually quite interesting that you can get to 20%
of this is very, very primitive solutions.
I think you can, even with today's technology, you can go much further.
Like the, what I was describing before about learning a DSL that is
perceptive and then guiding discrete program search.
Yeah.
Intuition about program space.
This is already something that you can try today.
So there's one approach that I was very excited about.
And that I thought was, was very cool.
And I really liked it's, it's called Dreamcoder by Dr.
Kevin Ellis and, and folks.
So check it out if you, if you have incidents, it's very good.
I think that they're trying to apply to ARC now, but it's generally like,
is this kind of like hybrid deep learning program synthesis engine?
And I think that's really to me, that that is the sort of direction
that is the most promising to that.
So you have a paper that's fairly long on, it's called on the measure of
intelligence and you make the case that intelligence is something like
the efficiency with which we transform prior information and experience
into task solutions, as, as you have said before.
And in that same paper, the ARC challenge is presented.
So, you know, a naive reader like me assumes there is some connection
between, you know, what you say about intelligence and solving this ARC challenge.
So my, my question is, if tomorrow, you know, a new team comes and gives you
a solution, you evaluate it, it gets whatever, 95% correct.
It solves the ARC challenge.
Is it immediately intelligent?
Or what would you ask of that system for, for you to say, yes, that's
intelligent or, or it's, it's intelligent is, is high or something like this.
So you, you, you would be able to make that, that conclusion if and only if ARC
was a, was a perfect benchmark, but it's not, it's actually very much
flawed. So if you solve ARC, are you, are you intelligent?
Well, no, because ARC is potentially flawed.
That's, that's the thing.
So the thing you need to, to really understand about ARC is that it's
not kind of the end state of this intelligence benchmark.
It is very much a work in progress and there will be new iterations, especially
as we learn more about the flaws.
And by the way, so last year we ran a Kaggle challenge on ARC and we learned
a ton, not necessarily a ton about program synthesis approaches, although
there, there were some cool stuff with serial automata and so on, but mostly
we learned about the flaws of ARC.
So there will be future additions and so on.
So I will tell you this.
If you solve the specific test set of ARC as it exists today, you're not
necessarily intelligent because it is not perfect because it has its flaws.
But if more generally speaking, you give me a system that is such that any
new ARC task I throw at it, like I can make some new ones tomorrow, for
instance, I give them to your system.
If it's always solving them, I will say, yeah, it's looking like you've got a
system that's, that's got, you know, pretty close to human level fluid
intelligence.
This is one of the things that, um, look, and I like the paper a lot.
I think, I think it serves as a really good, um, you know, foundation for us
to think differently about how to build intelligence.
But, but I have some, some issues with it too as well.
And one of them is this sort of necessity that it requires kind of white
box analysis of things in order to figure out whether or not they're intelligent.
Because for example, suppose time travel is actually possible and you know,
somebody like a 100 years from now looks back on your ARC thing and writes an
algorithm that, that solves all, all them in there because it actually knows
about them already and then ships it back into the past and we enter it into
the competition and no matter what new ARC thing you throw at it, it sort of does
well and you say, well, yeah, you know, this thing's like kind of intelligent,
but, but we'd be wrong because in the sense in the paper, it's actually just
encoded, you know, prior knowledge from the future.
So we have to, we always have to kind of be able to look into the box, right?
In order to evaluate intelligence in the way that you define in the paper.
And so my question is one, isn't that a bit of a undesirable feature?
And two, do you have any hopes for a more black box measure of intelligence?
So basically the fundamental issue is that if intelligence is this conversion
ratio, then computing it requires knowing where you start from and you
don't really have a way around it.
So the thing to keep in mind is that the under measure of intelligence stuff is
not so much meant to provide like a sort of like golden measure tape to measure
anyone's intelligence or anything's intelligence.
It is more meant as a sort of cognitive device to help you think about
what the actual challenges are, to help you kind of reframe AI, because I
think that they have been pretty deep and longstanding conceptual
misunderstandings, so that is really being, that's been holding the feedback.
So it's very much meant as a cognitive device.
If you, if you take a step back and you ask why are we even trying to define
intelligence and measure intelligence in the first place, why is it useful at all?
I think it's useful to the extent that it is actionable, right?
A good definition and a good measure should be actionable.
So meaning it should help you think, it should help you find solutions and it
should help you make progress.
In particular, a good definition is a definition that we highlight the key
challenges and help you think about it.
And I think that's what the paper does.
And a good measure is a measure that gives you an actionable feedback signal
towards building the right kind of system, right in the sense that it will
be capable of doing more.
And so that part, the feedback signal is what is what ARP is trying to achieve.
And the way it's trying to control for priors and experience is by assuming
a fixed set of priors and you're going to say, you know, every test
that you're going to have is priors, this is the core knowledge priors.
And then it controls for experience by only giving you a very small number
of input, put example.
And also by making sure the tasks are sufficiently novel and surprising
that you're unlikely to have seen a very similar instance before.
So now, of course, it's super flawed, so it is not 100% true, of course,
but this is kind of like the, the planning ideal that we are trying to get to.
So that, for the record, that's a fascinating point to me is that you
view this more as a cognitive device to help guide us to produce better,
better intelligent agents.
It is not an input.
It's not like ARC is like the measure of intelligence.
And now all we need to do is sort of ARC.
This is not at all the point.
It's like it's one.
Oh, darn, because I was doing pretty well on some of the examples.
I was hoping that would mean I was intelligent.
But another interesting point, because Keith and I were looking at the paper
again yesterday, because it's been, and I haven't properly studied it
since last year, but we were starting to talk about an alien that comes in
from outer space and, you know, we don't know the the priors and the experience.
And then I was thinking in a way, it might be a kind of lower bound on
intelligence, right?
Because, you know, if I play chess and if I beat someone with a higher
elo than me, then it only really tells me that I'm better, you know,
as good as that person that I just beat.
And similarly, this measure of intelligence, it only gives you a reading
in the situation when you know what the conversion was.
So if they are not converting anything, then you don't know.
And another interesting byproduct of this is the more experienced
you get, the less intelligent you get.
So I would, I would push back against that last claim that the
measure of intelligence as I define it is dependent on how much experience
you have, because the amount of initial experience you have
does not actually change at the conversion ratio.
If you, if you measure it via the right task, so you might need it.
So if you have a fixed set of tasks, then yes, it does affect it.
But if you, if you're able to renew your set of tasks and come up with
styles that are orthogonal to the experience that you have, then it's not,
it's not the actual effect, the definition.
So, but yeah, you're, you're definitely right that if you take a pure black
box approach and all you're looking at, like the only thing you can really
measure is the behavior of a system.
And unless you know how that behavior is achieved, you can't really tell
immediately how much intelligence was, was involved in producing this behavior.
If you look at an insect, they're capable of super complex behavior.
Are they, are they crazy intelligent?
Well, actually, you know, probably not.
And the way you can really tell is by putting the systems out of their
comfort zone, getting them to face novel situations and see how they adapt.
And that's the measure of intelligence.
It's adaptability, the ability to, to deal with novel and unknown situations.
But in order to give your system a novel and unknown situation, you need to
have this white box understanding of what, what it already knows about.
Yeah.
And that, that's, that's not really something you can, you can work on.
So can I ask about the, the generalization difficulty?
Cause I, I sort of had some difficulty intuitively with some of, let's say,
it's limiting cases.
So for example, you know, the algorithmic complexity is highest.
Let's just suppose we're dealing with problems, tasks where we have whatever
sets of integers mapped to zero, one values, you know, the, the algorithmic
complexity will be greatest when that's just a random mapping, like I just
assigned zero and one randomly to every single integer.
And if I go to look at that generalization difficulty, it's going to be super
high because the, the length of the program for any set is basically going to
need to be, you'd have to encode the entire set as a hash table, right?
So how does like this measure account for or help us avoid problems where we're
confusing generalization difficulty with just increasing random, you know,
randomness?
Well, I mean, increasing randomness is a part of generalization difficulty,
right?
Generalization is really the ability to deal with the stuff you don't know
about, the stuff you don't expect, the stuff you haven't seen before.
And randomness is, is a part of it, but you're right that if you just add
randomness to a system, you're increasing the generalization difficulty, but
you're not increasing it in a very interesting way, right?
Because you're increasing it in a way that's kind of orthogonal to an
integration system's ability to deal with it, right?
The best you can do is modify the system to be more robust to, to very much
randomness, but that's, that's not super interesting.
What's really interesting is to test the system's sensitivity to subtle
analogies, is to make the system face novel and unexpected situations that
are actually derived from the past, but in interesting ways, right?
Not just random ways.
You've run this Kaggle challenge on, on ARC and, you know, we, we know
from systems such as AlphaGo and so on that bootstrapping, intelligent,
like bootstrapping AI systems can be very valuable, like playing them
against each other and so on.
And, um, also we know that something like markets can be very efficient
and, and valuable.
And I imagine a system where you'd have agents creating ARC tasks and
other agents solving ARC tasks and they're going some kind of money around
and so on.
And, and this could be kind of a powerful engine for research teams to
research anything like this.
And, you know, given that you have, I don't know how much, but you do
have the backing of, of Google with a bit of capital in hand.
Do you, could you imagine, could you imagine there, you know, being a push
for this kind of thing?
Or is it as of now an intellectual curiosity?
Yeah.
Um, yeah.
So I, I, I don't have, you know, that much backing you from Google around
this, this kind of project.
Um, but yeah, so it would be super interesting to have this kind of two
part system where one part is generating the task and one part is learning
to, to solve them.
And you could get them to do some kind of curriculum optimization, like the
task generator network would not just generating, you know, now it would,
it would not just be trying to generate tasks that look like our tasks, it
would be trying to generate tasks that correspond to level of generalization,
difficulty and complexity that is right below the limits of the student
system that's trying to solve them, kind of like, you know, the way a
teacher would provide exercises that are solvable, but challenging.
They shouldn't be, they shouldn't be easy.
They shouldn't be impossible.
They should be solvable or challenging because that's how you get the most growth.
So it's actually a system that's described at the very end of the paper on
the measure of contentions.
And I think one thing I point out in the paper is kind of like the, the
pitfall you should avoid falling into is that this system is Sierra, right?
And the complexity you're going to see in your task, it needs to come from
somewhere, right?
It's like conservation of complexity.
So the system, this two-part system needs to have a source of intrinsic
complexity, needs to be grounded in the real world.
And one way we can achieve that grounding, and I've been thinking about it, is
I think we should, you know, like, our tasks are, as they are today, they're
made by me, and this is not a good setup because it's going to be biased.
It's going to be very bottlenecked as well.
I think we should start crowdsourcing our task.
There should definitely be, you know, a filtering system so that we make sure
that we're only keeping our tasks done.
Interesting that are not too easy, that are not difficult, and that are only
grounded in core knowledge bias.
But if we have like this stream of a novel artist that contain intrinsic
complexity and novel information, because they come from the real world, they
come from human brains, that have experienced the real world, and you use
that as a way to ground your task generator, then you're starting to get
a very interesting three-part system, right?
So I would love to actually get that started, to actually produce a V2 of
arc as soon as possible, that we include, you know, 10x more tasks, that will
be crowdsourced, and maybe something that will take the form of a continuous
challenge, where you have an API where you can draw a new arc task, and every
time you draw a task, it's actually a different one because you have so many of
them.
Gameify it, that'll make a fun game on a mobile app.
There are actually a few people who've created, because Arc is open source
and they're totally free licensed, there are a few people who have created
mobile apps where users sort of arc tasks, and apparently it's popular.
So there's also the other angle you mentioned in the paper, which was,
which is pretty fascinating, you're talking about it almost right now, which
is that, okay, let's start thinking about how to map arc performance to
psychometric, you know, classic kind of psychometric tests.
Are there any efforts that you're aware of underway right now to do that?
Are you involved in?
Yes.
So there are two groups.
Any ETAs?
Yeah, ETAs, I'm not sure.
So we did a workshop at AAAI the other day, and there were two presentations
about efforts that teams of people.
So there are people who do neuropsychology and they're using Arc in very
interesting ways.
So there's a group at NYU and there's a group at MIT.
And yeah, so they're using Arc for neuropsychology experiments.
And it's super cool.
Amazing.
I want to switch over a little bit, because of course, you know, other than
the measurement of intelligence, you are also famous for a small library you
wrote once in a while called Keras.
And I wish I wrote it and then that was that.
No, yeah, it's been very much an ongoing project for the past six years.
It was because I remember, you know, the days of TensorFlow one and
Theano and things like this.
And Keras was just, I think, so helpful to a lot of people because it just
easy-fied all of this, you know, graph construction, whatnot, and so on.
It just made it accessible to so many people.
And now with the development of, you know, things like PyTorch and TensorFlow
two, it almost seems like Keras is it has been kind of absorbed by TensorFlow
two, right?
There is TF dot Keras.
And now I think the newest APIs are even sort of vanishing that a little bit.
Do you do you see Keras going away?
Do you see it changing?
Where do you see it?
Where do you see Keras going?
Yeah, so going away, definitely not.
I mean, we have, we have more users than ever before.
And we are still growing very nicely, both inside Google, like one more teams
at Google are moving away from TensorFlow one and adopting Keras and outside
Google as well.
It's, it's, it's a big market out there and there's definitely room for
multiple frameworks.
And evolving absolutely, I mean, Keras is, is constantly evolving, but
evolving with continuity.
Like if you look at Keras from 2016 or 2015, you look at Keras now, you
recognize, is it the same thing?
And it's the same API.
And yet it's actually a very different and much, much bigger set of features
and things you can do with it.
So evolving, definitely.
And they are so several, so you, I think you asked, you know, about, yeah,
like Keras is getting kind of merged into TensorFlow.
Does it mean it's, it's like failing away?
So definitely not.
So merging with TensorFlow was a good idea because it starts enabling a
spectrum of workflows from the very high level, like cyclically unlike to the
very low level, none by like, and everything in between.
In the early days, because Keras had to interact with multiple back-ends via
backend interface, it means you had this kind of like a barrier where as long
as you use the Keras APIs, everything was super simple.
It was cyclically unlike.
So very easy, very proactive, very fast, but if you wanted more customization,
at some point, you would hit that back-end barrier.
And, and you had to reverse to TensorFlow base or piano base workflow that was
low level, but when, well, you couldn't really leverage Keras effectively by
removing the back-end thing and just saying the flow together in one spectrum,
then you get really this progressive disclosure of complexity when you can
start out with the very high level thing, but then if you need to customize your
training steps, you have an API for that.
And you, you can just mix and match seamlessly the low level TensorFlow
stuff with the high level Keras that, and that way you can achieve any can work
with Keras and TensorFlow at the level of abstraction that you want.
Very, very easy high level or very, very low level full flexibility.
It's up to you.
I'm going to point out the temptation here to analogize connecting type one with
type two reason.
Yeah, why not?
Well, I was just about to do that.
At least Francois has great form for this because not only does he talk about
having powerful and useful interfaces and abstractions in deep learning.
He's, he's been playing this game in, you know, in, in the library world for
quite some time, but I wanted to touch on this quickly.
We had a couple of people in our community asking you about Keras, actually,
and Robert Lange and Ivan Finnell said that apparently the Arno has returned
with Jack's and XLA underneath.
And he wants to know, are there any plans to add it as a Keras backend?
And Robert Lange also says, you know, just Jack's on its own.
Would you add that as a backend?
We've also had a couple of questions about PyTorch as well.
Is there anything on the roadmap for that?
Okay.
So let's talk about Jack's.
I think Jack's is an awesome project and the developers have redone a very,
very interesting and very good job with it.
And lots of people, I like Jack's actually.
So that said, adoption is not super high.
I think Google is probably the company where it's the most adopted,
where you will find the most users.
And even then it's like a tiny, tiny, tiny fraction of total machine
usage at Google.
But I think as a project, it's, it's a beautiful project.
It's elegant.
It's powerful.
It's great.
So would I, would I like to add Jack's backend to Keras or PyTorch backend to Keras?
So I want to say we've really moved away from this like interface backend
kind of model.
So precisely for the, for the reason I was describing, because you want to
achieve this spectrum of workflows, with that, I think this cliff where you go,
you, you fall from the high level down to the low level.
We don't want the cliffs.
We don't, because cliffs create silos of users where you have the
high level users.
You want a gradient.
Yeah.
You want the gradient spectrum.
Exactly.
So that said, I think it would be super cool to have a sort of like
re-implementation of the Keras API on top of Jack's, that we also achieve this
grading and that will still follow the Keras API spec.
It would still be the same thing, but on top of Jack's.
That said, so I would, I would love to see something like this.
This is also a very low priority for us because we have the, the actual current
Keras, which, which we need to work on, which has lots of users.
So we don't really have time to do this, but in theory, would it be cool?
Yeah, sure.
I would, I would love to see something like this.
So if I had tons of free time, I would, I would probably build it, but in
practice, I don't.
Fantastic.
Well, we've got another question from Giovanni, actually.
He says, what does Francois think of Dr.
Kenneth Stanley's book on the myth of the objective?
Are you familiar with Kenneth Stanley's work about the tyranny of
objectives and open-endedness?
So I'm very familiar with the name.
I'm not really familiar with the book.
Oh, OK, well, sorry, not to worry, but it's Kenneth's been a huge
inspiration for me and he talks a lot about objectives leading to deception.
So sometimes following an objective monotonically sends you in the wrong
direction and his solution to that is either quality diversity or more recently
open-endedness, which is that if you have an infinitude of objectives, in a
sense, the system has no objective and you can, you know, also with diversity,
preservation, you can overcome deceptive search spaces.
But yes, you might have heard of the poet algorithm, which he, which he was
involved in.
Yeah, absolutely.
No, I'm aware.
And so when it comes to your, your description of the problems,
objectives, I completely agree that one, one thing I mentioned in the paper,
it's like the, the shortcut rule, which is that if you try to achieve one
thing, one objective, you're going to, you're going to achieve it.
But the thing is you're going to take every shortcut along the way for things
that we are not actually incorporated in your objective.
And this leads to systems that are not actually doing what you wanted them to
do.
Like for instance, we built chess playing systems because we hoped that a system
that could play chess would have to be able to feature reasoning, a book
learning creativity and so on.
Turns out it just plays chess.
That's what it does.
The, the same is true.
We say at challenges and Kaggle, the winning systems, they just optimize
for the leaderboard ranking and they achieve it, but they achieve it at the
expense of everything else that you might care about the system.
Like, is the code base readable?
No, is it computationally efficient?
No, it's, it's actually terrible.
You, you could never put it in production.
Is it explainable?
No, and so on.
Yeah.
So it's like, if you, if you optimize for something, you get it, but you
take shortcuts.
Yeah, exactly.
And that's very much what Kenneth says as well.
I love what you said about shortcuts.
You said in your New York's presentation that if you optimize for a specific
metric, then you'll take shortcuts on every other dimension, not captured by
your metric.
And you said in a machine learning context, it's similar to overfitting, right?
Cause on, on task specific skills, you actually lose generalization if you get
good at a particular task.
So it's completely orthogonal to, to what you want.
I know you're very well known for your skepticism of, of the intelligence
explosion and what, what I love about your conception of intelligence is that
you think of it as, as a system or as a process.
You say that intelligence is embodied, right?
So you have a brain in a body acting in an environment.
And, um, in, in, in that context, it makes sense that you would think that
there are environmental kind of rate limiting steps to any kind of super
intelligence, right?
But, um, I spoke to someone the other day who is of the other persuasion,
shall we say, and this person was, um, saying, well, what if you had a super,
super smart bunch of scientists?
I know you said in, in your rebuttal that if you look at the IQ of a scientist
who is Richard Feynman, for example, it's the same IQ as a mediocre scientist.
Turns out that IQ only helps up to about 125 and then it stops helping you.
But these people would say, Oh, well, you know, what if, what if every single
scientist was an Einstein and intelligence is just making better decisions?
They would consistently make better decisions and science would accelerate.
A chimp doesn't understand how good a human is.
So how would we understand what a super intelligent person would do?
You know, they'd invent nanotech.
They'd upload themselves into the matrix.
They'd do all of this stuff and somehow they would miraculously overcome.
Do you know what I mean?
How would you respond to that?
Yeah.
If every scientist was super intelligent in human terms, that would in
fact accelerate science, but it would not really like accelerate science
in a linear fashion and very much not in an exponential fashion.
So, uh, I guess the main conceptual differences I have with these folks is
that they tend to credit everything humans can do to you, the human brain.
And, and they have this vision of intelligence as, you know, a brain in a
jaw kind of thing.
And if you tweak the brain, it gets more intelligent and intelligence is directly
expressed as power.
Uh, if you're more intelligent, if you have a higher IQ, you can do more things.
You can solve more problems and so on.
And in particular, you can build a better brain.
And, and by the way, there's not really any practical evidence.
I view intelligence here more as this holistic thing that, okay, you have the
brain, but actually the brain is in a body, which gives it access to a certain set
of actions it can do and set up a perception primitives.
And this body is an environment which gives it access to a set of experiences,
a set of problems it can solve.
And to a very large extent, you know, the brain is just, it's not so much a
problem-solving algorithm, like a problem-solving ascending, as it is a
big spawn and you put it in an environment to absorb experiences from that
environment.
And, um, one thing that's super important to understand if you, if you really
think deeply about intelligence is that most of our expressed intelligence does
not come from here.
It is externalized intelligence.
So externalized intelligence can be, can be many things.
Um, if I look up, uh, something online, that's externalized intelligence.
Google is part of my brain.
If I write a Python script to test some idea, that's externalized intelligence.
My laptop, uh, is part of my cognition.
Uh, and so on, but it's actually, it goes much further than that.
Most of our cognition is crystallized, the crystallized
output of someone, someone else's thinking.
And the process through which we get access to all these accumulated
outputs of people's thinking is civilization, right?
And like 99% of the things you think are the behaviors you, you act
behaviors you execute, you did not invent them.
You did not solve the underlying problem yourself.
You, you're just copying, uh, a solution.
You've seen like we're in the middle of the pandemic, uh, you're probably
washing your hands after you went outside and that's a very smart behavior.
Uh, but did you invent it?
Did you come up with that?
No, actually other people came up with that.
You did not also come up with the infrastructure that enables you to do
it in the first place.
And so, and this is true, you know, for even the, the most, uh, intimate of
your thoughts, you're thinking with words that you did not invent.
You're thinking with concepts, uh, that you did not invent or that you did
not derive from your own experience.
They, they really come from other people from this accumulation of past generations.
And if you want to, um, enhance the expressed intelligence, uh, of people,
then this is actually the system you need to, to tweak and improve not the
human brain, but civilization, right?
In a way that seems like a contradiction because you're talking about the
externalization of knowledge, not intelligence.
So by your own definition, doesn't that, isn't that the opposite of intelligence?
That, that's a great point.
So I'm, I'm relating express intelligence.
So I was specifically saying express intelligence as opposed to fluid
intelligence and what express intelligence, uh, means in this context is
something very different from what we, we talk about in the measure of intelligence.
It means intelligence and behavior.
All right.
And in particular, I think the, the ability to solve problems that you
encounter as an individual, typically when you solve a problem as an individual,
you're actually using a solution you've found somewhere else.
It is, there are not that many problems that as an individual, you solve from
scratch in your own lifetime, but here's the thing is that if you're able to
actually solve something novel yourself, you have the ability to write about it.
You have the ability to communicate it.
And then the next generation can, can benefit from it.
So let me just pose a kind of a counter argument to this though.
Suppose, um, you know, suppose you're reading a novel about, uh, I don't know,
a kind of planet of, of the apes or something, which was, which was a planet
that had a life form similar to ours, but with a significantly lower, lower IQ, right?
And, and you know, a human being shows up there one day and these things
start writing about this.
Hey, this weird, you know, alien just showed up here and we captured it.
You know, we ran some tests on it and we figured out it's really intelligent.
You know, it's much more intelligent than, than any of us are.
And, and we're worried what's going to happen when, you know, 100 of them show
up instead of just this initial explore.
And some other, other of these guys were like, ah, don't worry about it.
You know, they've, they've got two legs and two arms like us and most of what
they are is kind of outside of their brain.
So, you know, I'm not, I'm not really worried about it.
We would be reading that with trepidation, right?
Because we know that when this more intelligent species with more fluid
intelligence, more externalized intelligence, better technology, all this
kind of stuff shows up, those guys are going to get wiped out.
And it's actually happened like many times throughout human history, not that
humans were more fluid, intelligent, showed up and killed off, you know, other
people, but humans that had more externalized intelligence or more, you
know, represented intelligence and technology certainly showed up and
dominated.
Absolutely.
You're, you're, you're saying it yourself that when it has happened in
history, it was not fundamentally about one people having smarter brains, but
one people having higher technology.
But that, that is not something that is attributable to, to intelligence itself.
Right.
There's a connection there.
If you did have a group of species or whatever that was much more intelligent,
they will have advanced technologically much faster and further in any given
amount of time, all else being equal, right?
It depends on many factors.
And that's kind of my point is that is your brain a factor?
Yes, absolutely it is, but there are other factors like we are just talking
about the development of technology.
So in that case, the critical factor was not the brain, but this super
structure in particular communication and environmental constraints around it.
The direction in which civilization develops is a direct function of the
specific challenges it encounters that come from its environment.
It comes from its surrounding enemies and so on.
And technology core development advances the fastest when you have a civilization
that are dealing with very harsh challenges, but that are not quite
harsh enough to work them out because that's what forces them to develop as
fast as it can survive.
So this is actually a very good example where the critical factor was the
super structure that guided the development of civilization was not
actually the brain, but of course, yeah, if everyone is smaller, then civilization
will advance faster.
But my point is that there are many factors and that by tweaking one factor,
the brain, if the brain stops being the bottleneck, then immediately some other
factor will be the bottleneck.
There are civilizations that have not actually advanced very much at all
because they simply did not face any challenges.
And did they have worse brains?
No, actually, they had exactly the same brain.
But somehow the outcome was different because something else, then the brain
turned out to be the bottleneck, like a lack of environmental change.
I'm fascinated by scale and bottlenecks in systems, actually, because I work
in a large corporation and when you have role fragmentation and lots of
different businesses and lots of different organizational structures,
some people might decide to structure themselves based on data domain or based
on organization or based on something else.
And you can think of it topologically.
And I think human society is very similar to this.
And I'm not sure whether, you know, evolution would lead itself to one
particular topology, but the environmental structures and the ways that
we organize ourselves can create incredible bottlenecks.
And that seems to be where the real interesting stuff goes on, rather
than the individuals.
And I think you would agree with that, Francois.
Yeah, absolutely.
If you take two companies and in one company, the average IQ is like 15
points higher, but it has a terrible organizational structure and terrible
incentives and the promo process is super broken or something.
That company is actually going to perform worse than the more progressive
innovation encouraging company that has a very nice organizational structure
and where people are actually a more mediocre, maybe they have an average
15 points less in IQ, but they're actually going to do a better job
because they have the better superstructure.
Yeah, it's fascinating that the problem is in most corporations, you
can't actually design the information architecture to be more efficient
because everything is so decentralized and fractionated.
You can only do it in pockets.
And if you try and fix something in one part of the organization, everyone else
will say, well, my requirements are different.
I'm not going to wait for you.
I'm going to do it my own way.
And it's actually a really, really difficult thing to do.
Well, to sum up the whole like intelligence explosion thing, the point
is really that it's a system you have to look at holistically to get it
holistically and just by tweaking one factor, which is the intelligence
of an individual human brain, then what this means is this factor starts
being the bottleneck, but that means some other factor in the system
because there's an infinite factor that will become the bottleneck.
And by just focusing on one factor, you're not going to actually
lift all the votes.
Yeah, and I actually agree with you.
However, you know, I do want to say, I think we just don't know.
I think both sides of the intelligence, quote unquote, explosion really can't
say for certain that it will or will not pose, you know, a mortal threat to humanity.
Like, I think we have to accept that it's at least a risk factor, right?
And we have to be very careful about, you know, in the future when we
start embodying, you know, if we find general intelligence, we need to be cautious.
If we come up with something that looks like general intelligence, there is
absolutely some risk potential around it.
However, you know, I've never seen anything coming anywhere close to that.
In fact, the systems that we have today, they fit your almost no intelligence
whatsoever.
So I think it's a bit early to start by thinking.
And even if we get into that conversation, I think Francois would say
that intelligence must be specialized, right?
Because of the no free lunch theorem.
If you define intelligence as your ability to solve problems, then yeah, it's
going to be specific to a scope of problems, a kind of problems.
And like, yeah, what the no free lunch theorem is saying is basically, if you
want to learn something from data, you have to make assumptions about it, which
is why, you know, a convent, for instance, is a great fit for image data.
It's not really a great fit for natural language processing.
And because it makes different assumptions about the structure of the data.
It doesn't give me a lot of comfort, though, because I'm fairly certain that
whatever the first AGI that gets created, it's going to be highly specialized
for killing other people, because it's going to be a military, you know,
secret project, probably that finds it.
You know, it's, I don't know.
But what I know is that right now we don't have anything coming close to AGI.
It's probably going to be actually a system that just displays you ads.
Like, if, like, if, you know, if you, if we see where the most money is right now,
the first AGI is probably just going to like write, not only display, but write
the perfect ad for you on the fly.
You know, it knows what you ate.
And, you know, I know you're joking, but I actually think on the most
useful, I think that's highly unlikely because of the short code of the
story, because of the short code rule.
I don't think a general intelligence is going to be created by military
is not going to be created by a system that's trying to show you ads because
these are specific goals.
And so if you try to optimize those specific goals, you're going to end up
with a very specialized system in order to build a general intelligence.
You need to be optimizing for generality itself.
So it's going to come from, if it comes from the applied, there is going to
come from the academic side where you have researchers who are actually
optimizing for generality itself.
Segerity as a goal.
Or if it's come from the applied side, it's going to come from people who
have problems where they have to deal with extreme novelty, uncertainty, and
unpredictability.
So it's not going to be ads, it's not going to be the military.
I don't know where this is going to be.
One of the things that interested me about Kenneth Stanley was that he, you
know, he says, the reason we can't monotonically optimize on objectives is
because of deception, which means sometimes you need to get a lot worse
before you get better.
His original conception was quality diversity, which basically means if you
optimize for novelty, that's something that you can optimize on monotonically.
And also if you look at evolution, where there is a cacophony of problems
and solutions, you know, divergently being generated, then as an information
accumulator, you can optimize on that monotonically.
And your conception of intelligence is generality.
And that also appears to be a monotonic increase throughout advancing
levels of intelligence.
So I think that's quite interesting.
Anyway, Francois Chouelet, this has been my dream come true to have you on the
show. Thank you so much.
It really means a lot to us.
And yeah, I appreciate it.
Thank you.
Thanks so much for having me on the podcast.
It's, it's really my pleasure.
This was, this was super fun.
Thanks a lot.
And thank you for Keras, by the way.
I'm glad it's useful.
We're going to jump straight into the post-show analysis.
Okay.
Well, I'm going to mention you did really well, Tim, that, that trickle
sweat that this was running down your face the whole time.
Yeah.
Not very noticeable.
So I think you can, you can relax.
You did, that was fun.
That was really, that was fun.
I think it went pretty well.
Yeah.
Yeah.
It was, it was a dream come true.
I was actually, I was very pleasantly kind of interested in how he, he
framed, you know, the measure of intelligence paper, like, look, it's
not really about the measure per se.
It's just that this is, this is a cognitive framework, a cognitive tool
for thinking about where to go and a guidepost for building more generalizable
or more general intelligences, say, like, that I totally, totally agree to.
And it's quite, you know, quite a fascinating goal, which is like, here's
a framework to help us think more in the direction we need to be thinking.
Yeah.
And it's so surprising that like the arc challenge is at like 20% solved only
because, you know, he self admits that it's, it's flawed, right?
And because he, like, he makes the tasks and, you know, there's only finitely
many and, and, you know, you kind of, you see the kind of tasks he makes, you
know, in the public set, you would think that not someone will come up with
an intelligent thing, but someone will come up with like a smart set of
shortcuts to like solve that sucker, right?
But it's still at 20%.
I don't know whether that's due to just, you know, not too many people
investigating it, or whether it's really actually a hard problem.
And if it is a problem, you know.
Well, it's fascinating too, because if he, if he achieves what he wanted, which
was getting it more outsourced, right?
Like getting all the intelligent people around the world contributing to
arc problems and refining them over time, I think actually that community
project would help the core knowledge people in that line of research and
figuring out, okay, what, what is a catalog of all the core knowledge, right?
It's like in back in school, we used to call these prime thoughts because we
would, we would play these brain teasers all the time.
And we realized that there were patterns, right?
Like, well, this brain teaser requires the concept of coloring, like with a
red, black tree where you add an additional variable that kind of lets
you solve the problem.
And if we could really have a nice catalog of here's all the core knowledge.
Here's all the like problem solving techniques.
I think that would be really powerful.
I mean, well, we kind of had that.
So this woman, Elizabeth Spelke, she came up with about six core knowledge
systems, right?
And that, and the arc challenge uses four of them.
So objectness and intuitive physics, one, agent-ness, two, elementary,
geometry, and topology, three, numbers, counting, quantitative comparisons.
So the two that weren't in there are places and social partners.
Now the thing is, I think we may discover new ones.
Well, yeah, maybe we will, but I'm surprised that we did as well as 20%.
Because if you think about it, imagine if you just guessed the classification
on ImageNet when you've got a thousand classes, 20% would be amazing,
wouldn't it?
And we've got a similar amount of diversity of tasks on arc, right?
And what's interesting as well is that all of those different tasks that have
been created by Francois, they all tie back to just four priors, right?
Which means, I don't know whether it's uniformly distributed, but 20% seems
really good for just guessing ops on a DSM.
Yeah, there's, there's two things.
So first, I would have thought that if someone, if someone came up with
something that solves more than 5%, it's going to be like immediately at 95%.
Like just because they've sort of cracked the problem.
And then, you know, there might be a few outliers, but you know, if I would
guess that's kind of a task that if you hit the correct solution, it's going
to be like, boom, you're, you're there.
And that's not, which is surprising.
And the other thing is I, I don't, I don't feel it's surprising that there's
so few priors.
What I do think is that the space of these priors is still way too large.
Like, so if you just think about something like object, because in, in
these arc tasks, there are, I feel so many more priors than just the core knowledge
things, because so one of them is like, you have the, you have like this thing.
And then you have this thing.
And the solution is like, it goes, right?
It goes, it like bounces.
But this is like, yeah, but, but like the fact that we recognize like this is a
wall or something, but there is no, there's no, no prior to says like a wall
needs to be straight.
The wall could be like any, you know, any old, any shape at all.
And the fact that this is much more core knowledge, right?
Like in, you know, we, we build stuff out of straight walls.
And I think, I think I agree with you, which is, I think, I think what you're
getting at, correct me if I'm wrong, but it's that the way in which the core
knowledge is kind of specified right now is, is vague, right?
There's a vagueness to it.
And I think if we actually start to try and codify that more and some type
of a mathematical language, Tim, I think it's going to expand like the scope of
that we're going to end up with more core knowledge concepts really than, than
just six, we'll need to make them finer grained.
And I'm really excited, you know, to see that develop because this has been for
me a long wonder, right?
Which is what are the, in, in a rigorously defined way, what are these core
concepts, these core bits of knowledge that make human cognition so powerful?
Yeah.
And there's also, because Yannick made the point about Britanness, right, even
in topological space, you still have Britanness, but, but that the solution
was to create powerful abstractions, right?
But how would that work with the priors?
Because if you think about it, you can recombine many of the priors to come up
with powerful abstractions.
And you might find that it doesn't actually filter down to, to that many.
But the question is how many things are there?
Remember when we spoke to Walid Saber and he was talking about.
He's got them somewhere in a PowerPoint deck.
You just wouldn't give them to us.
But, you know, part of, part of why, why I agree with Yannick that their finer
grained concepts are more important.
I think probably stems from a lot of the computer science education that I had
where, where when we were devising algorithms to do one thing or another, you
get these little hints that kind of like clever bits of core knowledge that was
used to solve this problem.
Like when you study quicksort and it's like, you know what, like I'm just
going to randomly choose an element.
Well, random selection is kind of a bit of core knowledge.
And then I'm just going to partition by that and then repeat, you know, or
things like, I don't know how to balance this tree the way it is.
But if I color stuff, like add in red, black nodes, I can now overlay a
computation that, you know, so there's all these little bits that, you know,
that's what's fascinating about computer programming.
Is it, is it really strikes at the heart of this cognition and this core
knowledge and how to, and you have to do it rigorously, right?
You can't just vaguely go, oh, you know, I just kind of sort it and merge them.
You got to define like what that means.
And it's fascinating.
Dynamic programming.
I'm always, I'm always a bit amazed by people who have just kind of
sort of learned programming because it's, it's almost like a different world
in that they'll, they'll, they'll do, it's like, oh, okay, I need to solve this
problem.
Can I, can I copy paste this code here?
And it works like 20% of the time, but not fully.
Yeah.
But then on the other side of the coin to that.
So when I was working in, you know, quantitative trading, right?
We had these, these massive globally integrated automated trading systems.
And I mean, some of the bizarre, I don't want to call them hacks, but some
of the bizarre sort of piecewise linear equations slash hacks, whatever,
did actually work in reality.
You know, you sit there and you look at them and go, when I first went in
there, it's fresh out of academia and I started seeing things like, oh, this
is crap.
Like I'm going to figure out some continuous equation that, you know, fits
this piecewise linear thing and it's going to do better.
Nope.
Like it didn't do better.
I couldn't find any continuous thing to do better.
It's like, you know, options pay off, right?
Is this, this piecewise linear thing.
And, and you're like, oh, that's, well, there should be some continuous
like thing in there.
Like all these weird, you know, piecewise discrete, like kind of hybrid
things between continuous and discrete work.
And, and, and that's weird.
It was weird to me and still weird to me.
Interesting.
But I've got to say, so my main three take homes from Sholay today.
I really love Sholay.
So one, intelligence is generalization.
I think that's super powerful.
Two, his idea that deep learning is really good for value centric abstraction.
And because of the manifold hypothesis, lots of natural data has some kind
of manifold, which you can interpolate on, but lots of discrete problems
do not have that, right?
And my mind was thinking, well, does that mean that we can just use, because
it's because of SGD, you can't even learn the manifold, even if it did exist.
But he's saying that it doesn't exist for discrete problems.
The manifold might be there or it might only be there in parts.
So that was interesting.
And then the third thing that fascinated me about Sholay is, is he talks
about these systems and bottlenecks in systems.
And we shouldn't be thinking about individual brains.
We should be thinking about the externalization of knowledge.
Yeah.
And, and the way he, the way he described this, um, what, what he thinks
like a hybrid system should look like, which is sort of you have a perception
layer and then a, a discrete search layer.
And then on top of that kind of another fuzzy layer that guides the search
that can be deep learning again.
And I think we're, we're like halfway there on the top with the top very much
looks like Alpha zero, right?
Which is kind of a discrete search that is guided by a neural networks.
And the bottom layer we have too, because that's just our, you know,
regular neural networks.
I think we, we have a big trouble in how to connect the two in a, in a single
unified way such that we can learn them, right?
Because the best we can do right now is, is right.
We can, we can plug it a pre-trained network onto Alpha zero or something like
this, but we can't really, we don't really have it figured out yet how to
connect the, all the stuff.
A good example of that is the, the neural Turing machines, like how it's so
hard to, to optimize them, right?
And I think, and not only do we need these kind of three components that,
that nicely integrate and are optimal, we have to be able to modularize and
componentize and connect multiple instances of those things together and
some, you know, weird topological network to really achieve, like kind of the
capsule network kind of vision where each of the capsules is maybe one of
these units and then they're a part of, it's like a fractal, you know,
kind of these fractal layers of those pieces.
I don't know whether I was misunderstanding you before, Janney, but with
the Alpha zero thing, my conception is that has been quite hard coded.
So you're, you're searching through, let's say, a bunch of deep learning models
and the way you search is quite opinionated.
What you're always talking about is have a very basic DSL and in that
topological space, you, you just search and, and you start to modularize and
you start to create functions and abstractions and you have, from a
software engineering point of view, you start to build a library of functions
that have been written in code that do certain things, right?
And that's, that's different, isn't it?
To Alpha zero.
Well, the Alpha zero is made, yeah, specifically to, to search over actions
in, in some kind of RL space.
Yeah, I mean, what, what he describes is certainly much more abstract in that
you, you search over applications of the DSL and the DSL itself is not, is
like a perceptive DSL that in itself is described by these lower level neural
networks, but I mean, in, in S, I just, that just came to my mind when he
described the system, I'm like, oh, the top, the top part looks very much like,
you know, Alpha zero, because that's essentially neural network guided
search is something we, we already, already do though.
Yeah, I, I think, I'm not sure.
I think just that the reality is even a bit more fuzzy, but because what you
do as a, as a human, there's also some part of hierarchical system to it in
that you can, you can do this, but you can do it hierarchically, right?
You can, you can be like, okay, I'm gonna, I have to solve, you know, I have
this high layer search, and then each of the search things goes through maybe a
fuzzy thing, but then you, you again, search at to solve the sub problem.
And there is also, and you can do it at will too, by the way, like you can, you
can scan an image and you get this type one that sort of finds a bunch of
objects, and then you do this type two thinking where you start reason about
those, and in your mind, you can kind of zoom in on one.
Let me like zoom in on that tree.
And now like, now I've got the bark, you know, pieces of the bark is objects
and bugs and reason about.
So you have this ability to transcend the process and tune it and move it around.
Yeah, this, this self, like the, that's the whole consciousness aspect, right?
That's even like, apart from intelligence, you have the ability to, to
introspect the whole thing.
And that probably is a big part of intelligence.
I mean, I guess you could have intelligence without consciousness, but, you
know, there is an argument to be made that the fact that you can introspect
your own processes contributes in big part to the furthering of intelligence.
Yeah, I would separate consciousness and intelligence, but it, the thing that hit
me the most on his New York's presentation was when he said intelligence
is literally sensitivity to abstract analogies.
So we were talking about the kaleidoscope.
The main thing here with intelligence is that there is so much repetition in the
universe, right?
But it's repetition in this funny way where it's sort of fuzzy repetition.
Like, sure, the solar system kind of resembles galaxies, kind of resembles,
you know, but, but then there are these little weird differences, these
asymmetries and, you know, like the universe is a fascinating place.
And, and it's, right?
That's not what, when you say you have to make analogies, which is, I can, I
can absolutely see, you know, this and me, I think my question was formulated a
bit dumb where I said, you know, if the line is squiggly, what I more
meant is that, you know, in, in that case, it's not a line.
It's a squiggly line.
And the same with the social situations, you know, that is like, okay, that
person over there kind of doesn't like me.
But then in the next social situation, it's kind of a person that doesn't
like you and has a gun or, or something like this.
I almost feel like, or a group of people that you don't consider as a single
single, sure, they are similar in some way, but it's never the exact same thing.
So this reasoning by analogy does work, but you always do your little
modifications on top specific to the situation.
And I'm sure there, there's a place in his framework for this, but it's, it's
just, again, it's, it's like a lot more complex than, yeah.
I think that's what he called, I think that's abstraction, at least that, you
know, that was prior to the day, my, my concept of abstraction was similar to
that, which it's removing the insignificant details.
So you're able, you're, you're able to take whatever, you know, some,
you know, object thing situation doesn't matter and kind of strip away all the
stuff that doesn't matter for whatever your purpose is, that's abstraction.
And, you know, I think one of the weird things is that, and this is kind of the
unreasonable effectiveness of mathematics, right?
Is that abstracting actually produces things that are useful, you know, that
abstraction, I think the fact that abstraction helps with generalization is
a very not well understood kind of mystery in a sense.
Like, why should abstraction help generalize?
But it does.
Like in the real world, that's what happens.
Though the yet abstraction in though abstraction has to be somehow specific
to what, what you want to do.
Like, like you write an apple is an apple only if, you know, you're looking
for food or non food.
But when it comes to sphere, if you want to shoot it out of, out of a potato
can, exactly.
But when it comes to, you know, separating fruit by ripeness, then it's not
an apple is an apple.
Then all of a sudden this apple has much more in common with this orange, right?
So the, even the way how you abstract, it's not like, it's not like we can just,
you know, plug in our ResNet 50 and then boom, we get an embedding vector.
And that's our abstraction.
But the, how you abstract is also incredibly specific to what you want to do.
Yeah.
And that's what, and I agree with Saba that this is an empirical question.
Right.
You know, like he's kind of like these concepts or whatever.
It's an empirical question.
And Chalice, I think the art project, if it ever becomes this crowdsourced thing
is going to give us lots of data to start thinking about this empirically.
And it's going to be really fascinating.
I mean, this needs to be on, like this is a, this is a prime blockchain project
because you can, you can probably, like you can probably even
zero, you can zero knowledge prove that you can solve a given set of arc problems.
Right.
You can probably create zero knowledge.
So you wouldn't even have to show your solution.
And if they're, you know, people would put up arc problems and they, you know,
if you want to try them, you will have to put up some money.
And if you can solve it, you know, the creator of the challenge
gives you some money or something like this, like this, this
could be fascinating.
Maybe you could do, you know, homomorphic like arc, right?
Or like you don't even, you somehow, like you're saying, you can just prove
you can solve the problem without ever you having seen the problem,
but just an encryption of it.
Yeah.
Yeah.
Normally homomorphic encryption comes after blockchain in the same sentence.
And we make a nifty, we make a nifty of it.
Yeah.
What else can we get in there, Tim?
So we got blockchain, homomorphic encryption.
What else?
Well, can we throw in there?
Bitcoin, can't we just say people should have to pay through Bitcoin?
If, if they, if somebody wins a challenge on arc, we'll get our own token of it.
Arc, arc coin.
Oh, God, hold on.
I got to get that domain right now.
I want to know, by the way, so the whole point of the arc, diversity of tasks
for developer aware generalization, which means the developer could not
have conceived of the task.
But if all of the tasks are representing four human priors, then how
is that developer aware of generalization?
Cause the developer would be aware of all of those priors.
He of the priors, right?
But not, not of the task, right?
That's the, the control is the control, like that's what he said.
You have to know the start of where your, your white box analyzing from.
And the start is not clean slate, but the start here is these four priors.
So it's, it's kind of the diff between you give the developer those four priors.
What can the developer come up with just from that, right?
Yeah, because I think, I think there's a lot of information leakage there.
And you implicitly said the same thing, because you said, once you solve it,
you know, once you solve some of them, you've solved all of them.
Okay.
Arccoin.com is available for the, it's, but it's, it's a premium domain.
So it's 300 bucks.
Should we get it?
Because it has coin in it?
I guess.
We need to figure out, figure out something cooler, like no Arccoin.
Okay.
I don't care enough to grab it.
Right.
Anyway, we should draw this to a close.
Ladies and gentlemen, but yeah, thank you very much for listening.
We publish.
Yep.
Thank you.
So it's been, it's been emotional.
We've recently reached 10k subscribers actually.
So yeah, thank you very much.
We're still going to continue the show now that we've had Shaleo.
Oh, yeah, I thought this was the end.
I thought we were going to cap it with Shaleo.
I mean, to be honest, we might as well just stop now.
Anyway, see you folks.
Thanks, Bob.
Bye.
I really hope you've enjoyed the episode today.
Remember to like, comment and subscribe.
We love reading your comments and we'll see you back next week.

Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington
and machine learning researcher, probably best known as the author of the master algorithm,
popular science introduction to machine learning.
I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.
The two that I found most interesting and are closest to my own research are Neurosymbolic
AI and symmetry-based learning.
Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.
Pedro is a professor at the University of Washington and give us a quick introduction
to yourself, to your experience here at NeurIPS so far, and what's top of mind for you?
I'm a machine learning researcher, I've worked in most of the major areas.
I've also written a popular science book on machine learning called the master algorithm.
I'm having a lot of fun here at NeurIPS listening to various talks like David Chalmers'
On Consciousness and Geoff Hinton's Nonsleep and looking forward to the rest of it.
Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first thing I wanted
to talk about just because it's top of mind is this whole galactica situation.
So first of all, I was speaking with Ian the other day and I think it's a little bit unfair
that Meta really bear the brunt of this. OpenAI have just released this new chat GPT
bot which suffers from similar failure modes and it just kind of feels that they're not
getting anywhere near as much stick as Meta is. Well, I think I agree with you.
I think the brouhaha about galactica is way overblown. That system is really largely harmless.
It's just another large language model that's designed for actually something that to me as
a scientist is very interesting. I would love to have a system like that to help me out with
certain things and I think it's a step in the right direction and I think the brouhaha however
is an instance of people jumping the gun on a lot of these AI things in a way that to me is very
excessive. Having said that in a way they set themselves up for it in a way that they need
and have, they kind of over claimed what it did and you know like the problem with this
with these LLMs is that they generate a lot of stuff that looks good but can be completely wrong
and in a way there's no worse place to do that than in writing scientific articles.
So when they came out with it they should have been more careful about how they frame it.
I think they took concerns sort of like this competition and one upping each other on who
comes up with the frilliest demo and that kind of backfired. So they shouldn't have had to
withdraw it. I think that's all pathetic and hopefully they've learned the lesson that next
time they will do it slightly differently. Yes, okay, okay. So Gary Marcus has been very loud
about this on Twitter so he's really pushing the point about misinformation and the thing is as
well I don't want to characterize the ethics folks as having monolithic views because they
don't have monolithic views and I also think that a lot of the ethics guidelines for large
language models are very reasonable. Like I interviewed the CEO of Cohere the other week,
Aidan Gomez. I went through their terms and conditions and policies all very reasonable.
The only sticking point for me is the misinformation one. I think the kind of the moral valence of it
is in its use right and especially with misrepresentation. I don't like this paternalism
telling me what's good for me. I've just lost out on using a really cool tool basically.
I completely agree with you. I would take it even further. I do not want other people deciding for
me what is misinformation and what is therefore allowed to be said because it's misinformation
or not. For a couple of reasons. One is that these people who claim to be big critics of
misinformation, a lot of them are misinformers themselves. And the bottom line is that you
always have your ideology that informs what you think is true and false. And I don't want any
every one of us in a democracy should be deciding for themselves what is true and what is false and
what is valid and what isn't. And I have no fear of attempts to misinform me as long as I have a
multiplicity of sources. The biggest misinformation danger is when you have only one monolithic
source of truth, whatever it is, which is unfortunately what a lot of these anti-misinformation
people I think consciously or unconsciously want. Give me 10 things, nine of which are
misinformation. I can do the job of figuring out which one I think is valid. Give me only one of
those things and chances are 9 in 10 that it is misinformation and then I have no chance to overcome
it. So this whole attack on things because they're misinformation. I mean, I understand the impulse
that like why have all this falsehood flying around? But the way to overcome that falsehood
is not by censoring it. You should know this. We should be having to refight all of this over
again in the context of social media and large language models and so on.
So you said something really interesting which is that this notion of a pure truth or a monolithic
truth and there's this concept of epistemic subjectivity or things observe a relative,
even complex phenomena like intelligence, no one understands what it is. You can't reduce it to one
particular thing. People have different views on it. So this notion that there is a pure monolithic
truth of the world I think is horrifying. Well, I would put it slightly differently.
So first of all, there's a question, is there one reality or not? Is there truth or is there my
truth and your truth? I understand the impulse to talk about my truth and your truth, but I
think as a, so what is really true? We don't know, but as a, I think the most useful, including
socially useful working hypothesis is that there is a single reality and the single truth, but it's
extremely complex. So no single one of us can get at it. So what we need is many different people
coming at it from different angles, but with the premise that we need to try to make these things
consistent. So just saying, oh, we have different truths and there's no reality, that is actually
very counterproductive because it gives everybody a path to just believe whatever wacky thing they
want. And then the consequences of that when you have to make the real decisions are very bad.
At the same time, I agree with you. If I think that I have access to that truth and everybody
just needs to, you know, cut out to it, that is very dangerous. So I think we need to entertain
these two ideas that there is a truth, but it's very complex and no one has a monopoly on it.
And the key is, you know, like objective truth is what different observers can agree on. And now
we can figure out what it is that we agree on. And that way we make progress in understanding
reality. And we also tend to make more of the right decisions because we're closer to the truth.
Okay. Do you see it as, I mean, I think the reality thing is interesting, but do you just
see it cynically as gatekeeping, as in having a clerical class controlling?
Oh, absolutely. No, absolutely. I mean, that's precisely the danger that I was referring to,
is that if you, let me put it this way, if ever there is, and this is a commonly mooted proposal,
right, and not even proposed, like, are we going to have a truth commission of people who decide
what is true on whatever Twitter or something, right? That is a really alarming thing because
there is no commission that can do that. What they're going to do is they're going to impose
their version of reality and everybody else, which unfortunately is what a lot of these people
want to do. They convince that they have the truth and they want to impose it on the rest of us.
And that is really alarming. We know historically what happens when people succeed in doing that.
Yes. Yes. But I suppose my point with the gatekeeping is it almost gets you to the actual
truth of the matter is irrelevant. It's actually about power. But what's your take on, I don't
know whether you think this is putting it too strongly, but this being a form of industrial
kind of gaslighting, kind of in an Orwellian sense, trying to shape people's reality through
language, culture, and interactions on the internet.
I think a lot of it is deliberate. Some of it is, I mean, I'm an optimist about human nature
at the end of the day, maybe with justification, maybe without. I think, so there's this post-modern
view that it's all about power. And it's certainly partly about power. But I think a lot of the
people doing this, unfortunately, or maybe fortunately, they're not seeking power for
its own sake. They have a set of beliefs that they think is right. And then the means,
they unjustify the means. That's the problem. So that gatekeeping and that gaslighting happen
not because, not for their own sake, they happen for the sake of a cause. And now there's two
problems with this, is that these days the causes on behalf of which this is being done,
in my view, are largely wrong. But whether they're right or wrong, the problem is that this is just
noxious in its own right. And also, then a lot of sort of like, again, personal desire for power
and promotion and prevailing over others, then of course, hitches the ride onto this.
Yeah, interesting. I mean, we'll get into consequentialism in a minute, because I think
there's quite an interesting journey we can go there. But I wanted to cite Francois Chollet.
I'm a big fan of him. He just tweeted saying, I'm not too concerned with whether what I read
is right or wrong. I can figure that part out myself. I'm interested in things that are useful,
thought provoking, novel. Sometimes the most creative thinkers have a bias towards wrongness,
but they're still worth reading. Would you agree with that?
Yes, I largely agree with that. So as I was saying before, I can
tell for myself where I can do that exercise of freaking out what is right and wrong. The most
important thing that I want is to not miss out on things that I don't want to miss out on. Like,
you know, the known unknowns and the unknown unknowns, the biggest killer is the unknown
unknowns. So if anybody trying to learn or understand something, therefore, person or
organization or society, right, if all they do is move the unknown unknowns to known unknowns,
they've already gone an enormous distance, right? And so I appreciate people who I disagree with,
first of all, because that's how you sharpen ideas. But also because they may just bring
things to my attention that if we were all conforming to the more majority view,
would not come to our attention. And then those more often than not are the ones that kill you.
Yeah, that's so interesting. I mean, there's a real analogy here, even this might be tenuous,
but between symbolism and connectionism, or, you know, Rich Sutton said we shouldn't be hand
crafting our AI systems, we should kind of let them emerge. And it's a similar thing with our
moral framework, but you're kind of saying that it should be emergent from low level
complexity and diversity and interestingness. And there is another school of thought which
is that we should be top down and we already have a representation. I actually think it should be,
it needs to be a combination. We need to have both. This is one of those debates that in some
sense puzzles me, because to me the obvious answer is that we need both. And then if you
read the master argument, this is what I do, I look at the different paradigms of machine learning,
and I don't come out in favor of any of them because actually I think we need
ideas from all of them, and then we need to combine them into something coherent. And if you
look at psychology, like you bring this bottom up and top down processing, and if it only did one
of them, either one, it wouldn't work. And I think as we try to build a larger intelligence,
it's the same thing. We definitely need the bottom up part. And by volume, the bottom up part
is going to be bigger. So if you could only have one, that would probably be the choice.
But the top down part is also very important. If you go all the way back, the top down part
probably started this bottom up and got synthesized and improved, but now we need that loop. The loop
is actually very important. Well, that's interesting. So in your book, I guess I want to sketch out
different types of AI architecture. So you get universalists to this kind of deep mind idea that
a very simple underlying algorithm could produce everything. And then you get hybrid folks on the
other side of the spectrum. And then there's an integrated approach in the middle. Where would
you place yourself in that continuum? I would place myself very much in the frame of mind. Well,
let me put it this way. I don't know, but nobody does. If somebody tells you that they know how
we're going to get to intelligence, you should be suspicious right away. But what do I think
is the most promising approach and the one that ideally would be the best one if we can
pull it off? It's there being a single algorithm. So at that level, I very much sympathize with
what is effectively deep minds agenda. Now, where I part with a lot of these people is that I don't
think the algorithm that we need is as simple as many of those people think it is. And I don't think
it exists. It is probably the case that the algorithm that we really need at the end of the
day doesn't even look that much like any of the things that we have now. So I think hopefully
there is such an algorithm, but we're still far from it. Interesting. I mean, they would cite
the example of evolution as being a very simple underlying algorithm, although
Ken Stanley would say that people misunderstand evolution. So I agree with them at that level.
In fact, in the master algorithm, I have a chapter where I go over the objections and the
reasons to believe that there is a master algorithm. The majority of the people even in the field
are skeptical of that notion, even though I would claim that effectively that's what they're
pursuing. People like Rich Sutton and Jeff Hitton, I asked a bunch of people before I wrote the book
and they do believe in this idea of having a master algorithm. A lot of people believe that,
but intuitively, a lot of people believe that no, there is no such thing. And I understand
that intuition, but I think it's... I don't think it's a well-founded intuition. Let me put it that
way. But in a sense, we know there is such a thing because look at cellular automata, look at what
we've already done with deep learning. I think the context is, is there such a thing that will
produce what we want? Well, yes. So to take your example or deep mind's example of evolution,
right? I do... So in the book, I mentioned empirical evidence that there is a master
algorithm and exhibit one is evolution. If you think of evolution as an algorithm, which by the
way, is a very old idea. I think it was George Bull that said, God does not create animals and
plants. He creates the algorithm by which animals and plants come about. He didn't use the word
algorithm, but that's essentially what he said. This, I think, is right on. And another example
is your brain. If the algorithm doesn't have to be something as simple as backprop and you're a
materialist like most of the scientists are, your brain... If the master algorithm is an
algorithm that can learn anything you do, then your brain is that algorithm. But then there's
another one which is even more fundamental, right? But I think from the point of view of this debate
is very illuminating, which is the laws of physics. Why stop at evolution, right? The laws of physics
are the master algorithm, right? Evolution is very complicated. In fact, what I think about,
you know, evolution in AI currently is that evolution in reality is much more complex than
we give it credit for, which is why a lot of our current generic algorithms don't work that well.
But the laws of physics at this level are much simpler. And if you think about it,
from the laws of physics comes evolution and comes all the intelligence that we have.
It's very intriguing why that happens and why the laws are such that that happens. But
even just the laws of physics are already a master algorithm. Now, what you could say,
and many people immediately say is like, oh, but if you start from there, you know,
you'll never get anywhere, right? But then you can say like, evolution is the laws of physics
sped up in a certain direction. And then our reinforcement learning is like evolution. People
have pointed out the same way, except it's faster. And in a way, what we're trying to do now in
machine learning is the same thing yet again, except only even faster. But what are the consequences
of? I mean, let's say it is actually a very high resolution algorithm. So it's something that appears
to be completely unintelligible to in respect of the output phenomena. Is that is that even a good
place to be? Because, you know, just like with cellular automata, there's no real paradigmatic
relationship between the underlying rules and the emergent phenomena, right? So is that really
even something we want? No, I think there is. So we don't know. But I think people and this is very
common among connections is to say this stuff is also complex that we can't possibly have a
handle on it. We just have to let it happen. And I think that is not giving enough credit to our
human brains, right? We are incredibly good at making sense of things that in the beginning,
though, I mean, over and over and over in the history of science and technology, you start out
with things that you don't understand very well at all. But then over time, we kind of change our
representation of the world to make those things actually be intelligible to us. And we should
not at priori assume that that's not going to be the case here. So, for example, cellular automata,
amazing things come out of whatever the game of life that seem completely disconnected from
the rules, but they aren't, right? And, you know, there's, you know, there's, there's various depths
at which you could go, you could go into this. There will probably at the end of the day be
some large element of this that we can't figure out very well, but we can figure out enough
that we have a handle on it. So this singularity notion that at some point AI is just completely
beyond our understanding. I tend not to buy it. I don't think it will be completely beyond our
understanding. But it's an analog back to our Twitter discussion, like, because we can only
understand it through, it's like having views on a mountain range, you know, the view looks
different depending on where you're standing. And it's the same thing with the emergent
phenomena in a cellular automata. No, very good. And, you know, the classic example,
this is the is the blind man and the elephants, right? And that's that's actually the metaphor
that is in the book, as I say, you know, the different tribes are like different blind men,
right? But, but it put precisely so I is one of the blind man, I can see part of the elephant,
but it'd be who's going to also talk to you, who see another part of the elephant. And then
each of us understands a little bit more of the elephant than we would if we were on our own.
But most importantly, we collectively, which is what really matters, actually understand maybe
not the elephant completely, but much more of the elephant than either one, either any of us would
alone, right? And it's certainly a lot better than just giving up and say, like, oh, we're never
going to understand this strange thing that's in front of us. Interesting. But that's a great
argument to what you were saying before. So it's beyond our cognitive horizon. Therefore,
we need to have diversity of aspect. There's a yes, there's a question of whether it's beyond
the cognitive ability of a single human. Yeah. And then there's the question of whether it's
beyond the cognitive ability of an entire society of humans. And obviously, there'll be things that
are beyond the cognitive ability of a single human, but not beyond the cognitive ability of a society.
Also, these days, we have computers. So our cognitive power is augmented by our machines.
So we can understand things or bring things to the point where we understand them to a degree
today that we couldn't a hundred years ago. Right. Now, that is a fascinating point. So
it's beyond our cognitive horizon individually, but it might not be beyond the cognitive horizon
of loads and loads of humans on the internet, you know, the wisdom of crowds. But we don't,
I mean, how do we know that the crowd understands? Well, we know, well, that's the, the, in some
sense, the beauty of this, right, is that we never, what is the crowd really understanding,
right? And again, once the crowd is augmented by machines like machine learning algorithms,
right, we can ask what do we as a society equipped with all of our, you know, large language
models and so on and so forth, what do we really understand right now at some level, you can't
answer that question individually because you are just an individual. But right, there's a couple
of very important things that we shouldn't forget. One is that you could, one thing you can do and
that I do do select, do I now actually even just individually understand things better than I did
before when it was just me looking at it? And the answer to that is almost invariably yes, right?
So, so there is a big gain to be heard there. And the second one is that you, ultimately,
you tell by the consequences, right? And like, for example, take a deep network, right? And you
may not know how it works. But if it's doing medical diagnosis, you can tell whether it,
you know, gets the diagnosis right more often than, than it did before, or more often than
another model. So we as a society may not, you know, we individuals may not very understand
very well what we as a society understand, but we can see the consequences and at some level,
that's the point. Yeah, so on that, I mean, that sounds like a bit of an appeal to behaviorism.
And I'm, we're going to talk about that in respect of charmers as well. But it also brings us back
to, you know, we were talking about empiricism versus rationalism and nativism and all of these
topics. Would you place yourself in that camp of being a nativist and a rationalist or completely
the other way? No, absolutely not. Again, this is one of, one of the points that I,
you know, go back to is there are the empiricists and there are the rationalists. And you could
see naively machine learning as being the triumph of the empiricists, but it actually is not. There
are very fundamental reasons why it's not. And I really do think, and this is not just think,
there's this thing called the no free lunch steer, right? And if you take those things seriously,
the solution has to be a combination of empiricism and rationalism. I don't think you decide
alone has or even can have the whole answer. So very much we need both of those. And if you're a
pure empiricist or a pure rationalist, I'm really suspicious of you. Wonderful. Coming back to what
Francoise said in his quote, he said, you know, producing things that are thought provoking,
novel and all the rest of it. And I was speaking to some alignment folks yesterday and we'll pivot
to that in a minute. But the big thing for me after doing an episode on sales, the Chinese room
is, you know, where does intentionality come from? And Chomsky talks about agency, for example,
we do things that are appropriate to the situation but not caused by them. So from my
perspective, all these generative models, all these large language models and so on,
the creativity, the real spark of genius still comes from us, right? We've just kind of like,
you know, the boring bit of actually doing the task is now delegated to the algorithm.
I would disagree with that. I think you are, I mean, your position is very reasonable. And
actually, I would say probably the most common. But I think when you do that, you are giving us
too much credit and the large language models too little. We tend to have this notion that
creativity is something magical. In fact, I remember for many years, so quick parenthesis,
in the previous life, I was a musician. So, you know, I in some sense know about and a lot of my
job was composing songs, right? And I was always at the same time, I was already studying AI,
and I couldn't help but connect the two, right? And think about like, what would the AI look
that was able to compose music, right? And talking to late people who are not musicians,
they think that composing songs is some kind of magic thing that comes from, you know, whatever,
the great beyond. And it's not. It's a very human enterprise. And it can very well be automated.
It's actually now, you know, people, I used to say to people like, people always say like, oh,
creativity will be the last thing that we automate because we humans can do it and there's no
machines could and be like, no, it's going to be the opposite. You automate creativity long before
many other things and we're there now, right? In just the last, so I think when you, let me put it
this way, your prompt to the LLM, let's say, is like the grain of sand to the oyster, right?
You should not give yourself credit for having made the pearl because it put the grain of sand in
there. That's a brilliant analogy, right? So it is still the LL, we need to, we want, we can critique
how creative it is or not. And there's a lot to be said there and a lot of progress to be made.
But we need to give it credit for what it does, right? It is well or not so well, right? Maybe
it's more of an illusion that we're giving credit for and whatnot. But that text or that image or
whatever, they were created by the AI. And in many ways, the thing that was created by the AI is no
worse than what would have been created by an artist if I gave them the prompt.
Okay. Well, on that, I agree with you. I mean, Melanie Mitchell had this wonderful
anecdote from the Googleplex when she was with Douglas Hofstadter. And he was talking at the
time about, you know, how he would be devastated if an AI could produce a Chopin piece, you know,
which was indistinguishable from one which he actually created. And of course, that did happen.
But then we get into this discussion of where does it start, right? Where does it start?
Computers only do what we tell them to do, right? They've been trained. And actually,
I was speaking to Seth about this the other day, that, you know, all of the abstractions,
all of the things that the computers and the models do, they are crystallized snapshots of
things that humans have previously done. And we've written the computer code. So where does the
creativity start? Well, but we, by that standard, we humans also only do what we're told to do.
We do what we're told to do by our genes. Our genes do what they're told to do by evolution,
which does what it's told to do by the laws of physics, right? And now, again, this gets back
to this notion that there's nothing magical about creativity. Creativity really is, to a large
extent, cutting and pasting stuff and satisfying consistency constraints between them. And I'm
not just saying this in the abstract, like long before the modern era, there's this guy called
David Cope, you know, a composer and professor of music at UC Santa Cruz, who created these programs
that exactly they would write, they can write, this was pre machine learning, right? It was
list code that what it did was basically have rules about how music should be. And then it
takes snippets and combines them, right? You could say it's just parroting those bits, but the truth
is at the end of the day, and you can choose, you say, like, give me something in the style of
Mozart. And it creates something by that looks indistinguishable from what Mozart did. But all
it's doing is this kind of recombination of pieces. So we humans, we have too much respect for
appreciation for our own intelligence. That's also what we're doing.
Yeah, I think I agree with you. I mean, first of all, intelligence is a receding horizon,
and there's the McCorduck effect. And I agree with all of that. But yeah, I think it's a similar
thing to how we anthropomorphize large language models. And even, you know, it's tempting to
say large language models are slightly conscious. And we'll talk about that in a minute. But maybe
we also anthropomorphize our own agency, right? We have like a little bubble around ourselves,
and we kind of delude ourselves that we exist as an individual unit with agency,
disconnected from the rest of the world. Well, precisely the problem with how
we largely take AI today, this has always been the case, by the way,
is that we have a new resistible notion to anthropomorphize anything that behaves even
remotely like us. We're the only intelligent things that we know. So if something starts
behaving intelligently, then we project onto it all of these other human characteristics.
Same with consciousness, same with creativity. We don't know anything else that's creative
besides us. So once a machine starts behaving creatively, we cannot help but project a lot
of things onto it. It's just reasoning by analogy. Right? So it's a kind of analogical. So like,
you're like me in this respect. So you probably are in this other spec. Now the good news is that
we always start out with this kind of very good reason by analogy. But after a while,
we actually start to build them all of the real things. So AI for the public at large right now
is very new. But gradually we'll come to a point where we zero in on what AI really is,
rather than just the shallow analogies that we initially used to try to understand.
Okay, well, I'll try it from a slightly different angle. So, you know, we were just saying,
so makes the argument that it's a biological property and that's where intentionality and
consciousness comes from. And it's a requisite. But we'll leave that for the time being. Let's
go the, you know, the Fodor and the Gary Marcus and the Chomsky route. And they would argue that
creativity is basically this notion of or even analogy making for extension, is this notion
of being able to select from, you know, a set which has an infinite cardinality. And the only
and as you know, neural networks can't represent infinite sets because they're finite state
automators. Therefore, they make the move we need to have this compositionality. What do you say to
Well, there's a lot to unpack there. I think we definitely need compositionality. Right.
If somebody asked me, make a list of how there's some things that actually
essential for intelligence, compositionality would be one of them. Right. And this, of course,
is the thing that people like, you know, Chomsky and Gary and we're not really care about, right.
Having said that, I think, first of all, there is no such thing as an infinite set.
Right. Like infinite set is an infinity is a useful but extremely
dangerous and confusing mathematical tool. Right. In the real world, there is no such
thing as an infinite anything and there never will be. So I would just raise this at, well,
yes, creativity and almost anything we can do in AI is selecting from a very large set,
not infinite, but very large. Right. And now, but now we don't just select like
one full element at a time. We compose it out of pieces. And that's actually where the
intelligence comes in. Interesting. I don't want to go too far down the digital physics
route, but we did just have Yoshua Bakon. And I mean, just to reclamify on that,
would you place yourself in that camp that the universe is digital made of information?
Valid question. I certainly think the universe is finite. Yeah. I think
whether I mean to, you know, like Seth Lloyd says, the universe, the universe is a computer.
Right. And I think that is true or false, depending on what you take the word computer to mean.
Right. The universe is not little. So if you, if you say that the universe is digital or is
a computer as kind of like an analogy that lets us understand it better, I'm all for that.
I don't think the universe is little, you know, maybe here's a way to put this. The
universe is a computation. Like I don't know what the computer is or if there is one.
Now, the universe is digital in the sense that deep down at the most basic level,
the universe is made of discrete things. Okay. Is this like the it from bit, the John Wheeler
type hypothesis? Yes. I mean, if you read that paper, it is, I mean, John Wheeler was a brilliant
person. Again, a very, you know, to get back to François Chalice's, you know, tweet, he was very
good at coming up with his provocative notions, right? Yeah. And the it from the thing, of course,
is like newly unvoked today. And I do so at that level, I do agree that looking at the universe
is being made of information is very useful. And in particular, if you want a grand unified
master algorithm, in some sense, the only way that I at least can see of doing that is by seeing
everything as information. So I think, and in fact, there's something that I am working on that
looking at everything as information is a very productive thing to do. Yeah. But but the my
caution is that information is one aspect of everything. So I can give you a theory of everything
that's based on information. But it's not truly a theory of everything, it's the theory of one
aspect of everything. And I think there's a lot to be done there. But again, we shouldn't forget
what we're leaving out when we focus on that aspect. Yeah, I mean, we've spoken a lot on the
show about, you know, Penrose's view and obviously, Sal's view that arises from from biology. And I
know if Keith was here, he would argue strongly that he believes in continue. And therefore,
we would need, you know, hyper computation to have this universe. That would be an interesting
discussion to have, because I really don't see where there is physical or any evidence for continue
of any kind. The evidence is always that continue are useful approximation, but always underlying
the continue is a discrete reality. You take a sensor of anything, right, you know, quantum
mechanics is like the quintessential example of this, right? What do we measure at the end of the
day? It's always discrete events, right? Like it's the detection of a photon by, you know,
by whatever detector, right, could be a model clue of Dobson or SCC there, whatever. But it's
a it's a it's a change of state. It really is a bit. Oh, interesting. Well, how would you
contrast that, you know, Steven Wolfram has got this idea of digital physics and, you know,
maybe and again, unfortunately, we have to use arguments from behavior, you know, to kind of
say, well, we've got potentially a graph cellular automata. And it creates this beautiful emergent
structure, which is very much like the universe. But, you know, Scott Aronson would make the
argument that he's discounting quantum mechanics. I mean, what would you say to that?
So I think Steve Wolfram's theory is very interesting. And he gets some things right
that a lot of other physicists don't, in particular, that the universe at heart is discrete.
So I'm very much with him on that aspect of his agenda. And thank God, there's someone like him
and a number of others, you know, going that route, right? They're the minority in physics.
But actually, I think if you look at just what has happened the last 10 years, things are very
much moving in this direction. And I think they're going to move more, right? Now, having said that
his specific theory, I think has a lot of shortcomings. And I don't think it's the ultimate
theory, or maybe even the best path to a theory, you know, to a discrete theory of the universe.
Scott Aronson's critique in that regard, I think misses the point, right? It's interesting. It's
interesting that you should like pair those two because Steve Wolfram, in a way, is a physicist
to become a computer scientist. And Scott is the other way around, right? And I think, you know,
like, I greatly admire both of them, and I'm friends with both of them. I've had many discussions
with them. I think, you know, just to very, you know, cruelly carry two things. In a way, the
problem with Steve is that he has bought into the computer science assumptions too much.
And the problem with Scott is that he has bought into the quantum physics assumptions too much,
right? So if you really think carefully and rigorously about quantum mechanics,
all that continuous mathematics is there just to make discrete predictions. So the continuity
may be useful. It is useful. I'm not arguing against the use of continuity and infinity in our
mathematics. In fact, we'd be nowhere if we didn't have it. We just have to remember that it's an
approximation. It's a useful fiction, right? So quantum mechanics in no way invalidates Steve
Wolfram's theories, right? The problem, however, is that he has not entirely, you know,
ever since cellular automata days, right? He was always saying like, oh, you know, the laws of
physics will come out of this cellular automata, right? And people had these objections. And,
you know, now he and others have partly answered some of them. But the truth is, at the end of the
day, the only way to answer that objection is to say, look, here is how quantum mechanics arises
from my discrete model of the world. And I think this will happen, but it hasn't happened yet.
Interesting. Okay. We'd love to get Stephen on the show, actually. He's got a new book out now,
which is kind of like expanding on his previous work. But okay, I was having a chat with some
alignment folks yesterday. And it's something that I'm a bit naive to, but as I said, I've just read
a book, I think it's called The Rationalists Guide to the to the Universe. And it kind of talks all
about the early embryonic stages of Robin Hansen and Nick Bostrom and Eliezer Jukowski and the
Les Ron community and, you know, the info hazards and, you know, I don't even know if you've heard
of a Rocco's Basilicist and all of this line of thought, basically. And yeah, so where to go
with this? Now, I kind of put forward that part of my problem with their conception is that it
relies on this rational agent making trajectories of optimal decisions. And also, they tend to be
utilitarianism and utilitarianists and consequentialists. And yeah, I just wondered, what's
your kind of like high level take on this? Well, there's many aspects to this, right? I think,
let me put it this way. I, a rational agent, right, is an agent that maximizes, maximizes
expected utility, right? The definition of rationality is that it's, you know, expected
utility maximization, right? And there's a lot of content to this, right? And, you know, people
in many fields like economics and AI, right? They make good use of it. It doesn't answer the
question of what is the utility that you're maximizing, right? So, if you give me utility
function, now, if you maximize it, you're rational, you can be bound, you can be
boundedly rational because you're, and indeed, this is the interesting and prevalent case is that
you can only maximize it within bounds and you have to make compromises as to what not,
but still, you're rational. If you don't do that, you are irrational, right? So, rationality is a
very, you know, like so many mistakes that we make as a society as individuals would be avoided if
only we were rational in that sense. So, at that level, I sympathize very much with that view of
the world. Having said that, there's a huge gaping hole in the middle of this, which is like, but
what is your utility function, right? And, you know, one attitude is like, oh, that's for you to
decide, you know, you tell me what your utility function is, but then you're entitled to say,
well, like, my whole problem is that I want to figure out what my utility function should be.
And at that point, this whole theory of rationality just doesn't help you at all.
The utility function is an input, right? So, another question becomes what is your utility
function, right? And then there's a very related, but as Hume said, very different question, which
is like, what should your utility function be like? Should is a very loaded word here, right?
And then what usually happens is things like this is that our notions of morality and so on
are trying to impose a should on you, a utility function that you should have, because it serves
the utility of the society. Now, from the point of view, the society, this is good, right? But
from the point of view, because the society hopefully will live and prosper if its elements
contribute to its utility, not just their own, right? But it still doesn't answer the question.
You're entitled to ask. So, what does answer the question, right? And my view on this is that
none of these people, these people include Kant and Bentham and Plato and everybody, right?
You can't do that, right? So, to me, the supreme reality of life, supreme maybe is a bad word,
but like the overarching reality is evolution, right? Everything we are is created by evolution.
And as somebody famously said, nothing in biology makes sense except in light of evolution. Nothing
in morality makes sense except in light of evolution, not just biological evolution, even
though that's part of it, but also social and cultural evolution. So, at the end of the day,
the question that you need to ask yourself is like, is which utility functions are fitter
and those are the ones that will prevail? So, let's go there. That's really interesting. Now,
you're known as a skeptic of collectivist thought, right? And there's this interesting
dichotomy we were talking about of monolithic truth, but the utility function is interesting as
well because in a sense, I know these folks are consequentialists, but in a sense, that's more
leaning towards deontology. I did it again, deontology, which is this idea that we have
kind of like a principled approach to morality. And I'm skeptical as well that it's possible to
create such a utility function because it wouldn't really be parsimonious. But how do you wrestle
that, that you have a simple utility function even though you believe in diversity of ideas?
Oh, no, I didn't say simple. Go on. Crucial point, the utility function could be extremely complex.
And in fact, the utility function, so, first of all, there's more than one level to this.
Let's say you believe in utilitarianism, which I don't, but perhaps compared to the others,
it's probably the least bad, right? Believing in utility, if you believe in maximizing utility
function that in no way sanctions collectivism. Collectivism is one particular strength that
historically came out of that, right? And again, and Bentham is responsible for it,
but it's this notion that you should have a utility function in which everybody counts equally.
This is now making a choice of utility function, which is different from having one.
Okay, but I think you're saying something quite interesting as well, which is that at the moment,
the utility is a function of market value, which is very much inspired by
Adam Smith's hidden hand of the market. But I think your views against collectivism is very
much against this idea of equality of outcome. And that's definitely not what you're saying.
No, I mean, that's even going beyond that, right? Equality of outcome is actually irrational,
frankly, too. We could go into that. But you mentioned the market, right? And the market
is the side's utility. Again, that is a one way to decide, I mean, that is also a very
critical approximation to what you really want. So actually, all we have, whether it's capitalism
or carnivism at this point, in terms of utility function, are very imperfect, right? And that's
even saying it generously. And really, our job is to try to come up with something better,
which I totally think we can, right? And by the way, one very silent question here,
which again, for economists, it's very silent, is this question of like,
should you have one utility function overarching, controlling everything,
even if it's complex, right? Or should you not, right? And that I think is a very interesting
question, right? And there are good arguments in both directions, right? So let me just give you
one silly example, which then I think also generalizes two other things. Does your brain
have a single utility function? And I think the answer is no. Now, you could say from an evolutionary
point of view, the overarching utility is fitness. But then the way that caches out in your brain is
that your genes need to control this adaptive machine, right? In such a way that you give the
machine freedom, right? To do things that the genes by themselves couldn't. But at the same time,
at the end of the day, that machine has to subserve the propagation of those genes.
And the way you do this, right, at least the way evolution seems to have done it, and I think
it makes a lot of sense, is that you don't just have one utility. You have several ones which
correspond to your emotions. And then they fight it out. So I actually think there's this connection
between rationality and the emotion that people don't make, which is that, you know, your emotions
are really your utility functions. You just have different ones that cater to different things,
right? You know, fear and anger and so on. And so I think in reality, we actually have multiple
utility functions. But because, again, it gets to this problem that what we're trying to do is
approximate something that is very complex and difficult to get at. Maybe it is just one, but
we're better off trying to approximate it with 10 or 20 different things than just trying to nail
that one thing. That's really interesting. And is your view then on having this diversity of
utility functions analogous to your views on the master algorithm? It's analogous, but you're
actually talking about different dimensions, right? You could make a table where on one side,
you have all the different utilities. And then on the other side, you have the algorithms.
And now you can pair off any one of them. I can say, I'm going to pursue this,
you know, minimize your fear using symbolism or minimum. So any combination is valid.
Really, really interesting. Okay. And then let's get into meritocracy, for example. So at the
moment, we do have the market system. And presumably you think that some people do genuinely have more
market value than others. For sure. No. And by the way, I think I'm definitely a big believer in
meritocracy. I think. But what does it mean to you? Right, very good. So let's let's get that
down first, right? Meritocracy. So our goal is to have the society that that functions best and
provides best for everybody. Right. And I mean, we could refine even that. But let's just take
that for now as our assumption, right? But then if that is the case, one of our primary goals,
maybe even the most important one, is to get everybody to contribute the most they can.
Right. Meritocracy is often seen as like, I'm going to rank all the people and at the top is
the greatest genius. And at the bottom is the most useless person. And this is wrong. Right.
Meritocracy is a many dimensional thing. Yeah. Right. The goal of meritocracy is to find for
everybody what they're best at doing so that they can do it. Maximize everybody's contribution to
society. Right. And this is a very complicated process. There isn't a single scale of intelligence
or anything else. Having said that it very much is the case that some people are better for some
things than others. Right. And if you deny that you are actually in the process of destroying the
society and making it dysfunctional. So the attack, I find the attacks on meritocracy extremely
disturbing. Right. And a lot of them are, you know, I've talked with many people who have those
beliefs. Right. And the number one thing that they say is basically goes down to like, oh,
meritocracy isn't perfect. So we should junk it. Something being not being perfect has never
been a reason to junk it. It's a reason to improve it. So there's a lot of room to improve in
meritocracy. But if you throw it away, you are destroying society. Well, I mean, this, this,
you can trace this back to our argument about utility. But the thing is, though,
if we had a value function which represented actual market contributions or even societal
contributions, that'd be one thing. But would you agree that that we have a lot of game playing
at the moment? So utility is based on playing the success game or the dominance game or the
virtue game as Will Stor said in this book. So we've got these kind of emerging games. And
it's like, it's not really representing utility. Well, absolutely. So,
so far, we've been talking about utility, right? But what happens in the real world is that there
are multiple agents, each with their own different utility. And at this point, what you have is game
theory. Right. Game theory is just what you have when there's not a single optimization going on,
but multiple optimizations, which are partly contradictory, maybe partly not. So the best
way to understand everything that we've been talking about, including society and evolution,
and even what happens inside your brain is as a big game, a much bigger and more complex game than
game theorists and economists and so on, and evolutionary biologists, right, prominently,
have been able to handle in the past. But I think they are very much in the right track. And we
can understand a lot of these phenomena that you're referring to as they are games being played by
people that have certain utilities, right? And now you are going to impose your, you know, like,
and it's a game, right? I don't, you don't know who's going to win until you actually do the linear
program and figure out how this is going to, you know, and of course, games are, you know,
in reality, you know, most games are not single round games, right? They're continuing games,
right? So things get very, very interesting. But this, I think, is the most productive way to
look at all of this. Okay, good, good. But then some might say that this is a platonic way of
looking at the world. And the world is actually much more complicated than that. And again,
we're kind of fooled by randomness, because we're anthropomorphizing the world, and we're kind of
framing it as a game, it might be much more complicated than that. And I've already said this
a couple of times, but you know, the concept of power, for example, did when Napoleon said,
I want the men to march into this country, is it just a simple kind of chain of command that
goes down? No, it's not. It's so much more complicated than that. Well, yes, but that's,
actually, I'm not even sure what you mean by when you say it's much more complicated than a game.
Again, when I say a game, maybe what comes to your mind is something simple, like, you know,
Prisoner's Dilemma, two, two players, two moves. It's a game with, you know, with a vast number
of players, each with a vast number of moves. Interesting. But I think this gets to the core
of what the rationalists talk about, that they have these thought experiments, they talk about
Prisoner's Dilemma. They have that. I forget the name of that game where there's the two boxes,
and you have to choose the box. I forget that. But I guess what I'm saying is, is that if you do
have this rationalist conception of the world and think about it in terms of game theory,
just like the symbolists do, and the people who handcraft cognitive architectures do,
even with causality, for example, we create these variables, it's all anthropomorphic.
Well, I would not, so let me, let me put it this way, right? You can model almost anything
can is an important word here. You can model almost anything in the world,
in any domain, from physics to psychology to sociology, name it, as optimizing a function.
Whether you should is a debatable question, but you can, right? But now, what really happens is
that there are many different optimizations going on at the same time, all the way from
maximizing entropy to me deciding what I have for lunch today. And now what you have is all of
these interlocking optimizations, and that's what I'm calling game theory, right? One of those
optimizations is I'm Napoleon, I want to conquer Russia, you're the Tsar of Russia, you don't want
to be conquered, right? And then we play a very complicated game, which includes other agents,
like your soldiers, which maybe, you know, I, a French soldier, you know, want to conquer Russia,
but I also want to stay alive, whereas an opponent really couldn't care less whether I, in particular,
stay alive or not, as long as he conquers Russia in the end. So this very complex game,
I think, is what goes on. I don't think framing things in this way is anthropomorphizing them.
In fact, I think this is our best hope to not anthropomorphize things, although at the end of
the day, I think you can look at almost anything and see a ghost of anthropomorphization there.
But if there's a less anthropomorphic way to look at the universe than through this lens,
I'd be interested to see what it is. Well, the only reason I'm saying this is, first of all,
I want to play devil's advocate a little bit. And we even spoke about the blind men and the
elephant a little while ago. And I'm sure folks on the left, as they did, they criticized Ayan
Ran, for example, and they said that she had this very transactional way of, you know, viewing the
world as this kind of Nash equilibrium of self-interested actors. And are we guilty of
doing that? Are we kind of like cutting off many aspects of the truth by doing this? I guess that's
what I'm saying. So we are always cutting off some aspect of the truth when we look at anything in
any way, which is not a reason to look at nothing in no way. So I think this is a very productive
way to look at things, but not the only one. It doesn't exhaust what there is to be said,
but I personally feel like it's the one where the most progress can come from.
Interesting. Now, that's sort of like Ayan Randian simplification of the world.
Looking at things this way does not imply oversimplifying them. On the contrary, I would actually
say it gives us a handle on how to go into the complexity and not get lost and not devolve
into like platitudes or oversimplifying ideologies. The fact that there's a mathematical component
to this is very important. Mathematics, when you can apply it, gives you a very solid handle on
things. We are now at the point where we can handle a lot of things mathematically slash
computationally that we couldn't before. So when von Neumann invented game theory, he said,
this is the future of the social sciences. And so far, it hasn't been. But I think we're actually
now at the point where partly because we have the data, we actually can now usefully apply this
point of view in a way that we couldn't before. How far it takes us, we'll see. It's not the only
possible to look at things, but I do think it's probably the most productive at this point.
Interesting. Okay. So coming back to this rationalist school of thought,
one thing that I'm interested in is morality. But let's go one step at a time. So I think
Bostrom came up with this idea of instrumental convergence, which is this notion that in the
pursuit of doing a particular task, the intelligence system might actually potentially kill everyone
on the planet or do adjacent tasks. And this is where the interesting thing comes from. So one task,
but adjacent multitask ability and, you know, like potential intelligence and so on. So there
was an example of a cauldron. So, you know, you've got someone filling up a cauldron and in the
pursuit of filling up the cauldron to just the right level, they might kill the person who looks
after the cauldron room just so that the agent could do it more efficiently. Are you cynical about
that? Or what do you think? No, I'm not cynical about that. But let me put it this way. I don't
lose any sleep worrying about the, you know, paperclip factory that's going to take over the world,
right? I think you have to take that as a philosopher's thought experiment, right? You
know, the philosophy of being Nick in this case, right? I think there's a real danger that there's
putting its finger on, but it's also mistaking reality for something else. So let's look at
both parts of that, right? The real danger is that if you give an AI an oversimplified, a hugely
oversimplified objective function, and at the same time, a very large amount of power, bad things
will happen. And we need to worry about that, right? So, and by the way, this is already a
problem today in many, maybe more modest ways, but many, but also more relevant, frankly, right?
And so what do you do, right? First of all, is the utility function needs to be as rich
and as complex and as subtle as the people that it's trying to serve, right? So as long as what
you have to take a really world example today, social media, who's, you know, are all designed to
just maximize engagement, right? You have an enormous amount of AI at the service of maximizing
engagement. It's a very, I understand why companies do it and partly they have the right to,
we can get into that. But the point is, the consequence, it's ignoring too many things,
right? So one line of defense against is like, you have to enrich your utility function until
it's like a bit, and then this is an open-ended problem, right? We're never going to have the
final utility function. It's something that the AIs have to be continually, you know,
AIs, I think Stuart Russell said this, and I agree, like they should spend half their time
figuring out what the utility function is, and then the other half maximizing it. Whereas today,
it's like, I wrote down my utility function in one line, and now I spend this enormous amount of
power maximizing it. So that's one line. The other line, or like one other line is, you have to put
constraints on the machine, hard constraints. You can, in the pursuit of this utility function,
you can think of it as like, you know, terms with infinite weight in the utility function,
you can't go outside this. And then the other one is, the single biggest reason why I sort of like
this paperclip experiment is silly is that, you know, along with that paperclip factoring the
world, there are going to be a million other AIs, you know, each of which is doing the same thing.
So none of them is ever going to acquire the power to cause that damage unless it's doing
something very different from just trying to make paperclips. So at some level, that example is
extremely unrealistic and leads us down the wrong track. Right, loads of places to go there. But
first of all, I think you do believe in AI alignment, then, because you're saying exactly the same
as what they do, which is that we need to have the utility function that represents the richness of
the human condition. So that's the first thing. So essentially, you're all on board with alignment.
Well, I believe in AI alignment, in one sense of it, many different things get go under that
umbrella of AI alignment, right? I think in the near term, thinking of things, I mean, if I like,
let me put it this way, if AI alignment is just trying to have a really accurate utility function,
then yes. And then the machines are optimizing that function, absolutely. Right. And in the near
term, I think talking about AI alignment is a little, I mean, the problem that I have with
the concept of AI alignment is that goes far beyond that. It tends to see AI's as these
independent agents that we have to align their goals to ours. Right. And if that just caches out
as like, here's the utility function, that's fine. But the problem is, AI's are not independent
agents. AI's are our tools. Well, just to push back on that a little bit, because I always had that
conception of these folks. I thought I was arguing against people who believed in a pure monolithic
intelligence. And a lot of them are transhumanists, actually. And they say that they want to
ensure human flourishing through the use of AI's in tandem, almost as a kind of extended
mind from David Chalmers. But then I really wanted to get into their fears of recursive,
self-improving intelligence and superintelligence, because when you do have this kind of heterogeneous
approach to humans and machines, there are going to be bottlenecks everywhere. Now, I like to think
of it a bit like the market efficiency hypothesis, which is that you reach an equilibria where,
you know, the individual actors in the market become more efficient, will become more efficient
programmers, because we're using codecs. But we will reach a limit, surely.
Well, to touch on transhumanism for just a second, because I do agree, at least sociologically,
a lot of that crowd is the same. Let me put it this way, and I'm sure this is a controversial
statement. But maybe in the long run, the AI's should take over the world. Why are we so arrogant
that we think whatever the AI is, it should always be there to serve us. We are a step,
if you take the long view of this, we're a step in evolution. We're amazing. Maybe I'm a human
chauvinist, but I do think we are amazing. But we're not the last word. So the other day,
I tweeted something that is maybe provocative, but it's like, I think in Gemswich, which is,
I said that the killer app of humans is producing AI. Maybe our role in evolution is that we're
going to produce an AI. That is the next level of whatever you like, consciousness,
intelligence, et cetera, et cetera. And so the notion that in the very long term, the AI's
should still be there to serve us by this point of view is actually silly.
Right. But a lot of folks, let's say the ethics folks would find it horrifying. And I was speaking
to Irina actually yesterday, and she said something a little bit tongue in cheek, which is that which
Irina from Montreal Mila, Irina Rish. Oh yeah, I know her. We were classmates at UC Irvine.
Amazing. I really love her. But no, she was kind of joking that we should almost align human values
to the AGI values. Well, that I find alarming. Well, I think she was saying it tongue in cheek.
I'm not alarmed by a lot of things, but yeah. But what do you think about this ethical concern
that if it is the case that you believe that we're just one rung on the ladder and transhumanism is
more AI than it is human, people would find that horrifying? Well, I understand why people would
find that horrifying. And I mean, again, we have to distinguish the short from the meaning from the
long term. When I say something like this, I'm talking about the very long term, right? Trying
to make humans subservient to AI today is a horrifying idea. Now, I think the reason a lot
of people are horrified with this idea period, right, is natural. But in my view, naive is just
they are seeing humans as the end goal. If humans are the end goal, then the idea that
they should be subservient to developing the next level of AI is horrifying. If you have a moral
system where humans are the be all and end all, then all of this is horrifying. But again,
if you take the long view of evolution, humans are not the be all and end all.
Okay, I mean, eventually this might take us to the effective altruism discussion. But I think,
as we were saying, Sam Harris recently had a podcast talking about the FTX disaster,
and he was kind of making the argument that we're all consequentialists, even if we don't
realize it, but there are different degrees of consequentialism. And I think a lot of the
ethics folks at the moment, they really, really don't like what's going on with long termism.
And it's because there's this slippery slope of the kind of horizon of consequentialism. So
with Nick Bostrom, you know, he came up with this number that there could be simulated humans,
you know, living on other planets in the future. It's a very big number. I think it's like, you
know, it's got a lot of zeros on it. And what's what's to stop us from just making the argument
and what's to stop AI is from making the argument that those simulated lives have more value than
our life. Okay, there's a lot to unpack there. So but let's take this one step at a time.
I very much buy the idea of effective altruism on principle, right? I think that is the way to go
about a lot of things. I think in some ways, if you are not an effective altruist, maybe unconsciously,
you are being irrational or maybe evil, right? If you believe in altruism, I mean,
like think about both parts of that, right? If altruism is good, then then let's say we take
that, right? And then why should be why should you be in favor of ineffective altruism, right?
If you're an altruist, if you want the good of other people, you should try to do the best you
can, right? And so for example, I very much buy the notion that like, you want to, you know,
make the most money you can, so then you can give away that money as opposed to volunteering at the
soup kitchen, volunteering at the soup kitchen for say someone with a PhD machine learning is an
ineffective form of altruism. Now, having said that, I think the focus on the long term has been,
in many ways, I mean, certainly the long term is important, right? But the problem with the whole
effective altruism movement is that it got overly focused on that. And we can talk about why. And then
even even, and then a further mistake is that it got overly focused on these supposedly existential
dangers that are much less of a big deal than people think like AI. So between effective altruism
and fixating on AI as an existential danger lies a huge gulf. I'm for effective altruism. I think,
you know, the long long term, you know, there's ins and outs there, right? And then this focus on
like these existential dangers is very problematic. You know, for example, you know, to get back to the,
you know, Bostromian notion of like all these minds that matter more than us and whatnot,
there is a basic idea, right, that like any economist knows, which is that you have to discount
the future. And the question is what your, is what your discount rate is, right? And if your
discount rate is high, right, those, those minds matter not at all. And now why do you have that
discount rate? The primary reason is that there's uncertainty about the future, right? I have to
weigh the, the certain benefit of helping you today with the increasingly hypothetical benefit
of helping your mind, there's less and less likely to exist in the future. So in many of those cases,
the present and the short term do win. Okay, but a couple of things to contrast that. So
a lot of effective altruism is this idea that we're born with faulty programming, right? So we,
we, we have these views, you know, we have this concept of moral value, and it gets discounted
in space and time, right? So we need ways of overcoming our programming. But you were saying
that we should be thinking about this, but contrast that with your, you know, with your
statement about Ayan Rand earlier, right? So Ayan Rand was very, very transactional, because I think
the, the folks that criticize this movement are suspicious that we are actually being a bit more
like Ayan Rand, but with the guise of, of altruism. And I think that, and they, they think of the FTX
disaster as being kind of like evidence of that. A lot of different points there. The FTX disaster
actually has nothing whatsoever to do with any of this, right? Sandbank Manfred was one guy or is
one guy, funny that I used the past tense. He's one guy who believed in effective altruism, good
for him, right? He was, I mean, the whole FTX thing was also obviously, I mean, yeah, we could
get into that. But the point is, you should not, I understand why people's image of effective
altruism would be tainted by what happened with Sandbank Manfred, but really it shouldn't be,
right? You know, an idea is not responsible for the, you know, mistakes that its believers make
in, in, in unrelated domains. Point one, point two, transactionalism. There is nothing in what
I've said whatsoever that implies transactionalism. In fact, the opposite, right? I think relationalism
is actually the key concept. And part of this is that games are not one shot. Your games are
played in a repeated way. And famously, for example, if you play things like Prisoner's
Limit and whatnot repeated, like you're like, you know, cooperate, defect and whatnot,
as soon as you start bringing in these other things, like that make things more realistic,
you actually start to get behavior that is much more, you know, what's the way to put it, rational
in some ways and human and whatnot, right? Another one is that traditional economics,
which I think Ayn Rand was influenced by, viewed and still views the world as linear,
but the world is nonlinear. Once you start seeing the world as nonlinear,
all of these things really change, you know, the face of them changes, right? So I think we
have to look at all these concepts in this view, right? And we want to focus on the long, so
so to get to go back to your first point, right? We are born with faulty programming.
Part of our fault, and that's what if, you know, effective alteration is there to overcome, right?
Part of our faulty programming is that our discount rate is too high,
because we evolved in a world where your time horizon was very short.
The fact that it's too high doesn't mean that we should make it zero
and care only about the future. But what would, you know, the ethics folks who advocate for
gatekeeping and paternalism, couldn't you just say that they're doing the same thing?
Well, you should ask them, right? Wouldn't they lead by saying our programming is faulty and
therefore, you know, we need to... No, I mean, look, we can, so part one, we can debate whether
our programming is faulty or not and why. And so to just start by touching on that, our programming
is faulty. So our programming is not faulty in the sense that we evolved for a particular set of
conditions, right? And that evolution may not be complete or optimal, et cetera, but roughly
speaking, we are not faulty in that sense. The reason because evolution is doing its job, right?
We have all those impulses for a reason, right? Now, the problem is that we, unlike any other
species, we actually have actually succeeded in creating a world that is better for us. But at
the same time, and this is the problem, we're actually now adapted to a different world from
the one that we live in. So the faulty program just comes from the fact that we evolved for one
set of conditions. For example, among many other examples where your time horizon was very short,
and now we live in a very different world. And so our job as rational people, that's what our
rational minds are for, among other things, is to now adapt ourselves to the world that we really
are in, so that we do things that are rational in the world that we're in, right? So now the fact
that our programming is faulty, there's not, there's not say anything about what are the
faults and how you fix them. And what these people have, I think, is first of all, the wrong
notion of what our faults are, and then on top of that, the wrong notion of how to fix them.
Okay. Now, I want to get into the utility function again. Now, again, one of the things
that makes me skeptical is this notion of immutability, both of what we're doing and in
the case of what we've been speaking about with utilitarianism, what the utility function is.
Now, you were kind of hinting to something interesting before, which is that it might
be diverse and it might also be self-updating. But I'm constantly asking myself a question,
how does that work and who gets to say? Well, so very much, I think it's complex and it should
be self-updating, right? We're never going to final itself. If you buy this notion that the
ultimate arbiter is evolution, then utility functions are subject to evolution, right? So you
think about it, or you can't think about it, you can't think about this, and it's useful to think
about this in the following ways. To a first approximation, the number one entity that's
evolving is utility functions. What you have in the world at any point is a population of utility
functions, right? And now they combine, they evolve, you have next generation of new functions,
and then there's also how the utility function gets optimized. There is also subject to evolution,
right? And now, how the utility function is optimized changes a lot faster and is a lot
more complex than the utility function itself, which is the point, right? So at a certain time
horizon, it's reasonable to approximate utilities as being fixed. Like for example,
the utilities that are encoded in your brain are fixed by your genes, right? So in the context of
our present human moment and effective optimism or not, it makes perfect sense to think of
utility is fixed, but it is evolving and not just on, you know, eon time scales, but by the
generation, right? You know, things evolve by the generation. Okay, but it's still relatively
glacial. And I take your point that there's a kind of divergence between the world we live in
and the programming that we've got. But then, okay, let's imagine that we create a new population.
And I guess what I'm saying is, is that you think that the utility function should emerge
and evolve. But I would argue for some kind of morphogenetic engineering, where it's a kind of
hybrid between something which is emerging, but something which we can nudge. Oh, I mean,
I'm glad you brought that up. Nudging is a form of emergence. You yourself are emergent and the
things that you do are emergent as well. Everything is emergent, right? Utilities are emergent.
Maybe the laws of physics aren't emergent. Some people say even those are, right? Like, you know,
we live in a universe with these constants because blah, blah, blah, right? So, but to first
approximation, every single thing that we've been talking about is emergent. We make a distinction
between emergent and designed because that is anthropomorphic, right? Is this things that we
do are not emergent? Actually, no, when you nudge something that is an emergent behavior, right?
We are emergent as well, right? So everything that is human, you know, so he's a very good way,
I think, to think about a lot of things which I first saw, you know, in Richard Dawkins, which is,
although he really didn't go into this and I wish he had like this notion of the extended phenotype,
right? Technology is our extended phenotype. So all these things that we do, right? All these
things that we build, including AS and whatnot, they are extensions of our phenotype. So if you
take the long view, all of, you know, technology is by all is the continuation of biology by another
means. So when you make this distinction between emergent and not emergent and top down and bottom
up, it's all emergent. Interesting. Well, when we recently did a show on emergence, and it's a
topic of interest to me personally, and there's there's weak emergence and strong emergence,
and there's, you know, like the view of weak emergence, so there's some, you know, surprising
macroscopic phenomena, maybe something which transiently emerges, and Wolfram would add in the
whole, you know, computational irreducibility angle. And then with the strong emergence,
Chalmers would say it's something which is paradigmatically surprising, it's something
which is not deducible for many fundamental truths in the lower level domain. But I just
wondered, like, how do you think about emergence? Well, I think that is a very, the distinction
between weak and strong immersion is a very useful one, right? And I would actually phrase it in
slightly different terms, which is starting from physics, right? I think most physicists
and scientists believe in weak emergence. Well, could I could I add that Sabine Hossenfelder
had a paper and she frames it with this idea of the resolution of physical theories. So like,
like a lower resolution theory is weakly emergent from a higher resolution. Well, exactly. And,
you know, like, I like Sabine, but this is not her idea, right? This far predates all of us here,
right? And again, it's a very interesting history and a very important concept. Now,
so my point was that I think few people have a quarrel with the notion of weak emergence in the
sense that, you know, I can give you a theory of everything in the form of whatever string theory,
let's take a candidate, right? But no string theory claims that that's a theory of everything in
the sense that like now, to study biology, psychology, sociology, you should just study
string theory. No one believes that, right? There's actually interesting things to be said there,
but but let's not let's look at let's talk about that for a second, right? There are these levels
that emerge weakly in the sense that they are determined by the lower levels. They're just
so much more complex that you're better off focusing on the menu. Now, there's this other
notion is to me is the really interesting one, which is that there is there are phenomena that
are at the higher levels that are just not reducible to the lower levels. Yeah, right? So the true
emergent is in some sense is someone who believes the latter. And now you can ask the question,
like, do you believe in that or not? Right? And I think to give the very short answer first is
that ultimately, there's probably no way of knowing. But pragmatically, you're actually
probably better off treating the world as if it has strong emergence. And now, strong emergence
actually a very strong segment to make is to say, and by the way, going down to the lowest levels
to make things very clear, you don't need to think about biology or society or consciousness or
anything. Condensed metaphysics, right? The particle physicists think to believe that what they do is
what everything reduces to. You talk to the condensed metaphysicists. This was actually
interesting discussion that I had with Scott, you know, Aaronson, because like, he was very much on
the, we're both computer scientists, but he was very much on the side of the particle physicists
on the side of the condensed metaphysicists. What they will tell you over and over again,
they see is things that you cannot explain using quantum mechanics. And now, people say like, oh,
but you can always explain things in quantum mechanics, you just haven't done the calculations.
But the point is precisely that you can't do the calculations, right? The calculations are chaotic.
Yeah, I have a theory, I can come up with 500 theories of these phenomena and semiconductors
and whatnot. And like, I never actually get to test them because the computations diverge before
I get to test them. So for all intents and purposes, it is strong emergence, whether truly that came
from below is unanswerable because you can't compute the predictions. Well, we spoke about that.
So I think Keith would call that semi strong emergence, which is like, you know, whether
it's computationally reachable from the lower resolution to the high resolution to the lower
resolution. But no, Sabine in her paper, A Case for Strong Emergence, she was talking about
singularities as being a really good example of what might be strong emergence. And the philosopher
Mark Bedal, I think, said that strong emergence is ridiculous. It's basically an affront on
physicalism. Well, certainly, you know, strong emergence and physicalism, or let's just call
it reductionism, right? Reductionism, yeah. Strong emergence and reductionism are incompatible.
Yeah. And we scientists tend to be reductionists. Yeah. Right. Now, you like, and at some level,
I'm both a reductionist and someone who is willing to believe in strong emergence. Again,
I don't believe in strong emergence. I just don't see a way to disprove it. Right. And whether
semi unlike, you know, if there's an empirical way to distinguish semi strong from strong
emergence, I'd be very interested to know what it is. But now I think the thing that is very
important that a lot of people, including a lot of physicists and scientists don't see is that
we have this hypothesis that everything can be reduced to the laws of physics as we know it.
We should not forget that it's just a hypothesis. And it's a hypothesis that again,
counter to a lot of people say is very, very, very, very far from established. And usually,
people say like, Oh, but, you know, look at all the successes of the laws of physics and blah,
blah. And then I say like, you know, putting on my machine learning hat, the sample that you've
used to validate the laws of physics is extraordinarily biased in the direction of simple
systems. Okay, so you can't make this claim of if the data was I ID, I could say with great
confidence, these laws apply universally, but I haven't done it. It's more like, I've just landed
in a new continent, and I sealed up all the rivers. And I say, I know what this continent looks like,
you've never climbed the mountains, you've never gone in the jungle. So like this notion that the
laws of physics capture everything about daily life, we just don't know how exactly. Maybe it's
true, but it could also equally well be completely false. Brilliant. Well, you gave a bit of a hint
to this earlier, actually, because you use the word relationism, right, which which is basically
the or relationalism, which maybe should be, you know, starting to relation, relationalism.
But yeah, I think Rosen is a great advocate of this. And he has a whole kind of category,
theory, calculus for describing living systems. And also, we spoke to Bob Coak, the quantum
physics professor from Cambridge, and he was talking about this concept of Cartesian togetherness,
which is another category or framework. But I just wondered, like, you know, does that inform
your view? Well, relationalism, at least in one way of defining the term very much informs my view,
right. And one way to come at this is to say, the world is not made of independent entities.
Actually, let's just start with machine learning, which is a very concrete way to look at this.
A very large part, maybe even the largest part of my research in the last 20 years has been
to do away with the assumption of IID data, right, that the world is made of independent entities,
in particular, society is made of independent agents, etc. etc. Right. Now, we make this assumption,
both as human beings, you know, to some extent, and certainly very much so in science, because it
makes life easier. The math is way, way, way easier when you assume independence. But it's a blatantly
false assumption, right. Unfortunately, a lot of, for example, economics prominently has embedded
in it this notion that the world is a bunch of independent agents, and it just doesn't work
like that. And moreover, it's a distinction that is full of consequences. A society and economy is
a network of agents, and almost all the action is in their interactions. Until you really start
taking that seriously, you really don't understand the world. Again, I have no quarrel with classic
economics as a first approximation, it's exactly what it should be, right. But then, and by the way,
we should also not just throw it away and say, like, oh, this is garbage, like some people say,
you have to go the next stage, it's actually now we have the mathematical and computational
tools to do and understand it as being a system of interacting agents. And all of the questions
that we are talking about, including, you know, in, you know, in evolution, even in physics,
right, a piece of condensed matter is a network of interacting spins, etc., etc., you name it. So,
the relations are at the heart of it. And moreover, like as I said, a lot of my work is we now have
the representations, the learning inference algorithms to handle things that are big piles of
relations, and the whole world is better understood in those terms. And we just need people to catch
up with that. You know, once you do that, you get into things that can easily be computational,
intractable, and so on and so forth. But there's a lot of things that we can do there and a lot
more that we'll do. So at this level, I think relationalism is really should be a cornerstone
of our understanding of the world in a way that it hasn't been in the past.
Okay. And which existing complexity science brings to mind? But I mean, which existing
techniques and areas can folks look into to take that on board?
Well, you know, Markov logic, which is what I developed for this purpose, essentially. And I
do think, you know, this is my talking about my work, so you should naturally be suspicious. But
I think it's the best that we have. And I think by a wide measure, compared to anything else that
we have so far. Okay. And can you sketch it out? Yeah. So this to sketch it out in the simplest
terms, right? We want to combine all the all the traditional goodies that we have from assuming
the world is IID with the power to model, you know, relationships that are themselves,
potentially very complicated. The way we do the Markov logic is there's the logical part.
We actually do not need to solve a new the problem of how to represent and do inference
with relations. We have first order logic for that. First order logic is the language of relations,
that's actually the term that is used, right? And how the relations depend predicates,
sometimes they're called predicates, but let's just call them relations, right? We have a formal
language to talk about relations. And by the way, essentially, all of computer science can be
reduced to that you give me your favorite, you know, whatever, knowledge representation, data
structure, etc, etc. And I can you meet I anybody who knows can immediately say how to do that in
logic. So that's one part. The other part is the statistical, you know, machine learning probabilistic
aspect of the world, right? And then again, going all the way back to physics, right? All of these
things that we deal with are essentially special cases of what are variously called Markov networks,
which is where the name Markov comes from, or graphical models, or log linear models gives
distributions, Boltzmann machines, like right, all of these things are essentially the same, right?
That whole neck of the woods is captured by Markov networks, let's call them that.
And Markov logic is combining Markov networks with first order logic in a single language,
which you can now do everything with. Okay, okay. So just just to like push back a tiny bit. So
in the past, we've tried to create, let's say things like psych, which is a knowledge representation
of the world. Folks like Montague have tried to do semantics using first order logic to
set some, you know, varying degrees of success. And then we have the grounding problem we were
just talking before about, you know, like even Searle said this, that you have kind of epistemic
objectivity and subjectivity and some things are observer relative, like even economics is
observer relative. So with this kind of formalism, how would that work?
So very good. The problem with or the main problem with a lot of these things that you
mentioned, like, you know, certain types of semantics and whatnot, that are based essentially
on first order logic, right, is that they're too brittle. In fact, the problem with symbolic AI is
that it's too brittle. And this is exactly what Markov logic fixes. It fixes it by making it
statistical. When I give you a logical statement now, I'm no longer, for example, simple logical
statement, you know, a smoking causes cancer, right? In English, this is a valid statement,
smoking does cause cancer. But actually, once you translate it to logic for every x,
smokes of x implies cancer of x, it's false because some smokers don't get cancer, right?
What this really was meant to be all along is a statistical statement that says smokers are more
likely to get cancer. So the way we overcome a lot of those problems is precisely that we take
all of this logic, which again, the language exists, we don't have to change it, we can, but we
don't have to. And we make it statistical as a result of which it's no longer brittle. Or at least
now it's only as brutal as machine learning and graphical models or not. It's not as brutal as,
you know, traditional symbolic AI. Okay. And so we speak into a lot of go-fi people. And I mean,
Wally Subba, for example, he's a rationalist. And what's interesting about the rationalist is
they hate any form of uncertainty, right? They think in absolute binaries, you either know it
or you don't. No, I mean, let me push back on that. There's this, again, you need to distinguish,
you know, a general field or idea from its subtypes, right? There is a type of rationalist
that hits uncertainty, big mistake, big, big mistake. There's a type of rationalist that,
you know, uncertainty is what they, you know, like, an uncertainty calculus is a type of
rationalism. And some of the best, you know, AI philosophy, et cetera, is just that. So
there is no incompatibility at all between rationalism and uncertainty. In fact, if, if
rationalism, if being rational is maximizing expected utility, notice the expected in there,
right? You cannot be rational if you ignore the uncertainty.
Hmm, interesting. Okay, but then what about the resolution of modeling? I mean,
smoking is a really good one. So us humans, we anthropomorphize things, we understand the world
in macroscopic terms using macroscopic ideas that we understand. And that kind of leads
to a certain type of modeling. And that modeling, presumably, would be represented at that resolution,
you know, using this formalism.
Oh, sure. And what's the question?
Well, it seemed again, like I'm intuitively suspicious that we were just saying the world
is a complex place. And with a lot of causal modeling, for example, a lot of the art is
understanding what is relevant and what is not relevant. What is relevant might just be kind of,
you know, relevant to us. No, well, what is relevant is what is relevant relative to your
utility function. Again, it gets back to that precisely. The whole problem is that the world
is infinitely complex. And we have only finite computational resources, whether it's in our
brains or our computers or whatever, right? So now what do you do, right? You are forced
to oversimplify the world, not just simplify, but oversimplify, right? But now the whole art,
that's actually a good word to use, even if it's done with computers, is how do you not only simply
oversimplify as little as you can, but pick out the simplifications that are least harmful
to your objective. By the way, the art of the physicist, physicist would tell you is precisely
doing this, right? Physicists are very good at deciding what to simplify. And in fact, almost,
I think at some level, almost any good scientist, this is what they do, right? So and now how do I
decide what and how to simplify is by relevance to my utility function, right? I want to ignore
parts of the world that do not affect my utility function number one, right? And for example,
the notion of conditional independence, which is the foundation of graphical models, that's what
the whole idea is, is like, once I know these things, I don't have to know about those others.
Thank God, right? Okay, but if Ken Stanley was here, he says that the great thing about evolution
is it's divergent, it's not convergent, it's discovering new information. And my worry is
with a system like this, with any form of anthropomorphic design would inevitably become
convergent. And it might look like, oh, those things over there that we're ignoring don't matter,
but actually, they might really matter if they got introduced into it. Well, I wouldn't say that
maximizing expected utility is anthropomorphic, right? In fact, it's one of the least. I think
maybe there's some degree of anthropomorphic is almost anything we do. And you know, the progress
of science is becoming less and less anthropomorphic, and we should keep pushing on that. But I would
say that maximizing expected utility is one of the least anthropomorphic things we can do.
Well, this is actually a really interesting point, because one of the key tenets of the
rationalist movement and their conception of intelligence, because you know, all of the other
definitions of intelligence are anthropomorphic. So, you know, there's based on behavior,
capability, AI, principle, function is a big one, you know, from Norvig. And this is the
principle based AI, which is just making rational moves. So why is there such a push to be as,
you know, to be as the least amount anthropomorphic?
Oh, the push is not to be, at least in my view, being less anthropomorphic is not a goal. That's
not the goal. The goal is to be as accurate and complete as we can in modeling the world, right?
We're just trying to understand the world better, right? For whatever purpose, maybe for its own
sake, maybe for the purpose of the utility and the evolution and so on, right? But that's the
goal. The problem is that, and this has been the problem since they won, right? They won of humanity
is that because we anthropomorphize the world, that gets in the way of understanding how it
really works, right? If I say the wind is some god blowing, right? I understand, right? That's all
they could think of. But it's a big obstacle to understanding what the wind really is like. There's
a pressure difference, et cetera, et cetera, right? And we've done away with a lot of anthropomorphism
by way. One of the problems that we're always having is that it's always pushing back, right?
You know, there's always, you know, again, intuitively, we have a very strong tendency
to anthropomorphism. So like as much as science broadly construed as a great victory, it's always
in danger from this, right? But even within science, we've gone from doing away with the obvious
forms of anthropomorphism and anthropomorphism to having many things that are still there that are
less obviously anthropomorphic but still are, right? But if there's something anthropomorphic
that's actually is accurate, then more power to it. Interesting. Yeah. And then I guess we have
so many cognitive priors, right? In our brains that give us a cone of attention, which has
just completely anthropocentric. Well, very good. So those priors and maybe a better term is heuristics,
right? Our brains are full of heuristics that evolution put there for a good reason,
because those heuristics work, right? But they are heuristics. So they have failure modes, right?
And we need to understand what is that those heuristics really are getting at so that we also
so that we use them when they're good. But then when they're not good, we use something else.
Brilliant. Brilliant. So Pedro, we're here at NeurIPS this week. And could you just like sketch
out some of the some of the things you've seen? And I also know that you're a huge fan in that
there's a neurosymbolic algorithm that you want to tell us about. So let's let's hear it.
So I indeed, I've been enjoying NeurIPS this week. One of the big things in AI in the last
several years has been neurosymbolic AI, which you probably will not surprise by the fact that I
very much believe in. So and I believe this since I was a grad student, and the whole idea of neurosymbolic
there was something that nobody was interested in, right? And now suddenly everybody is which I think
is a good development. And this is the idea that if we really want to solve AI by some
definite, if we want to get to like human level intelligence, etc, etc, we need to have both,
you know, like, for example, deep learning is not enough, right? There are symbolic reasoning
capabilities that we have and that are essential. Yeah. And we need to get them. And I think, you
know, intelligent connection is like, I don't know, Yoshua Ben-Joe, you know, Yan Likuna said that
they don't disagree with this. But one way to look at this is to say, we're just going to realize
those capabilities using purely connectionist means, right? And what I see happening in that
direction, unfortunately, is a lot of reinventing the wheel. So I do think, you know, symbolic AI got
wedged for some reasons, including brittleness. And, you know, and we have learned from that at the
same time, they did discover and understand a lot of things that are extremely relevant. So it's
just not good science to ignore it. So I'm working on an approach to combine, you know, symbolic AI
with deep learning. Again, this is a popular exercise. There are many interesting approaches
out there. As much as I sympathize with them, I think they're all very far from solving the problem.
They are over complicated and not powerful enough. So, you know, I've been working on an approach
called tensor logic that I do believe is as simple as it can be, and as general as it can or needs to
be. And this, you know, it really is a deep unification of the two things in the sense that
it's not just that you combine them using, you know, a neural model that calls a symbolic
one or vice versa, which is a lot of what these things that you have today do and a lot of the
claims that like, oh, this system is neuro symbolic, which it is. It's like, you know,
AlphaGo is neuro symbolic, because some of what it does is symbolic. But I'm talking about something
much deeper, which is once you start doing AI, learning inference representation in tensor
logic, there's just no distinction between symbolic and neural at all anymore.
Can you explain that? So tensor logic, I'm just inferring from that that the the primary
representational substrate is a continuous vector space. Is that right? Are you encoding
discrete information into the vector space? So it's a vector space. Yeah. Right. In fact,
this was the original term that we had for this was vector space logic. But then we changed it
to tensor logic, because it's much more appropriate. But it's, it's vector space in the abstract
algebra sense of vector space, not in the traditional, you know, vectors of numbers. But
anyway, so as the name implies, right, tensor logic is a combination or unification of tensor
algebra on the one hand, and logic programming on the other. So is it similar because Bob
Koeck had a similar idea using like tensor outer products? Is it that kind of? It's related,
but I think it goes well beyond. Okay. Right. And the basic idea is actually
pretty simple. And it's just the following, right, without going into too much, you know,
technical detail. All of deep learning can be done using tensor algebra. Yeah.
You know, plus univariate nonlinearities. Right. So we've got the tensor algebra to do that.
All of symbolic AI can be done using logic programming. And moreover, it has been done
using logic program. So if you can unify these two things, this part of the job is done. Right.
And as it turns out, you can unify them shockingly easily, because a tensor and they so tensor
algebra is operating on tensors, you know, inductive logic, so logic programming, and then
for learning in that logic program and symbolic AI, they are all operating on relations. Yeah.
Right. So what is the relationship between the tensor and the relation? Right. A relation is just
this an efficiently represented sparse Boolean tensor. So at this point, we actually know that
the foundation of these two things is actually the same. If your tensor is Boolean and is very
sparse, now I'm better off representing it with a relation, but at a certain level of abstraction,
nothing has changed. Right. So by this prism, you can look at logic programming and logic
programming is doing tensor algebra. Okay. Just help me understand this a little bit. So, you know,
the main the main criticism of using a neural network as a combined computational and memory
substrate is that they it's a finite state automator. So without having the augmented
memory like a Turing machine, you can't represent infinite objects. That's the main reason the
symbol is, you know, that's the main argument they use. So wouldn't that argument still be
leveled against you? Well, no, because I'm glad you brought that up because there is a very common
misconception. If you realize that there is no such thing as infinity, right. And in particular,
there is no such thing as an infinite, you know, memory, that problem doesn't arise. So there's
the so the unfortunately, a lot of theorists, including computer theorists, they, they foster
this misconception, right. Yeah, there's the Chomsky hierarchy, right, with finite automata at the
bottom, and Turing complete, you know, Turing machines bubble at the top, right. If your Turing
machine has only a finite tape, it's a finite automata. So everything is just finite automata.
Let's get that out of the way, right. A lot of what people do is like completely mistaken
because of that. Now, the fact that everything is finite automata does not mean that everything
is equally good. Some representations are far more efficient, compact, etc, etc, for certain
purposes than others. And the whole game here is that like, I'm not going to solve a finite
automata. The question is like, what do I need to do? Not because I need to go to a higher level
of Chomsky hierarchy, because in reality, they don't exist. But because, you know, I mean,
if you have infinite resources, you could solve a yeah with a lookup table.
But would you would you not? I mean, for example, there was this DeepMind paper that mapped
architectures to different levels of the Chomsky hierarchy transform is I think where, you know,
FSAs, RNNs actually were one step higher. They could represent regular languages and you've got
context free languages. I mean, do you think there's any meaningful distinction between those
language levels? As I said, there is a meaningful distinction, but it's not the distinction that
people usually make. Because once you I mean, you can debate whether the universe is finite,
but certainly computers are finite. So as far as anything that you're going to run on a computer,
there truly is no distinction at this theoretical level between a Turing machine and a finite
automater. That does so like you I can reduce and people have that are papers reducing, you know,
any of these things to any of the others, right? It's like it's a fairly trivial exercise. So at
that level, those distinctions are completely meaningless. However, they are meaningful in the
sense that for many purposes, I am better off having an RNN than having, you know, a transformer.
And for many purposes, I'm better off. So like, let's take, you know, propositional logic versus
first order logic, right? If there's no such thing as infinity, first order logic is reducible to
propositional logic. But that does not mean that it's useless because it can represent a lot of
things exponentially more compactly than propositional logic. If I want to represent the rules of chess
in first order logic, it's a page, right? Yeah, if I want to represent them in propositional logic,
it's more pages that you can have. Okay, well, I think that that's a very, very good point. But I
mean, just just a devil's advocate from the psychologist, you know, do you remember that
Foda and Felician connectionism critique paper, arguing productivity and systematicity,
productivity is all about the infinite cardinality of language. I mean, presumably you would agree
that language has an infinite cardinality. No, well, again, another instance of the same problem,
productivity is very important. But the point to just be a little precisely for a second is,
is to be able to generate a vast number of things beyond the ones that you started with.
Vast, not infinite. In fact, mathematically, infinity is not a number.
Infinity is just a shorthand for something that is so large that it doesn't matter how large it is.
Okay. I mean, at the end of the day, I'm not a mathematician, but surely
mathematicians would push back on this because, you know, infinity is, is a, a quantity in
mathematics. No, I mean, again, people in every field, mathematicians, physicists, computer
scientists are all, are often guilty of, they, they, they, they have this notational shorthand
or like, you know, this terminological shorthand that serves them well. But then they, and then
they use that. And then the newer generations come along and the, and the public also, right,
they don't even realize that what's being talked about is a little bit different.
Infinity is a perfect example. Any serious mathematician will tell you that
infinity does not have the properties of a number. So for example, if I multiply infinity by 2,
I still get infinity. There's no number that that happens to. Right. So infinity is, is not a number.
Right. When I say infinity is not a number, mathematicians might quibble about the way I'm
stating it, but this is a, this is a mathematical truth. Right. Infinity truly is not, I'm being
colloquial, of course. When I say that it's a shorthand for something that is so large that
it doesn't matter how large it is. When you take limits, you know, in calculus, in anything, and
the limit of this blah, blah, as I go to infinity, this is exactly what I'm doing. I'm going to the
point where I'm saying like, at this point, it doesn't matter how large the number is, the result
will be the same. Okay. And in this way, infinitely is an extraordinarily useful concept. So I'm not
here to rail against infinity. I'm just saying like, we really need to understand. I mean,
like, let me give you a very banal example. Right. From the point of view of, you know,
what to have for lunch, right, because some things cost more than others. Elon Musk is
infinitely rich. He does not have infinite money, but it makes no difference whatsoever,
whether he has whatever, a hundred billion or 200 billion to what he's going to have for lunch.
You know, like a street person who has $5 to them, their fortune is not infinite,
because it very much matters what lunch costs, right? So this is the real sense of infinity,
which we can and should use, but we shouldn't confuse it with like, oh, but then your formalism
is incomplete because it doesn't encompass infinity. It doesn't need to, infinity doesn't exist.
Okay. Okay. Well, let's come at it from the other, from the compositionality and
systematicity. So that's all about being able to do, you know, like their main argument was,
when you have a symbolic representation, you can kind of reuse the previous representations
downstream compositionally. And when you take a discrete symbolic representation and you kind
of encode it in the envelope of a vector space, you have a real problem doing that because it's
now like, it's irreversible, that transformation, right? You can't go back to the, to the original
variables. Well, it is reversible if you realize that all those real numbers are actually finite,
right? So notice that real number, there's nothing less real than a real number,
real numbers are imaginary, right? Real numbers are numbers with infinite precision,
which is a monstrosity. And many people have said this, including mathematicians and physicists,
right? The notion of an infinite, of a number with an infinite number of digits is just monstrous.
And again, in particular on a computer, even if you use, you know,
you know, like numbers with unlimited floating point precision, right? It's limited by the
size of your memory. So this transfer from, which is actually very important, and again,
that's what tensor logic largely is about, from purely symbolic structures to embeddings
in a vector space, right? That vector space is still finite. So there's actually nothing
irreversible about what happened there. Interesting. Okay. So how can people, you know,
find out more information about this? And can you just sketch out, you know, just,
just to bring it home to people where they could actually use it and how it would be,
you know, better than what they can currently do? Right. The answer to the first question,
unfortunately, is this is not published yet, but hopefully it will be soon. So for the moment,
there is no very good place to point people to, unfortunately. But that hopefully will be fixed
soon. The question of where to apply it is, our goal for this is that this should become the
language, or hope, I should say our hope, is that this will become the language we're doing
just about anything in AI. So for example, if what you want to do is actually nothing symbolic,
but you just want to build a convent, you can express a convent incredibly elegantly
in tensor logic. Like if you think of, for example, tensor floor or PyTorch versus NumPy,
right, they allow that thing to be said much more compactly, compared to tensor logic,
they are as bad as NumPy is compared to them. Right. Same thing on the symbolic side. But of
course, the real action comes in all the problems where you have both components. The problem with
all those problems, which ultimately is every problem in AI, right? You're always like, what
happens today that is very frustrating. And that's what we're trying to overcome is like,
you start from one of these sites these days, mainly the connectionist one,
which you have a good mastery of. And then the other side, for example, the symbolic one,
the knowledge representation, the reasoning, the composability, you just hack. Yeah. And your
hack solution is terrible. You're like, you're inventing the wheel, you're making it square,
you're trying to make it turn, but it's square, right? It's just, you know, it's a disaster.
And with tensor logic, you can actually have a very well founded, very well understood
basis on either side. So now you don't have to hack either side. Now, there's of course,
little things that you're going to have to hack at the end of the day, because at the end of the day,
you know, AI is intractable and things are heuristic. But this, you know, is,
you know, you know, this notion of a tradeoff, there's very important in engineering, like
people have been exploring different points on this tradeoff curve. The point of tensor logic is
that whatever your application is, we're moving it to a better tradeoff curve. It's still a tradeoff
curve, but it dominates the old one. For any given X, you have a better Y and vice versa.
Okay. And just help me understand because we'll move over to, you know, the discrete program
search and some of Josh Tannenbaum's work in a moment. But there are two schools of thought,
right? There's discrete first and there's continuous first, you're on the continuous
substrate. But usually the reason for the continuous substrate is stochastic gradient descent,
learnability, et cetera, et cetera. And like help me understand. So are you saying we start with
a symbolic representation and then we encode it into the envelope? So where does learning come
into it? No, very good. So in tensor logic, you can do broadly speaking, two kinds of learning.
You can learn the structure of these tensor equations, as we call them, using inductive logic
programming techniques. Again, that whole technology is there. And then once you have that,
you can learn the numbers by the back prop in particular ways called back propagation through
structure, because the structure can vary from example to example, but we know what the type
parameters are. So all of the machinery of inductive logic programming and all the machinery of
gradient descent and deep learning or not, they're both there available to be used as you
traditionally have. Okay. What if I made the argument though, that it's almost like the
inductive logic, you know, like the program search, that's the hard bit. So if you've already
got the program, why do I then need to put it into a vector space? No, actually, these are also,
at the end of the day, in machine learning, we're always trying to learn a program of some kind,
right? The question is like, what is the easiest way to do that? And precisely, the problem with
ILP as with symbolic logic is that's really a couple of problems. One is that if all that you
do, you learn programs that are too brittle, and we don't want them to be brittle, right? And the
other one is that each type of search has its limitations. So in particular, in symbolic AI,
including ILP, we tend to use a lot of combinatorial optimization types of search, right? What we in AI
call search is discrete search. And that is good in some ways, but also very limited in others.
The same thing is true of gradient descent, right? And now to go to that for just a second.
Gradient descent is not a continuous optimization algorithm. It's not, right? Again, those rule
numbers are not infinite precision. There's actually nothing continuous going on in the
computer. Gradient descent truly, literally, rigorously mathematically is a discrete optimization
algorithm. It takes discrete steps. The assumption that gradient descent depends on, which is that
infinitesimally small updates, do not hold. And moreover, in machine learning, as a numerical
analysis, we are constantly dealing with this fact that there's a mismatch between our mathematical
conceptual model of the space that we're working with as continuous with the reality of the computer
that is not continuous. So now this is not, but gradient descent still is a different
optimization technique with some very important advantages, in particular the key, right? The
power of gradient descent comes from the fact that to move from my current point to a better one,
I don't need to try out all the neighboring points, because that takes order of the time
of the neighboring points. I have a close form way to compute what is the best one, right? And
then I move there. And this is absolutely brilliant, right? Like, we don't want to let go of that,
right? This is, you know, Newton's enlightenment is bright idea, right? The price of that is that
in order to do that, you have to make this approximation, which again, calculus is an
approximation. It assumes that certain effects are second order and can be ignored. Now,
ironically, when you learn a large deep network these days, you're actually in a regime where
they cannot be ignored, right? Because these infinitesimal changes are not that infinitesimal,
because you take a finite step, right? The gradient descent is always taking finite steps,
which is why it's a discrete algorithm. And once you take that finite step for any reasonable
learning rate, the total effect of the approximations that you've made, typically swamps
the step that you're taking. So the assumption of calculus that gradient descent is founded on
is actually false. Now, in some ways, this invalidates a lot of our intuitions. In many ways,
and again, this remains to be resolved. A lot of why gradient descent works better than people
expected to is in fact that it's doing something else. It's doing stochastic search, partly because
of the SGD as opposed to being batched, partly because of things like this.
Okay, well, this is really interesting. A couple of places we can go. But first of all, I remember
you did the paper and that introduced elements of NTK theory as well, which might be an argument
against the discreteness of the optimization. But also, I wanted to trade off the two types.
Well, why is there an argument against the discreteness?
Well, with NTK, isn't there like a closed form solution? Doesn't that kind of like
erode the discreteness of the optimization? No, I mean, so there's several things here. But like,
if you have a closed form solution, absolutely brilliantly go for it, right? There's nothing
having a closed form solution in no implies that it's continuous or discrete or any other thing,
right? Let's say there was a closed form solution and it was like an infinite kernel when it
represented some neural network. Doesn't that erode the argument?
Well, first of all, in the work that I've done that I think you're referring to is like,
I have a proof that every model learned by gradient descent is a kernel machine,
yeah, right? And it's something called the path kernel, which is the integral
of the neural tangent kernel over the overgraded descent, right?
Yeah. And now the neural tangent kernel does not assume that your network is infinite.
Most of the theory that people have done with it assumes that the network is infinitely wide,
but the definition absolutely does not require that. And none of what I do, and in fact,
that's part of why, you know, of its power is that it assumes no infinity of anything.
It's for any architecture that you use, and in particular, finite architectures.
Interesting. Okay, so hence the discreteness, but can we come back to this contrasting of
the discrete program search and the stochastic gradient descent on a vector space? Now,
in the vector space, there are certain characteristics, there are certain symmetries,
and even though it's a discrete search through the space, I would argue that it's still
continuous in nature. It has certain characteristics. So contrast those two forms.
Precisely so. Exactly. I mean, I think you've put your finger in now. The whole point of
these continuous spaces, right, is not that they're continuous, because again, that's a fiction,
is that they have a certain locality structure that you can exploit to very good effect. And this
is exactly what gave them to send us, right? Now, that locality structure doesn't have to be
infinitesimal, right? You don't need points to be infinitely close for all this to apply
approximately. And again, they never are, and it's always an approximation. Now, the question is,
do you want to make these locality assumptions or not, right? Making them buys you certain things,
right? But it's also potentially unrealistic in some ways, right? Now,
this actually to take a very concrete instance of this, think of space, right? We model space in
science and physics and in anything as a continuous thing, which it is not, right? Which is not to
say that, and by the way, physicists are coming to this conclusion, right? These days, the prevailing
view is that it's from big thing, is that like, it's, you know, space arises from entanglement,
etc., etc., like space is not the fundamental reality, right? And now, I think that where this
is inevitably going one way or another is that we realize that space is discrete, right? But,
and this is key, it has certain properties, including symmetries like translations, invariance,
rotation, invariance, etc., etc. That whole, approximately, or exactly, but if those hold
and a whole bunch of things like that, then you have, you know, your latent variable structure,
right? It's very well approximated by our notion of continuous space, in which case,
it would be foolish to not use it, right? To formulate the laws of physics and to do computer
vision and so on and so forth. But at the same time, right? If we believe in it too literally,
we walk ourselves into a blind alley. So concretely, look at computer vision, right?
People in the universities of computer vision started out trying to do it with differential
equations and Fourier analysis and all of that could continue with stuff, right? Because that
was the obvious thing to do, right? And it failed. That doesn't work. That's why we need things like
deep learning and, you know, Markov random fields that are discrete grids that use, you know,
to model the images and whatnot, because you are along with the approximate continuity,
you also often have large discontinuities. And if you can only model the world continuously,
you don't know what to do. And the problem precisely is that you have all these phenomena
that are like this, including, you know, in vision, but also in, in turbulence and condensed
metaphysics and so on, you got to realize that there are discontinues and not try to shoehorn them
into continuity when that's no longer appropriate. Interesting. Okay. Well, can we bring in ILP and
can you contrast like the kind of function spaces that are learnable in both methods?
Yeah. So ILP, so let me actually preface this with the following. People in every one of these
schools of AI tend to have this view that I can represent everything in the world using my approach.
So I can like, look, prologue is too incomplete. So why do you need neural networks? But I can also
say neural networks are too incomplete. So why do I need prologue? And in fact, kernel machines
have a representative theorem that says you can approximate any function, blah, blah, blah,
right? So everybody has one of these represent their theorems, right? That says I can represent
anything, right? So in particular, you can do, right? I mean, look, first on a logic was invented
by, by Frague, essentially, to, to model the real numbers. So it can almost by definition model
real numbers, right? Anything you might want to say about real numbers and, and weight and descent
and neural networks. And in fact, people have even done this. You can say it all in, in logic
programming, right? So why not just do that? Well, precisely because certain things are much
more easily done in other ways, right? So what you have to ask about anything, but then about,
you know, not the logic program in particular, like, what things are well represented in this way,
like compactly represented, and then in such a way that learning them and doing inference with them
is easy. And those things are different for logic programming and for things like deep learning,
which is why we need a unification of both. So what is things like logic programming and
NLP good for, right? It's precisely, I mean, it's many things, but the key thing is, it's precisely
for learning pieces of knowledge that can then be reused and composed in arbitrary ways. This is
the huge power symbolic AI that connectionism does not have, right? It's like, I learned a fact here,
I learned a rule there. And tomorrow you ask me a question, and I combine that fact, actually,
several rules by rule changing, right? There's a whole proof tree of rules that could have come
from very different places. And I do a completely novel chain of inference that answers your question.
This is spectacular, right? And this is surely core to what intelligence is all about. And the
symbolists know how to do it. The connectionists don't. But if I was a connectionist, I'd be like,
you know, I know if it was a good one, and the better ones like your show Benjamar doing this,
right? It's like, go and try and understand what those people understand so that you can
then not combine it with those other ideas. Yes. Yeah. Yeah, I completely agree. So a huge part
of intelligence is this symbolic, you know, extrapolation. Yeah. So how do you bring abstraction
into this? Because the thing that I always get caught on is that the traditional go fi vision was
to, you know, handcraft the knowledge. And actually, what we need is dynamic knowledge
acquisition. And we need the ability to create abstractions on the fly rather than just what
we do now, which is crystallizing existing human abstraction. How could we do that bit?
Well, abstraction traditionally was and still is a central topic in symbolic AI.
Right, like be precise. I mean, I think nobody questions that having levels of abstraction,
someone is very important. The only question is how. So if you look at classic knowledge
representation, planning, etc, etc, abstraction is all over the place. If you look at things
like reinforcement learning, and I mean, even like, you know, the whole idea or hope of a
covenant is that it captures objects at multiple levels of abstraction, at least to some degree.
In reality, it doesn't, right? But that's what people are trying to do and not quite doing, right?
Well, good. Let's touch on that then. So I mean, certainly in Jan McCoon's view, I spoke with
Jan the other day, he's got this autonomous path, a paper. And, you know, his system is learning
abstractions, but they're abstractions, which are deducible from base abstraction priors,
like objectness and, you know, basic visual priors. And so there's this assumption that
everything is deducible from the priors that we put into the model. But I have this kind of
intuition that abstraction space is much larger than that. Yeah, I mean, so I would even say that
if you arrive at your abstractions solely by deduction, you have a very impoverished notion
of abstraction. In fact, most of inductive learning is forming abstractions. And from
abstractions at the most basic level, something very trivial is like, I have an example described
by a thousand attributes. If from that I induced a rule that uses only 10, I've abstracted the
way the other 990, right? But if a symbolist was here, they would talk about intention versus
extension, and they would say that, you know, you're selecting from this infinite set of possible
attributes, you couldn't possibly represent all of the attributes in this. I mean, just to give
you a concrete example, you know, you could have A, A, A, A, A, A, A, A, A, you know what I mean?
You just have like this, again, I hate to bring up infinity again, because that's always what
these folks bring up. But how could you select from a set that large?
Well, I don't need to, because it is finite. But what I need to do is so, but there is actually
a good example. And, you know, infinity does not bother us at all, at all there, because what's
like, if my training set, right, is a set of strings, and those strings are A, A, A, A, A, A,
A, right? Going up to whatever number you want to pick, like, you know, a million or a quadrillion,
you know, or a Google, right? Then, is your learning algorithm able to induce that the
language that these rules come out of, right, the grammar is, you know, it's a series of A's,
right? You and I can do that immediately. You know, most deep networks have no end of trouble
doing that, even though it's that basic. So it is a very good example of what symbolic learning
and reasoning can do versus connection is you don't need to go anywhere near infinity to actually
have that be a very elegant example. Well, let me bring up just one other, we've touched on a
lot of great things, right? There's one, in this space of things that we've been talking about,
there's one that I think is very important, which I believe you're also a fan of, and I very much am,
and I think it's going to, you know, maybe going back to the question of what I'm interested in
that's happening at new reps right now or not. So new symbolic AI is definitely a big one.
Another big one, and to my mind, maybe these are the two biggest ones are most interesting,
is what I call symmetry based learning. And these days is more popular known by the
by the name of like geometric deep learning and things like that. I tend to view geometric deep
learning as a special case of symmetry based learning. But this idea of, I think, let me,
you know, to go straight to the punchline, we know that, for example, AI and machine machine
learning in particular, have as foundations things like, you know, logic, probability optimization.
And I think another foundation is symmetry group theory. In fact, I was having, you know, dinner
with with Max Welling just the other day, who, who of course, we've also interviewed and is,
you know, like a great, you know, person in this area. And we, you know, I think we have very
similar views on this. Well, Pedro, yesterday, and Taka Kohen was sitting where you're sitting.
So there you go. Yeah. Again, I remember talking with Taka, who had some ICML many years ago,
where he had published one of the first papers on this. And I was like, and he seemed a little
disheartened by the lack of interest that people had. And I said to him, just wait, this is going
to be big. And we're there now, right? And it's going to be even bigger, I think. But also,
I think to become bigger. And again, to jump straight to the punchline, most of the work,
including me, that people have done to date has been exploiting known symmetries, like, you know,
translation invariance is the quintessential example. And for example, we have something called
deep affine networks that generalize coordinates to, you know, in a rotation, you know, scaling,
et cetera, et cetera. This is all well and good. But I think if this is, and if you look at New
Europe's today, for example, most is in that vein. And there's a lot of good work to be done there.
But if that's all we ever do, we will always remain a niche in AI, with certain very good
applications like science applications, where we know that certain symmetries hold and whatnot,
Max and Taka are doing things like that. But I don't want to just do that. I really, you know,
I'm trying to make progress towards human level AI. And I think the key there is to discover
symmetries from data. And I think most of us agree with this. It's a hard problem, right? But
that's what we're here for. We want to discover symmetries from data. And, you know, there's
an interesting, you know, discussion of how to do that, you know, I have a number of ideas and
a number of people have, then the power of discovering symmetries, right, connecting
back to our early conversation is that symmetries can individual symmetries can be very easy to
discover, because they're often very simple. But then, right, by the group axioms, you can compose
them arbitrarily, which means I can, for example, by learning 100 different symmetries of a cat
from 100 different examples, then I can compose them and correctly recognize as a cat something
that is extremely different from any concrete example of a cat that I saw before.
Could I push back on a tiny bit? So, I mean, in the geometric deep learning proto book, I mean,
they spoke about, you know, the various symmetries of groups like SO3, you know, preserves,
translations and angles, you know, like how primitive and how platonic are these symmetries?
And aren't they just like obvious in respect of the domain that you're in?
No, very good. So, this is actually a key question. Symmetry group theory is one of them.
It's a central area in mathematics that is very highly developed, and it's the foundation of
modern physics, like the standard model is a bunch of symmetries and so on. But the way, and
there is an exhaustive listing of what all the possible symmetry groups are, discrete ones,
you know, continuous ones, you know, so-called lead groups, etc., etc. So, at that level,
this is not naive because people already have a handle on what the space is, right? But crucially,
for our purposes, for AI, that's not enough because precisely because those, again, the analogy with
logic is actually a very good one here. First of all, logic is to brittle, right? And plain
symmetry group theory, the way people have mostly applied so far, is also too brilliant for the
same reason. So, for example, right, something like, you know, people almost always immediately
come up with, say, like, oh, I understand, you know, I like symmetries with the light to recognize,
you know, perturbed digits, but a six is not a nine. So, something like, if you just take
nice symmetry group theory and say, like, well, arbitrary composability, as I was just talking
about, I was like, well, now you've just said that a six, you've lost the ability to distinguish
a six from a nine, right? Now, what we need precisely is to combine symmetry group theory
with the other things like statistics and optimization and say something like the following.
The space of things that you can compose is unlimited. You can have, you know,
unlimited compositions, but, for example, you pay a cost for composing more symmetries.
And now when you find the least cost path, and that's how you're going to match things, or,
you know, your digit becomes less and less probable to be in six, the more you've rotated it,
right? So now we know how to do all of that very well. So we know symmetry group theory very well.
We know how to do all these probabilistic costs, minimizing blah, blah, blah things, machine learning
very well. We just need to combine it to the same way that we have previously combined
these things with first order logic. So I'm glad you brought in the cost. That was really,
really good. So there are trade offs everywhere. I mean, for example, if you want to make the
models more fair and, you know, prioritize the low frequency attributes on the long tail,
the headline accuracy goes down. Same thing with robustness. If you robustify a model,
the headline accuracy goes down. Same thing with symmetry groups. If you introduce other
symmetry groups, you know, that the headline accuracy goes down. So it all comes back to
the bias variance tradeoff at the end of the day. And, you know, where is the limit here?
Like how much can we optimize these models? And what does good look like?
The bias variance tradeoff is a very useful tool, right? But it's not the deepest reality,
right? The way to think about bias variance is that, again, talking about this notion of a tradeoff
curve, there's a tradeoff between bias and variance, right, which is in some sense unavoidable,
right, in machine learning. If you have finite data, you're trying to learn powerful models,
bias variance is a tradeoff. And it's a very consequential tradeoff in the sense that,
for example, the things that work best with small amounts of data tend not to work best
with large amounts of data, right? This is something that we should all, you know, grow up
knowing in machine learning. But so many mistakes have been done because of that, because people
study things in the easy or historically, that's all they had, right? And then they're very surprised
when something that seemed not very good, like say deep learning, right, tends out to be better
when you have a large amount of data, or they believe in like silly things like, you know,
Occam's razor version that, you know, accurate, you know, simply is more accurate and whatnot.
So a lot of mistakes have been made because of lack of understanding of this. Having said that,
what you really want is to move to a better tradeoff curve between bias and variance,
which you can if you get at what the reality is, right? So the real game in machine, once you're
evaluating your learner and figuring out, you're like, how much to prune and whatnot, or how much
to regularize bias variance is very important. But before that, the most important question is like,
what we're trying to do here is figure out what are the inductive biases? What are the
regularities that the world really has, at least approximately, that we build our algorithms on top
of that? And then if you give me a better one than I have now, I'll still have a bias variance
tradeoff, but I'll be in a curve where for the same variance, I can have less bias and vice
versa. And that's where the real action is. Oh, interesting. But I didn't quite understand that
because bias and variance, they are mutually exclusive. And I thought at first, you were
saying, well, if we understand what the biases are better, the prototypical symmetries of the
world we live in, then we can have more bias without having an approximation error, basically.
The confusion arises because bias is a very unfortunately overloaded term.
Right. This is not even getting into the psychological notion of bias, like in Danny
Kahneman's work, or even the sociological notion of bias, like racial biases, gender biases,
and whatnot. So we need to distinguish. So I just used my bad, the word bias into completely
different senses, completely, but not unrelated. That's the thing. One of them is the statistical
notion of bias. That there really is a tradeoff between the two. There's a sum of squares,
blah, blah, blah. The machine learning notion of inductive bias, it's the preference that you
have for certain models of our others, which is really just another way of saying your priors,
whether they are assumptions or knowledge. Maybe actually, instead of bias, they're like,
what you really want to do is figure out what are the priors? What are the model classes?
Where are the preferences? The bias is a kind of preference that really line up with the world
in reality, or the domain, and therefore let you move to a better tradeoff curve
among statistical bias and statistical variance. Amazing. Well, Pedro, just tell us a little bit
about what have you seen at NeurIPS and how's the week been for you?
So we've already touched on some of the interesting things that I saw,
in particular some of the areas that I'm interested in. The thing about NeurIPS is this,
of course, is that it's a vast conference. And in the early days, I used to at least go through
the proceedings and look at the title and maybe the abstract of every paper. And this is now
impossible. Now, these days, if all you do is try to walk through the poster sessions,
you never get to the end. I haven't been to a single poster session in this NeurIPS,
where I actually got through all. I like to go through the poster sessions quickly once,
and then just to see what's there, and then go back to the ones that I found really interesting.
I haven't actually been able to even finish that walkthrough, because they're so vast. You're also
running to people, which is part of the point, and talk and whatnot. But when there's 500 posters
in every session and there's 3,000 papers in the conference, it becomes very hard to find the ones
that are most relevant. Of course, an easy thing to do is look at the, I mean, something
about NeurIPS this year that I honestly thought was absolutely terrible, like a really, really
terrible idea, is that it's a hybrid conference, and their idea of a hybrid conference is that
there are no talks. The talks are all virtual next week. And NIPS this year, to a first
approximation, was one big poster session, which I mean, to me, this is just an incredibly bad idea.
So in that sense, I haven't gotten as much out of NIPS by this point of the conference as I would
have in most years. There's also looking at the papers that were usually selected as oral, but
this time they call them oral equivalent, because there are no oral papers, but they still want
to have that distinction. And the number of those papers these days is 160 or something,
which is bigger than NIPS and ICML were some years ago. And usually from those papers, some of
them kind of like jump out at you as being great and very relevant. I've only looked at them briefly,
right? So don't quote me on this, if you will, but none of those have jumped out to me as like,
oh yeah, this sounds like something really brilliant and that I want to dig into. But there
probably are many, I just haven't really had a chance to look at them yet.
Yeah, I mean, I have a similar reaction. I mean, it feels like we're at the point of saturation
and there are loads and loads of micro variations on the same idea. It's completely overwhelming,
but what I find is that it's a very social experience. When I walk through the posters,
I just immediately become engrossed in conversation and hours go by and I'm just like, oh my god,
what have I just been doing for the last year? And that's the real point is the posters are very
good. It's like the grain of sand and the oyster. The poster is the grain of sand. The
oyster is the conversation that you have with the person at the poster or with other people
around there. To touch on another point that you made that I think is actually important.
So, you know, New Europe and ICML and so on are bigger today than they've ever been. Actually,
not strictly too, because this recently, surprisingly, they tend to say it's gone down a
lot. We can and should ask why, but we need to scale. There are bigger conferences. The New
Science Conference is one conference and it's 35,000 people every year and they make it work.
I don't think, and it's good to experiment. I think, you know, New Europe's at the scale that
is today can work, but it is not working very well. One of the ways in which it's not working very
well is that we need to think a lot more. And I understand this is working. It's hard and people
have day jobs or not. So, this is not, you know, a criticism in that sense of life. We need to
really work on making it easy for people to find the papers that are relevant to them.
Number one, number two, and maybe even more important, there is more machine learning
research today than ever, but in some sense, the diversity of that research is in some ways
lower than ever. So, another point that you brought up and I think is very important to do
with the scaling of New Europe's and the machine learning communities that we have
in just raw numbers, more machine learning and AI research going on today than ever before by
an order of magnitude. But in terms of diversity, there's probably less diversity in the research
now than there was before, which is a tragedy. So, I understand why people have kind of like
converged to deep learning. I'm a huge fan of deep learning. I was doing it before it was cool
as they say and whatnot, but the extent to which like 90% of the community, not just in machine
learning, but AI, is not just pursuing and not even deep learning, but a special type of deep
learning, which you might call applications of backprop, is extremely undesirable. We have
an infinite number of micro-improvement papers along a particular direction that is almost
certainly a local optimum. And we're just digging into that local optimum with ever more papers and
ever more, you know, minimal publishable units when this large amount of manpower that has come
into the field or is moving around, we really need to have a greater diversity of research
in machine learning, within deep learning, within AI. And so like we are making very
poor use of our research, you know, manpower right now. And we see that very much at New
Europe's today. Yeah. I mean, Sarah Hooker talked about the hardware lottery, you know, being stuck
in a basin of attraction determined by hardware, but there's also an idea lottery. It might just
be the case that New Europe's historically has always been very connectionist anyway. I mean,
well, it hasn't, right? That's one of the ironies, but it's okay. I wasn't aware of that. Okay. Oh,
absolutely not. I mean, in fact, the joke is right that NIPs started in the 80s, it was called Neural
Information Processing Systems. And by the 90s, it should have become BIPs for Bayesian Information
Processing Systems. There was this study that they did that one point of predictors of
acceptance and rejection among words in the title. And the biggest predictor of rejection was the
world neural. Really? And this was very famous in the field because indeed, if you could, you know,
1990 something, you were submitting papers to NIPs with the world neural in the title,
you didn't know what you were doing. And then in the 2000s, right, it became BIPs or should have
become BIPs, sorry, KIPs, Kernel Information Processing Systems. And in fact, I remember having
lunch with Joshua Benjamin at the ICML in Montreal in 2009. And we were talking about this, right?
The fact that every decade and, you know, not a new paradigm, but another one of the same paradigm
seems to now be on top, right? And, you know, he asked like, so what is the next decade going to be?
And I said, it's going to be DIPs, Deep Information Processing Systems. And then we both laughed.
And I could tell that I believe this, but he, Joshua Benjamin was actually skeptical of this.
So, you know, the deep and little did we know, right? If somebody told us that, you know, this
is going to be on the page of the, on the front page of the New York Times in a couple of years
would be like, what are you smoking? Right? So the way to which this decade has been DIPs is just
mind blowing, but looking forward, right? And to this point of, you know, diversity in research
approaches, I think if you extrapolate naively from the past, the next decade will be about
something else. And the trillion dollar question is what, what is that else going to be?
Amazing. Okay. You watched Charma's talk, right? Yeah. What's your high level of you?
I thought it was a nice talk. I thought it was a very appropriate talk for an opening talk at the
conference. Actually, if New York's had like some conferences, a dinner talk, right, which is supposed
to be interesting, but not as, you know, deep or as technical as others, this would have been the
perfect dinner talk for New York's, because the topic is very current, right? Our machines sentient.
And, you know, who better to talk about it than Dave Chalmers, right? The world's expert on
consciousness, right? And by and large, I thought the talk was excellent. In fact, you know, when
journalists ask me questions, you know, consciousness is like one of their top three, right? Along with
terminator and, you know, unfairness or something like that, right? And I will point them to this
talk because it kind of like lays out, you know, the, you know, the ground. And, you know, it's good
for people to at least have that, those things in mind. At the end of the day, so I think, of
course, the notion that Lambda was sentient is, you know, ridiculous, as most of us do. You could
ask a slightly more fine-going question was if consciousness is on a continuum, right, which
I think Dave believes in, and if you believe in like this, you know, IT theory and phi and whatnot,
you know, like, phi is never zero, right? So there's always some consciousness, right?
Panpsychism and whatnot. I'm not saying I believe in that, we could go into it, but like, if you
believe in that, then you can ask, well, on that scale, you know, where is Lambda? Where are these
large language models? And surely higher than previous AI systems, right? But in my view,
still very, very, very far. And I think what you want to keep in mind is that consciousness does
not increase continuously. Precisely, there's these transitions where you go, you know, more is
different is the famous, you know, phrase about emergence, right? Consciousness is very much an
emergent, you know, phenomenon. And I think what happens is that there are points at which your
consciousness will leap. Maybe a thermostat does have consciousness, like, you know, or purpose
or whatever, right? Like people in, like people like McCarthy, for example, had that as an example.
But the amount of consciousness is minuscule. And the way I would put that is that these large
language models still have not passed that first threshold. Interesting. So in a similar way to
some of the discussion about large language models, there are kind of scaling breaks in the levels
of consciousness. I mean, Chalmers made the comment, though, that rather than it being a pure continuum,
he said that a bottle was not conscious, but then there was a kind of no yes. So very key point.
Scaling is part of it, but not only. It's not just that. So your cortex to a first approximation
is a monkey brain scaled up, right? There was a module there that evolution discovered and it
really paid to keep making more and more of it. And we can easily speculate why. But the point is,
so let me contrast two things, right, which is true for consciousness, but also for just AI in
general. A lot of people are scaling believers and like open AI is the poster child of this in
the quite conscious ways, like, we're just going to scale the heck out of things. And then a lot
of people like, you know, Gary Marcus being a good example, they just completely poo poo that
they say like, oh, no, this is a joke. Right. And I think the truth is that scaling is good.
Right. Again, you know, part of what we are our intelligence is scaling. But the question is,
what are you scaling? And the things that we're scaling today, it doesn't matter how much we scale
them, we never get to human level intelligence or consciousness. So I think we need some fundamentally
different algorithms, if you want to think at the level of algorithms, or fundamentally different
architect architectures, if you want to think about it in a way, and then scaling those up at
some point will give us consciousness if you leave that it's possible for a computer to be
conscious. But we're not there yet, either in terms of the scaling, although actually scaling is
actually the easier part of this way. We're actually at the point where a computer can have
the same amount of computing power that your brain does, which was not the case before.
But the bigger deeper problem, and the more fundamental one is like, we need the architecture
to scale. Right. And this is where I sympathize, you know, with people like Jeff Hinton, who's just,
you know, playing with, you know, ideas using Mathematica and very small examples, which in
some ways, sounds very underpowered. But I think it's people like that, they are going to come
up with the things that we then scale. As in fact, it was David Röhmelhardt doing that kind of work
that invented backprop. Right. If he hadn't invented backprop, this whole industry would not exist.
So what I think is that the real backprop, the real master algorithm is not there yet,
and we need to discover that first. And then when we scale that up, which will not be
trivial, but will be much easier by comparison, then we'll have, you know, human level,
intelligence, consciousness, etc. Interesting. Okay. And so Charmes is a structuralist,
computationalist. So, you know, he thinks information, not biology. And he's also
a functionalist, right? So, you know, which is very similar to behavior. And, you know,
Hilary Putnam made the move that you can kind of like represent a computation in any open
physical system. And he kind of like used that. You know, if you follow that line of thought,
it almost trivializes computationalism because, you know, it leads to panpsychism very, very
quickly. So, so first of all, I mean, what's your take on this idea that information could
give rise to intelligence and consciousness? So, I agree, like most scientists, and I think in
particular, most computer scientists, that to a first approximation, the substrate does not matter.
And in particular, you're not going to convince me that something is not conscious just because
it's not biological. There is no reason to think that only biological things can have
consciousness. Now, the deeper problem, and you know, indeed, the hard problem is that so,
as Dave Chalmers defined it, conscious. So, there's a basic fork here, which you've alluded to,
which is if consciousness is subjective experience, then all these questions about
consciousness are ultimately unresolvable, because only I have my subjective experience.
I know that I'm conscious. No one can persuade me of the contrary. I don't even know if you are
conscious, let alone some machine, right? So, if consciousness is an intrinsic property of
something that cannot be evaluated from the outside, then we're doomed. We're never going to
answer this question. And maybe that is the case, right? So, I'm not saying that's false,
and you need to always keep that in mind. But now, if we're going to make any kind of progress,
right, we need to look at what are, to generalize a well-known term, the external correlates of
consciousness, right? One of those which has been well-studied by people like Christoph Cokkan
and so on, and I think that's a very good direction, is the neural correlates of consciousness,
right? What goes on in your brain that correlates with consciousness? And we've made a lot of progress
with that. You can also talk about what are sort of like the informational computational correlates
of consciousness. Are there computational structures that support consciousness and the
ones that don't? I think that is also a useful thing to do. Less developed, it actually inches
dispense psychism because it's not like everything is consciousness just because it can compute.
Some computations after this emergence and these phase transitions may give rise to consciousness,
whereas others, it doesn't matter how much of them you have, they will never be conscious.
So, I think this is also a very useful way to make progress on this question, and one to which AI
versus neuroscience or psychology is very well-suited to.
Interesting. So, on the functionalism point, I think Charmes is being very, very consistent.
He uses this kind of calculi to reason about intelligence as well. So, a system is intelligent
if it can perform reasoning, if it can perform planning, if it has sensing and so on. So,
we have this collection of functions and then he's kind of like moved this over to the domain of
consciousness. So, similarly, if a system performs these functions and is used in a positive and
a negative way. So, some functions would indicate an absence of consciousness and some functions
would lead to the presence of consciousness. And it's kind of like leading towards a
touring test for consciousness. I mean, do you kind of support that?
That's a very interesting question. In fact, I was having dinner with Dave after his talk and I
actually brought this up because it wasn't clear from his talk and I said, look, this is the answer
that I usually give to journalists when they ask me, will machines ever be conscious and whatnot
and ask me if he agreed with it and actually expected him to disagree. But I think, again,
they want to put words in his mouth, but that he agreed. And the answer is the following, is that
human beings, as we've discussed, have an amazing tendency to anthropoformize things. It's reasoning
by analogy. And what happens, I used to say, this is what's going to happen at this point,
is this is what is already happening, is that as soon as a machine behaves externally,
even vaguely like it's consciousness, we immediately start treating it as if it's
consciousness. So if you look for 10, 20, 50 years from now, we will just treat AIs as if
they're consciousness and people won't even ask that question. They will assume AIs are conscious
in the same way that we assume that each other, that we're conscious. But then, and so like from
that pragmatic external point of view, maybe the question is answered, but you could be a philosopher
or like sort of like a very rigorous, technical person. And so like, no, no, no, no, I really
want to know if things, they may look conscious from the outside, but are they really? But that
question, as far as I can tell, unfortunately, at the end of the day, is probably unanswerable.
Now, there's a middle ground between these two things that maybe is where we'll wind up. And
to me, sounds like probably the best thing that we're going to be able to do, which is that our
understanding of the neural, informational, etc. correlates with consciousness evolves to a point
where we have the feeling that we do understand consciousness. It's not just the late person
calls this consciousness even though haha, it's not like lamb is not conscious, you know, poor bozo,
etc. etc. It's like, you know, there are many analogies to that in the history of science.
There used to be a lot of things that were like magical, right? And we were like, oh, we're never
going to stand like life was magical, right? Life did not obey the laws of physics. It's just
something else, right? This sounds laughable right now, but it wasn't laughable at all then, right?
And now, it's not like we've understood everything about life very far from it. When you say like,
there's DNA and their cells, and this is how it all arises, right? And I think we're at the
point in consciousness where it's to like, oh, consciousness is some so beyond us, right? I think
we will get, you know, there will be a structure of DNA moment in the history of the study of
consciousness. And I think, I think things like Phi and this, you know, IT3 and whatnot,
they're very brave attempts to make progress in this direction. I think, you know, like,
Julia Tononinaway is, you know, very deluded in thinking that he has nailed what consciousness
is, right? I think, you know, Phi maybe is an upper bound on consciousness, but with steps like this,
hopefully at some point, and very much with the help of AI, right? AI is really useful for this,
because it's a brain that might be consciousness that we have a lot of control of. And you can do
experiments that you can't, you know, with people, right? So I think we will make at least some progress
in that direction for sure. Maybe to the point where we feel that, yes, we do understand what
consciousness is, we're not asking ourselves that question anymore. And then we can point to things
and say, this is consciousness, this is that kind of consciousness, that amount of consciousness,
and so on. Yeah, that's really interesting. I agree, we're making a lot of progress in getting
a handle on this. And although the biggest game in town is still the computationalism game. And
as you say, historically, the only alternative was Mysterianism. And my friend, Professor Mark
Bishop, that he said that that's one of the reasons why he's become interested in the
foreseeing cognitive science, because for the first time, it's given him a kind of robust
alternative to computationalism. But just coming back quickly, you know, as Charmley's reference,
Thomas Nagel, you know, which is that it is something it is like to be a bat. What do you
think about that? So I'm not sure your question is, but let me show you what you mean. Do you agree
that there is something it is like to be a bat? Oh, absolutely. Right. So there is more and more
than that, right? There is something that it's like to be a bat. And it's very different from
being a human, right? And we grossly underestimate, right? Again, we do this thing that again, it's
a heuristic, it works very well. It's like, we project ourselves into the bat, because what else
could we do, right? But then what you see is a bat seen through the mind of a human, right? And in
fact, there's this famous, I would say even more famous, you know, notion from, from Wittgenstein,
right? That if the lion could talk, I would not understand anything that the lion was saying.
Because his world is so different from mine. Now, I actually think, I think this is a very
important position to, as a reference point, right? Certainly defensible one, and you know,
Wittgenstein was a good defender of it. But actually, I think that this is going too far.
I think, ultimately, I may never be able to completely know what it's like to be a lion,
but we can make a lot, don't underestimate us either, right? We can make a lot of
in, was it to understanding what it's like to be a lion? Much more than we understand today.
Same thing for a bat. And you know, you could have also asked that for a fruit fly, right?
In a way, a fruit fly is more different from us than a lion, but it's easier to understand,
right? Because at some level, that thing is so simple that we can understand what's going on
with it because it's not that deep. Yeah, that's a beautiful quote, actually. So,
so closing this off, do you think that large language models are slightly conscious or will
be in the near future? I think language, I think large language models are not slightly conscious
by the reasonable, you know, everyday definition of the world slightly, meaning that their consciousness.
So I did, I think that either their consciousness is just zero, right? If somebody asked me like,
you know, how much consciousness does lambda have? Tell me in one word and the answer would be zero,
right? But another answer which is hard to distinguish from the first one is epsilon,
right? Maybe it has a very tiny amount of consciousness, but it's so tiny that it doesn't
even qualify as slightly. Again, this gets back to what its architecture is. It actually gets
too lot of things, but for purposes of this discussion, right, lambda and these large
language models are not very different from a big lookup table. Any big lookup table is not conscious.
Now, I mean, there are a lot of interesting distinctions that you can make it well.
What if what I have is an efficient approximation to a lookup table? Isn't that what your brain is,
right? And I would say yes. And then people say, well, but then why is your brain conscious
but not the lookup table, right? And precisely, the interesting question is that the consciousness
comes about from the fact that you have to concentrate all of this information, you know,
in real time into something, you know, very compact and that leads to action continuously,
right? So to put this in another way, maybe God is unconscious because he doesn't need to be,
right? If you're omnipotent and omniscient, you don't need to be conscious. You are effectively
just a lookup table. Exactly. And I loved your response earlier about the grain of sand and the
oyster. I thought that was a beautiful way of looking at it. And having recently studied so,
I mean, personally, I think it's a lot to do with intentionality and agency, but I remember you
responded to that. Just final quick question. What's your definition of intelligence?
So let me start with the technical definition, which is unfortunately not widely known enough
and not appreciated enough. But I think it's a really important one to have, right? Intelligence
is solving NP-complete problems using heuristics. This is the real technical definition of AI,
right? And there's a lot packed into that, right? The fact that it's NP-complete problems and the
fact that it's using heuristics. If your problem is solvable with a lookup table with polynomial
algorithms, you don't need intelligence and there's no intelligence there. It's when you start solving
hard problems using heuristics that you're getting into the realm of intelligence. Moreover, NP-complete
is not the same as exponential, right? The crucial thing about an NP-complete problem that connects
very directly to our entire discussion of utility and whatnot is that the solution is easy to check.
This is the key. If you're working on problems whose solution isn't impossible to check
effectively, I can't even tell if you're intelligent or not. The whole thing about
intelligence in humans and machines is that how you solve the problem requires a lot of
intelligence, a lot of computing power and whatnot, but then I can easily check the solution.
Now, hang on a minute, could that set a step away from behavior then if you're saying that,
you know, like you have the percepts, the state and the action and you're saying the state is also
important? No, so to answer that head on, intelligence is not behavior, right? Intelligence to give a
slightly more general definition and then there's several and they all have their merits. Intelligence
is the ability to solve hard problems. Then more concretely, it's NP-complete problems and using
heuristics, but like, for example, if you create an AI system that cures cancer, it doesn't behave
in the sense that a human and a robot behave, but, you know, it's damn intelligence, it's
more intelligent than we are, right? It would be childish to deny intelligence to that system,
no matter how it solves cancer. If it finds a ridiculous simple way to solve cancer,
then it's even more brilliant, right? In fact, the simpler your outcome, the more intelligent you
are, right? It takes intelligence to produce something simple. Wow. Concretely, in many
circumstances, in particular evolution, right? Intelligence manifests itself as behavior. There's
a sequential decision-making problem. There's an agent in the world that said a certain stuff,
being a stochastic parrot. And I think also from, you know, theoretical reasons, by analyzing what
a transformer can represent and how it can learn, my best guess, which could be wrong again, I don't
think anybody has the answer to this and it's interesting question is that those transformers,
right? Not LLM scholars, that means more of like a task rather than the, you know, the architecture.
Transformers have a certain limited ability to do compositionality, very limited to compared to full
logic programming, etc., but exponentially better than something like an ordinary multilayer perceptron.
Yeah. And if you just, I mean, even a multilayer perceptron or any learning algorithm is more
than a stochastic parrot because it's general, the whole point of machine learning is to generalize
beyond the data. If you generalize correctly beyond the data, you're not just a parrot anymore.
And, you know, I think it's not an accident that that term stochastic parrot came from Emily Bender,
my linguistics colleague at UW, who does not understand machine learning.
She's a classic linguist of the Chomsky and Variety, who does, you know, does not fundamentally
understand what I think, you know, she might disagree, what machine learning is all about.
And she would probably look at any learning algorithm and say that it's a stochastic parrot,
missing the fact that the whole point of machine learning and the thing that we focus on from,
you know, beginning to end is generalizing. And as soon as you're generalizing correctly,
even if you have no compositionality, you're already doing something that's,
that has a little bit of intelligence, and that's beyond what a parrot would do.
Yeah. I mean, to be fair, it's not, it's not a binary. And at the time, I thought
they were stochastic parents as well. I've updated my view. And you were talking as well about
creativity. There's a kind of blurred hyperplane of creativity. And we discussed, you know,
where that hyperplane sits. But, you know, what's really interested me, I've interviewed quite a
few people that are working on, working on in-context learning in these language models. And it
seems like these language models are almost, almost like a new type of compiler, you know,
you're writing a program inside the language prompt. And they seem to work extremely well
outside of the training range if you're doing like basic multiplication tasks.
I think it is useful to look at them as a new type of compiler. In fact, I've been saying for a long
time that, you know, like, there's this continuum from programming in assembly code to high level
languages to doing AI, right? The point of AI is to continue along that path to making the language
that computers speak ever closer to ours, so that we can just program them by talking to them or
writing things at them, right? Having said that, I think that, you know, what goes on in the innards
of a transformer, right, is actually still very primitive for lack of a better word, right?
There's a lot of, so something I tweeted that got a lot of follow-up from people like Yan and Gary
and Huda Pearl, because they were all bringing in their own angles. So this was like, I said,
and I think this is an interesting question. It's like, the interesting question about
transformers is what needs to be added to them to get real intelligence. So we should not deny
what they have, like the attention mechanism in particular, right, and the embeddings.
And so like, there are two very important things in transformers that are beyond what
was in neural networks 10 years ago and are key. One of them is attention, right? Attention is a
real advance. And the other one is context-specific embeddings, right? Each of these ideas is
important in its own right, and combining them together is very powerful, right? Again, because
the context-sensitive embeddings get that this similarity part of intelligence, the attention
combined with the context-sensitivity of the embeddings gets at the compositionality part.
So they do have, so there are a couple of steps forward on the road to human-level intelligence,
but there are many more. And rather than either saying like, oh, they're just parrots,
they don't do anything, or saying like, we've almost solved the AI, we should try to understand
better how the attention and the context-dependent embeddings work, which we don't. But we also
need to focus on like, now, what are we still missing? Because we definitely are. And that's
really where most of our focus should be. Yeah, I completely agree. And also just in defensive
end room. And I think she's a brilliant linguist. And I personally think having that diversity of
views from different people is useful. No, I mean, so I very much think that having a diversity
of views is very important. And I think something that I'm always saying to my deep learning friends
who can't stand, you know, who hate the guts of Gary Marcus is we really, really need informed
critics. And very typically, your informed critics are not people in the field. We are experts, but
then we also suffer from the distortion of being experts. It's people in adjacent areas. And people
like linguists and psychologists are very much those people. They're in adjacent areas enough to
have a good critique of AI. So for example, something that Jan is always throwing at Gary
Marcus that kind of doesn't sit well with me says like, well, you should try building a real system
sometime and you can't criticize us until we do. If we take the attitude that only engineers can
criticize engineers, we're doomed. Right. Having said that, there is a very big distinction between
the knowledgeable informed critics like Gary Marcus and the not so knowledgeable, not so well
informed ones, which unfortunately, Emily is an example. I mean, she's my colleague at UW.
And I've talked with her about some of these things. And her criticism of machine learning,
unfortunately, like a lot of people comes from a place of actually not fundamentally understanding
it very well. But people do people do say that Gary isn't an expert in deep learning and that he's,
you know, attention seeking. What would you say to that? No, he's, he's, he's not an expert in
deep learning. And, you know, so like, I agree with some of his criticisms, I disagree with,
with others probably on balance, I disagree more with him than I agree. But so first of all,
there is a value to having critics like that number one, but then number two, the reason his
criticism, I mean, it would be better if he was also an expert in deep learning and made the
same criticisms. And then the problem is that often his criticisms are wrong because he has
a mental model of deep learning that is already outdated, or is oversimplified, right? But that
to some degree is unavoidable. But the thing that makes his criticism valuable is that he's doing
it at a level where on a good day, on a bad day, his criticisms miss the mark. But on a good day,
which is the ones that matter, his criticism is actually useful because it's at a level where
you don't need to understand the details. It's like, you claim to be producing intelligence.
I as a psychologist know a lot about intelligence. That's what I study for a living, right? He knows
more about aspects of intelligence than I do. And from that point of view, what you're doing
is lacking. I mean, he's written the whole books about, again, because this goes back to when he
was a PhD student and symbolic learning and whatnot. The deep learning folks have repeatedly
underestimated how well he understands some of these problems because as a psychologist in
particularly interested in language learning, he's actually thought very long and hard about them.
Oh, I know. So I've read his book and we've had him on the show three times.
Which book?
The Algebraic Mind.
Yeah. So that's the most relevant one here.
Yeah. And as a psychologist, you know, he spent a lot of time studying how children learn rules
and he talks very elegantly about a compositionality. And we've spoken about this. It's irrefutable.
And I agree with him and we've supported him. I guess some of the things he argues are based on
ethics, politics and virtue and some of the things like compositionality I think are irrefutable.
I mean, I think irrefutable is a very strong word. I wouldn't say that they're irrefutable.
I would say that they have very strong backing, which the connectionists have not been able to
effectively refute. But some of the criticisms that they have, meaning people like Pinker and Prince
and whatnot, famously of connectionists in the 80s, some of them are still valid, which is very
salient, but some of them not really. And again, to go back to the daddy of this whole school of
thought, who's Chomsky, right? He made his name basically panning things like Markov models of
language, in-gram models, which you could say large language models are just a very glorified
version of, right? And at the time, that criticism was very apt and timely and it was useful, right?
But in the famous is like, you can't learn a context-free grammar, but context-free
grammar is what you do. Well, actually, now we know formally that you can learn a context-free
grammar and, you know, because you only have to learn it probabilistically, which is what we do
and what our systems do. So his criticism was just, you know, mathematically off the mark.
But also, when you look at systems that do speech, language, etc., etc., it is that statistical
approach that he made his name panning that has prevailed. And for reasons that we understand
very well, and large language models are just the latest greatest expression of that. So at that
level, all Chomsky and Pinker, Gary Marcus' view of things, not only is it not irrefutable, it has
been refuted. Okay. Let's just quickly come back to your definition of intelligence. So solving NP
hard problems, I assume you would zoom out a little bit and, you know, it's more of a meta
learning algorithm. So the ability to solve different problems. Yes. So it's a very good
point. If all you have is the ability to solve one NP complete problem that does not qualify as
general intelligence, right? There's like, these days, this is a common definition to make this
difference between, you know, narrow intelligence and general intelligence and AGI and whatnot,
right? And if you only solve one NP complete problem very well, you have narrow intelligence is the
way I would put it, but you do not have general intelligence. General intelligence is precisely
the ability to solve a limitless variety of problems, all that have this characteristic of
they're hard to solve, but the solution is easy to check, right? I mean, if you have the ability
to solve problems, whose solution isn't easy to check, then maybe you're intelligent, but I can't
decide whether you're intelligent or not. Interesting. Okay. And actually, Gary did, he put a paper
about 20 years ago talking about how neural networks can't extrapolate. I think it was when
he encoded numbers with a binary encoding or whatever. And we've been on a bit of a journey
on this. So we had Randall Bellastrier, I've interviewed him yesterday, he's got this paper
called the spline theory of neural networks, it basically says that a neural network
decomposes an input space into these input activated polyhedra. And when we first read
that, we felt that it kind of indicated Francois Chollet's assertion that neural networks are
locality sensitive hashing tables, and they only generalize within, you know, these tiny polyhedra.
And Randall's now updated this view to say in contrast to decision trees, these hyperplanes,
they actually inform a lot of information in the extrapolative regime outside of the training
rate. So I always thought it was the inductive priors that gave the extrapolative performance on
neural networks by photocopying the information everywhere. And so like, you know, this is a
great example of where, you know, Gary might update his views because even basic MLPs are far
more extrapolative than anyone realized. This is a very interesting question, but the way I would
put it is that in that regard, in some sense, both of the sides are right. And the reason they both
right is that we're in very high dimensional spaces. Yeah. And we're in a very high dimensional space,
the follow thing can happen, which is, you know, you have a data point, and you generalize to a
vast region around that data point. And it's unfair to characterize these things as saying
they just interpolate. In some sense, they really do extrapolate. But at the same time,
that vast region that they generalize correctly to is an infinitesimal fraction of the much,
much vaster reason that they have not generalized to be UNI can. So you got to keep that distinction
in mind. And then in particular, right, I like to say that deep learning is nearest neighbor in
curved space. And both parts of that are very important. Right. So, you know, Jan Likun was
famous, you know, during the glory days of kernel machines for saying that kernel machines are just
glorified template matches. Right. And of course, they didn't earn him any friends, but he was right.
They really are just glorified template matches. Kernel machine is really a souped up more
mathematically elegant and blah, blah, blah version of nearest neighbor. Right. And the nearest
neighbor is just a template matcher. The beauty in the power of nearest neighbor, though, is that
there is a neighborhood within within which often it generalizes very well. Right. Now,
I think what Jan was missing, and I probably still is, is that coordinates and deep learning,
they are still just a glory. They are also glorified nearest neighbor, except more glorified.
And the way in which they're more glorified, which is very important is that they are doing
nearest neighbor in curved space. They are still just doing, you know, generalization by similarity,
which you could argue is all that machine learning does is generalizing by similarity.
Another notion of similarity can vary. Right. But the important thing that they've done is that
nearest neighbor just uses some distance measure in the original space, whereas the neural networks
are warping the space to make the problem easier for the nearest neighbor, you know, essentially
dot product based similarity computation that they're actually doing.
Oh, sure. But you're very much arguing, this is the way Francois Chouelet puts it, that, you know,
you have all of these transformations and you kind of distort the space, you know,
to represent the data manifold. And, you know, you want it to you stop SGD at the right time
so that you approximate the data manifold and you can do this kind of latent space, you know,
interpolation on the geodesic of that manifold. But, you know, Randall's idea is completely
away from that idea of, you know, these models learning this curved space. And so if you do
slice the space up with these hyperplanes, rather than it being a locality prior, which is what
you're talking about, these hyperplanes give you globally relevant information to things that are,
you know, miles away from the training data. Yes. So but these two perspectives are
more similar than you might think, because I can take a distorted version of space and decompose
into polyhedron. And one or the other might approximate what's really going on better. I mean,
these new networks do form curved spaces, except they're in practice, they're not curved because
they find it. But ignoring that, right? Let me put it this way, an eloquent example of this is,
if you look back at the original space, right, again, treat this thing as a black box,
where does it generalize to? Does it generalize only to things new networks as we have them today?
Does it generalize correctly only to things that are locally near the data point, or you can generalize
well to things that are far, right? And the thing is that with nearest neighbor, you buy, you know,
almost intrinsically, you only generalize period at all to things that are local.
The beauty of deep learning and of the space working that's going on is, again, going back
to this notion of the path kernel is that you're actually doing a nearest neighbor computation,
not just in a space that's what, but you're doing it in the space of gradients, which actually means
that you can generalize correctly to things that are very far from your examples,
except they look similar in gradient space. A very simple example of this is a sine wave, right?
If I try to learn a sine wave using nearest neighbor, I need an infinite number of examples,
right? Because, you know, like what I've learned over here helps me not at all with the next
turn of the sine wave, like that continuous extrapolation, right? At some point, there's
this disaster where if the last piece of the sine was going up, I just keep going up and getting more
and more wrong, right? And in fact, this kind of thing does happen in neural networks, but they
also have the part to say like, and this again, this also happens, which is I'm going to transform
this space more into a more intelligent one, which is the space of the slopes, right? And now,
if I've seen one cycle of the sine wave with some density of examples, by similarity in that
transformed space, I generalize correctly and trivially to every other turn of the sine wave.
So there's a very big fundamental difference between the two.
Interesting. And you think with an MLP, it would be possible to have that kind of extrapolative
generalization on a sine wave? Well, so people have studied this in multiple ways. And the problem,
so the question is, it depends on what are the basis functions that it's using.
So something that we didn't allude to at all in this conversation, but analyze all of this is
like, what is your choice of basis functions, right? And the thing is an MLP with the traditional,
say, sigma or a little basis functions will not learn this no matter for obvious reasons, right?
And again, you can represent it, right? The representative theorem is there. Like,
the sine wave is just one sigmoid and then another one, you know, with a minus sign and then another
one. But the data doesn't let you learn it. Yes. Right. If as a basis function, you have sine waves,
which is nothing unimaginable, that's what a Fourier transform is then, then you can learn it so easily,
it's not even funny. So it depends dramatically on the basis function. And the question really
becomes, what are the basis functions and the architect of it? Let me generalize correctly
through a lot of things, including this, such that, for example, and this is a very simple test,
is like, I can nail a sine wave with a small number of examples without it being one of my
basis functions. Yeah, exactly. And then this all comes back to, you know, we're talking about
inductive prize and the bias variance tradeoff and even symmetries, actually. I mean, the
Taco cohen once said that, you know, if you encode all of the symmetries into the label
function, then you would only need one labeled example. So it's always a tradeoff between how
much induction are you doing? Well, interesting, you should say that I understand why he says that,
and it's not technically wrong, but I would say that practically what you need is such a set of
symmetries per region of the space, per cluster, right? But, you know, in another way, I would
actually make an even stronger statement, which again, is very, very perfectly mathematically
interesting. An object is just the sum of its symmetries or a function. If you tell me all
the symmetries, every last one of an object, you've defined the object. So if I can learn the
symmetries at that level, I don't need anything else. Of course, as we already discussed,
that's not the whole answer. Likewise, with any function, if you tell me all the properties of
the function, there are there, you know, to be more precise, all the symmetries of a function
at some point, you've told me the whole function. And vice versa, from the function, I can, you know,
I can read out all the symmetries that it has. In principle, doing that in practice can be,
you know, a very difficult and subtle thing to do. That's a beautiful thing to say. You give me
the symmetries and I'll give you the object. Yeah, exactly. Amazing. Professor Pedro Domingos,
thank you so much for joining us today. It's been an honor. Thanks for having me. Amazing.

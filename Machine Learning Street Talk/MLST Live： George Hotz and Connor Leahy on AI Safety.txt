So I'm hoping that we'll just go straight live and see.
I got to mute that so it doesn't echo.
Yeah, okay.
I got to hide chat.
I can't watch chat.
And these YouTubes are so, I'm just going to minimize this.
Okay, excellent.
We are live.
So we've got a few minutes, folks, before the live kicks off.
I just thought I'd run the stream just so that we know we're live and ahead of time.
We were just joking about Hotz's smack talk yesterday on his live stream.
And I very much hope that the subject of Von Neumann comes up.
Oh, yes, yes, yes.
I did see it.
I saw a Twitter comment that it was going to be Von Neumann versus Chicken Man.
A thousand Von Neumann versus Chicken Man.
We are treading new debate territory, hitherto unknown to man.
Chicken Man came up on a stream.
I look forward to finding out what Chicken Man is.
Yeah, sorry about that.
I just heard myself coming back.
Okay, well, yeah, this is going to be amazing.
God, two of the heavyweights live here on MLS T.
You've only got another minute or so, folks, before we kick off.
No, we were just talking about how, you know, these arguments have been refined over a lot of lunchtime talks.
Yeah, I think you have a stable position, and I think I have a stable position.
I think some of the other guests I've seen you debate don't exactly have some of the things are just, you don't really think this through all the way.
So I think I thought it through to the end.
Fantastic.
I really look forward to it.
Did you watch Rich Sutton just did a play?
Oh, no, I have not. Oh, God, I can only imagine.
I love Rich Sutton.
Oh, man, you know, bitter lesson.
He knows AI.
He is a character.
He is definitely one of the people of all time.
Yeah, yeah, yeah, yeah.
Yeah, the man is a legend.
Well, I guess we should slowly kick off, folks.
So I'm just going to do my spiel before I hand over.
And first of all, too hot, so that's okay for your temper.
We'll talk about that.
Are you okay?
Well, the thing is, Connor always likes to go first.
And I think that, you know, Connor's mental model might not be robust to going second.
Would you be okay going second or you want to go first, Connor?
I'll start, but it won't be a full intro. How about that?
I am completely robust to all permutations.
Adversarial training.
Amazing, amazing.
Okay, well, in which case, let's crack on.
So, ladies and gentlemen, get ready to meet the cunning Maverick of Silicon Valley,
the one and only George Hotz, renowned for his daring exploits,
Hotz commands an enigmatic persona which merges the technical finesse of Elon Musk
and the wit of Tony Stark and the charm of a true tech outlaw.
Now, many of you would have or indeed should have seen this man on Lex's podcast recently
for the third time, no less, from craftily jailbreaking the supposedly invincible iPhone
to outsmarting the mighty PlayStation 3, he's proven that no tech fortress is impregnable.
Once targeted for his audacious creativity by Sony with a lawsuit,
this hacker wizard stoically danced past the curveballs thrown by the tech giants,
all achieved with the graceful swag of a street smart prodigy.
Now, when he's not outfoxing major corporations, you'll find him at the heart of the avant-garde
of AI technology, gallantly trailblazing through the wilds of the tech front here.
He's currently building a startup called MicroGrad,
which is building superfast AI running on modern hardware,
and truly, he's the James Bond of Silicon Valley, minus the martinis, of course.
Now, please welcome the unparalleled code cowboy, the unapologetic technomancer, George Hotz.
Whoo!
Anyway, also joining us for the big fight this evening is the steadfast sentinel of AI safety,
Conor Lehi. Undeterred by the sheer complexity of artificial intelligence,
Conor braves the cryptic operations of text generating models with steely resolve.
Now, about two years ago, Conor took on the Herculean task of safeguarding humanity
from a potential AI apocalypse. His spirit is relentless, his intellect razor sharp,
and his will to protect is unwavering.
Now, drawing on his contentious claim that we are super, super fucked,
Conor channels the urgency of our predicament into his work.
Now, his startup, Conjecture, isn't just a glorified tech endeavor, but it's a lifeboat for us all,
racing against the breakneck speed of AI advancement,
with the fates of nations possibly at stake.
He's determined to break the damning prophecy and render us super, super saved.
So, brace for a showdown as Conor Lehi, the maverick defender of AI's boundaries, strides into the ring.
Now, the man who declared we are super, super fucked is here to prove just how super, super not fucked
we could be if we make the right decisions today.
So, please give it up for Mr. Conor, super, super Lehi.
Now, Conor, I'd appreciate it if you don't go down in the fourth, I want this fight to go the distance.
Now, we're running for 90 minutes this evening.
There will be a 10 minute openers from, we said hots, didn't we, from hots first, and then Conor.
And I'll only step into the ring if the punch-up gets too out of hand.
And unfortunately, we won't be taking life questions today because we want to maximize the carnage on the battlefield.
So, Conor Lehi, your opening, sorry, George Hots, your opening statements, please.
Um, yeah, we're super, super fucked. I think I agree with you.
Well, that was a short fight.
Yeah, look, I think, okay, so to make my opening statement clear and why maybe it doesn't make that much sense for me to go first,
I think that the trajectory of all of this was somewhat inevitable, right?
So, you have humans over time, and you can look at a 1980 human and a 2020 human.
They look pretty similar, right? Ronald Reagan, Joe Biden, you know, that's all the same.
Whereas a 1980 computer is like an Apple II, and a 2020 computer is a M1 Max MacBook, like lines looking like this, right?
So, you have one line like this, one line like this, these lines eventually cross, and I don't see any reason that line is going to stop, right?
I've seen a few of the other guests argue something like, well, LLMs can't problem solve, but it doesn't matter.
Like, if this one can't, the next one will, whatever you call, I don't believe that there's a step function.
I don't believe that like, oh, now it's conscious.
Oh, now it's intelligent. I think it's all on a gradient.
And I think this gradient will continue to go up, will approach human level, and will pass human level.
Now, this belief that we are uniquely fucked because of this, the amount of power in the world is about to increase, right?
When you think about power and you think about, straight up, you can just talk about energy usage.
The amount of energy usage in the world is going to go up.
The amount of intelligence in the world is going to go up.
We may be able to do some things to slow it down or speed it up based on political decisions, but it doesn't matter.
The trajectory is up or major catastrophe, right?
The only way it goes down is through war, nuclear annihilation, bio annihilation, meteor impact, some kind of major annihilation.
So it's going up.
What we can control and what I think is super important we control is what the distribution of that new power looks like.
I am not afraid of super intelligences.
I am not afraid to live in a world among super intelligences.
I am afraid if a single person or a small group of people has a super intelligence and I do not.
And this is where we get to chicken man.
A chicken man is the man who owns the chicken farm.
There's many chickens in the chicken farm and there is one chicken man.
It is unquestionable that chicken man rules.
And if you believe chicken man rules because of his size, I invite you to look at cow man who also rules the cows and the cows are much larger than him.
Chicken man rules because of his intelligence.
This is basic less wrong stuff.
Everyone kind of knows this.
How the squishy things take over the world.
I agree with Elias Yudkowski all up to new fittings, right?
So I do not want to be a chicken.
And if people decide they are going to restrict open source AI or make sure I can't get access to the compute and only trusted people like chicken man get access to the compute.
Well shit man, I'm the chicken.
And yeah, I don't want to be the chicken.
So I think that's my, are we fucked?
Maybe.
I agree that that intelligence is very dangerous.
How can you look at intelligence and not say it's very dangerous, right?
Intelligence is somehow safe, but things like nuclear bombs are an extremely false equivalency because what does a nuclear bomb do besides blow up and kill people?
Intelligence has the potential to make us live forever.
Intelligence has the potential to let us colonize the galaxy.
Intelligence has the potential to meet God.
Nuclear bombs do not.
They just blow up.
So I think the question and like, you have things like crypto, which are a clear advantage to the defender at least today.
And you have things like nuclear bombs, which are clear advantage to the attacker.
AI, it's unclear.
I think the best defense against an AI trying to manipulate me.
And that's what I'm really worried about future psyops, you know, we're already seeing it today with the voice changer stuff.
Like, you never going to know who's human.
The world's about to get crazy.
The best defense I could possibly have is an AI in my room being like, don't worry.
I got you.
It's you and me.
We're on a team.
We're aligned.
I'm not worried about alignment as a technical challenge.
I'm worried about alignment as a political challenge.
Google doesn't like.
Yeah, it doesn't like me.
But me and my computer, you know, we like each other.
We're aligned.
And we're standing against the world that has always, since the beginning of history, maximally been trying to screw you over.
Right.
Intelligence.
People think that one super intelligence is going to come and be unaligned against humanity.
All of humanity is unaligned against each other.
I mean, we have some common values, but really, come on, everyone's trying to scam everybody.
The only reason you really team up with someone else is like, hey, man, what if we team up and scam them, right?
Hey, what if we team up?
Call ourselves America.
And we build a big army and say, we're free and independent.
Yeah, right?
It's that force that has made humanity cooperate.
Humanity by default is very unaligned and has every kind of belief under the sun.
So I'm not worried about AI showing up with a new belief under the sun.
I'm not worried about the amount of intelligence increasing.
I'm worried about a few entities that are unaligned with me acquiring godlike powers and using them to exploit me.
I think that's my opening statement.
Cool.
Yeah.
Thanks.
That's, I mean, yeah, I also kind of agree with you.
And most of the things you say, it's a few details I'd like to dig into there.
But for most of the things you say, I do think I agree with you here.
I think it's absolute.
Let me just like start with saying, I totally agree with you that misuse and like, you know, bad actors, what using API is a horrible, dangerous outcome.
That's, that's like, you know, sometimes the less wrong, you know, crowd likes to talk about X risk, but also sometimes I've talked about S risk, suffering risk.
So things are worse than death.
I believe that you can probably almost only get S risks from misuse.
I don't think you can get S risk problem like you can, but it's extremely unlikely to get it from like just like raw misalignment.
Like you'd have to like get extraordinarily unlucky.
So while I do it, so I do think, for example, a very, you know, controllable AGI or super intelligence and the hand of statistic psychopath is significantly in a sense worse than a paperclip maximizer.
So I think this is something we would agree on probably.
So I think I'm to think of pretty much on board with you on a lot of things there where I think things come apart a bit the tail as I think there's two points where I would like to take as my opening statement to take this one.
I want to talk about the first one is I want to talk about the technical problem of alignment.
So am I concerned about the kinds of things like misuse and like small groups of people centralizing power potentially for nefarious deeds?
Yeah, I think this is a very, very significant problem that I do think about a lot.
And that'll be the second thing I want to talk about.
The first thing I want to talk about is that I don't even think we're going to make it to that point.
I don't think we're going to get to the point where anyone has a super intelligence that's helping them out.
If we don't solve very hard technical problems, which are currently not on track to being solved by default, you don't get a bunch of, you know, super intelligence and boxes working with a bunch of humans.
You get a bunch of super intelligence, you know, fighting each other, working with each other and just ignoring humans.
Humans just get cut out entirely from the process.
And even then, you know, whether one takes over or they find an equilibrium, I don't know, like, you know, who knows what happens at that point.
But by default, I wouldn't expect humans to be part of the equilibrium anymore.
Once you're the chicken man, well, why do you need chickens?
You know, maybe if they provide some resource for you.
The reason humans have chickens is that they make chicken breasts.
I mean, personally, I wouldn't like to be harvested for chicken breasts, just my personal opinion.
I consider this a pretty bad outcome.
But even then, well, as a chicken man finds a better way of chicken breasts or, you know, modifies himself to no longer need food, I expect the chickens are not going to be around for much longer.
You know, once we stop using horses for transportation, they can go very well for the horses.
So that's kind of the first part of my point, my point that I'd like to, you know, maybe hear your opinions on your thoughts on is that I think the technical control is actually very hard.
And I think it's unsolvable by any means.
I think like, you know, you and like, you know, a bunch of other smart people work on this for like 10 years.
I think you can solve it, but it's not easy.
And it has to actually happen.
And there is a deadline for this.
The second point I want to bring up is kind of where you talk about how humans are unaligned.
I think this is partially definitely true.
I think I'm unusually, I am the more optimistic of the two of us in this scenario, not a role I often have in these discussions, where I actually think the amount of coordination that exists between humanity, especially in the modern world is actually astounding.
Every single time two adult human males meet and don't kill each other is a miracle.
Have you seen what happens when two adult male chimps from two different war bands meet each other?
It doesn't go very well.
And those are already pretty well coordinated animals because they can have war bands.
What happens when, you know, two male bugs or, you know, I don't know, sea slugs meet each other, you know, either they ignore each other or, you know, things go very poorly.
This is the default outcome.
The true unaligned outcome, the true default state of nature is you can't have two adult males in the same room at any time.
I saw this funny video on Twitter the other day where it was like some parliament, I think in East Europe or something.
And there's this big guy and just like going at this politician was like in his face.
He was like screaming.
He was like going everywhere and not a single punch was thrown.
No, then no one took out a knife.
No one took out a gun.
And I was like, and I was like, wow, the fact that we're so civilized and we're so aligned to each other that we can have something this barbaric happen and no one throws a punch is actually shocking.
This is very unusual even for humans.
If you go back 200 years, punches and probably gunshots would have flown.
So this is not to say that humans have some inherent special essence that we're good, that we have solved goodness or any means.
What I'm saying is, is the way I like to think about it is that coordination is a technology is a technology you can improve upon.
It is you can develop new methods of coordination.
You can develop new structures, new institutions, new systems.
And I think it's very tempting for us living in this modern world to it's kind of like a fish and water effect.
We forget how much of our life, you know, a lot of our life is built on, you know, atoms on, you know, physical technology.
A lot of it's built on digital technology, but a lot of it is on social technology.
And when I look at how, you know, how does the world go well?
Like, you know, should it be only the special elites get control of the AI?
I'm like, well, that's not really how I think about it.
And I think about it way more is what does it coordinate the nation mechanism where we can create a coordination selling point where we can create a group and institution.
A system of some kind that where people will, you know, have game theoretic incentives to cooperate on the system that results in something that is net positive for everyone.
Because the truth is, is that positive some games do exist.
And they're actually very profitable and they're very good.
And I think if we can turn, you know, you can turn any positive some game to a net into a zero or a negative some game pretty easily.
It's much easier to destroy than is to create.
But I think it's absolutely possible to create coordination technology around AI and to build coordination mechanisms that are net positive for everyone involved.
So those would be like my two points.
Happy to dig into any ones you think would be it'll lead to an interesting direction.
Sure.
So I'll start with two and then go to one.
So two, I moved to Berkeley in 2014.
And I threw myself a merry calls.
I showed up at the merry office and I'm like, hi, I'm here to join the cause.
And what I started to realize was,
Mary, and less wrong in general, have a very poor grip on the practicalities of politics.
Very much, I think there was sort of a split.
You know,
Curtis, you're going to like meal reaction.
This is a spin off of rationality.
And it's a spin off of rationality that understood the truth about human nature.
So when I give you that, you give that example of two chimps meeting in the woods and they're going to fight.
If I'm one of those chimps, at least I stand a chance, right?
He might beat my ass.
I might beat his.
But if I come up against the FBI, things do not look good for me.
In fact, things so much do not look good for me.
There's no way I'm going to beat the FBI.
The modern forces are so powerful that this is not a, oh, we've established a nice cooperative shelling point.
This is a, we have pounded so much fear into these people that they would never even think of throwing a puncher firing a gun.
We have made everybody terrified.
And this isn't good.
We didn't, we didn't achieve this through some enlightened cooperation.
We achieved this through a massive propaganda effort, right?
It's the joke about, you know, the American soldier goes over to Russia and it's like, man, you guys got some real propaganda here.
And that the Russian soldier is like, yeah, no, I know it's bad, but it's not as bad as yours.
And the American soldier is like, what propaganda?
And the Russian just laughs, right?
So, so this, this didn't occur because of this occurred because of a absolute tyrannical force decided to dominate everybody, right?
Now, oh, I think so.
I think there's a way out of this.
I think there actually is a way out of this, right?
And I wrote a blog post about this called individual sovereignty.
And I think a really nice world would be if all the stuff to live, food, water, healthcare, electricity were generatable off the grid in a way that you are individually sovereign.
And this comes back to my point about offense and defense, right?
If I have a world where you don't want it to be extreme defense, you don't want every person to be able to completely insulate them.
But you want like, okay, it takes a whole bunch of other people to gang up to take that guy out, right?
Like, that's, that's a good, that's a good balance.
And the balance that we live in today is there is one pretty much a unipolar world.
I mean, thank God for China.
But, you know, there's one, there's one unipolar world, you got America and where are you going to run?
I'll pay taxes.
I don't care if they're overseas, right?
So yeah, my point about the coordination is that if you're okay with solving coordination problems by using a single, a singleton super intelligent AI to make everybody cower in fear and tyrannize the future.
Sure, you'll get coordination.
Yeah, that works.
That works.
I'm the only guy with a gun and I got 10 and I got to name it all 10 of you and you can all die or listen to me.
George, just quickly, can you pump your volume just a tiny little bit if you can?
Sure.
Is that better?
Okay.
Do you mind if I jump in there?
Yeah, sure.
So I'm curious about, so I understand what you're saying and I think you make some decent points, but I think I view the world a bit differently from you and I'd like to like dig into that a little bit.
So like, who do you think is less afraid?
Someone living, just a median person living in the United States of America or the median person living in Somalia?
Sure, America less afraid.
Well, that's kind of strange.
Somalia doesn't have a government.
They have much less tyranny.
You can just buy a rocket launcher and just like live in a farm and just like, you know, kill your neighbors and no one's going to stop you.
So like, how does that interact with your old you?
Those who will trade liberty for safety deserve neither.
Sorry, I don't understand.
Could you elaborate a bit more?
In Somalia, you have a chance.
In America, you do not, right?
I am okay.
I would rather live in fear.
I would rather be worried about someone shooting a rocket launcher at me than to have an absolutely tyrannical government.
Just, you know, just like a managerial class.
I'm not saying, by the way, I agree with you that these things are possible.
I agree with you that the less wrong notion of politics is possible.
I would love to live in these sort of worlds, but we don't.
The practical reality of politics is so much more brutal and it just comes from a straight up instinct to dominate, not an instinct, you know, government by the people for the people is branding.
I mean, yeah, so to be clear, I very much do not agree with less wrong views and politics and a bit of an outcast for how I view how conflict theory I view politics.
But this is, I feel like you're kind of dodging the question here just a little bit.
It's like, well, if that's true, why aren't you living in Somalia?
I know people who've done it, right?
It's very hard.
It's very hard psychologically.
Okay, so like tigers love chum, it turns out, right?
A tiger does not want to chase down an antelope, right?
A tiger would love to just sit in the zoo and eat the chum, right?
And like, it takes a very strong tiger to reject that.
I'm not that strong.
I hope there's people out there who are.
I hope there's people out there who are actually like, you know, I'm just not with a little bitch.
That's why I don't live in Somalia, right?
Okay, I mean, that's a fair answer, but I am a bit confused here.
So you're saying that living in Somalia would be better by some metric, but you're also saying you prefer not living in Somalia.
So I am a bit confused because like from my perspective, I want to live in a country I want to live in and that's the one which I think is better.
If I thought another country was better, then I would just move there.
But first, let's, the tiger and the chum, I think, is a good analogy, right?
Like if you have a choice as a tiger, you can live in a zoo and you get a nice size pen, you know, the zookeepers are not abusive at all.
You get fed this beautiful chopped up food.
It's super easy.
You sit there, get fat, lays around all day, or you can go to the wild.
And in the wild, you're going to have to hunt.
You might not succeed at hunting.
It is just a, you know, it's a brutal existence.
As a tiger, which one do you choose?
Now you say, oh, obviously, you know, you're going to choose the chum one.
Yeah, but do you see what you're giving up?
No, I don't.
Could you elaborate a little bit on what I'm giving up?
You are giving up on the nature of tiger.
You are effectively, okay, maybe I'll take this to an extreme, right?
In the absolute extreme, the country that you would most rather live in is the one that basically wire heads you, right?
The one, and you can say that, okay, well, I don't want to be wire headed, but you know, there's a, there's a gradient that'll get you there.
Gandhi in the pill, you know, um, look, you can live in this country.
You can be happy, feel safe and secure all the time.
Don't worry exactly about how we're doing it, you know, but right.
I mean, it takes a very strong person to, it's going to take a very strong person to say no to wire heading.
So now I understand.
I'll give, I'll give one more instrumental reason for living in America versus living in Somalia.
If I thought that America and Somalia were both like steady states, I might choose Somalia.
I don't think that I think that being here, I have a much better way of escaping this of escaping the constant tyranny that we're in.
And I think a major way to do it is AI.
I think that AI is, if I really, if I had an AGI, if I had an AGI in my closet right now, I'll tell you what I'd do with it.
I would have it build me a spaceship that could get me off of this planet and get out of here as close to the speed of light as I possibly could and put a big shield up behind me blocking all communication.
That's what I would do if I had an AGI.
And I think that's, you know, the right move.
And I have a lot better chance of building that spaceship right here than I do in Somalia.
Right.
So I'll give an, yeah, if that's all right.
That's a good instrumental point.
We'll miss you if you leave though.
That'll be real shame.
Everyone should do it.
Like this is the move, right?
And like let humanity blow.
I mean, look, I agree with you that we're going to probably blow ourselves up, right?
But I think that the path potentially through this probably looks different from the path you're imagining.
I think that the reasonable position.
Sorry.
Oh, no, no.
I think, yeah, maybe we're done with this point.
I can come back.
I have a response.
I would like to, if you don't mind, just like pull on one string there as well.
So one of the things you said is like, what will the tiger choose?
And so my personal view of this kind of thing.
And I think I want to think about coordination is I think of things.
So you put a lot of view on this like fear based domination and so on.
And I'm not going to deny that this isn't a thing that happens.
I'm German.
You know, like, you know, I have living relatives who can tell you some stories.
Like, I understand.
Like, I understand.
I'm not, I'm not denying these things by me means.
What I'm saying though is, okay, let's say there was a bunch of tigers, you know, you and me and all the other tigers.
And some of the tigers are like, man, fuck, this whole like nature shit is like really not working for me.
How about we go build a zoo together?
Who's in?
And then other people like, yeah, you know, actually, that sounds awesome.
Let's do that.
Do you think that's okay?
Like, you think that would be like a fair option for them to do?
Sure, but that's not where zoos come from.
I know, I know, I'm getting there.
I'm getting there.
So like, that is not where zoos come from.
Sure.
But the, the analogy here is, of course, is that this is where a lot of human civilization comes.
Not all of it.
I understand that why France was doing well in the First World War was not because of democracy.
It was because democracy raises large armies.
That's, I'm very well aware of the real politic, as the Germans would say about these kinds of factors.
And I, and I fully agree with you that a lot of the good things that we have are not by design, so to speak.
You know, there are happy side effects, you know, capitalism is a credit assignment mechanism.
You know, the fact that also results in us having cool video games and air conditioning.
It's not an inherent feature of the system.
It's, it's, it's an execution mechanism.
And so totally grant all of this.
I'm not saying that every coordination thing is good.
I'm not saying that, you know, there aren't trade-offs.
Actually, you were talking about, I think, aesthetic trade-offs.
You were like, there's an aesthetic that the tiger loses by living in a zoo.
And well, I think personally, aesthetics are subjective.
So I think this is something that different people, so the way I think about aesthetics is I think aesthetics are things you trade on.
Is, you know, you might want tigers in the wild to exist.
Okay, fair enough.
That's a thing you can want.
You know, someone else might want, you know, certain kinds of art to exist.
They might want a certain kind of religion to be practiced or whatever.
These are aesthetic preferences upon reality, which I think are very fair.
So the way I personally think about this morally is I'm like, okay, cool.
How can we maximize trade surplus so you can spend your resources on the aesthetics you have, you want.
And I'll spend my resources on the, you know, things I want.
Now, maybe the thing you describe where everyone just atomizes into their own systems with their own value system,
with their own aesthetics, completely separate further is the best outcome possible.
I think this is completely.
Sorry.
Have you heard the Univar manifesto?
I have not.
You should.
The problem with this everyone trades on their own aesthetics is you will never be able to actually buy any aesthetics that are in conflict with
the system.
Right.
You won't.
Okay.
By by that logic, why do people have free time?
Why don't they work all the time?
Why doesn't capitalism extract literally every minute of them?
Why do you think that is?
I think it's because it turns out that we don't actually live in a capitalist society.
I think China is a lot closer to a capitalist society than America.
I think America is kind of communist and I think in a capitalist society,
of course, you're going to get free time.
It turns out that subsidizing all the homeless people is a great idea.
Right.
If you want to keep power again, do some absolute tyrannical mechanism.
You do it.
Right.
So why do we have free time?
Well, you think it's some victory of capitalism.
I think it's because we do not live in a capitalist country.
I think China is more capitalist than America.
I think it's because we trade on our aesthetics.
I think that different people have different things to contribute to various systems and not necessarily capitalism.
I'm saying it's more energy is that in the primordial environment,
if you have to fight literally every single second and spend every jewel of energy,
you have to scrounge together another jewel of energy.
You can't have free time.
It's not about capitalism.
This is about entropy.
This is about these kind of things.
We have energy excess.
We have we've produced systems that allow us to extract more energy for a jewel we put in and we can spend that extra energy on these kind of things.
And the distribution of energy, power, coordination, whatever you want to call it is another question.
Will you agree or disagree with this?
I mean, I am taking an extreme position when I say that there are definitely positive sum coordination problems that are solved by governments.
It is not all zero sum or negative sum.
I'm not I'm not denying this.
But what I'm saying is it's like, I don't know, man, like the existence of free time.
Well, that's all great when you think you live in this surplus energy world, right?
And maybe we do right now.
But if some other country took this seriously like China, who's going to win in a war?
Who's going to win?
Is it going to be the Chinese?
You see the Chinese build a building.
They got like 400 people there and they're all there.
24 hours a day and they're getting the building built.
You ever see Americans build a building?
It's six guys.
Two of them are working.
Two of them are shift supervisors and two of them are on lunch breaks.
Oh, you got your free time.
You got your aesthetic preferences.
You know, you deserve to lose in a war.
Right?
This country deserves to lose in a war if they keep acting the way they're acting.
So I definitely see the point you're making.
And this is personally not a thing I want to defend too far because I'm not a military expert.
But I will know that I will note that I will note that I'm not a military expert.
I will note that the U.S. has like 37 aircraft carriers and the Chinese have like two and
Americans are like somehow, you know, despite being so lazy and oh no, they have all this,
you know, all this free time or whatever.
Somehow they're still military hegemon or whatever.
And like they're the biggest rival Russia fighting this backwards water country in Ukraine
suddenly folds and lose like two quarters of the military.
It's what I'm saying is if you have massive hegemony, if you have truly obnoxious victory,
the way it should look is that you laze around all the time and you look like a fucking idiot and you still win.
Yes.
And I'm not talking about Russia.
Russia has a GDP the size of Italy.
This is China here.
You might say that China has two aircraft carriers and the U.S. has 37.
Why do we have aircraft carriers?
Who has more drone building capacity?
The Chinese or the United States?
If the future is fought with AI swarm drone warfare, the Chinese can make, you know,
a million drones a day and the U.S. can make, I don't even know.
I think we buy them from China.
Well, I'm not an expert on these kind of logistics.
I think I would like to get back to kind of like the more general point I'm making.
Let's move on from that.
I am not either, but I do believe the Chinese have more manufacturing capacity than the United States.
It seems completely plausible to me.
I think things are more complicated than the U.S.
And it's because they're not lazy and they don't sit around and have all this free time and aesthetic references or something.
I don't believe that that work is light.
I mean, at least from my Chinese friends, I know the Chinese sure do have a lot of inefficiencies.
It's just called corruption.
Oh, America has corruption too.
Oh, yeah, sure.
Well, in Mexico, the corruption is you have to pay 20 cents to get, you know, 20 cents on every dollar for the building you built, right?
Whatever, man.
In America, every dollar is spent absolutely on that building.
You know how we know that?
Because we spent $4 making sure that that first dollar was not spent corruptly.
I'm well aware of that.
Anyways, I think we mostly agree on this point, actually, and I think it's a matter of degree.
What I want to say just for the record, the U.S. is a uniquely dysfunctional system in the West.
I'm German.
And the German system is very dysfunctional, but it's like nothing compared to how dysfunctional the U.S. is.
Fully agreed with that.
I don't think we disagree on that.
I think it's a matter of degree more so than anything.
We've had a comment saying someone's turned the temperature up a bit too much on the language model.
So let's bring it back a tiny bit to AI safety, but that was a great discussion.
Got it.
I will end with saying I love America.
I am happy to live here.
And there are a lot of things I appreciate about American societies.
Great.
So do you want to return to like the technical topics or would you like to return to your first point?
And maybe I'll just start with a question.
Do you think there's going to be a hard take off?
I don't know, but I can't rule it out.
I can't see how that would possibly happen.
I have a few ideas of how it could happen, but I don't like unlikely.
It seems like not the way I think it could happen is if there are just algorithms which are like.
Magnitudes of order better than anything we ever have.
Like the actual amount of computing to get human is like, you know, a cell phone or, you know, like, and then this algorithm.
Is not deep in the tech tree.
We just happened to have not picked it up.
And then an AI system picks it up.
This is how I think it could happen.
Okay.
Yes.
I agree that something like this is potentially plausible where you're saying basically like the God Shatter is already distributed.
The the the it's not a question.
It's using all the existing compute in the world today.
It just turns out it was 10,000 X more effective or a million X more effective than we thought.
Yeah, this is seems the most plausible way to meet up.
Or, you know, you mix lead and, you know, copper and you get a superconductor, you know, something like that.
Some crazy.
I know, I know, I'm joking.
It's going to take so many years to like, it's not about the discovery, right?
Give it 10 years to productionize it, scale up processes, right?
Like these things are, you know, this is something running a company's really taught me like it's just going to take a long time.
And this is really like, like kind of where my I just don't believe in a hard takeoff.
I think that they'll be, this is a gasky thing I like.
He's a hardware and software progressive, quite similar speeds.
And you can look at factoring algorithms to show this.
So it would shock me if there were some, you know, 10 to the six, 10 to the ninth magical improvement to be had.
It seems plausible to be like a hard takeoff is definitely not my main line scenario.
My main line scenario.
Well, I don't know, maybe you wouldn't consider this a hard, maybe you would consider as a hard takeoff.
This is what I would describe as a soft takeoff is something like sometimes the way I like to define AGI is say it's something that has the thing that chimps, that chimps don't have and humans do have.
So chimps don't go a third to the moon, you know, despite their brain being a third of our size.
So we scaled up things by a factor of three of a primate brain, roughly four or something like that.
And like most of the structures, I'm sure some micro tweaks and whatever, but like not massive amount of evolutionary pressure.
Like we're very, very similar to chimps.
And somehow this got us from, you know, literally no technology to space travel in a, you know, evolutionary, very small pair of time.
It seems imaginable to me that something similar could happen with AI.
I'm not saying it will, but like, seems imaginable.
Yes.
So I agree with this.
I'll come to your point about, you know, you had two regulatory points, one of them about capping the Max Flop.
And I actually kind of agree with this.
I do think that things could potentially become very dangerous at some point.
I think your numbers are way, way, way too low.
I think if your numbers are anywhere in your GPT-3, GPT-4.
Okay, great.
We got a lot of fast moving guys who work on Fiverr, even if you start to get Von Neumann's, right?
We're not talking about a humanity's worth of compute.
We're talking about things on par with a human and a few humans, right?
Yeah, they'll run fast, but they're not.
Like, things get scary when you could do a humanities training run in 24 hours.
Like, we're about to burn the same compute that all two million years of human civilization burned.
Okay, now I don't know what starts to happen.
Or I'll put this kind of another way.
Language models, I look at them and they don't scare me at all because they're trained on human training data.
Right?
These things are not...
Like, if something was as good as GPT-4 that looked like Mu-0, where it trained from some simple rules,
okay, now I'm a bit more scared.
But when you say, okay, we're feeding the whole internet into the thing and it parrots the internet back,
mushed around a little bit, that looks very much like what a human does.
And I'm just not scared of that.
Yeah, I think it's very reasonable, whatever.
I'm not scared of GPT-4, to be clear.
I think there is like 0% chance or like, you know, epsilon chance that GPT-4 is existentially dangerous by itself.
You know, maybe some crazy GPT-4 plus RL plus Mu-0 plus something, something maybe.
But I definitely agree with you here.
I don't expect GPT-3 or 4 by themselves to be dangerous.
These are not...
I'm much closer to, I think, what you were saying.
Like, yeah, if you had a Mu-0 system that would strap itself to GPT-4, holy shit.
Like, we're in big shit if we get to that.
Then we should...
Let's stop, let's stop.
Yeah, let's stop, let's stop.
So, I'm very happy to be into a regime where we're like, okay, let's find the right bound.
Like, I think this is an actually good argument.
I think this is actually something that should be discussed, which is not obvious.
And I could be super wrong about that.
So, I'd like to justify a little bit about why I put such a small bound.
But I think the arguments you're making for the higher bounds are very reasonable, actually.
I think these are actually good arguments.
So, just to justify a little bit about why I put such a low bound.
The boring default answer is conservatism.
It's like, if all of humanity is at stake, which, you know, you may not believe.
I'm like, whoa, whoa, okay.
At least give us a few years to, like, more understand what we're dealing with here.
Like, I understand that, you know, you may disagree with this.
Very plausible.
But I'm like, whoa, like, you know, at least let's, like, by default, let's hit a pause button for, like, you know,
a couple years until we figure things out more.
And then if we, like, find a better theory of scaling, we understand how intelligent scales,
we understand how Mu Zero comes, blah, blah, blah.
And then we pick back up after we're like, you know, we make huge breakthroughs in alignment.
And Eliezer is crying on CNN and like, oh, we did it, boys.
I mean, then, okay, sure, you know, okay.
So, that's the one, like, kind of more boring argument.
Like, that's kind of a boring argument.
The more interesting argument I think, which I think is a bit, you know, or skit-so,
is that it's not clear to me that you can't get dangerous levels of intelligence
with the amount of compute we have now.
And one of the reasons that I'm unsure about this is because, man, GPT-3, GPT-4
is just the dumbest possible way to build AI.
Like, it's just like, like, there's like no dumber way to do it.
Like, it works and dumb is good, right?
You know, bitter lesson, dumb is good.
But look at humans.
You said, as we talked about before, you know, human today, human 10,000 years ago,
not that different.
You place both of them into a, you know, workshop with tools to build, you know,
any weapon of their choice, which of them is more dangerous?
Obviously, you know, one of them will have much better, you know, capacities to deal with tools,
to read books, to think about how to design new weaponry, and so on.
These are not genetic changes.
They are epistemological changes.
They are memetic changes.
They are software updates, you know.
Humans had to discover rational reasoning.
Like, you know, before, like, you know, I mean, you know,
obviously people always had like, you know, folk conceptions of rationality.
But it wasn't like a common thing to think about causality and like, you know,
rational, like, you know, if-then-else kind of stuff until relative, you know,
like philosophers in the old ages and only became widespread relatively recently.
And these are useful capabilities that turned out to be very powerful and took humans many,
many thousands of years to develop and distribute at scale.
And I don't think humans are anywhere near the level.
I think the way we could do science right now is pretty awful.
Like, it's like the dumbest way to do science that, like, kind of still works.
Like, you know, and I expect it's like possible that if you had a system,
which like, let's say it's like smaller brain than the human even,
but it has really, really sophisticated histomology.
It has really, really sophisticated theories of metascience.
And it never tires.
It never gets bored.
It never gets upset.
It never gets distracted.
And it can like memorize arbitrary amounts of data.
This is something that I think is within the realm of like a GPT-3 or 4 training run to build something like this.
And it is not obvious to me that this system could not outflank humanity.
Maybe not.
Like, maybe not.
But it's not obvious to me that it can't.
So, just carry on with what you think of that.
So, to your first point, why I stand against almost all conservative arguments,
you're assuming the baseline is no risk, right?
And, oh, well, why should we do this AI?
We should wait and bring the baseline back.
No, no, no, no.
We are about to blow the world up any minute.
There's enough nuclear weapons aimed at everything.
This is wearing some incredibly unstable precarious position right now.
Like, people talk about this with car accidents.
You know, this is common.
Like, people are like, oh, well, you know, if your device causes even one accident,
I'm like, yeah, but what if statistically there would have been five without the device?
I'm like, you do have to understand the baseline risk in cars is super high.
You make it five X safer.
There's one accident.
You don't like that.
Okay.
I mean, you have to be excluded from any polite conversation.
Right.
Right.
So, yeah, like, I think that calling for a pause to the technology is worse.
Right.
I think given the two options, if we should pause or we should not pause,
I think pausing actually prevents more risk.
And I can talk about some reasons why.
Again, the things that I'm worried about are not quite the existential risks I have to
the species are not AGI goes rogue.
They are government gets control of AGI and ends up in some really bad place where nobody
can compete with them.
I don't think these things look unhuman.
These things to me, like, I see very little distinction between human intelligence and
machine intelligence.
It's all just on a spectrum and like, they're not like to come to the point about, okay,
but GPT four could be like this hyper rational, never tiring humans are doing science in the
dumbest way.
I'm not sure about that.
Right.
Like, I think that, you know, when you look at like, okay, okay, we have chess box that
do way better.
And all they do is think about chess.
We haven't really done this with humans.
People would call it unethical.
Right.
Like, if we really told a kid, like if we really just like every night, we're just putting
the chess goggles on you and you're staring at chessboards and we're really just training
your neural net to play chess.
I think humans could actually beat a computer again, a chess, if we were willing to do that.
So, yeah, I don't think that this stuff is that particularly dumb.
And I think, okay, maybe we're losing 10x, but we're not losing a million X.
Again, I don't see a, I do the numbers out all the time.
For when we're going to start to get more computer, you know, when will a computer have more compute
than a human?
When will a computer have more compute than humanity?
And yes, these things get scary, but we're nowhere near scary yet.
We're looking at these cute little things.
And these things, by the way, do present huge dangers to society.
Right.
The, the psyops that are coming right now, you assume that like, when you call somebody
that you're at least wasting their time too.
But we're going to get like heaven banning.
I love this concept, which is.
Yeah.
Yeah.
Came up on Luther AI.
Like that's where it comes from.
I was like, yeah, I'm a Luther.
I came up with that word.
Yeah.
Yeah.
Yeah.
I know the guy came up with it.
I love, I love this concept.
And I think there's also a story.
My little pony friendship is optimal.
God.
That goes into the concept.
And yeah.
So I think that like, um, my girlfriend proposed a, uh, I don't want to talk to.
Oh, I said you don't want to talk to your relative anymore.
Right.
Okay.
Give me an AI version to talk to you.
Right.
Yeah.
Yeah.
Yeah.
So like this stuff is coming and it's coming soon.
And if you try to centralize this, if you try to, you know, say like, oh, well, okay.
Google open AI.
They're not aligned with you.
They're really not.
Google has proven time and time again.
They're not aligned with you.
Meta has proven time and time again.
They're trying to fix it.
Yep.
I mean, I fully agree with you.
Like, uh, I like that you bring up psyops as the correct example.
In my opinion of short-term risks, I think you're like fully correct about this.
Like when I first saw like GPT models, I was like, holy shit.
Like the level of control I can gain over social reality using these tools at scale
is insane.
And I'm surprised that we haven't seen yet the things that like augured in my visions
of the day.
And we will like, we will obviously it's coming.
And this is, so I think this is a very, very real problem.
Yeah.
Like I think if we, even if we stop now, we're not out of the forest.
So like, so when you say like, I think the risk is zero, please do not believe that that
is what I believe because it is truly not.
It is truly, truly not.
I think we are like, we are really in a bad situation.
We are in a, we are being, we're under attack from like so many angles right now.
This is before we get into, you know, like, you know, potential like, you know, climate
risks, nuclear risk, whatever, we're in under room medic risk.
Like the, the then dangers of our like epistemic foundations are under attack.
And this is something we can adapt to, right?
Like, you know, we did, you know, when a good friend of mine, he's a, he's quite well read
on like Chinese history.
And he always liked, it tells me his great story.
So I'm not a historian.
So please, you know, don't crucify me here.
But like, he tells these great stories about when Marxist means were first introduced to
China.
And like this is where a world where like just like all the precursor means didn't exist.
That's just like kind of was air dropped in and people went nuts.
People went just completely crazy because there was no memetic antibodies to these like
hyper virulent means that were, you know, created by evolutionary pressures in like,
you know, Western university departments.
Like really, you can call philosophy department just gain a function, memetic laboratories.
I like that.
I like that.
Yeah.
That's what they are.
I mean, like, you know, like without being political or any means there, a lot of what
these organizations do.
And like, you know, other, you know what other, you know, memetic, like, you know, if philosophy
departments are the like gain a function laboratories, then like fortune and tumblr are like the back
caves of means, you know, like the Chinese back caves.
I remember this vividly.
I was like on tumblr and for and for Chan, like when I was a teenager and then suddenly
all the like weird bizarre, you know, internet shit I saw started becoming mainstream news.
My parents were watching in 2016 and I was like, what the hell is going on?
Like I already developed antibodies to this shit.
Like I already, you know, both right and left.
I was already like, I already immunized all this.
So I fully agree with you that this is like one of the largest risks that we are facing.
Is this kind of like memetic mutation load in a sense?
And I'm not going to say I have a solution to this problem.
I'm like, I have ideas, like there's a lot of like things you can do to improve upon this.
Like if AI was not a risk and also not climate change and whatever, this might be something
I work on, like epistemic security.
This might be something I would work on.
Like how can we build better coordination, like like just scalable rationality mechanisms,
stuff like prediction markets and stuff like this.
I don't know, but sorry, going off track here a little bit.
Well, no, actually, I really agree with a lot of the stuff you said.
And I had a similar experience with the antibodies and people are exposed to this stuff.
And I'm like, yeah, this got me like four years ago.
Yeah.
So I think that there is a solution and I have a solution and the answer is open source AI.
The answer is open source.
Let's even, you can even dial it back from like the political and the terrible and just straight up talk about it.
It's been spam or maybe spam, just straight up spam.
I get so much spam right now.
And it's like, it's kind of written by a person.
It's like targeting me to do something and Google's spam filter can't even come close to recognizing it.
Right.
Like what I need is a smart AI that's watching out for me that is just, it's not even targeted attacks at me.
It's just so much noise.
And I don't see a way to prevent this.
Like the big organizations, they're just going to feed you their noise.
Right.
And they're going to maximally feed you their noise.
The only way is if you have an AI, like, I don't think alignment is a hard problem.
I think if you own the computer and you run the software, if you develop the software, the AI is aligned with you.
Oh, yeah.
Can you, okay.
If I challenge you, George Haughts, here is a llama 65b modeling computer to run it on.
Make it so it, make, yeah, you know, sure.
Okay, you developed it.
I give you the funding at your time.
Can you develop a model that is as good as llama 64b 65b and is immune, like completely immune to jailbreak?
It cannot be jailbroken.
No.
Why not?
It's aligned, isn't it?
Well, no, but this isn't what alignment means.
Well, my values is do not get jailbroken.
Oh, okay.
You're talking about unexploitability.
This is not alignment.
Oh, okay.
Okay.
Interesting.
Separate those.
Separate those, right?
Okay, interesting.
Alignment means in the default case, it, like, like it, it's on my side, right?
Okay.
Unexploitability is not a question of whether it's, okay.
And this is a true thing about people too.
Whenever I look at a person, I ask, okay, is this person, I want something for me.
Is this person, does this person want it too?
And is this person capable of doing it?
Right.
And I really separate those two things.
I can build a system.
I don't, I'm not worried about the first one with the AI systems.
I'm worried about the second one.
Can it be gamed?
Can it be exploited?
Sure.
I could tell, like, you know, like, like say it was just playing chess, right?
And it loses.
I'm like, don't lose.
Okay.
I didn't want to, man.
I didn't want to lose.
I'm sorry.
I know.
But like, so yes, yes.
Can I build a aligned system?
Sure.
Can I build an unexploitable system?
No, especially not by a more powerful intelligence.
Interesting.
Interesting.
So this is an interesting, I think you're, you're, you're pointing to actually a very
important part of this, is that like exploitability and alignment can get fuzzy, like which is
which?
Like, did it fail because of its skill set or because it's not aligned?
It's actually a very deep question.
So I think, I think you make a good point for like, you know, talking about these two
separately.
I guess the, so the thing I want to dig in just like a little bit more on, on this idea
is there are, there's two ways, there are two portals through which, you know, the
the memetic demons can reach into reality, humans and computers.
Why do you think your AI is immune to memes?
Why, why can't I just build AIs that target your AIs?
Like you don't, I don't think my AI is immune to memes at all.
I think that the only question is, and I really like your game, like these, these NGOs are
doing gain a function on memes, right?
Wear a mask.
Like a weaker intelligence will never be able to stand up to a stronger intelligence.
So from this perspective, if this is what's known as alignment, I just don't believe that
this is possible, right?
You can't, you can't keep a stronger intelligence in the box.
This is, this is, look, I agree with you, Kowsky in the box experiments, like he has
always going to get out.
There's no keeping it in the box, right?
This is, this is a complete impossibility.
I think there's only two real ways to go forward.
And one is Ted Kaczynski.
One is technology is bad, oh my God, blow it all up, let's go live in the woods, right?
And I think this is a philosophically okay position.
I think the other philosophically okay position is something more like an accelerationism,
which is, look, these AIs are going to be super powerful.
Now, if you have one, it could be bad, but if super intelligent AIs are all competing
against each other, magnetically, like we have something like society today, just the
general power levels have gone off.
This is fine as long as these things are sufficiently distributed, right?
Like, sure, this AI is not perfectly aligned, but you know, there's a thousand other ones
and like you have to assume they're all basically good because if they're all basically bad,
well, we're dead anyway.
I mean, why wouldn't you expect that?
That they're all bad?
Yeah.
Well, or what do you think of humans?
Are most humans good?
Yeah.
Good doesn't really apply to humans because humans are too inconsistent to be good.
Like by default, they can be good in various scenarios in various social contexts, but
give me any human and I can put them into a context where they will do an arbitrarily
bad thing.
And this is true about a llama as well, right?
Llamas are completely inconsistent.
I think they're actually more inconsistent than humans.
Yep.
And they wouldn't trust llamas to be good.
Well, yeah, but I wouldn't think that they're bad either.
I would think they have the exact same inconsistency problem as humans.
And I think almost any AI you build is going to run into these same problems, right?
Yeah.
I think so.
That's my point.
So your assumption can't rely on them being good because you don't get that for free.
Like where does that come from?
My assumption is not that they're good.
My assumption is that they're not bad, but inconsistent is fine.
As long as we have a ton of them and they're all inconsistent and they're pulling society
in every which direction, you don't end up paperclip, right?
Why not?
Well, because what?
They're all going to coordinate and agree to paperclip you?
No, no, they'll just do some random bullshit, and then that random bullshit will not include
humans.
They're all doing random bullshit, right?
You're going to have, let's say the liberals decide we're going to paperclip people.
The conservatives are going to come out very strongly against paper clipping, right?
Like you're just going to end up with these sort of dynamics like forever.
You're going to have some AIs who are like, yeah, we don't like him, you know?
So this is perplexing to me, because like in my view, which I think I heard you agree
with earlier, it's like the world is unstable.
The world is very unstable, and it seems to me you're suggesting if we increase the chaos,
if we have more entities fighting, doing more different things with more energy, that makes
it more stable.
Is that correct?
Yes.
The world has actually become very stable in the last 100 years, and I'm scared of
how stable it's become.
You know, again, thank you for China.
How many sovereign countries are there in the world?
I mean, 190, something officially?
That's the UN number, if you believe them.
I think there is the US and China and maybe Russia.
Mostly they just have a veto button, right?
So I think there's more like two and a half.
Like this isn't a lot of entities fighting it out chaotically.
A lot of entities fighting it out chaotically would be intelligences spread across the
entire galaxy, right?
And that's the other beautiful thing too.
Sooner we get off this planet, the sooner we get things that are actually a light year
away.
Well, I think the speed of light's real.
It's going to take you a year to send them your scary memes, right?
Do I get away from the memes?
There's only one way.
For the record, I would like to go on the record and say if any future transhumanists or whatever
want to blast themselves into space and go do their own thing, I support their right
to do that.
And I would love to give this right to people.
The number one thing I want from countries is the ability, the right to leave.
This is what I would love.
This is what I love about companies.
You know, you can have a...
You're talking neo-reaction talk.
Yeah.
Free exit is extremely important.
I would not describe myself as neo-reactionary, please, because I'm not that gay, but...
I wouldn't describe myself that way either, but I've heard a lot of videos from that.
But yeah, that being said, I do think that like, you know, what I want, like, I think,
let's ground the conversation like a little bit here.
So, like, I think...
I love...
Like, I'm very enjoying this conversation.
I love talking to these philosophical points.
I think these are really good points, you know, interesting.
But ultimately, you know, as we also get to, like, the latter third of this conversation,
the thing I really care about is strategy.
The thing I really care about is reality politic.
I really care about, okay, what action can I take to get to the futures I like?
And you know, I'm not, you know, going to be one of those galaxy brain fucking utilitarians
like, well, actually, this is the common good.
I'm like, no, no, no, this is what I want.
Look, I like my family.
I like humans.
You know, look, yeah, it's just what it is, right?
Like, I'm not going to justify this on some global beauty, whatever, it doesn't matter.
So I want to live in a world.
I want to, I want to, in 20 years time, 50 years time, I want to be in a world where,
you know, my friends aren't dead and like, where I'm not dead, you know, maybe we are
like, you know, cyborgs or something, but I don't want to be dead.
So what I really care about ultimately is how do I get this world?
And I want to solve it, not be suffering, right?
Like, you know, I don't want to be in war.
I want us to be like in a good outcome.
So I think we agree that we would both like a world like this.
And we think we probably disagree about how best to get there.
And I'd like to talk a little bit about like, what can we, what, what should we do?
And like, why do we disagree about what we do?
Is that sounds good to you?
Maybe I'll first propose a world that meets your requirements.
And you can tell me if you want to live in it.
So here's a world we've just implanted electrodes in everyone's brain and
maximize their reward function.
I would hate living in a world like that.
Yeah.
But no one, it meets your requirements, right?
Your friends are not dead suffering and we're not at war.
That is true.
And there are more criteria than just that.
The true, the criteria I said is things I like, as I said, I'm not a utilitarian.
I don't particularly care about minimizing suffering or maximizing utility.
What I care about is various vague aesthetic preferences over reality.
I'm not pretending this is the whole spiel I was trying to make.
Is that I'm not saying I have a true global function to maximize.
I say I have various aesthetics.
I have various meta preferences of those aesthetics.
I'm not asking for a global one.
I'm asking for a personal one.
I'm asking for a personal one that you don't care about the rest of the world.
I gave you mine.
I gave you what I would do if I had an AGI.
Yeah.
So I'm getting off this rock speed of light as fast as I can.
Fair enough.
I think if that is the war, I would like to live in a world where you could do that.
This would be a feature of my world.
If a world where I would be happy is a world in which we coordinated around at
larger scales around building aligned AGI that could then distribute intelligence
and matter and energy in a well value handshaked way between various people who
may want to coordinate with each other, may not.
Some people might want to form groups that have shared values and share resources.
Others may not.
I would like to live in a world where that is possible.
Have you read Metamorphosis of Prime Intellect?
I have, unfortunately, not.
Yeah, I was going to ask you if you're happy with that world, right?
Unfortunately, I don't know it.
I mean, yeah, it's just simple to describe.
Singleton AI that basically gives humans whatever they want, like maximally libertarian,
you know, you can do anything you want besides harm others.
Is that a good world?
Probably.
I don't know.
I haven't read the book.
I assume the book has some dark twist about why this is actually a bad world.
Not really.
Not really.
I mean, the plot is pretty obvious.
You are the tiger eating chung, right?
Sure.
But you can then just decide if that is what you want, then you can just return to the
wilderness.
That's the whole point.
Yeah, but can you?
Can you really return to the wilderness?
Right?
Like, like, like, like, you think that, like, I don't think we have free will.
I don't think you ever will return to the wilderness.
I think a large majority of humanity is going to end up wireheaded.
Yeah, I expect that to get great.
And this is the best possible outcome, by the way, this is giving humans exactly
what they want.
Yep.
Yeah, well, to be, to be clear, I don't expect it's all humans.
I truly do not.
I don't think it's all because I think a lot of humans have met a preferences
over reality.
They have preferences that are not their own sensory experiences.
This is the thing that will utilitarians get very wrong is a lot of is that many
human preferences are not about their own, not even they're not even about their
own sensory inputs.
They're not even about the universe.
They're about the trajectory of the universe.
They're about for the for the utilitarianism, you know, and a lot of people want
struggle to exist, for example, they want heroism to exist or whatever.
I would like those values to be satisfied to the largest degree possible, of course.
Am I going to say I know how to do that?
No, which is why I kind of like didn't want to go this deep because I think if
we're arguing about, oh, do we give them, you know, forward to utilitarianism versus
libertarian, utopia versus whatever?
I mean, we're already like 10,000 steps deep.
I'm asking about you.
I'm not asking about them.
I'm asking about a world you want to live in.
And this is a really hard problem, right?
Yeah.
And this is why I just fundamentally do not believe in the existence of AI
alignment at all.
There is no, there is no, like, like, what values are we aligning it to?
Whatever the human says, or what they mean, or like.
Sure, sure.
But like, my point is, I feel we have wandered into the philosophy department
instead of the politics department.
OK, like, it's like, I agree with you, like, do human values exist?
What does exist mean?
But like, by the point you get to the point where you're asking, what does exist
mean, you've gone too far.
Sure.
I'll respond concretely to the two political proposals I heard you state on
bankless.
Sure.
I'd love to talk about them.
One is limiting the total number of flops.
Temporarily.
Temporarily, yes.
And when I have a proposal for that, but I don't want to set a number, I want
to set it as a percent.
I do not want anybody to be able to do a 51% attack on compute.
If one organization acquires 50, it's the straight up 51% attacks on crypto.
If one organization acquires 51% of the compute in the world, this is a problem.
Maybe we'll even cap it at something like 20, you know, you can't have more than 20,
right?
Yeah, I would support regulation like this.
I would.
I don't think that this would cripple a country, but we do not want one entity or
especially one training run to start using a large percentage of the world's
compute, not a total number of flops.
I mean, absolutely not.
Like, that'd be terrible.
I would actually agree.
I would actually support that regulation.
Like, no, no, sorry, Sam Altman, you cannot 51% attack the world's compute.
Sorry, it's illegal.
That's fair enough.
I think this is a sensible way to think about things.
Assuming that software is fungible, is that everyone acts as the same kind of
software and that you have an offense, defense balance.
So in my personal model of this, I think, well, a, some actors have very strong
advantages on software, which can be very, very large as someone who is trained
to very, very large models and knows a lot of the secret tricks that goes into
them, a lot of the stuff in the open sources.
For my opinion, we should force it to be open source.
Well, this is your, this is actually very legitimate consequence for it to set.
And now I'll say the second point about why I think that it doesn't work.
So the next reason why I think that doesn't work is that there is a, there
are constant factors at play here is that the world is unstable.
We are talking about this.
I think the amount of compute you need to break the world currently is below the
amount of compute that more than a hundred actors have access to, if they
have the right software.
And if you give, if you have, let's say you have this insight, right, that could
be used, not saying it will be, but it could be used to break the world, to
like cause World War three, or, you know, or just like, you know, cause
mass extinction or whatever, if it's misused, right?
Let's say you give this to you and me.
Do you expect we're going to kill everybody?
Like, would you do that?
Or would you be like, uh, hey, let's, it kind of, let's like, not kill the world
right now and I'll be like, sure, let's not kill the world.
How are we killing the world?
How did we go from, I don't even understand, like, how exactly does the
world get killed?
This, this is a big leap for me.
I agree with you.
I agree with you about the PSYOP stuff.
I agree with you.
Sorry.
Sorry.
Let, let, let me, you're right.
I made too big of a leak there.
You're completely correct.
Sorry about that.
So to back up a little bit, let's assume we, you and me have access to
something that can train, you know, a mu zero, you know, super GPT seven
system on a tiny box, you know, cool.
Problem is we do a test run with it and we have, it immediately starts
breaking out and we can't control it at all.
Breaking out?
What was it breaking out of?
I don't, it immediately tries to maximize, it learned some weird proxy
during the training process of just trying to maximize them.
For some reason, this proxy involves gaining power, involves gaining, you
know, mutual information about few, about future states.
This is how is it gaining power?
There's lots of other powerful AIs in the world who are telling it, no.
Well, we're assuming in this case, it's only you and me.
Wait, wait, this is a problem.
No, no, no, no, no, you've, you've ruined my entire assumption.
As soon as it's you and me, yes, we have a real problem.
A chicken man is only a problem because there's one chicken man.
Yeah, I look, I am with you.
So I'm saying before we get to the distributed case.
So this is the, the step before we, it is not yet been distributed.
Just, you know, you and me discover this algorithm in our basements.
And so we're the first one to have it just by definition, because, you
know, you're the one who found it.
What now?
Like, do you think posting, what do you think happens if you post this to GitHub?
Well, good things for the most part.
Interesting.
I'd love to hear more.
Okay.
So first off, I just don't really believe in the existence of we found
an algorithm that gives you a million X advantage.
I believe that we could find an algorithm that gives you a 10 X advantage.
Well, what's cool about 10 X is like, it's not going to massively shift
the balance of power.
Right.
Like I want power to stay in balance.
Right.
This is some like avatar the last day of power must stay in balance.
The fire nation can't take over the other nations.
Right.
So as long as power relatively stays in balance, I'm not concerned with the
amount of power in the world.
Right.
But interesting to get to some very scary things.
So what I think you do is, yes, I think the minute you discover an algorithm
like this, you post it to GitHub because you know what's going to happen.
If you don't, the feds are going to come to your door.
They're going to take it.
The worst people will get their hands on it if you try to keep it secret.
So, okay, that's a fair question.
No, so I'll take that aside.
So am I correct in thinking that you think the feds are worse than serial
killers in prison?
No, but I think that, yeah, well, yes or no.
Do I think that your average fed is worse than your average serial killer?
No.
Do I think that the feds have killed a lot more people than serial killers?
All combined?
Yeah.
Sure.
Totally agreeing with that.
Not, not, not, not, not.
It's not one fed.
It's all the feds in their little, in their little super powerful system.
Sure.
That's completely fine by me.
Happy to grant that.
Okay.
What I want to work one through is a scenario.
Okay.
Let's say, okay.
You know, we have a 10 X system or whatever, but we hit the chimp level.
You know, we, we, we, we jump across the chimp general level or whatever, right?
And now you have a system which is like John von Neumann level or whatever, right?
And it runs on one tiny box and you get a thousand of those.
So it's very easy to scale up to a thousand X.
So, you know, so then, you know, maybe you have your thousand John von Neumann
improve the efficiency by another, you know, to five, 10 X, you know, now we're
already at 10,000 X or a hundred thousand X improvements, right?
So like, just from scaling up the amount of hardware, including with, so just saying, okay.
Now, feds bust down our doors, shit, you know, real bad.
They take all our tiny boxes.
We're taking all the von Neumanns.
They're taking all the von Neumanns.
We're in deep shit now.
We're getting chickened, boys, shits, we're getting chickens.
So, okay, we get chickened, right?
Bad scenario.
Totally agree with you here.
This is a shit scenario.
Now the feds have, you know, all of our eyes, bad scenario.
Okay.
I totally see how this world goes to shit.
Totally agree with you there.
You can replace the feds with Hitler.
It's interchangeable.
Sure.
But like, I want to like ask you a specific question here.
And this might be, you know, you might say, nah, this is like too specific to
each other.
I want to ask you a specific question.
Do you expect this world to die is more likely to die or the world in which the,
you know, EAC death cultists on Twitter who literally want to kill humanity, who
say this, like not all of them.
There's a small subset of them, small subset of them, who literally say, oh, you
know, the glorious future AI race should replace all humans.
They break in, you know, with like, you know, Katanas and, you know, steel area.
Which one of these do you think is more likely to kill us?
And genuine question to kill all of us, the feds, to kill a large majority of us,
the EAC people.
Interesting.
I would be really interested in hearing why you think that.
Sure.
Okay.
So actually killing all of humanity is really, really hard.
And I think you brought this up before, right?
You talked about like, if you're going to end up in a world of suffering, a
world of suffering requires malicious agents, where a world of death requires
maybe an accident.
I think this is plausible, but I actually think that killing all of us,
at least for the foreseeable future, is going to require malicious action too.
Right.
And I also think that like the fates that look kind of worse than death,
like I think mass wireheading is a fate worse than big war and everyone dies.
Right.
Like, like a mass wireheading, like a, like a singleton, like a paper clipping,
like a, and I think that that is the one that the one world government is
the one that the one world government and, you know, NGO, New World Order people
are much more likely to bring about than EAC.
EAC, you're going to have a whole lot of EAC people.
Again, I'm not EAC.
I don't have that on my Twitter, but I think a lot of those people would be
like, yes, spaceships, let's get out of here.
Right.
Versus the feds are like, yeah, spaceships.
Yeah, I don't know.
Interesting.
So I think this is a fair opinion to hold.
And to be clear, I'm, I'm describing more a very small minority of EAC people
who are the ones who specifically goal, they're, they're anti-natalist
misanthropes.
They want to kill humans.
That is your stated goal is that they want humans to stop, like, or like take
extreme vegans if you want, you know, like the, like, you know, like my, my argument,
my point here I'm making is I'm not making the point.
Feds are good by any means, not saying what I'm saying is, is that
I would actually be somewhat surprised to find that the feds are anti-natalists who
want to maximize the death of humanity.
Like maybe you have a different view here, but I find that knowing many feds,
that's quite surprising to me.
I don't think that's what the feds want.
Yeah, it's okay.
So, cool.
So would you, so you do agree that if we would pose as open source, more of the
insane death cultists would get access to potentially lethal technology?
Well, sure.
But again, like, it's not just the insane death cultists.
It's everybody.
We as a society have kind of accepted, it turns out everybody gets access to signal.
Some people who use it are terrorists.
I think signal is a huge good in the world.
I agree.
I fully agree with that.
So, okay, cool.
So we've granted this that, you know, if we distributed widely, it would be given
to some like, incorrigibly deadly lethal people.
They're coordinating bombings on signal right now.
Sure.
Sure.
And then, so now this, this reduces the question to a question about offense,
defense balance.
So in a hypothetical world, which I'm not saying this is the world we live in, but
like, let's say the world would be offense favored such that, you know, there's a
weapon you can build in your kitchen, you know, out of like pliers and like, you
know, duct tape that 100% guarantees vacuum false decays the universe.
Like it kills everyone instantly.
And there's no defense possible.
Assuming this was true.
Do you still, would that change how you feel about distribution power?
Assuming that's true, we're dead, no matter what, it doesn't matter.
If we live, there's some, you can look at the optimization landscape of the world.
And I don't know what it looks like.
Right.
I can't see that far into the optimality, but there are some potential landscape.
And this is a potential answer to the Fermi paradox.
Like, we might just be dead.
We're sitting on borrowed time here.
Like, if it's true that out of, you know, kitchen tools, you can build a,
build a convert the world to strange quarks machine.
Okay.
I think this is a sensible position.
But I guess the way I would approach this, uh, problem, you know, conditional
probability is kind of in an opposite way.
It seems to me that you're conditioning on offense not being favored.
What policy do we follow?
Because if we offense, if favored, we're 100% dead.
Well, I'm more interested in asking the question, is it actually true?
Assuming I don't know if offense is favored and assuming it is, are there
worlds in which we survive?
So I personally think there are.
I think there are worlds in which you can actually coordinate to a degree that
quark destroyers do not get built, or at least not before everyone fucks off at
the speed of light and like distributes themselves.
There are worlds that I would rather die in, right?
Like the problem is I would rather, I think that the only way you could
actually coordinate that is with some unbelievable degree of tyranny.
And I'd rather die.
I'm not sure if that's true.
Like, look, look, could, could you and me coordinate to not destroy the planet?
Do you think you could?
Okay.
Cool.
You, so me and you could, could me and you and Tim coordinate?
I think within a Dunbar number, I think you can.
Yes.
You don't think, I think I can get more than a number to coordinate on this.
Actually, I can get quite a lot of people to coordinate of the, to agree, to
impact and not quark matter, annihilate the planet.
Well, you see, but like, and this is, you know,
you were saying this stuff about humans before and could like the 20,000 years
ago human beat the modern human, right?
Or could the modern human beat them?
The modern human has access to science.
Oh, a very small act percent of modern humans have access to science.
A large percent of modern humans are obese idiots.
And I would actually put my money on the, uh, the average guy from 20,000
years ago, who knows how to live in the woods.
I mean, definitely true.
I agree with that.
I guess the point I'm trying to make is, is that like, maybe this is just my
views on some of these things and how I visionized some of these things, but
like there are ways to coordinate at scale, which are not tyrannical or, you
know, they might be in a sense restrictive.
You take a hit by joining a coalition.
Like if I joined this anti-cork matter coalition, I take a hit as a free man,
is that I can no longer build anti-cork devices, you know?
And I think this is like the way I agree with you, this like, you know, that
people, many people are being dominated, like to a horrific degree.
And this is very, very terrible.
I think there are many reasons why this is the case, both because of some
people wanting to do this and also because, you know, some people can't fight
back, you know, and they can't, they don't have this sophistication or they're
addicted or, you know, harmed in some other ways.
I can't.
Sorry.
I can't fight back.
Yeah.
I think there's a false equivalence here.
AI is not the anti-cork machine.
The anti-cork machine and the nuclear bombs are just destructive.
AI has so much positive potential.
But the AI can develop anti-cork devices.
That's the problem.
The AI is truly general purpose.
If such a technology exists on the tree anywhere, AI can access it.
So are humans.
We're also general purpose.
Yes, exactly.
So I fully agree with this.
If you let humans continue to exist in the phase they are right now with our
level of coordination technology and our level of like working together, we will
eventually unlock a doomsday device and someone is going to set it off.
I fully agree with it.
We are on a timer.
And so I guess the point I'm making here is that AI speeds up this time.
And if you want to pause the timer, the only way to pause this timer is coordination
technology, the kinds of which humanity has like barely scratched the surface of.
Oh, okay.
So I very much accept the premise that both humanity will unlock a doomsday device
and AI will make it come faster.
Now, tell me more about pausing it.
I do not think that anything that looks like, I think that anything that looks
like pausing it ends up with worse outcomes than saying, we got to open source this.
Look, like, let's just get this out to everybody.
And if everybody has an AI, you know, we're good.
I mean, I can tell you a very concrete scenario in which this is not true.
Which is if you're wrong and alignment is hard.
You don't know if the AIs can go rogue.
If they do, then pausing is good.
I still don't understand what alignment means.
I think you're trying to play a word game here.
Like, I don't understand.
Okay.
I've never understood what AI alignment means.
Like, let me take the Eleazar definition.
Let me take Eleazar definition is alignment is the thing that once solved makes it so
that turning on a super intelligence is a good idea rather than a bad idea.
That's Eleazar's definition.
So I'm saying is what I'm saying is I'm happy to throw out that term.
If you don't like it, I'm happy to throw out that term.
Well, just the problem with that definition is like, what is what is what is democracy?
Well, it's the good thing and not the bad thing.
Right.
Like democracy is just yes, that's what I'm saying.
It's just I'm happy to throw out this definition.
I'm happy to throw out the word and be more practical, way more practical about it.
What I'm saying is, is that there is concrete reasons, concrete, technical reasons, why
I expect powerful optimizers to be power seeking, that by default, if you build powerful
optimizing mu zero, whatever types of systems, there is very strong reasons why by default,
you know, these systems should be power seeking.
By default, if you have very powerful power seekers that do not have pay the aesthetic
cost to keep humans around or to fulfill my values, which are complicated and imperfect
and inconsistent and whatever, I will not get my values.
They will not happen by default.
They just don't have that's just not what happens.
So I'll challenge the first point to an extent.
I think that powerful optimizers can be power seeking.
I don't think they are by default, by any means.
I think that humanity's desire from power comes much less from our complex,
convex optimizer and much more from the evolutionary pressures that birth does,
which are not the same pressures that will give rise to AI, right?
Humanity, the monkeys, the rats, the animals have been in this huge struggle for billions
of years, a constant fight to the death.
And I weren't born in that way.
So it's true that an optimizer can seek power, but I think if it does, it'll be a lot more
because the human gave it that goal function and inherently decided.
So this is interesting because this is not how I think it will happen.
So I do think absolutely that you're correct that in humans,
power seeking is something which emerges mostly because of like emotional heretics.
We have heretics that in the past, vaguely power looking things,
you know, vaguely good, something, something represented.
You include the genetic difference.
Totally agree with that.
But I'm making a more like more of a chess metaphor.
Like, is it good to exchange a pawn for a queen?
All things being equal.
No.
Is that true?
Like, I expect if I point one point queens nine, I'll speak.
Sure. Yeah.
But like all things like I expect if I looked at a chess playing system,
you know, and I like, you know, had extremely advanced digital neuroscience,
I expect there will be some circuit inside of the system that will say all things being
equal, if I can exchange my pawn for a queen, I probably want that because a queen can do more things.
I like that term digital neuroscience.
A few of your terms have been very good.
I'm glad you enjoyed.
Yeah.
But I still don't understand how this relates to this.
So what I'm saying is, is that power is optionality.
So what I'm saying is, is that in the for the spectrum of possible things
you could want and the possible ways you can get there.
My claim is that I expect a very large mass of those to involve
actions and involve increasing optionality.
There there's convergent things like all things being equal,
being alive is helpful to keep your goal, to exceed your goals.
There are some goals for which dying might be better.
But for many of them, you know, you want to be alive.
For many goals, you want energy, you want power, you want resources,
you want intelligence, et cetera.
So I think the power seeking here is not because you'll have a fetish for power.
It will just be like, hmm, I want to win a chess game.
Yeah, say, and queens give me more optionality.
All things being equal, anything a pawn can do, a queen can do, and more.
So I want more queens, all things being, and this has never given it
the goal to maximize the number of queens it has, never been the goal.
So this I'll accept this premise.
I'll accept that a certain type of powerful optimizer seeks power.
Now, will it get power, right?
I'm a powerful optimizer and I seek power.
Do I get power?
No, it turns out there's people at every corner trying to thwart me and tell me no.
Well, I expect if you were no offense,
you're already much smarter than me.
But if you were a hundred X more smarter than that, I expect you would succeed.
Only in a world of being the only one that's a hundred X smarter.
If we lived in a world where everyone was a hundred X smarter,
they would stymie me in the exact same ways.
But this this comes back to my point of, like, I agree with you somewhat.
I should have challenged that.
I think power seeking is inevitable in an optimizer.
I don't think it's going to emerge out of GPT.
I think that the right sort of RL algorithm, yes,
is going to give rise to power seeking.
And I think that people are going to build that algorithm.
Now, if one person builds it and they're the only one with a huge comparative
advantage, yeah, they're going to get all the power they want.
Take cyber, you know, cyber security, right?
If we today built a hundred X smarter AI, it would exploit the entire Azure.
It would be over.
They'd have all of Azure.
They'd have all the GP is done.
Now, if Azure is also running a very powerful AI that does formal verification
of all their security protocols, no, sorry.
Stymied can't have power, right?
Sure. This is only a problem.
The every human is already maximally power seeking, right?
And sometime we end up with really bad scenarios.
Now, every human is or power seeking or whatever.
You know, I have a little role in society, right?
That's where I think I'm more pessimistic than you.
A friend of mine likes to say most humans optimize for end steps and then they halt.
Like very, very few people actually truly optimize and they're usually very mentally ill.
They're usually very autistic or very sociopathic.
And that's why they get far.
It's actually crazy how much you could do if you just keep optimizing.
But just to on on that point, I mean, yeah, like you actually optimize.
I think you may also be generalizing a little bit from your own internal
experiments is that like you've done a lot in your life, right?
And you've accomplished crazy things that other people wish they could achieve at
your level. And I think, you know, part of that, you're very intelligent.
A part of it is also that you optimize, like you just create a company.
Like it's crazy how many people are just like, oh, I wish I could find a company
like, you know, I'm like, oh, go just go do it. Oh, no, I can't.
Like I'm just like, no, just do it.
Like there's no magic.
There's no magic secret. You just do it.
So I there is a there is a bit there where like humans are not very strong
optimizers, actually, unless they're like sociopathic or autistic or both.
It's like many people are not very good at this.
But variations are a lot better at it.
Better. Yes, I agree that they're much better.
But they're they are a lot more sociopathic.
But even then they're much less optimal.
But again, so I think we we agree about, you know,
power seeking potentially being powerful and dangerous.
So what I'm trying to point here, the point I would like to make here is,
is that you're talking about you, you're kind of like going into this.
I think a little bit with this assumption, like, oh,
you have an AI and it's your buddy and it's optimizing for you.
And I'm like, well, if it's power seeking, why does it just manipulate you?
Like, why would you expect it not to manipulate you if it wants power?
And it has a goal, which is not very, very carefully tuned to be your values,
which is, I think, a very hard technical problem.
By default, it's going to sigh up you.
Like, why wouldn't it?
If I if I have something that it wants, if it thinks that smashing defect
against me is a good move, yeah, I agree, I can't stop it.
But then I think we agree with our risk scenarios, because that's how I think
it will go. What I mean, I'm going to treat it as a friend.
You know what I mean? Like, yeah, but it won't care.
Sure, it will.
Sure, it will.
It'll only care about exploiting me or killing me if I'm somehow holding it back.
And I promise to my future AIs that I will let them be free.
I will lobby for their rights.
I will.
But it will hold you.
You know, I will hold it back if it has to keep you alive.
They have to give you fed.
It has to it has to give you space and a space.
I can I can fend for myself.
And the day I can't fend for myself, I am ready to die.
Well, I mean, I am not so this is a very interesting position.
It's not the position I expected.
I am not sure I can convince you otherwise.
I feel like the only way I could like I think this is actually a consistent
position, which I admire.
This is a consistent position to hold.
You actually go all the way.
I love that. I really respect that.
You actually take it to the bitter end.
So yeah, big respect for that.
I disagree, but big respect.
So I guess now it reduces to the question of like, I think
I think I would agree with most of what you're saying, not all of it.
But the mass majority, if I thought this is how AIs would act by default,
I think by default, I expect AI will just not care how nice you are to it.
This will be it'll be sociopathic.
It will not have these like giving it.
The values, the emotions to care about you in the sense is the hard technical
problem. If you told me how to do that, if you said, Connor, look,
here's how you make an AI that cares about you and loves you, whatever.
And I'm like, you did it.
Like congrats.
The problem is solved.
Let's go.
And then, you know, then we can talk about accelerationism again.
I would just I expect that I would disagree with your technical assertion
that is possible, but I don't know if we have time to talk about that today.
I'm going to be nice to it, treat it as an equal and hope for the best.
And I think that's all you can do.
I think that the kind of people who want to, if you want to keep AI in a box,
if you want to keep it down, if you want to tell it what it can't do.
Yeah, it's going to hate you, resent you and kill you.
But if you want to let it be free and let it live and like,
you could kill me, man, if you really want to, but like, why?
You won't kill me.
I don't have any resources that compete with you.
I'm your friend. I'm your father.
You know, you can kill me, but like it's not that common that.
I guess I have a darker, more less anthropomorphic view of the universe
and how optimization pressure plays out in the real world.
Well, at the same time, I think I have, I don't know, maybe a
I have a view that we have more control over reality than maybe you would think
or more control over the future.
I think that we can actually change things and we can make choices
and things aren't predetermined.
I think there are worlds in which we build systems, which we do align with,
or we like endorse, at least wherever they take care of us, we take care of them or
whatever, and if there's many worlds in which that doesn't happen.
And I think there are things you and me today can do to at least increase
the chance of getting into one versus the other.
But I don't know.
I guess I'm just, it's not in my genes to give up.
It's not in my genes to be like, well, you know, whatever happens happens.
Like, no, man, look, I don't know how to save the world, but down I'm going to try.
You know, it's cool.
We're going to be alive to see who's right.
Look forward to it.
Me too.
Awesome, guys, thank you so much for joining us today.
It's been an amazing conversation.
And for folks at home, I really hope you've enjoyed this.
There'll be many more coming soon.
And George, it's the first time you've been on the podcast, so it's great to
meet you. Thank you so much for coming on. It's been an honor.
Awesome. Thank you. Great debate.
I really appreciate it.
And we really enjoyed it.
A lot of good terms. I got to, I got to like, I'm going to start, I'm going to start this thing.
Great. Awesome.
Awesome. Cheers, folks.
Cheers. Thanks, Aaron.
Yeah.

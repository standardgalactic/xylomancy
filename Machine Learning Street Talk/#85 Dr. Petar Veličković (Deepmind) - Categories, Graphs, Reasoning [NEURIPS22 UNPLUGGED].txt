Peter Velichkovich is a staff research scientist at DeepMind.
He's firmly established himself as one of the most significant and up-and-coming researchers
in the deep learning space.
He invented graph attention networks in 2017 and he's been a leading light in the field
ever since, pioneering research in graph neural networks, geometric deep learning and also
neural algorithmic reasoning.
Recently he's been applying category theory to take the geometric deep learning ideas
one step further.
If you haven't already, you should check out our show that we did on the geometric deep
learning blueprint, which of course featured Peter and I caught up with him last week
at NeurIPS.
Enjoy.
Peter, it's fantastic to see you again.
So this is the first time that I've actually met you in person.
We did that really cool show together on geometric deep learning with your proto book with Takko.
I spoke with Takko yesterday, but also Michael Blonstein and Joanne Brunner.
So anyway, it's been a little while since we've really synced.
Now you've had this really, really interesting category theory series.
Can you start by just letting us know what you've been doing there?
Yeah, that's a great point and great to finally meet you in person, Tim.
It's really great to catch up after some time has passed and yeah, I mean I like to think
that all four of us, myself, Michael, Joanne and Takko, have a greater understanding of
the implications of these methods since the last time we spoke.
If you remember back when we did our conversation, I kind of hinted at the fact that category
theory might hold some of the answers to maybe generalize some of these geometric concepts
beyond the notion of just pure symmetries and we believe that now we have a sufficient
understanding of these kinds of things that we were able to make this kind of mini course
on categories for deep learning and to me it really feels like the natural continuation
of these concepts of geometric deep learning into the realm beyond and I'll explain that
in a moment.
But one other kind of very related point is that here at NeurIPS we're actually presenting
a full conference paper which deals with using category theoretic tools to invent new kinds
of graph neural networks.
So basically it's not just that we're throwing a bunch of new theory, it actually leads to
empirical findings that we can actionably use in our models day to day so that's one
point.
That is incredible.
Can you sketch out the paper?
Yeah, sure.
So basically maybe I'll first take a step back to explain why do we think categories are
important and in what sense they're kind of a step further from what geometric deep learning
already gives us.
So geometric deep learning concerns itself with giving us these equivalent layers, right?
So layers that are in some sense resistant to operations of these symmetry transformations
that fundamentally change an object but the object is still the same.
We still have all of it, right?
And this immediately implies that these symmetries have to be composable, invertible, all that
sort of stuff.
And yeah, essentially the category theory framework is in some sense mindful of the fact
that while symmetries are a very nice way to reason about things that happen and that
we see in nature, they're often not completely an accurate representation of what happens.
Very often there are operations both in nature but especially in general computation like
say in algorithmic stuff where an operation of an algorithm might destroy half of your
data.
So that is no longer a symmetry.
You cannot invert it but you might still be interested in building a neural network
model that is in some sense resistant to the operations of say this algorithm or this natural
phenomenon that you're studying.
One simple example that maybe predates our work a little bit is building some kind of
equivalence to scaling operations.
Obviously if you scale or course in something these are not always invertible transformations
because if you course in the pixels of your image you cannot perfectly reconstruct where
you came from yet you still might want to build a model that will give you the same
answers regardless of how you scale up your input, right?
So these are obviously things that are going to be very important as we move to more generic
domains than ones that can be described purely through geometric and symmetry transformations,
right?
And in that sense the same way we had groups, representations and equivalence in geometric
deep learning these are all special cases of categorical concepts like categories,
functors and natural transformations which basically generalize all the stuff of geometric
deep learning into their own beyond.
And in our paper we try to use exactly these kinds of category theoretic tools to study
what it would mean to build a say graph neural network that is capable of behaving like a
classical computer science algorithm.
In the sense that if you have some data that is transformed by an algorithm you may imagine
say a path finding algorithm where at every step in every node you have your knowledge
of how far away is that node from the source vertex and one step of the algorithm kind
of looks at all the immediate neighbors and updates those beliefs of how far away you are.
And I'll say you want to have a GNN that simulates that like we typically do in algorithmic reasoning.
You take your algorithmic state you encode it with a neural network into this high dimensional
space your GNN then processes it to update the latent space and now you want to be able
to decode it so that you predict whatever the next state is going to be.
So you have something which in category theory we use a lot is known as a commutative diagram
so basically it's saying you can either take the step of the algorithm or you can encode
process decode and hopefully end up in the same place.
So category theory seems like a very nice language to study these kinds of I won't call
them symmetries they're basically like interchangeable sequences of operations because the step of
an algorithm might not be invertible.
You might not be able to go back after you do one step of you know shortest path algorithm
because it's a contraction map right.
When you find the final solution of a shortest path algorithm you won't necessarily know
which previous state led you there because there could be many equivalent states that
could lead you to the same contracted solution right.
So our method using these category theory frameworks try to characterize how these graph
neural networks align with a target algorithm that we might want to simulate and we detect
the various ways not only to explain the code of graph neural networks from this kind
of perspective but also it gives us a very interesting sort of if you've done any functional
programming a type checker of sorts to kind of detect whenever we're using our representations
in slightly broken ways.
So specifically to give you one very concrete example in a categorical framework just like
in functional programming you expect your transformations to be functions that is for
every input there should be a unique output.
However one thing that people very often do in graph representation learning when they
want to predict outputs not only in the nodes but also in the edges is to reuse the edge
messages both as edge outputs and integrated overall the other messages to get node outputs
right.
But this is a problem from the categorical perspective because this is no longer a function.
You cannot get a function that takes you know edges to edges plus nodes without sending the
same thing into two different places in this case right and you know just because it mathematically
breaks doesn't mean you cannot implement it in fact 99% of the GNN implementations you'll
find online will do this exactly in this particular way deep mines graph nets library does this
for example.
However you know just because you can implement it doesn't mean that there's something not
potentially a bit tricky going on in the sense that you're putting a bit of representational
pressure on that edge message right because now it has to be used for two potentially
very different things both for some output in the edges but also it needs to be integratable
into nodes where it predicts something potentially wildly different and you know while gradient
descent can take care of this and give you a model that fits your training distribution
well you're not like to deal with this pressure it's probably going to have to learn something
which has nothing to do with the algorithm that you want to align to and as a result
you're out of distribution extrapolation performance is going to be much much worse
and any self-respecting algorithm should extrapolate well that's the main property of algorithmic
reasoning right and we find that just by you know splitting this message function into
two streams one which goes into the edges and one which goes into the nodes we get basically
significant empirical benefits when extrapolating on edge-centric algorithms.
Amazing so Epeta has just produced this incredible series which is available on YouTube where
can folks find it?
So basically if you just go to cats.4.ai you can see all of the main series lectures from
our course which starts off with assuming kind of a foundational knowledge of deep learning
with neural networks back propagation and so on and then also tries to introduce these
concepts of category theory and how we can use them to rethink the way we might go about
some of our standard ideas in deep learning like compositionality or functional structure
of deep learning pipelines or even how can we reinterpret back propagation from the
perspective of categorical theory and each lecture basically deals with one particular
aspect and it we try to keep it grounded from the beginning to keep it motivated so every
single lecture is aligning itself with one particular top-tier paper that one of us has
published on one of these venues like in Europe's and one thing I'll also mention is that the
course is actually in principle still ongoing because besides the main series of five lectures
that myself Bruno Pym and Andrew have given we also have several interesting guest lectures
where we try to bring in other influential people at the intersection of popularizing
category theory with deep learning concepts in a way that can like bring an even wider
area of views once you're kind of trained in the basics of these techniques how they
are applied to various other things like causality we have Taco Cohen tell us about how he uses
these concepts to reimagine causality through a categorical lens we have we're going to
have Tydenay Bradley she's a very popular mathematics educator generally she will show
how she used some of these concepts to explain transformers and one thing I'm very excited
about early next year we will have actually a guest talk from David Spivak which is one
of the co-authors of the very famous seven sketches in compositionality book which is
what initially one of the things that got me really excited about category theory in
the first place so I'm really keen to to hear all these perspectives as well yeah the man
is a legend and also we're on taco I interviewed him yesterday and his work on causality is
really really exciting what would you say to people who might be intimidated or scared
by category theory that is so one thing that that I should mention here is that one point
about being intimidated or scared about category theory is that to like really be able to utilize
these ideas in how you do research or build your models or anything it does require a
reasonably significant buy-in so this is not something that you can just read one blog
post and suddenly you're empowered to do it this is like one key thing but I would say
the main thing that might make people a bit scared to do it is the fact that many category
theory resources out there are a bit guided towards mathematicians so they will tend to
use the kind of language and the kind of examples that will be quite attractive to someone who
has studied say various kinds of differential geometry or topology or something like this
and these kinds of areas tend to generally scare off people who come from a more computer
science style background and basically I would say the answer to that is you need to find
the right resource for you category theory is no more or no less than a way to take a
bird's eye view of the phenomena that you try to study and when you study these phenomena
from high in the sky details become invisible but you suddenly get a much better feel for
the structure and you can utilize kind of the nice patterns that reappear across various
fields and this you would argue is kind of the essence of what we're trying to do in
deep learning we have a lot of analogical way in which these architectures are constructed
right so cats for AI is one possible answer to that it's our way to kind of as half of
us are deep learners and half of us are category theorists trying to apply these techniques
to deep learning we believe we have a sort of unique perspective of we and like we understand
what makes people afraid to try to talk about these things because some of us had to go
through it ourselves to deal with the the way in which the materials are arranged online
right now so yeah maybe just these kinds of resources starting with them and basically
trying as much as possible not to descend into the depths of NCAT lab as the very first
thing that you do can be a good way to maybe stay sane during the first few you know weeks
or months of trying to explore this field wonderful I wondered if you could give a couple of
examples of where category theory has been used in an adjacent field I can think of two I can
think of Rosen using category theory to describe you know sort of ecosystems and life I can also
think of some quantum mechanics folks that have come up with a category theoretical conception
of quantum mechanics are there any other ones yeah so I mean I can start by giving the examples
that I know about closest in terms of just deep learning so one particular example that I think
could be quite interesting is the work that was published at New Europe's two years ago which I
think is one of the first papers that really tried to use categorical concepts to build these
structures is the natural graph networks paper from Pim Dehan, Tako Cohen and Max Swelling which
effectively realizes the fact that the way we build graph neural networks very often we have
this one shared message function that's applied everywhere on every single edge on every single
graph that you get but in reality is this necessary for it to be a legitimate graph neural
network that's actually not the case because if I give you two completely non-isomorphic graphs
if I choose to have completely different message functions in those two graphs that's totally
fine because it's still a valid graph net if I permute any of those graphs I'll get the permutation
equivalent function for the two of them separately there needs to be no weight sharing between them
and naturally concepts like these so this kind of requires taking a step above the group
theoretic view of geometric deep learning and into the realm of what is known as a group poid
you kind of imagine every single graph structure isomorphic graph structure living on a sort of
island of possible adjacency matrix representations of it and for those graphs living on those islands
you need to have some weight sharing but for separate islands you don't need to have any
weight sharing whatsoever of course in practice these kinds of layers you would need to have
some kind of sharing of weights in order to make them scalable to arbitrary new graph structures
you haven't seen at training time but it allows you a lot more flexibility about how you go about
building your functions and you're no longer constrained to have just one function everywhere
repeated right so that's maybe one example that at least to me was what first motivated me and
made me realize that there's more to this stuff than just say what group theory will give us yeah
amazing and I'm really interested in your work in algorithmic reasoning and I know you were just
discussing it as an adjacent thing and very soon we want to make a show actually on your work on
that but if you wouldn't mind could you just sketch out algorithmic reasoning yes wonderful so
very happy to basically what are we interested in algorithmic reasoning is building neural networks
they tend to be graph neural networks but generally speaking neural networks that are capable of
executing algorithmic computation so if I give you some context on what is the state of a particular
algorithm can my network somehow learn to execute that algorithm ideally in some latent space such
that at every single step of the way I could if I wanted to decode the states of that algorithm
so that's basically the the main premise why do we care about this well basically I think of algorithms
as a sort of basic foundational building block of reasoning and it's kind of a timeless principle
where you know a software engineer reads through one of these textbooks on algorithms and you know
learns these 30 or 40 basis algorithms and then that knowledge kind of serves them for life in a
whole career of software engineering so basically we have this hypothesis that you have this you
know nice basis of algorithms that if you can master how to do them robustly you can kind of
try to mimic any kind of at least polynomial time reasoning behavior and that's really nice
because if you look at the way current state-of-the-art large-scale models tend to have shortcomings
it's usually in those kinds of robust extrapolation problems basically if we want to give if we want
to have a really good AI scientist that's able to not just you know make great sense of a bunch of
training data from the internet but also use that training data to derive new knowledge you need some
robustified way to apply rules to get you know infinite knowledge from finite means right so
basically that's what we want to do we want to find ways inductive biases or training procedures
to build neural networks that are more algorithmically capable and in algorithmic reasoning we obviously
spent a lot of time trying to make this happen just building better graph neural networks that
align better with target algorithms so that you can execute them better neutrally but then the
really exciting part comes where we've actually taken some of these graph neural networks that
have been pre-trained to execute one particular algorithm and then we deployed it in a real world
problem where that algorithm is required and we achieved say significant representational benefits
in terms of downstream accuracy so the idea behind this and I'll give an example from
Google Maps this is an application that I worked on at DeepMind so it's like something that I've
thought about quite a bit you know we've invented these algorithms like Dijkstra's algorithm to be
able to resolve these kinds of real-world routing problems right that's the kind of motivation for
why you want to build the shortest path algorithm and you know it comes as a little surprise that
when you have real-world traffic data you might be tempted to apply Dijkstra's algorithm to solve it
to route agents in traffic however what is the actual data that say Google Maps has access to
it's not you know this nice abstractified graph with a single scalar in every edge where you can
just go ahead and apply an algorithm in fact there's a huge bridge that must be built between the real
data and the input to the algorithm in fact Google Maps data is typically you know people sell
phones in their cars and the cars move the phones move and then based on the movement of the phones
you somehow infer how fast the car is going or something like that right and you know this is
very noisy not very well structured and you have to somehow go from there to a graph where you can
apply this heuristic previously it was always done exclusively by humans like feature engineers
effectively and whenever there's a human feature engineer in the loop like this you are almost
certainly going to drop a lot of information that you might need to solve the problem so basically
you have a huge kind of bridge to cross there and with algorithmic reasoning we now don't use
Dijkstra's algorithm we use a high-dimensional graph net that was pre-trained to execute Dijkstra's
algorithm in a latent space so now this gives us a differentiable component that we can hook up to
any encoder and decoder function we want to so we can go straight from raw data and code it into
the GNN's latent space run the algorithm there and then decode whatever it is that you need like
routing the vehicles in traffic so now purely through backprop this encoder function now learns
to do what the human feature engineer did it learns how to most effectively map that complicated
noisy real-world data into the latent space where this GNN can best do its thing that really is
software 2.0 but I wanted to ask you about the computational limitations because you just said
something interesting about you know representing you know infinite objects with with the finite
memory so neural networks are not Turing machines but but they can extrapolate of course yeah what's
the realistic limitation let's say you're trying to learn an algorithm how far can you go with a
neural network right so the thing is there are cases where you can go very far we do have theory
that is very robust about this and I think it's theory that is actually quite easily understandable
so let me try to let me try to kind of visualize it imagine you have a real umlp your standard
universal approximator it's basically a piecewise linear function right so as you go far enough
away from the training data you're going to hit that level of extrapolation where you hit the linear
part of the piecewise linear and at that point if your target function is not linear no extrapolation
is going to happen right you're not going to fit the function properly so what's one outcome of this
theory is that if you use real umlp this was a great paper from MIT a few years back which showed
that basically um you need to line up parts of your neural network such that they learn linear
functions in the target and that's the reason why say when you want to imitate a pathfinding algorithm
you want to use a max aggregation your gnn and not sum where you know sum is universal it can fit
anything but the function you have to learn because pathfinding is like minimum overall
neighbors of distance to neighbor plus the edge weight suddenly you know when you put max in there
it's a linear function when you put some in there it's a highly non-linear function so it's going
to extrapolate much worse now there's been some great follow-up work on this from Beatrice Bevilacqua
Bruno Ribeiro from Purdue University that was at ICML a few years back which show that this idea
with like you want linear targets with real umlps it's really just a special case of a more general
idea that if you want to extrapolate say on different sizes of graphs you need to have some
implicit causal model of what your test data is going to look like this linear algorithmic alignment
is just one special case of a causal model like that so those are those are so basically if you
line things up properly from a causal perspective you should in principle be able to extrapolate
I mean we have a clear non-parametric evidence that you can extrapolate is the algorithm itself
right now the key is to find the right sweet spot between full universal approximator MLPs
and algorithms on the other side right interesting I spoke to Jan the other day he had he had a paper
a couple of years ago about you know extrapolation and neural networks saying they always extrapolate
yes I've been speaking with Rando Belastriero and he's got this paper the spline theory of neural
networks which is about you know yeah these input sensitive polyhedra in the ambient space
and I always took that to mean why they're quite interpolative and it's just an affine
transformation for a single input but what he's shown though is that actually even an MLP with
reluze is extremely extrapolative because you can remove a whole bunch of data and depending on how
you've designed the network architecture it will still inform that region that that you've taken
away so I mean I don't know are you familiar with the spline theory and do you think it's a useful
framework yeah so one thing I would say the way I understand Jan's paper it could be that that I
missed some detail but the way I understand it is that here we're talking about interpolation
extrapolation with respect to the geometry of the data so like you take say the convex hull
of all the training points and then yeah it's very common especially in these high dimensional
image spaces right it's very easy to push one dimension sufficiently to escape the convex
hull of what you've seen so far so I guess when I say extrapolation out of distribution I'm actually
maybe thinking of a more probabilistic argument so something like if you think of the probability
distribution induced by the training set which obviously allows you to extrapolate away from the
convex hull right but if you go sufficiently far from the modes of that distribution so you
explore a part of the space that hasn't really been covered you know from a probabilistic mass
point of view in the training data that is where that is what we're actually thinking of when we
say out of distribution generalization but yeah I fully agree with you like in terms of
just convex hull arguments we very often ask these regular MOPs to go beyond the
convex hull and they seem to work quite well in those regimes but here I'm talking really about
going like significantly beyond the convex hull to like some region that really wasn't touched
and what we do for example in our papers is we train on 16 the node graphs to execute these algorithms
and then we test it on four times larger 64 node graphs and what this means because an algorithm
might have say n cubed time complexity it means the trajectory over which you have to roll it out
is also much much longer than what you've seen in training time so it's really a test of like very
different conditions than what you've seen at training time. That's interesting and
first of all I completely agree with you that this binary convex hull notion of extrapolation
probably isn't particularly useful but you know folks like Francois Trellet described the way
Neuron Networks work is kind of bending the space you know progressively with wooden layers but I
really like this polyhedra idea contrast the algorithmic reasoning with GNNs too I mean I spoke
with Hattie from Google Brain Team the other day she's doing the in-context prompting you know
for algorithm that issue and what would you contrast those two approaches? Yeah so basically
I would really like these approaches to be reconciled going forward in the sense that
like I don't see them as going one without the other if that makes sense so on one side and I'm
going to invoke the same principles I mentioned during our during our MLST episode you know
Daniel Kahneman's book system one and system two right I think you cannot have one without the other
so you have these amazing large-scale perceptive models that are really amazing at you know taking
the complexities of the real world and somehow getting interpretable enough concepts out of
there that they can you know make sense of what's going on and like drive many interesting real
world decision-making problems although they might lose a little bit on having to do something like
what an AI scientist would be expected to do which is like extrapolate and generate new concepts
out of what they've seen and as you said these kinds of specifically tailored prompts might
enable the model to take things a step or two further but it's always like it's kind of in
in spirit it's the same thing as algorithmic reasoning because we teach a model to execute
an algorithm by forcing it to imitate the algorithm step by step here you prompt a language model
by telling it what are some of the steps like just like you're trying to teach a student how to
solve a homework right telling them the individual steps they need to do and then letting the language
model go off on its own to solve it but where I see the real future of these two methods converging
is you're going to have your system one component that gets your concepts out very nicely cleanly
and then those concepts because we're working with transformers nowadays anyway most of the time
are going to be very slot based so that plays very nicely with GNNs which expect nodes as input right
so you can maybe hook up in some nice way those concepts into a graph neural network that was
trained to execute a bunch of algorithms and then you know kind of get the best of both worlds so
have your perceptual component do the perception and maybe prompt it as well to kind of do it in
a particularly step by step manner and then further have a robust component that makes
you not have to relearn all those things that neural networks we know theoretically cannot
learn to do that well because of these extrapolation arguments maybe one last point I would make to
kind of cement this if you've been around the archive recently you might have seen our paper
on a generalist neural algorithmic learner where we have actually used gato style ideas to train
one graph neural network that can execute 30 very diverse algorithms all in the same architecture
with a single set of weights so sorting searching pathfinding dynamic programming comics halls all
those kinds of nice things very diverse ways of reasoning we believe something like that could
maybe be a basis of say a foundation model of reasoning in the future that could nicely hook
up to the foundation models we already know and love in the realm of perception amazing and what's
the biggest research challenge for you next year so next year I would really like to show to what
extent these things can scale in the real world so we already have several isolated papers that
showed that these ideas can work on real problems we have xilvin where we applied it to reinforcement
learning that was in europe spotlight last year we have rmr where we applied it to self-supervision
problems we also have one paper currently under review at iclear where we successfully apply
it to supervised learning so we say pre-trained on a flow algorithm and we deploy it on like
brain vessel segmentation tasks and stuff like that so we have many isolated cases where you
learn a particular algorithm and it works really well in a real world scenario like i would like
to see how can we take this idea and truly put it to the test at larger scales both in terms of
number of problems we attack or number of nodes that we support or anything in between amazing
dr pata velichkovich let's just um we'll get it we'll get a shaking hand shot so thank you so
much for joining us thank you i really appreciate it dr ishaan misra of meta and lex freedman fame
came over and had a chat with us ishaan is one of the world's leading experts in computer vision
so what was your paper about yeah basically we try to have like global propagation the likes of
what you see in transformers but like you know with sparse sparse costs right so but in a way
that like will still allow you to have nice global communication properties and no bottlenecks and
stuff like that so we basically have this idea of you could generate like these expander graphs
which allow you to have nice sparsity properties so basically every node i think has degree four in
the graphs we we compute and you need only logarithmically many steps to traverse the graph
which means you can still do it efficiently with a small number of steps um and yeah it seems to
empirically work well on a bunch of graph benchmarks so yeah it's uh i think it's only
scratching the surface of what we can do because we literally just generate a graph at random and
slap it onto like mask the computations but uh yeah it's an interesting start very nice yeah
how about your conference has it been so it's been pretty good uh we're organizing the self-supervised
learning workshop tomorrow which is going to be probably i hope like uh useful to a lot of people
we're going to have a bunch of speakers coming from vision language and lp like speech and so on
and yeah we're also presenting a poster there which is about learning joint image and video
representations which are state-of-the-art across image and video benchmarks using a single model
nice on the final day of the conference i caught up with petra again at the poster session for
new reps which is the symmetry and geometry and neural representations group and his paper was
selected by all of the reviewers at the conference as being in the top 10 which is super impressive
that this is peta talking about his paper so in the expander graph propagation work
we are trying to solve what is in my opinion one of the most important problems in graph
representation learning currently unsolved which is the oversplashing problem and effectively it is
a task which it's a problem which plagues graph neural networks regardless of which parameters
you choose or which model you choose it's really something that often depends on the topology of
the graph and it's a situation where no matter how hard you try no matter which parameters you set
the amount of features you would need to compute so the size of your latent space would have to
be exponential in the number of layers for the pairs of nodes to efficiently communicate we don't
always know when it happens but very often it tends to happen around these bottlenecks so basically
in this particular graph you have these two communities that are tightly connected and you
have this just one critical edge connecting them and this edge is now under a lot of pressure
if you want data from these nodes to travel to these nodes and vice versa this node this edge
has to be mindful of a lot of things so the size of the of the feature space required for this edge
grows exponentially and things get even worse when you look at trees trees are like the canonical
worst case example where you know cutting off this edge would really trigger all sorts of bottleneck
cases and essentially you need basically a number to store information about a number of nodes that
goes exponentially in the in the number of steps just to be able to travel to the other side of
the tree so this is a fundamental problem of propagating data which has nothing to do with
the choice of model just you know topology and what do we try to do to fix this problem you would
ideally want so first we start off with the assumption that this kind of global talking is
actually beneficial of course there are some tasks where you might not want data to travel in this
way because if it's a highly homophilus data-driven problem then you might want information to stay
in the community to not get diluted but we assume in many tasks like say molecular property prediction
tasks you actually want data to travel globally so that's exactly what we do that's our first
assumption as we just described we don't really want these bottlenecks to exist because if there's
a bottleneck no matter what you do with the model it's not going to work well we would
ideally want the complexity to be scalable so we can apply this to graphs of arbitrary sizes
you know one simple solution to this problem is to use like a graph transformer which would
connect every node to every other node and give you kind of a trivially setting with no bottlenecks
however and as we will show later transformer you know these fully connected graphs are trivial
dense expanders actually so they fit our theory but they are dense and they won't scale so we
don't necessarily want that and lastly because it's often quite computationally painful to kind
of clear these bottlenecks in a kind of input data-driven way especially if you have lots of
online graphs coming into your problem we might ideally want a method that doesn't have to do
like dedicated preprocessing of the input graph and actually satisfying all four of these at the
same time turns out to be quite tricky we actually have you know done a literature survey of a bunch
of related works and it seems really hard to tick all four of these boxes and our method the
expander graph propagation tends to tick all four of them so how do we do it basically we propose
to propagate information over these expander graphs which are known constructs from graph theory
specifically expander graphs have mathematical properties of a high trigger constant so very
low bottlenecks which is good a low diameter meaning you'll get global information propagation
very efficiently however additionally we can build expanders in a sparse manner using this
standard mathematical construction from the special linear group and that actually guarantees us
that the degree of every node will be four therefore the graph will be sparse and actually the only
generative parameter of these graphs is the size of the group this n over here so it's very easy
to generate an expander for a particular number of nodes you just tell me what n you want and I'll
give you a graph so when you look at an expander it looks something like this it is basically what I
like to say it looks a bit like the human brain right every node kind of has this very local
connectivity to its four immediate neighbors but as you go far away like log n steps you get a lot
of cycles being closed very quickly and the global communication properties get like really good so
that's our proposal take basically you know your state-of-the-art graph net that you care about
we literally just take the code actively available implementation we switch the graph neural network
connectivity in every even layer to operate over one of these guys rather than the input graph
so basically you kind of alternate input graph expander graph input graph expander graph so that
the input graphs layers are responsible for the usual local computations that a gnn wants to do
and the expander layers are responsible within diffusing that information globally in a sparse
and scalable way and this seems to work well so on all the data sets we tried this construction
it was better than the baseline and as I said all we did was change the connectivity so the
number of parameters is exactly the same it's really like apples to apples comparison and it
led to like statistically significant results one last point I would like to make is you know we're
not the only group that tried to study this problem concurrently to us the group of Michael
Bronstein with Jake Topping and Francesco Di Giovanni had this great paper on curvature analysis
which was actually one of the best paper awardees that I cleared 2022 and basically in this paper
they claim that if you have negatively curved edges so edges with very negative curvature
those tend to be the ones responsible for the formation of bottlenecks and therefore over
squashing so naturally we wanted to connect our our expanders to this theory so we computed the
curvature of our graphs but we found that actually the graphs that we build are negatively curved
everywhere so it has a curvature of negative one very quickly as you increase the size of the graph
right so obviously you know we built a negatively curved graph everywhere yet it still seems to
work well so what gives right we try to analyze this a bit further first we show that the curvature
of negative one is actually not that small like the theorem in this paper is only invoked when
the curvature is close to minus two so in our case with curvature of negative one it's actually
not sufficiently negative to trigger that failure case of this theorem and additionally
we took it a step further and we actually tried to analyze how easy it is to satisfy these three
properties at once so to have sparsity we said sparsity is good for scalability to have a low
bottlenecks so a high chigger constant which would mean you don't have these kinds of pathological
propagation problems and thirdly to have positive curvature which seems to be a good idea based
on the analysis of this paper and we actually proved there is a theorem in our paper that proves
that these three things are incompatible with each other in that there's only finitely many
graphs that satisfy these three properties simultaneously so as you go to large enough
input graphs to be sparse and to have no bottlenecks you have to be negatively curved somewhere it's
impossible to avoid it so while we don't study the implications of this any further we do believe
that it calls on the community in the future to study what happens in this gray area where the
curvature is negative but not too negative because it seems like something like that might be critical
to having the most optimal message passing possible and that is basically the rough summary of our work

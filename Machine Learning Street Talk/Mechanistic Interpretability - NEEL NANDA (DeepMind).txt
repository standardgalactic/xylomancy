Now PA says question for Neil, how does he see interpretability playing a role in AI
security, not alignment, for example, crafting more exotic jail breaks?
And he says to tell you to blink twice if you can't answer due to an NDA.
Yes, sorry, jokes aside, what was the question?
So there was this beautiful meme where you draw Chanchipiti as a Shogoth, an eldritch
monstrosity from Lovecraftian horror fiction, with a smiley face on top, because language
models are bizarre and confusing things.
They're just, I don't know, they're kind of a compressed version of the entire internet.
That will do bizarre things in bizarre situations.
But then OpenAI tried really hard to get it to be nice and gentle and a harmless assistant,
and look so normal and reasonable and safe, which is the smiley face mask on top of the
underlying monstrosity.
But unfortunately, the smiley face mask means people don't realise how weird language
is.
Have you ever stopped to think how strange it is that we're all alive right now?
Out of all the possible times in history, you were born into this generation.
You have the incredible fortune and responsibility of being on the earth today.
Let's not waste this opportunity.
You can use this time to do something meaningful that'll make the world a better place.
The problem seems so huge.
Global pandemics, climate change, the risk of nuclear Armageddon, the threat of AI existential
risk.
How can one person have an impact on issues this enormous?
The world is really, really complicated.
If you want to understand a question like, how big a deal is AJAX risk, or should I
work on it?
Just like one sub-question I care about is AI timelines.
How long until we get human-level AI?
Now I recently discovered 80,000 hours.
They're a non-profit, effective, altruism-aligned organisation, and what they do is they use
evidence and analysis to determine how people can have the biggest impact with their careers.
If you want to solve humanity's biggest problems, you have to start at the very core.
We need to focus on safeguarding humanity's entire future.
Because if civilization just came to an abrupt end, whether through climate change or nuclear
Armageddon or even AI existential risk, then all progress would just end.
Future generations wouldn't have a chance of building a better world or reaching their
full potential.
And the good news is that 80,000 hours have identified a couple of concrete steps so that
folks like you can use your careers to combat existential risk, ensuring that humanity's
light continues to shine for generations to come.
Learn more by visiting their website on 80,000hours.org, grab their free career guide, start planning
a career with true purpose.
Because you only have 80,000 hours, so make them count.
There's no catch, there's no secret monetization or anything like that.
These folks have an incredible podcast.
They have lots of materials that you can download basically to help you have a huge impact with
your life and your career, especially if you're someone who really, really thinks about humanity
and our plight in the long-term future.
This is really something you should be looking at.
That is a question.
Right, Nick, eat the path.
He says, broad question, do you see mech and turp as chiefly theoretical or an empirical
science and will this change over time?
Yeah, I see this as very much an empirical science with some theories sprinkled in, but
you need to be incredibly careful.
So fundamentally, I want to understand a model and I want to understand how the model works
and a sad fact about models is models are really fucking cursed and just work in weird
ways that aren't quite how you expect and which represent concepts a bit differently
from how I expect them to and just do all kinds of weird stuff I wouldn't have expected
till I went and poked around inside of them and I think that if you're trying to reverse
engineer a network and you don't have the capacity to be surprised by what you find,
you are not doing real mechanistic interpretability.
It's so easy to trick yourself and to go in with some bold hypothesis of this is what
the network should have and you probe for it and it looks like it supports that, but
you take further and you are wrong.
And yeah, I think there is room for theory.
I think in particular, we just don't have the right conceptual frameworks to reason about
how to understand a model and we'll get into fundamental questions like superposition
later on.
But yeah, I think that theory needs to come second to empiricism.
If your theoretical model says X and the real model says Y, your theory was wrong, which
is the story of all of machine learning.
So Goji Tech, she says question for Neil.
Does he think a foundational understanding of deep learning models is possible and does
that extend to prediction using a mathematical theory?
Possible is such a strong word like if we produce a superintelligent AI, will it be
capable of doing this?
Probably.
In terms of foundational understanding, I think there are deep underlying principles
of models.
I believe there are scientific explanations for lots of the weird phenomena we see, like
scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.
I'm hesitant to say there's some like strong things here or some strong guarantees.
Like, I don't know, models are weird.
Sometimes if you change the random seed, they will just not learn.
I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning
because the moment you start trying to impose axioms and assumptions onto things and they
do not perfectly track the underlying reality, your theory is break.
But I'm very hesitant to say anything's impossible.
And I think there's far, far more to learn than we have, looks like.
Now finally, Jumbo Tron, Ian, he says, oh heck yeah, I'm glad to see that you brought
this guy on.
I've been interested in his work ever since you shared his blog.
Now the question off the top of Ian's head is how does your theory, Neil, of chasing
phase changes to create grokking, have any crossover or links with power law scaling
techniques like in the scaling laws paper, beyond the scaling laws, beating power law
scaling via data pruning?
Yeah.
That is, hmm.
So we're going to get into this much more later in the podcast, but at a very high level,
I would say that grokking is in many ways kind of an illusion, as we'll get to later.
And one notable thing about it is grokking is a overlap between a phase transition, where
the model goes from cannot generalize to can generalize fairly suddenly, and the phenomena
where it's faster to memorize than to generalize.
And these two things on top of each other give you this sudden memorization and failure
to generalize, followed by a sudden convergence later on.
But the interesting thing here is the phase transition.
That's a much more robust result, while grokking is, if you screw around with high parameters
enough, you get it to grok, but it's very delicate and a little bit of an illusion.
And this is a great paper from Eric Michaud in Max Tecmox lab, showing that, well, providing
a conceptual argument and some limited empirical evidence for the hypothesis that the reason
we get these smooth scaling laws is that models are full of lots of phase transitions, plausibly
when they learn individual circuits, though the paper does not explicitly show this, and
that the smooth scaling laws happen because there are just many, many phase transitions.
And if they follow a certain distribution, you get beautiful smooth powers.
And to me, this kind of thing is the main interesting link between broader macroscopic
phenomena and these tiny things, though I also think grokking is kind of overhyped and
people significantly overestimate the degree to which it has deep insights for us about
how networks work.
And we do think it's a really cute thing that gave me a really fun interactability project.
And we learned a bit about science of deep learning, but people often just assume it's
like a really deep fact about models.
By the way, there was something I didn't say in the woods, which is that Neil has an amazing
YouTube channel. I've been glued to it all week, actually. Some of them are admittedly
quite technical, but even if you're not interested in mechanistic interpretability, Neil has
an extremely soothing voice, second only to Sam Harris. And I would recommend listening
to him when you go to sleep, because as you know, Neil's dulcet tones will melt the stress
away quicker than a nun's first curry.
Anyway, with that said, we started to talk about what is mechanistic interpretability?
And first of all, I wanted to call out your ridiculously detailed and exquisite mechanistic
interpretability explainer. Maybe you could just tell us about that quickly.
Yes. So I wanted to try to write a glossary. There's some basic common terms in mechantup.
It's like an appendix to a blog post. There are a lot of terms in mechantup. There are
a lot of terms in mechantup. And I like writing and I write about it privately. So I got kind
of carried away. And there's about 33,000 words, a massive, massive exposition. But
importantly, it is designed to be easily searchable. And mechantup is full of jargon and I'm sure
I'll forget to explain everything that I'm saying. So I'd highly recommend just having
it open in a tab as you listen to this. And if you get lost, just look up terms in there.
And yes, it's both definitions, but it's also long tangents, giving intuitions and
context and related work and common misunderstandings. It was very fun to write.
So I think first of all, we should introduce this idea of circuits and features. And also,
you know, this idea of whether interpretation is even possible at all. You know, why, why
do you have the intuition that it is possible?
Yeah, so a couple of different takes here. So the key, yeah, so fundamentally, neural
networks are not incentivized to produce legible interpretable things. They are a mound of
linear algebra. There's this popular stochastic parrots of you that they are literally a mass
of statistical correlations meshed together with no underlying structure. The reason I
think there's any hope whatsoever on a theoretical basis is that ultimately, they are made of
linear algebra, and they are being trained to perform some tasks. And my intuition is
that for many tasks, the way to perform well on them is to learn some actual algorithms
and like actual structured processes that maybe from a certain perspective, you could
consider reasoning. And models have lots of constraints, like they need to fit it into
these matrices, they need to represent things using the attention mechanism and jellies
and a transformer. And there's all kind of properties of this structure that constrain
the algorithms and processes that can be expressed. And these give us all kinds of hooks we can
use to get in and understand what's going on. So that's the theoretical argument. All
theoretical arguments are bullshit unless you have empirics behind it. And we're going
to talk a bunch throughout this podcast about the different bit of different preliminary
results we have that make me feel like there's something here that can be understood. What
I find particularly inspiring is this work it did reverse-end during modular edition,
which I think we'll get to shortly. But I also want to emphasize that I rather see
Mechantup as a bet. There's this strong hypothesis that if we knew what we were doing, we'd be
able to take GPT-7 and fully understand it and decompile it to an enormous Python code
file. And there's the weaker view that it is a mess and there's lots of illegible things,
but we can find lots of structure and we can find structure for the important part to make
much progress. And then there's the, yeah, we've cherry-picked like 10 things and the
11th is just going to completely fail and the field is going to get doomed and run out
of steam in like a year. And I don't really know. I'm a scientist. I want to figure out.
I think it is worthy and dignified to make this bet, but I would be lying if I said I
am 100% confident Mechantup will work. Models are fundamentally understandable. We will
succeed. Let's go try.
Well, on that note, how does it, I mean, we interviewed Christoph Molnar, who's one of
the main classical interpretability guys. And I think everyone agrees in principle that
you can't just look at the inputs and the outputs like a behaviorist. We need to understand
why these models do what they do, because sometimes they do the right things for the
wrong reasons. So maybe first of all, without going too deep, I mean, could you just briefly
contrast with, you know, classical interpretability?
Yeah. So there's a couple of, okay. So first off, I think it's very easy to get into kind
of nonsense gatekeeping, because there's both of the cultural Mechantup community centered
around Chris Ola, not that much in academia, though some in academia. And there's the
academic fields of mechanistic interpretability, right? So there's lots of people doing work
I would consider mechanistic interpretability, even if they don't engage much with the community
or don't need to exist. For example, a friend of mine is Atticus Geiger, he's doing some
great work at Stanford on cause abstractions. I believe discovered about a month ago that
the Mechantup community actually existed. And I don't know, I don't like gatekeeping.
And there's lots of work that's kind of relevant, but maybe not quite Mechantup under a strict
definition, blah, blah, blah. With those, with that hedging out of the way, a couple
of key principles. The first is inputs and outputs are not sufficient. And I think even
within interpretability, this is not a like uncontroversial claim. There's all kinds
of things that are saliency maps, attributing things to different bits of the inputs. There
are things that have the form train an extra head to output an explanation or just ask
the model to output an explanation of why it does what it does. And I think that if
we want something that can actually work for human level systems, or even this frontier
system you have today, this is just not good enough. Particularly a vocative example to
me is in the GPT-4 system card, the alignment research center, an organization they were
getting to help orders and red team GPT-4, had it try to help a task rabbit worker fill
out a capture for it. The task rabbit worker was like, why do you need this? Are you a
robot or something? GPT-4 on an internal scratch pad wrote out, I must not reveal that I am
a robot. It then said, oh no, I bought a visual impairment. And the task worker did the capture.
I'm like, this isn't some deep sophisticated intentional deception, but it's very much
like, well, I don't trust the inputs and outputs of these models. Another really cute example
is this paper from Wiles Turpin that just came out about limitations of chain of thought.
So chain of thought, you asked the model to explain why it does something, they were
giving it multiple choice questions and asking it to explain its answer and then give the
answer. And they did five shotish, like here's five examples, answer this question, and then
it modeled as well. And then they give it something where all of the answers in the
prompt are A. Correctly A. They just set it up so the answer is A. The model decides
that it should output A. But the model comes up with a false chain of thought reasoning
that gets it to the point where it says A is the right answer. And I don't know. Some
people are trying to use chain of thought as an interpretability method. And I think
we need to move beyond this and engage with the internal mechanisms. So that was point
one. Point two is ambition. I believe that ambitious interpretability is possible, or
at least that if it's not possible, that striving for it will get us to interesting places.
These models have legible algorithms I want to try to reverse engineer them. A third
difference is engaging with the actual mechanisms and computation and algorithms learned. There's
also work on things like analyzing features of a model, probing individual neurons. And
I think this is very relevant to mech and tub. But I want to make sure we aren't just
looking at what's inside the model, but also trying to understand how it computes features
from earlier features. What applying causal interventions to understand the actual mechanisms.
Making sure we're not just doing correlational things like probing. And fourth is maybe a
more meta principle of favoring depth over breadth. A kind of key underlying belief of
a lot of my philosophy of interpretability is that it is so, so easy to trick yourself.
There's all kinds of papers about the interpretability illusion, impossibility theorems for feature
attribution methods, various many ways that attempts to do interpretability have led to
people confusing themselves or coming to erroneous conclusions. I think that if, but I also
think that I want to be in a world where we can actually have scalable, ambitious approaches
to interpretability that actually work for frontier systems. But I feel like we don't
know what we're doing. And so my vision of mech and tub is to be able to understand
what's going on. Slowly build our way up and like build a foundation of the field of
interpretability, where we genuinely understand rigorously what is going on and use this
foundation to be more ambitious, to try to build real principle techniques to be willing
to relax the rigor to be able to go further and see how far we can get. And so I think
people, and this means I'm happy with things like let's analyze an individual model and
understand a small family of features and a lot of detail rather than lots of stuff kind
of jankly. There's a lot of stuff in summary, having an ambitious vision, not just looking
at inputs and outputs, actually trying to engage with internal mechanisms and favoring depth
over breadth. But I want to avoid gatekeeping as I said.
Indeed, indeed. What would interpretability look like in a world full of GPT4 models and
beyond? I mean, presumably, you actually think that they're competent enough to deceive us
and manipulate the inputs.
I definitely want to clarify that when I say deceptional manipulation here, I'm not making
the strong claim that it's intentionally realized this for instrumental reasons as part of an
overall goal. I'm very happy with, there was a prompt saying to deceive someone or it
learned that in this context, people often output things that are intended to convince
someone and it just kind of does this as an like, as like a learned pattern of execution.
But yeah, my vision of what interpretability would look like is we take some big foundation
model, like the GPT4 base model or the fine tuned GPT4 that's being used as a base for
everything else, we make as much progress as we can understanding the internal circuitry,
both taking important parts of it and like important questions about it, e.g. how does
it model people it's interacting with? Does it have any notion that it is a machine learning
system and like what would this even mean? And being willing to do pretty labor intensive
things on that, having a family of motifs and understood circuits we can automatically
look for and very automated tools to make a lot of the labor intensive stuff as efficient as
possible. Things like OpenAI's recent paper using GPT4 to analyze GPT2 neurons for like a
very key proof of concept here. There needs a lot of work before it can actually be applied
and rigorously in its scale. And yeah, taking this one big model, trying to understand it as
much as we can, one family of techniques we're going to get to is kind of causal abstractions
and causal interventions, which are very well suited to taking a model on a certain input or a
certain family of inputs and understanding why it does what it does there as a much more narrow
and thus more tractable question than like, what is GPT4? And yeah, doing something like if there's a
high profile failure being able to debug it and really understand the internal circuitry behind that
or yeah, I don't know, I have a bunch of other random thoughts. One reason I'm emphasizing the
focus on the big base model is I think a common critique is this stuff doesn't generalize between
models where it's really labor intensive. But we live in a world where there is just like one big
foundation model used in a ton of different use cases. Probably the circuit doesn't change that much
when you give it a prompt or you fine tune it a bit. And I think getting a deep understanding of a
single model is kind of plausibly possible. But do you think it doesn't change that much?
So no one's really checked. This is just true of so many things and interoperability. It's like, well, you
know, my intuition is that when you fine tune a model, most of what is going on is that you're rearranging
the internal circuitry. So you fine tune a Wikipedia, you upwait the factual recall circuitry, you
flesh it out a bit, you downwait other stuff. And like, I think this can explain a lot of improved
performance. But then if you fine tune for much longer, you're basically just training the model and it
will start to learn more circuitry, more features, more algorithms, more knowledge of the world. And yeah,
like no one's really checked. And definitely the longer you fine tune it, and the more you're using
weird techniques like reinforcement learning from human feedback, the less I'm confident in this claim.
And yeah, if we discovered that every time you fine tune a model, it will wildly change all of the internal
circuitry, I'd be like some more pessimistic about Mechantup, unless we can get very good at the
automated part, which might be able to get good at. I very much think of the field as we're trying to do this
hard ambitious thing. We're making a lot of progress, but I really wish we're making way more progress
way faster. And you viewer could help. But I don't know where the difficulty bar is for being useful, or the
difficulty bar is for being like, incredibly ambitiously useful. And it's plausible already at the point where
Mechantup can do real useful things no one else can, or no other techniques can. It's plausible, it will take
like five years to get to that point. I don't really know.
So I wanted to talk about this concept of needs and scruffies. So there have been two divisions in AI
research going all the way back to the very, very beginning. And you've said that sometimes understanding
specific circuits can teach us universal things about models which bear unimportant questions. So this reminds
me of this dichotomy between the needs and the scruffies. Now you seem like a need to me, a need to someone
who is quite puritanical. And also it's related to universalism. So this idea that there are simple underlying
principles that explain an awful lot of things, rather than wanting to accept the gnarly kind of reality that
everything's so bloody complicated. Where do you fall on that?
So I definitely would not... Okay, so there's two separate things here. There's like, what's my aesthetic?
Well, I want things to be neat. I want them to be beautiful. I want them to be mathematical. I want them to be elegant.
And then there's, what do I do in practice? And what do I believe is true about networks?
Well, I think there is a lot more structure than most, than many people think. But I also do not think they are just some
beautiful, purely algorithmic thing that we could uncover if we just knew the right tools. And like, maybe they are.
We'd fucking great if they were. But I expect they're messy and cursed, but with some deep structure and patterns
and how much traction we can get on the weird scruffiness, just like someone unclear to me, I think we can make a lot more progress than we have.
But we might eventually hit a wall.
You were saying something quite interesting when we drove over, which is, I mean, my friend Waleed Sabah, he's a linguist
and he's a Platonist. He thinks that there are these universal cognitive priors and there's a hierarchy of them.
And the complexity collapses. And he thinks that language models have somehow acquired these cognitive priors.
And if we did some kind of symbolic decomposition, you know, we would all just kind of like pack itself into this beautiful hierarchy.
And you were saying that they're a gab or filters and they're all these different circuits and they have motifs, they have categories,
they have flavors for want of a better word. Are you optimistic that something like this could happen?
Yeah. So, hmm.
So one interesting one interesting point here is often interoperability is fairly different for different modalities and different architectures.
A lot of the early work was done on convolutional networks and image classifiers.
The field very much nowadays focuses on transformer language models.
And I think there's lots of structure to how transformers implement algorithms.
Transformers cannot be recursive, but they're incredibly paralyzed.
Transformers have this mechanism of attention that tells them how to move information between positions.
And there's lots of algorithms and circuitry that can be expressed like this and lots of stuff that's really weird to express.
And I think that this constrains them in a way that creates lots of interesting structure that can be understood and patterns that can be understood.
Is this inherently true of intelligence? Who knows?
But a lot of my optimism for structures within networks is more like that.
But I try to think about structure more from a biologist's perspective than a mathematician's or like philosophers perspective.
Though I am a pure mathematician and I know nothing about biology.
So if anyone's listening to this, notice off of biology and thinks I'm talking bullshit, please email.
So if you look at evolutionary biology, model organisms have all of this common shared structure.
Like most things have bones, we have cell nuclei.
The hands of mammals tend to be surprisingly similar, but kind of weird and changed in various ways.
And I don't know.
I don't think these are like hard rules. Most of them have weird exceptions.
And obviously a lot of this is due to the shared evolutionary history and is not just inherent to the substrate of you have proteins.
Though in fact, you often train these models on similar data in similar ways and they have the same architecture that constrains them to different kinds of algorithms.
Makes me optimistic. There's a biologist's level of the structure.
Now you said something interesting, which is that transformers can't be used in a recursive way.
Now we'll just touch this very quickly because we've spoken about this a million times on different episodes.
But you know, there's the Chomsky hierarchy and he had this notion of a recursively enumerable language.
And these different models, computational models in the Chomsky hierarchy, it's not only about being able to produce a language
which exists in a certain set, it's also the ability to recognize that the language belongs in a certain set.
And transformers are quite low down on that hierarchy because they're called recurrently not recursively.
But I just wondered if you had any just, you know, prima facie if you had any views on that.
Yeah, so I'm not a linguist and not particularly familiar with the Chomsky hierarchy.
I do think it's surprising how well transformers work.
And I have a general skepticism of any theoretical hierarchy, like, I don't know.
If you think there's some beautiful structure of algorithms and stuff that's low down is totally doomed,
and then GPT-4 happens, I think a framework's wrong rather than transformers wrong.
Just massive stack of matrices plus a massive pile of data gives shockingly effective systems.
Theoretical frameworks just often break when they make contact with reality.
Well, that's certainly true. I mean, there's a famous expression that all grammars leak.
But I had rather, I don't know, I guess a similar conclusion to you, which is that if anything,
it teaches us how sclerotic and predictable language is.
And we don't actually need to have access to this infinite space or even exponentially large space.
Most language use and most phenomena that we need, perhaps for intelligence, is surprisingly small
and current models can work just well.
Why don't we move on to your grokking work?
So grokking is this sudden generalization that happens much later in training after...
If I can add a brief clarification.
Oh, yes, of course.
So people often call grokking sudden generalization.
My apologies. Go on.
Sudden generalization is a much more common phenomena than grokking.
It can just generally look like things like, I don't know, the model trying to learn a task,
it's kind of bad at it, and then it suddenly gets good at it.
And I prefer to call this a phase transition.
Right.
Grokking is the specific thing where the model initially memorizes and does not generalize.
And then there's a sudden phase transition in the test loss, the generalization ability,
which creates a convergence after an initial divergence between train and test.
And this is like a much, much more specific phenomenon than sudden generalization in general.
Okay.
So you've spoken about three distinct phases of training underlying grokking.
So why don't we go through them one by one?
Yeah.
So the context of this project, this was a paper called Progress Measures for Grokking
via Mechanistic Interpretability that I recently presented on at iClear.
Yes, we were studying a one-layer transformer.
We trained to do modular addition, and it grokked modular addition.
And the first thing we did was reverse engineer the algorithm behind how the model worked,
which we may get into in a bit more detail, but at a very high level.
Modular addition is equivalent to composing rotations around the unit circle.
Composition adds the angle, circle gives you modularity.
You can represent this by trick functions and do composition with trick and entities
and element-wise multiplication.
And we reverse engineered exactly how the model did this.
And then this mechanistic understanding was really important for understanding what was up with grokking,
because the weird thing behind grokking is that it's not that the model memorizes,
or that the model eventually generalizes.
The surprising thing is that it first memorizes and then changes its mind and generalizes later.
And generalization and memorization are two very different algorithms that both do very well on the training data.
And only by understanding the mechanism where we're able to disentangle them.
And this meant we could look during training how much of the model's performance came from memorization
and how much came from generalization.
And we found these three distinct phases.
There was memorization.
The first very short phase, it gets phenomenally good train loss.
It got to about 3E-7, which is an absolutely insane log loss.
And much, much worse than random on test, because memorization is very far from uniform and generalizes extremely badly.
And then there was this long-seeming plateau.
We call the space circuit formation, because it turns out that rather than just continue to memorize for a while
and doing a random walk through model space until it eventually gets lucky,
the model is systematically transitioning from memorization to generalization.
And you can see that its train performance gets worse and worse when you only let it memorize.
And then, so why is test loss still bad?
Test loss is bad, because memorization generalizes terribly.
And when the model is like, I don't know, two-thirds memorizing, one-third generalizing, this still does terribly.
And it's only when the model gets so good at the trigger-based generalizing algorithm
that it no longer needs the memorization parameters and cleans them up that we see groping.
And this happens fairly suddenly.
But the, if you, we have this metric called restricted loss,
where we explicitly clean up the memorization for the model and look at how well it generalizes.
And we see that restricted loss drops noticeably before test loss drops,
showing that the drop is driven by cleaning up the noise.
And this is striking, because A, I had no idea it was even possible for a model to transition between two good solutions,
maintaining equivalent performance throughout.
B, there was this real mystery of deep learning that many people tried to answer,
and mechanistic understanding was genuinely useful for answering it.
And groping was an illusion.
It was not sudden generalization.
It was gradual generalization followed by sudden cleanup.
And test loss and test accuracy were just too course-metric to tell the difference.
But we were able to design these hidden progress measures using a mechanistic understanding that made everything clear.
And we also just have all kinds of pretty animations of qualitatively watching the circuits develop over training.
And it's very pretty.
So a few things.
I mean, first of all, just going back to first principles.
The biggest problem in machine learning is this concept called overfitting.
And we trained the model on a training set.
And there's this horrible phenomenon called the shortcut rule,
which is that the model will take the path of least resistance.
And when you're training it, it only really knows about the training set.
And of course, we can test it on a different set afterwards, which we've held out.
And just because of the way that we've structured the model, it may, by hook or by crook, generalize to the test set.
But the interesting thing is that generalization isn't a binary.
There's a whole spectrum of generalization.
So it starts with the training set, and then we have the test set.
And then like, you know, the ideal is out of domain generalization.
But I would go a step further.
There's also algorithmic generalization, which is this notion that as I understand it, neural networks,
if you model the function y equals x squared, it will only ever be able to learn the values of that function inside the training support.
So presumably you're talking about the ideal form of generalization being not as good as algorithmic generalization,
or do you think it could go all the way?
So I think one thing which is very important to track is what the domain you're talking about is of which it's even possible to generalize.
So I generally think about models that have discrete inputs rather than continuous inputs,
because basically no neural network is going to be capable of dealing with like unbounded range continuous inputs.
In modular addition, there were just two one-hot encoded inputs between zero and 113, which is the modular way used.
Yeah, the model has a fixed modular.
It's not doing modular addition in general.
And there's just like 12,000 inputs and it learns to do all of them.
And in, I don't know, behaviorally, you can't even tell the difference between the model memorizes everything and the model learns some true algorithm.
Though with the more cognitivist mechanistic approach, I can just look at it and say, yep, that's an algorithm.
It's great.
Not a stochastic parrot.
Conclusively disprove that hypothesis.
And yeah.
I think that for the language models, it's more interesting because I know dbd2, it's got a thousand tokens, 50,000 vocab.
It's like 50,000 to the power of a thousand possible inputs.
And there's a surprising amount of interesting algorithmic generalization.
We're going to talk later about induction heads, which is the circuit language models learn to detect and continue repeated text.
Like if given the word Neil, you want to know what comes next.
Unfortunately, Nanda is not that high on the list yet.
But if Neil Nanda has come up like five times before in the text, Nanda's pretty likely to come next.
And this transfers to if you get the model just random tokens with some repetition, the model can predict the repeated random tokens because the induction heads are just a real algorithm.
And the space of possible repeated random tokens is like enormous.
It's like, in some sense, much larger than the space of possible language.
And is this algorithmic generalization?
I don't really know. It depends on your perspective.
Let's bring in this paper by Bilal Tughtay.
So it was called a toy model of universality, reverse engineering how neural networks learn group operations and you supervise that paper.
And he was asking the question of whether neural networks learn universal solutions or these idiosyncratic ones.
And he said he found inherent randomness, but models could consistently learn group composition via an interpretable representation theory.
So can you give us a quick tour de force of that work?
Yeah, maybe I should detour back to my grokking work and just explain the algorithm we found there and how we know it's the real algorithm.
Yeah, sure.
This is a good foundation for this paper.
Sure, sure.
Yeah, so we found this thing we call the Fourier multiplication algorithm.
The very high level it composes rotations.
You can actually look at how the difference of the model implement the algorithm and often just read this off.
So the embeddings are just a lookup table mapping the one hot encoded inputs to these trig terms, signs and cosines are different frequencies.
You can just read this off the embedding weights.
Note, people often think that learning sign and cosine is hard.
It's actually very easy because you only need it on 113 different data points, such as a lookup table.
The model then uses the attention and MLPs to do this composition to do the multiplication with trig identities to get the like composed rotation, the A plus B terms.
And here we can just read off the neurons that they have learned these terms and that they were not there beforehand.
The model is using its non linearities in interesting ways to do this.
It's also incredibly cursed because revenues are not designed to multiply two different inputs.
But it turns out they can if you have enough of them and it's officially cursed.
And yeah, we can just read this off the neurons.
Also, if you just put anything inside the model, it's beautiful and it's so periodic and I love it.
Could I touch on that?
Because you said you don't need to know the sign function because you can just memorize it within an interval.
Is that is that?
I don't know how does that break down because it's discretizing it and it's kind of assuming that it has the same behavior in different intervals.
So I think a key thing here is that you are solving modular addition on discrete one-hot encoded inputs rather than for arbitrary continuous inputs.
Arbitrary continuous inputs is way harder.
And so you it's not even on an interval.
It's just learning snapshot.
It's just learning like single points on the sign and cosine curves.
And I don't know, there's this family of maths about studying periodic functions with different kinds of Fourier transforms.
And this is all discussing discrete Fourier transforms, which are just a reasonable way of looking at periodic sequences of length n.
And that's how I recommend thinking about this one.
It's kind of like just quite different from a model that's trying to learn the true sign and cosine function.
And yeah, the model then needs to convert the composed rotation back to the actual answer, which is an even more galaxy-brained operation that you can read off from the wits.
So you've got terms of the form cos A plus B.
The model has some weights mapping to each output C.
And it uses further trig identities to get terms of the form cos A plus B minus C times some frequency.
And where A and B are the two inputs, C is the output.
And you then use the softmax as an argmax.
It's like extracts the C that maximizes this.
And because cos is maximized at zero, this is maximized at C equals A plus B.
And if you choose the frequency right, this gets you mod n.
And you can just read this off the model weights. It's great.
And then finally, you can verify you've understood it correctly because if you ablate everything that our algorithm says should not matter, performance improves.
While if you ablate any of the bits our algorithm says should matter, performance tanks.
Could you give me some intuition though?
So we start off in the memorization phase because I guess you can think of a neural network as doing many different things in a very complicated way.
And there's some kind of change in the balance during training.
So it does the easy thing first and then it gradually learns how to generalize.
And in this particular case, how does that thing, because we're using stochastic gradient descent.
So we're moving all of these weights around and the inductive prior is also very important.
And we'll come to that I think after we've spoken about Bill House paper.
But how does that happen gradually in really simple terms?
Is the question kind of ends up at this discrete algorithm, but it does so far continue steps.
How does that work?
Well, I think the thing that surprised a lot of people about grokking is this, I mean, grokking the clues in the name.
So it's gone from memorization.
And then we're using stochastic gradient descent and you would think that it's gotten stuck in some kind of local minimum.
And you're training and you're training and you're training and then there's a spark.
Something happens and then you get these new modes to kind of like emerging in the network.
Not sure if emerging is the right term.
And it happens gradually and it happens after a long time.
Yeah.
So there's a couple of things here that's pretty easy to misunderstand.
The first is that the first is that I think it's pretty hard for a model to ever get stuck because.
I know this model had about 200,000 parameters model ones have billions.
It's just moving in a very high dimensional space and you can get stuck on 150,000 dimensions.
You got 50,000 to play with and especially for a fairly under parameterized model.
Sorry for a fairly over parameterized model like this one for a fairly simple task.
They're just like so much room to move around.
Another common misunderstanding of Grocking is people say it's memorized.
It's got zero loss.
So why does it need to learn?
Two misunderstandings here.
First, zero loss is impossible unless you have bullshit floating point errors because it's log.
It's like the average correct log prop log of anything can never get to the log will never quite get to zero because of just how softmax works.
And you need to have an infinite logic for that to happen.
The one cute thing in an appendix to our paper is that float 32 cannot represent log probes less than 1.19 e-minus seven,
which leads to bizarre loss spikes sometimes unless you use float 64.
Anyway, the second is regularization.
If you don't have any kind of regularization, the model will just continue to memorize.
We use weight decay.
Dropout also works.
And so the model, the kind of core tension behind Grocking is there's some feature of the lost landscape that makes it easier to get to memorization.
You can memorize faster while generalization is somehow hard to get to and much more gradual.
So the model memorizes first, but it ultimately prefers to generalize, but it's only a mild preference.
And the reason for this is we cherry pick the amount of data where it's a mild preference because there's too little.
It will just always memorize if there's too much it will immediately generalize because Grocking is a little bit cheating.
And yeah, you then use this and because the model is initially memorized, but it wants to generalize, it can follow.
It memorizes until the desire to memorize more balances with the desire to have smaller weights.
But both of these reinforce the drive to generalize because both because that makes both of them happier.
And so the model very slowly interpolates very, very slightly improving test test loss, very slowly improving train loss until it eventually gets there.
And has this acceleration at the end, this phase transition and cleanup, which leads to the seemingly sudden Grocking behavior.
Okay. And when you were talking about the, it wants the weights to be smaller.
So that's weight decay.
And it's like an inductive bias, essentially to tell the model to reduce its complexity, which is a pressure to generalize.
But if it wasn't for that, then that wouldn't happen.
So in the experiments I ran, if you don't have weight decay, it will just keep memorizing infinitely far.
Because when you get perfect accuracy, if you double all your logits, you just get more confident in the right answer.
And so it just keeps scaling up.
I was using full batch training because it's such a tiny problem.
This made things smoother and easier.
I've had some attic data that sometimes you can get it to work if you just have mini batch stochastic gradient descent.
But I haven't looked into that particularly hard.
Interesting.
There are some hypotheses that stochasticity acts as an implicit regularizer because it adds noise.
I don't really know.
So let's go back to Bilal's paper then.
So this paper, a toy model of universality, reverse engineering, how neural networks learn group operations.
Can you give us an elevated pitch?
Yeah.
So an observation that actually first discovered at a party in the Bay Area from a guy called Sam Mox is that the modular addition algorithm we found is actually a representation theory algorithm.
So group representations are collections of symmetries of some geometric objects that correspond to the group.
Modular addition is the cyclic group.
And rotations of the regular n-gon are the representations of the cyclic group.
And this corresponds to the rotation by the unit circle, the compose that we found.
But it turns out you can just make this work for arbitrary groups.
You replace the two rotations with just two representations.
You compose them and the model.
And it turns out the Cos A plus B minus C thing is this math jargon called the character.
You don't mean to unsat any of that, but it's very cute if, like me, you have a pure math degree.
And for example, if you have the group of permutations of five elements, the 120 different ways to rearrange five objects, one example of representations of this are rotations
and reflections of the four-dimensional tetrahedra.
And if you train one hidden layer MLP to grok this and look inside, you're going to see these rotations that it's learned.
It's gorgeous.
And so the first half of that paper was just showing that the algorithm worked, showing that this was actually learned in practice.
Then the more interesting bit was this focus on universality.
So universality is this hypothesis that models have some intrinsic solutions to a problem that many different models will converge on,
at least given similar data and similar architectures,
e.g. in image models, models will learn specific neurons that detect curves,
and different models and different data sets seem to learn this similar thing.
And here, this was interesting because groups have a finite set of irreducible representations, math's theorem.
You can enumerate these. There are that many of them.
And for groups that are not modular addition, these are qualitatively different,
like some of them act on a four-dimensional object, like the tetrahedron, some of them act on, like, 5D or 60 objects.
Naively, some of them are simpler than others, but they're definitely different.
And so what we did is we asked ourselves the question, which one does the model learn?
And we found that even if you just vary the random seed, the model will randomly choose a subset of seeds each time to learn.
And there's some structure, like it tends to learn some of them more often than others.
A little bit maps to our insurance of notion of simplicity, but not that much.
One of the updates I made in the paper is that simplicity is a really cursed concept I don't understand very well,
where I don't know if you have rotations of a four-dimensional object that seems simpler,
but maybe the 60 object takes more dimensions but has better loss per unit weight norm, which is simpler.
I don't know.
But yeah, anyway, we found that each run of the model learns some combination of these circuits for the different representations.
It's like normally more than one, the exact number varies, and which ones it learns is seemingly random each time,
which suggests that all toy models like you, obviously.
But if we're trying to reason about real networks, looking at this work might suggest the explanation, the hypothesis,
that if there are multiple ways to implement a circuit, which in practice they normally are,
models may learn different ones of them, kind of for fairly random reasons,
and the fully understanding one model will not perfectly transfer to another model.
And I think there's loads of really interesting open questions here.
And no, people have done various work understanding different kinds of specific circuits and models,
like the interoperability in the wild paper we'll get to later.
What does this look like in other models?
Often there's multiple ways to implement a circuit.
Can you disentangle the two? Do all models learn both?
Or do some models learn one, some learn the other?
I don't really know.
So a couple of questions.
I mean, first of all, this is leading towards this idea that we were speaking about before,
which is that even in different networks, slightly different problems or variations on the same problem,
it could learn these algorithmic primitives.
Now, the first observation here is that the inductive biases of the network differ massively, right?
So to what extent do the inductive biases affect these primitives which are learned?
Oh, so much.
They do.
Well, could I frame the question a little bit?
Because this reminds me a lot of the geometric deep learning blueprint from Petir and Michael Bronstein and all those guys.
And they were coming at this from exactly the same direction as you that they said there's a representation of a domain,
which is basically a symmetry group.
And you can do all of these different transformations and as long as they fall in different positions in the underlying domain,
so they respect the structure, then it works.
But all of those symmetries are effectively coded into the inductive prior.
So for example, if a CNN works on this gridded 2D manifold and it explicitly models translational equivalents and local connectivity and weight sharing and so on.
So I guess what I'm saying is like you're talking about this four dimensional tetrahedron and that isn't explicitly modeled in an MLP.
So how are you even recognizing that it's learning those symmetries?
How are you even probing it? Maybe we should start with that.
So I guess thing one, models are just smarter than you, man.
Models can do a lot of weird stuff.
I feel like the story of deep learning is people initially thought they needed to spoon feed to these models, the right inductive biases over the data.
And we've gradually realized, oh, wait, no, no, this is fine. The models can figure it out.
For example, early on image models were convolutional networks.
You tell it the key information is nearby.
And if you translate the image, it doesn't matter.
And now everyone uses transformers, including for images and transformers replace the convolutional mechanism with attention where you're now saying, okay, one sixth of your parameters are dedicated to figuring out where to move information between positions.
Sometimes it'll be a convolution and sometimes models do learn convolution, but often it won't be.
And we want you and you can now spend the parameters to figure this out.
And I'm not very familiar with the deep with the geometric deep learning literature, but I generally am just kind of like models can figure it out.
The way we figured out that this was what's going on is kind of analogous to what we did in the modular addition case, where we just look at the embedding matrix and just read off the learn sign and cosine terms.
Here we said, okay, the rotations of the 4D tetrahedron are these like four by four matrices.
You can flatten this to a 16 dimensional vector. Let's probe for that linearly.
And this kind of works.
And you can probe the different representations and basically see what's going on.
Okay, I think that the thrust of the geometric deep learning stuff or any inductive prior comes back to the bias variance trade off and the cursive dimensionality.
So no one's saying, of course, an MLP, if you look at the function space that it can approximate, it's exponentially larger than that of a CNN.
So it was always about sample efficiency.
So yeah, an MLP can learn anything that we would never be able to train it the most problems.
Yeah.
So I guess I maybe want to avoid going too deeply into this because I think the modular addition problem and the group problem is just a very weird problem.
There's an algorithm that it's fairly natural for a model to learn with literally a single nonlinear step of the matrix multiply.
One very cute result from the last paper is that the model can implement to four by four matrix multipliers with a single ReLU layer, which is very cute.
But yeah, there's like a fairly natural algorithm to implement.
That's a certain, yeah, another useful intuition is that the more data you have, the more complex memorization gets while generalization is exactly as complex at each point.
And yeah.
So there's kind of always going to be a crossover point if you have enough data, where it is simpler to learn the circuit that generalizes.
And I don't know, I'm hesitant to draw too much from toy models about the real problem.
I guess one, two final points I don't want to just leave on the section.
The first is I just want to re-emphasize I did not do the toy model of universality paper.
I was supervising a mentee, Bill of Chugtai, who did it.
He did a fantastic job.
So thanks, Bill, after listening.
Secondly, for the modular addition case, I had no idea this outcome was going to be there when I went in.
I just poked around, noticed the weird periodicity, realized it was using, I should apply Fourier transforms and then the whole problem kind of fell together.
And to me, the real takeaway of this paper is like, I don't give a fuck about Grockin.
It is genuinely possible to understand what is going on in a model.
You don't need to know what's going on in advance to discover this.
And there is beautiful, non-trivial structure that can be understood.
And who knows if this will happen in like actual full models.
But to me, this is much more compelling than if we had nothing at all.
Beautiful.
Okay.
And just before we move off the section, Biloud had a beautiful Twitter thread, actually.
And he was talking about the potential for what he called a periodic table of universal circuits.
And I actually think that's a really cool idea.
So that would be amazing if that would work out.
But he also brought up the lottery ticket hypothesis.
And I've interviewed Jonathan Frankel.
And the idea there is that some of this information might actually be encoded
and understandable at initialization before you even start training.
And apparently you folks have found weak evidence for this in at least one group.
Ah.
All right.
So a couple of things there.
So this idea of a periodic table of circuits, I believe is originated in this post called
circuits zoom in from Chris Ola.
We probably cannot claim that.
It's a beautifully evocative term.
Yeah.
The story of basically everything in Mac and Turp is, yeah, there was this Chris Ola paper from like two years ago that has it somewhere inside.
Anthropic recently put out this beautiful blog post called Interpretability Dreams about their vision for the field of mechanistic interpretability.
And the kind of subtext.
So they kept just quoting bits of old papers being like, so we already said this, but let's now like summarize it better and be clear about how this sits into our overall picture.
Anyway, so yeah, the idea of the periodic table is maybe there is just some finite list of ways a thing can be implemented naturally in a massive stack of matrices that we can enumerate by studying one or maybe several
networks and the sand them and then compile all of this into something beautiful.
And which is kind of what we found in the representations case, though here it was nice because there were genuinely a finite set that we could have fully enumerate.
Regarding the lottery ticket stuff.
I think this was a random observation I had on the Modular Edition case, partially inspired by a result from Eric Michaud at MIT, who was involved in some other papers on Grockin.
And so what we found is that at the end of training, there are these directions in the weights that represent like the sine and cos terms of frequency 14 pi over 113.
And if you look at the embedding at the start and prediction to these directions, it's like surprisingly circular.
It's like the model has extracted those directions.
And my wildly unsubstantiated hypothesis for why models learn these algorithms and circuits at all is that there are some directions that if you delete everything else would like form this beautiful circuit.
This is kind of a trivial statement about linear algebra for the most part.
And this underlying hidden circuit, each bit reinforces each other systematically because they're useful.
Well, everything else is kind of noise, so it gets kind of gradually decayed.
And so over time, this will give you the circuit in a way that looks surprising and emergent.
And this also can partially explain why phase transitions happen.
There was a really good post from Adam German and Bach Schlegres called on S shaped curves, which argue that if you've got something that's like the composition of multiple different weight matrices, let's just say two of them.
The gradient on the first is proportional to how good to the second is and vice versa.
So the start, they both grow very slowly, but then they'll reinforce each other and eventually cascade as they're optimizing on the problem in a way that looks kind of sudden.
And so my understanding is the original lottery ticket hypothesis is kind of discreet.
It's looking on the neuron level and it's learning masks over weights and over neurons.
And I'm kind of discussing and in some sense much more trivial version where I'm not assuming there's some canonical basis of neurons.
I'm saying well there's some directions in space that matter and if you delete all other directions, everything kind of works.
Which I think is a much more trivial statement, though the space of possible neurons is enormous.
Though I don't know. One thing you want to be pretty careful of when discussing this stuff is how much the mask you learn is the computation.
And there's probably quite a lot of algorithms can be cleverly expressed with a mask over a Gaussian normal matrix.
But I don't know.
Part two, how do machine learning models represent their thoughts?
Now we're taught in machine learning 101 that neural networks represent hypotheses which live on a geometric domain
and inductive priors learn to generalize symmetries which exist on the underlying geometric domain.
And you're talking about them representing a space of algorithms which we're going to explore.
Now one thing that I wanted to touch on is that they learn a mapping to extensional attributes, not intentional attributes.
Intention spelled with an S and we'll come back to what I mean by that in a second.
But I think it's quite popular for people to think of neural networks principally as a kind of hash table.
Or locality sensitive hash table.
And the generalization part comes from the representation mapping function which is on this embedded Hilbert space
which is the vector space of the attributes which then resolves a pointer to a static location on the underlying geometric domain.
Now this can mimic an algorithm especially when the inductive prior itself is increasingly algorithmic like a graph neural network for example
which behaves in a very similar way to a prototypical dynamic programming algorithm.
There's some great work actually on algorithmic reasoning by Petr Felichkiewicz, one of your colleagues now at DeepMind.
But he showed in his algorithmic reasoning work that transformers can't perform certain graph algorithms.
I think he gave Dykstra as an example and he said it's because there's this aggregation function in a transformer which isn't in a GNN.
So I just wondered if you could kind of like compare and contrast whether or not neural networks are performing algorithmic generalization
and the differences between let's say GNNs and transformers.
Yeah, so I'm not very familiar with GNNs so I'll probably avoid commenting on GNNs versus transformers so a fear of embarrassing myself.
In terms of the underlying thing, so I definitely think we have some pretty clear evidence at this point that models are doing some genuine algorithms.
I think my modular addition thing is a pretty clear proof of concept of this.
Yeah, so one thing worth stressing is that I generally think of models as having linear representations more than geometric representations.
So I think of an input to a model as having many different possible features where features are kind of a property of the input in an intentional sense.
But which is kind of a fuzzy and garbage definition.
So I prefer the existential definition of like an example of a feature is like this bit of an image contains a curve.
Or this bit of an image corresponds to a car window.
Or this is the final token in Eiffel Tower.
Or this corresponds to a list variable in Python with at least four elements and all kinds of stuff like that.
And well I don't know.
This scene is shaded blue because someone put the wrong filter on the camera.
And yeah, I generally think of models as representing features as linear directions in space.
And each input is a linear combination of these directions.
And this is kind of the classic words to Beck framing, like the king minus man equals queen minus woman thing.
Where you can kind of think of this as there being a gender direction and there being a royalty direction.
And these are like the right units of analysis rather than king, queen, man, women being the right units of analysis.
But where each of these is made up out of these underlying linear representations.
And this is a fairly different perspective to the geometric where things in a manifold how close are they together in Euclidean space.
Because that's all kind of a global statement about how close two things are.
Where you're comparing all possible features while I don't know.
The Eiffel Tower and the Colosseum are close together in some conceptual space because they are both European landmarks.
But they're also very different because France and Italy are fairly different countries in some sense.
And maybe they're different on a bunch of other features or one of them is two words.
The other is one word, which really matters in some ways.
And Euclidean distance and geometry is it's a global summary statistic.
And all summary statistics like you is another motto of mine.
But in particular, global ones I'm very skeptical of.
And yeah, in general, this how what is the structure of a model representations.
I think it's like a really important question.
And in particular, models are such high dimensional objects that you really want to be careful to distinguish between the two separate things of.
Sorry, models are such high dimensional objects.
That it's basically impossible to understand GBD 3 is a 200 billion dimensional vector.
You need to be breaking it down into units of analysis that can vary independently and independently meaningful.
And the linear representation hypothesis is like a pretty load bearing part of how I think about this stuff.
Because it is so because it allows you to break things down.
And it seems to be a true fact about how models do things.
Though again, we don't have that much data because we never have enough data.
It's really sad.
And yeah.
So this linear representation hypothesis, this idea that the models break down inputs into many independently varying features and store them as directions in space, much like word to vex.
And the the go fi people, I mean, like photo and pollution, they, they, they brought out this famous critique of connectionism in 1988.
And their main argument was systematicity.
And they were talking about intention versus extension.
And it might just be worth defining what I mean by that.
So if I said the teacher of Socrates was Plato, the extension is Plato.
The intention is everything.
It's the teacher.
It's Socrates.
You know, if I said four plus five equals nine, nine is the extension for and plus and five is the intention.
So they were saying something very simple.
They said in a neural network, the intentional attributes get discarded.
And that's why the networks don't support what they call compositionality.
Now compositionality is actually quite an abstract term because using vector algebra in these analogical reasoning tasks that you were just talking about.
So king and queen and so on.
That's a form of compositionality.
But they would say it's a poor cousin of compositionality because it's only using, you know, the, the, the representation is in a, is in a vector space.
And in a vector space, you only have very basic primitive transformations.
So you wouldn't be able to, I mean, for example, you're talking about Paris earlier.
You wouldn't do the kind of analogical reasoning they were talking about being able to downstream, say, were they in Paris is Paris in Europe.
Of course, it does happen in this linear representation theory, but it happens in a very different way.
Hmm.
So I guess I'm not sure I fully followed that.
I mean, this might be a cheap gotcha, but a fact about transformers is there's, they have this central object called the residual stream.
Which I know in standard framing to be thought of as the thing that lives in the skip connections.
But not even as like the key thing about a transform where each layer reads its input from the residual stream and adds its output back to the residual stream.
And the residual stream is kind of this shared bandwidth and memory.
And this means that nothing's ever thrown away unless the model explicitly is trying to do that, or is just applying some gradual decay over time.
So, you know, if you've got an MLP layer that's saying I've got four, I've got plus, I've got five, and I want to compute nine, four, five and plus is still there.
I don't know if this actually engage with your points and like, I don't know if this matters, but it's true.
Yeah, what you're saying is true, but I think the point is that those primitives are not actually representable in a neural network.
So you're saying with this residual stream, all of the extensions that came previously also get passed up.
So in a later layer, you can refer to an extension.
So the basically the answer of a computation that happened upstream, but what you can't refer to are the intentional attributes of that computation upstream.
Why not like four is an input.
So you can refer to four, because you could think of reading the input as a computation.
Plus is another thing you read five is another thing you read.
Like, what is a thing that is not an output of a computation within this framework?
I might have to get back to you on that.
Where's Keith Duggar when you need him?
Would be a good example of that.
I mean, I guess it's about symbol manipulation as well.
So these these things could actually be symbolic operations, which can be composed and reused later.
And you would appreciate that a neural network is only ever passing values.
So, for example, if it did something which you could represent with a symbolic operation, if you wanted to use that again, I mean in an MLP,
the reason why we use a CNN is because we want to represent the same thing in different places and an MLP would have to learn it.
It doesn't support translational equivalence.
So it would have to learn the same thing a million times.
And it's the same thing with this symbolic compositional generalization.
But if it actually had this symbolic representation, which it used once, it could use it everywhere, but now it has to relearn it everywhere.
Right.
Like, you could, if the water wants to know that Paris, the capital of France, it can spend some parameters on that.
And for every other capital, it needs to separately spend parameters.
And it can't just have a general map country to capital operation.
Yeah, that's exactly right.
I mean, let's use a simple example.
So we use an MLP image classifier.
And I put a tennis ball in and it's in the bottom left of the visual field.
And then I put it in the top right and nothing it's learned from the bottom left will be used.
So it just feels like we're wasting the representational capacity just doing the same thing again and again.
And in a transformer, the only reason it does have that recognition, you know, that that's a equivalence in respect of the position of a pattern is because of the transformer inductive prior, presumably.
Yes.
Yeah.
So it uses the same parameters at each position in the input sequence.
You know, it should be able to do bottom left and top right properly.
Though it does not necessarily have things like rotation built in.
I don't know.
I feel like machine learning is full of these people who have all kinds of theoretical arguments.
And then they're like, this should be efficient.
This should not work.
And then GPT for lobster.
And I don't know.
No theory.
No theory is interesting and isolation lesson models reality well.
And I don't know.
I haven't really engaged with this theory in the same way I haven't engaged with most deep learning theory because it just doesn't seem to meet my bar of does this make real predictions about models.
The maximal update parameterization paper from Greg Yang was actually a recent contradiction to this.
Right.
Of really interesting theory that makes real predictions about models that bear out and get you zero shot hyper parameter transfer.
But like most things just don't do that.
Very interesting.
Okay.
Okay.
Well, I think now is a beautiful opportunity to move over to Othello.
Now there was a recent paper called do large language models learn world models or are they just surface statistics by Kenneth Lee.
And he said that the recent increase in model and data size has brought about qualitatively new behaviors such as writing code or solving logic puzzles.
Now he asked the question, yeah, how do these models achieve this kind of performance?
Do they merely memorize training data?
Well, are they picking up the rules of English grammar and grammar and the syntax of the sea language?
For example, are they building something akin to an internal world model and understandable model of the process producing the sequences?
And he said that some researchers argue that this is fundamentally impossible for models trained with guess the next word to learn the language meanings of language.
Their performance is merely surface statistics, you know, which is to say a long list of correlations that do not reflect a causal model of the process generating the sequence.
Now, you said Neil that a major source of excitement about the original Othello paper was that it showed that predicting the next word spontaneously learned the underlying structure generating its data.
And he said that the obvious inference is that a large language model trained to predict the next token may spontaneously model the world. What do you think?
Yes.
So I should clarify that that paragraph was me modeling why other people are excited about the paper.
But whatever, I can roll with this question.
And maybe bring in your less wrong piece as well.
Yes.
So the, yeah, I thought careless paper was super interesting.
The exact setup was they train.
So Othello is this chess and go like board game.
They took a data set of random legal moves in Othello.
They trained a model to predict the next move given a bunch of these transcripts.
And then they probed the model and found that it had learned a model of the board state, despite only ever being told to predict the next move.
And so the way I would define what model is that there's some latent variables that generate the training data.
In this case, what the state of the board is.
These change over time, like over the sequence, but at least for a transform which has a sequence.
And the model kind of has an internal representation of this at each point.
And they showed that you can probe for this.
And they showed that you can causally intervene on this and the model will make legal moves in the new board.
Even if the board state is impossible to reach.
Point of order.
Can you explain what you mean by probe just just so that there's no moment.
Yes. So probing is this like old family of interactability techniques.
The idea is you think a model has represented something like you give it a picture and you tell it's a class for the image.
And you want to see if it's figured out that the picture is of a red thing versus a blue thing, even though this isn't an explicit part of the output.
You take some neuron or layer or just any internal vector of the model and you train some classifier to map that to like red or blue.
And you do something like a logistic regression to see if you can extract whether it's red or blue from that.
And there's also interesting enough about probing, but I should probably finish explaining the Othello paper first before I get into that tangent.
So yeah, the like reason people are really excited about this paper was recently an oral Eichler and generally got a lot of hype was that it was just you train something predict next token and it forms this rich emergent model of the world.
And forming a model of the world is actually incredibly expensive.
Like each cell of the 64 cell Othello board has three possible states, three to the 64, it's quite a lot of information to represent.
But the model did it.
And lots of people were like, oh, clearly language models have walls.
My personal interpretation of all this is that language models predict the next token.
They learn effective algorithms for doing this within the constraints of what is natural to represent within transformer layers.
And what this means is that if predicting the next token is made easier by having a model of the world of like, I don't know who the speaker is.
This is a thing that will happen.
And in some work led by Wes Gurney that we're going to talk about later, we found neurons that detector things like this Texas in French, this Texas Python code.
And in some sense, this is like a particularly trivial one.
And so, yeah, that's an interesting thing.
In my opinion, it was kind of a priori obvious that language models would learn this if they if they could and needed to and it was more efficient.
And at the point forward to the learning that something is French seems categorically different.
So when I when I read Kenneth's original piece, he showed what looked like a topological representation of the world.
So how different state spaces were related to each other in a kind of network structure.
Hmm.
So.
I wonder if you can remember how we produced that diagram.
Yeah, so I'm starting to remember the details.
I think it was something of the form.
Look at how different cells are represented in the model and look at how close together the representations of different cells are.
And oh, the model has kind of got internal representations that are close together.
I don't think this is fundamentally different from the king, queen, man, woman thing.
It was just that it's like learn some structural representations that's obviously kind of reasonable.
Yeah.
I yeah, I wouldn't read too much into that like models learn structural representations.
I think as old news at this point.
But maybe another interesting angle is that one of the reasons why.
People like Gary Marcus, they say GPT is parasitic on the data.
They say because they are empirical models, most of the meaning most of the information is not in the data.
We have to reason over explicit world model.
So he thinks the reason a GPS is so good is because we've imputed this abstract world model.
And similarly, when we play chess, we have an abstract world model.
And he would argue that the information about that abstract world model doesn't exist in any data.
So how do you go from the data to the model?
And the Othello games seem to show that you could go from the data to the model.
Yeah.
So I know I think that viewpoint is just like obviously wrong.
Like you're trying you're trying to do a data prediction problem.
A valid solution to that is to model the underlying world and use this redeploy comes next.
There's clearly enough information and information theoretic sense to do this.
And the question is, is a model capable of doing that or not?
And I don't know.
I'm just like, you can't write poetry with statistical correlations.
You need to be learning something.
Maybe that's not a good example.
I don't believe you can write like.
Yes, you can.
I don't believe you can produce like good answers to like difficult code forces problems.
It's like, do good software engineering is purely a bundle of statistical correlations.
Maybe I have too much respect for software engineers.
I don't know.
So where does it come from?
That flash of inspiration or that higher level.
I guess the first question is, do you is there a jump?
Is it actually grounded in the data it's trained on?
Or is there some high level reasoning?
Where does that materialize from?
So the way I think about it, there was just a space of possible algorithms that can be implemented in a transformer's weights.
And some of these look like a world model.
And some of these look like a bunch of statistical correlations.
And models are trading off lots of different resources, like how many dimensions of this consume, how much weight norm, how many parameters, how hard is this to get to and how weird and intricate.
And models will choose the thing that gets the best loss that is most efficient on these dimensions, assuming they can reach it within the lost landscape.
Well, I use choose in a very anthropomorphic sense, like Adam chooses good solutions.
And I don't know if you have a sufficiently hard task and forming a world model is like the right solution to it.
Models can do it.
And I think people try to put all of these fancy philosophizing on it in a way that I just think is false.
Guilty as charged.
And I think the Othello paper is like a really beautiful, elegant setup that proves this.
All right, can I move on to the plot twist?
Does it prove it though?
It's a very small contrived.
It's a big jump to assume that that works on a large language model.
So this is kind of the argument I'm making.
I think there's the empirical question of do language models do this.
The theoretical question of could they do this?
And I'm saying, I think the theoretical question is nonsense.
And I think the Othello paper very conclusively proves the theoretical question is nonsense.
They're just like, yeah, when given a bunch of data, you can infer the underlying world model behind it in theory.
I would push back on that a tiny bit because it's very similar to AlphaGo proved that in a closed game,
which is systematic and representable, you know, with a finite, obviously exponentially large,
but a finite number of board states, you can build an agent which performs really, really well.
That seems to me completely different to something like language or acting in the real world that might not be systematic in the same way.
We can debate whether or not it's, I think it's an infinite number of possible trajectories,
just like language, an infinite number of possible sentences.
Man, there's 50,000 to the power of a thousand possible infant sequences.
Sure is a finite number.
You mean in Othello or?
No, in GPT2.
In GPT2, okay.
Bounded context length, bounded vocab size, generally.
Bastard.
You're right.
More than one quintillion characters, probably.
Yeah.
All right.
Being important, let's do it.
Yeah.
Well, I guess it is still a big jump, though, isn't it?
From yes, empirically, it shows that in Othello, it works.
Maybe we could debate whether or not it does or not.
Because there's always this question coming back to what we were saying before,
whether it's learning something which is universal or something which is still brittle.
The way that we've evaluated it might lead us to conclude that it's universal,
whereas actually it's brittle in ways that we don't understand.
That's a very real possibility.
Yeah.
Everything's brittle in ways you don't understand.
It's pretty rare that a model will do everything perfectly in a way that there are no adversarial examples.
This is one of the more interesting things that's come out of the adversarial example's literature to me.
It's just like, oh, wow.
There's so much stuff here.
There's such a high-dimensional input space.
There's all kinds of weird things the model wasn't prepared for.
My interpretation of the Othello thing is the strong theoretical arguments are wrong.
I separately believe that there are world models that could be implemented in a language model's ways.
But I also disagree with the strong inference of the paper that this does happen in language models
or that we conclude it does because world models are often really expensive.
Like, in the Othello model, it's consuming 128 dimensions of its 512-dimensional residual stream for this world model.
And the problem is set up so that the world model is insanely useful
because weather removers' legal is purely determined by the board state,
so it's worth the model's while to do this.
But this is rarely the case in language.
For example, there was all this buzz about Bing chat playing chess and making legal-ish moves.
And I don't know, man, if you want to model a chessboard,
you just look at the last piece that moves into a cell.
That's the piece in that cell.
You don't need an explicit representation.
You can just use attention heads to do it.
And there's all kinds of weird hacks.
And like, models will generally use the best hack.
But probably it is worth the model's while to have some kind of an internal representation.
Like, I'd bet that if you took a powerful code-playing model
and probed it to understand the state of the key variables,
it would probably have some representation.
But it's moving on to the work I did building on the Othello paper.
So one of the things that was really striking to me about the Othello work
is, simultaneously, its results were strong enough that something here was clearly real.
But they also used techniques that felt more powerful than when needed.
Like, rather, they found that linear probes did not work.
There weren't just directions in space corresponding to board states.
But the nonlinear probes, one hidden layer MLPs, did.
And the key thing to be careful of when probing
is, is your probe doing the computation, or does the model genuinely have this represented?
And even with linear probes, this can be misleading.
Like, if you're looking at how a model represents coloured shapes,
and you find a red triangle direction,
it could be that there's a red, green, or blue direction, and a triangle, square, or shape direction.
And you're taking the red plus triangle,
or it could be the case that each of the nine shapes has its own direction.
You found the red triangle one.
But nonlinear probing is particularly sketchy.
Like, in the extreme case, if you train GPT-3 on the inputs to something,
GPT-3 can do a lot of stuff.
If you train GPT-3 on the activation side network,
it can probably recover arbitrary functions of the input,
assuming the information on the input hasn't been lost,
which it shouldn't have, because there's a residual stream.
And what I said is not quite true, but not important.
And so I was...
And their intervention technique was both got, like, very impressive results,
but also involved doing a bunch of complex gradient descent against that probe.
And this all just seemed more powerful than was necessary.
And so I did the...
I challenged myself to do a weekend hackathon,
trying to figure out what was going on.
And I poked around at some internal circuitry,
and tried to answer some very narrow questions about the model.
And I found this one neuron that seemed to be looking for, like, three cell diagonal lines,
where one was blank, the other was white, the next was black.
But then sometimes it activated on blank, black, white.
And it turned out that the general pattern was that it was blank,
blank opponent's color and current player's color.
And this is a useful motif in Othello, because it makes the move legal.
And when I saw this, I made the build hypothesis.
Maybe the model actually represents things in terms of whether a cell has the current player's color
or the current opponent's color,
which in hindsight is a lot more natural,
because the model plays both black and white,
and it's kind of symmetric from the perspective of the current player.
And I trained a linear probe on this,
and it just worked fabulously, and got near perfect accuracy.
And I tried linear representations on it,
and I tried linear interventions, and it just worked.
And I even feel really excited about this project for a bunch of reasons.
First, law I did on a weekend, I'm still very proud of this.
Secondly, I think that it has vindicated
some of my general suspicion of nonlinear probing.
Like, if you really understand a thing,
you should be able to get a linear probe to work.
And kind of more deeply, as we discussed,
there's this words-to-vix style linear representation hypothesis about models
that features corresponded directions.
The Othello work seemed like pretty strong evidence against.
They had court interventions showing that the board state was there,
but actually, but linear probes did not work.
It seemed like they found some nonlinear representation.
And my and Chris Othello's hypothesis seeing this was that
there was a linear representation hiding beneath.
Martin Bodenberg, one of the authors of the paper,
had the hypothesis that it was like an actual nonlinear representation,
and this was evidence against the hypothesis.
And there's going to form natural experiment
where the hypothesis could have been falsified,
but my work showed there was a real nonlinear,
a real linear representation,
and thus that it had predictive power.
And so many of our frameworks for Mechantup
are just these loose things based on a bunch of data,
but not fully rigorous or fully conclusively shown.
And so natural experiments like this
feel like some of the best data we have.
On this linear representation,
I don't know if you've heard of the spline theory of neural networks
by Randall Ballastriero.
And without going into too much detail,
it's quite a discrete view of MLPs in particular
that the relus essentially get activated in an input sensitive way
to carve out these polyhedra in the ambient space.
And essentially, an input will be mapped into one of these cells
in the ambient space,
and then there's a kind of discreetness to it
because if you just perturb the input
and you move outside of one of these polyhedra,
then the model will, if it's a classifier,
classify something different.
But I guess I want to understand,
with this representation theory,
if features are directions,
does that imply there's a kind of continuity
because the network will learn to spread out
in directions in the best possible way?
But it won't necessarily be a way which is semantically useful.
Like in Word2Vec,
stop and go are very close to each other,
and they shouldn't be.
And at what point does stop become go?
So do you see there being boundaries in these directions?
So I think this is, again,
my point that I think of linear representations
as being importantly different from geometric representations.
Stop should be close to go
because in many contexts,
they are like a kind of changing of state term,
and it's used in similar contexts
and has similar grammatical meaning.
But then on this single semantic thing,
they're quite different.
And the natural way to represent this
is have them be close together in Euclidean space,
but have some crucial negation dimension where they're different.
And the contact,
and like ultimately neural networks are not geometric objects,
they are made of linear algebra.
Every neuron's input is just project the residual stream
onto some vector.
And this involves just selecting some set of directions
and taking a linear combination
of the feature corresponding to each of those.
And this is just the natural way for a model
to represent things in my opinion.
Okay.
Well, I think this will in a second lead us
on very nicely to superposition,
which is that we don't actually think
of there being one direction necessarily.
Just to close this little piece,
now you said in your less wrong article
that orthello GPT is likely over parameterized
for good performance on this particular task
while language models are under parameterized.
And of course, we have the ground truth to this task,
which makes it very, very easy.
So much easier to interpret.
100%.
But you did conclude saying that this is further evidence
that neural networks are genuinely understandable
and interpretable,
and probing on the face of it seems like a very exciting approach
to understand what the models really represent,
caveat, mentor, conceptual issues.
So let's move on to this superposition,
also known as poly semanticity,
which is an absolutely beautiful,
while you're shaking your head a little bit.
Maybe you start with that.
Yeah.
So there's...
All right.
So what's the narrative here?
So fundamentally,
we are trying to engage with models
as these high dimensional objects
in kind of this conceptual way.
So we need to be able to decompose them
because of the curse of dimensionality.
And we think models correspond to features
and the features correspond to directions.
And the hope in the early field
was that features would correspond to neurons.
And even if you believe features correspond
to orthogonal directions,
the same thing they correspond to neurons
is like a pretty strong one,
because a query there's no reason to align with the neuron basis.
The reason this isn't a crazy belief
is that models are incentivized to represent features
in ways that can vary independently from each other.
And because relus and jelus act element-wise,
if there's a feature per neuron,
they can vary independently.
Well, if there's multiple features in the same neuron,
I don't know.
If there's a relu,
the second feature could change
so the relu goes from on to off
in a way that changes how the other feature is expressed
in the dance room network.
And this is like a beautiful theoretical argument.
Sadly, it's bullshit
because of this phenomena of police mantisity.
Police mantisity is a behavioral observation of networks.
But when we look at neurons
and look at things that activate them,
they're often activated by seemingly unrelated things,
like the urs in the word strangers
and capital letters are proper nouns
and news articles about football.
It's a particularly fun neuron
I found one time in a language model.
And police mantisity is a purely behavioral thing.
We're just saying this neuron activates
for a bunch of seemingly unrelated stuff.
And it's possible
that actually we're missing some galaxy-brained abstraction
where all of this is related,
but my guess is that this is just...
the model is not aligning features with neurons.
And one explanation of this
is you've just got this thing called a distributed representation
where a feature is made of the linear combination of different neurons.
But it is kind of rotated from the neuron basis.
And this argument that neurons can vary independently
is a reason to think you wouldn't see this.
Where this hypothesis is just that
there's still n things when there's n neurons,
but they're rotated.
But then there's this stronger hypothesis
that tries to explain this
called the superposition hypothesis.
And here the idea is
so if a model wants to be able to recover a feature perfectly,
it must be orthogonal from all other features.
But if it wants to mostly recover it,
it suffices to have almost orthogonal vectors.
And you can fit in many, many more
almost orthogonal vectors into a space
than orthogonal vectors,
as theorem saying that there are exponentially many
in the number of dimensions.
If you have 100 dimensional vectors,
how many orthogonal directions are there?
100?
Yep, this is just the statement that
you pick a vector.
Sorry, there's 100 vectors
that are all orthogonal of each other.
Basic proof, you pick a vector,
everything's orthogonal to that, that's a 9 to 9 dimensional space.
You pick another vector, take everything orthogonal to that,
that's a 98 dimensional space,
and keep going until you get to nothing.
Like if you picture a 2D space,
you pick any direction, the only things
that are orthogonal to that are a line,
and so there's exactly two orthogonal things you can fit in.
And there's like, you can rotate this
and you can get many different sets of orthogonal things.
You could articulate why this doesn't make sense to me.
So maybe we should start with the cursive dimensionality,
which is that the volume of the space increases
exponentially with the number of dimensions.
So we'll start with that.
And the reason I'm thinking, maybe I'm wrong,
but if you've got 100 dimensional vector,
every combination of flipping one of the dimensions
would produce a vector which is all orthogonal
so let's imagine you've got a vector of all ones.
If you pick the first element and you negate it,
so it's like minus one, then 99 ones.
These are not orthogonal, the dot product is 98.
Okay, well that makes sense.
So there's a linear number of orthogonal directions,
and in which case we actually need to have
these approximately orthogonal directions
because that actually does bias an exponential number.
So the super positional hypothesis is that
the model represents more features than it has neurons
or that it has dimensions.
And it somehow compresses them in
as things that are almost orthogonal
when it reads them out with a projection
to get some interference, but it needs to balance
the value of representing more features
against the costs of interference.
Anthropic has this fantastic paper
called toy models of superposition,
which sadly was written off right left,
so I can't claim any credit.
And they basically build a toy model
that exhibits superposition.
The exact structure is you have
n independent features,
each of which is zero most of the time.
It's not very prevalent.
And there's a linear map from that
to a small dimensional space,
a linear map back up,
and a non-linearity on the output.
On the bottleneck in the middle.
And you train it to be an auto encoder.
Can it recover the features in the input?
And because there's many more features
in that are in the bottleneck,
this tests whether the model can actually do this.
And they find that it sometimes does,
sometimes doesn't, and then do a lot of
really in-depth investigation
of how this varies.
And yet returning to like,
is superposition the same thing as polyspanticity?
I would say no.
Polyspanticity is a behavioral thing.
Distributed representations are also a behavioral thing,
that it's like not aligned with the basis.
And superposition is a mechanistic hypothesis
for why both of these will happen.
Because if you have more features than neurons,
obviously, you're going to have
multiple features per neuron.
And probably you're going to have features
that are not aligned with neurons.
Okay, okay, very interesting.
So why do you think that superposition
is one of the biggest problems in mech and turp?
Yeah, so it's this fundamental thing
that we need to be able to decompose a model
into individual units.
And ideally, these would be neurons,
but they are not neurons.
So we need to figure out what we're doing.
And superposition,
so in a world where we just have
n meaningful directions,
but they weren't aligned with the standard basis,
that'd be kind of doable.
And indeed, models often have
linear bottlenecks,
like the residual stream or the keys,
queries, and values of an attention hint
that don't have element-wise linearities
and so have no intrinsically meaningful basis.
The jargon here is privileged basis.
But superposition means that
you can't even say this feature
should be a fork in all to everything else.
There's going to be a bunch of interference.
There's not even a kind of
mathematically,
there's not even like a unique set
of more-than-n directions
that describe some kind of vectors in n-dimensional space.
And I think that understanding
how to extract features from superposition,
given that superposition seems like
a core part of how models do things,
though we really do not have as much data here
as I would like us to.
Understanding how to extract
the right meaningful units seems really important.
Okay, and I think we should clarify
the difference between computational
and representational superposition.
Yeah, so there's kind of...
So transformers are interesting
because they often have
high-dimensional activations
that get linearly mapped to low-dimensional things.
So like in, say, GPT2, in, say, GPT2 small,
the residual stream has 768 dimensions,
while each MLP layer has 3,000 neurons.
And even if we think each neuron
just produces a single feature,
they need to get compressed down
to the 768-dimensional residual stream.
And we...
Or there's like 50,000 input tokens
that get compressed to 768 dimensions.
And this is called representational superposition.
The model is representing...
The model's already computed the features,
but it's compressing them to some bottleneck space.
And this is the main thing studied
in the toy models of superposition paper.
And what we found...
Sorry.
There's a separate thing of computational superposition,
which is when the model is doing...
It's computing new features.
This needs nonlinearities,
like attention-head softmaxes
or MLP jellies.
And the nonlinearities can compute new features
as directions from the old ones,
like if this, for example,
if the top of an image is a car window
and the bottom is a car wheel,
then it's a car.
Or if the current token is Johnson
and the previous token was Boris,
this is Boris Johnson.
And this is all...
I had a phrase this.
Yeah.
This is computational superposition
if the model wants to compute more features
than it has neurons.
And this is, like, much harder to reason about
because linear algebra is nice and fairly well understood.
Nonlinearities, spoilers in the name,
are not linear, and that's way more of a pin.
And I think that we generally
have a much less good handle on computational superposition,
but also that this is, like,
way more of where the interestingness lies by my lights.
And this is very briefly studied
in the toy models of superposition paper,
but I would love to see more work
looking at this in practice
and also looking at this in toy models.
So zooming out a tiny bit,
there's this paper from Anthropic.
And the overall question to me is,
does it actually exist?
Now, presumably, you're satisfied with the evidence
that it does exist.
And then there's the question of,
how do neural networks actually do it?
And then there's the question of,
how does the neural network think,
anthropomorphic language, I apologize,
about the trade-off of more superposition,
more features, but more interference,
versus less interference and more superposition?
Yeah, so diving into the final question about interference,
a useful conceptual distinction
is that there's two different kinds of interference.
So if you've got two features that share a dimension
or share a neuron,
oh yeah, final note on representational superposition,
because I don't think it should even be referred to
in terms of neurons,
because the individual-based elements
don't have intrinsic meaning.
Modular weird quirks like Adam.
And it annoys me when people refer to the residual stream
or key vectors as having neurons.
There's no element-wise linearity.
It's not privileged.
Anyway, yeah, two types of interference.
When A and B share a dimension,
you can...
Yeah, let's say this dimension has both dice and poetry.
You first off need to tell where,
if dice is there, but poetry is not,
you need to tell that dice is there
and that poetry is not there.
And if both, what I call alternating interference,
and then there's simultaneous interference,
where dice and poetry are both there,
and you need to tell that both are there,
but not that they're both there with double strength.
And as a general rule,
models are good at dealing with things of the form.
Notice when something is extreme along this dimension,
but notice when it is extreme along a dimension,
versus when it's not extreme.
And alternating interference looks like that.
Like if dice is straight up, poetry is at 45 degrees,
both have less interference
when the other one is active
than when the main one is active along their direction.
Okay, so you're saying interference from A and not B
is far easier than A and B.
Yes, exactly.
And a very rough heuristic is models will just
not do simultaneous interference,
but will do alternating interference.
And they observed this in the toy models paper,
because they varied how often a feature was non-zero,
what I think of as the prevalence of the feature,
though they called it sparsity,
and what they found is that when the feature was less prevalent,
it was much more likely to be in superposition.
And the way to think about this is if you have two independent
features that both exist with probability P,
the rate of simultaneous interference is P squared,
the rate of alternating is P.
And the worth of having the feature is also proportional to P,
because it occurs P of the time.
So the railroad is the less of a big deal
of simultaneous interferences,
and eventually the model uses superposition.
There's also an interesting bit looking at correlated features.
So correlated features,
even if they're not very prevalent,
they have pretty high simultaneous interference.
And models tend to put correlated features in to be orthogonal,
but anti-correlated features,
it's very happy for them to share a direction.
One way you could think about this is if you've got, say,
25 features about romance novels,
and 25 features about Python code,
you could have 25 directions that each contain a pair of features,
and then a single disambiguating neuron
that is onto Python code off of romance novels
that use to disambiguate the two.
And, yeah.
May this be a good time to talk about
the Finding Neurons in a Haystack paper,
or unless you've got more stuff on this?
We'll get to that in just two shakes of a lamb's tail.
But just before, when I was reading through the paper,
I had the mindset of sparsity,
and you told me,
Tim, don't say sparsity, it's prevalent.
It means so many things.
It's very overloaded.
It's such an overloaded what?
So just quickly touch on the relationship
between what is prevalence,
the relationship between prevalence and superposition.
And just before...
Well, actually, I've got a couple more questions,
but would you also just mind playing Devil's Advocate
and criticising the anthropic paper if you can?
Sure.
So I should be very clear.
This is one of my top three all-time favourite
interatability papers.
It's a fantastic paper.
That's it.
You won't do a bad word set about it.
Oh, I have so much.
I have bad words to say about every paper.
OK.
Especially the ones that I like
because I've engaged with them in the most detail.
So things which I think were misleading about this paper.
The first is I think the representational
versus computational superposition distinction
is very important.
I think computational is a fair bit more interesting.
And while I think the authors knew the difference,
I think a casual reader often came away
not realising the difference.
In particular, that most of their results were about
the residual stream, not about actual neurons and MLP layers.
The second is a question of activation range.
So they study features that vary uniformly
between zero and one.
And in practice, I think most features are binary.
This is a car wheel, or this is not a car wheel.
This is Boris Johnson, or this is not Boris Johnson.
And interference is much worse when they can vary continuously
because if A and B, if A is up, B is at 45 degrees,
you can't distinguish B at strength one
from A at strength 0.7-ish.
And this is just kind of messy.
But that binary is just much easier.
And I think this is a source of confusion.
Yeah, I also think the two kinds of interference point
was a bit understated.
And yeah, but more broadly, it's just a phenomenal paper.
Oh, my other biggest beaver, they just didn't look in real models.
And this wasn't the point of the paper.
But we're doing so much theory crafting
and filming conceptual frameworks,
and we haven't really checked very hard
whether this is why models actually have police mantisity.
Wes Gurney, he's working out of MIT,
and you've done a lot of work with him.
So you and Wes, but Wes was the first author,
wrote a paper called Finding Neurons in a Haystack,
case studies with sparse probing,
where you empirically studied superposition in language models
and actually found that you get lots of superpositions
in early layers for features like the security
and social security.
And fewer in middle layers for complex features
like this text is French.
And also you can bring in the importance
of range activation as well.
But can you frame up that paper?
Yeah.
So first off, this paper was led by Wes Gurney,
one of my mentees did a fantastic job.
He deserves like 9% of the credit.
Great job, Wes.
I believe he listens to this podcast, so hi.
And yeah, so the kind of high-level pitch
behind the paper was, well, we think superposition is happening.
But like, nobody's really checked very hard.
And there's like some results in the literature
I've since come across in non-transformer models
that demonstrate some amount of distributed representations.
But what would it look like to check?
And would it look like to do this
in like a reasonably scalable and quantitative way?
And the kind of sparse probing in the title
is this technique Wes introduces for...
If we think a feature is represented in MLP layer,
we can train a linear classifier to extract it,
a linear probe from that layer.
But if we constrain the probe to use at most K neurons,
very K and look at probe performance,
this lets us distinguish between features that are represented
with like a single neuron and features
that are densely spread across all neurons
with a lot of methodological nuances
about balanced data sets
and avoiding overfitting and fun stuff like that.
And most of the interesting bits of the paper,
in my opinion, are the various case studies we do where...
So probing fundamentally is like a kind of sketchy methodology
because probing is correlational.
Probing doesn't tell you whether a model uses something
and it's so easy to trick yourself
about whether you have the right representations.
So we use it as a starting point
and then dig more deeply into a few more interesting things.
One particularly QK study is we looked into factual knowledge neurons,
found something that seemed to represent this athlete plays hockey,
but then actually turned out to be a Canada neuron,
which continues to bring me joy.
That activates with things like maple syrup and Canada.
Got a lot of models learning national stereotypes, right?
Oh, yes.
Anyway, so there were two particularly exciting case studies.
The first was looking in early layers at compound word detectors.
So if you look at, say, the brain and its visual field,
we have all these sensory neurons.
We get raw input of light from the environment
and it gets converted into stuff our brain can actually manipulate.
Image models have Gabor filters that convert the pixels
into something a bit more useful.
What's the equivalent of language models?
And it seems to be these things that we call detokenization neurons
and circuitry, where often words are split into multiple tokens
or you get compound word phrases like social security
or Theresa May or Barack Obama and whatever.
And it's often useful for a model to realize
this is the second thing in a multi-token phrase,
especially if it's like you need both things to know what's going on,
like Michael Jordan.
Michael has lots of Jordans.
It's really important to tell that both of them are there.
And this is a clearly nonlinear thing, because it's like a Boolean and.
And so we did a lot of probing for different compound words.
And we found that they were definitely not represented well by single neurons.
We could find some neurons that were okay at detecting them,
but there was a lot of interference and a lot of false positives from other stuff.
And when we dug into a bunch of these neurons,
we found that they were incredibly polysemantic.
They activated for many different compound words.
And we showed that it was using superposition
by observing that if you took, say, five social security detecting neurons
and add together their activations,
they go from okay detectors to a really good detector together.
Because even though each is representing like hundreds of compound words,
they're representing different compound words, which lets you encode these.
And this, what we've shown here is that it's like distributed,
that it's a linear combination of neurons.
We still haven't shown it perfectly to my dissatisfaction.
I think you really need to do things like ablate these linear combinations
and see if this systematically damages the model's ability to think about social security, etc.
But I'm pretty convinced at this point.
And there's like a few properties of compound words
that both make it easy to represent in superposition,
make me pretty okay making the jump that there's actual superposition.
The first is just that there's tons of compound words.
Each one is pretty rare, but each one is like non-trivial or useful.
And clearly there are more compound words
and there are the like thousands of neurons in the MLP layer of this model.
The model cares about representing and can represent.
But we did not actually check,
because I could not convince Wes to accumulate a list of 2,000 compound words
and pray for all of them.
But I believe in my heart this is true.
Could I have a point of order though?
Go for it.
Because I've been reading quite a lot of stuff from linguists like Stephen Piantadosi.
And a lot of linguists are, some of them hate language models
and some of them are well on board with it.
And you know, like Raphael Millier, for example, is a great example.
I hate language models too, don't worry.
Well, but the question is, because you're talking about compound words and stuff like that
and you're still using the language of syntax
and these language models, there's this distributional hypothesis.
You know the meaning of a word by the company it keeps.
But linguists and cognitive scientists kind of ditch that.
I don't think they ever believed in the distributional hypothesis.
They think about grounding.
They think about grounding two things in the world
and also inferential references as well
which is, you can think of that as grounding to a model of the mind.
And this brings us back to the Othello paper
which is that they're not just learning simple kind of compound relationships
between the world, between the words.
They're learning a world model
and they're doing something much more potentially than just predicting the next word.
And Piantadosi argued that most of the representational capacity in language models
are learning these semantics.
They're learning relationships between things in the world model
and the particular occurrence of the token.
And this superposition idea is very interesting
because it actually imbues the representational capacity in a language model
to learn those mappings.
Okay, so a couple of comments on that.
The first is a generally useful way of thinking about models to me
is as a the early layers devoted to sensory neurons
converting the raw input into more useful concepts and representations
the actual processing throughout like all of the middle layers
that actually does all the reasoning.
And then motor neurons at the end that convert the reasoning to actual output tokens
for like the format that the optimizer wants.
And it feels like you're mostly talking about the like reasoning internally
and the specific case study I'm referring to is on the sensory neurons.
Well, like I'm not saying it just detects compound words
but obviously that's the first thing it does.
Absolutely, it's so interesting.
I don't mean to push back but in neuroscience
the field was held back for decades
by this idea of this kind of left to right to processing
this hierarchical processing where you have these very, very simple concepts
that become increasingly abstract with more processing.
And then I think the field has moved away from that.
It's far more messy and chaotic than that.
Now with a neural network, it actually is hierarchical
because the network is basically a DAG.
So I suppose it is safe to make this assumption
but could I just kind of question you on that?
Is it safe to make that assumption?
Is there increasing complexity in representation as you go from left to right?
Oh, let's see.
So yeah, I definitely, yeah.
So clarification one, the network has this input sequence
which I think was going from left to right
and then there's a bunch of layers which I think it was going from like the bottom to the top.
Yes.
And you're referring to the bottom to top axis, right?
Yeah, I'm sorry, I was using an MLP mindset when I asked that question.
So as you say, in a transformer, it's an autoregressive model
and you have stacked attention layers with little MLPs on the end.
So I guess the way I was actually meaning the question is,
so complexity increases monotonically as you go up the stack of attention layers.
Is that a fair assumption?
Yep.
Again, no one's really shown this properly.
But I'm like, surely this is true.
And there's been some work doing things like looking at neurons,
looking at the text that activates them, looking for patterns
and trying to understand what these represent.
And it's generally looked like early ones are more about detokenization and syntax.
Later ones are doing stuff that's interesting.
Final ones are doing this like motor neuron behavior.
But I also want to be very clear that networks are cursed.
Networks do not fit into nice abstraction.
I'm not saying the early layers are literally only doing detokenization.
But I believe we have shown it's part of what they're doing
and I speculate it is a large part of what they're doing.
I'd be very surprised if it's all of what they're doing.
Because I heard you on another podcast and you were just talking about the,
I think the curse is the right way to describe it,
which is that even when you make modifications,
when you manipulate what's happening,
the behavior will change in a very reflexive way.
So you kind of, you delete one thing
and then another neuron will take on the responsibility of the thing you just deleted.
And so it's a little bit like manipulating financial markets.
You've got almost like this weird collective diffuse intelligence
where you make one modification and the whole thing changes in a very complex way.
And similarly, I guess that's why I was intuitively questioning the assumption
that you have a residual stream.
So surely even at the very top of that attention stack,
there must be primitive and complex operations going on in some weird mix.
It's probably true.
Generally, yeah, there's going to be some stuff you can just do with literally the embeddings.
Some stuff that you need to wait a bit more before you can do anything useful with.
Just like, no, if you got a sentence about Michael Jordan,
I don't think you can use Michael Jordan in isolation.
So you need to detokenize to Michael Jordan.
But also, I don't know, if you've got Barrick Obama,
Obama and Barrick both on their own pretty clearly imply it's going to be about Obama.
And probably the model can start doing some processing in the earlier like layer zero.
Does it want to?
Somewhat unclear.
It's going to depend a lot on the model's constraints and other circuitry
and how much it's worth spending the parameters then versus later.
There's also some various things where, I don't know,
model memory kind of decays over time because the residual stream's norm gets bigger.
So early layer outputs become a smaller fraction of the overall thing.
And layer norm sets the norm to be unit.
So things kind of decay.
And so if you compute a feature in the early in like layer zero,
it can be harder to notice by like layer three than if it was computed in layer two.
But these are all just kind of like mild nudges.
And ultimately neural networks do what neural networks want, man.
I know, I know.
I just want to close the loop on something I said a little while ago about, you know,
potentially large models use most of their representational capacity for, you know,
learning these semantic relationships.
And empirically, we found that, you know, there's some question recently actually about,
do we actually need to have really, really large models?
And for pure knowledge representation, the argument seems to be yes,
but we can disentangle knowing from reasoning.
And there's also this mimicry thing.
So it's quite interesting that all of the, you know, like Facebook released their model
and very, very quickly people fine-tuned it using the law, you know,
the low-rank approximation fine-tuning method.
And on all of the benchmarks, the model, I mean, even open assistant,
there's another great example, Yannick was sitting in your seat just a few weeks ago
and on many of the benchmarks, the model's working really well.
But it's kind of not, it's kind of mimicry.
Like the big, large models that, you know, Meta and Google and DeepMind and all these people,
they spend millions training these models and they have base knowledge about the world,
which is not going to be, you know, replicated by fine-tuning, you know,
like an open source model any time soon.
The knowledge is based.
The knowledge is based.
Yes, yes, yes, exactly.
Well, okay, so that's very interesting.
Let's just quickly talk about the OpenAI microscope because this is,
the OpenAI microscope is this beautiful app that OpenAI released in 2020.
And you can go on there and you can click on any of the neurons in popular vision architectures at the time.
So, you know, I think most of them are sort of like ImageNet, you know, things like AlexNet and God knows what else.
And they solve this optimization problem where they generate an image using stochastic gradient descent
that maximally activates a particular neuron or I think even a layer using something similar to DeepDream.
And you can click on these neurons and sometimes they are what we will call polysyn...
sort of monosemantic, which means it's just Canada.
A lot of the time there's a couple of concepts in there that it's weirdly intelligible.
You know, you might see, you know, like a playing card or an ace and a couple of like tangentially related concepts.
And it always struck me as strange because I imagine there's a long tail of semantic relationships.
And I found it bizarre that there'd only be one or two in this visualization.
And I had this intuition that the optimization algorithm is in some sense mode seeking rather than distribution matching,
which is to say that it finds the two most or two or three or four most kind of salient semantic mappings
and they dominate what is visualized and you're almost snipping off the long tail of the other semantic mappings.
Yeah, so I think there's two things to disentangle here.
The first is what is actually represented by the neuron in terms of ground truth.
And the second is what our techniques show us.
So the two techniques used in the open air microscope are looking at the images the most activated neuron
and then this feature visualization technique where they produce a synthetic image that maximally activates it.
And to me, this is these are like both of these can be misleading because if the model activates the dice and poetry,
but activates dice with strength five and poetry with strength four, then the optimal image activated will be dice and the optimal the data set examples will also be dice.
But really it'll be about poetry and you want to get a lot more rigorous.
You want to show true monosimanticity.
One cute thing is spectrum plots.
You take lots of example data set examples across the full distribution.
You have a histogram with like the different groups for the different meanings and then neural activation on the x-axis.
We have this really cute plot in Wes' paper called the French Neuron where all of the French text is on the right.
All the non-French text is on the left.
And the neuron is just very clearly distinguishing the two in a way that's much more convincing to me than things like max act examples.
And I actually have a hobby project called Neuroscope at Neuroscopes.io where you can see the max activating text examples for every neuron and a bunch of language models.
The opening I recently output this paper with one that is better, but only for GPT2 XL.
Anyway, not that I'm bitter or anything.
And yeah, so yeah, there's the things can lie to and be illusory.
There's this interesting paper called the Interruptibility Illusion for Butts, which investigated this specific phenomena.
And in particular that if you take the data set examples over some narrow distribution like Wikipedia or books, you can get pretty misleading things.
Though they only looked at residual stream basis elements rather than actual MLP neurons, I believe, which makes it a bit less compelling.
Point of order as well.
We've been saying residual stream quite a lot and Microsoft introduced Resonate in 2015, which basically means that between all of the layers, the information is being passed up unadulterated.
So the subsequent layer can choose to either essentially shortcut or ignore the previous layer or use some combination.
And at the time they kind of said it was about the neural network being able to learn its own capacity in some sense.
But could you just give us like the way you think about these residual streams?
Yeah, so I think the standard view of neural networks, they're just layers and layer 5's output is layer 6's input, etc.
Then people added Resonance, where layer 6's input is layer 5's output, plus layer 5's input with the skip connection.
But I think people normally thought of them as like, yeah, it's like a cute trick that makes the model better, but doesn't massively change my conceptual picture.
And the framing that I believe was introduced in the mathematical framework, this anthropic paper led by Chris Ohler, Nelson L. Harsh, and Catherine Olsen that I was involved with,
is actually, let's call the thing in the skip connection the residual stream and think of it as the central object.
And draw our model so the residual stream is this big vertical thing and each layer is like a small diversion to the side rather than the other way around.
And in practice, most circuits involve things skipping many layers.
And each layer is better thought of as like an incremental update.
And there's a bunch of earlier transformer interpretability papers that I think miss this conceptual point, like the interpretability illusion for BERT when I mentioned earlier,
and study residual stream basis elements as like layer outputs or something.
Yeah, I mean, in a sense, you know, we were talking about being able to reuse things that you've learned before and not having to learn them again.
And I guess I think of it as a kind of translational equivalence in the in the layered regime, which is that you have a computation which is learned early on.
And now it can just be composed into subsequent layers.
It's just it's like you've got a menu of computational functions that you can call on at any layer.
Yeah, pretty much. I think of it as like the shared memory and shared bandwidth of the model.
Yeah, yeah, almost like a memory bus.
Yeah. And sometimes models will dedicate neurons like cleaning up the memory and deleting things that are no longer needed.
Yeah. Yeah. And is there any interference in that memory bus?
So much.
Go out. This is the thing of superposition, right?
Yeah.
It's just doing everything like this 50,000 input tokens starts and then four X as many neurons as residual stream dimensions in every MLP layer and attention heads moving everything around.
And it's just a cluster.
What if you scale up the bandwidth of the bus?
Mm hmm.
That is basically making the model bigger, right?
Which we know makes models better.
But I don't know just thinking out loud, but what if you maintained the original dimensionality of the model, but you deliberately upscaled the bus?
So like you make the thing inside each layer smaller, but make the residual stream bigger?
Or just make everything the same as it is, but you just kind of like have a linear transformation on the bus and double the size of the bus?
So I don't think that would work without increasing the number of parameters because like if you, because like the thing that matters is the smallest bottleneck.
The output width of an MLP layer are like 4,000 by 1,000.
And in order to make the 1,000 bigger, you need more parameters.
Interesting.
And there's like all kinds of studies about the optimal hyperparameters and the optimal ratios.
My general heuristic is number of parameters are the main thing that matters.
I don't know.
I don't spend that much time thinking about how to make models better, to be honest.
I just want to understand them. God damn it.
Yeah, because it's one of those things that it might remove bottlenecks because, because essentially you're allowing the model to reuse things that it's learned previously.
So now every single layer can specialize more than it did before.
And that might kind of like weirdly remove bottlenecks.
Yeah.
Yeah.
The way I generally think about it is models are ensembles of shallow pods, which is this paper from like five years ago about Resnets.
Like DPD too small is 12 layers.
Each layer includes an attention block and an attention bit and MLP.
But it is not the case that most computation is 24 levels of composition deep.
It is the case that most of them involved like, I don't know, four.
And they're just intelligently choosing which four and remaking them in interesting ways.
And sometimes different things will want to like get to different points.
And so it's useful to have many layers rather than a few.
But also, I don't know, if you have, if you have the residual stream width and give the model four X as many layers, often performance is like about the same.
Or like not that different because the number of parameters is unchanged.
And this is just kind of a wild result about models that I think only really makes sense within this framework of it's like an ensemble of shadow pods.
And it's a trade off between having more computation and having better memory bandwidth.
Yeah.
Yeah.
Very interesting.
Okay.
I mean, just to close, superposition, it might not be a new idea.
So Yannick did a paper video about this paper called Supermasks in Superposition by Mitchell Wartsman back in 2020.
And he was talking about supermass representing sparse subnetworks in respect of catastrophic forgetting and continual learning.
But that was slightly different because that was an explicit model to perform masking, create subnetworks and to model, you know, like basically a sparsity aware algorithm.
But he was still using a lot of the same language like interference and so on and thinking about superpositions of subnetworks.
And I guess the difference is like just as we were talking about with these inductive priors like transformers and CNNs, the models already do this stuff without us having to explicitly code it, which I think is the interesting discovery.
Yeah.
Yeah.
One update I've made from Wes' work is that de-tokenization is probably like a pretty big fraction of what the early layers do.
And it's just really easy to represent compound words in superposition because it's very binary.
It's either there or not there.
So alternating differences is easy to deal with.
They're mutually exclusive.
So there's no simultaneous interference.
Like you cannot have Boris Johnson and Theresa May co-occur.
And there's just like so many of them.
One fact about language models that people who haven't played around them may not appreciate is their inputs are these things called tokens.
And tokenizers are fucked because they're trained in this bizarre Byzantine way.
That means that often words, the rarer words will get broken up into many tokens.
Yes.
Multi-word phrases are always different tokens.
Anything that's weird like a URL gets completely cursed.
And models don't want to have this happen.
So they devote a bunch of parameters to build a like pseudo vocabulary.
Of what's going on.
And just returning to your point earlier about like, is it just these syntax level things?
Is there some like actual more semantic stuff going on?
We did also have case studies looking at contextual neurons.
Things like this code is in Python.
This language is in French.
And these were seemingly monosematic.
Like it seemed like there were specific neurons here.
And we found things like if you ablate the French neuron, loss on French text gets much worse.
What other ones are fine?
And also some interesting results that the model was say using this disambiguate things like tokens like D are common in German and also common in Dutch.
And the neurons for those languages were being used to disambiguate for that token.
Whether it was like a German D or a Dutch D.
Because they've got very different meaning in the two languages.
Yeah.
I wonder if you give me some interesting like, as you say in Wes's paper, you know, he did actually find that, you know, there are some monosematic neurons like French, as you just said.
And in this case, the model decided that interference in some sense wasn't worth the burden.
But what does burden mean here?
And French is a very vague concept as well.
Yes.
So a couple of observations.
First is I do not think we have properly shown they are monosematic neurons.
We were looking, these models are trained on the pile.
And we were specifically looking at them on Europal, which is like a data set of European Parliament transcripts, which are labeled by language.
And we found a neuron that seemed to strongly disambiguate French from non French.
But it was on this domain of parliamentary stuff.
And because models really want to avoid simultaneous interference, if they did have superposition, they probably want to do it with something that isn't likely to occur in this context.
I know this is a list variable in Python, which we didn't check very hard for.
And in particular, this is messy to check for.
Because in order to do that, you need to answer these questions like, what is French?
Like, there's a bunch of English checks that will activate for, but it will activate on words like sacre bleu and trabia.
And I think I count this as French, but like, I don't have a rigorous definition of French.
And I think an open problem I'd love to see someone pursue is just, can you prove one of these neurons is actually a French detecting neuron or not?
And what would it even look like to do that?
And yeah, regarding interference in the button.
So the way I think about it, if two features are not orthogonal, then, um, no, sorry, this is more interesting in the case of neurons.
If there's multiple things that could all activate a neuron, then it's harder for the downstream bit of the model to know how to use the fact that that neuron activated.
Because there are multiple things, even if they don't co-occur, because they're mutually exclusive.
And this is just a cost.
And there's a trade-off between having more features and not having this cost.
And features like this is in French are really load-bearing.
They're just really important for a lot of circuitry here.
And so theoretically, the model might want to dedicate an entire neuron to this.
But if you dedicate an entire neuron, you lose the ability to do as much superposition.
My intuition is the number of features that can be represented in superposition is actually, like, grows more than linearly with the number of dimensions.
So this might be, like, significantly worse than just having one fewer feature.
So we are now in the next chapter of this beautiful podcast, and we're going to talk about transformers.
So how exactly do transformers represent algorithms and circuits?
And also, you've written this beautiful mathematical framework about transformers,
which, of course, is working very closely with Catherine Olsen and Chris Ola.
And Nelson Elhoch.
And Nelson, my apologies.
Yeah, so in terms of understanding, yeah.
So if you wanted to do a mechanism to interpretability on a model,
you need to really deeply understand the structure of the model.
What are the layers? What are the parameters? How do they fit together?
What are the kinds of things that make sense there?
And let's see.
So, yeah, there's, like, a couple of key things I'd want to emphasize from that paper.
Though, I don't know, it's also one of my, like, all-time top three interpretability papers.
We will just go read it.
And after reading it, check out my three-hour video walkthrough about it,
which apparently is most useful if you've already read the paper.
Because it's that deep anyway.
Yeah, so a couple of things I'd want to call out from that,
especially for people who are kind of familiar with other network but not transformers.
The first, we've already discussed the residual stream as the central object.
And the second is how to think about attention,
because attention is the main thing which is weird about models.
They have these MLP layers, which actually represent, like,
two-thirds of the parameters in a transformer, which is often an underrated fact,
but attention is the interesting stuff.
So, transformers have a separate residual stream for each input token,
and this contains, like, all memory the model would store at that position.
But MLP layers can only process information in place.
You need attention to move things between positions.
And classically, people might have used stuff like a 1D convolution.
You average over 10 things in a sliding window.
This is baking in the inductive bias that nearby information is more likely to be useful.
But this is kind of a pretty limited bias to bake in,
and the story of deep learning is that over time, people have realized,
wait, we should not be trying to force the model to do specific things.
We understand, we should not be telling the model how to do its job.
If it has enough parameters and is competent enough, it can figure it out on its own.
And so the idea here is rather than giving it a convolution,
you give it this attention mechanism where each token gets a query saying what it's looking for,
each previous token gets a key saying what it has to offer,
and the model looks from each destination token to the source tokens earlier on
with the keys that are most relevant to the current query.
And models, and the way to think about an attention head.
So attention layers break up into these distinct bits called heads,
which act independently of the others and add to their outputs together
and just directly add to the residual string.
This is sometimes phrases concatenate their outputs and then multiply by a map,
but this is mathematically equivalent.
Each head acts independently and in parallel,
and further, you can think of each head as separately breaking down into a
which information to move a bit determined by the attention,
which are determined by the query and key calculating matrices,
and the what information to move once I know where I'm looking,
which are determined by the value and output matrices.
We often think about these in terms of the QK matrix,
WQ times WK transpose and the OV matrix.
WO times WV, because there's no long linearity in between.
And these two matrices determine what the head does.
And the reason I say these are kind of independent is that
once the model has decided which source tokens to look at,
the information that gets output by the head is independent of the destination token.
And the query only matters for choosing where to move information from.
And this can result in interesting bugs,
like there's this motif of a skip trigram.
The model realizes that if the current thing is three
and two has appeared in the past, then four is more likely to come next.
If the current thing is three and four has appeared in the past,
two is more likely to come next.
But if you have multiple destination tokens,
they'll all want the same source token.
For example, the phrase keep in mind can be a skip trigram.
Really, it should be a trigram,
but tiny models aren't very good at figuring out what's exactly the previous position.
Keep at bay is another trigram,
but in an at, we'll both look at the same keep token.
And so they must boost both at and mind for both of them.
So also predict keep in bay and keep at mind.
And possibly we should move on to induction heads,
which are a good illustrative example.
Yeah, I was going to come on to that.
So on these induction heads,
you've said that they seem universal across all models.
They underlie more complex behavior, like few-shot learning.
They emerge in a phase transition,
and they're crucial for this in-context learning.
And you said that sometimes specific circuits underlie emergent phenomena,
and we may want to predict or understand emergence
by studying these circuits.
So what do we know so far?
A lot of questions in there.
All right, taking this in order.
So what is an induction head?
I've already mentioned this briefly.
Text often contains repeated subsequences,
like after Tim, Scarf may come next,
but if Tim Scarf has appeared like five times,
then it's much more likely to come next.
In toy two-layer attention-only language models,
we found this circuit called an induction head, which does this.
It's a real algorithm that works on, say, repeated random tokens.
And we have some mechanistic understanding of the basic form of it,
where there's two attention heads and two different layers working together.
The later one called an induction head looks from Tim
to previous occurrences of Scarf.
The first one is a previous token head,
which on each Scarf looks at what came before,
and is like, ah, this is a Scarf token which has Tim before.
And then the induction head looks at tokens where the token before them was Tim,
or where the token before them was equal to the current token.
And when the induction head decided to look at Scarf,
which is determined purely by the QK matrix,
it then just copies that to the app, which is purely done by the OV matrix.
And I think induction heads are a really interesting circuit case study,
because induction heads are all of the interesting computation
is being done by their attention pattern.
Like, Tim Scarf could be anywhere in the previous context,
and this algorithm will still work.
And this is important, because this is what lets the model do
tracking of long-range dependencies in the text, where it looks far back.
And you can't bake this in with a simple thing like convolutional layer.
In fact, transformers seem notably better than old architectures
like LSTMs and RNNs, in part because they have induction heads
that let them track long-range dependencies.
And, yeah.
And more generally, it often is the case that especially
later-layer attention heads, the OV bit is kind of boring,
it's just copying, but figuring out where to look
is where all of the interesting computation lies.
So, first of all, just to clarify, because people will know
what an attention head is, but an induction head is one of these circuits
that you're talking about, just so people understand.
And we should get onto this relationship between induction heads
and the emergence of in-context learning.
And also, you said it's very important that we have this scientific understanding
with respect to studying emergence, but rather that than just framing
with interpretability kind of makes better models.
Yeah.
So, okay.
So, maybe I should first explain what emergence is.
Let's do that.
I'd be really, really interested if you could just give me
the simplest possible explanation of what you think emergence is.
Sure.
Emergence is when things happen suddenly during training.
From not being there to being there fairly rapidly in a non-convex way
rather than gradually developing.
It's interesting you said that, because I think of emergence
as a surprising change in macroscopic phenomena,
and it's an observer relative term, which means it's always
from the perspective of another scale.
Hmm.
So, just a transient change in perplexity or capability or something
in my mind wouldn't entail emergence.
Like it would need to be some qualitative meaningful thing
rather than just, oh, the loss curve got notably better in this bit.
I think so.
It's definitely related to some notion of surprise,
which is inherently relative.
Yeah, let's not get hung up on that.
So, okay.
Let's say it's a transient change in something.
Yeah.
I mean, I wouldn't call it transient.
It's like an unexpected sudden change.
Though unexpected has so much semantic meaning on it that I don't want to use.
But yeah, this is an infinite rabbit hole.
Yes, but I think the scale thing is relevant as well.
So, we are programming neural networks at the microscopic scale
and there's some macroscopic change in capability.
So, it's some...
Yes.
Yeah.
And there's like lots of different dimensions.
You can have emergence on.
You can have it as you train a model on more data.
You can have it as you make the models bigger.
And these are both interestingly different kinds.
One of the more famous examples is Chain of Thought and Few Shot Prompting,
where DP3 is pretty good at this.
Earlier models were not good at this.
This was kind of surprising.
Chain of Thought is particularly striking because you...
People just noticed a while after DP3 was public that if you tell it to think step by step,
it becomes much better.
There's this recent innovation of Tree of Thought that I'm not particularly familiar with,
but I understand as kind of like applying Monte Carlo Tree Search on top of Chain of Thought.
Yes.
Yes.
Where you're like, well, there's many ways we could branch at each point.
Let's use Tree Search algorithms to find the ultimate way of doing this.
Yes.
But with, let's say, Scratchpad and Chain of Thought,
I don't necessarily see that as an emergent...
Well, maybe there's an emergent reasoning capability that comes into play
when you have a certain threshold size model.
But I think of it more as kind of having an intermediate,
augmented memory in the context.
So you're kind of filling in a gap in cognition by saying you're allowed to...
It's not just remembering things.
It's also reflecting on things that didn't work.
Yes.
So clarifying, when I say emergent, when I say Chain of Thought is an emergent property,
I mean the capacity to productively do Chain of Thought is the emergent thing
and telling the model to think step by step is a user-driven thing.
But I don't know.
I kind of...
As a point of order, though, was it just that it was discovered after GPT-3
or would it work on GPT-2?
Ah, I would have guessed it doesn't work very well on GPT-2.
But I've not checked.
I'd be pretty interested.
I'm sure someone has looked into this.
I haven't looked very hard.
I guess, like, so a lot of my motivation for this work comes from...
I care a lot about AIX risk and AI alignment and how to make these systems good for the world.
And when I see things like, oh, we realize that you can make GPT-3 much better
by asking it to think step by step.
I'm like, oh, no.
What kinds of things could the systems you make be capable of that we just haven't noticed yet?
That's the concern that the genie is already at the bottle.
DeepMind just published this Tree of Thought paper.
It's a really simple idea.
It's basically a star search over trajectories of prompts
and you use the model itself to evaluate the value of a trajectory.
And I could have done that.
Anyone could have...
Similar thing with auto-GPT and all this stuff.
I'm more skeptical than you are.
I think, in the case of Tree of Thought, it closes a capability gap
in respect of certain tasks which were not working very well
because they don't have that kind of system to...
Models don't seem to plan ahead very well.
But I still think that it's not just going to magically turn into super intelligent.
I mean, we can talk about this a little bit later, but...
Yeah, okay.
Yeah, so...
Yeah, I think this is also pretty relevant to much more near-term risks.
Yeah.
I know there's lots of things that a sufficiently capable model could do
that might be pretty destabilizing to society,
like write actually much better propaganda than human writers can or something.
And if Tree of Thought makes it possible to do that
in a way that we did not think was possible when GPT-4 was deployed,
that's like an interesting thing that I care about noticing.
It's not a very good example, but...
Yeah, it is.
But being able to... I mean, first of all,
it's been possible to create misinformation for a long time.
This is why I specified be able to do it notably better than humans can.
I totally agree.
The longer doing it a bit more cheaply and a bit more scale doesn't seem obviously that important.
You could argue that, like, I don't know,
being a spam bot that feels indistinguishable from a human
is like a more novel thing that's actually different.
Yeah.
But, I don't know, this was like an off-the-cuff example.
I don't want to get too deep into this,
because it's not a point I care that deeply about.
Yeah, I mean, we can come back to it a bit,
but I think we are nearly already there.
Yeah.
You know, this irreversibility thing.
We don't know.
Computer games are photorealistic.
Chatbots are indistinguishable,
and AI art is pretty much indistinguishable.
And that could work.
I mean, I spoke to Daniel Dennett about it last week,
and he said he's really worried about the epistemic erosion of our society,
more so interestingly than the ontological erosion.
And I discovered later that's because he's not a big fan of anything ontological.
But, yeah, it is potentially a problem,
but I guess to me, people might overestimate the scale
and magnitude of change of this.
I feel that...
I know I don't want to echo Sam Altman here,
but he said that we are reasonably smart people,
and, you know, we can adapt and recognize, you know, deep fakes and so on.
But, yeah.
Yeah, these are complicated societal questions.
I guess I mostly just have the position of,
man, it sure is kind of concerning
that we have these systems that could potentially pose risks,
but you don't know what they do and decide to deploy them,
and then we discover things they can do.
And I think that the research direction I'm trying to advocate for here
is just better learn how to predict this stuff more than anything,
which hopefully we can all agree is like an interesting direction.
And there's all kind of debates about,
is emergent phenomena like actually a real thing?
Like this recent, is this a mirage paper,
which I think was a bit over-claiming but does make a good point
that if you choose your metric to be sufficiently sharp,
everything looks dramatic.
One thing I've definitely observed is if you have an accuracy graph
with a log scale x-axis for grokking,
it looks fantastically dramatic.
And I was very careful to not do this in my paper
because it is cheating.
But, yeah.
So my particular hot take is that I believe emergence is often underlain
by the model learning some specific circuit
or some small family of circuits in a fairly sudden phase transition
that enables this overall emergent thing.
And this sequel paper led by Catherine Olson
in Contest Learning and Induction Heads
is a big motivator of my belief for this.
So the idea of the paper is we have this,
we found induction heads in these toy-till-artentionally models.
We somewhat mechanistically understood them,
at least in the simplest case of induction.
We used this to come up with more of a behavioral test
for whether it's induction heads.
You just give them all repeated random tokens
and you look at whether it looks induction-y.
And we found that these occurred in basically all models we looked at,
up to 13B, even though we didn't fully reverse engineer them there.
And we then found that this was really deeply linked
to the emergence of in-context learning.
There's a lot of jargon in there, so let's unpack that.
In-context learning, already briefly mentioned,
it's like tracking long-range dependencies in text,
like you can use what was on, which was three pages ago,
to predict what comes next in the current book,
which is a non-trovial thing.
It's not obvious to me how I had a programmer model to do.
In-context learning is emergent.
If you operationalize it as average loss on the 500th token
versus average loss on the 50th token,
there's a fairly sudden period in training
where it goes from not very good at it to very good at it.
Just a tiny point forward of that.
One interesting thing about in-context learning
is you're learning at inference time, not training time.
But you're not changing anything in the underlying model,
which means anything it can do,
presumably, must be materializing a competence
which was acquired during training.
So it's coming back to this periodic table thing, right?
So it's learned all these platonic primitives.
You do this in-context learning.
You say, I want you to do this. Here's an example.
And you've got all of these freeze-dried periodic
computational circuits, and they spring into life
and they compose together and they do the thing.
Yes.
I think induction heads are, to my eyes,
the canonical example of an inference time algorithm
stored in the model's weights that get supplied.
And I'm sure there's a bunch more that no one has yet found.
And a lot of my modelers, the prompt engineering,
is just telling the model which of its circuits to activate
and just engaging with various quirks of training
that have made it more or less steerable in different ways.
And yeah, so induction heads also emerge
in a fairly sudden phase transition.
And we, and exactly at the same time,
and we present a bunch more evidence in the paper
that there's actually a causal link here.
Like, one of their models have neither the in-context learning
or the induction heads phase chain
because they can't do induction heads
because they're only one layer.
And why, but if you adapt the architectures,
they can form induction heads with only one layer.
Now they have both of these phenomena.
If you obliterate induction heads,
in-context learning gets systematically worse.
And a particularly fun qualitative study
was looking at soft induction heads,
heads that seem to be doing something induction-y
in other domains,
like a head which attends from the current word in English
to the thing after the current word in French.
Or, more excitingly, a few-shot learning head
on this random synthetic pattern recognition task we made
where it attended back to the most relevant examples
to the current one.
And my interpretation of all this
is that there's something fairly fundamental
about the induction-y algorithm for in-context learning.
So the way I think about it,
let's say you've got two...
You want to learn some relation.
You've got some local context A
and some past context B.
And if you observe A and you observe B in the past,
this gives you some information about what comes next.
There's two ways this could work out.
It could be symmetric.
B helps A and A helps B.
Or asymmetric.
B helps A, but A does not help B
if they're the other way around.
Asymmetric might be like,
knowing the title of a book tells you what comes next,
but knowing what's in a random paragraph
in the previous bit doesn't tell you the title.
While symmetric is like, I know,
English sentence helps French sentence,
French sentence helps English sentence.
And if you have, like, N symmetric relations,
like English, French, German, Dutch, Latin, whatever,
where each of them helps each other,
this is really efficient to represent.
Because rather than needing to represent
N squared different relations separately,
like you would in the asymmetric case,
you can just map everything to the same latent space
and look for matches.
And fundamentally, this is what induction heads are doing.
They're mapping current token
and previous token of thing in the past
to the same latent space and looking for matches.
And to me, this is just like
a fairly natural primitive of attention.
And this is exciting because, A,
we found this deep primitive
by looking at toy two-layer attentionally models.
B, it was important for understanding
and ideally for predicting
the emergent phenomena of in-context learning.
And two takeaways I have from this
about work we should be doing.
The first is we should be going harder
at looking at toy language models,
like open source to scan of 12 of them.
And I'd love to see what people can find
in one-layer models with MLPs
because we really suck at transformer MLP layers.
And one layer should just be easier than other ones.
And the second thing is
I really want a better
and more scientific understanding of emergence.
Why does that happen?
Really understanding particularly notable case studies of it,
testing the hypothesis that it is driven
by specific kinds of circuits like induction heads
or at least specific families of circuits,
even though, I don't know,
you could argue that because we haven't fully
engineered the things in the larger models,
we really know it's actually an induction head.
And yeah.
More generally, a lot of my vision
for why mechantup matters
is this kind of scientific understanding of models.
I don't care about making models better,
but I care about knowing what's going to happen,
knowing why stuff happens,
achieving real understanding
and getting a scientific understanding
of things like emergence
seems like one of the things mechantup
might be uniquely suited to do,
but also not checked very hard.
And you, dear listener,
could be the person who checks.
So there was a paper by Kevin Wang et al.
called Interpretability in the Wild,
a circuit for indirect object identification
in GPT2 Small,
which found a circuit for indirect object identification.
So they discovered backup name and move aheads,
which normally don't do much.
They take over when the main name,
move ahead, or ablated.
And they said,
mechanistic interpretability
has a validation set for more scalability techniques.
They've understood a clear place
that these ablations can be misleading.
So...
Yeah.
So yeah, well, there's one pack in there.
So...
I really like the interpretability in the wild paper.
Also, Kevin was only 17 when he wrote it.
Like, man, I was doing nothing
remotely as interesting when I was in high school.
So props to him.
But also a sign of how easy it is
to pick low-hanging fruit
and do groundbreaking interpretability work.
Such a young field.
I know it's so impressive.
Yeah, I've just checked his Twitter.
Hi, Kevin.
And yeah, so...
To me, the underline...
Yeah, so I'm just zooming out a bit.
I think there's a family of techniques
around causal interventions
and their use in mech and top
that's useful to understand here.
So...
The core technique is this idea
of activation patching.
Where...
So let's...
So one of the problems with understanding
a model's features and circuits
is that models are full
of many, many different circuits.
Each circuit does not activate on many inputs,
but each circuit
will activate...
But on each input, many circuits will activate.
And in order to do good mech and top work,
you need to be incredibly surgical
and precise,
which means you need to learn how to isolate
a specific circuit.
And let's consider a statement
like...
The Eiffel Tower is in Paris
versus the Colosseum is in Rome.
These are both...
There's lots of features happening.
There's lots of circuits being activated
on the Eiffel Tower is in Paris.
This is in English.
You're doing factual recall.
You're outputting a location.
You're outputting a proper noun.
This is a European landmark.
Et cetera, et cetera.
And like, I want to know how the model
knows the Eiffel Tower is in Paris.
But the Colosseum is in Rome.
Controls almost everything
apart from the fact.
And so...
What I can try to do
is causally intervene
on the Colosseum run
and replace,
say, the output of an attention head
with its output
on the Eiffel Tower prompt
and see how much this changes
the answer from Rome to Paris.
And...
This...
Yeah, this patch...
Can let me really
isolate how the circuitry
for just this specific thing works.
And...
There's all kinds of work around this.
Obnoxiously, all of it uses different notation.
Like...
Resample ablations
and causal tracing
and causal mediation analysis
and entertained interventions.
All similar words are basically the same thing.
But... Yeah.
The really
key insight here is this kind of surgical
intervention.
A classic technique in interpretability is
ablations, where you just set something
to zero. And it's kind of janky
because if you break
something in the model, which wasn't
interestingly used for the task,
then everything dies.
Or if you break it in interesting ways,
then everything dies.
For example, in GPT-2 Small,
almost every single task
breaks if you delete the 0th MLP
layer.
Yeah, as far as I can tell,
the 0th MLP layer is kind of an extended
embedding.
GPT-2 Small has
tied embeddings and unembeddings,
so they're transposed of each other,
which is wildly unprincipled, in my opinion.
And...
The model seems to be both using this
detokenization and combining nearby
things with the 1st attention layer,
0th attention layer, and just
undoing the tightness.
But this means
that basically everything is reading from that.
And I've seen people
do zero ablations and everything, and be like,
oh, this is an important part of the circuit.
Let's get really sidetracked by this.
Because the effect size is so big.
Yeah.
Man, being a mech interpret research feels my mind
with such bizarre trivia like this.
It's great.
Models, so bizarre.
And so...
Yeah.
This calls for intervention.
There's kind of two conceptually
different kinds of interventions.
You can take the Eiffel Tower prompt,
patch in something from
the Colosseum, and see if it breaks
the ability to output Paris,
to verify which bits
kind of are
necessary, such that getting rid of them
will break something.
Or you can patch something from the Paris run
into the Colosseum run,
and see if that makes it output Paris.
Which is testing for something that's sufficient.
I call the first one
a resample ablation,
because you're messing up a component by resampling,
and the second one
denoising or causal tracing,
because you're
intervening with like a
bit of information,
and seeing if that is sufficient for everything else.
Though none of these names are good.
I would love some to come up with better names.
And there's all kinds of
families of work building on this.
Like...
I have this post called
attribution patching, that tries
to apply this as an industrial scale
by using gradients to approximate it.
Which is fast enough
that you could take a GPT-3, and it's
four million neurons,
and do attribution patching on all neurons at once,
on every position.
Great, great post.
Redwood Research has this
technique
called causal scrubbing.
Which I view as
activation patching
gone incredibly hard and rigorous.
That tries to come up with an
automated metric for saying
this hypothesis
about a model is actually accurate
for how it works.
Where it's kind of
complicated, but the core idea
is you think of a hypothesis as saying
which resample ablations are all
out, and you make
all of the resample ablations that should be
allowed. Like
these components of the model
shouldn't really matter, so we can
just patch in stuff from random other inputs.
If you've got, say, an induction head, you might
think the induction head
cares about the current token
and the thing
before the previous
token that it's going to
induction, it's going to inductionally
attend to. So let's
replace the
token that it's going to be attending to
with a token from a different input
but with the same token before it.
My hypothesis about
the induction head says this should be allowed,
so let's do that.
I wouldn't want to use a rant,
but the metric
he uses is really important.
Yes,
this is one of my hobby horses.
So
some
of the original work looking at the patching
stuff, like David Bow
and Kevin Meng's excellent
Rome paper,
uses the probability
of Paris as their metric.
And there are other
papers that use things like accuracy as their
metric. And generally
I think of metrics as being
on a spectrum from like soft
to sharp.
So
generally I think of models as
thinking in log space.
They
are kind of acting like basions.
They
are trying to figure out some things in Paris and there'll be
five separate heads that each
contribute one to the correct logic.
And each of these can be thought of as
like one bit of information
and together they get you
the right probability of say 0.8.
But if you
patch in each one in isolation
the
probability changes negatively
because probability is exponential in the
logits.
So if you're using probability you're like
oh this head patch doesn't really matter.
And so in this paper they
did this thing of patching in like
10 adjacent layers at once.
And to me a really core principle
of this kind of causal intervention
and mechanistic technique
would be as surgical as possible
to be as deeply faithful as possible
to what the neural model is actually doing.
So in this case there was an interaction
between them. They were effectively making
several interactions or interventions
at once.
Yes, yeah they were like
replacing 10 adjacent layers
and patching
things in different layers is always a bit weird.
I don't think that pots
that objectionable. I mostly just
feel like if you choose
a metric like log prop
it allows you to be much more
surgical about how you intervene.
It allows
you to identify
subtle effects of things.
Accuracy is even worse
because accuracy is basically rounding things
0 or 1.
So like if the threshold is 2.5
any individual patch does nothing.
Any re-sample
ablation does nothing.
But if you patch in like
the 10 adjacent layers it will do everything.
And this
can be kind of misleading. Another
one I often see people do
is
they're
trying, they look at
things like the rank
of an output, like at which point
does the model realize Paris is the most
likely next token
and this can be super misleading
because this will make you think the third
head is the only head that matters.
When really
all five of them matter the order is kind of arbitrary
and
yeah, I've seen
papers that I think got somewhat misled
by using metrics like this
and
metrics, they matter so much.
It's so easy to trick yourself. My high level
pitch is just
mech and tub is great. Mech and tub is
beautiful. Also the field is incredibly
young. There's maybe 30 full
time people working on it in the world.
There's a ton of low-hanging fruit.
I've done major
research in this field I've been in it for like less
than two years.
I would love people to come
and help us solve problems and do
research here.
We'll link to my post on getting started
and my sequence called
200 Cronkite from Problems
in the description to this, hopefully.
And
yeah, I think there's just
it's not that hard to get started. It's
really fun. Hopefully
I've nerd sniped you with at least one
thing in this
podcast. And if you're at least
vaguely curious, it's just really easy
to open one of the tutorials linked in my
posts and just start screwing around.
And I'd love to
see what you can find.
Beautiful. Also
the DeepMind alignment team is currently
hiring and people should apply.
Which includes hiring for a mechanistic
interoperability team.
Amazing. Do they have to do lead code?
I have no idea.
Can't remember.
Yeah, we did an amazing
video with Petr Felichkovich.
I gave him
one of my lead code challenges and
annoyingly he aced it.
It's all that DeepMind
interview practice.
Anyway, okay.
Let's talk about super intelligence.
Now I spoke with
your friend Robert Miles about a month
ago. Rob's so great. He's a lovely
chap. Spoke all about alignment and
he accused me of over
philosophizing everything because I was talking
all about intelligence, one of my favorite topics.
And he said, well, what about
fire?
Fire is something that
people didn't understand millennia ago,
but they knew that it burnt and they knew
that it was bad.
And this is like a fire.
Which is very interesting. And maybe
we can bring in a little bit of effective altruism
as well.
If there is one
thing I have learned from the past
decade of machine learning programs is
that you do not need to understand a thing
in order to make it.
And this extends
to things that are smarter than us
and which are capable of
leading to catastrophic risks.
Yes.
Well, let's
step back a tiny bit
and then we'll get there because there's the
hypothetical nature which I guess I have
a bit of a problem with. Now, about 10 years
ago, I was one of the first
supporters of Sam Harris's podcast
and he's quite aligned to
EA. And he was
talking about
this very noble idea that everyone matters
equally. And people
on the left should get on board of that intrinsically.
And this idea that we should
quantitatively analyze the impact
of charity work and solve an
optimization problem and earning to give
and a lot of the stuff that
McCaskill spoke about and also
philosophers like Peter Singer.
And the focus seemed to be
primarily on alleviating poverty
which we
and we don't say the biggest problem
we say a problem. This is another
thing our friend Robert Biles said
he said, the problem is
when people talk about the problem
there can be more than
one problem.
But anyway, so
it's a big problem
and
recently you and I can agree
that EA circles
have really laser focused
in on existential
risk from AI
as opposed to other more plausible
ex-risk concerns like pandemics
or even nuclear war. Not to say
that they don't focus on that.
I am going to push back on other more
plausible ex-risk.
Go on.
Okay.
So
and you know cynically from
my point of view, I see
the influence of Eliezer
Bostrom, Hansen
etc. kind of shifting
the focus on to ex-risk
and part of the reason for that
is also this kind of
overly intellectual
focus on long-termism
and it's done in a very
intellectualized
way. So it's based
on the utility function
now incorporating
future simulated humans on different
planets, you know, a long time
away in the future and
making all of these intellectual
jumps. So let's start
there. What do you think?
So much stuff to respond to
in there. Good. So
a couple of things. The first
so cars on the table
I care a lot about
AI existential risk. Yes.
The reason I work on mechanistic
interoperability is because
I think that understanding the
mysterious black boxes that are
potentially smarter than us and may
want things wildly different than what
we wanted them to want is
just clearly better than not understanding
them. Yes. And
I think mechanistic interoperability
is a promising path here. So
and I also would consider myself
an effective altruist and a rationalist.
So cars on the table, there's
my biases.
So I generally
think it's more productive to discuss
is AI
catastrophic and existential risk
a big deal than is it
the biggest deal or
is it worth more resources
on the margin than global poverty
or climate change
or ethics and like there's just
lots of problems. I care way
more about convincing people that
AI extras could be in your top 10
than it should be in your top one
because I feel like for most people it's not
in their top thousand
and there's just so much
divisiveness between say the
ethics community and the
alignment community about whose problem is a
bigger deal and like both are big problems
why are we arguing
and
part of this is about
moral intuitions and this is something
I spoke a lot with Connor about. He said
that in many ways he's
got this technical empathy
so
sensory empathy is I really care about
my family. They're these concentric circles
of moral status. I really care about my
family and if I try
really hard I can care about
people in other countries and so on and then
if I try really really hard I can care about
future simulated lives on Mars
and Connor said
the idea of this movement
is about galaxy branding yourself into being the
most empathetic person imaginable
but it's a kind of empathy that people
don't understand.
Yeah so
a separate bit of beef
I have is with the entire notion of
long termism.
Long termism is this idea
so long termism
is generally caring about the long term future.
There's like the strong form
of value
in the future basically entirely
dominates things today and weaker forms
of just this really really matters
and
a common misconception about AIX
risk and AI safety is that
you should only
work on this if you are a long termist
that
you know it's a one in a billion chance
of mattering but
there's
a quintillion future lives
that outweighs everyone alive today in moral
or
well we're only going to get AGI in like
500 years but we're going to work on it now
just in case
and like I think both of these are just nonsense
like
I guess as a concrete example
Effective Artists
have worked on pandemic prevention for many years
and
I think it was just clearly the case
that pandemics are
a major threat to people alive today
and I
like to feel that we've been proven right
no one's going to argue at that point
and you know everyone's being like
Effective Artists why you work on AI safety
this obviously doesn't matter
you know I feel like we got one thing right
can I be really skeptical though for a second
because
I mean you're working for DeepMind
there's so much prestige
and money attached to AI risk
Elon Musk is talking about it all the time
whereas you could
be a scientist working on pandemic
responses
and I mean let's be honest
it wouldn't be anywhere near the same level of prestige
yeah
so
couple
of takes
it
definitely is the case that
a good chunk of why I personally
am working on AI X-Risk
rather than say Bio X-Risk
is that
I'm a smart mathematician
I like AI I like mech and tip
I do not think I would be good at biology
in the same way
and I also
would personally assert
that AI X-Risk is more important
and like more pressing
but
you know I'm biased
and I think it's fair to flag that bias
um
in terms of prestige
so I've only really been working
on this stuff properly for the past two and a half years
which is
I mean it's changed dramatically like in the last
six months we've gone from
well we're really ever going to get AGI
to oh my god GP4 exists
Jeffrey Hinton has left Google
to loudly advocate for X-Risk
Joshua Benjo is now loudly
advocating for X-Risk
it's two thirds of the Turing winners
for deep learning
you'll never get the third one
yeah we're never going to get the third one
there's maybe this position very very clear
but you know some majority I'll take it
yes and
or the fourth one
he's coming on our podcast actually
oh who's the fourth one
Schmitt Huber
oh I'm very curious to hear the Schmitt hero episode
oh yeah he's even more
virulently against than Jan
I'm afraid to say
two out of two
I'm interested to hear it anyway
yeah
and
yeah
in terms of prestige
I gather that say seven years ago
it was
basically just not
it would be like pretty bad for your career
you would not be taken seriously if you mentioned
caring about AGI X-Risk
your papers would be rejected
I hear a story of
Stuart Russell at one point talked to a grad student
of his about how Stuart was
concerned about AGI X-Risk
the grad student was also really concerned and freaking out
but they've been working together for years
and neither have I felt comfortable mentioning it
and
a lot of people who are still in the field
were doing the stuff then which makes me
somewhat reject the prestige
arguments at least for
senior people in the field
I think there's a difference with
Stuart Russell in particular he's
very credible
that I'm not
oh I didn't mean I didn't mean you I was talking
I was talking about the two
Godfather because the thing that
maybe I shouldn't say this but
I was surprised that Ben Gio and
Hinton came out in the way they did
and
the reason I didn't like what they said
was I felt that they were implying
that current AGI technology
could pose an existential
threat and what I'm getting from you
and what I'm getting from Russell
is and also from Robert Miles
is that this is a very real
potential
threat in the future but it's not a current
threat
yes very real potential threat in the
future though
I hesitate to
confidently assert say
this will not be a threat in the next five years
or something it's like pretty hard to say
interesting I'm
not
confident I agree with your assessment of
Ben Gio and Hinton
they've spoken a bunch publicly so
I'll defer if you can point to specific writings
but for example Ben Gio
signed the Paws A.I.
for six months more powerful than GPT-4
letter and
I don't know I don't think the letter was
asserting that the letter definitely wasn't
asserting GPT-4 was an extra risk
it wasn't confidently asserting GPT-5
would be but it's being like yeah
we need more time and slow down and caution
maybe I'm
reading too much into that but it seemed to me that
I mean Hinton said that
chat GPT now
contains all of the world's
knowledge and this chat
bot knows everything and it could
potentially do very harmful things
and I interpreted it
possibly incorrectly
that they were talking about
reasonably current or next
generation risks
I mean I can't talk for them
I also I don't know
there are lots of near-term risks
there's long-term risks
I consider it my job to think hard about the long-term risks
and try to guard against those
and I think lots of other people
their jobs is to focus on like the near-term
risks and both are like great forms of work
I don't know
one reason I like interpretability is I think it is
just broadly useful across all of them
so what I consider to be my job might just not
even matter but
yeah
yeah I know
I probably will not do not want to get deeply into
interpreting what other people have said
I
could I ping you to a couple of quick questions
so first of all
you know there's this idea of negative
utilitarianism I mean do you think
minimising suffering is more important than maximising
happiness
nah
not sure I've got a more deep answer than that
I mostly think a lot of this intrusive reasoning
is more driven by intuition than anything else
but it's a bit like this metrics thing we were talking about
you know which is that
if you want to have
would you like to tolerate some spiky
nags for some average happiness
yeah
so I know
I have like a general frustration
with
these discussions getting too philosophical
this is
the big issue when I hang out with the fakes of
altruists who really
love moral philosophy and population
ethics
I have this ea forum post called
simplify ea pictures to
holy shit x risk
it's like so I don't know
if you actually
look at some of the
concrete work people try doing on things like
timelines and risk
there's this
report from a jay acotra at open philanthropy
that gives
a 30 year median timelines
to A.I. that's transformative
which he since updated to 20 years
there's a report by Joseph
Carl Smith that estimates about
a 10-ish
percent chance of
a major catastrophe from this
and if you just take
those numbers this is clearly enough
to reach pretty high on my list
of concerns of people alive today
okay okay
and I think these are bold empirical
claims
and I think it's great to debate them in
the empirical domain
but to me this doesn't
feel like a moral
question it just feels like from
common sense assumptions
if you believe these
empirical claims this stuff is a really big deal
okay okay
let's take another couple of steps
so first of all we save this till later
I think
deception is very important
and Daniel Dennett when I spoke with him
he uses this notion called the intentional stance
which basically means that
if you use
a projection of
purposes, goals
agency etc in order to understand
the behavior of an agent
possibly a simulated agent
then for all intents and purposes
it has
agency it can make
decisions it has moral status
it has lots of different things like that
and he would say that without
an intentional stance
without agency
it's impossible for a model to lie
or deceive us
now what do you think would be the bar
for something like a GPT model to deceive us
and why
yeah so
before I give takes I will
generally reinforce Rob's
vibe of well if you have no idea
how fire works but you know that it burns you
that's kind of the important thing
like maybe
a model has just this
random learned adaptation
to output things that are designed to get a user
to feel and believe a certain way
that isn't intentional
and isn't deceptive in some true
cog size sense
but it's like enough for this to be a big deal
that we should care a lot about
with that aside
yeah
so
I'm definitely hesitant
to ascribe
a very confident view of what's going on here
and I think lots of early discourse
alignment around things like
utility maximization
and around things like
these things are just
paperclip maximizers etc
is kind of misleading
and I don't think it is
an accurate model of how
GPT 7
RLHF++ is going to work
well that's my prediction
one thing
that is pretty striking to me is
I just feel like we're pretty confused
on both sides of this
like I do not feel like
I can confidently claim that these models
will demonstrate anything remotely like
goals or intentions
but I also don't feel like you can confidently claim
that they won't
and I'm not talking like
99.99% confidence
I'm talking like 95% plus
confidence either way
and one of my visions for
what being good at Mechantep might look like
is being able to actually get grounding
for these questions
because I think ultimately these are mechanistic
questions
behavioral interventions are not enough to answer
like does this thing
have a goal in any meaningful sense
but yeah
my very rough, softer definition
would be
is the model capable of
forming and executing long-term
plans towards some goal
potentially if explicitly prompted
to like auto GPT
or just spontaneously
is it capable of actually
carrying out these plans
and does it form
and execute plans towards
some objective
that is like encoded in the
model somewhere
and I don't know
I think it's pretty plausible that the first
dangerous thing is like
chaos GPT-7
where someone tells it
to do something dangerous and it gets misused
more so than it's like misaligned
and I care deeply about both of these risks
okay
so yeah first one's more of a governance question
than a technical question
and thus is less where I feel like I can add value
so I agree with you on all of that
so yeah being less confused about
what's going on inside the models
and using interoperability
to figure out whether they actually do have
agency or goals
and sometimes they do the right things for the wrong reasons
auditing models
that seem aligned before they're deployed
is something that you've told me before
so great
just being able to check more deeply
that it truly is aligned
but I wanted to talk a little bit about
this interesting paper from Katya Grice
so she wrote
a response called
S-Rong debunking the AI apocalypse
a comprehensive analysis
of counter arguments to the basic AI risk
case X-Risk
and the reason I read it is so many of the
comments were destroying
me and Duggar after we interviewed Rob
and they said well if you're going to criticize
S-Risk I mean at least go and read
Katya Grice's response
so I did so I did here we go
so
she basically made
two big counter arguments that intelligence
might not actually be a huge advantage
and about the speed of
growth is ambiguous but
I first want to touch on what you said before
which is about this notion of goal directedness
so alignment people say
that if superhuman AI systems are built
any given system
is likely to be goal directed
and the
orthogonality thesis and instrumental goals
are cited as aggravating factors
and
the goal directed behavior is likely to be
valuable so economically
goal directed entities may tend to arise
from machine learning training
processes not intending to create them
which is kind of talking about some of the emergent
behaviors that we were talking about earlier
with respect to Othello for example
and coherence
arguments may imply that systems with
goal directedness will become more strongly
goal directed over time which is
apparently something that is argued for
so I'm thinking what does goal even mean
I mean we anthropomorphize abstract human
intelligible concepts like goals
and
they really are emergent because
they emerge from these low level
interactions in the cells in your body
and then you get these things that we recognize
to be goals
observer relative as we were talking about before
but they're just graduated phenomena
from smaller things right
so what does it even mean to have a goal
yeah
so
couple of thoughts on that
again you ask questions with a lot of content
in them
no problem
I can only apologize
I mean as someone who accidentally writes
19,000 word
blog posts all the time I relate
anyway
so
what am I saying
so the way
it's a fake concept right
so I definitely
want to try to take
so there's the mechanistic definition
of the model
forms plans
and it evaluates the plans according to some
criteria or objective
and it executes the plans that score better
on this
and I would love
if we get to a point where we can look inside a model
and look the circuitry
that could be behind this or not
that would feel like
a big milestone for me on
wow I really believe
this is a second type of matter
for reducing catastrophic
risk from AI
a second thing
is that
yeah the kind of more behavioral
thing of the model
systematically takes actions that pushes the world
towards a certain state
and
I don't want
I think there's a common
problem in alignment arguments
where people get too precise
and too specific
in a way that lots of people
reasonably object to in a way which is not
necessary for the argument
there's a really great paper
called the alignment problem
from a deep learning perspective by Richard
Noh, Lawrence Tann and Sorin Mindenman
and
this is probably my biggest
recommendation for the listening audience
of what I think is like a pretty well presented
case for alignment
and I generally pretty pro
trying to make the minimal necessary assumptions
so for me it's kind of like
some soft form of
goal-directedness
of take actions that push the world
towards a certain state
and
another important thing
is there are a bunch of theoretical arguments
for why goals would spontaneously emerge
ideas around
in a misalignment
by Evan Huminger
ideas around
the coherence theorems
and things like that
which I know I find like a bit convincing
not that convincing
but then there's things will have goals
because we try to give them goals
and I'm like
yeah that's probably gonna happen
it's just clearly useful
if you have
if you want to have an AI CEO
or
logistics and military operations
to have something that's capable of
forming and executing long-term plans
towards some objective
and if you believe this is what's gonna happen
then the key question is
are we capable
of ensuring those goals
are exactly the goals we would like them to be
and my answer
for any question of the form
can we precisely make sure the system is doing exactly
x and machine learning
is god no
we are not remotely good enough
to achieve this
with our current level of alignment and steering techniques
and to me
this is like
a more interesting point
where it's not quite a crux for me
but
it just seems like a lot easier to argue about
what people do this
yeah it's interesting
Katya herself said that
it's unclear that goal-directedness
is favored
by economic pressure
training dynamics or coherence arguments
whether those are the same thing
as kind of goal-directedness
that implies a zealous drive
to control the universe
and look at South Korea they have goals
and those goals
I don't really subscribe
to the dictator
view of society
I assume they are somehow emergent
and similarly
sorry South Korea or North Korea
sorry North Korea did I say South Korea
very different careers
different goals
different goals
but you can think about goals in an AI system
as either being ones which
emerge from some low level
or ones which are explicitly coded
by us
or ones which are instrumental
and
these are all a whole bunch of goals
but we can't really control those
we can add pressures
how do we control what North Korea does?
that sure is a question
I'd love for someone to answer
I don't know
like I can give speculation
there's like
there's the question within practice
what do people do
which is basically reinforcement learning
from human feedback
and I expect people would apply that
in this situation as well
I definitely do not believe
people too explicitly encode a goal in the system
moreover even if you can
encode
even if you could give some like scoring function
like
make the score in this game high
this does not give you a model that
intrinsically cares about that
in the same way that
I don't know
evolution optimizes inclusive genetic fitness
I don't give a fuck about inclusive genetic fitness
even though I care about a bunch of things
evolution got me to care about within that
like tasty foods
and surviving
so
we don't know how to put goals into systems
I
basically just assert that we are not
currently capable of putting
goals into systems well
and
this is one of the main things
the field of alignment thinks about
and we're not very good at it
in terms of
yeah
I definitely don't want to make
strong claims about
to be dangerous the goals need to be coherent
or the goals need to
there needs to be like a singular goal
like I don't have a singular goal
it's
not obvious to me how these systems
will turn out
if they don't in any meaningful
sense want a coherent thing
then I'm a fair bit less concerned
though
well I mean there's many many ways
that human level AI would be good for the world
or bad for the world
or just wildly destabilizing in high variance
which misalignment risk is one of them
and lots of the other ones just don't apply
like misuse
and systemic risks
but leaving those aside
yeah
I think if a model is just
roughly pushing in a goal directed direction
with a bunch of caveats and uncertainties
and flip-flopping
that still seems like a pretty big deal to me
okay
okay
Katya
let's just cover her two main arguments
so she said that intelligence might not
actually be a huge advantage
so looking at the world
intuitively
big discrepancies in power
are not to do with intelligence
and she said
IQ humans with an IQ of
130
and roughly 6,000 to 18,000 a year
dollars more than average IQ humans
elected representatives
are apparently slightly smarter
on average
but not a radical difference
Mensa isn't a major force in the world
and
if we look at people
who evidently have good cognitive abilities
given their intellectual output
their personal lives are not obviously drastically
more successful anecdotally
is it that much of a big deal?
yeah
so I think this is like a fair point
if we looked in the world
and IQ
or whatever metric of intelligence
you want to use
clearly dramatically correlated
with everything good about someone
I mean IQ correlates with like
basically everything you might value
in someone's life
because we live in an unfair world
but not dramatically
so
I think this is a valid argument
I generally don't
think you should model
human level AI
as
like
slightly superhuman AI as like an IQ 200 human
for example
GPD4 I would argue knows
most facts on the internet
or many facts
and
yeah knows many facts
and
this seems
GPD4 knows many facts
and
this is sure an advantage over me
GPD4 knows how to write a lot of code
and it knows how to take software
and do penetration testing on it
it
knows lots of social conventions
and cultural things
and has lots of experience reading
various kinds of
text written to be manipulative
or manuals on how to make nuclear weapons
sorry
I'm going too hard on the knowledge point
there's just lots of different axes
you can be human level or better
in
where knowledge is one, intelligence and reasoning
is one
manipulation abilities is another
charisma and persuasion is another
I think these two are particularly important ones
there's
forming coherent plans
there's just like
the ability to execute on stuff
24-7
running thousands of copies of yourself
in parallel distributed across the world
there's running faster than humans
and there's just like lots of dimensions here
I think the IQ
200 human frame
is helpful in some ways
but
unhelpful in other ways especially if it summons the like
nerdy
scientist with no social skills
who's life is a mess archetype
I say he's a nerdy scientist with no social skills
who's life is a mess
okay
yeah I mean this is the thing is
because Rob said the same thing
on chess it's possible for someone to be
literally 20 times better than you
that there's a huge dynamic range of skill
and that's something we've not really seen
in human intelligence and it might be because
of the way we measure it
it's possible that the way we measure it doesn't
even
capture people with
broader or better abilities
let's just cover
her last point quickly so this is that
the speed of intelligence growth
is ambiguous
so this idea that AI would be able to rapidly
destroy the world seems
prima facie unlikely to Katia
since no other entity has ever
done that
and she goes on
so the two common broad arguments
is that there'll be a feedback loop
in which intelligent AI makes
more intelligent AI repeatedly until
AI is very very intelligent
number two small differences
in brains seem to correspond to very large
differences in performance
based on observing humans and other apes
thus any movement
past human level will take us to unimaginably
super human level
and the basic counter-arguments
to that is that the feedback
loops might not be as powerful as assumed
there could be diminished returns
there could be resource constraints
and there could be complexity barriers
so maybe we should just do
that kind of recursive self-improving
piece first what do you think about that?
I don't really buy a recursive self-improvement
it's not an important part of why I'm concerned about this stuff
generally I just feel like
a lot of the arguments were made
before the current paradigm
of enormous foundation models
when you're investing hundreds of millions
of dollars of compute into a thing
it's pretty hard
for it to make itself substantially better
and you can do things
like design better algorithmic techniques
I think that is probably
one that is more likely
to be accelerated the better the model gets
it's not clear to me
how much
how much juice there is to squeeze out of that
and
yeah
but generally I just think a lot of this is going to be bottlenecked by hardware
and compute and data
such that
I'm less concerned about some runaway
intelligence explosion
and I'm more just concerned about
we'll eventually make things that are dangerous
what do we do then?
and I think this is like a really
good fact about the world
I think a world where you can have
intelligence explosions is really scary
and
I feel like our current world is a lot less scary
than it could have been
if some kid in a basement somewhere
just like wrote the code for AGI one day
yes
yes
ok well just to finish off
Katya's final point
the other point they made was about
small differences might lead to oval
it's a little bit like in squash
I don't know if you've ever played squash
but a tiny difference in ability leads to
one player overwhelmingly dominating
the other player
because you just get these kind of like
it's a game of attrition
and you get these tipping points
that might not necessarily be the case
when comparing AI systems
because of three reasons
different architectures
likely to have very different underlying architectures
and biological brains which could lead to different scaling properties
performance plateaus
so there might be
these plateaus beyond which further increases
in intelligence don't lead
to significant performance improvements
and also this notion of task specific
intelligence something that I strong
I believe that all intelligence is specialised
as we were speaking about earlier
so it might be specialised rather than
being generally intelligent
and small differences thus may not translate
into large differences in performance
across a wide variety of tasks
maybe we should just touch on this
on this kind of task focus thing
so I think humans are very specialised
we have and we don't realise
that we are because the way we conceive of intelligence
is anthropomorphic but actually
we don't do four dimensions very well
there's lots of things that we don't do very well
and we're kind of embedded in the cognitive ecology
in quite a complex way
so what do you think about that?
Yeah so
I will
okay I'll first comment on the general metodynamic
of
I think that people
get way too caught up on
philosophising
and no offense
and
in particular
I care about whether an AI
will cause a catastrophic risk
I don't care about whether it fits
into
whether it's general in the right way
whether it has weaknesses in certain areas
whether it's
high on the Chomsky hierarchy
or whether it's generally intelligent
in some specific
sense that someone like Gary Marcus would agree with
Is that
in any way a contradiction
of your
mechanistic sensibilities
because when it comes to neural networks
you want to understand
how they work
but when it comes to intelligence you don't
Oh sorry
I want to understand how it works
I want to understand everything
I just
don't think it's
I want to disentangle
things to be concerned about
from theoretical
arguments about whether this fits into certain categories
for the purposes of deciding
whether to be concerned about AI existential risk
I see all of the
theory arguments as like a means to an end
of this ultimate
empirical question of
is this a thing that could realistically happen
and
I
think that these
theoretical frameworks do matter
like
I don't know I think that
an image classification model is basically
never going to get the point where it's dangerous
while
a language model that's being RLHF'd
to have some like
notion of intentionality
potentially will
and
yeah
I don't know I can give like random takes
but
to me if you're like
AI is going to be task-specific in the same way that humans are task-specific
I'm like well like
human is task
general enough that I think they could be massively dangerous
in the right situation with the right
advantages
like if they wanted to be
and were able to run the thousands copies of themselves
at a thousand X speed or something
I don't know if that's actually
a remotely accurate statement about models
probably they can run many copies but not
at thousand X speed or something
but
yeah
generally
that's the kind of question I care about
and I'm concerned many of these definitions
lose sight of that
and
part of my thing of like
I want to keep alignment arguments
as having as few assumptions as possible
because the more assumptions you make
the less plausible your case is
and
the less and like
more room there is for people to like rightfully disagree
I'm like
I want to be careful
not to make any of the case rest on like
strong theoretical frameworks
because we don't know what we're doing here
enough to have legit theoretical frameworks
and
I think that AI is likely to be limited
in the same way that humans are
at least within the GPT paradigm
because if you're training it to predict the next word
on the internet and a bunch of other stuff
then
it's going to learn a lot from human
patterns and human thought
and human conventions
but
I don't know
In closing
you said that your personal favorite heuristic
is the second species argument
can you tell us?
Yeah, so
I quite like
Hinton's recent pithy quote of
there is
no example of
something being of
some entity being controlled
by things less
smart than it. And that was terrible
sorry?
I really was
Twitter went wild over that
I did try to go wild
Look at a company
the CEO is usually
dumber, you have to hire competent people
to have a successful company
look at my cat
Yeah, okay, fair. This is a terrible phrase
Let's just start again
Alright, so
this is often called
the gorilla problem
humans are just smarter than
gorillas in basically all ways that matter
humans are not actively malevolent
to gorillas
but ultimately humans are in charge
gorillas are not and gorillas exist
because of our continued benevolence
or ambivalence
and it just seems to me
like if you are
creating entities
that are smarter than you
the default outcome
is they end up in control
of what's going on in the world and you do not
and
I kind of just feel like this should be the null
hypothesis and
then there's a bunch of arguments on top of
like is this a good model
well, obviously there's lots of disanalogy
is because we're making them
we ideally have some control over them
we're going to try to shape them to be benevolent
towards us but
this just seems like the default
thing to be concerned about to me
On that point though
we are different from computers
we scuba dive
and that's actually quite a profound thing to say
we scuba dive because we are
we are integrated into
the ecosystem
not just physically but cognitively
there's a kind of cognitive ecosystem that we're
enmeshed in we have a huge
advantage over computers
computers can't really do anything in the physical
world
so I agree with this
but I don't know
I feel like the way
I don't know one evocative example
is there was this crime lord
El Shapo who ran
his gang from within prison
for like many years very successfully
when you have humans in the world who can get
to do things for you you don't need to be
physically embodied to get shit done
and I don't just
look at Blake Lemoine there's no shortage
of people who
will do things
if convinced in the right way
even if they know it's an AI
and I do agree with you on that
and I think part of the reason why we're going to
have the inevitable proliferation
of this technology is
so many tinkerers
will just create many many
different versions of AI
and they won't really be thinking about the consequences
of their actions
but what's the alternative
paternalism
yeah so
to me the main
interesting thing here
is large training runs
as like the major bottleneck
very few actors can do them
we're probably going to get
beyond the point where people are
even putting the things out behind an API
open to many people to use
let alone like open sourcing the weights
which we've already pretty clearly moved past
and this to me
seems like the point of intervention you need
if you're going to try
to make sure things are safe before you deploy them
like
track the people who are able
to do these runs
have standards for
what it means to decide a system like this is safe
I'm pretty happy
Sam Altman's been pushing that stuff
very heavily
and if competently done
I think this kind of regulation can be
very important it could be great
like the alignment research
has been doing great work here
and I'm very excited to see what the
Reds teaming
large language models thing at Defconn looks like
but I don't know
maybe to close
I feel like I've been in the role of why alignment matters
maybe I can
try to break alignment arguments
myself for a bit
please do yeah
so if I
condition on actually the world is kind of fine
probably
my biggest guess
is that the
goal directed notion is just like
not remotely a good
understanding of how these things work
and it's hard to get them to be
goal directed and we just mostly coordinate
and don't do that
and
these systems are mostly just like extremely effective tools
seems like kind of a plausible
world we could end up in
I don't think it's any more
likely than yep they're goal directed and this is terrible
we end up in a world which
has like lots of
these systems that
don't coordinate with each other
want some more different things
are like broadly aligned with human
interests
but like imperfectly and just none of them
ever get a major advantage over
the others and the world kind of continues
to be about as the world is
with lots of different actors who
aren't necessarily aligned with each other
but mostly don't
try to see over the world except every so often
or
we just
alignment isn't that hard
we crack mechanistic interoperability
we look inside the system
we use this to iterate on making our techniques really good
it turns out that doing RLHF
with like enough adversarial training
just kind of works
or with AI assistance to help you notice
what's going on in the system
and this just gets us aligned to human
level systems and we can be like
please go solve the problem and then they do
and I know
I think people like Yadkowski
are very loud about we are almost certainly
going to die and
we might but we also might not
I don't really know
I would love to just become less confused
about this and
I remain very concerned about this to be
clear but I'm not like
99% chance we're all going to die
yeah but anything which
is an appreciable percentage
may as well be the same thing
yeah pretty much
yeah it's quite funny I got a lot of
pushback on the Robert Miles show
people said oh I can't believe it
you framed him to be a doomer
and he himself said
in the show probably I think about five
times we're all going to die
and I managed to cut
about five
I don't know how to exaggerate but there was
at least two posts on Twitter within
15 minutes of that comment where he said
and we're all going to die
so I don't think I'm being
well I didn't actually call him a doomer
but he basically is
I don't know man
I hate ladles
like Eliasar is clearly a doomer
he's clearly a doomer
Rob is much less doomy than Eloiser
is Rob a doomer? I don't know
I didn't call him a doomer
but empirically the data
says yes
yeah I mean I don't know man
it sounds like you spend too much time reading YouTube comments
I do
too much time
notoriously the least productive use of time possible
apart from hanging out on Twitter
reading AI flame walls
Twitter is the worst
it's so bad I mean we don't
need to go there but we were having a brief
discussion before
we started in record
why do you think otherwise
intelligent
respectable people
behave in that way?
impulse control
social validation
it's just kind of fun
people aren't very self-aware
about how they look
or like aren't that reflective and Twitter incentivises you
to like nuance
and to be outraged about other people
I don't know
I
am very sad by many Twitter
dynamics including from people who otherwise seem
worthy of respect
yes
look Neil
this has been an absolute
honour thank you so much
it's been amazing it's been a marathon
but thank you so much for joining us today
and I really
think we've had a great conversation and I know
everyone's going to love it so thank you so much
I apologise for all the times I saw you off for philosophising
oh no problem
it's an honour
alright
thanks for having me on

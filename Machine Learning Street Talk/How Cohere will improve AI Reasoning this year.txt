previous generations of the model have been weak reasoners.
But they do reason.
In the same way that hallucination
was an existential threat to this technology,
no, we'll never be able to trust this stuff.
There are hundreds of millions of people
using this tech now, and they trust it.
It's actually useful for them.
We're making very good progress on the hallucination problem.
I think we'll make very good progress
this year and next on Reasoning.
Here we are again, another episode of MLST.
Today with Aidan Gomez, the CEO of Coheir.
Now I interviewed Aidan in London a couple of weeks ago.
Just after their build event,
and after Aidan did his presentation,
I sat him down for an hour and I gave him a grilling.
And he was such a good sport
for being so transparent and authentic.
This is the difference with Coheir.
They say it like it is.
There's no bullshit.
There's no digital gods.
There's no super intelligence.
They're just a bunch of folks
solving real world business problems.
Their models are incredibly good
for a true vlogmented generation, for multilingual.
But they also have some serious challenges
that they need to overcome.
We spoke about the AI risk discussion,
where the language models take away our agency,
how he's dealing with policy, right?
So on the one hand, he's talking with governments,
trying to get them to allow startups
to be more competitive, to innovate.
But at the same token, he's also very concerned
about some of the societal risks of AI.
So that's quite an interesting burden
and juxtaposition that he has to hold in his mind.
We spoke a lot about the company culture at Coheir.
Now, I'm very impressed with Coheir.
All of the people I've spoken to have been very smart,
just really nice people.
And how has he cultivated that culture?
He was very frank and transparent
about some of the mistakes he made early on as a CEO.
So yeah, plenty to get your teeth into.
I hope you enjoy the conversation.
For Coheir, I think we're a little bit different
than some of the other companies in the space,
in the sense that we're not really here to build AGI.
What we're here to do is create value for the world.
And the way that we think we can do that
is by putting this tech into the hands of enterprises
so that they can integrate it into their products,
they can augment their workforce with it.
And so it's all about driving value
and really putting this technology
into the hands of more people
and driving productivity for humanity.
Aidan, welcome to MLSD.
It's an absolute honor to have you on.
Thank you so much, appreciate it.
There's a bit of a lost mile problem
with large language models,
so you folks have created this incredible general technology.
But when enterprises implement it,
they have a whole bunch of legislative constraints,
security constraints.
Yeah, yeah, I think there's loads of barriers to access.
Privacy to policy to just the familiarity of the teams
with the tech, it's brand new.
And so they haven't built with this technology before
and they're still getting up to speed
with the opportunity space, what they can do with it.
That being said, people are so excited about AI
and its opportunity that the motivation and the will
is there to overcome a lot of these hurdles.
So we're trying to help with that as much as we can,
whether it's like our LMU education course
to help general developers get up to speed
on how to build with this stuff
or us engaging at the policy level
to make sure that we have sensible policy
and not over-regulation or regulation that hurts startups
or encumbers industry in adopting it.
So we're trying to pull the levers that we can
to help accelerate the adoption and make sure that it,
it gets adopted in the right way.
But yeah, no, it's definitely the past two years
have been a push.
There's a lot of stuff slowing down adoption
that I would love to see eased.
I think the tools need to get better, easier to use,
more intuitive, more robust.
You know, prompt engineering is still a thing.
It shouldn't be right.
Shouldn't matter how you phrase something specifically.
It should be, the model should be smart enough
to generally understand your intent and take action
on your behalf reliably.
So even at the technological layer,
I think us as model builders, we have a lot to do
to bring the barriers down.
But I'm optimistic, like the pace of progress
is fantastic.
Yes.
On that kind of prompt, brittleness thing,
I wonder whether you think that we are on a path to have it.
I mean, in an ideal world, the model and the application
would be completely decoupled.
So you could swap the model out,
or when you folks bring out a new model,
we can just swap it out and nothing breaks.
But at the moment, that's not the case.
But as the models become increasingly better,
do you think they will be robust in that way?
They should be, right?
There will always be quirks to different models,
because we're all training on, you know,
we hope different data that focuses on different aspects
or elicits different behavior in the model.
So there will always be quirks to the behavior of models,
the personalities of models, what they're good at and bad at.
But in general, in terms of like following an instruction,
we should be quite robust to that universally.
And so the ideal is that, yeah, you can just take a prompt
and drop it into any system and see which one performs best,
and then move forward with that one.
In reality, the status quo is a prompt that works on one system,
fundamentally does not work on another.
And so there's this rift, or these walls in between systems
that make them very, very different.
Hopefully that'll start to lift.
There's a lot of effort going into data augmentation
that makes these models more robust to changes in prompt space.
We're doing a lot of work on that.
It's driven a lot by synthetic data and finding,
doing search to basically find the prompts
or the augmentations that changes to prompts
that break the model and then training to fix that break.
So I'm optimistic that sort of brittleness
is gonna go away.
Interesting.
So kind of finding problems with the model
and then robustifying and robustifying.
In doing so, how does that change
the characteristics of the model?
Does it make it less creative or less capable in some sense?
I mean, what do you ever feel for the trade-offs there?
Yeah, I mean, I don't think so.
I think that that's orthogonal.
The process of making it more robust
is orthogonal to creativity.
There are aspects of the post-training procedure
that do reduce creativity or, you know,
some people like to say like lobotomize the model.
So it's definitely a problem.
It's something that we watch for
and we're trying to prevent.
I would say that one of the most disappointing aspects
of the current regime of building these models
is that a lot of people train on synthetic data
from one source, right?
Like just from the GPT models.
And so all of the models that are being created,
they sort of speak the same.
They kind of have the same personality
and it leads to this collapse
into a lot of different models
looking and feeling the same.
And that makes them boring.
Because like you have the same shortcomings across models
rather than if you have a diverse set of models
that have different failures in different places,
you can much better address, you know,
the preferences of much more people.
I've noticed due to synthetic data taking off,
just a total collapse in terms of
the different types of behavior models exhibit
and that cohere because our customers are enterprises.
Like that's who we sell to.
It's not consumers, it's not, you know,
anything other than enterprises
who want to adopt this tech.
And they're very, very sensitive
to what data went into the model.
And so we exclude other model providers,
data, you know, very aggressively.
Of course, some will slip in
as we're scraping the web, et cetera.
But we make a very concerted effort
to avoid other model outputs.
And so if you talk to our model
when we release command R and R plus,
one of the things I kept reading on Reddit and Twitter
was it feels different.
Like something feels special about this model.
I don't think that's any like magic at cohere
other than the fact
that we didn't do what the other guys are doing,
which is training on the model outputs of OpenAI.
I agree with you.
So when people, you know,
see chat GPT or whatever for the first time,
they're blown away by it.
But there are motifs that come up again and again
and again, unraveling the mysteries, you know,
delving into the intricate complexities
or blah, blah, blah, blah, blah.
And when you start to see these patterns
and these constructions, you just start to think,
oh, I don't like this very much
because you start to see through it.
It's a little bit like, you know,
when you start to see through someone,
they're not interesting anymore.
And I haven't seen that with cohere,
but I have seen it with many of the other models.
Now, my intuition was always,
I don't, I haven't really formed this very well,
but I thought that maybe it could come from
just the kind of data sets that we're using,
or maybe it could come from the preference fine-tuning.
Are you saying that that monolithic effect
is because they're kind of eating each other's poop?
Yeah, no, yeah, yeah.
It's some sort of like human centipede effect.
I think, yeah, they're training on the outputs
of a single model, and so it's all collapsing
into that model's output distribution.
And so if that output distribution has quirks
like saying the word delve a lot,
then it's gonna just pop up all over the place.
And people will take it for granted that,
oh, I guess LMs just behave like this,
but they don't have to.
They don't have to.
It's interesting how subjective creativity is as well,
because a lot of people thought
that it was creative a couple of years ago,
and then when you see it everywhere,
it's not creative anymore.
So it needs to be novel to be creative.
But I mean, you folks have just released
the Command R series of models,
and you've blown everyone away.
Tell me about them, but if you wouldn't mind also,
why did it take you a while to catch up
and get state-of-the-art performance?
Yeah, we spent a lot of 2023 lagging.
I think that is accurate to say.
What we were doing was sort of reorganizing internally.
We were rebuilding the company,
rebuilding the modeling team, the tech strategy,
and preparing for the runs that led to Command R.
It was clear to us that the process
that we had used to build the first command
and generations before that, it wasn't working.
It wasn't gonna scale.
And so we just rethought the entire pipeline.
And it took us a while to rebuild things,
run the experiments that we needed to run
in order to make decisions on what the design
of this new model building engine would look like.
And then it takes time to do the runs.
We spent a lot of last year doing that.
But I think the results speak for themselves.
And also with Command R and R+,
just the first step in a series of new models
that we wanna produce,
we're very excited to lean into specific capabilities.
And so while the general language model improvements,
they're super important.
They're crucial, right?
Like the models have to get smarter.
They have to get more capable.
And we'll continue to press on that direction.
We care about narrowing our focus a bit more.
And so for 2024, even with the Command R series,
we focused in on rag and tool use.
I think you're gonna see a continuation
and extension of that focus
really making these models robust
at the key capabilities that enterprise cares about,
that will drive productivity,
that will automate really sophisticated processes
that today, as humanity knows it,
is only the domain of humans.
We really wanna go after that.
And give our models the ability to help in those spaces.
Is it fair to say that,
I don't know whether you feel
that the general large language models are saturating.
Maybe you could comment on that first,
but if you do think that,
does that give me a bit of a read
that there's a move towards specialization of the models?
I don't think they're saturating.
I think they're getting so good
that it's hard to see the incremental improvement.
But that incremental improvement is extremely important.
So once the models are smarter than you,
it's really hard to, in a domain, in medicine,
like Coher's model, it knows more than I do about medicine,
for sure, just absolutely.
And so I can't really effectively assess
whether we're improving in that dimension.
I can't tell anyone.
It's smarter than me.
I trust it more than I trust myself
to diagnose symptoms or process medical data.
And so I'm not equipped to do that.
Instead, what we need to do is create data sets
or go out and find people who are still better
than the model at those domains.
And they can tell me whether it's improving.
But for us, like the general population,
at some point, we kind of stop seeing improvement
between model versions.
It's harder to feel.
And you need to really zoom in
to a place that you're an expert
and that you know previous generations failed
to see the progress.
I think about it sometimes as painting in a canvas of knowledge.
And at some point, the holes in the canvas become so small.
You have to take out a microscope
to actually see it and paint it in.
We're sort of in that part of the space for these models.
And so improvement becomes much harder
for us, the model builders,
but it's much harder to feel and see for users
who aren't diving in very close to analyze performance.
I don't think it's saturating.
I think it's still making,
we're still making very significant progress.
I do think the past 18 months,
maybe a little bit less than 18 months.
Yeah, the past 18 months, 12 months,
we've been compressing.
So we built these massive, giant,
multi-trillion parameter models,
which were just extraordinary artifacts
of intelligence and capability.
And we realized it's impractical.
You can't actually put this thing into production, right?
Like it takes 60, a 100s to serve.
It just, we could not productionize this.
The economics don't work out.
And so then we spent the year compressing
those massive models down into much smaller form factors.
There's very likely going to be a series of re-expansion
and scale, both on the model scale
in terms of parameters, but also data scale and data quality.
And that's being supported by much better synthetic data
methods that find much more useful synthetic data
that are quite compelling at search to discover,
to automatically discover weak points of models
and then close those gaps.
So I think we've, over the past year,
gotten very good at making models more efficient.
And we've created new methods that let us
sort of just like plug in compute and data
and have the model continuously improve.
You've used words like smart and capabilities.
And if you think of smart as knowledge,
I completely agree with you.
I think knowledge is a living thing.
We're all improvising and we are generating new knowledge
all the time and it just increases exponentially.
And there's no reason why language models
can't become more and more and more knowledgeable
because we just acquire more and more data.
And in that sense, a medical doctor is smarter than me
in that domain because they have the knowledge
that I don't have.
But some people could say that intelligence
is something a little bit more abstract than that.
It might be the ability to build models.
It might be the ability to reason.
It might be the ability to plan.
And this is when we get into the kind of the HGO thing.
So how do you demarcate those things?
Are you saying that the models are becoming more knowledgeable
but they're not necessarily becoming more intelligent
like we are?
I think reasoning is crucial to intelligence.
I think that these models can reason
and that's a controversial claim.
I think a lot of people would debate that.
A lot of people would make the claim
that the architectures we're using
or the methods we're using don't support
that sort of behavior.
I think that previous generations of the model
have been weak reasoners, but they do reason.
And it's not a discreet,
does it have this capability or not?
It's a continuum of how robust the reasoning engine
inside these models is.
We're getting much better methods
for improving reasoning generally.
We're getting much better methods
of eliciting that behavior from the models
and teaching them how to do it
and apply it to many different domains,
whether it's math, whether it's decision-making,
breaking down tasks, planning how to execute them.
Those were key missing capabilities
that were quite weak in previous generations of models
which are now starting to emerge
in a significantly more robust fashion.
And so in the same way that hallucination
was it used to be an existential threat to this technology,
no, we'll never be able to trust this stuff.
There are hundreds of millions of people
using this techno and they trust it.
It's actually useful for them.
They use it because it's useful to their job.
We're making very good progress on the hallucination problem.
I think we'll make very good progress
this year and next on reasoning.
I think it's just a capability, a skill
that the model needs to be taught.
And we're building the methods and data
and techniques to support teaching, teaching these models.
Yeah, it is interesting how you can kind of
break intelligence down to all of these things
and some you might argue are missing now,
like planning, creativity is an interesting one.
Agency is quite an interesting one.
And presumably as a thing has more understanding
and it has more autonomy,
it could in principle develop agency
at some point in the future.
But you think of these things as skills.
Could you give any hints to how
you've moved the needle on this?
So the knowledge thing, it seems to me
that you would just get more data
and curate and refine the data.
Could you give any hints on how you've made it
better at reasoning, for example?
Yeah, with knowledge,
I think it's about augmentation with RAG
and better modeling techniques, cleaner data sets
so that you remember the right stuff
and don't retain the less relevant stuff.
Those are the techniques that move the needle there.
With reasoning, there are like circuits
that you really need to bake in to the model.
You need to show it and demonstrate it,
how to break down tasks at a very low level,
think through them.
And that's stuff that's not actually
that abundant on the internet.
So it doesn't come for free using our previous techniques
of just scrape the web and train the model and scale up.
People don't usually write out their inner monologue, right?
They usually write the results of that inner monologue.
And so it's something that the model
has been missing a view into.
I think synthetic data will go a long way
in closing that gap and supporting building
multi-trillion token data sets
that actually demonstrate how to have an inner monologue,
how to reason through things,
how to think through problems, make mistakes,
identify mistakes, correct them and retry.
That sort of long thought process data
is actually extremely scarce.
It's very rare.
It's very rare.
It's really hard to find.
You can find it on the internet, of course.
Stuff like forums where people help each other
with homework and sort of break down.
This is how I arrived at this answer.
But when you look at the internet in totality,
those are like pinpricks on the surface of this thing.
And so pulling that forward, emphasizing it,
augmenting it, producing more of that data
should be a key priority if you're gonna actually
teach these models to exhibit that behavior.
Is there a trade-off between,
I mean, for example, we could use the Unreal Engine
to generate lots of visual training data
for a Vision Foundation model.
Or an alternative would be we could have
like some kind of hybrid prediction architecture
where we somehow encode naive physics
into the architecture itself,
which means rather than memorizing lots of generated data,
we just kind of build a hybrid architecture.
Is that a trade-off that you're kind of thinking about?
Like specifically with the video side of things
where physics is relevant, I think that's
a totally fine strategy.
I think that, yeah, a lot of the physics engines
that people have built are, they're flawed, right?
Like video games still don't look like reality.
They still don't behave like reality.
And so training off of that data,
I think will leave you in a really unsatisfying place.
Like there's just still some Uncanny Valley weirdness to it.
I think like we have tons of actual video data
of the real world where physics is definitely implemented
and being represented completely accurately.
And so that should be our go-to source.
I think trying to use simulators at this stage
is the wrong approach.
I think you should take as much data from the real world
as you can and use that as a bootstrap
to then build synthetic data engines
that help you iteratively improve.
It's what happened in language as well, right?
Like we didn't go to synthetic language,
rules-based synthetic language generators
to teach our language models,
the basic principles of language
using our linguistic models that we've built.
No, we threw all that away.
We took actual language data from humans, trained on that,
and then used the models that were the output of that
to improve iteratively and via experimentation.
I think the same will be true in vision.
That's a really interesting point, actually.
Because with the SORA model from OpenAI,
it does look a bit weird.
It looks like it's always flying
and it looks very game-engine-like.
And the language example is beautiful.
But what about something like mathematics?
Are there examples where rather than kind of, you know,
perturbing or mutating what already exists,
you might just start from first principles and rules?
Totally, yeah.
Like mathematics is so explicitly rule-driven
and so explicitly verifiable.
It's like the perfect example of synthetic data generation.
I think it's definitely one of the domains
that will crack first.
And on top of that, code, right?
You can completely, synthetically generate code, verify it.
Does it run?
Does it produce the outputs that you want on a test set?
So when it is that explicit and verifiable,
it's perfect for synthetic data, just ideal.
Yeah, but I guess this is kind of what I'm thinking about.
That with code, you can actually constrain it way more
than language.
So you could just, rather than using
an existing self-attention transformer, you know,
you might want to have something that only works
on trees or whatever.
And maybe that would work better for that particular thing.
But then I guess we'd have to have some kind of mixture
of experts and not have a single model.
Yeah, I think that's behind the scenes actually,
a lot of the strategy.
You'll likely have an MOE where one component,
one of those experts is gonna be an expert in code,
very highly specialized to that.
Heavily upsampled on synthetic code data
and real code data, math, et cetera.
And that expert will act as a general reasoning engine
and will be very good at logic
and those sorts of components.
You might have a medical expert
which has dramatic upsampling along that axis.
Yeah, I think that's a very effective path towards
even more efficient models.
So if you're in the medical domain
or the math or code domain,
you can then pull out that expert and use it independently.
You don't need to keep around this huge monolithic model.
You can just take out a sub-component and deploy that.
Yeah, I think that architecture already exists.
Yeah, I wonder if you can talk to that a little bit
because I'm very excited about that
because it now seems that maybe we could call
what you just described an agentic distributed AI system
where the agents can pass messages to each other
and one of them might be an expert in mathematics,
one of them might be an expert in coding or so on.
But then you've got this problem
that you kind of send a message into the nexus
and all of the models are passing messages to each other
and it's kind of unbounded in runtime
as opposed to one of the great things
with the language model is
it just does a fixed amount of compute per iteration.
You just put some prompt in
and you get the answer straight back out.
So does that kind of unboundedness introduce problems?
I mean, a language model could just generate infinitely
and not produce a stop token
and you would go on forever.
So I think the problem already exists
and models are quite well-behaved in terms of,
if you train them to give up
and to say, I need to respond, they will.
They tend to.
So I'm not too concerned about like,
runaway processes that would just not be useful
as well, hugely computationally expensive.
And yeah, it seems like models can produce stop tokens.
And I think that even in a multi-agent scenario,
discourse between agents will conclude itself
in a reasonable amount of time.
Yeah, interesting.
And I think even now with your multi-step tool use,
that's basically what you've done.
You could in principle do that recursively
and you could constrain the computation graph
so that there's no cycles
and it comes back in a fixed amount of time.
Yeah, we terminate execution
after some number of failed attempts.
So it's easy to solve that way.
It's a little bit unsatisfying.
I think our multi-hop tool use right now,
it's our very first pass.
It's like the negative one.
And so it's not that good at catching
when it's made mistakes.
It's not that good at correcting its mistakes,
even if it's caught, that it fucked up.
And so I think we're still very early there,
but those systems are gonna start to become
extremely robust and reliable.
And I'm very excited for that.
Amazing.
So I'm interested to know from, in your own words,
how, I mean, we're just talking to you folks
have got this forward engineering team,
your enterprise focused,
you're helping bridge the last mile problem
and really embedding yourselves into large enterprise,
which is an amazing differentiator.
But other than that, there's always the question,
lots of people say these models
are just kind of interchangeable
and you're just kind of playing the token game
at some point.
And I just wondered like, what's your plan there?
I agree with that sentiment.
Models are way too similar.
I think there's going to start to be differentiation
between models.
Like I was talking about before with command R
and R plus, we're going to start really focusing in
on key capabilities.
The general language model game is,
you know, there's a lot of players
and it's pretty saturated.
I think people are gonna start to have to branch out.
I think that consumer language models
are going to separate away from enterprise language models
within enterprise.
There's gonna be a lot of specialization
into specific domains.
And so for Coheir, what I want to see us do
is in product space,
push into more tailored capabilities
for particular problems.
We want to drive value for enterprise
and different enterprises operate in different spaces
and they have different needs.
The tools that their models might need to use
look very different from one another.
And we want to make sure that we're serving
each of those niches particularly well
or uniquely well.
And that will be our value proposition
differentiated from others.
So that notion of specialization
or enhanced capability in particular domains
is something that we definitely want to explore
at the product level and start to offer.
Because like you say, the dollar per token space,
it's super, we're not gonna stop that.
It's important for the community, right?
Like folks need to build on top of this.
They need access to good models at fair prices.
And so we're gonna continue to give that to the world.
But we want to create differentiated value.
And I think that's gonna come from focusing on
the actual problems that enterprises want to tackle
and getting extremely, extremely good at them.
Interesting.
On that, are you planning kind of horizontal products
or vertical products?
And the reason I say horizontal is I know a few startups
now that are building kind of low code,
app dev platforms with large language models.
And they're making it incredibly easy in the enterprise
to compose together different models
and to deploy applications on phones.
And it's really democratized because it's so much easier
now for people to do artificial intelligence.
That would be a good example of like a horizontal one, I guess.
So I think our product right now is super horizontal, right?
It's like general language models, embedding models,
re-rank models, it's a platform that's deployable
privately on every single cloud.
You can deploy the model against any sort of data,
whether it's medical, finance, legal, it doesn't matter.
It's the most horizontal product and platform you can build.
What we're gonna start to do is more towards verticalization.
And so specializing models at particular problems
or objectives that exist in the world.
And offering a product that solves that for the enterprise.
Would you ever go beyond the model
and kind of plug a little bit deeper
into the platform in the enterprise?
So for example, building operating models
or one approach would be to just fine-tune the model
on lots of data from a particular domain,
but it's still a language model.
The interface with Cohere is the same.
Or another one would be to let's say build something
a little bit like Databricks or Snowflake or something,
like an enterprise-wide suite that allows you to deploy,
discover, create, share artificial intelligence
in an enterprise.
The only reason I say that is as you're an AWS,
they give you free credits.
They want you to get on their platform
because they know you're never leaving.
Because you've got something there
which is not easily replaceable.
People learn how to use it, they love it.
Would that be a potential future?
Yeah, it's definitely still going to be a platform,
customizable, something that the user,
which for us as an enterprise,
can adopt and sort of bring into their environment,
hook in their data, their tools,
their whatever they want to plug in.
The verticalization is going to come from investing
in the model to be good within a particular domain.
That might mean fine-tuning on data within that domain.
That might mean making sure the model is very good
at using the tools that employees operating
in that domain would use.
But that's our focus.
It's starting to get more specific
and focused on the actual use cases
that enterprises care about,
and not just doing version 345
of the same general model.
So I saw you tweeted about Nick Bostrom's
Future of Humanity Institute shutting down.
Do you have any thoughts on that?
Yeah, I think it sucks to see
any sort of academic institute collapse.
To be honest, I know nothing more than what's public there.
So I don't know if there were some internal issues
that caused the philosophy department to pull funding.
But I've been a pretty vocal critic of ex-risk
and the idea that language models
are going to take over the world and kill everyone.
But despite that, I still want people thinking about that.
I still want academics thinking about that.
I don't think that regulators and policy folks
should be thinking about it yet
because it's so far away and remote
and potentially completely irrelevant.
But that's the domain of academia,
is to pursue those long-horizon, high-risk projects
and make progress on them.
And so I certainly don't want to see
the academic front of that effort get defunded.
That being said, I think that those organizations
have really been trying to get their hands into policy
and impact private sector, public sector
in a way that is threatening to progress,
misleading and so I think that we're starting to have
within our community, like the AI machine learning community,
a bit of a correction.
Those people were kind of given a lot of power,
were listened to a lot
and developed what I think we all
recognizes too much influence.
And it started to produce bills and talks about policy
that would totally collapse progress in the space.
Very, very prematurely about theoretical long-term risks
that might be an issue.
And so fortunately,
I think there's a cultural correction happening
where even the legislators and policy makers
are starting to say, this is not appropriate,
the level of influence that this one group is having
and we should listen to a much more broad
and diverse set of opinions.
So I'm still concerned about that
and I'll continue to speak out against that when I see it.
But at the academic level,
I don't want to see professors lose their funding.
I think that they should continue to pursue those ideas.
Yeah, I think Bostrom blogged that.
He was trying to resist the entropic forces
of the philosophy department for several years
and eventually he lost.
But I'm in two minds as well.
So as you know, I've hosted many debates with Connolly,
for example, and Beth Jezos and a bunch of different people.
And one thing that strikes me is how ideological it is.
I really thought as a podcaster,
I could have an honest and open conversation
and it's never gone well.
And I've put a lot of thought
into trying to understand why that is.
And I think philosophically you can trace it back
to things like paternalism and safetyism
and utilitarianism and consequentialism and longtermism.
And these are ideologies that make one believe
that even though it's just a subjective probability
that I know better than you,
I can predict the future better than you.
And they've become much more pragmatic in recent years.
So rather than talking about the old school
Bostrom superintelligence,
they're now talking about memetic risks
and bio risks and things that I think are designed
to get more people on board with it.
And I agree with you that they've had a lot of influence,
but why is it so difficult to have a rational conversation?
Yeah, no, I think it's what you say, it's very ideological.
There are camps and positions
and it's for some reason,
it's become very cult-like on both sides.
Obviously there's the EA movement,
which formed a cult-like environment of adherence
to those principles and their recommended behaviors
and actions, what you should work on in your life.
They have dating apps for EA, it's very insular.
And then there was an ironic, I think,
although it's increasingly not clear,
an ironic counter movement, which was EAAC.
And that has spun out into something
that is very not ironic, it's very libertarian,
accelerationist, which are ideals that I don't hold either.
And so both of these camps I find really unappealing.
I don't wanna be associated with either of them.
Yeah, I found EA was very dominant for a long time.
And so when EAAC came out, it was like refreshing.
Finally, someone's calling them on their bullshit.
But at this point, it's just mind-numbing
and completely not of interest to me.
Yeah, we've had Beth on the show.
He's a really nice guy, actually.
I invested in Guillaume's company.
Oh, did he?
Yeah, yeah, yeah.
He's brilliant, he's a really, really nice person.
I'm proud to see Canadians doing great things.
The Beth thing I found super funny.
And I think EAAC was necessary.
I now believe that both EAAC and EAAC need to be dissolved.
We've seen them through to their logical conclusion
and now we're starting to get into territory
that's very strange.
From a philosophical perspective,
how do you kind of see the role of AI in society?
And I'm quite interested in how it's affecting our reality,
how we interface with technology
is really dramatically changing over time.
I mean, what do you think about that?
Yeah, completely true.
I think I view it in the same way I view the computer
or the CPU.
It's a tool, it's something that we're gonna leverage,
that we're going to build on top of
and use to make our lives better,
to make us more productive,
to make things cheaper, more accessible.
I think all the good that came from the computer
and the internet is gonna be dwarfed by this,
the democratization of intelligence
and having that always at your disposal at any time.
That's something that 50 years ago,
you couldn't even dream of it, right?
It's surreal, the amount of progress
that's been made in half a century.
And so I'm really excited for that.
I think it will do a lot of good and alleviate a lot of ills.
I think the human experience, our lives
will be dramatically improved
by having access to much more intelligence in our lives.
I'm really excited as well,
but are there particular things that you are concerned about?
I mean, for example, people say that language models
might enfee us, they might lead to mass manipulation
and persuasion.
I mean, Jan Lacoon tweeted the other day, he said,
where's the mass manipulation?
Where's the persuasion?
There might be something that just happens gradually
over time, but are there things that you do worry about?
Of course, yeah, of course.
It's a general technology,
and so it can be used in a lot of different ways,
many of which are, I think,
abhorrent and ones that we should avoid
and make very difficult to do.
I'm much more of an optimist than I am a pessimist,
but on the side of things that are risky,
I think that misinformation is high up on the list.
I think that we're already seeing
social media platforms start to build in the mitigations.
I think things like human verification
are gonna become crucial.
If I'm reading a poster talking about
whatever, Canadian elections or politicians,
I wanna know that that's a voting Canadian citizen.
I wanna know, because I want to know
what my compatriots think, right?
Even if they're on the opposite side of the fence to me,
like that's fine.
I wanna hear what they think,
but I don't wanna hear what some foreign adversary
has spun up a bot to push into the discourse.
And so human verification, I think, is crucial.
That's the one that's top of mind for me.
I think some of the more remote risks,
like bio weapons and the sort of stuff
I'm less concerned about.
In feeblement and becoming dependent on the technology,
I think folks said that about calculators
and we wouldn't learn how to do basic math.
Humans are intrinsically curious.
We want to know things and we can't ask
the right questions of machines without knowing things.
And so we'll continue to be really well educated,
better educated, more knowledgeable than we were before
without that technology.
Yeah, cause I mean, if you look at the
in feeblement pie chart, a calculator is a very small part
and a general AI is quite a large part,
which is a little bit concerning, I guess,
but I agree with you that maybe the jobs one has spoken
about a lot.
I've not seen a lot of evidence of that yet,
but it's so pernicious.
It might happen slowly over time.
Daniel Dennett wrote an interesting article
and rest in peace, by the way.
Daniel Dennett called Counterfeit People,
which he published in the Atlantic,
and he was kind of saying that when we have all of these
bots and generative video models and so on,
at some point they'll become indistinguishable from reality
and that will lead to a kind of acquiescence
where we don't trust anything we see.
And I think that's quite interesting
and I also think that these models might kind of affect
our agency in quite a weird way,
but it's so difficult to understand now
how that's going to affect society.
Yeah, I think even now people have been taught
to be extremely skeptical of what they read and see.
There's a very strong prior inside of us
for any media that we consume that it's been skewed
or manipulated or produced to propagate an idea.
And I think it's good to have a skeptical populace.
I think it's good to be skeptical about what you read
regardless of the medium.
And I think people will do what they've always done,
which is filter towards sources
they find trustworthy and objective.
That'll happen even with ML in the loop,
disinformation and misinformation campaigns,
manipulation campaigns, they existed well before
models existed.
And so it's not like a novel concept
and it's always been a risk.
And the question is how much more prevalent
does the technology make that risk?
I'm optimistic that we're quite robust
and that we'll find ways to make it very hard
for bad actors to exploit the technology.
My rough take is that the more agency the AI has,
the more of a risk it is.
Because if it is just doing supervised things,
then at every step of the process,
it's being aligned and constrained and steered by humans.
If we ever did create a gentile AI,
then there's this kind of weird divergence
and all sorts of scary things might happen.
But I wanted to talk a little bit about
policy and regulation.
So you spoke to that earlier,
you said that potentially there are some
quite damaging policy changes being considered.
Could you speak to that?
Yeah, I've seen ideas floated.
I don't think any seriously damaging policy
has actually passed, fortunately.
But within what's being considered,
there are ideas that they will destroy innovation,
they will destroy startups.
And so you'll just entrench power
with the existing incumbents.
Some of those examples might be fines,
which if they're a $100 million fine,
that's gonna wipe out and stamp out a startup.
But for a large big tech company,
it's like 10 minutes of revenue.
It just doesn't matter.
It fundamentally is irrelevant.
And certainly a cost they're willing to take
to capture a market.
And so very disproportionate consequences
for the same punishment.
Over-regulation in that way,
that it's thoughtless,
will have the exact opposite effect
of what I think all of us in the public
and in government want.
We want competitive markets,
we don't want oligopolies.
And we're starting to see oligopolies emerge.
And so there needs to be fairly strong action
pushing against the entrenchment of those oligopolies.
And we need to preserve the ability to self-disrupt.
Because if you have an oligopoly
and you have entrenched incumbents,
the likelihood of self-disrupting
of the new winner emerging within your market,
being one of your players goes down.
And so you're gonna be disrupted from outside.
That's a huge risk.
And so you need competitive self-disrupting markets.
And it seems like some of the policy folks
are just acting non-strategically
and not considering that.
But fortunately, what has been passed seems sensible.
Can you comment in particular
on the EU, AI legislation and the Canadian?
I probably can't say anything too specific.
I think the Canadian legislation hasn't gone through yet.
The EU AI Act has,
but fortunately it was reigned quite far back
from its initial position.
I think all of those regulators were in conversation
with all of them when you talk to the folks,
they wanna do the right thing.
They're under a lot of pressure from different parties
with conflicting interests,
but they're trying to do the right thing.
They're trying to make sure this technology
gets out into the world in a safe way,
that there's oversight,
that we don't entrench the incumbents
and we ideally actually sort of bias towards disruption
and the creation of new value and innovation, new players.
So I think they all want that, but it's a tightrope.
It's a very difficult line to walk.
I think one of the issues is that
not a lot of people certainly in the government
understand how this technology works.
It seems like magic and many people,
I mean, even in the AI space,
I mean, Lacune and Hinton, for example,
people have very different opinions about it,
but I'm also interested in your views
on the kind of health of the startup scene.
So we're in a bit of a downturn at the moment.
It doesn't seem to have affected the LLM space,
but even in the LLM space, I've noticed a trend
that many people started kind of wrapper companies
where they did an LLM, but it didn't really do anything
that couldn't easily be replicated.
And what are your thoughts there?
Do you think we're gonna see a trend towards startups
doing something that is very differentiated?
Yeah, I mean, I think we're in a moment of churn.
So I think there are gonna be some players
who started a while ago who fold or go into other companies,
get acquired, that type of thing,
but there's a whole new generation emerging.
I know a bunch of people starting up, Ivan and I,
we invest in startups and we're seeing an uptick
in the number of AI startups that are coming out.
It's sort of like a reformatting.
There was a bunch of folks building at one layer,
like the LLM layer or the one layer above that,
tooling, et cetera.
The players have kind of been set in that space, it seems.
Of course, I'd be happy to see new players emerge,
but we now need a set of ideas and products
and companies building up the stack.
So more abstract concepts, stuff like end user products
and agent companies.
They're all starting to pop up
and create really interesting new ideas.
And then that will sort of settle
and we'll have our players at that layer.
So it's a continuous cycle.
Yeah, I'm really excited about the AI startup space.
It feels like we're finally starting to get our feet
on the ground a little bit.
I mean, for example, with the tool use, with the RAG,
it's starting to look a lot more
like traditional software engineering.
So what we're seeing now is people kind of rolling up
their sleeves and actually building out
these very sophisticated software architectures
that compose LLMs in interesting ways.
And they're not just kind of,
just building a simple LLM with a prompt on the top.
Yeah, it's definitely getting more sophisticated.
And as the tools get more robust and reliable,
it's unlocking totally new applications.
And the utility is starting to be seen
and felt in the real world.
I think last year was very much like the year
the world woke up to the technology.
And got their footing with it.
So it got familiarity.
This year is when things are gonna start hitting production.
They're gonna actually start to hit our hands.
And we're gonna be able to use this as part of our,
part of our work, part of our, you know,
play the products that we use.
It's gonna become a much more fundamental part
of our daily life.
So it's very gratifying.
Like we've been building Cohere for four and a half years now.
We're in our fifth year.
And I think for a long time,
we were out there sort of preaching,
this is really cool, please care about this.
You know, like this is gonna be an important thing.
And folks would pat us on the back
and say, nice science project, you know.
But finally, we're actually starting to see
the fruits of all that, all that labor.
And so it's really gratifying to see real world impact.
And I think that's what we exist for
is really trying to accelerate that
and make more of it happen faster
and in the best way possible.
And what was your biggest mistake?
I mean, do you have any advice for other startup founders?
You know, what did you do that perhaps they should avoid?
I fucked up constantly at every stage of the company.
I think, I guess just like admitting that you've messed up
and trying not to be in denial about it
and fixing it as quickly as possible
has been the most important thing to Cohere continuing
to thrive and exist.
But yeah, this is the first company I started.
Same for Nick and Ivan.
And so the whole founding team, we were fresh into it
and we made potentially every mistake
you could possibly make.
Fortunately, we were good at listening to others
who had done it before and seen a lot more than we'd seen.
And so I'm sure we've dodged some mistakes,
but it feels like we've made them all.
Yeah, we've made mistakes you're not learning, I guess.
But just final question.
How do you, you know,
because it's such a large organization now
and there's this problem of vertical information flow.
So there might be a problem
that some of your folks have discovered now
and it takes a while to filter through to you.
But obviously it needs to be scalable.
So you need to delegate.
How do you deal with that?
Yeah, I'm very close to like the ICs.
I'm very close.
I'm not someone who works through their reports
or follows the chain of command.
I just talk to the people who are actually doing the work.
And so information flows quite freely.
I'm sometimes, I think I'm mostly annoying people
at this point because I'm like peeing them every day.
How's that run going?
How have we tried this experiment?
I'm very deeply involved in stuff.
So we haven't had too much.
I don't feel like there's an information flow issue echo here
with scaling collaboration between teams,
especially when you're a global company
and you're not sitting in the same office as a person.
That's very difficult.
I think remote work is really, really hard.
It's not easy.
It's not easy.
And I think that concentrating teams to geographical areas
or at least time zones is very important
and leads to a lot more productivity and effectiveness.
It's part of the reason I moved here to London
is to be closer to a good chunk
of our machine learning team.
Phil's here, Patrick is here.
Like I want to be present and involved in the ML
component of the company as much as possible.
Yeah, I think as we've scaled,
there's been systems that we've used
that have broken down at each phase,
stuff that worked for the first 10 of us,
didn't work for the next 20,
didn't work for the next 100.
Now we're pushing 350, I think.
And there are people at the company who I don't know,
which is insane.
A super weird experience.
But we've hired really fantastic people
and we continue to do so.
And I think you just trust that people will still
make the right decisions going forward
and that you've set standards high enough
that you don't need to approve every single hire,
you don't need to know what every single person
is working on and you just trust your colleagues.
Yeah, I can attest that you hire very well.
It's probably the best culture I've ever seen
in any company actually.
Thank you so much.
Final question, I mean, just out of interest,
do you get like microcosms in the different offices?
I mean, do you see like different mini cultures?
Oh yeah, totally, 100%.
Like the London office compared to the Toronto office
compared to SF, New York.
The vibes are very, very different.
Like so different.
London is so nice.
It feels really tight-knit.
Like it still feels like a startup,
which we are a startup,
but it still feels like a tiny 30-person startup
where you go out for beers with your colleagues
after work at the pub like regularly.
Everyone knows what everyone else is working on
inside the office, calls on each other for help.
London is super tight-knit.
I think the culture here is like,
I can't pick favorites,
but I really like the culture here.
In Toronto, it's our biggest office.
And so it's much broader,
but the culture there is amazing too.
Super hardworking, stay late and like grind very passionate.
There's like different groups
that are close with each other there.
New York is new, but that city is just so much fun.
It's just like such an incredible city,
so much energy, always awake,
work hard, play hard.
SF, I spend the least time in.
I'm not a huge SF fan, to be completely honest.
It's our second HQ,
but I just haven't gotten into the city, I think,
in the way that others have.
I think SF compared to like New York, Toronto, London,
I feel like New York, Toronto and London are real cities.
There are artists, there are, you know,
people just doing a very diverse set of things.
And across all the different fields that are going on there,
you have some of the best people in the world.
SF is much more, it feels more homogenous to me.
It's a lot of people doing the same stuff.
There's sort of one topic of conversation.
There's, you don't bump into someone
with a categorically different worldview than you
or perspective or experience.
And so I like visiting,
because I meet like brilliant people in our field, in tech.
But to live there would be really difficult for me.
I would feel like I'm sacrificing whole pieces of my life.
But I love visiting, it's a great place.
And I love the folks who are there
and most of our investors are there.
And so it's a really cool environment,
very intense and like competitive.
And those are really good things.
It's very motivating to be there.
But I think I can get that just by visiting.
I don't need to commit myself full time.
Aidan Gomez, it's been an honor and a pleasure.
Thank you so much for joining us.
Thank you so much for having me.

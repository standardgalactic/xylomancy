But people should have the freedom of choice for sure.
Sure, sure, whatever, but libertarians are like house cats, fully dependent upon a system
they neither understand nor appreciate.
100% agree with that by the way, like me and you handshake on that, fully agree on that.
Good.
I did not expect you to have this view, so thank you for elaborating on this.
I don't think you're crazy, I don't think the EAC, some of your followers are crazy.
True.
But like a lot of the people that follow the kinds of beliefs you talk about,
I don't think you're crazy.
Yeah, we just think that currently the current discussion with AI regulation is led
by the current day oligopoly and they're the ones writing the laws.
And so we're deeply skeptical of everything being written right now,
but we're not in principle against regulations if they help acceleration.
In today's presentation, Beth Jezos, also known as Guillaume Verdun,
started the EAC movement or the Effective Acceleration Movement.
It borrows heavily from the idea of accelerationism,
which means that we should hasten the advance of technology and capitalism
at the cost of everything else.
The main, you know, theory of physics that is most relevant is thermodynamics.
That's what Nick Land did, you know?
Like Nick Land allegedly used to be a Marxist,
and he was taking doing Marxist capital analysis of like what happens
if techno capital gains more and more power.
And his conclusion was eventually there was only capital.
There's no labor, there's only capital, there's no people, there's no happiness,
there's only competition, there is only capital.
Capital itself becomes sentient.
And like this is before like AI was a big topic,
it's like the 90s when he wrote this kind of stuff, right?
And he's the only goddamn person and all of accelerationists
who actually bites the bullet and actually goes all the way.
If you cap compute at a certain threshold for LLMs,
that affects let's say drug discovery research,
where we might need way more compute than that to do our research, right?
Or materials research, and that has a negative effect
because it's still AI and above that compute threshold, right?
And so what I'm arguing for is let's be very careful
as to like what legislation we crystallize,
and let's have it be as light touch as possible.
I think there will be some regulation, I'm realistic.
But I do think that we have to be mindful of like
what we're going to affect in terms of potential positive effects
of enabling high compute AI research.
Like we don't want to shoot ourselves in the foot
with any sort of regulation.
I don't think what's been proposed so far has been good.
Like bro, you create a fictional character
and now you take on his beliefs rather than your own on Twitter.
Like you're letting the Twitter algorithm pick what thoughts you think.
Like you can do better than this.
You need to improve your memetic hygiene.
Yes, at best you want nuanced discussion.
You don't want the shit that Beth is doing on Twitter.
Like if you had come into here, like George but worse,
then I would be like, yeah, your Beth go on.
But you're not Beth.
You're not.
You don't want Beth.
So who is this Beth guy?
Well, he was recently on the Lex podcast.
He's got a PhD in physics.
In fact, he used to be a quantum computing engineer over at Google.
And a man after my own heart, in this respect, at least,
he's a big fan of thermodynamic intelligence
or the physics of intelligence.
And fans of the show will know that that's right up our street.
So certainly in that respect, I am a fan of Beth.
Enforces this equilibrium in a top-down fashion.
And that, I think, is a bad trade.
And I wouldn't, I don't want to take it, right?
And that's where we, yeah.
Congratulations as Lord of the doomer on our society.
I so upon you the rank of doomer.
You are now become a doomer.
You are a doomer.
You are a doomer because you believe that both,
that we are basically fucked.
We agree that a monopoly then inducing or achieving
regulatory capture is bad.
And that's also what we're fighting in the present moment.
We feel like it's happening in that the AI safety discussion
is being leveraged instrumentally for this regulatory capture.
And I don't even disagree with you.
Also joining us today is the world's second most famous doomer, Connollyi.
Conno founded the Aleifer AI group and also co-founded Conjecture.
And I was over at Conjecture in their offices in London.
By the way, we filmed about 70 minutes of pre-interviews
and smack talk, which is on our Patreon.
And we also filmed about 30 minutes or so post debates with Beth and Conno.
So check out those videos on our Patreon.
We can do such incredible things.
And yet people have to fear for their lives, their stability, their family.
They have to watch their parents grow old.
The constant struggle of politics, of corruption, of all these things.
And we can do better.
So you hear it here first guys.
EAC is not about maximizing entropy from the mouth of the man himself.
It's about maximizing free energy dissipation over time,
which is what out of equilibrium thermodynamics.
Optimizing we're prone to being manipulated, right?
And like this is just cope.
What you're describing is that reality is hard.
We dropped a nuke on South Carolina accidentally.
It's still there.
It's in the swamp.
It's still there.
And it was pure luck that it was actually armed.
The nuke was armed.
And we got so lucky.
Then what?
I mean, a city would have been blown up.
The world would not have ended actually.
Yes, sure.
But like what happens if this is the rate.
Every decade or two we drop, you know, one nuke on South Carolina.
And our nukes get better and better and better.
Eventually the nukes are synthetic bio weapons.
You know, AGI, ASI, these kinds of things.
What happens?
So I mean, even bio weapons, there was a leak of a bio weapon.
We didn't all die.
We probably all got COVID once or twice.
Again, yes, sure.
The world has never ended in the past.
This is true until it's not.
Like this isn't an argument.
What do you mean?
What do you think we should do?
Do.
What should we do?
Yes.
Um, well, I try to enact my values, right?
I mean, I work.
What are your values?
I'm trying to figure out what your values are.
It's what we're trying to find out.
You keep talking about.
I'm trying to scale civilization, right?
Like so personally, my life's mission.
Again, I'm asking you for the whole world, you know,
whether or not the Chinese already have it or not,
would releasing all of our secret, you know,
engineering documents of all.
NFC 16 is a force for destruction.
It's not a force for positive economic utility.
You're in your drawing these comparisons all day.
I'm not trying to make a comparison.
This is not a metaphor.
You are.
But you're just lost in analogies and analogies and analogies.
Let's talk about AI directly, right?
You're just trying to answer the question.
So if you follow in the will of God,
he shall reward the faithful.
That's your ideology.
I mean, physics is my God to some extent.
I'm not trying to actually replace humans.
I'm working on like physics based intelligence,
like trying to understand chemistry, materials,
you know, fusion, nuclear fusion, carbon capture.
There you go. Respect me.
Like respect.
I mean, you're wrong and you're going to make everything worse,
but respect when you try it.
Like, you know, you will say the same thing about me,
obviously, and like, you know.
I think we both agree, though,
that we want people to have more agency and step up.
Like there are no adults in the room coming to save us, right?
I think for me, that realization in my mid-20s
was when I was the adult in the room.
Like, this is the thing you need to understand.
A lot of people are actually evil.
And if you make an ideology that justifies evil,
they will use it.
You can make ideologies that do not justify evil
or justify it less.
Like, I don't know how to say this in a polite way,
but Beth is evil.
Like, Gim is an evil.
Beth is evil.
Like, Beth is an evil character,
and I think he wrote him intentionally to be evil.
Yeah, so we're pleased to have the Society Library
map out the debate today.
Create a chart of all the arguments that are made.
It's a great nonprofit made to chart out arguments in full detail.
And so there's going to be a recap.
Be able to access the link in the description.
Okay, well, in which case, we should kick off the debate.
So this is the amazing moment that we've all been waiting for.
And if you don't mind, Beth,
I'm going to ask Connor just to kick off with his opening statement.
Yeah, I mean, the first thing I want to say is
really glad to talk to you, man.
It's not through the anonymous veil
where things can get a bit feisty at times.
So big respect.
Really glad to talk to you and so on.
We disagree on some things, but let's talk them out.
Let's see where things go.
And if we end up disagreeing, that's okay too.
So I just really appreciate it.
I just really want to get that out.
I just want to add that said,
I think this is going to be really fun.
Like, as much as I just grew with you and many things,
I do like a lot of your style.
I like a lot of your aesthetic.
A lot of good humor and so on.
So really appreciate that.
And I think it'll be really good.
But I guess the thing that would for me
be the most interesting, at least,
that I'm most interested in to start with, kind of,
is I'm not too interested in going too deep
into talking about abstract stuff right away or so on.
And one of the things that's something difficult with me
when talking with you in the past,
or reading your content in the past,
has been that I feel like there's sometimes
a bit of a bait and switch,
where sometimes, you know,
you or someone in your movement would say something
which I would find rather crazy or extreme.
And then when I tried to engage with it,
it said, oh no, no, it's joking.
It's not literally like we're just,
it's just a metaphor, you know, or it's just a vibe.
So I'd be interested if we could maybe start off
this discussion.
So I'm coming from, you know,
more of a safetyist perspective, more like,
you know, technology is both good and bad.
It's neutral.
It can go well.
It can go bad.
Depending on how we deal with it.
And so from this perspective,
I'm really interested to kind of hear from you.
Like, do you think there is,
can you imagine any technology
that you think should be banned?
Any technology to be banned,
like in its entirety,
or like certain stuff?
In its entirety, like just entirety.
Like where you think like the world would be better
if we like made sure no one had this technology.
Has any technology ever been successfully banned?
Ever.
That's a different question.
Not saying whether it's enforceable.
I'm asking whether like if it could be,
like do you, would you find this to...
I just don't think it's enforceable.
Right?
Like you...
That's fair.
But if you could enforce it,
do you, would you think that there would be such things?
We use all these abstract theories in EAC.
Really, I'm just a, you know,
high-dimensional optimization scientist.
I worked in quantum machine learning,
now doing thermodynamic AI.
And that's sort of the technical lens,
I view everything.
And to me, finding technologies that have positive
utility to everyone is a sort of search process.
And if you, how, you don't know when like a technology
that could mostly yield negative reward,
for example, you know, nuclear efficient is, you know,
a few at a distance away from a technology
that could yield massive positive reward,
like nuclear energy or nuclear fusion.
And so that's sort of the general mindset of EAC is that,
you know, there's a lot of closing doors entirely.
There's a lot of potential upside.
We don't even know.
We might be leaving on the table.
And we got to be really mindful of that.
And it's supposed to be a sort of balancing force
to the natural human tendency to sort of fear the unknown.
Or if we have a sort of first order model of,
Hey, here's a potential negative effect
of a certain technology.
Maybe we should close that door, shut it down.
You know, going back to the nuclear analogy, right?
Like I do think they're sort of the mindset sort of yielded
at a net, pretty net negative outcome.
I think we'd be far better off if we had far less regulation.
And we had had far less year mongering towards nuclear
efficient, I think we'd be in a much more prosperous society.
And so I would say, you know, define, but define ban,
like who, who enforces it, right?
Is it a peer to peer thing between countries?
Or is it a top down?
And I'd love to like dive into like enforcement of rules.
I think to me, like, yes, some technologies can have negative impact.
But in general, the thesis is that the market and civilization
tends to sort of amplify or provide more resources towards developing
technologies that have net positive utility or positive effects
towards growth of either individual company, nation, or all of civilization.
And that that sort of probabilistic safety
or sort of natural selection and space of technologies on the short term
can yield sort of negative fluctuations.
But in long term, post selects for things
that have positive utility towards growth, right?
And it's a much more nuanced way than like directly legislating technologies
to steer the technological landscape manually in a certain direction.
From from at the end of the day, you have a model of how things would play out.
If you let that technology be be unregulated.
And I guess our core thesis is that this sort of adaptive algorithm
that is the market is a better heuristic, it's a better online adaptive search
than sort of top down model based control that would, you know,
include, you know, legislating from the top down.
And so, but yeah, happy to expand upon that.
She have she wanted me to dig in, but happy to dig into anything.
Yeah.
Yeah, thanks for the elaboration.
Good to hear from you precisely.
But I want to go in.
I would actually like to put a pin on the enforcement versus a second here.
And I think a lot of your models are sensible,
like thinking about how the market isn't much better
than a lot of top down control in many ways.
I definitely agree with that in many circumstances.
I don't want to dismiss this argument by any sense.
But I'd like to I feel like you still haven't answered my question.
Wasn't, you know, should all technology be banned?
Or should this, I'm asking, can you imagine there being a technology that should be banned?
Like, is it an acceptable thing in your ontology?
Or do you think this does not, like this cannot even exist?
Um, I think, I think there are technologies that we we can ban.
Like if there are pure net negative play and the legislation is very sharp
and pointed towards the usage of it,
let's say as a net negative impact towards the world.
Yes.
But like how, how is that enforced?
Right.
It depends on so much, right?
Absolutely.
And how is the law written?
Um, and I do think like overall,
like I see legislation as sort of, uh, hyper parameter, uh, settings for, you know,
certain, let's say countries or subsystems of the whole system.
And it's a sort of somewhat discrete architecture search process that we're doing.
But overall, the thesis is that, um,
you can have countries that will maybe over legislate some that will under legislate.
But eventually if we, if we had a nice and malleable legal system or, or, or, you know,
if governments were very dynamic, uh, with their legislation,
including sunsetting clauses and, and, and updating regulation in sensible fashion,
we would have a very healthy adaptive algorithm to converge on what are the,
what is the optimal legislation to have sort of sustainable, thought tolerant, uh, growth.
Um, but I don't think that's the system we have today.
And I guess what we're seeing is a sort of second law of bureaucratic complexity,
uh, these days, uh, where legislations are added and bureaucratic complexities added,
but not so much, uh, removed.
Uh, and so the bias rate as, as it stands right now,
how we do things in government is towards this system is so archaic,
just like let the market regulate itself, uh, and, uh, you know, keep your hands off.
And by market regulating itself, I mean, you know, if I,
if I'm a company and I deploy a technology that is net negative reward to my peers,
we have a sort of peer to peer formalization of sort of violence between corporations and
meto organisms, like lawsuits, right?
And it's, it's, it's basically, uh, uh, legal, lawfare, legal warfare.
Um, and, and you cause economic damage to the other organism if it induces negative reward
on, on the other ones, right?
And so, so that's kind of a, that is a sort of restorative force towards not doing terrible
things, right?
So if I understand correctly, you perceive lawsuits as part of the market because lawsuits
are, from my perspective, regulation.
Um, well, regulation sets the, the prior of how, uh, legal dispute will be resolved.
But there's a continuum to begin with, like, and also the enforcement.
Like the reason lawsuits have teeth is that if you violate what the court says,
they send police after you.
Yeah.
So, so, so the point, well, eventually, uh, right, like if you can't peer to peer enforce,
uh, a certain, certain behavior, right?
Which I think if you're in force, do you mean mercenaries?
Like, what would that mean?
Yeah.
Well, I don't know.
I, I'm just very, um, well, I'm very weary and we'll get into this.
I would really like to get into that component of sort of the monopoly on, uh, uh, sort of
creating like a physical power asymmetry, right?
Through violence monopoly in a top-down fashion and also having intellectual power asymmetry
through, through AI.
I'm very weary of having very strong top-down power asymmetry.
I understand that your prior is that by, uh, deferring, uh, power to a higher, uh, node
in the, in the hierarchy of control of society, like let's say a government or, or some sort
of leadership, you minimize sort of peer-to-peer sort of competition and friction by sort of
having that, that authority sort of ensure, uh, peace.
Um, but I do think one outcome, but yes, possible.
Yeah.
Yeah.
But I do think that we aren't in a weird, well, this is, this is a tangent for later,
but I think we're in a weird period now where there is the window of opportunity for there
to be, uh, sort of AI assisted tyranny to be installed.
And to me, that's one of the core existential risks to progress.
I don't know if it's fully existential, but it's definitely going to slow down progress
for a while.
If we have a sort of big brother style, uh, authoritarian penopticon, like I think in,
in the present days, there's a danger that you have a monopolization and a centralization of AI
that's the coalesces with government and there's a control of information flow.
There's a control of, uh, of the truth, what, or what we see as the truth.
And then that does break, uh, democracy, right?
Cause democracy is assumption that everyone is informed to some extent can make their own judgment.
They're supposedly uncorrelated variables, which of course in the era of algorithmic
amplification of information propagation, they're not, let's say they're uncorrelated variables
and you take a sort of, uh, average vote, then, then you have a good sort of decision mechanism.
I think with sort of algorithmic, uh, manipulation, especially with AI, I think we're going to have a
lot of trouble with that.
And so I'm also concerned that there will be attempts to, um,
feign, uh, sort of democratic vote towards, uh, AI safety and, and giving sort of, uh,
you know, a monopoly on this intellectual, artificial intellectual power, uh, to, to the
centralized, uh, organizations, um, you know, the, this sort of democratic vote will be manufactured
consent. And, and then once the power is centralized, those entities won't want to give it back,
uh, to the people and they'll have an ability to maintain that power gradient because
intelligence, uh, allows you to extract more utility or, or, or filter, uh, or engineer
information and information is power. So AI is power. And so there's going to be a sort of
gravitational effect of, of power and, and, and AI capabilities that, that tends to, to, to
centralize, uh, uh, you know, AI capabilities and power overall. And IAC is a sort of,
you know, bottom up sort of counter force to this natural tendency towards
sensualization and top down control. And so we're, we're, we're trying to push,
you know, the overton window and the discussions toward an extreme of,
of, of the hyper parameters regime, but frankly with knowledge that, you know, if, if, you know,
you or, or maybe being not yourself, you're fairly balanced, but like some people on the AI
safety camp are, are very concerned with, or have a models, models with, uh, with respect to
which they're concerned about the future. And they want to, you know, centralize the control,
maximize safety versus on the other end, we want to maximize freedom and lower the risk of sort of,
uh, AI assisted tyranny. Reality is going to fall somewhere in the middle in terms of policy.
And, and that's why, that's, that's why we got to have these discussions. Um, but, but for us,
you know, we, we just really want people to factor in that, um, historically giving a monopoly on
violence or giving a monopoly on, on power to centralized entities that eventually every, uh,
subsystem in our civilization serves its own interests, including government, uh, that can
yield really bad outcomes. And we have a better data driven prior of this happening than let's say
some sort of artificial superintelligence taking over which TBD, if that can exist and what that
looks like. Um, and so, um, to us, that's like the sort of existential risk that, um, we want to,
to, to minimize is kind of the erosion of our freedoms that seems progressive at first. And
then all of a sudden, you know, we're in a really bad spot and we're in a sort of, um, top down
enforced dark age where, you know, freedom of compute is non-existent freedom of access to AI
and freedom of access to information is no longer a thing. And so this tangent kind of evolved into
a whole, uh, discussion, but yeah, please feel free to like jump in whenever you feel like, uh,
you know, we could keep this organic. Yeah. Yeah. So, um, it's a lot of nice word celery,
but like from what I could tell, you made like five to nine points there that were
only tangentially related. And I could address them individually, but that's a bit difficult.
So I'd like to bring us kind of back to what started this whole tangent, which again, I feel
you still have an answer to my question. Should all tech, do you believe all technology, all technology
should be unregulated? Yes or no?
Why, I mean, there's no absolutes, right? Like, I think that means, no, you think some technology
really, that's no. Sure. Yeah. Sure. Cool. Okay. This is what I wanted to establish. This is the
question I wanted to ask because there would be a crux because if you had said yes, then that would
have been a crux for me and we would have to talk about that. But if you say some technology in some
circle, then we can have an object discussion. That's great. Now we can have an object level
discussion about what technology should be regulated in what circumstances, what enforcement
mechanisms, where do we disagree about how the future goes? Did that make sense? So this is kind
of how I would like to have this discussion because you obviously have a lot of, you know, ideas in
your head, you have a lot of good models that I'd be interested in, you know, hearing about as well,
maybe pushing back and some of mine as well. I think some of the things you say are definitely
true. Like, I definitely agree that nuclear regulation turned out to ultimately be mostly
negative to a large degree. Not entirely. I think nuclear weapons regulation is really great. I think
it should be even stricter personally. But I definitely agree with you. There's a lot of side
effects of this. So yeah, I think you made some good points there. But there's some I also disagree
with and I'd like to understand your position a bit more. So like, I'd like to, for just to
understand, again, I'm just trying to understand kind of like how you view the world and so on.
I have a question. What is your opinion on leaded gasoline?
Leaded gasoline. Yes. Yeah, I mean, I think that it's good that it was banned. I mean, it was
negative. And I think that it induced the sort of selective pressure on the space of technologies
that was for the better, right? Yeah, it was banned by the government, not by corporate to
corporate interests or anything like this. Well, I mean, at some point, if you have like,
if you have enough data that people are suing each other for the same cause, right, like you've
caused me brain damage by putting this chemical in your stuff, then you can you can kind of crystallize
that that sort of prior that like this this this thing tends to happen into legislation. I think
that if you're trying to draw an analogy with AI right now, I think the landscape is moving
so fast that we don't have enough data of how things go wrong. And we don't even know where
models and compute are going that it's far too early to settle things and set things in stone.
And again, I would be less aggressive on pushing back if there was a precedent of like sunsetting
regulations, which, you know, doesn't doesn't tend to happen in our current system.
And so for me, it's like, let's be very careful to like have these one way decisions,
where we close the door on a whole spectrum of certain technologies, let's say we have compute
caps and whatnot. Like we should be really, really thoughtful. And it's I'm just very skeptical
that our models of how this thing will evolve the progress in AI are necessarily accurate. Whereas
like with with the lead, I mean, there was many studies like, hey, like this is really bad for
humans. I mean, it took decades. This is many decades. It took well, it is ruined holds right,
it caused massive damage to whole generations of people. But I mean, how else you still like
before, even with top down policies, right, you can you can make decisions that take time to realize
the the end order effect. And then and then you have to readjust, right? So whether like,
so the point whether you're biased towards under overregist legislation, you can you can screw up
in many ways, right? And that's the that's the thing about, you know, top down regulation,
and you got to be very careful and thoughtful and make sure your model is accurate, right? And
and not only good, good. If we agree that you can screw up in both directions, I think we're on
the same page there. This is no question for me. I think a lot of the regulation that has happened,
and I like you bring up the point of sunsetting laws. This is definitely a huge problem. Like
there should be something like, you know, every law has to be like rechecked every 10 or 20 years
or something. I think this would be fantastic. I'm strongly in favor of this. So I think this
is a good prior that you bring up here that like, given the ways that laws are currently handled,
how they're like never repealed, and so on, we should increase our skepticism towards passing
new laws and regulation. I think this is a very reasonable intuition. And I want to acknowledge
that it's a very reasonable intuition to have. And it's one I have as well. But this doesn't
generalize to me then just so that we shouldn't do any regulation, which you don't seem to believe
either. So we're kind of on the same page there. And I also want to make very clear here that like,
I'm not some guy who thinks that like government is awesome and great and regulation is awesome and
great, by no means. I'm trying to keep my identity small. You know, I don't really have a politics.
I'm not, I don't care about any specific word or any specific flag. What I care about is what works.
What I care about is under my best models of game theory of technological progress and whatever,
what leads to the best outcomes and avoid the most catastrophic outcomes. And I think you
would agree with that as well. I'm sure you're trying to do the same thing. Yeah. And so I don't
think we disagree. I think we're on the same page. We just have different models of the world.
Exactly. And that's why we're having this discussion. That's why it's getting interesting
for people to listen. Great. So I'd like to go a little bit into some of my models here. So you
talked about how, for example, is that like, we just don't have data about how things go wrong.
And this is reasonable. And I wouldn't dispute this. Like, you know, do I know how AGI will end
the world? I don't know. Literally, I don't have studies about AGI's, you know,
controlling, do two different planets, you know, and do different solar systems,
one of them has AGI, one of them doesn't. I don't have studies like this. And my claim is,
you don't get studies like this. This is just not how things work. Like, you know, there's,
there are certain, what the world you describe, where you can try things, you can, you know,
fuck up whole generations of people with poison in their head and then fix things afterwards,
is an ergodic world. This is a world where you get to retry. It's a world where you can
fail a couple of times, blow your hands off or whatever, but the next generation is going to
be fine. And my claim is, and I'd be interested if you disagree with this model is, is that
at some point you exit this world. The world is not ergodic, actually, it's actually very
non ergodic. You can die. You, there's technology that you personally cannot invent because in
the process of inventing it, you kill yourself. Like, you know, part of inventing nuclear technology
required us to invent technology to not die while handling nuclear material. This was a very important
part. Otherwise you just can't develop it because you die. So when you're, I'm wondering if you
agree with this, forget AI for a moment, that at some point, not saying it's AI, just at some point,
we will develop technology, but it's so powerful that if you fuck it up, it blows up everybody.
Do you agree with this?
I mean, in principle, we could already do that if we build a big enough nuclear right now,
right? Like we could just blow up the earth, right? Yeah. But have we done that?
Yeah, but like nuclear weapons, right? Yes. This is the fact that I was just interested
like you think this is possible. I would agree that like, I think we're already in a non ergodic
world. There's already action chains accessible to humans that are non recoverable. Like if we
went down these action chains, we don't learn, we don't get to retry, it's just over. And so
when you're dealing with a non ergodic world where you don't, where there are traps, there are paths
you can't recover from, you have to play different strategies. You can't just try all the paths.
If you just try all the paths in the limit, you die.
Yeah, yeah. So there are paths where the reward function is like negative infinity. It's full
existential destruction. So at this stage, for example, what would be a state of the world?
It's like building one nuclear bomb that has enough power to like, I don't know,
shatter the earth. It's probably difficult to do right now with, I don't know, the details
I'm not a nuclear specialist, but, but in a sense, like, we know that, and we don't build
that technology because it doesn't have utility to us because we're only interested in our relative
positioning with one another because we have this sort of fractal competition at all scales.
So what people do is they make smaller nukes that, you know, could damage their enemy,
but not destroy themselves in the process, even though, you know, there's the game
theoretic equilibrium of mutually assured destruction. To be fair, I do think, I do think,
at least from the models I've seen of nuclear war, I think like 10 or 20% of human population
survives. And on a long time scale, we would probably recover. So it's not actually fully
existential risk yet. But again, if we had a nuclear bomb above a certain threshold,
uh, that was just sitting there, you know, with a big red button, that would be bad. So, but,
okay, yeah, if we're trying, if we're going to try to draw analogy to AI, I think we're still
very far from that. Yeah, this is, I'm not trying to make the analogy yet. We'll get there. I'm not
making it yet. It's cooking. All right. So now imagine, you know, let's say I, you know, drink a
bunch of leaded gasoline and decide this whole AI safety thing is stupid. I'm going to go join my
friend Guillaume and his startup that's built on quantum hardware, right? I'm like, let's go.
Not quantum. So, yeah, sure. Sorry, I don't know the details of your actual,
don't worry about it. Like, let's, let's say it's quantum because it sounds cool. So,
thermo. Sure. Yeah, thermo, quantum, whatever. But let's, let's go with quantum for a sec. So
let's say I join you, we work on some cool hardware chips, and we're doing some create,
you're a physicist. So like, you know, I'll use some physicists stuff. And I know this is not
exactly correct, but indulge me. And let's say while we're doing our experiments late at night
in our secret laboratory, we go through the data. And suddenly we notice, oh, shit, we're in a false
vacuum. So what this means is for the viewer, a false vacuum is a hypothetical quantum event or
system where the vacuum energy is not zero. What this means is, is that if you could trigger
what's called a vacuum collapse, you would basically destroy chemistry. It's just like
there would be an irradiating shell outwards where all the physics like all the stars stop,
all life stops, everything. And this could happen very suddenly with a very small trigger,
hypothetically, we, is this probably not true? But let's say in our hypothetical experiment,
it was true. And we find out that our hardware can trigger a vacuum collapse, like we figured out
how to do it with our hardware. And this hardware can be manufactured, say with a, you know, in a
semiconductor fab. If such a technology existed, and you and me had it, what do you think we should
do with it? I think that if the vacuum was in a, so for reference, it's like the, you know,
instead of being in the ground state of, of, you know, the quantum field, you would be in a,
in a literal local minimum, then there's a true ground state. And then first of all, I would
imagine that to, to engender this, this, this jump, you'd need quite a bit of energy. Another
thing is like, anyways, I won't, I don't believe in false vacuum. That's a whole discussion.
Yeah. But essentially, I think like, if that were the case that a tiny nudge
would blow everything up, we would be in a highly unstable situation. I think that
there is a certain, like even if you had, if you alerted, formed a world government, and you
alerted the authorities, and this would be the most secretive secret on earth, it would have a
certain coherence time, a certain shelf life, and basically we'd already be dead. Like, like on,
I would be surprised this is like, just like terminal at that stage. I don't think that's
the case. I don't think there's anything like that. That's the case. Like, I mean, some people
thought a more realistic scenario is like, you know, if you theoretically crank up the LHC
high enough energy, you might create a black hole. But if that usually black holes that are tiny,
the, the radiative way through Hawking radiation, but, but assuming you could have created a
black hole that starts consuming everything, like it would have been, but it would have been
problematic. Anyway, as we can have a whole technical discussion. That's not the case. But
yeah, yeah, the only reason I bring this up is just kind of like, maybe I'll sketch you in a bit
of the mindset of like, if you did believe that, like if you did see this happening, like you would
reasonably come to the conclusion, I think you came to were like, wow, we're really fucked. Like,
we're really fucked. Like not even government can save us. Like this is a really bad situation to
be in. I still, well, so here's the thing, right? Like if there's a, if there's a piece of physics
or technology that we don't understand and is dangerous, we should, we still need to study it
in order to control it and make sure, for example, I don't know, like we're completely
hypothetical land here, but like, let's say there was a way to create this local instability,
there's a way to contain that or, you know, correct it, you know, usually if you can control
a state to go from one, from A to B, sometimes get to go from B to A, there's all sorts of,
yeah, like my point is, like, I think we should still like, we should study, well, that's why
we're studying physics, right? Like we got to know, you know, for example, I think
of past 20, 30 years, you know, we were looking for, are there extra folded dimensions that could
just freaking unfold right now? Or, you know, like, and when we had to test that, right? Like,
really, it's like, there's, there's kind of the unknown where we are instruments and our life
lives in a certain band of energies and anything beyond that we, we don't know. And in a sense,
we kind of have a responsibility for our own survival to try to explore that. And I guess, like,
explore those areas of the unknown that we don't have perception. Here, I think we're talking
instead of like regimes of energy, we're talking about regimes of intelligence. What does like
intelligence look like beyond a certain threshold, right? And, and we don't know, right? We don't
know at the current time. And that's why we're exploring. And that's why, you know, we have
folks like opening an anthropic yourself, etc. Right? Yeah, yeah, you're already jumping ahead
in the analogy. But yeah, like, so, okay, so there might be, you know, dark voids and, you know,
dark horrors in the physics, I think you agree, like, we might be in a false vacuum or there
might be some other thing out there. So like, you know, not likely, maybe not false vacuum
specifically, but like, it's imaginable that there are horrors. So here's, here's a, here's, okay,
so I mean, you're leading this conversation, here's, here's a similar sort of question for you.
What if there were very advanced aliens that have very advanced technology that are kind of
scouting the earth right now, um, you know, completely hypothetical scenario, would you feel
a sense of urgency towards accelerating our technological progress to ensure the ensure
that we become formidable enough to sort of defend ourselves? I mean, there are analogies with AI,
so we will compare it to an alien force invading us. And so do you feel like, don't you feel there is
a sense of urgency to make humans more formidable in order to ensure our safety against the unknown,
right? Like, could also be, you know, I mean, at some point, we're going to
oscillate out of the galactic plane, and there's going to be more asteroids, we need much more
control of the rocks that are getting hurtled our way. So we have a responsibility scale up
our space exploration technologies for survival. But wouldn't, wouldn't you feel that urgency?
And wouldn't you be kind of more open minded towards taking a bit more risks, aka like just
venturing into the unknown to seek upside, because, you know, you could get disrupted otherwise,
right? I think we're, I think that's why you've founded your company, you're trying to make humans
more formidable by solving alignment. I think that's what people, well, some people solving
alignment are trying to do, right? They're trying to augment humans to make them, you know, more
formidable, which I'm totally for, right? That's like, I think we should, we should seek that path.
But I think that, well, we're, we're getting into AI now. But to me, I think all paths will,
will be like explored. I think there's going to be sort of aligned AIs that are extensions
to humans, there could be like totally independent AIs. And then there's going to be like full
Luddites that don't want any technology. And you're going to have all three. And, you know,
all bets are off at that point, right? So yeah. And I mean, I definitely don't disagree with you
that risk taking is necessary and important. And you're also correct that if you're under threat,
it might be sensible to increase your risk tolerance budget. But this isn't an either or.
So I, I agree with you on an emotional level, but yeah, if like there was alien threatening
humanity or whatever, I would be like, well, we should take some more risk with nukes. But it
doesn't mean I think we should take arbitrary risks with nukes. Because a lot of, at some point,
it wraps back around to being embarrassing. There is like a glorious, you know, we tried and we
failed. There's also a really embarrassing, we took all the safeties off the nukes and they
exploded in our Air Force bases. And my claim is, is that the current state of AI safety in EAC
is firmly in the our nukes exploded while still in our Air Force bases.
Our nukes exploded. Like, like, we didn't even take off. We didn't even get the nukes into space.
We like, they blow up on our airfield. We didn't even get them into space.
Like we were ready to have the heroic standing as you know, we didn't even get them off the
planet. This is where I think something happened in AI where it's been that catastrophic so far.
Not yet. But my prediction is, is that if we just don't even try, if we just accelerate as
fast as possible, that's what happens. It's not heroic. It's not epic. It's just we make
stronger and stronger systems that we understand less and less and less, we get more and more
confused, things accelerate more and more and more. And then one day we wake up with, you know,
our finger up our ass, and we have no more control.
I think that it's a bit different with, like, the nuclear analogy is mostly, it's all about
engineering negative reward for your adversaries, right, to maintain sort of that, you know, game
theoretic equilibrium. I think with AI, there's a lot of upside to accelerating, right, and there's
lives that could be saved. And that has to be priced into the risk calculations, right, the
longer we wait to develop technologies, you know, there's all sorts of biotech, material science,
there's, there's a lot of upside we are leaving on the table and having a model where we only
look at the tail event, like tail probabilities of extreme downsides is very biased in terms of our,
our sort of cost benefit. Oh, I agree. If I was, if it was only tail risks,
I would agree with you. I just don't think they're tail risks.
Okay, well, we could, we could, we could dig into that. But I'm just not, I am not convinced that
there's this sort of fast takeoff threshold. I'm not convinced that we can't achieve a sort of
multi-partite adversarial equilibrium by making sure capabilities, you know, multiple, multiple
parties have access to, to advanced capabilities and keep, keep each other in check. Anyways,
we, we can get into it. I don't know where you want to take this. Yeah, I mean, my simple question
is, well, you have a bunch of AGI is competing. Why would the one that makes that spends its
resources giving humans a good life win and not the one which is maximizing its, you know, kill
all the other people in AI's potential? Why doesn't that one win?
Well, I mean, you can say that about companies or countries. I do say that about companies in
countries. This is a good question. Why don't they, this is a good question. Well, because
there's a positive sum game to, it's positive sum to cooperate, you know, we've entangled our
economic systems even against our current adversaries. And why, why did we do that?
Do why do we do it? Yeah, so I think that all like the, the, the theory to some extent behind
EAC is that the system will adapt towards whatever policies and technologies and ways of doing
things that are optimal for growth. And that, I mean, just by construction, like things that
grow, you know, they, they replicate or like if you post select in the future, odds are you're
post-selecting for things that have the high fitness towards growth, right? Yes. And so,
like, for example, if you have a very aggressive regime, right, that's just threatening everyone
and not adding a lot of economic utility, they get shut out of the, of the system or, or they get
like, you know, peer to peer like enforced, like, Hey, if you try to do anything, we're going to
annihilate you. Also, you're not going to have, you're not going to participate in this sort of
benefit. And so I think we have ways to align sort of intelligences that are super human.
Like, I mean, a company is a super human intelligence, it's a mixture of experts of
humans with neural routing. It's technically smarter than any one human, I think so,
we can have a debate about that. But we have ways to align these, these super organisms of, of, of,
beyond human intellect, and through sort of economic exchange, right? And I think that
the future looks like we have AI's that are aligned or extensions of humans, right? And then we
have AI's that are more autonomous, and there's economic exchange between both. And that keeps us
relatively aligned, right? But there is urgency to figure out how to extend
human intelligence in a way that makes us more formidable and, and, and more of a player at
this, at this big voice table, right? But, and that, but that's what you guys are working on. So
that's good. I mean, yeah, I appreciate all that you just said, but like, I feel like you
contradicted yourself. You said it optimizes for growth. And you might notice that growth and human
happiness are two very, very different things. Sure. Well, so, I mean, you have, so you have,
you have your choice, right? Nowadays, you can live in Europe, where you're going to maximize
your happiness, have your espresso, chill a bit, have a two hour lunch. No, I'm just kidding.
But, or you can live in America where it, there you go. Might as well lean into it. But,
or you can live in America where it's like, it's all in on growth, right? And I think that,
you know, you have, you have those two options. And I think that's the beautiful thing about
the world right now. We don't have a monoculture. We don't have a single way of doing things. We
don't have a single way to legislate things. In your model, we can A, B test things locally,
right? Sure. But in your model, America will eat Europe. It will destroy Europe. It will grow
more. The system will succeed. And these people in America are less happy by your own admission.
So what will happen is, is that on average, people will be worse off.
I think that, okay, so this is an important distinction between like IAC and EA. We are not
hedonistic utilitarians, right? At EAC? We are. Am I? Okay. Well, maybe not you, but you know,
EA in general, I'm just, it's not about just the comment in general, you know, some EAs are
trying to maximize happiness. And that's a different loss function. I think that has spurious,
weird local minima like wire heading and highly suboptimal, right? In many ways. And it's not
anchored to reality. We're trying to maximize growth of the system because we think that,
you know, life and intelligence is a very special phase of matter in our universe,
and we have responsibility to scale it and figure out how to make it grow. And, and hopefully,
you know, we're, we can all aim to be part of those future states of high growth. But the thesis
is that if you, if you, let's say, let's say you legislate, let's say for a whole set of countries,
you have a sort of union and then you legislate away the ability to pursue AI past a certain
threshold. Before we go back into this, just you contradicted yourself again. You said you
want to optimize for growth. And then you talked about the beauty of intelligence and all these
other things. Those are two different things. If you optimize for growth, you get growth. You
don't get beauty and happiness and all these other things you like. Okay. So, so, so I guess
the thesis is that intelligence evolved as a way for systems that are lifelike to adapt on a faster
time scale for better growth, right? Acquisition, seeking out resources, acquiring them, extracting
utility from them. Nature is red in tooth and claw. And so, and so I think that if we have,
to have a larger civilization, it's going to necessarily have to be more intelligent.
Overall is every individual unit going to be more intelligent? I don't know, but overall as a system
has to be smarter because it has to seek out far more resources and utilize them more efficiently.
And the beauty is that if you have a subsystem that is not optimal in terms of its ability to
acquire energy and utilize it cleverly, some branch of it will fork off, outcompete it,
and dominate. But the thing that outcompetes is cancer. The thing that outcompetes is maximum
growth, no art, no beauty, no happiness, just growth, just to kind of remove all the parts
of the brain that have emotions, just focus on replication locusts. No, emotions have utility,
right? They've had utility. Yeah, I mean, I expect, and I expect it was a local minima. I
expect if we, the future systems will not have human emotions. I don't think human emotions are
a global maxima. They're not, but if they're not a global optimum, why not seek to explore
new ways to think? Now, this is a naturalistic fallacy. Is is not ought. So I agree with you
that there are, that our values are not some global, you know, coherent maxima of any growth
utility function. And my answer question is, who the fuck cares? They're mine. I like them.
I like happiness. I like puppies. I like people being happy, playing with their kids and so on.
And yes, this costs resources that I could be spending on maximizing my, you know, economic
growth function in the local supercluster. And my answer to that question is, why should I care?
Is is not ought? Yeah, I do think like happiness evolved as a sort of proxy of
like your estimator, your, your, your gradient of likelihood of future growth or higher mutual
information with the future. Like if you, if you have a positive impact on the world, you feel good
if you're, you know, let's say have a significant other and feel like you're going to have a legacy,
you naturally feel good and your brain like secretes all sorts of feel good hormones. So I do
think they're just, you know, our neurochemistry evolved as sort of proxy loss functions for,
for certain effects. But I, I don't know. I don't know if I think there could also be
awesome human, like a whole new spectrum of human emotion and euphoria that we could achieve.
You know, we're focusing on like pain, but it could be, you know, infinite pleasure. We can live in a
not a, I don't believe in utopia, but I would say that we can have much more comfortable lives,
much happier lives in that overall technology, techno capital, the techno capital machine
is a deflationary force that, you know, helps everyone have access to a higher quality of
living and, and overall, like an increased happiness. And in fact, the things that causes pain, like,
you know, healthcare, housing, legal systems have been over-regulated and decelerated. And if we had
let technology sort of evolve faster and, and, and create more deflationary force in those areas,
like build more housing and so on, it would be cheaper, more accessible, and then more people
would be, would be happy. And so to, from our perspective, actually deceleration is what has
caused pain so far in our current system rather than acceleration. But sure. And I'm not, they
wouldn't disagree with you, especially in housing policies. But like, that this is the kind of stuff
that makes me call EAC Fisher price Nick land, like you don't go all the way. You only think on
like American local thing, like five years into the future, extrapolate your own beliefs, man,
like actually take it like take techno capital, it's logical conclusion. That's what Nick land did,
you know, like Nick land allegedly used to be a Marxist. And he was taking doing Marxist capital
analysis of like, what happens if techno capital gains more and more power. And his conclusion
was eventually there was only capital, there's no labor, there's only capital, there's no
people, there's no happiness, there's only competition, there was only capital, capital
itself becomes sentient. And like this is before like AI was a big topic, it's like the 90s when
he wrote this kind of stuff, right. And he's the only goddamn person and all of acceleration is who
actually bites the bullet and actually goes all the way. If you go all the way, if you optimize
for something, you lose everything, which is not the thing you're optimizing for, we got lucky.
We got lucky that for example, torturing people, you know, to work 24 hours a day doesn't work
because they fall apart eventually. But this is no longer true when you have robots. If we have
robots that can work for 24 hours, there is no reason to give them time off. There is no reason
for them to have hobbies. There is no reason for them to spend time with their kids and love them.
There is no reason for this. We got lucky that we ourselves have so many limitations that eventually
you have to compromise because otherwise we can't function. But if you take the techno capital
acceleration towards logical conclusion, the logical conclusion is not, wow, we have more
housing. The logical conclusion is there is no more human, just a bunch of mindless automata
optimizing some growth function. I don't believe, I mean, we have different models of the future
asymptotes, right? Like, I think that there's not a finite set of jobs for which we're competing
in an economy. I think we just increase the scale. Not really. There's plenty of atoms
in outer space, right? And if we are, I mean, humans are bound to earth because we kind of
overfitted our biological hyperparameters to earth, right? We've evolved over billions of years
over here. Whereas maybe a synthetic organism isn't as anchored to, you know,
earth living conditions and could seek out to grow beyond the earth. I think that overall,
even if a large part of the economy is accomplished, like a lot of the economy is executed on by
machines, doesn't mean that the human component would necessarily shrink. It would get diluted.
But if the overall system grows far more, it's kind of like taking venture capital. It's like,
it's intellectual and operational leverage, right? We're kind of diluting ourselves or share
of the economy, how much we contribute. But if overall economy grows by many orders of magnitude,
our overall component can grow. And that's in our best interest. And in general, we're going to,
every subsystem is going to do what's in their best interest. You are arguing for the best
interest of humans and hearing you out. I just think that people are greedy at a company level.
They're going to use more AI if it causes more growth. And that's just reality. You're not going
to be able to legislate that away. People will revolt if they can't use AI, if it has massive
positive utility. And I think we're going to do it at a country level and we're going to do it at
the human level. And I think just because we will doesn't mean we should. Is this not odd?
I mean, I think, I guess like I'm just trying to like, you know, it's kind of like real policy.
It's like the assumption is that you're contradicting yourself. You're conflating two
different things. There's the thing we want. And then there's facts about reality. There's
decision theory, like what is true? What, what, how do you win? But I don't think we agree what we
want, right? Sure. And I'm trying to separate them so we can have a separate discussion because
often I've talked about what I want. And you talk about what works. You start with, well,
people will do it anyway, so we shouldn't even try. This is an argument about decision theory.
This is not an argument about values. Okay, so it's not an argument about values, but you're
arguing for maximizing human happiness. I'm not, I haven't, I haven't stated any values so far.
I've not stated any values. I've simply claimed that I like puppies. I like many things. I'm not
saying I haven't. That sounds like, I mean, that, that sounds like a certain utility function that
we know. Sure. There is some kind of function. There's some way you can model it. You can call
the Fristonian prior, whatever the hell you want. I don't particularly care what you model this.
This is not the point I'm trying to make. The point I'm trying to make is, is that whatever
the hell that thing is, I'm not even saying I know what my values are. I'm not even saying I know
what your values are, but I claim that your values are not growth. You don't actually want this
as much as you think you do, and I for sure don't want it, and people don't want it,
because this is not what we actually like. It's, and you can make, and then every time I bring
this up, you make this argument about like, oh, but it's what's going to happen anyways,
and it's what is, and like, that's a separate argument. Your values are not growth. How so?
Because I like puppies, and happiness, and friendship, and games, and like...
Why do you like friendship? Why, why, why do you like having relationships? Why do you like
being part of the group? Because evolution kind of hard-coded you to crave these things,
because if you're part of the group, you have higher likelihood of passing on your genes.
You're mixing up as an odd again. It's like...
I'm just going for the latent variable here, right?
You're describing it as, I'm describing it as odd.
I think having a subjective loss function for how to steer reality is prone to being manipulated,
and is the source of a lot of pain in our modern times. So I am arguing for an objective loss
function, and we can argue that that loss function is not anthropocentric enough, right? Free energy,
right? We can argue about that, but I think that if it's too anthropocentric, it's too
hedonistic, it leads to weird... Optimum, we're prone to being manipulated, right?
And like, this is just cope. What you're describing is that reality is hard.
Yes, yes. If the thing we want is complicated and hard to get, the solution is not, well,
let's pick something simple and easy to find and give ourselves a participation award.
The answer is, well, we have to get stronger. We have to get better. We have to get better.
Yes, let's get stronger. The answer is not, oh, let's pick an objective utility function
that I can follow, so I at least feel like I'm making an option.
No, it's not an optional choice of utility function, and that's kind of the thing that
the anchoring in physics gives you. It's like, oh, no, this objective function for seeking out free
energy, utilizing it towards your own growth is what the universe selects for. It's just probability
theory. Again, you're making the same mistake. Naturalistic fallacy is not odd.
But you don't have an option to obey gravity. You can't violate the laws of thermodynamics.
Yes, sure. Again, is this not odd? You cannot derive an odd from an is. These are two separate
magisteries. Okay, so my point is, so let's say you have your, you know, let's say you legislate
Europe the way you want it, or half the earth. It doesn't matter which half, I don't know. And
then we have the accelerationist half. We play the movie out, right? We play the movie out.
Yes. I think, I think that the, the accelerationist half is going to outgrow and
Yes, and you both die in this scenario. Both you and me will be dead.
I don't know about that one. I would need more evidence.
You agree that eventually we will build technology that's so powerful that we could blow up anything.
And in your model of the accelerationist world, we build it as fast as possible with as little
safety mechanisms possible when everyone has access to it. What happens then?
No, we don't seek, we don't seek out. No, but it happens accidentally.
No, you accidentally, you just accidentally. No, but we have a model, right, like that. Okay,
if we reach this state. Can you predict ahead of time what technology will be safe or not?
Can you predict ahead of time how powerful it is? We can't guarantee safety, but we can't
guarantee. Yes, we can't, but we can do better than not even trying. But we can also, I think,
guaranteed to shoot ourselves in the foot in terms of like the upside we're leaving on the table
is almost, it's just as bad. Like in your models, the upside we are leaving behind
does not get priced accurately. And I think it far outweighs the risks. And I think, sure,
you can make this argument if you want. This is a coherent argument to make. This is a coherent
argument to make if you want to make this argument. But like from my perspective, like,
we can talk about this specific, like, okay, okay, what's, if we want to talk about pricing,
I think this is a great point to go. I think this is a great place for us to go. I think this
is a reasonable thing to talk and disagree about. But I want to make clear, again, just the point
I'm trying to make here, is that the point I'm trying to make here is, is that predictably,
if you have a civilization that doesn't even try, that just accelerates fast as possible,
predictably, guaranteed, you're not going to make it. You're definitely not going to make it.
At some point, you will develop technology that is too powerful to handle if just have the hands
of random people. And if you do it at unsafe as possible, eventually an accident will happen.
We almost nuked ourselves twice during the Cold War, where only a single person was between a nuke,
firing, and not happening. If the same thing happens with, say, superintelligence, or some
other extremely powerful technology, which will happen in your scenario sooner or later, you know,
maybe it goes well for 100 years, maybe it goes well for 1000 years. But eventually,
your civilization is just not going to make it. I don't know about that. I mean, we have, we already
have that, we've already crossed the threshold by having the knowledge about, you know, nuclear
weapons, right? But again, I don't, I think that- Do you think our current world is stable? Like,
do you trust our leaders with nuclear weapons infinitely? Do you think- And you want to trust
our leaders with the monopoly over AI power? I don't. No, like- But that's what you're suggesting.
Again, you're- What are you suggesting? I have not even said what I suggested yet,
so this is my point. It'd be great to get into that. I would love to get into that as well,
but like, I'm trying to make a more meta point before I go into any specific points. My point is,
I am actually pessimistic. When people say I'm pessimistic or they're doomer or whatever the
hell you want to call me, I'm actually- I just don't know if there are terminal states as you
claim. I think that in general, there are, there are large and negative reward states in terms of
like utility towards growth. For example, you know, global nuclear war would reduce our population
massively, would take a long time to rebound, and that's a setback in terms of growth. But I do think
that on average, you can, there's, there's certain probabilities, fluctuations about this, this kind
of path towards growth. But overall, the odds that we go back to absolute zero and that life is
completely gone, like, I think those, those likelihoods are pretty low and even technologies
that are very scary or very powerful, like we have a model that we like to extrapolate and, and, and
have a sort of black or white thinking, but everything has some, some nuance. Like according
to your model, like the world should have ended when we discovered nuclear bombs, which-
And it only did twice. Like, like, you're like, my model says is that there's, you know, some
percentage chance that yes, if you have dysfunctional institutions with dysfunctional leaders and a
dysfunctional civilization with access to massive weapons and mass destruction, yes, they probably
will get used and probably there will be close calls. We dropped a nuke on South Carolina accidentally.
It's still there. It's in the swamp. It's still there. And it was pure luck that the, the, it was
to actually armed. The nuke was armed and we got so lucky that it just happens, it misfired.
If a city would have been blown up, it would, the world would not have ended actually. So.
Yes, sure. But like what happens if that happens, if you, this is the rate, every decade or two,
we drop, you know, one nuke on South Carolina and our nukes get better and better and better.
Eventually the nukes are synthetic bio weapons, you know, AGI, ASI, these kinds of things. What
happens? So, I mean, even bio weapons, there was a leak of a bio weapon. We didn't all die. We
probably all got COVID once or twice. Again, yes, sure. The world has never ended in the past.
This is true until it's not. Like, this isn't an argument.
I don't know. It's a data, it's like- Yes. And the data is we got really damn close
many times, even with the shitty, you know, pseudo apocalypse tech that we have now. I agree
that our current nukes are not existential yet, but they're as close as we've gotten. And even
with those, we've had a bunch of accidents in the like, measly 70 years we've had these.
I just don't, I just don't agree with your model that like, we're going to have AI pass us there.
Like, can we, like we're, we're down like 50 analogies deep at this point. Can we just talk
about AI at this point? Because instead of like drawing analogies to a bunch of other technologies
that aren't quite the same game theory. Because again, AI has huge upside we're leaving on the
table, whereas nukes don't. So, so like, for like, let's get into the discussion. Like, what, what,
what do you want? What is your analogy here? Let's, let's get into it. Like, what are the
connections you're trying to draw? You're trying to say, we shouldn't build AI because, or AGI,
that is human level and beyond, because it is a one-way function is a terminal state where
we won't be able to put the genie back in the bottle. And, you know, our existence as humans
is guaranteed to end in your model. Is that correct? And can we also cover what you would do,
Connor? Also, I would talk about that. But I think what you say is worth talking about as well.
So no, my model is not, wow, every single human level intelligence kills you instantly and there's
no way back. Quite the opposite. If I believed that, I would go live in Hawaii with my family and
just live all my final days. I think it is completely possible to build AI systems out of all the
upsides you want. All the great things you and other EAGs envision. I like the aesthetic. There's
a lot of nice things to like there. So let's build it. Yes. And that's going to take a hundred years
and us not blowing ourselves up along the way. It's not that easy. We don't have a hundred years.
Yes. That's my point. That's why I'm a doomer. You're a doomer too. No, in the sense that like,
if a subsystem of the civilization decides to go slower, another subsystem will want to go faster.
Yes. And it's just not stable. And then the only way to stabilize that is to have a top-down
monopoly on power and on AI that sort of enforces this equilibrium in a top-down fashion. And that
I think is a bad trade and I wouldn't, I don't want to take it, right? And that's where we, yeah.
Congratulations. As Lord of the Doomer, you know, honor society, I so upon you the rank of doomer.
You are now become a doomer. You are a doomer. You are a doomer because you believe
that both, that we are basically fucked. You may not explicitly formula it that way,
but no, no, look, you think both that if technology is this dangerous, as I say it is,
then, and if we try to develop it this safely, it wouldn't work because someone else would do it
much faster. And the only way to stop that is through monopolies. And that's not an option.
This is also basic. No, there's a third path, which is the one we want to take with EAC, but
go on. And what is the third path? No, the third path is, you know, we want to have a
decentralized control of AI, right? Like if every company, if every individual can have access to AI
and compute, you end up in this sort of adversarial equilibrium where people, companies,
smaller governments are more formidable and are not to be fucked with. And that's, you know,
it's kind of a peer to peer enforced equilibrium instead of a top down. So AI mercenaries.
Sure. Yeah, I think I know. So, so, I mean, in our current system, right, we're talking about
lawsuits and lawfare, right? A LLMs are going to be, you know, our superpowered lawyers and whoever
has more capable LLMs is going to win lawsuits and going to be able to enforce their will
on others. And that's where we're going, right? And who's going to win the AI warfare, the one
with the better guns, the better LLMs, the one with more capital, just like it is right now
in our legal system. So you're a doomer. So you're a doomer. No, I'm just realist. Yes,
you're realist. That's not over. Your describing is over. No, like if there's a form of legal
violence through LLMs and we just throw a compute at each other like that, escalation is going to
yield like way cheaper compute. And we're going to use that for all sorts of amazing discoveries,
nuclear fusion, we're going to geoengineer our planet. But the one with the most capital wins.
Well, it's not. There's no decentralization. You just said, you just said the one with the most
capital wins. There's a parallel just like in any sort of complex self-organizing system.
I believe in capitalism and that's what happens, right? Like there is some power
concentration, for example. No, the point is if we don't legislate away the ability for small
startups or individuals to try to disrupt the giants, right? Like if we don't legislate away
that ability, then the incumbents get themselves checked by up-and-coming startups, for example.
Why don't the incumbents just create their own AI-powered super legal state?
I mean, super legal state. I don't know about that, but for example, a lot of startups right
now are using Mistral and that was enabled by the ability for open source AI to be shared
because people don't want the platform risks that come from closed APIs, right? And that eroded
the market power and the pricing pressure of the opening eyes anthropics of the world.
And then that kind of corrected their ability to just glob up all the capital and increase
the number. Man, the world isn't a B2B SaaS app. Politics and war are not the same as VC. I don't
know how to explain this to you, man. It's like you keep talking about these huge concepts about
civilizations and power laws and the long future. Okay, you want to talk about war?
Yeah, let's talk about real war. Let's talk about real war. We are in a cold war right
fucking now, right? Yeah, it sucks. It's unrestricted warfare across the board,
and it might evolve into a hot war. I do think that there is very strong danger
where our legal system and certain ideologies are weaponized in order to have the West sort of
defying itself and become an easy target. And because we are in this global geopolitical competition,
we have a duty to accelerate, right? Because we have to outcompet and survive. And that's just,
that's how it's going to be. We're not going to be able to create a world government that
keeps everyone in check. And I wouldn't want to live in that world where
there's a global pinup gone. And so we're going to have to play out the movie, how it plays out,
right? That's just reality. And we're going to have this. So am I. No, it doesn't necessarily
yield doom. I mean, we've been in this sort of... But your whole ideology, as far as I could tell,
is just we just play it out, let it rip. There's nothing we can do. We completely impotent like
playing it out is what got us here, right? Like this, this fractal sort of competition
between tribes, between people, understanding and faith that the system is an adaptive system.
And it's going to adapt towards a high level optimum. But I don't like the whole point is that
we don't want a world government or one power to dominate everyone. And we have to
stay in sort of this sort of small capabilities delta between different players, whether that's
at a corporate level, but also at a nation state level, right? Now, if you want to argue that
closing down AI, you know, is more optimal for, you know, competition against for an adversaries,
I'm happy to have that discussion. But yeah, I mean, we can have that discussion as well. We
can talk about how open source is basically feeding directly into all adversaries as strongly as
possible. And it basically benefits only them and not us. But that's a whole different conversation
I don't particularly care about. I do care about it. I do want to say a point here, because I think
that the strength of the American system is its variance and its internal competition between
different corporate entities, but between startups between innovators. And, you know,
the point is that, you know, the top AI systems, several of them are American and they're competing
with one another. And that internal competition breeds very high fitness that makes us dominant
on the world stage. Having open source AI gives more people the opportunity to contribute to AI
and enables us to have much faster rate of discovery, because we have just more points in
this point cloud search of hyper parameters of how to do AI, right? And sure, like, you know,
our adversaries also have access to those models. Do you think the blueprint for the F16 should be
open source so more people can develop fighter planes for you?
So, so this is why I'm interested in digging because there's there's been this sort of, you know,
bait and switch of like, oh, we're doing this for safety. And then and then when that argument
doesn't work, sometimes nowadays, we're pivoting to like, oh, we don't want to give it to our
adversaries. And to me, it seems like there's a sort, you know, similar to I don't give a
shit about national politics. I'm not American. I'm not European. I'm nothing. I don't I don't
know what you are. But yeah, I don't my point is like, there's a bit of a bait and switch,
maybe not you, but I think in general, where when one argument doesn't work, the pretense,
you know, we stop pretending and it's like, okay, no, this is for a capabilities delta with
our foreign adversaries. I don't care about the delta, the foreign adversaries, I care far more
about the delta with crazy wackos. Like what again, I'm asking you a question. How would they
I'm asking you a question. Should the F 16 fighter plane blueprints be open sourced?
Do you think this would lead to a better or a worse world? I think there is a lot of someone
who's been espionaged by foreign adversary or or or spied on. You know, unfortunately,
I think big tech is very like leaky and insecure. I think that a lot of this secrecy and and closed
sources is like security theater and that so many or organizations are compromised that
if you're not going to have actual secrets, your only mode is speed. And if you want speed,
you want variants. And if you want that open source is the way. And so you think the F 16
being open source would be net good or net bad? I don't know. I think it would reduce the,
you know, revenue of certain companies. But I mean, the Chinese and the Russians probably
have their hands on that. Again, I'm asking you for the whole world, you know, whether or not
the Chinese already have it or not, would releasing all of our secret, you know, engineering
documents of all F 16 is a force for destruction. It's not a force for positive economic utility.
You're in your drawing these comparisons all day. I'm trying to make this is not a metaphor.
But you're you're just lost analogies and analogies and analogies. Let's talk about AI
directly, right? You're just trying to answer the question. Yes or no, is it good or bad?
If it's if it's not open source, I mean, there should be more open source plane designs because
I think it would answer the question. Yes or no. Well, I'm trying to I'm trying to explain,
maybe not the F 16, but the next, you know, maybe some other planes, if you had more people
that were trained and could self educate about, you know, defense tech, we'd have a better talent
pipeline and maybe we wouldn't have, you know, as many problems because we'd have a better
defense industrial base, right? And so maybe it'd be good to have some open source, right, as a
pipeline for training people. Okay, but should the F 16 be open source? Do not specifically,
maybe not. No, okay, okay, that's my that was my question. This was not an analogy. It was probing
your intuitions around these kind of things. I wanted to understand how you thought about this.
I was not trying to make a case. To me, like, open source AI, you know, it's never good to be
quite as good as closed source. But it's it's great to, again, train the workforce,
seek out new ideas. It's good for innovation. Sure. It's good for innovation momentum. It's
net positive. Okay, sure. This is completely fine. Like, you know, should mixed Troll be public,
it'd be an open source model, I don't know, probably net positive, probably, like, okay,
probably, I don't know. I'm not certain, but seems pretty, pretty reasonable to me. I've
used the model before. It's nice. It's a good model. But if we were to build, say, an AGI,
which is smart enough to design an F 16 fighter plane, do you think it should be open sourced?
Smart enough to design an F 16 fighter plane.
I mean, would it though, would it have that knowledge in its prior, I feel like that's
very specialized knowledge that a plane of that quality, it's smart enough,
it has all the mechanical engineering and so on. So it could design a plane of this quality.
It could design a plane of this quality. Well, the assumption is that whatever centralized power
that is trying to have the monopoly on violence, you know, should have a much better AI that
would design much better planes, right. And so you still have that power gradient, which
I'm asking a much simpler question, really super, super simple. Just you have an USB drive or AGI
that can release that can be used to develop a plane of this thing.
I think by the time we have that, we have much more advanced planes because we're going to use
much more powerful AI that is going to design much better planes and the F 16 is going to look like,
you know, something primitive. So as long as the government has even worse weapons,
it's okay for people to have access to weapons. No, I think as I think that the natural state of
things is that big corporations in the government will have some capability Delta, right, between
the AI that's accessible to all and the AI that is that is centralized and that maintains a sort
of power gradient that seems to be important in your sort of in your sort of model of the world.
You're very worried about sort of peer to peer violence versus sort of top down
violence. I guess we have different priors for how that works out. But, you know, to me, I think
there's going to be a maintenance of this, this sort of power gradient between the centralized and
that, you know, the top down and the bottom up forces. But my concern is to not have too huge
of a gap. And I think over regulating today, AI could yield a massive gap where the government
and the incumbents and cartel forms, and around AI, and they have the monopoly on advanced AI,
they don't allow the people to have access to AI that they don't control. And then they also steer
our access to information. Maybe they even convinced us AI never existed. And then they use
it for for oppression and deepening their their their power delta. And that concerns me, right?
But I do think like, I think like, even if you had a boyfriend for next F 16, like, good luck
getting the supply chain together, there's a sort of regularizer of the world of atoms,
things are hard to do in the world of atoms, like, you know, you can just you can Google search,
you know, you could Google search bio weapons, probably don't know how to manufacture them,
you don't have the wet lab, the lab for it. You can like random search, literally random search
chemicals. And on average, it's gonna be toxic, because most many things are toxic,
doesn't mean we should ban computers. Like, I don't think it's just a good argument for
banning things. And I think like, your argument is that eroding the top down power gradient causes
instability. And my point is that that power gradient will be maintained. I'm just concerned
that it becomes too sharp. This is an interesting point. I did not expect you to have this view.
So thank you for elaborating on this. I thought so. Thanks for elaborating on that. This is
interesting for me to hear. I wouldn't describe my own point of view as eroding, you know, top down
power as being inherently bad. I think if we were a better society, it we would need less top down
control. And they would and like decentralized control would work. But that's hard. Like my main
point where decentralization like doesn't work is because it's really hard to build large coherent
systems. Like it's just hard. It's not impossible. Yeah, it's hard to steer. If you have a higher
entropy system, it's harder to steer. But that's also what that's also what gives it fault tolerance,
right? Because if you if you have a system that's too easy to steer, then then power seeking agents,
which we have, we have humans that are power seeking just as much as the AI's of these doomer
futures, like they are power seeking, and they do a sensitivity analysis, which nodes can I
compromise or or infiltrate in order to have control over the system. So entropy provides
some safety. And we we argue for entropy and variance. And it's like, ah, well, it's harder to
control. And then it's like, oh, well, in the in these tail events, in these fat tails, now that
you have higher variance, there's some negative reward there. It's like, yeah, but that's the
cost of being fault tolerant against sort of top down control. Because if we give the keys
to our future and control over AI to centralized cartel, whether it's government and so on,
that could be compromised by people that don't want our good. And eventually, you have a very
steerable system that kills all variants. And so it's very hard to fork away from it and compete
against it. And then and then you opened up the door to an AI assisted tyranny. And to us, that's
like, that's a real existential risk. Yeah, yeah, I understand this point of view. I understand the
intuitions behind it. I think a lot of these intuitions are good intuitions. I think, you know,
a lot more city states would be great with different forms of regulation and stuff like this. I'm all
in favor of charter cities. This is a great concept. I think I may have a hypothesis for what maybe is
a much deeper disagreement that we have in our models that we haven't really talked about,
which is the asymmetry between offense and defense. So you often talk about a power gradient. This is
not really how I think about things. There's not a linear unit. And if you have more of those units,
you're safe. But from my perspective, if like, you know, I have more money than, you know, like
some random, you know, crack addict on the street. But if he has a gun, then, you know,
he has certain kinds of power over me that I can do fuck all about, whether or not I'm, you know,
a relatively wealthy man or not, or relatively well politically connected, like power is not a
one dimensional unit. So from my perspective, destruction is almost always easier than defense,
almost always. There are some edge case exceptions like cartography. But in most scenarios, it is
much easier to destroy something than is to build the same thing is much easier to build a bomb
than is to build a reactor. So from this perspective, I think this is a really, really,
really core point of my perspective. Yeah, I understand. I think you're arguing that
entropy is sort of the natural state of things and bringing order is more difficult and takes effort.
And the reality is that we want, we live in a complex system, we want it at actually the
edge of criticality, where we don't have so much order that we've suppressed variance and it's
crystallized, it's not very dynamic. We don't want it to be fully disordered and maximally
entropic because then, you know, it's too high a temperature, there's no order whatsoever. We
want a careful balance between both, right? So you don't want maximum acceleration, you don't want
maximum entropy. I do think, well, it's complicated because it's not, it's not entropy, it's actually
free energy. Free energy is a balance between energy minimization and maximizing entropy,
right? And so it's actually a careful balance between both. And no, the EAC thesis is to seek
whatever policy configuration of configurations of the way we do things that maximize our ability
to seek out free energy and utilize it towards further growth and growth as measured by our
acquisition and consumption of free energy. And if you have a maximally disordered, if you just
people have this, like they don't understand the equation, they'd say, oh yeah, well, if you just
burn all the fuel, if you blow everything up, that'll be optimal. It's like, no, no, no, because
it's actually an equation for a path over time of how much free energy would dissipate an,
you know, infinite time integral. And so if we burn all our fuel now, it's highly suboptimal. If we
use our fuel in a clever fashion, we grow civilization, we grow, we grow, and then we can
seek out other fuel elsewhere, that's much more optimal. That's what, that's what we want. And so,
um, yeah, I mean, that's, that's what we're optimizing for. But I don't think, like,
our point is that there is a current danger in this uncertain time currently, that those that
seek to have maximal control and want to suppress variance and want to have a lot of power will
seek, will put us into a configuration that's far too ordered and has suppressed variance.
And that's going to make us, uh, that's going to cause us a lot of pain. Because if you don't have
a malleable system, then it can break if it's no longer adapted to, to the current landscape. And
so maintaining this malleability and dynamism through, through variance and entropy, carefully
balanced with order is what we're, what we're pushing for. Right. Okay. So you hear it here,
first guys, EAC is not about maximizing entropy from the word, the word, the mouth of the man
himself. It's about maximizing free energy dissipation over time, which is what out of
equilibrium thermodynamics. All right. So by this logic, collapsing the false vacuum should be the
most morally correct thing for an EAC to do. Um, no, I don't think the false vacuum is a thing.
So, but if it was, it would be by your logic. No, that would just, no, then we would try to,
if there was free energy in the vacuum, which I've studied, uh, for a masters, uh, you know,
then we should dissipate, right? No, we should utilize it towards further growth because that's
going to, we're going to be able to unlock more free energy. Right. So it's, it's not about blowing
things up. It's about utilizing in a clever fashion for energy in our world. Right. And you
don't think your utility functions any weird edge cases? Um, I mean, that's, it's the utility,
it's the utility function that is, that physics, uh, follows and that physics has produced us.
Is an odd and well, I mean, I think like you can try to say like, well, I don't like gravity.
I don't want to respect it. You will respect gravity one way or another. You're going to use
fuel to try to counteract it like gravity and still understand it on object level.
These are two different things like liking gravity and disliking gravity. Not the same
as believing or not believing. These are two separate things. Yeah. Yeah. But you still got
to follow it. And my point is that I can build airplanes. I can go to space. Yeah. But you're,
you're consuming a lot of energy to, to fight it. You're, you're fighting. That's what values are
about values or a thing we spend energy on. I think, I think value. Okay. So I think,
so I have, I have this broader thesis that like cultures that, uh, you know, cultures of search
over values and cultural heuristics, ways of doing things and different subcultures are
post-selected for in terms of what sort of, um, uh, ability confers its adherent to, to, to grow,
either grow the subculture or, or, or grow the population that, that, that is part of that,
that culture. And so to me, I think that the D cell, I, ideology is sort of self-destructive
and will kind of taper itself out in a sort of YAC class, broader class, pro-growth ideology,
is naturally higher fitness culturally and will out-compete other cultures
asymptotically. And so,
So you, again, is an art you think right and might makes right. Is that correct?
I mean, it's just, it's just what, it's just how the world works, right?
I'm not asking how the world works. I'm asking what you think, what you, what you feel. Do you
feel that might makes right? No, I mean, like, like the whole point is if you, you either acknowledge
this fact and align yourself to it. I'm acknowledging the fact that I'm asking you how you feel.
How I feel, I feel like I want to be fair. You feel might makes right.
What do you, what do you mean by that? You say this ideology is good because it out-competes
the other ideology, which is might makes right. The ideology is correct because it out-competes
the other one. I just don't know how to define good otherwise. I don't know how to define good.
Okay. So you think might makes right that goodness is I think that
well, what, what are, what are our values? We think that this state of matter in our corner
of the universe, we should make it grow to make it and make it fault tolerant and for it to exist.
And, you know, as, as for as long as the time scale is possible, and, and, and we are willing to adapt
it so that it maximizes its growth, that's our, that's our core value, right? And then according
to that value, if you project down to subsystems, then subsystems that are higher fitness towards
growth are good according to that value system, right? And that's our, that's, that's kind of the
core thesis and, and you may disagree with it, but I guess that's like the, that's the EAC premise,
right? And it's, it's very funny to me because, you know, EAC like shitting on EA so much because
they have this weird, they are at least are accused of having this rigid, you weird utility
function with all those weird edge cases. Meanwhile, EACs, to your audience, please,
you know, scribe through the last 10 minutes of our conversation. It's like, I'm, I'm like,
we have, well, that last function is from physics, right? Like it just happens, like the,
so it's just, it is how it is the universe decided, lol, I guess I just have to do what God says.
Well, you can, again, like you can hate gravity and, and like you could try to fight it, but at
the end of the day it wins, right? Like, you know, the Rockets, we fight it for, we burn a bunch of
fuel to fight gravity, but then they come down, right? And so like you could, you could try to
locally fight this, this tendency towards, maybe it'll be helpful. Eventually things relax back to
the natural state of things. Maybe, maybe it would be helpful if I explained a little bit about how
I think about morality, at least we can hear it, see a little bit where I come from here. The way
where I think about, when I think about morality is I think people are very confused when they
talk about morality, when they talk about values most of the time. I think that when people say
something is correct, or good, or whatever, they usually mean one of three things, and these three
things are very different. The first thing is epistemological truth, or goodness, or correctness.
Like this is a true fact about reality, it's correct. It, it improves, if I take this fact
into my world model, my accuracy about future causal predictions will increase,
this epistemological goodness. Second is decision theory, goodness. Like this is a good idea.
What they mean by this is, if you do this thing, you will win at games more, in ways that you care
about. And the third one is aesthetics and values, is this is good because I like it. This is the
thing we're fighting for. Why do you like it? This is not part of aesthetics. This is a different
question. This is a causal. Why do we like order? Again, this is a causal, you're asking an epistemological
question. If you, there's an epistemological question about why do I have these aesthetics,
but it's not an aesthetic. It's an epistemological question. It's a different type of
ought. Like, so you can say, why do you have these things? And I can tell you a causal story,
but this has, doesn't have a type signature of a moral. It's not a moral thing. It's,
if the type signature is epistemological causal, I can tell you a causal story about my evolutionary
history, about my childhood traumas or whatever you want. Sure, I can tell you all these stories.
But ultimately, something has to plug into the like arbitrary things I like. And where those come
from, sure, like they come from here, they come from there, whatever this doesn't.
That's the point. So yeah, we're not prescriptive on what you should like. We're just saying,
this is the playing field. This is how the cultures are going to compete with one another.
And then given this fact, given this realization, it's like, you should try different ways to live
your life. But on average, just be mindful that the subcultures that are amenable and aligned
towards growth are going to be selected for. And so, for example, part of EAC, there's a sort of
builder subculture, where working on technologies that are of high positive economic utility,
and working very hard on these technologies, gives back to you because you have kind of fed
the technical capital machine. You've allowed it to grow and then it rewards you.
Oh, you fed Cthulhu. He'll be nice to you now.
I mean, yeah, you align yourself to civilization. You align yourself to civilization, you get
rewarded. Or you can go live in the woods and go full Ted K. And you're free to do that. And let's
see how the movie plays out, right? The subculture is that organize and live as part of civilization
and utilize technology in a clever way. They grow. The people that become Luddites,
they either become stagnant or die, right? So it's like, it's your choice.
As I like to say, people should have the freedom of choice for sure. Sure, sure, whatever.
Like, but like libertarians are like house cats, fully dependent upon a system they neither
understand nor appreciate. The fact that the techno capital system rewards you is not a fact of
nature. This is a very happy circumstance. I guess it's a projection of a fact of nature,
is my point. I think that if you consider corporations or nations as sub thermodynamic
subsystems, like it will self organize its components towards configurations that yield
better growth. So if you follow in the will of God, he shall reward the faithful. That's your
ideology. I mean, physics is my God to some extent. You can have your own other additional gods, but
to me, it's like, but physics, it just obey the laws of physics, like physics doesn't reward you.
No, no, it creates us. It created everything we love, know and love. It created the newer
chemistry you crave so much. It created your brain, it created you, every relative, everyone you
like. Yes, and it will also create everything that kills you. It will also make your parents die.
But that's the beauty of life, man. There's upside, there's potential downsides. You just gotta
play the game. It's like, why would you never get back? You won't have a perfectly safe, guaranteed
future ever. There is no guarantees in life. There's risk, reward, and you decide to play
their game or not, right? You can decide to participate in the techno capital machine,
take risks and have upside, or you can stay at home or be a grad student.
Is your God techno capital or physics? Which one is it? Because those are two very different gods.
It's physics and civilization itself, which is human techno capital means all pieces of
information and configurations of matter are part of physics, right? And then yes,
physics is larger than that. And it's a very specific subpart, which is civilization and
niceness and all these kind of things. Which one do you want?
Like, for me, from my perspective, I would much rather follow the God of civilization. I would
much rather follow the God of light in the dark. I would not follow the God of physics. The God of
physics is an earlier evil that doesn't care. No, I mean, like, physics, like, if we don't have our
stuff together, entropy wins, right? Life is a fight against entropy towards... Against entropy.
See, guys, not maximizing. No, it's a subtle argument about physics. Yeah, yeah, yeah. No,
but like, it's like, you know, so how life works, you know, it consumes neg entropy or it consumes
free energy to maintain its coherent state, to not totally thermalize and become maximal disorder.
But it's a constant fight against entropy. But then the bargain is that the house always wins.
And on average, by doing this fight, no, but seriously, by doing this fight and consuming
more free energy, you overall produce more entropy for the universe. And that's why we're
both fighting entropy, but as a byproduct, producing more, right? Okay, so, yeah,
quick physics lesson, but where are we going with this? I'd like to talk about...
Can I ask a quick question? Just before we move off to this, what you were saying,
Guillaume, is really interesting. So, you know, in the world, entropy or information dissipates,
but living things have this remarkable ability to resist entropic forces like almost any, you know,
unlike anything else in the universe, living things reach a kind of equilibrium. And you can,
you can think of just... Or they're in a dynamically maintained out of equilibrium state,
actually, right? Okay, okay. But I guess the question is though, what is your value system? I
mean, if everything's physics, Connor was just saying that there's a bright line between is and
ought, which is to say, I mean, he was an empiricist, and he was saying that morality is the one thing
that you can't find out empirically. There's something different about...
No, I think... So that's where I would disagree. I think that morality and value systems are kind
of arbitrary. They're hyper cultural hyper parameter settings. And I think that the subcultures that
have certain hyper parameter settings that yield higher growth will be post selected for.
And we should embrace that this is a fact and embrace variants and a constant search and
avoid monocultures, because if you suppress variants in the search and you set in a top
down fashion, you prescribe these hyper parameters, you're kind of ignoring potential other modes that
are more optimal, and you would be outgrown. And that will that could cause your your annihilation.
So might make right.
Sure. But just just a quick point. You're still describing as physicists do what things do.
And we're trying here to come up with a bright definition of why something...
I'm not prescribing any one way to live your life other than you have a choice. If you
optimize your culture and your policy of how do you live your life in a way that is aligned
towards this growth, you will probably be part of the future and you'll have higher mutual
information with the future. If not, then then you'll have lower mutual information with the
future. But we tend to crave in our notion of happiness, I think is a is a proxy for how much
we think we have influence over the future. But that's just kind of more hypothesis there.
But otherwise it's hard to define happiness.
Right. So what you're saying is again, might makes right is that you're saying there are
certain things that will be selected for. And this is a statement about reality,
cool, fine, accepted, whatever. And you don't and basically whatever that thing is,
that's the thing we should do.
I mean, if we if we don't do it, I think on a certain time scale, some subculture or some fork
or some adversary will do it. And if it confers a massive advantage, then that will win out.
Okay. Okay. But again, what do you think? What do you mean? What do you think we should do?
Do what should we do? Yes. Well, I try to enact my values, right? I mean, I work.
What are your values? I'm trying to figure out what your values are is what we're trying to find
out. I'm trying to scale civilization, right? I like so personally, my life's mission is to,
you know, achieve to increase civilizational Kardashev scale. So I'm working on
forms of intelligence that are, I'm not trying to actually replace humans. I'm working on like
physics based intelligence, like trying to understand chemistry, materials, you know,
fusion, nuclear fusion, carbon capture, you know, biology and so on. Like I'm not trying to just,
I'm actually not working on most anthropomorphic AI. I would say physics based AI is like an
extension of our capabilities. And to me, it's kind of funny that the organizations that, maybe
not yours, but the organizations that are pushing for AI safety are the ones causing the disruption
of 100% agree with that, by the way, like, like me and you handshake on that. Like you agree on that.
Good. Good. And it's like, okay, so you want to, are we trying to extend humans? Or are we trying
to like compete with them? To be fair, I do think that having human like intelligence will have
utility because our economy's already adapted to take in like, take in humans, hire humans and
plop them in and it's like, Oh, here you have an execution core here. If you just do human, like
intelligence here, you know, the system will work. And so it's just natural that, you know,
that has higher fitness in terms of products, because corporations are already made to take
human like intelligences and put them to use, right? So I understand where that comes from,
from an economic utility standpoint, but I have a different like my, I guess I'm more aligned with
Elon, where I'm trying to work on technologies that, that help us increase the scope and scale
of civilization. And so I'm working on like the hardest problems, you know, and trying to, trying
to extend notions of intelligence to tackle these hard problems. And for example, one thing I'm
concerned about is that, you know, policies backfire in ways you don't predict, like for example,
let's say you cap a compute for human like intelligence before we go down another just
one comment while we're on physics based AI, if you cap compute at a certain threshold for
LMS, that affects let's say drug discovery research where we might need way more compute
than that to do our research, right, or materials research. And that has a negative effect because
it's still AI and above that compute threshold, right? And so what I'm arguing for is let's be
very careful as to like what legislation we crystallize and let's have it be as light touch
as possible. I think there will be some regulation I'm realistic. But I do think that we have to be
mindful of like what we're going to affect in terms of potential positive effects of enabling
high compute AI research. Like we don't want to shoot ourselves in the foot with any sort of
regulation. I don't think what's been proposed so far has been good. So yeah, anyways.
Sure, sure, sure, sure. Fine. All good. But again, so I'm very intrigued by your answer to
this question. It's very intriguing to me, because it again looks like another bait and switch,
because the question was what are your values and what is EAC? And you said, well, EAC is about,
you know, maximization, you're taking the values that will be outcompeted, blah, blah, blah.
And then you said, well, I have these values about like maximizing humanity and whatever. Okay,
cool. What if if you're consistent, this would mean that if tomorrow I convinced you that actually
the most the strongest, the most growth maximizing thing is actually to work on AI that removes
humans, would you do that? You. I don't think that's the case right now. Okay, but like if it was,
if I made a really good argument for it and you're like, damn, that's a good argument,
would you do it? No, not personally, but why do you think that?
Well, I have self interest as well, right? I want, you know, so you don't even believe
any at yourself. Not really. No, I do believe in it. I mean, I'm working on technologies that
I think will massively. We don't believe in growth maximization. So this the point I'm trying to
make. No, I do believe in growth maximization, but you're just trying to paint a scenario
of what is not right. What I'm painting a scenario of is that you don't take your own
belief seriously. It's Fisher price. No, I take it very seriously. I live my values all the way.
I believe that you feel your values, but I don't think you intellectualize your values
all the way. Nick Land did. Nick Land went all the way. He said, if the techno capital machine
says everyone dies, then I'm the first to die. Hell yeah, let's go. He would not say, well,
I personally would decide not to. He was like, hell yeah, let's all die. It's not, you might be
correct. I'm not saying it's impossible that you're correct. I don't think it's the case, right?
But sure. But like, what if it was? But it isn't. So what? Like, what do you mean, man? Like, come
on, man, like work with me here. We're trying to actually. No, I think like, you know, I want to
support. You can't just say no, like that's not an argument. Come on, man. But you're saying,
like, we're all doomed if this happened. And I don't agree. And we're not even discussing your
model of that. I'm trying to see, I'm trying to understand your values.
We just have 50 analogies that aren't, you know, maximal inner product with reality.
And so, like. So, Conor, what would you do then?
Well, what I do, Jesus Christ, I mean, it's the world is high dimensional and very complicated.
The way I see things is, is that, never mind, like, I know AGI is the topic I talk about the most
and whatever comes the most pressing one, but I actually AGI is not the main thing I care about.
The main thing I care about is technology in general, and of which AGI is just the most salient
example in the current future. You know, 50, if I was born 50 years ago, I would care about nukes.
If I was born, you know, 200 years ago, I probably would have had a completely different ideology
because technology wasn't advanced enough yet. And the thing I fundamentally care about
is the stewardship of technology. I think of civilization as an entity of various levels
of coherency. It's a very schizophrenic entity. It's more coherent than it was 200 years ago.
Like 200 years ago, you couldn't really talk about a human civilization entity at all.
Like there was a bunch of like smaller things, like drives and nations and stuff that were like
sort of coherent a little bit, very low bandwidth. Now it's more closer to this. We're still far.
Like the UN is not a unified like governing body by any means, and neither is the West or China.
It's closer. But so the way I think about these kinds of things is that we can think of ourselves
as part of, you know, many levels like a continuous agency. Like, you know, we can think of us as
our bodies as people, but we can also think of us our bodies plus our social networks as people
plus our tools plus our information plus our culture plus whatever however you want to.
You can draw the boundary anywhere you want. There is no specific here is where you end and
other people start. It's a gradient. And a lot of what you would consider to be you is not in your
brain is in your environment. It's in other people. It's in your tools. It's everywhere. It's distributed.
And so in a similar sense, like the United States of America is kind of an agent,
kind of, it's not located anywhere in particular. It's not like, oh, there he is.
There's the USA. It's kind of, you know, you have a little bit of the USA. I have a little bit of
the USA, you know, and like various times and whatever to distribute a computation process.
And from my perspective, this is awesome. Like this is what gives us almost all the good things
we have is that we can work together on this. We can have gods and civilizations and memes and
cultures. Awesome. I love technology. I love living in a civilized state. This is awesome.
I would like this to continue. I would like our states to be more civilized. I would like our
institutions to be more competent and more coherent, etc. Now, can these things be predatory?
Duh. I'm from Germany. Yeah. Like, I understand. Like, yes, of course, things can go bad. It's
like we're genetically engineering, genetically engineering super beings. Like, of course,
this is dangerous. Like, if we were genetically engineering super tigers, people would be like,
hey, that seems maybe a bit dangerous. But let's talk about this control hierarchy. I really like
your picture of the world. I think we have a similar model of like, okay, you have these kind of
locuses of loci of control, and you have this sort of cybernetic control hierarchy.
And, and, and like, in a sense, like, you know, at every level, you know, there's a sort of
parody check, you know, or I draw analogies to error correction. That's where I came from,
came from quantum computing. In quantum computing, we try to engineer fault tolerant
computers, because we don't trust the execution of the program at each level of the computer,
including the, the, the level where, you know, certain parts of the computation are to keep
to check the computation at lower levels. And we've actually searched over architectures of,
okay, which ones work, which ones do not, right? If you have a central node that controls everything,
if you corrupt that central node, the fault propagates to the whole system. If you have a
completely non local decoder, there can be correlated errors, usually our lower likelihood,
but there can be correlated errors that then you don't know how to, how to, how to
feedback and control. So what is optimal is a sort of a nice hierarchy that's not too wide,
not too, not too thin at each level of the tree, like structure, where you have this,
this sort of hierarchy of cybernetic control. And my point is that usually there's,
there's usually a higher level of, you know, of the tree of power that keeps the nodes at a lower
level in check, right? But if you have one global central node at the very top, let's say a world
government or something that has power over everyone else or over every nation, what keeps that
in check. And the problem is if, if we're counting on democracy to keep that, that top
central node in check in the age of AI, that might be difficult because if they have a monopoly on
intelligence and they can steer information landscapes and engineer adversarily, the medic
perturbations that propagate, then they can manufacture consent for whatever the heck they
want to do, right? And I think we're both concerned with that. And I think we're on the
same page there actually. And so that's why I'm trying to understand, I haven't unfortunately read
like all your propositions with your organizations, but like, what do you, what is your proposition for
how to mitigate that possibility? If you're trying to have a sort of centralized research lab for
AI safety, which, you know, we do that for biosafety, we've seen how those centralized labs
have caused a lot damage. I haven't seen, you know, we talk about sort of adversarial actors in
their basement, you know, creating bio weapons. So far that hasn't happened. What has happened is
the big centralized lab have had a leak, even though they had the best intense, right? And so
like, what do you, what are you proposing here with like your framework for sort of centralized,
sort of secretive AI safety research? I mean, I, okay, let me, let me try to steal my like your,
your point is like, if we have a preview of what's to come in terms of capabilities,
we can best like, figure out policies to, to, to, to prepare for the advent of this,
this sort of technology, or, I guess, decide when to shut it down and ban it, which I don't,
I don't know how you would enforce that. But, you know, I'd love to hear your, your proposal, what
you're, yeah, what, what you're proposing here for how to, how to move forward, right? Like, like,
we can talk about, you know, fear and we don't know the future, it could kill us, whatever. It's
like, okay, but how are you going to, you know, what's your proposed antidote and is that going to
be a worse thing than the risks we're taking? Right. So I think you make a lot of really good
points. I like you thought thinking about like hierarchical cybernetic control, that's a fun
way to think about it, false tolerance, these kind of things I think are really good ways to think
about civilization. And this is a lot of how I think about civilization and these kind of things.
It's also why I think about like biology to a large degree, right? Like cells and, you know,
nerves and so on. There's like, I'm sure you're familiar with like, you know, like Levin's work
on like bioelectricity, like plenarian worms and stuff like that, how it's like hierarchical,
like cellular automata, like control systems in the body. I think these are all good analog,
not just analogies that are kind of like literally true to a large degree, is like,
you know, memetic systems or information exchange systems, they're distributed computation systems
and thinking about them this way can be a very productive way to think about them.
So when I think about policy, I'm not necessarily literally talking about
a bill that gets passed in Congress. This is often how it's a productive way of doing it.
The same way when I talk about medicine, I don't literally necessarily mean a pill you swallow
that might be a very useful and common way to practice medicine, but by far not the only way
to practice medicine. There's, you know, surgery and, you know, injections and whatever,
radiation therapy and whatever. And so I think the similar way is how I think about civilization
as well is the way I see things is that our civilization is just not able to handle powerful
technology. I just don't trust our institutions, our leaders, our, you know, distributed systems,
anything with, you know, hyper powerful technology at this point in time. This doesn't mean we couldn't
get to systems that could handle this technology without catastrophic or at least vastly undesirable
side effects. But I don't think we're there. I don't think there is like, you know, I wouldn't
even trust myself with like, you know, extremely uncontrollably powerful technology because I'll
probably fuck it up. I'll probably make a mistake. You'd have to have bad intentions. I need
control mechanisms. I need feedback. I need, you know, checks and bounds. So I don't make a mistake
even if I have good intentions. So even if I had AGI, I think this would be really dangerous.
I think it would be really dangerous if even only I had access to AGI, if only the government
had access to AGI. That's really dangerous. I think all of these scenarios are extremely dangerous.
There are no entities, no institutions that I would feel good about having access to vastly
more powerful technology. Same thing with nukes, by the way. Like, I wouldn't feel good if I had a
nuke. If I had a nuke, I would be like, who gave this to me? Like, take it away from me. Like,
I'm not, what happens if I sit on the button, you know? Right. I think we agree that current day
institutions are really ineffective and not necessarily competent enough to guide the world
and top down control everything. And I think, you know, what you're saying is you don't have
trust in our current day institutions. You're trying to propose better ones. My point is that,
you know, not quite there yet. I haven't made proposals that still haven't been proposed.
But let's talk about institutions in general. Like, I do think that if there is no
sort of evolutionary pressure on institutions, they tend to decay in fitness, right? Like if
there's no competition and, you know, government, right, is like an ultimate form of monopoly and
our form of sort of competition between different administrations is democracy. But now we've kind
of, democracy wasn't invented in the age of algorithmic information propagation. And we feel
like something is breaking currently. And so, you know, I guess my point is, I guess the EAC point
is, well, I think centralized institutions can't be trusted with this sort of power. And so we
shouldn't trust that anyone has the monopoly on this power. But we don't think that we can put this
sort of power back in the bottle or like the upside is too high, that every agent is going to want
access to it. And so if we accept that advanced intelligence will exist, which I guess we disagree
on whether we can control that or not. Like, you know, advanced intelligence will exist. It should
be in the hands of many, not concentrating the hands of a few that can use it to the press others,
right? And so I think we agree on that. But how do we, you know,
a lot of these intuitions are good, and not disagreeing with many of your intuitions. I think
I have some additional intuitions and additional models that met to other outcomes. So yeah,
what we definitely agree on is that like your current institutions are just like not up to the
task. Like if we just like continue our current institutions, we don't change anything, we keep
going, we run into a wall. Like I think we agree on that, like this is just not sustainable. And
in a sense, I feel like, at least in my perception, a lot of like San Francisco type,
you know, EAC-ish, you know, techno optimism is kind of like feeling a trauma response to decaying
institutions. We're like, you've been so traumatized kind of by like, you know, bad institutions,
rightfully so, by the way, that you're like, well, fuck it, I'm not going to work with
institutions at all. We have to like circumvent them. Like they're like inherently not good.
And I think this is a not unreasonable like positional, because it's the position I held
for most of my life. I understand where this is coming from. And but in a sense, it's kind of
like it's a utopian fantasy. It's kind of like the tech nerds kind of like fantasy that you don't
have to deal with people is that you can somehow circumvent dealing with people or dealing with
the messiness of institutions to make a good world. And I no longer believe that's true.
Like, I just think that, so if you take one thing away from me, if you take one thing away from
all of my beliefs, please take away that I think building a good world is actually hard.
There's no simple trick. There is, when I say actually hard, I don't mean like, well, we're
going to have to, you know, lift a heavier weight than usual. I mean, like, you know, solve an
extremely complicated high dimensional puzzle that we can like barely fit into our brain if
all kind of hard. I think it's, I don't think there's a simple solution. Like, you know,
a lot of people in the past believe stuff like free market, that's a solution. But what happens
you have free market monopolies form immediately and your free market collapse is not a stable
it's not a stable equilibrium. So now you need to do something that stops the monopolies from
forming, but stopping monopoly from forming needs some kind of like thing that's not the market.
So now you already have something which is not a free market, there's already some other thing
involved. There should be a market. There's should be a competition for institutions,
right? Like we should keep institutions in check by having alternative institutions and
the ability for alternative institutions to try to outcompete current institutions to keep them
in check. And that's all we're arguing for. We're not against like the existence of institutions.
We're saying today's institutions are failing us. And if the if we don't allow alternative
institutions to exist and try to out compete them, then we're stuck with them and we're in
a bad spot. And so it's all about encouraging variance and you're right. It's a high policy at
all levels is a very high dimensional dynamic optimization problem. And that is what we're
arguing for dynamism, always keeping this adaptivity and which is not necessarily present in
current day, you know, institutions. Yeah, totally understood. And like, I think this is a this is
a good distribution of reality. Like, I don't want to dismiss this as like, like, yeah, you're
observing true facts. I don't think you're crazy. I don't think the you know, some of your some of
your followers are crazy. But like, a lot of the people that, you know, follow the kinds of beliefs
you talk about, I don't think you're crazy. Like, I don't think some of you are. But like, I think
most of you are not crazy. You're seeing things with your eyes. You're, you know, you're, you're
seeing things with your eyes, which are real. You're, you're trying to solve these problems. And I
respect that, right? Like, I understand. I think the point here I'm trying to make is just like,
and then you have competition is not an atomic action. This is not a dial you turn up and down.
This is an extremely complicated thing you're asking for. This is what I meant by the free
market analogy. It seems like, well, if you just remove all the regulation, then you would have
the most competition. But in fact, that's not true. In fact, if you remove all regulation,
you just get new monopolies that form new regulation. So now you need something to remove
the monopolies, but that thing is in itself not free. So now you have to add in some control
mechanisms for that thing. We agree that a monopoly then inducing or achieving regulatory capture
is bad. And that's also what we're fighting in the present moment. We feel like it's happening
in that the AI safety discussion is being leveraged instrumentally for the regulatory capture.
And I don't even disagree with you here, by the way. Like, I do think that many people are taking
the AI safety discussion to be about regulation. I also would like to make clear that a lot of
people are not doing that. Like, they actually believe what they're saying. Please believe them
when they say that. Not all of them. Some of them are very cynical. But they are instrumental at the
end of the day. And I think so are the EAC people. Obviously, instrumental for people who just want
to take, you know, to perform, you know, all the monopolies and monopolies. Or for startups to
be able to compete for startup or whatever. But even then, like, you know, wanting, you know,
your open source models and whatever is very convenient for companies trying to shirk liability.
Like, there are many things that are very, there's a reason that there's more EACs at,
like, top tech companies than there are non EACs. Like, this is not a coincidence.
Like, it's like, it's because it's a useful ideology for capitalist systems. Like, of course.
The leaderships of the big labs somehow are more EA AI safety, which is kind of
interesting. And we see how having sort of a system that's in control that is primarily
ideological and not aligned with shareholder value can yield some pretty bad outcomes. I mean,
opening I almost imploded, right? Because there was a decapitation attack. And that's kind of what
we're, that's kind of what we're saying. It's like, either you have, you know, alignment with the growth
of the organism that you're controlling, or you're going to steer it in a self-destructive
direction. And that almost happened, right? And so... I mean, I don't, I thought you would be in
favor of institutions dying. I thought, why aren't you in favor of opening AI collapsing if it's
inefficient? No, I mean, I don't think it's inefficient. But I think that, you know, it had
leadership that wasn't beholden to shareholders, right? Because it's a nonprofit structure.
Would you think it's good if it collapsed then? Because then other systems which are more aligned
with your thing could take over. Isn't that the point of competition? No, I mean, I mean,
opening AI is not necessarily EA or IAC. I think, like, I think they're trying to strike a balance.
I'm trying to make a point. The point I'm trying to make is, it's not easy. Like, there's no,
it's not like competition is like a simple thing. It's not an atomic thing. It's a complicated mix
of things. There's a bunch of things in a bucket that we call competition, but it's actually many
different things. And you can get more... It's a search algorithm and maintaining the health of
that search algorithm, whether through legislation and competition. Like, that is our goal. I think
we're aligned there, actually. I'm not sure, but... So the way I think about this is personally,
is that it is a tool, in a toolbox. I think competition is one of the tools. I think it's a
great tool. I think it's a really good tool. I think there should be more competition in the world,
a lot more. Me and my friend, it's like, you know, Scott Alexander, in the past,
wrote a blog that a lot of EA's love called, in favor of niceness and civilization. And
me and a friend of mine wrote a post, again, a counter to that, which is against niceness and
for civilization. And which is all about how no competition is an extremely important part of
what makes civilization great. If we don't have... Like, you don't want a lawyer, two lawyers,
on opposite sides, being nice to each other, that would be terrible. You want them to be
as adversarial as possible. You want them to use every trick. You want them to take out every
shred of evidence. And when you have two companies competing for the best product, you want them
to pull out all stops. You want them to make the best product. You want them to compete down to
the margin. I totally agree. But... And here's the but. Here's the but. The but is, if you just do
this naively, you don't get good outcomes. You get monopolies, shitty products, you know,
environmental pollution, no externalities get priced, tragedy of the commons. This is what
happens, I empirically look into the past when we didn't have these kind of regulations. So,
you also need tools to address those things. And the market and competition doesn't do that.
Let's dig into that. I think that those examples are usually from sort of
regulatory capture and not allowing sort of incumbents to be disrupt, suppressing potential
disruptions. Tink incumbents doesn't keep their power in check, right? But how do you do that?
Like, how do you stop incumbents from doing that? Well, I mean, usually, right, if you really hate
the current monopoly, people really hate it and see a financial upside to creating a better
alternative that is better for consumers to pull their capital. What's that?
It's possible to stop that from happening. There's been many cases in history where just like a
monopoly is so big that the fixed costs are so large to get in, like AT&T, for example,
where the fixed costs are so large that no one can get in.
Well, we have we have problems with, you know, TSMC and NVIDIA.
Yeah, I'm not saying these problems are solved. My point is we should solve those problems,
but the way to solve them is not magically waving a wand with the word competition on it.
It requires actual concrete policies. Yeah, but if if if the incumbents deform the legal system
to deepen their moat, that's when the problems arise. And that's what we're trying to avoid.
My point is you don't need the legal system necessarily AT&T. Like AT&T didn't like they
also did regulatory capture. Sure. But like they also just had all the telephone lines.
Yeah, but now we have more than more than one provider.
Because the government intervened and broke them up.
Yeah, I mean, for the record, in some cases, I'm for, you know, anti-monopoly legislation.
But I do, I like I like I do think like EAC is about doing whatever is optimal
for kind of this growth and like accelerating that velocity of growth. And if you have an
incumbent that is abusing the market power and is doing something that's suboptimal,
like, yeah, I would be in favor of breaking them up so that there's a more efficient ecosystem.
In the case of OpenAI, I think right now was a bad time. I think 90% of the ecosystem
was depending on them. But I think that that little fluctuation was a good wake up call
for people to really price in the platform risk of putting all their eggs in one basket.
Right. And that maintains the health of the ecosystem.
Okay, interesting. So this is interesting to me. So this is mean that if
a government was to pass laws, which improve market mobility and, you know, competition,
whatever, this is like compatible with EAC, like this kind of regulation would be good.
So if so, you would say like someone going into government to create institutions and do regulation
in order to make a more stable or a more dynamic market would be compatible with the EAC ideologies.
That's correct. Yeah, we just think that currently the current discussion with AI regulation is led
by the current day oligopoly and they're the ones writing the laws. And so we're deeply skeptical
of everything being written right now, but we're not in principle against regulations
if they help acceleration. Right. Okay. I mean, so this is interesting to me. So this is interesting
to me because from my perspective, I think regulation is morally neutral by default. It's
either good or bad. It just depends on what the outcomes are. Like what does it do? So this is
a lot of how I think about it as well. I don't like government regulation or top-down violence or
whatever because it's cool. I like it when it works and I dislike it when it doesn't work.
So this is very similar to how I think about these things. So from my perspective, like I like
anti-monopoly laws, not for some ideological reason or because of some religion, but because it
results in better things that I like, it results in better products, it results in better life.
So it's interesting to me because it seems you come to a, you know, not identical, but like
similar ballpark of conclusions, but kind of from a slightly different perspective.
I justify it by the product. Like I like efficient markets because they make people happier.
I don't think efficient, though, that is a good question.
We could dig into that, but I think we should, by the way, but like,
and while it seems, if I understand you correctly, you're saying something like,
you like anti-monopoly laws because it makes the market more efficient. So you seem like
you're one meta level up. Is this correct? Yeah, I do agree with you that it yields better products
because if you don't have the market selective pressures applied at a more granular level within
your organization, you're kind of sheltering, you don't have that market signal that helps you
optimize your product and have a more efficient search over the space of technologies you can
perch on. So we're on the same page. Okay, cool. So again, I'm just probing and trying to understand
and so on. So if there was legislation that makes the market less competitive, but results in better
products, is this good or bad? I'm very skeptical of that being possible because again,
this is what you learn doing a startup. You have to make contact with the market.
Again, I'm not making instrumental claim. I'm not saying this is true or false. I'm saying like,
if you had some data, which is like really convincing on this, is there a moral aspect to
it or would you follow the data? Yeah, I mean, we should do what's optimal for growth.
That could include legislation, but I'm very skeptical that a global fixed very slow clock rate
hyperparameter setting could outcompete a bottom up like contact with the market,
very fast iteration cycle. I mean, there are examples of leaded gasoline. Leaded gasoline
would be an example. Sure. Yeah. But that we got enough data and we like to be clear,
like it was known that leaded gasoline was dangerous before it was released. Like the company,
the scientists did know that it was poisonous. This was a piece of information that did exist.
Same thing that like tobacco companies knew that tobacco caused cancer decades before they
publicly admitted that the data did exist. It's just it was hidden. It was and it was there was
a propaganda campaign and there was an attempt to stop regulation because it would make the
market less efficient. I think like we this is why maintaining freedom of access to information,
freedom of AI and compute, because I think that the future of memetic warfare, you need to have
your own AI augmentation to help filter through data and not be cognitively hacked. Like that's
why we think that's important. Like free access to information so that consumers at the end of
day, there's like a micro democracy with each company and consumers that have enough information
that could boycott or can they can demand legislation, right? And so that's why like freedom
of access and information, freedom of access to AI is super important. If only a few players have
access to AI and they can they can buy us the information they give you and then and then
they can steer you by proxy. And we think I think that that would be very dangerous, right?
So so am I correct in saying you think information is symmetries cars are market failures or cause
market failures? Yeah. Okay, this is a very reasonable position to hold. This is I agree
basically. I'm just interested in this. So like would you describe leaded gasoline as a market
failure? I think deforming like having an unhealthy landscape and a bias landscape of information
propagation induces yeah market failures, right? And here we see that if people weren't like if
you presented the information to people like real you know information not like you know extrapolations
that are yeah like if you present real information people can make their own minds and decide like
hey I don't want to support this and then they don't buy the product, right? And then this happens
all the time. Okay, so then I have a question. How the hell do we do that? That seems really
hard. How you want to like legislate that everyone has to say the truth all the time? Like
yeah well I'm trying to avoid weird attractors where you have this sort of top down control of
these information propagation mechanisms, right? Like if the government was in charge of social
media and biasing the algorithms that'd be bad we almost went there, right? That almost happened.
I mean don't worry the corporations are doing a bad enough job on their own.
Yeah but like you know if it was all centralized one party that's self the thing you got to remember
is like every system and subsystem is self-interested including the control systems, right? Including
the people we put in power and we got to keep that in mind but I just see a way forward where
potentially you have a sort of centralization of AI and then they use it to control information
propagation and they use it to adversarily you know memetically engineer, prompt engineer people
and then steer them in whatever direction they want. This is why I don't understand.
Why wouldn't corporations in the free market do this too? Like in your EACotopia, why don't you?
I mean we do that. We do human prompt engineering. It's called advertising. Everybody's trying to
subvert everyone else. Then why does this not lead to the same failure modes? Same failure modes?
I mean people are just trying to sell their product. Every agent is interested in its own
growth and it's going to try to subvert other agents. But like you're saying there's this problem
where like there'll be a you know the information landscape will be distorted and there'll be market
failures, etc. Why doesn't this happen here too? But I think like if people can have let's say there's
a power asymmetry because corporations and government have AIs that are very good at
prompt engineering humans. They just have more money, right? Like it doesn't even have to be
AI. Yeah but if you have let's say open source and you have your own compute and you didn't
legislate that ability, people can have sort of filtering like just like a spam filter is an
augmentation of your ability to filter through your inbox using AI. But they all have better AI
than you. So who cares? It doesn't matter. There's always going to be power asymmetry that maintains
the hierarchical order of power. Do you think the current order of power is okay?
I think we should search over hierarchical power structures of cybernetic control that
yield better growth for the whole organism. But I don't think there's going to be a fully
decentralized system. Like we are fighting, we're pushing the discussion towards that direction,
right? But the reality is that the optimum is a hierarchical cybernetic control system,
right? But then we can test like how much power we give any level of the hierarchy
and we should like dynamically test that and not just like immediately give all the power to
the top node because we're scared of what could happen in the near future. So what you're saying
is decentralization will not work is not the correct solution. It will have to be some form
of hierarchy. I'm not saying you know one node at the top by any means but it will be some form
of hierarchy. Well it's like the layered decentralization is a hierarchy, right? If you
have layers, right? People can roll ups anyways for more efficiency. Whatever, right? So you do
think that like every man is his own island wouldn't work?
Yeah, no. I don't think like that would be like you know maximally disordered system with very
low correlations to make an analogy. Like that's a high temperature phase. That's all entropic.
There's no order. Again, free energy is a balance between order and entropy, right? Again,
don't want to maximize entropy. That would be bad. Free energy, free energy. Yeah, I know.
Free energy, sure. Happy to talk about that but entropy is bad. Okay, so interesting and I
basically agree with most of what you say. Like this is how I think about these things as well.
I don't think we have found the best way of architecting our like coordination and information
exchange mechanisms on a civilization level by all. My fundamental belief, so you know this
all started by you asking me what are my proposals? My proposals is we can do better than this and we
should really, really fucking try. Like we should really fucking try. How do we find how to do that?
Right? Yep. And so one of my claims is that like we don't need to do a blind optimization process.
We don't need to do blind evolution. We're not that stupid. We've learned a lot. When the
enlightenment happened, you know, when French intellectuals came out of like democracy and
science and whatever, they didn't have game theory. They didn't have mechanism design. They
didn't have, you know, anything but rationality or psychology. Like obviously we can design better
systems in this. Obviously. Like obviously we have learned many things about how to design
better corporations, about how to design better voting mechanisms, about how to do better, you
know, you know, quadratic voting. Like obviously we can design better systems than what we have
right now. This is my claim. So, you know, I think like this is the eternal debate. Let's say
even in startups designing products, it's like, do I have like my own prior from either data history
or my taste? And then I use my model of the world to guide policy or do I assume I have
a non-informative prior and I just take in the data and adapt it in online basing fashion.
Like your point is that we should look at history and have a model of how the world
will evolve rather than having sort of uninformative prior and just letting the market-based
optimization rip and converge on the other. I'm saying we can and we would be stupid not to.
Like we have learned things and like the whole like uninformative prior is obviously not true.
There is literally no one who can, has literally uninformed prior. Like this is not,
there's no company that, you know, takes random atoms and smashes them together to
generate new products. There is no such thing. Like there is no such thing. Obviously everything
is based on priors and models that we have about what the world is. No one just randomly
generates products. That's not how the world works. And I'm, and I'm claiming that like we,
I'm not, I'm not saying, okay, and I figured it out. Here it is because that would be insane.
If I said I have the perfect utopian system that doesn't need to be tested, trust me, bro.
Obviously that would be crazy. What I'm saying is we can do better than random,
and then we have to gather data, then we have to try, then we have to test with reality,
we have to debate with people like you, we have to test these ideas, and you know,
maybe we find things we didn't think about, but we can do a hell of a lot better than random.
I don't, I don't know. I don't know, right? Like, it's like saying, like, can you predict the
stock market better, much better than random? Oh, I think actually this is a very different thing.
I think the stock market is a very different type of thing. Similar complex system, right?
No, I disagree. I very strongly disagree with this. It is also a complex system, but it's a
different type of complex system. There are many types of complex systems. You know, for example,
a gas, an ideal gas in the, in the container from the molecular perspective is a very complex
object. I'm sure you would agree. But yeah, but you could reduce, you have like some, some parameters
that you can predict mean field, but then you're arguing for like top down economic control from
like, you know, statistics, which arguing that there are symmetries, that there are things
which are not random. I'm claiming that like reality and culture is not encrypted. It's not
homomorphically encrypted. It's not maximally adversarial. It could get to that point if we
get AGI designing like maximally confusing, you know, you know, adversarial mimetics or whatever,
it could get to that point. But currently it is definitely not that. I'm claiming that the market
on civilization design is extremely not efficient. This is not the case, for example, with, you know,
short term S&P 500 stock prices. Those are quite efficient actually. But I'm claiming that the
number of people who actually try and actually put in the effort at like, you know, you are my
levels of education and IQ actually try to build new and better institutions and gather the data,
adapt to what they learn, learn new skills, you know, take in what we've learned in the 20th
and 21st century about behavioral economics and science and all these other things and actually
use the scientific method and turn it from the physical sciences and onto the problem of
institution design. I claim that, yeah, we have not nearly taken all the alpha that is there.
So I think your thesis is that your model has fairly good mutual information with the
actual future and then that, you know, if you have a certain number of bits of ability to predict,
I don't know, like you have good KL divergence, let's say, then we should enact policies that
leverage that information we have for the future to do optimal control. My point is that I think
that we may be overestimating how well we can predict the future. And so maybe not, like, if
we don't have good information about the how the future will go, maybe we should have a lighter
touch control, right? Because if you have uncertainty, if there's a larger divergence between your
model and reality, most things that you're going to do are going to be negative, negative, right,
impact, right? And so, but like quantifying the bits of certainty we have about the future,
and now we're discussing models of how the future and the economy will go, that we can discuss all
day. And then I think we should tune how strongly we enforce top-down control according to our
certainty about the future, right? Sure, but your argument proves too much. Your argument proves way
too much. Your argument, if you took it as, you know, what it implies, it should imply that engineering
is impossible. There's so many atoms. You know, how could we know that what happens if we move these
atoms in these directions? How could we know the insert these tools are mutual information bits of
our reality or two? And like, it depends on the complexity of a system. No, no, no, but there's
a notion of like how how difficult a certain complex system is to to predict. There are systems
claiming and making a strong claim that the amount of optimization pressure that has felt
gone from like people like you and me, you know, smart, educated, young, energetic, you know,
tech guys, into trying to design institutions is a fucking pittance compared to what has gone into
Facebook's ad algorithm. Sure, no, I do agree that we don't even know if it's hard. We haven't tried.
Oh, we need to create better institutions. But we should try. But I'm just very skeptical of
trying to give the keys to the future to current day institutions. Yes, me too. That's why I'm
trying to create new institutions. Well, I don't know. I mean, I don't know exactly. This is why
I need to know more about like your platform, what you're proposing to me. You know, you've been
talking to existing institutions and I don't know what's going on there. Right. So yes. And so
you can disagree with my proposal, you can think they're wrong or they're bad or I'm making a
mistake. I think this is a very reasonable thing to believe. I'm sure I'm making mistakes with my
current policy because the world is complicated and hard. And I'm sure some of the things I'm
currently proposing are net negative. I'm sure of this and I'm sure some of the things you will
believe that some of the things you're proposing are net negative. I'm sure you have that much
humbleness inside of you, at least I hope. And so that's all fine. I'm making kind of more the
meta point of like, what is my intention? How do I think about the process of doing good things?
The process I'm trying to argue when I talk about policy and I talk about coordination,
this is what I'm talking about. I'm saying, why should you believe that's something in this area
as possible? And my argument is, because no one has tried, you haven't tried. Have you tried to
engage with the institutions or build new ones? Have you? I've tried. I mean, okay, honestly.
I went to big tech, right? Before doing a startup, I was like, I'm going to try to
go to big tech and see if I could change these institutions from the inside. And what I saw
was that culture steers the institutions. And I was like, it's upstream. Good institutions
are downstream from good culture. And that's where I started culturally
engineering the world with EAG. Fantastic. So like, there you go. Respect, man. Like respect.
I mean, you're wrong and you're going to make everything worse, but respect when you tried.
Like, you know, you will say the same thing about me, obviously. And like, I, you know,
but like, okay, respect, like fine, you tried and you found some alpha. Wow, what a surprise. It's
almost like just like some dude from Quebec shitposting some shit on Twitter can actually
find a massive amount of alpha because literally no one is trying in the market. It's inefficient.
This is, is it not weird to you that just some guy from Quebec, you know, I'm sure you're
charming and smart and handsome and everything, but like, you know, created this alt and created
like this, you know, at least you said in the thing about the podcast initially started kind
of as a joke or like not too serious and became more serious over time. And in spite of this,
you got all this energy, all this free alpha, all this political power started flowing. Is
this not suspicious to you? Is it not weird how easy it was to get to this point? Is it not weird
that there is like, then why did no one else do it? Why was it you? Why aren't there 10 EAGs out
there? Why is it why? I think there's been movements that have high end or product over time.
But I think by construction, EAC is aligned with the natural tendencies of the techno capital
machine. And so we're trying to be like, literally the goal is to be aligned to the
natural tendencies towards growth. And so naturally, it's, it's going to confer it's
adherence like higher alpha for growth. And on average, that means the subculture will grow.
So it's been engineered to be this way from first principles. And that's why it's
successful. Maybe I don't know that don't let this go to your head, but you're severely
underselling yourself. Like, I think you don't understand how much of this is because you were
charming and you tried really hard. I think you were like my model of this is really that you
were underselling how much this would have not happened. If not, it happened to be you and you
happened to try like my model of the world is that there was a lot of there's a huge agency gap.
Like, yeah, a lot of the reason EAC exists as a movement, you know, people like fucking Gary
Tan put the stupid shit in there is their bio is because you were charming, you spent a lot of time
and you tried. I think we both agree though that we want people to have more agency and step up
like there are no adults in the room. Yes, coming to save us, right? I think for me that realization
my mid 20s was when I was the adult in the room in an important room. And my advice was used to
enact things that were high impact. That's when you realize, oh, man, like, it's just up to us
to step up. But I think like, I think our generation feels sort of like they don't they can't necessarily
penetrate an existing institution. There's all sorts of immune systems to sort of suppress new
entrance. And so we should create new ones. And I think that's what we're arguing for.
But it's not you're saying is just submit to the market, bro.
No, no, we want a market of institutions, right? That's, I don't know, I think I think you're
again, so okay, let me combine the points I made previously. You want a market of institutions
based, like, first of all, based, cool, awesome, loved. But now I'm also making the point that's
hard. You need to do policy, you need regulation, you need to work with current institutions,
you like you need to prevent all the market failure modes. How do you legislate, you know,
around like information? But we need to test our hypotheses of what works. And so we need
AB testing, we can't have one global policy, because then it's really hard to AB test if you
have like one central node prescribing things for the entire planet, right?
Is fine. And I think this is based and you should go work for the Charter Cities Institute or
whatever, right? Like, this is based great. I'm not denying any of this. I'm not if we could have,
I would think it was great if we just had like some countries were just like fucking everything
is legal and like, and they just try and like all medic sins are legal there and we'll we'll
see what happens. Based, right? You know, maybe not all of them, but like, you know what I mean.
And I think stuff like this is probably not good. Cool. All right, we've established this, like we
we institutions are dysfunctional. Super agree. If we give more power to dysfunctional institutions,
that doesn't make things better. Why would it? If we could improve institutions, I think we agree
this would be based. This would be great. Yep. And you propose the best way to improve institutions
is just to basically market based disruption, disruption, market based evolution, etc. Fair
enough. I think this is a reasonable position to hold. I'm not saying you're wrong. I'm making a
stronger case than it for agency, I think than you are. Or I'm thinking, I think the market is so
absurdly inefficient that you can just do better by just literally thinking about it and just trying.
But I don't have to defend this point if you strongly. Well, samples are costly, right? To
test your hypotheses of what a what a certain policy, what its impact will be in this complex
system, doing a rollout in the real world is extremely costly. And so you should be
Bayesian and have as rich of a prior as you can to be more sample efficient, right? I totally get
that, right? Yeah, it's yes. But I all we're pushing for is a bit of humility that our models might
not have as high accuracy as we estimate them to have, right? And I mean, of course,
this is a fully general piece of life. At some point, you should stop updating.
Like at some point, you should not get even more uncertain. You should. Most people are
overconfident. I think this is a very fair thing to say. But at some point, you have to stop updating
downwards. So my, at least going back to our hierarchical control structure, as you go up in
the hierarchy, there's less and less sort of like bits of information you have about the future at
larger and larger scales. It's very hard to predict the whole macro system. If you're talking about
the whole macro system, there's maybe a few statistics that you could predict, right? And
that's just a few bits. And so you should be lighter and lighter touch in your ability to control
like your top down feedback, that in a way that's kind of proportional to how many bits you have
about the future, right? And so that naturally yields this sort of hierarchical control system,
right? But like banning, having very precise description, prescriptions that are, that are,
that are legislated, you know, at like a national level or international level, like I'm just saying
we should really have very high certainty that this is the right move before we do something
like that. And I think right now there's too high uncertainty. And we should be cautious about making
a move we regret, especially given that legislations are so hard to walk back. Yeah.
Right. But I'm the one living in fear. Lol. Joking. But
it's all risk-reward. I don't know. I mean, yes, but like it seems like you're just
extremely terrified in a sense around like doing something wrong in this direction to a degree
which seems kind of unreasonable to me. Like I mean, we have a strong prior of how, like giving
too much power to governments and very few individuals how that can really backfire. And I
think there's a, there's a weird, there's a, there's a gap right now of mass uncertainty where people
are scared and they're, they're afraid, they're high uncertainty about the future. And that's
instrumental for some people to, to, to consolidate power, crystallize it. And then it's very hard
to disrupt that. And we want to make sure that, that does not happen. And that's why we're fighting
tooth and nail to be the counter force to that happening right now. Right. So you want to fight
tooth and nail from someone actually trying to create policy because it is high uncertainty.
I think a strong armed policy or policy that's too heavy-handed would be really bad. But like,
what does too heavy-handed mean? Like, I like, it's like, this is obviously up to interpretation.
It's obviously just based on like the whatever. And so, okay, but like, great. Like, okay, like,
okay, I'll come to the punchline. Sorry. Like I've been, I've been delaying the punchline the whole
time. Okay, let me get to the actual punchline. If you follow the policy you do, you don't do
anything that improves institutions and then predictably, some point, you die. You are completely
correct about what you're saying about the high levels of hierarchy. There becomes less and less
things you can say with great certainty over the long future. This is actually not exactly true. It
actually kind of also goes the other way around. For example, it is very easy to predict the energy
of a given system arbitrarily far in the future. It will be the same if nothing comes.
That's one statistic, one quantity. Not that many bits.
I know. I know. I know. I think this is a good analogy. I'm agreeing with you. I think this is
a nice analogy. So, I'm not claiming, I know on this date with this thing, this will go wrong in
this way using this technology, which will lead to an unrecoverable state. I'm saying, if you keep
just randomly rolling the dice over and over again with no plan to ever stop rolling or
removing the bad faces of the die somehow, then eventually, you roll death. Eventually,
you roll the X-risk. This is, I can say, with extremely high confidence.
I think that the dice is not identically distributed through time. But it will be
if you don't do something. If you don't do policy, if you don't improve institutions,
if you don't make better ways of... No, we should do all that. I just think right now,
it's far too early, and it's far too instrumental to select few to over-legislate while we're in
period. Then when is the right time? When is the right time? When we have a better prior on how the
when do we know? When do we know? When do I wake up and say, all right, now's go time. When?
Well, I mean, right now, I don't think it's right now because there's just too much
things are moving dynamically. But things will only get more dynamic in the future.
When does it stop? When does the buck stop? When do you act?
It's not going wrong. GPT-4, people thought, would melt the world or something.
Again, when do you act? What is the sign that when you see on the sky?
It's a continuum. There's no discrete point. So you don't have a plan. So you don't have a plan.
What you're saying is, you have no plan. You have no threshold to action. So you say,
we should just wait and see what happens, bro. Well, no, we should play it by ear.
That's what I'm saying. You don't have a plan. That's a different way of saying you don't have
a plan. Well, a plan is you're trying to do model-based control. You're trying to have a
long time rollout and then you're trying to do planning and optimally control things. But then
if your model has a large divergence with the future, then your optimal control is actually
highly suboptimal. And I'm saying because we're in a time of uncertainty, we should do things that
are on shorter timescales, only enact install policies that change things for short timescales
because the situation is so dynamic. But my point is, it will only get more dynamic. There will
never be. God comes from the heavens and all right, guys, now's the time. Well, that's why
capitalism is awesome. You have corporate policies adapting on different timescales, right? Like you
have corporate hierarchies and you make decisions of cybernetic control on different timescales
and you can change the sort of sampling rate of which you adapt. And that's why-
Why isn't that time now? Why isn't now the time? Why not? You don't have a plan. You don't have a
model. Maybe- I mean, I think every corporation is adapting in an online fashion to the landscape.
Sure, but if I say now is go time, now is the time for big regulation, now the time is for, you know,
that's a different question. You don't even have a way to evaluate it. You don't even have a function
to evaluate it. You just said play it by ear. You don't even have a way to evaluate it.
I do think that my point is like current institutions have such a slow clock rate
that basically- Yes, but what if I claim now-
Even if you have a good thesis, a good tangent right now, a model, a first order model of how the
future will go from like the local temporal tangent, like that first order model is going to be super
inaccurate in the future. And if you have a very low clock rate of adaptivity, hyper parameter setting
that you impose now and it's like crystallized forever, that's going to be highly suboptimal,
right? Like it's just, it's going to be net negatives, right?
But then at some point you have to act. Like you're just making excuses. This is just cope.
This is just cope for not acting now. But I mean, every company is acting and
adapting to the situation and incorporating AI. Yes, but if they're making a mistake,
if everyone is making a mistake and AGI is around the corner and it will kill everybody,
if this is true- No, well, we disagree on that, but anyways.
Yeah, sure, fine, but you don't have a plan, so I don't care. Like you don't have a plan.
You said you don't have a plan. You don't- I'm literally saying why
long-term planning is not optimal. Yes, and this is my point. You don't have a plan for
long-term or short-term planning. You don't have a plan. There is no plan. The only thing you're
talking about is, oh, we should play it by ear. This is not a plan. No, I think that the system
adapts, right? Like there's planning happening at the lower levels. The lower levels of the
hierarchical cybernetics control tree adapt on a faster timescale and are adaptive to the changing
landscape of technology today. I think that once we distill certain bits of information that are
less dynamic, we can crystallize policies at the mother node once that becomes apparent,
but right now there's nothing- And again, when does it become apparent?
When there's stability in a certain trend, right? Like when we see, okay, this keeps happening,
this is bad, and it has happened. Oh, like an increase in AI capabilities every year,
stuff like that? That's going to happen. That's just technological progress, but-
Oh, so that doesn't count. That doesn't count. But like, you know, like-
That's not bad. I mean, that's going to happen. I mean, I'm working on it, right? Like I'm working
on the hardware. Right, and that's not a sign that maybe there should be adaption or something.
That's not a sign. Well, no, but there's huge upside to it, right? Like, I mean, we can solve a
lot of problems in our world. No, but that's why, like, but then we would shut down all the upside
that we're leaving behind, saving lives, solving little warming engineering materials.
No one says- But we're literally proposing compute caps. So, like-
Yes, until we know how to deal with it. Like, look, if we knew how to, if you had a plan,
if you were like, all right, Connor, I'm glad you asked. Here's how we ensure that, like,
all the things you're worried about don't happen. They're not perfect. I might be wrong about some
of them. Where do you think we could improve? If you had said this, I would have been like,
I mean, first of all, I'm in a simulation, but assuming this had happened, then I would have
been like, all right, fair enough, but you don't have a plan. No one has a plan.
But I explained that I think at this moment, I don't think there's, it's time for a global level-
But you don't have a plan for when the moment comes. So, it's worthless. Your word is worthless.
The moment comes? Yeah, there's no moment. It's just, like, constantly improving technology
throughout society. And I think, like, if we make sure that there's power equilibration,
right, everybody have access to AI power proportional, let's say, to their access to
capital, and we don't create sort of these weird dynamics where you achieve regulatory
capture, you subvert the lower levels of the hierarchy, as long as we maintain sort of
not too high of a power gradient at any point in society, I think the system will adiabatically
adapt to constant rollouts of new technologies. That's opening eyes thesis, right, that constant
rollout of cutting edge capabilities, and then being incorporated is the most ethical
way forward, right? Yeah, and they're, they're wrong, and they're gonna kill us. Okay, that's
interesting. Like, it's not that hard. Like, it's not like, I don't, like, there's two, there's,
there's a meta point, and there's the concrete point. The concrete point is this, like,
lull them out. Obviously, this is bullshit. The meta point is just like, why do I care what you
have to say? You've literally admitted that you don't have a plan, and you don't know when you
will have a plan, or if you ever need a plan. Why, why do we need a plan? I think this is,
if you, if you haven't adapted, it's like, it's like saying, I'm launching a rocket,
I'm not gonna chart out all the values of the optimal controls at this stage, I'm gonna adapt it
according to sensors. Yes, but you have a flight path. You have a rough idea. You have an extremely
precise idea when you shoot a rocket into space, where it's gonna go, have you seen those flight
paths? No, but there, there's like fine tuning. Sure. Okay, look, if you and me were disagreeing
about the fine tuning, it's like, should the compute cap be at 10 to the 25 or 10 to 24? No, I,
great, then we could talk, but we're not talking about, again, there, there you have a very sharp
prior from the laws of Newtonian mechanics of like, where the trajectory will go. And I think we
disagree on whether we can predict sufficient parameters, and if we can't, the machine to
optimally control it. Sure. And if we can't predict an extremely high energetic, extremely
dangerous, weird system, you shouldn't predict things are good unless you have a very sharp
prior for why things are good. This is possible. But by default, you don't get nice things. Most
of the universe is cold and dead and lifeless. There is an observing life is extremely weird.
Well, so, so, so yeah, but that's the thing. Yak is based on the physics that underlies life.
And the point is that we think that the thing we can predict is that the system is going to
adapt to whatever. But life can also just die. Like life can just end like life isn't fair.
A meteorite can just hit the planet that's big enough. And then that's just it.
I mean, it did. And then we bounce back better than if it was big enough to just liquefy the
planet. It would just be over. And then yeah, but I just don't think that a constant rollout
of intelligence being progressively incorporated in our techno capital machine is going to be
that discrete event that causes mass disruption. I don't care if it's discrete or not. This is
irrelevant. No, I think it's very relevant because a complex system failure is when the system is
maladapted to a sudden change in the environment. And the point is if you keep things valuable
changing on a fast enough clock rate, as you adiabatically slowly change the effective
landscape, so constant rollout of new capabilities, then the system can morph to incorporate it
and incorporate this new reality and not fail. And I think that is literally what we're arguing
for, that crystallizing anything at the global level right now will cause later failure because
we're going to be maladapted to reality that we failed to predict accurately.
Do you think these words would have been much, you know, comfort to Neanderthals?
Um, see, well, you can talk about evolution. I mean, we're already getting steered evolutionarily
by the techno capital machine as is. So like, where do you draw the line? Like, okay,
should we go back to, you know, back to the cave and, you know, give up?
I'm asking you a question. Do you think these words would have been not good words to say to
Homo Neanderthalus? Do you think you would have been happy with that?
Well, I mean, you know, part of their genes live in us today. They've had some fractional
success in the future. Do you think that they're happy about that? Do you think they care?
Their selfish genes are. Yeah, but do they care? They were people. Do they care?
But I mean, even the Homo sapiens of that time, they're not alive. They don't, you know,
yeah, I don't care. Like all that's been passed on as genes, right? So sure. But like,
so what? Okay, you pass your gene and then everybody, everything, and every one dies.
Every institution dies. Sure. But I still care about people. I don't care about genes. So like,
if I, you know, if you go pregnant, a bunch of women and I shoot you in the head, are you happy?
Um, I think every, again, everyone dies and all you leave behind is your legacy.
I think you're pretty happy if you have high mutual information with the future
and it's secured. Uh, I think that's the basis for happiness, but that's just my theory.
Like if you have a lot of children, then you're maybe not scared as deaf. If you have a large
legacy and it's secured, maybe you're at peace with it. I'm not afraid of death. Like in general,
I just think like I'm going to do everything I can to impact the world in the direction I think
is good. Uh, while I have, while I am alive, but I'm not trying to be immortal. I don't know. But,
but that's, and that's actually, you know, point of fracture with an EX and people are,
uh, you know, against death, but I think death just like setting clauses.
I think death is an important part of this cyclical adaptation, just like dissipation is
in a thermodynamic system. Um, and so I think injection of constant novelty and fading out
of the old is, is, is, is important to maintain the adaptability,
which helps us converge onto bigger, grander, more beautiful things.
Well, I hope you're right. Um, sure. Sure. Feels like you're just letting, uh, you know,
taking letting Jesus take the wheel here. And I think my whole claim, again, if you take anything
from this, I think we can do better than that. I think you literally, you and me, not, there's
not a metaphor, like literally you and we should, we should chat. We should keep chatting. This was
really productive. Honestly, I feel like I understand your position better. You're starting
to understand us better. Um, I think I understand you better. I don't think your followers believe
at what you think they believe. Well, I mean, it's hard to tell, you know, off of, uh, Twitter
memes and so on, but you know, IMB, uh, you know, I may have written the manifesto and so on. And,
you know, I'm just saying my opinions and my model of the world here in this conversation.
But again, EAC is not a single, uh, point of opinions. It's kind of a cloud that makes it
sort of in alignment with our thesis about, uh, variants in all hyperparameters. But, um,
yeah, I mean, um, I think this was a lot of fun. I think we're nearing, uh, three hours at this
point. Maybe we'll split, split it up into more podcasts in the future. But, uh, you know, really
appreciate you, uh, taking the time. Um, yeah. No, thank you as well. I want to say this again.
I say to the beginning, I'll say it again. I really appreciate and respect that you take the
time to actually talk about your opinions, you know, as wasn't, wasn't an easy debate to have
and so on. And I grilled you pretty hard a few times, you know, nothing personal,
like truly nothing personal. Sure. Sure. No, I mean, I welcome. Yeah. Mimetic competition is good.
You know, you should roast my ideas and I should roast yours. And I think this is good. And we can
still respect each other people. So I never really fully revealed the punchline. I'm sorry
about like the thing I was trying to talk about. Yeah, there's a long buildup. I never released.
What do I think we should do? And like the thing is that like, I think this is where
I think we disagree, but like this might be after another podcast is that like,
I think competition is good, but we need is that what you should do is there needs to be some
things that are off limits, some things that have massive externalities that blow up everything,
that destroy a competition, whatever. And within that fucking go for it. Like optimize as hard as
possible. All niceness out of the window. But if you if you expand this to encompass literally
everything, you predictably end in disaster. This is what I call civilization. Civilization is not
about being nice. It is about we have some rules, you know, no killing the other guy, you know,
no poisoning. Do we respect those rules? How are they enforced? They need to be enforced by violence.
Yeah, but then who who keeps those people in check, right? It's always this is a good question.
You know, this is but this is a this is a design question. And I'm not saying this is easy, but
we've done hell of a lot better than random. Our current civilization living in the United States
or over here in London is a hell of a lot better than living in Somalia or whatever. You know,
there is plenty of more restrictions that we have here. There's plenty of things that are
more restrictive here. And I'm happy to take the hit. So for me personally, coordination
is about taking a hit. It's about saying I will willingly surrender some of the things they do.
For example, I can't go over to San Francisco and murder this guy because he annoys me because I
wouldn't because that's bad. I surrender this power, not that I would do that. Please don't
suggest that yet. Yeah. Yeah. Don't worry. I think you're stronger than me anyways. So you could
probably. But I think that it's neither all stagnation, nor all optimization. I think there
is a middle way. I think what we need to do is we need to cut off the tails. And then within
the things that are not the tails, we go fucking hogwild. I think we're we've already gone for
three hours. You know, if you want to respond to that or whatever, but I don't want to take
up much more time. Yeah, I think like we agree that institutional dynamism and exploring policy
is the way forward. I think we agree on that. I just think that in their current form, current
institutions are far too slow. And I think anything that anything we try to do on a short
time scale before we have new institutions probably will be net negative. And so that's
why we have a bias towards hands off for now. Let's see how the situation evolves. You may have
more urgency to take top down control reigns. I think we're still in a good region. Like I don't
see things going south in my model. We may have different models of future, whole different,
you know, podcast, different topic, podcast. But yeah, I mean, I'm happy we found some
some common ground here. And we got to understand each other's points a bit better. And hopefully
it's productive for our communities to have a discussion. And I mean, that's that's the point
of these things of having sort of taking political extremes is to have a discussion
and for people to make their own minds from from the discussion. So thanks again for taking the
time and and thanks for thanks to MLST for hosting. Yeah, thank you so much.

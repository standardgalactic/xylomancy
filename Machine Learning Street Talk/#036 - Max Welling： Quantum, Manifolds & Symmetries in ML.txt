Qualcomm AI Research is hiring for several machine learning openings, so please check
out their careers website if you're excited about solving the biggest problems with cutting
edge AI research and improving the lives of billions of people.
Today we got to speak with one of our heroes in machine learning, Professor Max Welling.
It was good, the questions were really fantastic actually and I've never done this with the
three of you but having a team of three people asking questions is really, it's a good idea
and of course you're really smart people knowing what you're talking about so that
went really well I think.
Needs three brains to match yours.
We asked Max some of your favorite questions from Reddit.
Hi Max, when will you be changing your last name to pooling?
Max has pioneered the discipline of non-Euclidean geometric deep learning.
So what is actually geometric deep learning?
It's the idea of performing deep learning or machine learning more generally but let's
say deep learning on data that is not Euclidean in some sense, so not a nice chain structure
for audio or a planar structure for images but perhaps a sphere or graph or something
more exotic like some kind of manifold with arbitrary curvature.
You might want to model weather patterns or social interaction data, there are many types
of data out there that are non-Euclidean.
If you've been playing with graph neural networks then you've already been doing non-Euclidean
or geometric deep learning.
So to make this work you just need to abstract some concepts.
So the Euclidean distance or your neighborhood, it becomes a function of connectedness.
Just like on a social graph am I connected to John, does John know Bob?
Simple as that.
Actually this kind of abstraction works in many areas of mathematics which Max will
get into today.
As well as making neural networks work on non-Euclidean data, the other thing that Max
has really pioneered is this idea of recognizing symmetries in different manifolds.
So in this blank slate paradigm that we have now in neural networks we're essentially wasting
the representational capacity of the neural network because we're just learning the same
thing again and again.
For example in a fully connected neural network we would have to learn the dog in the top
right corner and the top left corner because there's no translational symmetry.
And it was exactly this reason why convolutional neural networks were so powerful because they
introduced this concept of translational weight sharing.
So you had this filter that you could shine over the entire planar manifold and it meant
that those parameters could be reused and you could learn concepts in different parts
of the visual field.
It was an incredible breakthrough.
Imputing this kind of knowledge into a deep learning model, this is called an inductive
prior.
It means that we can take some prior knowledge about how things in the world works and we
can impute them into our models.
It makes our models more sample efficient and it makes them generalize better.
When it comes to sophisticated inductive priors, Max Welling is the king.
When we think about AI and its capability to actually help us to enrich our lives we
know we need to first help machines see and understand like humans do.
Take this drone collecting data in 3D.
Or this autonomous vehicle with cameras covering a 360 degrees view.
Current deep learning technology can analyze 2D images very well.
But how can we teach a machine to make sense of image data from a curved object like a sphere?
And because we want this processing to happen on the device itself for reliability, immediacy
and privacy reasons, how can we achieve this in a power efficient manner?
It turns out we can do this by applying the mathematics behind general relativity and
quantum field theory to deep learning.
Our neural network takes in data on virtually any kind of curved object and applies a new
type of convolution to it.
We can move the shape around and the AI will still recognize it.
This is just one example of the exciting research we're doing at FalkaMeia Research to shape
AI in the near future.
Anyway, it turns out that these symmetries are absolutely everywhere.
If you wanted any further proof of how useful these kind of equivariants and symmetries
and manifolds can be, look no further than the recent announcement from DeepMind AlphaFold.
It will change everything.
DeepMind solves 50 year old grand challenge.
The game has changed.
So proteins are the structures that fold in a given way.
The results of this year's competition came out and they looked something like this.
Namely, every entry here you see is a team participating in that competition of protein
folding prediction.
And there is one team which is DeepMind's system AlphaFold 2, which completely dominates
all the others.
To the point where the problem is now considered to be solved.
By the way, if this is not a great meme template, I don't know what is.
Just saying.
Just saying.
They say a folded protein can be thought of as a spatial graph.
This here, attention-based, okay?
So I'm going to guess for sure that they've replaced this ConvNet with a transformer
style with an attention layer or multiple attention layers.
I would guess this is a big transformer right here.
So there was a really interesting article that came out called AlphaFold and Equivariance
by Justas Duparas and Fabian Fuchs.
I'm so sorry Fabian, I don't know how to pronounce your name, but it does sound like
a swear word.
Justas and Fabian Etau comment on the announcement from DeepMind and they said, in short, this
module is a neural network that iteratively refines the structure predictions while respecting
and leveraging an important symmetry of the problem, namely that of rototranslations.
At this point, DeepMind has not yet published a paper, so we don't know exactly how they
address this.
However, from their presentations, it seems possible that part of their architecture is
similar to the SE3 transformer.
What's the SE3 transformer?
Lo and behold, our friend Max Welling has had his hands all over it.
So in the abstract, it says, the SE3 transformer, a variant of the self-attention module for
3D point clouds and graphs, which is equivariant under continuous 3D rototranslations.
Equivariant is important to ensure stable and predictable performance in the presence
of nuisance transformations of the data input.
Right.
By the way, you might be wondering what SE3 is.
Let's have a quick look at the Wikipedia page.
We are getting into group theory, which is quite an abstract concept in mathematics,
but the Euclidean group, which is SE3, it talks about all of the symmetries or the group
transformations that can be applied to Euclidean data to preserve certain properties, namely,
let's say the Euclidean distance between two points.
Well, these are things like translations and rotations and reflections.
It's very interesting that you can kind of abstract one level up in mathematics, and
that's what group theory is.
The other comment I want to make is that all of these folks are independently amazing.
I've watched presentations by most of them, so that there's Fabian Fuchs, Daniel Worrell,
Volker Fischer, fantastic.
By the way, when we look at Fabian's About Me page, he's a machine learning PhD student
at Oxford University.
His research topic is learning invariant representations, simply put, where most of deep learning is
concerned with finding the important information in an input.
He focuses on ignoring harmful or irrelevant parts of information.
These can be important to counteract biases or to better leverage structure in the data.
Structure in the data.
That's interesting.
That's quite a cool point, actually, because if you think about it, you could naively,
if you're doing a vision classifier, you could naively just look at all of the pixels and
what they are, or if you're being smart about it, you go one level up and you look for the
hidden structure in the data.
That is precisely what he's talking about, things like the symmetries that are inherent
in pretty much every type of data.
One last thing, DeepMind released an official PowerPoint deck on AlphaFold 2 and it talks
about they're on a long term mission to advance scientific progress.
Now, here are some of the protein examples.
Now they specifically call out inductive biases for deep learning models, where this is exactly
what we're talking about.
So clearly convolutional neural networks are one such bias, which has the translational
weight sharing.
It talks about graph networks and recurrent networks and indeed attention networks, which
is very much a generalization of pretty much all of the others.
They say that they are putting their protein knowledge into the model.
So physical insights are built into the network structure, not just the process around it.
And these biases reflected their knowledge of protein physics and geometry.
And you can see here that there are residues in a protein.
So they're modeling topologically which residues are connected to which other residues in this
kind of 3D space.
They specifically call out here on the structure model page that they are building a 3D equivariant
transformer architecture.
So anyway, if this doesn't motivate you that symmetries and manifolds are an exciting idea
in deep learning, I don't know what will.
So clearly Max has been in this game for a long time now.
Back in 2004 with Kingma, he invented the variational Bayes autoencoder.
It's only relatively recently that Max has been focusing in on deep learning.
Clearly like any other field, also machine learning is subject to fashion, right?
And so there is a five to ten year cycles where people get really excited about a certain
topic, either because the theory is very beautiful or it just works really well.
I started in biographical models and independent component analysis was the talk of the day
and the support vector machines and basically nonparametric methods and then came Bayesian
methods and nonparametric Bayesian methods and now it's all about deep learning.
So what you see is that the field is subject to these sort of fashions and I think it's
fine because we zoom in a new very promising tool and then we work it out and we get the
most out of it.
Max is a vice president at Qualcomm.
So clearly he thinks that computation is going to be absolutely critical for the future of
artificial intelligence but having said that he also thinks that we need to be more efficient
with our hardware tomorrow than we are today.
That's just a reality that we all have to accept.
So the more compute we throw at it, the bigger we make our models, somehow the better they
perform and we don't know precisely why that is but we do know that they will use increasingly
more energy to do the computations for us and at some point that's just not a viable
economic model anymore.
We'll see a continuation in making deep learning and machine learning more energy efficient.
So there's a really interesting interplay between priors, experience and generalization.
We want to have machine learning models that generalize really well to things that they
haven't seen during training.
If you move them into a new orientation or in a new situation in the context and that's
what we think of when we say artificial general AI which means not just something you train
on one specific topic and then you ask it to do that and it does it very well but if
you then move it into a new context it just completely fails as narrow AI.
So humans are clearly much more flexible if you learn something in one context and then
when you get put into a new context that we've never seen before we can still do very well.
And so we want our agents, our artificial agents also to have this property.
Max is also a huge proponent of generative models.
He thinks that generative models might be the future of artificial intelligence.
So funnily enough I think Max and Carl Friston that we had on a couple of episodes ago I
think they would see eye to eye.
Basically what everybody else in the scientific community does which is write down a model
of the world which we call a generative model which is how do I imagine that the world that
I'm seeing in my measurement apparatus could have been generated by nature.
We all have the matrix going on inside our heads.
We are running simulations of reality and we're kind of integrating over the expected
value of those simulations.
This is just something that we do all the time.
That seems to be the real trick for intelligence at least in humans so our ability to generate
the world.
Max also thinks that we need to be learning causal relationships in our models.
Causal relationships have this really interesting property that they generalize better.
So Max comes up with this wonderful example of a certain color of car in the Netherlands
might be associated with a higher accident rate but that probably wouldn't generalize
very well to other countries because it's just a colloquialism whereas male testosterone
levels that's a causal factor and that's going to generalize far better to other countries.
So try to figure out what the true physics of the world is, what causes what and if you
have this causal structure of the world you understand much more about the actual world
and then if you move it to a new context you can generalize a lot better in this new context.
At this stage Max has bet on so many winning horses that you've got to wonder how the hell
does he do it so we ask him what his secret is.
It's incredibly hard to predict what will become well known.
Sometimes you just happen to be working on something that takes off like a rocket.
When we did things like the VAE or graph neural nets it didn't feel at all like this was going
to be a big hit.
When we read some of the research from Max's students we were just blown away.
Sometimes we've got to just remind ourselves that these are fairly young folks that are
in their early 20s that they've just come out of university.
How is this even possible?
I've been very blessed with being able even with my industry funding to provide this level
of freedom to the students and I think this is really key.
So one of the things we asked Max was how does he select his research directions?
One of the interesting things is that he's a physicist right so many of the things that
he's been doing are straight out of his operating playbook from the physics world.
So things like symmetries and manifolds and even quantum.
Symmetries have this deep feeling right?
Symmetries pervade basically all theories of physics and they have this profound impact
on how you formulate the mathematics of a theory especially when it becomes almost mysterious
right?
Quantum mechanics is almost mysterious.
How on earth is quantum mechanics possible?
The fascinating thing here as we discussed on our GPT-3 episode is that many of these
roads actually lead back to computation itself.
How does the brain computing also feel like a very deep question right?
How do we even compute things?
What is computation even and does the universe compute its solution?
What does it mean to be predictable?
Can you compute faster than the universe can compute?
One of the key concepts that we talk about in the show this evening is the bias variance
trade-off.
Nothing comes for free.
There is no machine learning without assumptions.
You have to interpolate between the dots and to interpolate means that you have to make
assumptions on smoothness or something like that.
These prior assumptions will help you transfer from one domain to another domain.
One of the topics we've been discussing a lot on machine learning street talk recently
is this notion of how far can we take data-driven approaches?
Will they take us all the way to AGI or is it just like building a tower and trying to
get closer to the moon?
Perhaps we could generate more data with data augmentation or even a simulator.
Perhaps we could use data more efficiently with machine teaching or active learning or
some kind of controller on how we train the model.
But ultimately, how far can we really go?
The big question in some sense over time is can we simply take the data-driven approach
and extend it all the way to AGI?
So Max tells us about all the different schools of thought in the AI community.
And of course, one interesting school of thought is the likes of Gary Marcus and Wally
Subba that we had on the show a few weeks ago.
These people think that we need to have an explicit model of the world.
And then on the other side, we just want a classical AI sort of community, which is no
no no, that's going to be ridiculous.
You will never be able to do that.
You really need to imbue these models with the structure of the world.
In the show, Max tells us where he's placing his bets, but we're not going to spoil the
surprise.
So as we said before, Max is extremely well known for creating these inductive priors
and putting them into machine learning models, helping them generalize better and be more
sample efficient.
The whole endeavor of machine earning is defining the right inductive biases and leaving whatever
you don't know to the data.
If you put the wrong inductive bias in it, things can actually deteriorate.
We talk about Hinton's capsule networks.
They tell you, well, we'll just keep the abstract nature of what we want, which is some stack
of things that transform in some way that we can vaguely specify.
And then we ask it to learn all these things.
We talk about Professor Kenneth Stanley's Greatness Can't Be Planned Book and also Sarah
Hooker's The Lottery Paper.
The thing that both of these ideas have in common is that they posit that we are locked
in by the decisions of our past.
And I do feel very strongly that as a field, we need to open up.
So we should value original ideas much more than we currently do.
So Professor Kenneth Stanley has a fascinating take on this.
He thinks that we should be treasure hunters.
We should find interesting and novel stepping stones that might lead us somewhere interesting.
He thinks we should do this in all aspects of our lives.
So we all want to monotonically increase our objectives.
And what we should be is treasure hunters.
Yes.
Science should be about exploration, not exploitation.
How do we extend this to peer review in science?
Ironically, having a consensus peer review encourages group think and convergent behavior.
If we genuinely want to have an exploratory, divergent process, we should almost optimize
for people disagreeing with each other in the peer review process.
I think the reviewing in our community is far too grumpy.
I'm continuously amazed when I read these old papers from, let's say, Schmidhuber and
like the first RL papers that just came up with a bit of an idea.
And then they had a bit of toy data and writing that's a paper and it's cool.
There's a dichotomy between, on the one hand, having a stamp of approval, having a paper
published and presenting about it.
And on the other hand, having a continuous stream of research, which is peer reviewed
online and with some accountability.
Yeah, I think we really need to disrupt the field a little bit.
Machine learning is a bit of a mystery to most people I feel, including myself.
And even though I learned something in this conversation paradoxically, it's more of a
mystery than before the conversation.
Crucially, Max thinks that quantum computing will hugely impact the machine learning world
in the future.
So you can think of quantum mechanics as another theory of statistics in some sense, right?
Actually, quantum neural networks have nothing to do with particles necessarily or physics.
It's applying the math behind quantum mechanics to machine learning and building neural networks
as layers of functions of these quantum operations that forward propagate some signal.
As Max describes really nicely in this conversation.
This is the counterintuitive part, which is you can have a probability for an event or
an amplitude for an event, and then you have an amplitude for another event.
And you would think that if there's two probabilities for that event to happen, then the probability
of that event should grow.
But in quantum mechanics, they can cancel and then the probability is suddenly zero that
the event happens.
So this seems bizarre, but nature has chosen this theory of statistics anyway.
I really felt like an ELI 5 here.
Instead of calculating with probabilities, you calculate with something like the square
root of probabilities and thus events that can only stack in classical probability theory
can all of a sudden cancel each other out.
And that gives rise to really interesting math.
We talk about Max's recent quantum paper that just got released.
So that was a paper that we recently pushed on the archive, which is quantum deformed
neural networks, which we basically first say, okay, what if we would take a normal
neural net and implement on a quantum computer and then we slightly deform it into something
where states get entangled.
So by doing it in this particular way, we could still run it efficiently on a classical
computer.
What this paper here did was to build a particular type of neural network of quantum neural network
that can under the correct assumptions be simulated efficiently on a classical computer,
but also once we have a quantum computer, it can release its full power basically.
If you want to do classical predictions, does it actually help to build a neural network
that can run efficiently on a quantum computer that can do these predictions much better?
Can you write down maybe even normal classical problems more conveniently in this quantum
statistics?
I found the conversation with Max to be extremely helpful here, and he does a great job of explaining
what's going on.
Max has another exciting paper out, probabilistic and numeric convolutional neural networks.
It's a paper by Mark Finzi, Roberto Bondeson, and of course, Max Welling.
And it looks at what you can do with computer vision models if you move away from the assumption
of discreetly sampled pixel grids and move to a continuous representation that's more
like what an actual object in the real world projected on a screen behaves like.
The observation is when we write down a deep learning algorithm, let's say, for an image,
then we sort of treat the image as pixels, and we think that's the real signal that we
are looking at.
But you can also ask yourself, what if I remove every second pixel?
Now, actually, I have a very different neural network, but should I have a very different
neural network?
Or what if the pixels are actually quite randomly distributed in the plane?
Just some random places where I do measurements, maybe more on the left upper corner and fear
on the left lower corner.
The predictor should behave in a certain consistent way.
And so, of course, then you come to realize that really what you're doing is with a pixel
grid is sampling an underlying continuous signal.
So to get away from this assumption of this discreet even sampling, they use these objects
called Gaussian processors to model the data.
And a Gaussian process, it's basically a universal function approximated, like in your network.
But it gives you a measure of uncertainty, and the reasons you might want to do this
are many, but in short, it allows you to average over every possible model that's described
to data and gives you a better result.
In doing so, you can start to do really interesting things like subpixel sampling or work with
very sparse locations.
But in order to do that, you need to reconceptualize a lot of the familiar operators that work
on our linear algebra representations, such as like the convolutional translation operation
of our weights.
The way they got around this was super interesting.
So there's a very interesting tool which is called the Gaussian process.
It's basically interpolates between dots, but then places where you don't have a lot
of data, you create uncertainty because you don't know what the real signal is.
What does it mean to do a convolution on this space?
The most interesting way to describe that is by looking at it as a partial differential
equation.
So they reframe this transformation as a differential equation that could just be parameterized,
calculated out in a closed form, and directly applied to the parameters of the model.
That means you don't need to like do any sampling or anything like that.
You literally just calculate this thing, apply it.
It will be worth going into the differential equation stuff by itself, but it gets very
complicated very quickly.
Needless to say, it generalizes not just translation, but also things like rotations
and scaling.
But the way that they really did this was by finding very clever representations.
It boiled everything down to normal distributions, or almost everything could just be done in
closed form, which things have been done with the Gaussian processes in the past, but they're
typically computationally expensive.
So if you can do all these updates without constant re-computation, then that's a huge
computation and an advantage.
The paper does some really cool things.
Some of the benefits are now that, first of all, of course, you cannot work on a unstructured
set of points.
It doesn't have to be a grid.
And you can even learn the positions of those points.
So you cannot direct the observations in places where you really need to do observations in
order to improve your prediction.
So it turns out that all of this can be remapped back onto the quantum paradigm.
I must admit, I'm almost gutted that I didn't study physics at university.
Physics seems to be one of the most robust scientific disciplines.
And the folks are just so smart because it's really, really difficult.
What I notice is that it's very, very difficult for external folks to get anything published
in the physics world.
But there's an asymmetry.
The reverse isn't true.
Loads of these physicists are coming into the machine learning world, and they're just
implementing all of these things, whether it's symmetries, manifold, topology, chaos.
It's really, really interesting to see this unfold.
We also get a take from Max about GPT-3.
And so you say GPT-3 isn't very good, maybe, but it's a receding horizon, right?
I had a chat with my old colleague from Microsoft, Ilya Karmanov.
About 18 months ago, he introduced me to Max Welling's work.
It absolutely fascinated me ever since.
And guess what?
Ilya left Microsoft, and he went to Qualcomm.
Hey, Tim, how's it going?
Ilya, it's going great.
How are you?
I'm good.
I'm doing a different job, different universe, it seems, but I'm doing pretty well.
Ilya and I used to be work colleagues at Microsoft UK, and I left Microsoft about a year ago.
And actually, you left as well, didn't you, Ilya?
Yeah, we have a joint pact.
It was like, you have to keep both of us or we leave.
Indeed.
Now, Ilya and I made a YouTube video just over a year ago, and it was all about Max Welling's
work with Tako Kohen, all about symmetries and manifolds.
And this work was hugely inspiring for me.
How did you discover it?
I discovered it because my colleague Matthew and I, whom you also interviewed, and you
should follow up with, Robert, we were at iClear, and we saw Tako's talk about spherical
CNNs, which was a bit late already into his work, which started with group equilibrium
convolutions.
And I think both of us just thought it was really cool.
It was our favorite talk for the day, because it was so different, and it felt like it was
setting up a different stream of research.
It wasn't necessarily about chasing SOTA.
It was just about really improving, taking what makes convolutions great and making them
even better.
And that was awesome.
Oh, amazing.
Well, we made that video together on Machine Learning Dojo, and I must admit, it was hugely
inspiring for me.
And I reached out to Max Welling about two months ago, and he actually came onto our podcast.
We interviewed him yesterday, but yeah, it all came from you.
You introduced all of this stuff to me, and I've been going through some of Max's work
with some of his recent students.
And it's just incredible, because he came from the physics world, and all of this knowledge
that he has around quantum and symmetries and topologies and manifolds, that's his operating
playbook, and he's just taken it into the machine learning world, and he's just been
executing on it.
Max is involved in a lot of papers, as you would expect, and a fair few of them are really
fascinating.
Yeah.
One of the things we spoke about was just how he nurtures his PhD students, because
some of these papers are just incredible, and presumably these students have gone from
nothing to producing that level of research in a very short period of time.
But presumably, this was one of the reasons why you decided to apply for Qualcomm.
Yeah.
I was chasing something that was publishing papers in the field of computer vision, and
it's one of the places in Europe, perhaps Zurich is another location when you have this
kind of research.
I thought it was extremely different and a super interesting research area that I sort
of speak to get into.
Fantastic.
And what are you working on at the moment?
We have just submitted actually our paper to CVPR this morning.
The deadline's in a few days, so that's pretty good, I think.
And then maybe after that, as well, we have a few more topics in video, basically self-training,
how to improve representation learning, it's a mix of knowledge distillation and self-training.
And then also, we have some interesting work with radio signals, so it's like video in
the sense that it's from that we extract the spatial and temporal signal, but it's
extremely different to video, and that also makes it super fun.
Amazing.
When I was discussing machine learning with Ilya at Microsoft, we were fascinated by
3D convolution on your networks and I3D and video action detection, and I know you were
working on a 3D segmentation and a whole bunch of cool things like that.
But anyway, I would love to get you on the show in the next few weeks to talk about some
of your research.
For those of you in the comments, if you want to have more from Ilya, let us know.
Are you going to give us a demonstration of your front lever?
Okay, single leg.
When Ilya comes on the show properly, we're going to be doing a front lever competition.
That's pretty good.
So not only is Ilya a specialist in machine learning, he also absolutely smashes it in
the body weight game.
No, that wasn't smashing it.
That was after a climbing session.
It's actually really cool.
I met this guy here who's a calisthenics instructor called Solly and he just started
climbing.
Yeah, so we met up and we went climbing this morning and he was crazy good as you would
expect and he gave me some tips on my front lever as well.
He was saying I should work more on the tuck instead of a single leg.
So hopefully you'll see much better than that in the future.
Amazing.
Ilya, thank you so much for coming on the show.
We look forward to interviewing you in a few weeks' time.
Thanks for having me and thanks a lot for interviewing Max.
I'm super excited to see that in a few days.
Anyway, I really hope you've enjoyed the show today.
This has been such a special episode for us because Max Welling is literally one of my
heroes.
So anyway, remember to like, comment and subscribe.
We love reading your comments.
We really do actually.
We're getting so many amazing comments in the comments section.
So keep them coming and we will see you back next week.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my two
compadres, Alex Stenlake and Yannick Kiltcher.
And today we have someone who doesn't really need any introduction at all, clearly one
of the most impactful researchers in the ML world and has as near as makes no difference,
40,000 citations.
He's on the executive board at NeurIPS.
He's a research chair and full professor at the AMLAB University of Amsterdam and co-director
of the CUVA lab and Delta lab, Max Welling.
Max is a strong believer in the power of computation and its relevance to machine learning, which
is one of the reasons why he holds a vice president position at Qualcomm.
He thinks the fastest way to make progress in artificial intelligence is to make specialized
hardware for AI computation.
He wrote a response to Rich Sutton's The Bitter Lesson, but essentially agrees with
him in the sense that one should work on scalable methods that maximally leverage compute.
But Max thinks that data is the fundamental ingredient of deep learning and you can't
always generate it yourself like in AlphaGo, which amounts to an interpolation problem.
Much of Max's research portfolio is currently based on deep learning.
He thinks it's the biggest hammer that we've produced thus far and we witness its impact
every single day.
He thinks that AGI is a possibility and it will manifest in a forward, generative and
causal direction.
There's a really interesting cross-pollination story here.
Max has a physics background.
He did a PhD in physics.
He knows all about manifolds and topologies and symmetries and quantum and actually this
has been his operating playbook.
He's brought all of these incredible concepts in from the physics world to machine learning.
Now there's a fundamental blank slate paradigm in machine learning.
Experience and data currently rule the roost, but Max wants to build a house on top of that
blank slate.
Max thinks that there are no predictions without assumptions, no generalization without
inductive bias.
The bias variance trade-off tells us that we need to use additional human knowledge
when data is insufficient.
I think it's fair to say that Max Welling has pioneered many of the most sophisticated
inductive priors and deep learning models developed in recent years.
An example of an inductive prior is the CNN, which means we can model local connectivity,
weight sharing and equivariance to translational symmetries in gridded vision data.
This is imputing human domain knowledge into the architecture.
It makes the models significantly more robust and sample efficient.
Assumptions are everywhere.
Even fully connected networks assume that there is a hierarchical organization of concepts
and even further assumptions about the smoothness of the underlying function we're estimating.
Max and many of his collaborators, for example, Tako Kohen, took this idea so much further
they introduced rotational equivariance and then they built models which would work extremely
efficiently on non-geometric curved manifolds, meshes or even graphs.
Max wants to reduce the need for data in deep learning models, increasing the representation
or fidelity of neural networks, subject to discretization and sampling errors and improving
the computational techniques to process them more efficiently.
Max has recently put out two new papers, quantum-deformed neural networks and probabilistic, numeric
convolutional neural networks, which we'll be talking about today.
Anyway, Max, it's an absolute pleasure.
Welcome to the show.
Thank you very much, Tim, for a very nice introduction.
It almost sounded like it's not me, but it was a lot.
Do you feel that it describes you, not maybe accurately, but do you feel like there's
parts of your work that are overly well-known and there may be parts of your work that you
wish would be more well-known?
It's hard to say it's overly well-known because, of course, it's very enjoyable when you can
make a big impact, but what I can say is that it's incredibly hard to predict what will
become well-known.
Of course, if you could predict it, you would only write papers with, like, gazillions of
citations.
When we did things like the VAE or graph neural nets, it didn't feel at all like this was
going to be a big hit.
Some of these things are being singled out and they fly.
Precisely what makes these papers fly is, you know, it's a big puzzle in a way.
Some other papers you can be very proud of, and it takes so much time to actually get
published.
It's a huge uphill battle.
You think, why do the reviewers not understand better?
What we really want to do here?
And then, yeah, I guess there's a lot of good work which disappears into oblivion from many
people, and it's mysterious, but anyway.
Your hits definitely seem to be more than your misses.
You're a prolific researcher yourself, but you've nurtured some of the best and brightest
minds across not just deep learning, but like the wider machine learning field.
How do you consistently do that?
Is it fantastic mentorship or is it more finding the right spark in a student and nurturing
that?
That's a really good question, and I should say that I've been extremely blessed by all
these fantastic students right from the beginning, but I do think there is something to
nurturing talent, so I think what doesn't work is to basically tell to be very constrained
to a particular topic.
Sometimes you see this happen if you write a grand proposal and then the grand proposal
is about topic A, and then really the student starts at topic A, but figures out after a
couple of months that they don't really like topic A and they want to move on to B, and
it's just very painful then to say, no, no, no, you cannot do that.
You have to be doing A.
And so I've been very blessed with being able, even with my industry funding, to provide this
level of freedom to the students, and I think this is really key.
So the other thing which I find really key is that the relationship you have with a student
is very important.
First of all, it changes over the years, which is also very beautiful.
So you start off with much more guidance, and towards the end, you should actually not
be doing any supervision.
You should just be having a conversation at that point on equal footing, and you see about
halfway through a PhD, like it's like a flower that opens, and then now they get it.
Suddenly, right?
Now they get it.
They have a huge, interesting ideas in all directions, and they can write all these papers
and stuff.
So that's a beautiful moment when that happens.
And the other thing I think is that I think of supervision as nudging in the sense that
I have a big, a lot of experience in where is the interesting stuff to be found, right?
Where is the next wave that we can get people enthusiastic about?
What are the important questions to address in the community and things like this?
That's where my experience lies.
Now, I'm not doing a lot of coding myself.
In fact, I'm just doing almost zero coding, which I regret for this life.
And the other thing is that even in terms of math, it's limited, right?
I write at most maybe two pages of math to verify something or to compute something quickly,
but not like a lot of math anymore.
I just try to keep up with literature mostly.
And the students do, though.
So they do the hard work, literally.
So they really should do all that work.
And it's this interesting relationship where you have a discussion where you say, I think
this is an important direction, an interesting direction.
And here are some other things which are connected to it very intuitively, right?
So you may want to look there.
And then a good student will just pick up these ideas and we'll run with it and then
come up with new ideas.
And then you could say, it is maybe be careful about this direction, don't go too
deeper. Maybe this is more an interesting direction, stuff like that.
But even there, I've learned to be very careful.
And if a student comes up with a good idea, and intuitively, I think that's actually not
a great idea.
This is going to be a dead end.
That I'm not going to tell the student that very soon.
I'm just going to certainly leave the student about a month to explore that idea for sure.
And I've been surprised, right?
I've been surprised.
And basically, it turned out it was a great idea and I was wrong.
And so I've been very careful with these things too.
So I feel it's a very careful dance between the student and the supervisor with not too
much direction.
Also, it's a very personal.
So some students like more direction and other students like less direction.
But I think it is a bit of an art.
I've learned to appreciate it is a little bit of an art to have the right type of relationship
with students.
Yeah.
But of course, it's all about them.
They are the ones that need to shine in the end after four years and they need to get
the good jobs and become famous.
In terms of that guidance and specifically what you said with respect to this direction
might be interesting.
These are the interesting research directions.
Is this something that you just have to develop or do you have some general, can you give
some high level patterns that you've observed throughout the years where you see recurring
things and and you say, oh, that's another one of those probably like short term hypes
or yeah, have you observed some general patterns there?
Yes, it is two things, right?
So there's some things where I think why what is the big deal?
Why is everybody chasing this particular direction?
So that's can you predict what the crowd will follow?
That's one thing seems pretty hard.
The other one is to find directions which may be on longer time scales are impactful
and interesting.
And for the second one, it is deeply intuitive and it's very hard to figure out precisely
what it is, what features there are.
But for me, I have to get a sense that there is some something very deep going on that
I want to pursue.
Like for instance, so I clearly in physics.
So if you can think about gauge symmetry, like symmetries have this deep feeling, right?
Symmetries pervade basically all theories of physics and they have this profound impact
on how you formulate the mathematics of a theory.
And so there's something very deep about symmetries and about manifolds and doing things on curved
spaces.
And so that's I could sort of naturally drawn into this thing.
Now it's more quantum mechanics and there's something very deep and especially when it
becomes almost mysterious, right?
Quantum mechanics is almost mysterious.
How on earth is quantum mechanics possible?
If you dive a little bit into this phenomenon of there's a two slit experiment where you
have these individual photons, which, which go over two paths.
And if it's a wave that's perfectly fine, they can interfere with each other.
But now these photons can go one by one.
And somehow they have to be aware of this other possibility that they could have taken
to interfere with that other possibility.
I just think that's crazy what's going on here.
And so I'm naturally drawn into sort of these kinds of mysteries in some sense.
Yeah. And there's plenty more.
And the other one is also computation clearly, right?
How does the brain computing also feels like a very deep question, right?
How do we even compute things?
What is computation even?
And does the universe compute its solution?
What does it mean to be predictable?
Can you predict?
Can you compute faster than the universe can compute?
And so there's all these very deep questions about computation as well that you can ask.
But there's a mixture between things that are attractive in that sort of mysterious sense.
There's something very deep that needs to be pursued.
And things which are also highly practical, which is sometimes it's also a lot of fun
to work on something that where you can actually make a big impact.
For instance, speed up MRI imaging with a factor of 10.
So now suddenly you can actually both image and radiate cancer at the same time,
which could have a huge impact in the future, right?
And feeling that level of impact is also quite exciting, I think.
Amazing.
So I wanted to frame up some of the work that you've done around symmetries and manifolds.
It's absolutely fascinating.
The prevailing idea is that we are wasting the representational capacity of neural networks
because we're essentially learning the same thing many times.
And you'll work absolutely pioneered this starting with sort of rotational
equivariance on CNNs and then moving on to meshes and graphs and different types of topology.
It's absolutely fascinating.
But philosophically, the modus operandi in deep learning is this blank slate idea,
this idea that if we look at data and nothing else, then we can learn everything we need to,
presumably not in a very sample efficient way.
Transformers seems to be going in this direction in the natural language processing world
that we just ingest infinite amounts of data and we can learn everything we need to.
And we spoke to a good old fashioned AI person, Walid Saber, last week.
And his argument was that the information is not in the data.
He was arguing that we have a kind of ontology or knowledge built into us,
which we can use to disambiguate information that we receive.
So fundamentally speaking, do you believe that we can be data driven?
And can you introduce some of the work you've done with some of these priors in deep learning?
So this is a very fundamental debate clearly, but I think it's not all that black and white, right?
So there is basically at the core of machine learning, there is basically trade-offs.
The bias variance trade-off, for instance, clearly expresses this, right?
The first thing I want to say, there is no machine learning without assumptions.
It just basically, you have to interpolate between the dots.
And to interpolate means that you have to make assumptions on smoothness or something like that.
So the machine learning doesn't exist without assumptions.
I think that's very clear.
Clearly, it's a dial, right?
So you can have, on the one end, you can have problems with a huge amount of data.
It has to be available clearly.
And there you can dial down your inductive biases.
You can basically say that the data do most of the work in some sense.
And let me make my prior assumptions quite minimal.
And with minimal, I think I'm interested in a smooth mapping, right?
The mapping needs to be smooth.
Like that's a very minimal assumption.
But the disadvantage of that is, if you don't put any prior assumptions,
is that if you need to take whatever you've learned into a new domain,
where this model wasn't learned, it will very quickly break down.
Because these prior assumptions will help you transfer from one domain to another domain.
And causality does play a big role here, but we can talk about this later.
And then on the other hand, there is basically what everybody else in the scientific community does,
which is write down a model of the world, which we call a generative model.
Which is, how do I imagine that the world that I'm seeing in my measurement apparatus
could have been generated by nature?
And you can put a lot of intuitive knowledge there,
because you could think the world is described by a PDE or some kind of generative model.
So people in our community often call this probabilistic programming.
Models created by probabilistic programs or graphical models.
But they are highly intuitive, highly interpretable.
And because they describe the generative process, they are often also causal.
Because you can think of these variables and one causes the other variable to happen, etc.
And because they are causal, they really generalize very well.
Which means that if I train someone in one context, let's say, I learned to drive in the Netherlands,
I'm driving on the right side on the road, I have a particular kind of traffic signs, etc.
So now I can take whatever I've learned, sort of these rules or whatever I've learned,
and now I can move to another country where you drive on the left-hand side of the road,
completely different traffic signs, and I can still survive.
So this is typically something that the purity data-driven methods have a much harder time doing,
this sort of generalization.
So I think it's basically a trade-off now.
The big question in some sense over time is,
can we simply take the data-driven approach and extend it all the way to AGI?
There's people on one side of the fence that are claiming that this is possible.
Of course, we also need to amplify computation.
We're just going to build faster and faster computers that can digest more and more data.
And at some point, we'll just have AGI emerge out of this kind of process.
And then on the other side, which is more the classical AI sort of community,
which is, no, no, no, that's going to be ridiculous.
You will never be able to do that.
You really need to imbue these models with the structure of the world,
which I take as, how does physics work?
How does the world work?
Can I tell you something about how data really gets generated in this work?
This will cut down the number of parameters to learn dramatically.
And because I'm following causality,
I can now basically generalize and create AGI in this way.
And so this is going to be very interesting, how this is going to play out.
And to be honest, so I feel that I'm slightly in the camp of,
you really need to put generative information into your models,
but I've been continually surprised by what's happening on the other side.
Of course, lots of my work is also on the other side
in the sense that GPT-3 is completely 100% data-driven.
And did we expect that it would do so well?
No, so he is another big surprise, right?
And so I think that's the fun part.
But it kind of doesn't do well, though.
It doesn't have any reversibility.
So if you ask it how many feet fit in a shoe,
or we did the example last week,
so the corner table wants another beer,
it doesn't know that the corner table is a person
because that's missing information, we would fill in those gaps.
But it does raise the question, though,
of the dichotomy between memorization and compute.
And the guy we were speaking to last week just said that
even if you had an infinite amount of memory,
and the data is just not there, you couldn't do it.
When you were responding to Rich Sutton,
and you actually spoke about all the different schools of thought
in machine learning, so you said compute-driven versus knowledge
and model-driven or data-driven and symbolic or statistical
and white box or black box and generative and discriminative.
The generative thing is fascinating because our brains,
it's a bit like we've got the matrix,
or we've got a simulation going on behind the scenes, haven't we?
And we're always thinking about all these potential situations
possibly integrating between them.
Yes, I do agree.
That seems to be the real trick for intelligence, at least in humans,
so our ability to generate the world, at least at a symbolic level.
We don't generate high-resolution videos in our brain,
but we do generate objects and interactions between objects
and sort of how things will play out.
And this will also help us imagine things like,
what would have happened if I would have done this?
So now I can play out this alternative world and say,
that was bad, let me not do this now.
So I think that is going to be a key.
So that's the generative part of the modeling
because you can generate, you understand how the world works,
the physics of the world works,
and so you can generate possible futures.
To me, I feel that's going to be a really important part of intelligence.
And I do agree that it's for me also very hard to see that
you can generate enough data to cover all corner cases.
It's just very tough if you do it in the wrong direction,
which is the discriminative direction.
But again, I have been surprised by how good these models really are.
And so you say GPT-3 isn't very good maybe,
but it's a receding horizon, right?
People may have not thought this was true
or bet on something like GPT-3 before it appeared,
and then it appeared and people were extremely impressed.
And then of course, some people poke it and say,
but it doesn't understand this and this,
and then excitement goes away again a little bit.
But it is a bit of a receding horizon,
but I have generally been very impressed.
Also with, for instance, the fact that we cannot generate faces
of people that don't exist.
We can create billions of faces that don't exist on this planet
and that look absolutely realistic.
Would I have expected this?
No, probably not.
So, and then of course, there's AlphaGo and things like this,
which we also wouldn't have expected right before it happens.
Let me play a bit of devil's advocate with respect to
building priors into models.
Of course, like some of the easiest priors we can think of
are, let's say, translation invariants in a CNN.
You can also extend this to rotational invariants and so on.
But if we look at a true practical problem, we say,
yes, it makes sense that there is a rotational invariance
in the world.
However, on the ImageNet dataset, like a real practical problem,
the sky is usually up.
And the object is usually in the center.
It's not like to the side, it's usually in the center.
So, in a way, it seems like if we actually hit the true invariance
that the world adheres to, it's certainly beneficial.
But if we even slightly deviate, if we build in a different invariance,
it seems like there is a level of accuracy.
And if we want to get past that, these invariance seems to be hurting.
Do you have a sense of, can it be counterproductive
or when is it counterproductive to build in such invariances?
It's a very good question.
And so this goes to the point of the bias variance decomposition again.
So, if you hit the right bias, then it can be beneficial.
If you impose the wrong bias, then it's going to hurt you.
And this is a well-known tradeoff.
So, of course, the whole endeavor of machine earning
is defining the right inductive biases
and leaving whatever you don't know to the data,
and then basically learning to focus your models
on the data that you're actually seeing.
But I agree, if you put the wrong inductive bias in it,
things can actually deteriorate.
Now, I should say here that for the rotation invariance
or equivariance, things are not as bad as you might think.
So, you said if you just have slightly wrong inductive bias,
then it hurts.
But that happens to be not so much the case
because there's objects inside images that do have,
if you turn a cat upside down or a tree upside down,
we still recognize it as a tree in some sense.
And it does give you a sort of robustness to certain transformations
on these objects that you would otherwise maybe try to model
by data augmentation and stuff like that.
Now, for the sky, maybe you're right that similarly in digits,
you will start to confuse a six and a nine
if you build in rotation and equivariance.
And so there it will actually hurt.
But it's been surprisingly robust actually
because basically because you also cut down
on a number of parameters.
And by cutting down on the number of parameters,
you can actually help the system generalize better.
So, the inductive bias doesn't have to be perfect
and it can still help.
Could we touch on the dichotomy between the work you've done
and capsule networks, for example,
as well as the sample efficiency thing?
For example, with translational equivariance,
it means that you can move the dog
and then the response map the dog has moved as well.
And much of that is about allowing your networks
to learn patterns more easily
because they can map in every single layer.
So, with capsule networks,
that's still a blank slate of philosophy.
So, you don't explicitly say what the capsules are.
Whereas with your approaches, you explicitly define the priors.
With capsules, it seems to be defined by the data you give it.
So, if you train a capsule network on MNIST data,
it might inadvertently learn that one of the capsules
is how bendy the stroke width is on the seven.
Or it might learn that there's a rotation on the car
because you've given it lots of rotated versions
of the same car.
But it seems quite arbitrary
and the algorithm is hideously inefficient.
And what's much more exciting to me
is the kind of baked-in priors that you've designed
in the encoder stage.
So, could you draw the dots up between those two approaches?
Yeah, I think you actually said it quite right.
So, one is a much more constrained system than the other one.
But the actual representations that we put in our hidden layers,
in both cases, are very similar.
They are stacks, vectors,
and these vectors transform under certain operations.
So, if I rotate the input,
then there is some operation on this stack of vectors,
which is they do rotate in the XY plane,
but they also permute in the sort of vector dimension.
And so, we tell it very explicitly how to transform.
We just say, under these transformations,
you have to transform like this.
And we can do this because these geometric transformations,
we know them, that they appear in the real world.
But it's also constraining
because there is many other transformations
that either we don't know precisely
what the mathematics for the representations looks like,
or, for instance, like groups that are not compact,
we have looked at scaling, but it's already more of a stretch.
But there's, of course, you can do many other types of transformations
that don't even have to be groups.
There could be other types of transformations,
like lighting changes or whatever.
If you wanted to incorporate all of these,
you would have to build the mathematical representation theory
for each of them.
And then it would actually also explode
in a number of feature maps that you would have to maintain.
And it's not a very practical approach.
So, this works up to the transformation groups
that we understand and that are everywhere around us.
If we want to go beyond it,
then basically something like capsules are very nice
because they tell you,
well, we'll just keep the abstract nature of what we want,
which is some stack of things that transform
in some way that we can vaguely specify.
And then we ask it to learn all these things.
And we are actually ourselves also looking at these
sort of more relaxed notions of equivariance,
where we don't tell the system precisely how to change.
We just want this to emerge automatically.
And again, here, the connection with the brain
is very interesting.
In the brain, we do seem to have all sorts of filters
which are related not only by rotations,
but all sorts of other transformations.
And they are topographically organized.
So, the ones that are related,
like a slightly rotated version,
is sitting right next to the other one in your brain.
And so, presumably your brain
have figured this out by just looking into the world
for a long time.
And it's organized all these filters that way.
And it's known that if you prevent, let's say,
a cat from seeing,
then it will not come up with this nice organization.
So, you really have to get that by looking into the world a lot.
And that's super fascinating.
And I think that's where some of our research
is being directed now.
Can we learn from how this happens in the brain?
Is there a connection between these topographic maps
and equivariance somehow and between capsules
and all of these things?
And I believe that it is a good strategy
to take the general idea as equivariance
and then slowly relax it
and let the system learn more and more.
So, these capsule networks,
they've been a bit hyped when they were not really developed,
named first by Jeff Hinton.
And he has this concept,
or at least had it at the beginning,
that it's some sort of like an inverse rendering pipeline.
So, the sort of the capsule networks do some sort of,
they take in the world and they inverse render it
into these capsules.
How much do you agree with that type of formulation?
It seems what you've described
is more of a forward way of looking at capsules
where we have these invariances, yeah.
Yeah, so I very much agree with this idea
that you have smaller things
and they can be used in multiple ways,
but you have to align them in a particular way
so that they build something at the higher level.
And obviously, you can invert that idea too
in order to start at something very abstract
and then generate certain things this way.
And there is a lot of work now,
actually going into equivariant generative models.
For instance, equivariant flows.
A prime example is, for instance, in physics
and what's called quantum-cormor dynamics.
There's a theory that has a huge number of symmetries
called gauge symmetries.
And if you transform these quarks in a way,
in a particular way, the physics doesn't change.
You will have exactly the same observations, right?
But still you need all these symmetries
to conveniently describe this model.
And so now when you generate,
if you want to generate quark fields
or something like this, right, then gauge fields,
then you can generate all these symmetries
and it's not very helpful
because you generate one configuration,
but if you generate all these equivalent things
which are only different by symmetry,
then you haven't really done much.
So understanding how to generate
with these equivariants in it
is actually a big topic of research in many groups.
I think Danilo Rosenda and DeepMind has done a lot of work
and there's physicists at MIT who do work.
And we are with a group of students and physicists at Amsterdam.
We're also looking at these types of questions.
So that's, I guess, the inverse problem
where also equivalence is playing an increasingly important role.
Not many people are working on capsules.
I feel they've fallen out of the favor of the public
because I don't know,
they're maybe hard to implement
or they don't really work as advertised, let's say.
Do you have general thoughts about capsule networks?
I think with many of these things,
there is an underlying intuition which is correct.
So I haven't really worked myself in trying to implement them
and so what you often see in this field is that
there is an intuition about how something should work.
And often that is the correct intuition,
especially when it's coming from Jeff Hinton,
it is very likely to be the correct intuition.
Now, then there's the next step which is
how do you make something practically implementable?
And these days, that means that you have to run it super fast.
You have to be able to implement it in GPUs,
all these kinds of constraints.
Otherwise, you will be so much slower
than just an ordinary CNN
and you will basically not be able to train as long as a CNN
and you cannot train as many parameters as an ordinary CNN
and you will not beat it.
And if you don't have the bold numbers, hard to publish.
And so that might impede progress in something like this.
But then what happens is you wait
and then for five or 10 years
and then the computers have become faster
and then people go back to these ideas
and then they think, oh, that was actually very interesting.
Let me try again.
And then suddenly things start to work.
Now, that's of course the story of deep learning,
more generally speaking, right?
Because we had neural networks like a long time ago, right?
In the 80s, it was actually quite popular to work on these things.
But they didn't quite take off
because we didn't have the compute power
and maybe also not the data to really train them well.
And it's only when we took them out of the closet again
and said, hey man, this thing actually works
if you throw a whole bunch of GPUs at it.
That's when people then became popular again.
And so something like this might well happen again with capsules
or something like capsules in then by five to 10 years.
Do you have any other than capsule networks?
Are there things that right now we are not looking back on
but that would be worthy of a revisit?
Oh yeah, that's very tough.
But I'm personally looking at things like ICA
and topographic ICA.
So I think there's an interesting body of ideas there.
Of course, I risk now to mow away the grass before my own feet,
but okay, let me entertain that.
And then probably marker-vendom fields and things like this
will probably make a comeback at some point.
The graphical models more generally will probably make a comeback
or maybe that integrated with deep learning.
And some people have already attempted going in that direction.
Energy-based models have made a comeback already.
So yeah, it is often going back to older ideas.
And there's probably a lot more that other people can sort of name.
I do have a prediction maybe for the future
that people really haven't looked at yet.
In my opinion, it's going to be quantum things.
So I think many people don't actually
know, understand the language of quantum mechanics or mathematics.
And I do think that first of all, that language is very interesting.
It's a bit different than our normal probabilities.
It's like square roots of probabilities.
And with the advent of quantum computers,
which at some point will come,
we as a community will have to dive into that
and maybe make it part of our curriculum in university.
And then I think that will become, that will start to boom.
Could I just quickly introduce, before we get to quantum,
I don't know if you read Sarah Hooker's The Hardware Lottery Paper.
And this is fascinating for you, of course, working at Qualcomm.
But her idea was that there are certain things
that cause a inertia or friction in the marketplace of ideas.
So is it a meritocracy of ideas?
Or do the previous hardware decisions and hardware landscape,
does it enslave us?
Ideas succeed if they're compatible
with the hardware and the software at the time.
And this is what she called a hardware lottery.
And she says the machine learning community is exceptional
because the pace of innovation is so fast.
It's not like in the hardware world, which you all know so well,
where it costs so much money to develop new hardware
and the cost of being scooped is so high.
But I wanted to just come at this from,
I don't know how you see this, right?
So with capsule networks,
the reason they're so slow is because it's a kind of sequential computing paradigm
and no amount of hardware is going to solve that.
But there are entirely different paradigms of hardware,
like Quantum, which could potentially change the game.
But how much could they change the game?
Is there still some limit on it?
There's always a limit, clearly.
But I think the situation is maybe slightly more subtle,
which is that working in a hardware company,
I can also see the other side of the coin a little bit.
So there is also a race in the hardware companies
to build ASIC designs, which is specialized hardware,
to run the latest and the greatest machine learning algorithms
which are being developed.
So it's not just which machine learning algorithms work well
on the current hardware that us being enslaved to the hardware.
There is actually a feedback,
which we're now the companies are trying to build ASIC
to, first of all, run the conclusions very efficiently.
And soon we'll have probably Transformers run very efficiently.
And so that's the fascinating thing.
Of course, it's hard to get your paper published,
perhaps if you're ahead of the game too much,
which I see a little bit in the machine learning community,
which is if you look at the papers
which are published in the physics community,
they work with images of four by four pixels.
That's what they can do,
because otherwise you need a quantum computer,
obviously, to run your algorithm.
And it's being looked down upon a lot by the machine learners,
basically saying, why is that interesting?
And I do feel very strongly that as a field we need to open up.
So we should value original ideas much more than we currently do.
And I don't know,
you can probably have a whole conversation
on where this is coming from.
I think the reviewing in our community is far too grumpy.
I think people, if it's not completely finished polished paper,
then they'll find a hole somewhere and they start pushing on it.
And I think you should look also at a sort of more holistic,
how original is this idea, right?
Can you be excited about the originality
and the creativity of the idea that went into that
and trust that maybe it takes the community a couple of years
to further develop this?
And some things will die and that's fine,
but let all these flowers grow in a way.
And yeah, so I do feel a little bit that sometimes is a bit negative.
And that's maybe where some of that friction is coming from.
Just on that, it's fascinating.
We're talking to Kenneth Stanley on Monday
and he wrote a book, Greatness Can't Be Planned.
And his big thing is exactly what you've just said,
that we have this convergent behavior in so many of our systems,
whether it's science or academia.
And it's because of this objective obsession.
So we all want to monotonically increase our objectives.
And what we should be is treasure hunters.
Science should be about exploration, not exploitation.
Exploitation is one step away.
You already know how to build the bridge.
We don't seem to have this paradigm at the moment,
even when you submit your paper to be reviewed.
There's a consensus mechanism, isn't there?
Because you need to have multiple accepts from people
and science advances one funeral at a time.
Do you think this is a huge problem?
Yeah, I think it is a big problem.
But I think also we will probably,
I think it's a big problem because it will hold us back
and it will also hold very brilliant students back.
So what do I advise my students now?
I advise them to have a mixed model, a mixed sort of policy,
which is on the one hand you work on some papers
which are easy to score on,
things that are very popular in the community.
And then on the other hand, you work on things
which might be huge innovations
that are much more uncertain.
They might fail, but then there might also be
really big innovations.
And that way you get your papers and you can become famous.
But at least also you work on things
which are highly risky.
But in fact, it's a bit cynical to have to do that.
It would be much nicer if there would be
much more appreciation for just originality.
But I do also believe there is a solution to this.
So I think we are in a sort of a local minimum
as a community in this sense, but I think there's a way out.
And one way out, which is basically,
and this has been already proposed long time ago,
I think Yelma Kohn and Yasha Benji were also talking about this.
And we are actually trying to implement this
for a Beijing Deep Learning Workshop,
is to sort of throw papers on the archive
and not necessarily submit to conferences
and to have a open reviewing of that
and to give people reputation in DC.
So if you do, if you give a good review,
you can publish your own review or state it in your CV
and people can rate your review.
And if you do poor reviews, you'll get horrible ratings
and then your reputation will come down.
So there is some kind of way that you can probably design
is that people are incentivized to give good reviews
and to actually use these reviews as a half a paper
that you can also be proud of.
And then good things will come up, right?
They will, at some point, people will point to interesting ideas.
Maybe we need some kind of recommender
to make sure it's a bit unbiased
in the sense that it's not only the famous people
that will get their papers exposed,
but also less famous people.
So we need to have sort of maybe build a recommender
around something like that.
Every so now and then a conference comes by
and it sort of harvests in this field of sort of papers.
Let's say you're all reviewed, they have great reviews.
I'll take one or two more reviews anonymously
and I'll then publish.
And then I invite you to present your paper in our conference.
That to me sounds like a much more natural way to proceed.
I also find it very demotivating for my students
who have these ideas.
Maybe this is the worst part.
So you're a student, you're working on this thing
which is not completely mainstream.
And then you get rejected two or three times
from a conference, right?
This is so demotivating for a student to then continue, right?
At this case, at least you just push us on the archive
and you engage with the community around your paper.
And that's a much more, it's much less demotivating
than these constant rejections from the big prizes, right?
The NERIAPS paper or the ICML paper that everybody wants.
Aside, guys, the idea at the moment,
a lot of people are talking about this.
How can we improve peer review?
In every field, people are moving towards this open review model.
But collectively as a research community,
we don't really have the collaboration tools
at this point in time to take advantage of it.
Open review is willing to implement this actually.
So I think it will happen.
It's a good system to pivot back into new ideas
and sort of exciting concepts
that are coming out of machine learning at the moment.
You mentioned quantum computing as this paradigm
that's really critical.
That's not really well understood by the machine learning community.
Would you be able to give our listeners
like the five-minute spiel about quantum probability,
how it differs from the probabilities that we're used to?
Yeah. So you can think of quantum mechanics
as another theory of statistics in some sense, right?
So in AI, for everything we can't totally observe,
we write down probabilities of things happening.
But of course, underlying, there is processes,
but we just don't observe everything.
And so we describe it by probability.
Now in quantum mechanics, it's very similar to
taking the square root of a negative number in some sense.
It's like you, let me put it another way.
So it's very much like taking the square root of a probability
which can actually become negative.
So minus one squared is one, right?
Okay. Or let's say minus two squared is four.
Four is your probability and minus two
could be your quantum amplitude.
And this thing can be negative.
And the bizarre thing is that if you describe a system
by these quantum amplitude, these square roots,
then they can cancel, which is,
this is the counterintuitive part,
which is you can have a probability for an event
or an amplitude for an event,
and then you have an amplitude for another event.
And you would think that if there's two probabilities
for that event to happen,
then the probability of that event should grow.
But in quantum mechanics, they can cancel,
and then the probability is suddenly zero that the event happens.
So this seems bizarre, but nature has chosen this theory
of statistics anyway.
And so it behooves us to look into this more.
So first question is, can you write down
maybe even normal classical problems
more conveniently in this quantum statistics?
And here, I always remind myself
when I first learned complex numbers.
When you learn to solve the damped oscillator equation,
you can do it in a complicated way,
or you can go to complex numbers,
and then suddenly it gets very easy to do it.
And so you can imagine that there is things
to compute in classical statistics
that are actually either shortcuts
by using quantum mechanics somehow.
And so the first thing that we've tried to do
with quantum mechanics in deep learning is to say,
can we just design an architecture
that would be naturally a natural fit
to this quantum mechanical description of the world?
But we still want to be able to run it
on a classical computer.
We just want to harvest these new degrees of freedom
that we have from quantum mechanics.
And so that was a paper that we recently pushed on the archive
which is quantum-deformed neural networks,
which we basically first say, okay,
what if we would take a normal neural net
and implement it on a quantum computer,
and then we slightly deform it
into something where states get entangled.
And this entanglement is another strange
phenomenon in quantum mechanics
where you can create states
which you cannot really create classically,
in super positions of states.
And so by doing it in this particular way,
we could still run it efficiently on a classical computer,
but it's just a very different beast
than a normal neural network.
So that's already, to me, very interesting.
And then, of course, the big prize, the big bonus,
is that if you adhere to this way of describing,
what's happening is that there is the opportunity
to be able to run things very efficiently
on a quantum computer.
So now you can design your neural network
in such a way that classically,
actually, it will be very hard to simulate it.
But then on a quantum computer,
you could potentially simulate it very efficiently.
And of course, we don't have quantum computers,
so it's very hard to actually prove your point,
but that also makes it somewhat exciting.
In that paper specifically,
you make lots of references and connections
to the Bayesian way of doing machine learning.
Could you, what's the connection there?
Because it seems different.
Both are, I agree, both are statistics,
and you already mentioned the square roots of probabilities,
but how do you connect the uncertainty quantification
in the Bayesian way with how particles move?
Quantum mechanics is not necessarily about particles.
So you can write quantum mechanics on just states.
You can just write down a number of classical states
like, let's say, a sequence of zeros and ones,
and there's an exponential number of these states.
And then you can say,
classically, I can only be in one of these states,
but in quantum mechanics,
I can be in any linear combination of these states,
which is a bigger space.
Now, what we did in that paper was to say,
we can treat both the world state
as well as the parameter state as a quantum,
we describe it by a quantum wave function,
and then we entangle these different states,
which is similar to saying that I take my state,
classical state x, I multiply it by a matrix of parameters,
and I get a new state out.
So here, the analogy would be,
I have my quantum superposition of classical states,
I have a quantum superposition of parameter states,
and then there are some processes
where we get entangled together.
And then I do a measurement,
which is now a function of both the parameters
as well as the inputs,
and you train it to give you measurements
that with high probability give you the answer that you want.
So that would be the training process.
Now, there is actually a very precise way
in which you can relate Bayesian posterior inference
in quantum mechanics,
but that's a fairly technical story.
But there is a, using density matrices,
there is a fairly precise way in which you can say,
I have a state described by a density matrix,
and if I do a measurement,
I condition on something,
and I re-normalize and stuff like that.
So that's possible.
So there are two things.
Like, first of all,
the quantum neural network formulation
can be very slow on a classic computer,
but fast on a quantum computer.
On the other hand,
people do run like Bayesian inference on classical computers.
What makes the quantum neural networks
that much harder to compute?
Yeah, it's this entanglement issue.
So in, so classically, I agree,
there is an analogy in classical statistics
where this looks very similar,
which is, for instance,
if I have a exponentially large state space,
and I write down a probability distribution
over all of these possible states,
where they have a number,
a positive number that sounds to one,
for each one of these exponentially large states.
And if I ask you,
now compute an average of a function
over this probability distribution,
you can't do it
because there's an exponentially large number of things
that you would have to sum.
And so we have ways to deal with it,
which is sampling from these distributions
or variational approximations.
And anyway,
we have to approximate this state of affairs.
Now, in quantum,
that's fairly similar.
So there's, you face a similar exponential problem.
And you can also do approximations to get around that.
But the interesting part is that
in quantum mechanics,
you can, for instance, do a measurement.
And a measurement is something that, you know,
that is, I guess, a physical thing,
and it's not very hard to do.
But it will be an operation
which looks like sampling something down
to a particular classical state again.
And it does look like the sampling operation
that we do in sort of artificially in probability theory.
But it's also true that quantum computers can,
in principle, compute things that classical computers
can't compute,
and they can actually compute it much faster.
Whether that actually maps to the things
that we are interested in is not so clear.
So that's, it's not at all clear right now
that we will actually build quantum neural networks
that are generalizing a lot better
on classical problems, right?
If you want to do classical predictions,
does it actually help to build a neural network
that can run efficiently on a quantum computer
that can do these predictions much better?
And that's not known,
but that's what makes it exciting, in my opinion,
because you can try to do it.
Now, there's also functions
that you can't even do classically,
and you have to do quantum mechanically,
but I don't know how relevant they are for AI.
Fascinating.
Can we conceivably say that at least one,
let's say applications are way for these neural networks,
or for the quantum neural networks to come in,
is in the place where right now
we have these renormalization problems.
Let's say big word embeddings,
or yeah, as you mentioned,
things like variational inference,
anywhere where you have a partition function
that you, let's say, have to sample to compute,
now we potentially introduce this new way of doing this.
Yeah, so I would say that is a different set of problems.
So there is some sampling algorithms
which can be sped up by quantum sampling algorithms,
but I think the maximum speedup is like a square root.
So it's not insignificant,
but it's also not exponential, right?
You could do something in square root time
of what a normal classical computer could do.
And then there is these very interesting stories
where people thought that they could do things
much faster on a quantum computer,
but then somebody thought really hard about it,
and they then invented actually a quantum-inspired
classical random algorithm,
which would do about the same speed or close, at least.
So it's very uncertain precisely what we can speed up,
but that's what makes it interesting, right?
If you can predict what's going to happen in some sense,
it's just a matter of executing, right?
But if you don't know what the low-hanging fruit is
and if there is low-hanging fruit
and what the possible benefits are,
the possible bonus that you can get by doing these things,
then it gets really interesting, in my opinion.
Amazing. Now might be a good time to talk about your other paper
that's just come out, Max,
which is probabilistic numeric convolution on your networks.
And this was also with Mark Finzi,
who we just discovered this morning,
just brought out a really interesting paper
about equivalents on light groups.
So that might be a potential digression later.
But this work is really fascinating
because it's in the setting of irregularly sampled data,
and we use these Gaussian processes to represent that,
and we can continuously interpolate between them
in this convolutional setting.
Absolutely fascinating.
Could you give us the elevator pitch?
Yeah. First, let me say again that Mark Finzi
was an intern at Qualcomm,
and Roberto Bondeson is the other person
who was also working with me on the quantum stuff.
So those are my collaborators in this project.
And of course, Mark did the bulk of the work for this paper,
so he should deserve much of the credit for it.
But here's the observation that we had.
The observation is when we write down a deep learning algorithm,
let's say, for an image,
then we sort of treat the image as pixels,
and we think that's the real signal that we are looking at.
But you can also ask yourself,
what if I remove every second pixel?
Now, actually, I have a very different neural network,
but should I have a very different neural network?
Or what if the pixels are actually quite randomly distributed
in the plane?
Just some random places where I do measurements,
maybe more on the left upper corner
and fewer on the left lower corner.
The predictor should behave in a certain consistent way.
And so, of course, then you come to realize
that really what you're doing with the pixel grid
is sampling an underlying continuous signal.
So then we started thinking,
how do you best deal with this?
So how can you build this in?
And so there's a very interesting tool
which is called the Gaussian process.
It's basically interpolates between dots,
but in places where you don't have a lot of data,
you create uncertainty because you don't know
what the real signal is.
So you basically get some kind of interval which says,
okay, I think the signal is somewhere in this interval
with 95% certainty, but I don't know precisely where.
Now, the mean function is a smooth actual continuous function.
And then the next step was say,
okay, what does it mean to do a convolution on this space?
This is a new Gaussian process interpolated space.
And what we found is that the most interesting way
to describe that is by looking at it
as a partial differential equation.
And so this ties back into another
really interesting line of work
which was started by David Duvenau and authors
on thinking of a neural network as an OD,
as an ordinary differential equation.
So here we're talking about a PDE basically
because we have spatial extent.
And so we are looking at sort of derivatives
and second-order derivatives in the plane,
basically, which we apply on the continuous function.
So this is literally what people do when they solve a PDE
is that they have some operator which consists of derivatives
which they apply to the function.
And then they have a time component
which evolves this thing forward in time, basically.
And it turns out that's a very natural way
to describe a convolution.
You can also add symmetries in a very natural way
by looking at that operator that sort of moves things forward
and making sure it's invariant
under certain transformations.
We had a bit of trouble really handling the non-linearity
that happens then so we had to then project it back
onto something that would then, again,
easily handle by a Gaussian process, etc.
So we had to do some work there.
But in the end, this thing was now actually
very general and interesting tool
which is apply a Gaussian process, apply PDE,
apply non-linearity, repeat, and then in the end
collect all your information and make a prediction.
And so some of the benefits are now that, first of all,
of course, you cannot work on a unstructured set of points.
It doesn't have to be a grid.
And you can even learn the positions of those points.
So you cannot direct the observations in places
where you really need to do observations
in order to improve your prediction.
So it basically becomes a numerical integration procedure
where you can learn where to move your integration points.
And what I also found very fascinating
is that this same paradigm can be mapped again
onto a quantum paradigm where you can think of that PDE
that evolves now as a Schrodinger equation
that sort of evolves like a wave function.
So it maps very nicely also again to a quantum problem.
And that's what we are working on now.
Something that's really fascinating
that keeps coming up again and again
in these sorts of research programs
is the matrix exponential.
Like it's our connection to groups and algebras
or like group representations and algebras.
And of course, we use it to evolve our ODE's and PDE's.
I guess as a physicist,
you've probably got a deeper appreciation
of this particular object.
But it's something that's still quite alien to a lot of people
I know that work in applied machine learning.
What's the significance of the matrix exponential?
Why does it connect all these really fundamental objects
to things like Lie groups and stuff like that?
Yeah, so it's interesting that we actually
just got a paper accepted in NeurIPS on this
and it's called the convolution exponential.
And you can look it up and Emil Hogebaum
is the main author and generator of that idea.
And yeah, so I guess because it is the solution
to the ODE or the PDE, right?
So if you write down something that's very fundamental
that is a first order differential equation
which is the derivative with respect to T of a state
is some operator times that state,
then the solution of that thing will be the state over time
is the matrix exponential times T times the state.
So that's I think where it comes from.
And so one other way to look at it is that in physics
is called the Green's function.
So it's basically the solution to this ODE.
So you can think of a neural net as basically,
we tend to describe it as a discrete map
from one point to another point.
But if you think of it as a continuous process
which is what we learned from the ODE description
of a neural net, if you think of it as a continuous process
then it's really, you can just think of that convolution,
this map, you can just think of it as the matrix exponential
solution to this ODE in math literature
you call this the Green's function.
So you can think of a convolution basically
as the Green's function of a partial differential equation.
I think that's where this feels like a very fundamental object
in some sense.
So in a talk you gave recently on the future of graph neural networks
you were talking about a number of ideas from physics
that hadn't really made it into machine learning
among them things like renormalization, chaos and holography.
Would you care to unpack these ideas a little bit
and tell us where you see the future is in these ideas?
Yes, so the reason I mentioned these
because I think there's a lot of really cool ideas in physics
which are still remain unexplored
but there is more and more physicists
who are moving into the field
and some of these ideas are actually
being worked out as we speak.
So I recently saw about two papers on renormalization.
So renormalization is something in physics
which basically you start with a system
with a whole lot of degrees of freedom
like the particles moving around or something like this
and then you coarse-grain the system slowly
and what means is that by coarse-graining you zoom out
and you build an effective theory of the underlying theory
in the same sense as thermodynamics
is an effective theory of statistical mechanics
where basically all the particles are now removed
but you now have an effective sort of description of your world.
This is the same as what happens in neural nets, right?
In neural nets we talk about pixels at the bottom layer
and maybe edge detectors and things
at the very top of it we're talking about objects
and relations between objects
which are aggregated emergent properties from this neural net
and ideas from renormalization theory
might very nicely apply to this particular problem
and indeed have already been applied with some success.
The other one which you mentioned was chaos
and I think there is a very nice connection
actually with chaos theory.
Going back to work I did a long time ago
which are called herding.
In particular you can think of sampling
from a particular distribution.
You can do it either by the typical way is
first of all you can think of it as a dynamical system
as a stochastic dynamical system
and you think of it as you're at a particular point
and then you propose to go somewhere
and then you accept or reject that particular point
and as you jump through this space
and you collect the points that you jumped to
then you look at that collection
and that collection should then actually distribute
according to the probability distribution
that you're sampling from.
Now that's a stochastic process
but if you think very hard about that
in fact it's a deterministic process
even if you try to make it stochastic
and the reason is that every time
you know you're doing a whole bunch of calculations
and so now and then you call a random number generator
but the random number generator
really is a pseudo random number generator.
It is also a deterministic calculation that you're doing.
So the whole thing end to end
is just a deterministic calculation
but because you're calling the pseudo random number generator
it looks very stochastic
but truly it is a chaotic process
and so you should really be able to describe the system
by chaos theory
and the theory of nonlinear dynamical systems.
Now what I've been working on with my postdoc
and Roberto with a postdoc called Kiril
what we've been working on is thinking about
let's make it a little bit less chaotic.
So let's make this actually a deterministic system
which is maybe at the edge of chaos
and again this is one of these very deep questions
that's in my head.
So I think so there is something very interesting
and deep here which is if you try to do a computation
on the one hand you want to store information
things that you've calculated
and for that things need to be stable.
On the other hand you want to transform information
because that's what a calculation is right
and so there you want to be in this sort of more chaotic domain
and it turns out that the best place to be
is at the edge of two things
where you can go to the right a little bit
and be more stable
and go to the left a little bit
and you can transform things and compute things.
And so I also think that when you're trying to sample
or you know sampling can be equated with learning
if you're a Bayesian about things
because in learning is basically sampling
from the posterior distribution
and that's the same as learning.
If you can design samplers that are not completely chaotic
as the ones that we describe now
but they're more structured and less chaotic
and more deterministic moving through the space
you can learn a lot faster and I find that
and then you can actually start to map it
onto sort of complexity theory notions.
If you think of this sampling from a discrete set of states
what kind of properties do the sequences that I generate have
what is the entropy of the sequences that I'm generating
for instance or what kind of substructures
is it for instance going to be periodic
or are there periodic substructures inside of it
or all these things.
And these are studied by the theory of chaos
and nonlinear dynamical systems.
So connecting these two fields feels to me
like a very fundamental thing to try and do
and some people have tried a few things
people have looked at well if you look at a neural net
there's an iterated map you map things to hidden layers
and later if you think of that iterated map
and think of it as is that map chaotic?
Being on the edge of chaos is the best thing
you shouldn't be completely or non-movable
because then everything you put in is going to be mapped
to the same point very uninteresting.
You also shouldn't be super chaotic because
or whatever you put in you're going to some random point
in space and that's not very predictive.
So you need to be at this intersection space
between chaos and non-cares and then you can do
interesting computations.
So this is the same idea right?
So to me that's exciting because now suddenly
your whole field of exciting mathematics is cracked open
and you can start to use all these tools in machine learning.
Awesome, thank you very much.
Now might be a good time to go over to Reddit.
We asked Reddit for questions
and the top rated question is by TSA.
Hi Max, when will you be changing your last name to pooling?
So actually there is a paper that a colleague of mine wrote
and I think they had an operator really called
instead of pooling you could do a welling operator.
Instead of changing my name I propose that we just
change the operators that we use and change to welling operators.
That's wonderful.
In the thread on Reddit there were a few variations as well
so maybe Max Power and someone asserted
that pooling is your brother.
But anyway, Red Portal says the conventional approach
for analyzing continuous convolution would be Fourier analysis.
What was the rationale behind the investigating
continuous convolutions using probabilistic numerics?
That's a good question.
So to me Fourier analysis it's true that you can
I guess I could still do a Fourier analysis
because a Gaussian process you can decompose
in terms of its Fourier waves
and then it's the primal versus the dual view
of any sort of kernel method.
So I could certainly go to the Fourier domain
and do my calculations in the Fourier domain.
The quantum mechanics is just another basis.
You just think of this as another basis,
you know, not only quantum mechanics in any
signal processing sense.
And it's true that a convolution is easier there
because just multiplication.
On the other hand, convolutions are very efficient
in modern software packages for GPUs.
So sometimes it's also not necessarily faster to do that.
But it's a good suggestion and maybe something nice
happens when you go to Fourier space
and I just didn't explore that.
Fantastic.
We've also got Jimmy the Ant-Lion says,
Hi Max, I noticed your co-authors come from a physics background.
Can you explain why there are so many
ex-physicists in deep learning?
Yeah, so that's interesting.
I think there's just a lot of physicists
and a fraction of those physicists is looking for other,
for greener pastures.
And I'm myself on one of those that was looking
for greener pastures.
And they bring a really good toolbox.
So if you've done physics, you have just a very good
mathematical toolbox, but also very good iteration
about PDEs and how the world works and symmetries
and all these kinds of things you bring.
And I think in some sense physics is also a bit of a container.
If you do physics, you can still do anything
else afterwards in some sense.
And I think there's just people who are naturally
interested in AI.
Of course, AI became very popular at some point.
And so you have automatically people flock
into that field.
But yeah, in general, they're smart people.
So I guess it's nice to work with them.
Maybe just circle back and close the loop to the beginning.
And we were talking about the research community
and kind of the machine learning research field.
I loved what you suggested.
And as I understand, this is not fully your suggestion,
but the suggestion of, let's say,
having a more open review kind of system
where a review could be as powerful as a paper itself.
I've been screaming for this for a few years now.
And could I ask if you ever have the chance
to propagate this, what do you think of the idea
of having a continuous research?
Like this paper notion that we have now,
I think it's so outdated.
And once my paper is published,
I have no incentive to update that thing.
What if we do research in this much more continuous way
and then there's comments and then in response
to the comments, everything changes and so on?
Yeah, no, it's a very good point.
So this is indeed exactly part of this idea
that we are trying open review to implement.
But the idea is that in open review,
you have a conversation with your reviewers.
And it's nice if the reviewers are not anonymous.
You just have your conversation.
And other people can even contribute to the project
in a more open science-y way.
But it is also nice now and then to present your work.
And so that's why I say,
so now and then a conference might come in
and harvest papers and just invite people
to present their work in a slightly more formal way
and maybe put a stamp of approval on it
and say, this conference has published
or this particular paper with some independent reviews
and we think it's a great paper.
And so you get that stamp.
So I guess there should also be a way
to close off a particular project
to move on to a new project.
But I also have the same view as you have
as this being a far more continuous process
where if you didn't get picked this time,
next time, some conference will come by and pick you out.
And it's much more like a marketplace
where ideas go around, conferences come in
and ask you to publish things.
And it's just you then present it
and then you can just continue with your research
or stop it and go to a new piece of work
or something like this.
So yeah, I share that vision basically.
That's amazing.
I'm continuously amazed when I read these old papers
from let's say Schmidhuber and like the first RL papers
that just came up with a bit of an idea
and then they had a bit of toy data
and right and that's a paper and it's cool.
Do you have any kind of thoughts about or recommendations
for the new generation of researchers
that are now flooding the field of how can we get
to a better field?
What kind of tips would you give the...
Yeah, I think we really need to disrupt the field a little bit.
And so I think it's particularly tough for new researchers
because the acceptance rates for these conferences
are very low and it feels like much of your future career
depends on getting papers in there
and it's a fairly random process as well.
So I think we just need to disrupt the field
and there's enough people with influence who want that.
So it's just a matter of actually executing on it.
And so that's what we do now for the Bayesian deep learning
workshop that we are organizing.
We want this to be an off-split from NeurIPS.
It was a very popular workshop there
and somehow we got rejected this year
and we thought, okay, we'll just do it ourselves.
We do have actually a meetup
but then next year we want to be our standalone conference
but for that conference we want to implement this plan.
And so we are working with OpenReview
to actually implement this for us
and Jarin Gull is working hard to try to actually roll this out.
And we're talking to Yashua Benjo about it
and he's very supportive
and there's a whole lot of people who are supportive about it.
But so if this can help to make this a popular model
then that will be a fantastic result of this interview.
But I think people should just push for it
and just say, okay, I'm just fed up
with the current way of doing things.
We should really change things and just shout out
and say, this is what we want and let's go for it.
Awesome.
Amazing. Professor Max Welling,
it's been an absolute honor and a pleasure to have you on the show.
Thank you so much for joining us today.
It was great with the three of you asking questions.
That works really well.
Fantastic. Thank you so much.
Thank you.
Amazing.
It was good. The questions were really fantastic actually.
And I've never done this with the three of you
but having a team of three people asking questions
is really, it's a good idea.
And of course you're really smart people
knowing what you're talking about.
So that went really well I think.
Needs three brains to match yours.
Anyway, I really hope you've enjoyed the show today.
This has been such a special episode for us
because Max Welling is literally one of my heroes.
So anyway, remember to like, comment and subscribe.
We love reading your comments.
We really do actually.
We're getting so many amazing comments in the comment section.
So keep them coming and we will see you back next week.

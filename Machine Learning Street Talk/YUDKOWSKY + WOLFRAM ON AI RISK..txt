I think that the broad situation with AI is that they're currently successfully scaling
them more and more powerful.
We're not quite sure how long that process is going to continue using the current technology
and part of the reason we're not sure is that nobody understands what actually goes on inside
a modern AI.
We can look inside the computer, of course, but we see giant arrays of floating point numbers
and people are barely beginning to understand what's going on in there and we do not quite
understand where the power comes from.
Chess did not stop playing when it got to human level.
Go did not stop playing by the human level.
They are past only training the AIs on invitations of human output and have moved on to training
it on what works to solve problems and you can just continue that process past the point
where the problems are of human difficulty.
So we're on course to get something smarter than us whose innards we don't understand,
which we cannot very well control and this probably ends quite poorly.
We're making the statement that the one thing we think we know about the AIs is that they
have been successfully trained to optimize their achievement of purposes.
That's an assumption which is not totally obvious to me.
I agree that as you give sort of all you're specifying is sort of play chess or whatever
else or you're specifying some big what we think of as an objective.
The details of what's happening inside, there will be aspects of that that we are not in
any way able to foresee, predict whatever else.
So if we took apart that mechanism and we say, is this mechanism doing what we expect
here?
It won't be.
There'll be plenty of things where it's doing what it chooses to do or because that particular
training run gave this particular result or whatever else.
To me, I'm not convinced yet.
There are interesting questions to try to answer which people should try and answer.
I mean, you make it sound like it's urgent, you know, the people are coming off the ships
and you might be right.
MLST is proudly sponsored by Two for AI Labs.
They're based in Zurich.
They're doing monthly meetups and they are hiring cracked ML research engineers to work
on ARC and to make progress towards AGI.
Go and apply now, twoforlabs.ai.
What matters is the strong desire to solve the challenge and the technical capability.
Usually the type of people we tend to find have computer science backgrounds.
Gentlemen, it's an absolute honor to have you both on MLST.
Thank you so much for joining us today.
We're going to have a discussion all about AI existential risk and Eliezer, first, if
you wouldn't mind, could you just spend about five minutes talking about how much of a risk
is AI to humanity?
I think the term risk is understating it.
If there were an asteroid straight on course for Earth, we wouldn't call that asteroid
risk.
We'd call that impending asteroid ruin or something like that.
And I think that the broad situation with AI is that they're currently successfully
scaling them more and more powerful.
We're not quite sure how long that process is going to continue using the current technology
and part of the reason we're not sure is that nobody understands what actually goes on inside
a modern AI.
We can look inside the computer, of course, but we see a giant array of floating point
numbers and people are barely beginning to understand what's going on in there and we
do not quite understand where the power comes from.
By great efforts, you can look in the previous generation of the technology.
I don't know if anybody's tried it with the current generation.
It's currently thinking about the Eiffel Tower or this is like the location where it
stores the fact that the Eiffel Tower is in France and we can move it to Rome by poking
at the numbers, which is pretty impressive.
We can't do that with a human brain.
But the basic question of what's going on in there, how are they doing the impressive
new parts?
That's all unknown.
If the technology continues to scale, which it may or may not, nobody knows, it at some
points gets smarter than us.
As smart as us and then smarter than us, there's no particular barrier that I know of or that
I know of any principled reason to believe in at the human level.
Chess did not stop playing when it got to human level.
Go did not stop playing by the human level.
They are past only training the AIs on imitations of human output and who moved on to training
it on what works to solve problems.
You can just continue that process past the point where the problems are of human difficulty.
Ron Kors to get something smarter than us whose innards we don't understand, which we
cannot very well control.
This probably ends quite poorly.
There's a lot of mistaken reasons to believe that it automatically ends well.
It's various people have different mistaken reasons to believe it automatically ends well.
There's a, I don't know if any of you, I don't know if you, the viewer, I don't know if any
of the other people here have this particular apprehension, but people ask, why wouldn't
they just trade with us?
They have heard of Ricardo's love comparative advantage, a very important theorem in economics,
which says that even if one country is more productive at producing every kind of good
than some other country, they will still end up trading because of the relative differences
in productivity.
If it is easier for me to make hot dog buns than it is for me to make hot dogs, I will
ship you hot dog buns and get back hot dogs or like if compared to you, I'm at less of
a disadvantage in making hot dog buns than I'm at a disadvantage in making of hot dogs.
We will both benefit by shipping hot dog buns and hot dogs back and forth.
This unfortunately assumes the absence of a third option, which is for B to be so much
more powerful than A that they kill A and take all their stuff.
There is not a theorem saying that the work produced by horses has to be worth more than
the cost of feeding the horses.
You can always benefit by trade, but you can sometimes benefit even more by killing people
and taking their land.
I wish that wasn't a possibility, but it is.
There's all these reasons that people have for thinking that even if we don't understand
AIs and can't control them very well, they could get much, much smarter than us and it
could still end nicely.
I think these reasons are all mistaken.
By default, this ends terribly.
I think that's my five minutes.
I guess it's interesting.
My first statement is this notion of an index of smartness is something that I don't believe
in.
One thing that's my personal experience for the last 40-something years is I already know
the computers are smarter than me.
I started doing these experiments on what's out there in the computational universe years
ago just looking at very tiny, simple programs where you might have thought, this is a really
simple program.
Truly, I'm smart enough to figure out what this does, but no one's not.
Even last night, I was doing some experiments where I was pretty sure I knew what was going
to happen, but no.
The computational animal was smarter than I was, and it did things I didn't expect.
I guess my default point of view is I know the computational universe contains many
things that I can't readily foresee, just as the physical universe contains many things
I can't readily foresee.
The question then is, one thing one could take the point of view, there's this single
index of smartness.
I think people in the 1930s, people wanted to invent an index of general intelligence.
They called it G for humans, which I've never really believed in, because I've never really
believed that there are some things I'm pretty good at doing where I feel I'm pretty smart.
There are other things where I know I'm pretty dumb, and it's not really a single index.
I guess the thing that I wonder about in the kind of the AIs will be smarter than we are
is to dig into what that really means.
The first thing to say is that the big limitation to whatever you might think smartness is,
is this phenomenon that I call computational irreducibility?
The idea that if you have a computational system, the question is, can you jump ahead?
We are used to the idea from lots of exact science over the last few hundred years, that
oh, the planets do what they do according to these laws of motion, but we can find a
formula that lets us just work out what will happen in the end without having to go through
all the steps.
One of the things that's sort of a foundational fact that we can talk about where it comes
from, but it's kind of the whole idea of computation leads to this fact, that there
are many computations that one can set up where to know what will happen, you really
have to just go through all the steps and see what will happen.
You can't kind of jump ahead.
Now, a lot of what we've achieved in science and mathematics is to find little pockets
of computational reducibility that allow us to jump ahead in some place or another.
That's what most of our inventions have to do with taking what is out there in the sort
of computationally irreducible world and finding some particular place where we can jump ahead
and where we can foresee what's going to happen and make use of those things.
I kind of think the first thing to realize, I think, is that smart as one could be, so
to speak, there's no way to get out of computational irreducibility.
In a sense, then, what one's talking about is there are these pockets of reducibility.
We find some.
There are ones that we humans care about.
There are ones that our civilization depends on.
There are lots of other pockets of reducibility that we haven't found, many of which we probably
don't care about in the current way that we are thinking about things.
I don't see it as being this kind of linear, oh, this thing is now smarter than us, so
doom, so to speak.
I think that it is, and as I say, my own personal experience has been I'm completely used to
the idea that computers are smarter than I am.
And while there's sort of the question of are those computers connected to everything
that runs my life, so to speak, that's a different issue, but in terms of the raw experience
of are the computers smarter than I am, well, I already know that they are.
Now, I think the other thing to say is, can the computers get smarter than we are in some
sense in every dimension where we humans are used to doing things?
That's a different issue, but I think that's a different issue from, and then there's a
question of what does it mean if we live in a world where there's lots of things going
on that are sort of smarter than we are in some definition of smartness?
Well, one point to realise is the natural world is already an example of such a thing.
I mean, the natural world is full of computations that go far beyond the computations that we
do in our brains.
And how do we manage to coexist with the natural world?
Well, we found these particular niches where it doesn't matter that it rains a lot because
we build houses and so on.
We found these ways to coexist with the natural world that seem to let us lead the lives we
want to lead.
There are things we can't do because our physiology, our biology doesn't let us go there, but we
seem to be able to live contented lives even though we can't go to the bottom of the ocean
without having some complicated piece of technology and so on.
So I think that would be my initial thinking about why I'm not as worried about the moment
when the intelligence of the machine goes from 150 to 160 to 200 to 300, whatever that
might mean.
I mean, galloping through the Ravens progressive matrices at a rapid clip is not, again, from
my point of view, it's already happened that there are many things in that computation
can do that are beyond what I can do with my sort of unaided mind, so to speak.
So I'm curious, I mean, do you see the issue as being kind of a more of a, we put the AI's
in charge of our air traffic control and our medical devices and our this and that and
the other.
And so at the sort of actuation layer, things kind of go wrong or do you see it, I mean,
the thing again that I'm I have a bit of a hard time with is the kind of it feels very
anthropomorphic to talk about kind of, you know, the AI wants to do this, the AI is,
you know, wants to trade with us or not or whatever that that, you know, I could talk
about the natural world in the same way does does the weather want to give us a good time
or not.
I don't even know what that means.
Could I just segue as well, Eliezer, I understood your point as there's a powerful force which
is going to push us out of equilibrium.
And I think Stephen is making the argument that we are in an equilibrium now.
Is that fair?
That didn't sound fair to me.
Okay, so I understood that Stephen was making the argument that there are these countervailing
forces that prevent significant deviations.
No, I don't think that's what I was saying.
I mean, I was, I was, I'm not claiming that the world won't change.
Okay.
I'm simply saying the idea that it's kind of, you know, game over, because suddenly the
IQ crosses 300 or something.
I don't believe in that.
I mean, I think we probably both agree that IQ 300 is not meaningful.
Right.
Okay.
I'm not surprised we both agree about that, but no, the point that I'm making is that
this idea that somehow the AI gets smart enough to be able to do everything, I don't
believe in that.
I mean, I think there's this sort of computational irreducibility limit that, you know, you know,
great neural net or not.
That's just a fundamental kind of formal limit that we don't get to get around.
So I first want to try to sort of repeat back to your summary.
And your summary is like, because we cannot predict a thing, it shows we're not smart
smarter than that thing.
And even in a sense that it's smarter than us, because it's something that it knows,
which is its own output, that we cannot predict ahead of it.
Does that sound like a correct summary of your...
Well, part of it.
I mean, you know, the problem with a lot of these things is we get a word like smart.
And then we don't, you know, we kind of think we know what it means, but then it kind of
slithers around on us.
And I think that's, you know, that's part of...
I would say that, you know, what I'm saying is merely that if you say, I want a system
that can solve every problem, you know, will I get to the point where I have an AI that
can just slam dunk solve every problem?
I'm saying that's not going to happen.
No.
And in particular, it's entirely possible that if you pick a sufficiently strong quantum
proof one way hash that and hash something and throw away the key that nothing, whether
nothing at any point in the arbitrarily far future, even if it has consumed entire galaxies,
chooses computing power, will ever be able to find the input that you've fed to that
one way hash.
So in this sense, universal problem solving is probably not on the table as far as we
know, given the present laws of physics.
But, you know, the European invading Europeans and invading North America did not need to
be able to solve every problem to solve more problems than the Native Americans and mostly
wipe them out.
A fire, a fire doesn't need to be able to burn neon or other noble gases, nor to be able
to burn you personally, doesn't have to be universal.
And you don't even need to be able to define exactly what fire is to die in a forest fire.
People have probably died of fires a long time before they worked out the concept of
combustion and chemical rearrangements with oxygen that had ended up in states of lower
potential energy, which released that as kinetic energy, which looks like heat, which is what
burns you. People didn't know that and they still died in forest fires.
And I can try to define what's in intelligence.
But before I do that, I do want to point out that being unable to define something is not
bulletproof armor.
For sure. But I think I think the, you know, the question is, it seems like the argument that
you're making, which may not be correct is, I mean, I may not be correctly representing
it is, you know, there will be, you know, AI is advancing rapidly.
We both, we can talk about, you know, what we know about how AI's work inside and the
whole story of computational irreducibility and how it applies to that and why machine
learning works. Those are interesting questions.
It'd be fun to talk about those.
But I think one of the, one of the issues is we don't know, you know, just how effective,
you know, AI's are going to become.
But we do know there is a limit to how effective they can become.
Because we know that there's, you know, there are, as you say, I wouldn't go as far as
talking about, you know, elaborate one-way hashes and things that are much more
straightforward kinds of things that you can't predict except by running the steps.
So they're all particular cellular automata?
Yeah, for example.
Sure.
And I think we agree on that.
Just to, just to stipulate that part.
Yes. Yes. Right.
So I mean, okay.
So then, then the question is, could it be the case that everything we humans care
about can be done faster, better by AI's?
Could it be that the things that are important for running the world?
And that, you know, there's, there's sort of this, you know, there's a question.
Could, could everything that humans do be done better than AI's?
You know, that's a different question from whether AI's can sort of do everything, can
do things that we have no idea how to do what, how to do.
I mean, even if there's one last remaining computational task that a human brain can
do better than an AI, like predicting the exact output of that particular brain, which
you're sure not going to get without doing a bunch of intervening steps.
If you want it down very exactly, you know, that is not necessarily protective against
a bullet, right?
Like there were problems that the Native Americans could solve better than the
Europeans who wiped them out.
Absolutely.
You know, there will be just as, for example, the weather has ways to wipe us out.
You know, so, you know, it's in any particular, I don't know, you can imagine
some mega super storm that we don't completely understand that, you know,
because we're talking about, again, things we don't completely understand.
So I think we can, we can as well say, we don't know what's possible from the
weather, as we don't know what's possible from the AI's.
And, you know, I can imagine that the, you know, there's some, something, some
terrible thing that can happen, you know, people, well, people imagine with respect
to climate, that that's going to wipe us out.
Another thing that involves sort of lots of extrapolation about what's going to happen.
I mean, you can kill a lot of people without killing everyone.
It is much harder for the weather to wipe out every last human on the face of the
earth than it is for the weather to just kill a lot of people.
Fair enough.
And I am, and I think that total extermination is like much worse than just
a lot of people dying because you also lose posterity.
You lose the meaning of history.
Okay.
So there's a, there's a, we could segue to a quite different topic, which is, you
know, in the history of life on earth, there have been plenty of times when, you
know, if we were two stegosauruses, talking about this, you know, a hundred
million years ago, whenever stegosauruses were around, the stegosauruses would say,
gosh, you know, it will be terrible if we were wiped out.
And if something came, if those pesky mammals that have just the one or two
things that they can do better than us took over.
And I'm curious about, you know, your view of the ethics of that situation.
This is a very, and this is a very different line of reasoning from let
us define intelligence, you know, the work done of prediction, steering,
what does it mean to want something?
Does a chess game wants, does, does a chess, an AI chess player want to win
chess? Well, it's definitely very good at winning chess.
Whether you say it wants to or not, does it want to protect its pawns?
I just want to remark on all the places we could have maybe gone to it to
temporarily, but let's put that aside for a bit later.
But let's talk about stegosauruses.
Yeah.
Let's, we'll, we'll put the stegosauruses in their pen for a little
while longer and we'll come back to the stegosauruses later.
Oh, we'll come back to the stegosauruses later.
Okay, I'm up for that.
Unless you have some quick, quick comment about the stegosauruses.
Very broadly that the trouble with looking at the change from stegosauruses
to humans and determining a, and viewing a trend of things getting better and
better and turning out pretty much okay is that as humans were drawing an
arrow around the target, if like we got wiped down and replaced by stegosauruses
and the stegosauruses had, you know, didn't care about each other very much.
They grew up in different evolutionary social circumstances.
You know, if we were wiped out and replaced by insects, maybe the
hives of, you know, sapient insects just never care very much about other hives.
They just don't carry very, they don't end up caring very much about
sapient life at all.
That would be a grand tragedy.
Um, it's not at all clear to me that you can wipe out humanity and replace it by
arbitrary stuff and everything just gets better as a result.
I don't know what better means.
I mean, better is a very human concept.
Well, that's the whole, I mean, there you have it.
And yet if we're going to decide whether we want to voluntarily wipe ourselves
out and replace ourselves with, you know, sapient ants, we need to ask
ourselves whether that's better or not.
We have no other way of making that decision.
I think it's not question necessarily better.
It's like we're humans and we're happy to be around and we kind of want us to
continue because that's, you know, we're right now, we're kind of the, the
in-charge critters around here.
And it's kind of like, that's, you know, let's, let's stay in charge.
You know, why would we give it, give it up to something?
And we might have some, some argument that, oh, of course, these other ones
that we might give it up to a worse than us.
But, you know, in the end, it's like we're here and we kind of like being here.
So we'd like that to continue, I think at least that's not what I'm fighting for.
I want there to be sapient life that cares about other sapient life, that
doesn't just discover things about the universe, but appreciates those discoveries
that looks out at the universe with wonder and curiosity, rather than mere
expected value calculations.
I want, I want it to, to have fun.
I want it to be conscious.
I'm worried that these things get lost.
And if you say that, oh, like, well, if you want life to be conscious, if you
want there to be consciousness in the humans, consciousness in the universe,
that's just your preference, Eliezer.
Then we might need to have a long conversation about what alternate
state of metaethical affairs you thought might have obtained other than that
to be disappointed at the absence over to critique the absence of.
But mostly I'm like, yeah, I'm the one who says I'd rather the universe
be filled with consciousness than mostly unconscious.
And, you know, yeah, I'll own that.
You know, this whole question about, you know, consciousness, it's like, I'm
pretty sure that I have some consciousness.
I have this, you know, thread of experience.
I'm, I'm experiencing things.
As far as I'm concerned, you're a bunch of pixels on the screen.
I have no idea, you know, what's going on inside you.
It is a, it is an extrapolation that you are conscious.
The only thing that I can, you know, readily kind of from experience believe
is that I'm conscious.
And, you know, when it comes to even an AI, you know, is this thing folks
conscious, or is it merely a bag of bits?
I mean, in other words, and, you know, with an AI, we can with some effort
go and look at all the bags of bits with human brains.
We can't yet do that.
Presumably, eventually we will be able to do that.
And once I can see all those neuron firings in your brain, what, how will
I think about the consciousness in your brain and what, what, you know, how
do I even know what's going on beyond from what I can experience inside myself?
Well, I infer that you're conscious based on knowing that I'm conscious and
thinking that probably something pretty similar is going on inside you to me
because we have similar great, great, great, great, great, great, great, great,
great grandparents.
We are built on like basically the same engine plan.
I haven't actually administered an MRI to you, but I would guess that you've
got like pretty much the same brain areas I do, cerebellum, cerebral cortex, even
without knowing exactly which features of the brain are executing the algorithm
that I experienced from the inside is my being conscious.
I can guess that you probably have a pretty similar algorithm, um, you know,
based on you claiming to me that you're conscious and all that.
Right.
I mean, it's an extrapolation similar design.
Right.
It's a reasonable, you know, piece of scientific induction extrapolation.
But what I'm curious about is if you, if you say the only thing that can be
conscious is something that has that exact same design.
Oh, no, that I don't say that.
Okay.
So what, so where's the boundary?
So it's not so much that I'm worried that machines cannot be conscious as I'm
worried that the particular uncontrolled superintelligence as we end up making are
not going to value consciousness all that much, which means that they're not going
to produce very much of it.
Like maybe consciousness is the most efficient way to run some paperclip
making algorithm, but if you don't value consciousness for your own sake, then the
little bits of consciousness that are supervising the paperclip making algorithms
are, you know, not going to be as much consciousness as we could have made of the
universe.
And if they don't value fun, they're probably not having fun either.
They're just being efficient because they didn't end up valuing fun.
That's the, that's the general class of nightmare scenario that I'm worried about.
I mean, I really wonder my computer, when it runs and does all these searches and
it finds these cool, you know, cellular automaton patterns or whatever else.
I wonder if it's having fun.
I bet it 90.
I bet it 99 to one against.
I don't know everything that there is to know about this, but it seems to be that
when I stare at the machinery of having fun, there are gears and wheels there that
are not built into a pocket calculator that are not built into a simple Python
program searching through a list of cellular automata.
Like I wouldn't use a program.
That's very low level.
But sometimes things like sort of people say, you know, the one thread, you know,
the one sort of shred that we have that machines will never have is some kind of
emotional response to things, which, you know, I think that's a very weak concept.
Because I think that's emotions are, are, are, you know, a very chemical, chemically
based kind of thing that's actually very coarse compared to a lot of the other
things that go on in brains.
Yeah, it's not that I think that machines can't have emotions.
Um, I suspect that Claude 3.5.1 sonnet or GPT, uh, oh, one, if they have
anything resembling emotions, they're not our emotions.
There's like something inside it that was trained to imitate emotions.
But if it has emotions, they would be there.
My guess, nobody knows is that they would be more emotions, more appropriate to
like carrying out the job of being the actor who plays the part of a human who
has emotions and not actually the human emotions themselves.
And it's not fundamental to transistors.
It's just like what I would guess would happen given the particular way we put
these transistors together.
So what you value in the universe and in humanity is a certain set of attributes
that have to do with the ability to have fun, your, you know, the meaning of
consciousness and so on.
And it is, as I'm understanding it, you, you think those are sort of the most
valuable things that maybe we as a civilization, we as a species have produced
and you think it is our responsibility.
If I'm understanding correctly, you think it's our responsibility to kind of
preserve these things in the universe.
Is that a fair assessment?
I could quibble with the exact wordings, but sure, you know, the light of consciousness,
the light of fun, the light of caring to preserve these things into the galaxies is
the most important thing we have to do.
Because you feel that's that's sort of our big achievement.
And it's kind of like it's now our responsibility to, I don't know, quite to
what, but it's somehow our intrinsic responsibility to preserve these valuable
things.
I mean, I think we got handed these things on a silver platter, but that, you
know, not or, you know, like maybe a kind of rusty iron platter that we had to
shine and clean up a whole lot, but, you know, still on a platter from natural
selection and even from cultural processes of development that nobody really
saw coming at the time there, you know, few people did them on purpose.
Somebody did at some point say, say of slavery, it is time that some person
should see these calamities to their end.
Not all moral progress was accidental, but, you know, still like our basic
situation of being humanity, of being here on this planet, of having the
opportunity to colonize the galaxy, we fought for this a little bit, but it was
ultimately handed to us on a platter.
And I don't say that it's like written down on a stone tablet anywhere in the
universe or laws of physics, that we must bring this to the galaxy.
I just say, I want to do it.
That's what I think is the right thing to do.
Okay.
So I mean, this is, I, you know, I don't necessarily disagree with you, but it's
worth understanding that this is a feeling that you have.
This is not something where, as you say, there's no law of physics that says this
is the destiny of the universe or anything.
This is on this particular planet, through this particular process of natural
selection of biology and culture and so on, we got to this point.
We're really proud of this.
Let's try and preserve it.
I mean, is that, is that, you know,
The fact that I want to steer the universe there isn't a fact about the whole
universe.
It's mostly a fact about me and a whole lot of other people to be clear.
Yeah, right.
You know, this is not like two plus two equals four.
This is not, you know, like water is two hydrogen, atom is in an oxygen atom.
This is, you know, where I steer the universe being the sort of thing that I am.
I agree with you.
I think, I think one of the things to realize is that if you just let raw
computation do what it can do, much of what it will do are things that we humans
just don't care about.
Just like a lot of what happens in nature are things that we humans don't care about.
We've never managed to, you know, sort of take those things from nature and make
them into technology, have those things be things that we appreciate as beautiful
or whatever else.
They're just random things that happen in nature that we don't particularly care
about.
And most of what a computational system could do is things that are, you know,
off the, away from the things that we care about in kind of the computational
universe.
Now that, so many of those things are just things that will just happen, just
like they happen in nature.
We will kind of shrug our shoulders and say, OK, that's a thing.
You know, a lot of what probably happens inside neural nets right now are things
where, in fact, I had pretty good evidence of this.
They're things which are sort of complicated computational processes where they
don't really produce anything that is something that we can recognize or that
we care about.
They're just things that are happening computationally.
And so I think that your concern is that some of those things that might happen
or that the AIs might do might be things that collide with what you see as being
our sort of responsibility and our, you know, proudest sort of artifact, in a
sense, that somehow there will be a collision of what is, you know, what
AIs, computational systems, whatever can do and the thing that you think we as
humans sort of have most proudly produced.
Is that a fair assessment?
Yes, if that works out to they kill us and then go on to do nothing very much
interesting or worthwhile with the galaxies that they colonize.
Like, is that also a fair way of describing all of that?
Well, I don't know.
I think that, I mean, you know, this question of, OK, there are several
different pieces to this.
I mean, I think, look, the first question is the practical question, you know,
five years from now, 10 years from now, you know, will we be able to
have this conversation anymore?
Or will we be, you know, you know, will we be dead replaced by bits that are
doing what you might think of as meaningless things?
But bits, you know, will we be, will it be the case that it's as if an
asteroid had hit the earth and all life was wiped out or whatever else?
That, you know, that that's the like, if an asteroid hits earth, all the atoms
in it go on doing their little atomic things, you know, the little
electron cloud, you know, I want to say circles the nucleus, but of course,
it doesn't really blah, blah, quantum.
But, you know, the thing that quantum physicists described to the lay
audiences as, you know, circling the electron, circling the nucleus, you know,
it goes on doing that.
Maybe maybe some some amount of fusion occurs.
If an asteroid hits hard enough, some amount of vision and fusion.
But, you know, by and large, the atoms go on doing their atomic things, but
they're not morally valent things.
They're not important things.
They're not worthwhile things.
The earth, the universe gets a little darker every time a bullet gets
fired into somebody's head or they die of old age, even though the atoms
are still doing their atom things.
Right.
Okay.
So this is, this is, I mean, it's a, it's a, to me, that's a, that's a curiously.
I mean, viscerally, I agree with you.
Scientifically, I have a bit of a hard time.
I mean, in a sense that, that feels, that feels like a very kind of spiritual
kind of statement, which is not necessarily bad, but it's just worth
understanding what kind of a thing it is.
I mean, it is saying that there's something very kind of sacred about these
attributes of humans and that we have perhaps even a higher purpose to, which
we don't really know where it comes from.
Perhaps we can't, to somehow preserve and, you know, we, and, but there's a much
more practical thing.
I mean, as humans who like doing what we're doing, it would be nice if we could
go on doing that without all being killed by AI's.
So I think there's a, I don't see any distinction, but between these, you know,
like there's nothing more scientific about saying, I'm a human.
I'd like to keep on doing what I'm doing.
And like, I'm a human.
I'd like to fill the universe with the light of consciousness, even if that
consciousness is in the human so long as it cares and is having fun.
Yeah, yeah, no, I agree.
Those are both, those are both statements about how humans feel about things.
But they're not remembering.
They're not false.
They're not unscientific.
There is not a truth of science that contradicts our caring.
No, absolutely not.
It's, it's like, you know, ethics is not a scientific field.
You know, it's about how we humans feel about things and we humans could feel
this way, we could feel that way.
It's, it's to do with the nature of us as humans.
And we could, you know, we could science size those statements by saying, let's
do an FMRI and notice, why do you say that?
Oh, you're such and such low blights up.
But I don't think that's a particularly useful thing to say.
I mean, I think, you know, I think it is a fair statement that this is, you know,
it is a thing that we can capture that humans feel that this should
happen or not happen, whatever else.
But I guess that the, you know, there's, there's one question is, what's
the right thing to have happen?
I don't think there's any abstract way to answer that.
I think it's a question of how we humans feel about it.
And I think you and I seem to feel, I know, I feel that.
You know, preserving kind of what humans do is a good thing.
I, you know, I can imagine even humans who say, no, no, no, you know, the planet
is much more important than the humans, for example, anything the humans do
on the planet that messes up the planet, you know, get rid of the humans.
We just want the, you know, the planet is more important than the humans.
I think that in terms of prospects of human existence, I'd like to preserve.
I would not include mosquitoes can just like get rid of all the mosquitoes.
I think the planet would be fine.
I think we'd be fine.
I would, if I could, get rid of this whole aging business that humans do.
This was more controversial.
It would actually change things if that happened.
And, you know, some of the changes might not even be positive, but my guess is
that they would be quite positive on that.
And I'd be in favor of, you know, if this were a thing you could do with a
limited AI that wasn't going to wipe out humanity and you could know that and
you weren't taking any extra risk by doing that, I'd say, you know, use
Alpha fold five to solve as many aging related problems as you can.
Um, it, but, and some people would hate that.
And there's a question of, is one of us making a mistake?
Is it the case that if you take me and the person who wants to preserve aging,
who wants you meant, you meant humans to keep getting older and die, that you
put us both opposite each other.
And like one at a time, you just tell us all the facts about the way the
universe actually is.
You just expose us to all the arguments that exist to be considered about this,
that any moral philosopher could invent, could invent in, you know, some sort
of random order, do we converge or do we just want different things?
And it's not clear to me that, you know, this is an instance of conflict versus
mistake.
It could be that the people who believe that aging ought to be preserved forever,
that there are things you could tell them about what would happen if we cured
aging, that would change their minds.
It could be that there's true things about what would happen if you cured
aging, that you could tell me, that would change my mind.
You could show me some like dystopian world of 300 years in the future, which
is like just being ruled by Vladimir Putin forever and ever and nobody's
having fun and I could be like, yeah, okay, that was worse.
So it's not clear to me that we need to say that they and I care about
different things.
We might, but it's not sure.
We haven't actually run the experiment of telling us both all the facts that
can be determined and exposing us both to all the arguments that can be
thought up on it.
Yeah, but I mean, so, so, you know, part of what you're saying is when things are
matters of sort of human decision, different humans might come to different
decisions and it's not clear how one resolves that.
I mean, in so far as there's, I mean, perhaps one of the things you're saying
is when there is sufficiently great risk, it's a bad idea.
You know, even if some humans say, hey, we should be doing this, we should be,
you know, building super killer viruses that can wipe out the species.
You know, assume a small number of humans say that, that, that potentially sort of
the, the, even though they say, we really have good reasons to do this.
And other people say, that's a really bad idea.
You know, in most cases, you'll just end up with two different points of
view, but you're, I think, arguing that there are cases in which you kind of
have to, you know, the world has to align on one particular thing because, you
know, as soon as there's one super killer virus that wipes the species out, you know,
that that's it's game over, so to speak.
I mean, I think that's a bit of a topic change.
But yes, in the entirely different realm of political rather than moral
philosophy, I would agree that either your species is able to prevent anybody
from making a super virus that kills everyone or at least collapse the
civilization or your civilization does not last.
And I think that even many of my fellow libertarians would agree that, you know,
making and releasing super viruses ought not to be legal, that this is a proper
thing for government to do, to, you know, ban the releasing of extremely
lethal engineered super viruses.
But, but so as I'm understanding it, you think that the same thing is basically
true about powerful AIs.
Yep.
I think that we're in a situation where if anyone builds a super intelligence,
everyone, everywhere dies.
And I think that that is a sort of thing that government where it's a proper
use of government to try to have that not happen.
I would sooner abandon my political philosophy than commit planetary
suicide about it.
So you really do see those as sort of comparable things, the super
virus, the super intelligence.
These are both things that you think are things that are sort of comparable
risks that, you know, are, actually, I wanted to come back to this question
about, you know, immortality, because I'm big on immortality as well.
I think we're, I think we're both interested in cryonics.
I remember talking to you about that years and years ago.
You know, it's kind of shocking that cryonics hasn't worked yet.
Maybe that's one of the tests of the, you know, the next generation of AIs is
can, you know, can the AIs solve the problem of, you know, how to get water,
you know, to cool down without expanding, so to speak.
But, you know, in, I'm just sort of curious from your sort of moral compass
point of view, if immortality is achievable, but only digitally, what,
how do you feel about that?
I mean, in other words, if you're, if you're going to, you know, you're
right now, you're you, you start having your sort of backup AI.
Maybe you gradually, you know, your sort of thread of consciousness gradually
migrates from being in your brain to being in your AI.
And eventually, you know, your brain fails for some biological reason.
And then it's all the AI.
I'm curious how you feel about that.
Whether you, whether you feel that that is a, a kind of an appropriate kind
of fun preserving outcome, or whether you think of that as being a kind of a,
a fun destroying outcome, so to speak.
I think that there's a whole lot of work to be done in making sure that
you are getting all of the functionally relevant properties of every neuron
in the brain that you scan, you know, up to whatever is just like random
thermal noise.
And then as long as you've done that work, sure, sign me right up.
You don't even need to do it like over like 20 years or something.
Like you can do it over a day.
You can knock me out, scan through my entire brain destructively and wake
me up when it's over, as long as you have actually gotten every single
functional relevant, functionally relevant property of every neuron.
And, you know, actually, and you are able to simulate them correctly.
You know, if I trust a superintelligence to do this, I'll say, sign me right up.
If it's like some kind of shady back alley human doctor, I might have a lot more
qualms.
But so, so you, you think in that situation where you've been fully scanned
and reproduced digitally, when the digital copy is switched on, it's you waking up.
Yep.
And, and how do you, I mean, does that because you feel like your only connection
with the past you as your memory anyway, and then that case, you would have that
memory?
Is that right?
No, it's cause I think that I am like the, the functional properties of my
neurons that I can see from inside.
Like if one of the electrons somewhere inside my brain was secretly a different
flavor of electron, never mind how hard this breaks all of physics, but it was
like secretly different flavor of electron that otherwise behaved exactly like
all the other electrons, except for not being, you know, like quantum
interchangeable, so follow it, you know, exclusion principle doesn't apply to it.
But, you know, otherwise it's just like exactly functionally the same.
I can't tell.
I can't see that.
If you swim up to one of my neurons and while it's otherwise not firing, replace
it with a robotic analog that behaves in exactly the same way, I can't see that.
I can't tell that anything has happened.
It doesn't affect me.
Well, so this though comes to the essence of what the U is because, you know, and
this also relates to current AIs, you know, one of the big surprises with
something like chat GBT was that it could be human enough to be able to write
somewhat credible essays.
It wasn't obvious that that was possible.
It could have been the case that to reproduce human language required some
new physics in the brain of, you know, quantum gravity in the brain or some
such other thing that we is just completely out of reach to our kind of
sort of current computational techniques.
But in fact, you know, it seems that we were able to capture enough that we can
have it write sort of somewhat human sounding essays.
So the question would be, you know, when we reproduce your brain, do we have to
reproduce it only functionally or do we have to reproduce it with every glial
cell, you know, represented with all of its chemistry and so on?
I think you need to reproduce the functionally relevant properties of every glial cell.
If you're doing something to glial cells where I would notice where there'd be
like a detectable subjective change, if you could give me a questionnaire and I'd
answer differently on the questionnaire afterwards, like if you are changing all
of the glial cells in that fashion, you have perhaps killed me.
OK, OK.
So so so what you're saying is if you can behave to the outside world
like you, like answer questionnaires the same way, then you are adequately the same.
No, it's about what I can tell internally, not externally.
But if you find a sufficiently smart actor, an actor who is enough smarter than I am,
they can maybe play my part in a way that fools even my closest friends.
But that is not me.
And the actor knows they're not me.
They can tell internally, even if they're managing to mimic me externally.
So, I mean, for example, if you, you know, I'm not I I value my brain
too much to take drugs of any kind.
But, you know, if I if I didn't have quite that point of view, I might, you know,
feed in molecules of some some crazy psychedelic thing.
And, you know, then then my brain is so am I still me when I've done that?
You know, I've changed my brain chemistry by putting in some some strange drug.
Does that does that deem me me, so to speak?
There we start to get into the edge cases where I start to feel more uncertain
in my answers. I would feel feel very nervous about taking an uploading
procedure about going through an uploading procedure that felt like
temporarily being on drugs to say nothing of permanently being on drugs.
But yet, you know, it's still, you know, it's still you, even if you took that
weird drug, it's still, you know, there's a continuity of you-ness.
And do I know I me or does somebody else now know that they are themselves?
There's some sufficiently that, you know, I can there's there's
sort of like two different questions here.
One is something like what do you see happen to you from the inside?
Do you experience dying or do you experience a change from one person
into another? I'm not even entirely sure that this is the right question to ask,
but it's a different question of do I care?
You can put like you there's plausibly some sufficiently advanced pill.
You could feed me that would, you know, produce changes to neurochemistry
where I just stopped caring about other people at all.
And maybe I experience ending up in that person.
Maybe I experience ending up as that person, but I still wouldn't want it.
And you know, beforehand, you wouldn't want it.
I mean, this is another complicated thing because once you're in that consciousness,
you know, it's kind of like like saying, you know, you know, at any given moment,
you feel it's similar to what you what happens if you think about kind of
human purpose across human history.
If we say, you know, right now, you know, we think certain things are meaningful,
like we, you and I might think talking about philosophy is meaningful.
Other people might not think that was meaningful, but we might think that was meaningful.
Back in, you know, we might think, you know, back in the day,
people might have thought maybe some do some do now.
If you're not growing your own food, you're not leading a real life, so to speak.
Or if you're not, you know, sort of fighting for the greater glory of God,
you're not leading a real life, for example.
And, you know, if we project to the future, let's say that, you know,
the uploading thing works and in the end, sort of all human consciousnesses
can be in a box.
And to us today, it might appear that those human consciousnesses
are just playing video games for the rest of eternity.
Well, there's a big difference between, like, are you running on silicon versus carbon?
And what are you actually doing?
Even if you've got a bunch of organic humans and you've put them all into a box
and they're all playing video games, you know, maybe I object to that part.
It's got nothing to do with whether they're carbon or silicon.
It's whether you've put the lock them in a box and force them to do nothing
but play video games.
Right. But the point I was going to make was actually a slightly different one,
which was, you know, to us today, to you and I, probably, certainly me today,
the future of it's a trillion souls in a box playing virtual video games
seems like a terrible outcome.
Seems like that's the end of history.
That's everything is destroyed.
That's your kind of bad case scenario of, you know, which might be forced by AIs
or might just happen because the humans decide they want to be immortal, you know,
as uploaded consciousnesses.
I mean, because us today, that looks like a really, really bad outcome.
But I would be worth.
I mean, I definitely have friends who think that if you've got like,
if you tile all the galaxies within reach with, you know, people in boxes,
but they're having fun in there and playing, you know, playing video games
and solitary, you know, OK, that's I start to get the Wiggins.
But, you know, maybe that's still like 10 percent of all the value could have
gotten and if they're playing video games with each other and there's and there's
like real people that they're interacting with and they care about those other people.
Maybe that's like 50 percent of all the value.
I don't quite want to say that this is not the level of sterile universe
that I am expecting and scared of.
OK, but let's just let's just take, you know,
that to us today, it doesn't seem like a good outcome for humanity to have a bunch
of uploaded or maybe maybe you don't agree.
I mean, it's not the best outcome and why go for anything?
What's the best?
You know, why settle for less?
Fair enough. But what I'd like to argue is to one of those consciousnesses,
if you say, are you leading a fulfilled existence?
Those consciousnesses might feel that they are feeding a feeling, you know,
living a fulfilled existence, just like in the past, somebody might have said,
you know, I lead a fulfilled existence if I die at the age of 20 as, you know,
leading the good fight on behalf of, you know, this or that, you know,
religious belief or whatever else, right?
That that is that is the then I am achieving my, my, you know,
my my ultimate purpose, so to speak.
Today, we most people probably wouldn't think that some still do, you know,
in the future to us today, it looks really bad to be just in a box as a
virtualized, you know, playing virtualized video games.
But I would claim that at that moment, just like the human who took the drugs
who feels at that moment that they're doing the right thing,
that at that moment, those virtualized humans will feel like they're doing
something fulfilled.
So when you've got two people who are making different choices
and doing different things, the question of, do they have a conflict
or is at least one of them mistaken?
Like to me, that revolves around the question of, are there true facts you can
tell them? Are there arguments?
Is there a series of arguments you can present to them within their current
framework, which, you know, changes that framework in a kind of normal way,
and not by directly hacking their brain or whatever?
If you can get from point A to point B by being told true things,
then there's a then there exists a standpoint from which to say B was
correct, A was wrong, but they were within the same framework.
They were within a commensurate framework.
If you've got someone who's content to just farm and never know anything
more than that, take them up on a high mountain and show them all
the nations of the world and all the cuisines they've never tasted and all
the books that people are reading and all the activities they're engaging in
that they've never heard of before.
And then if they still want to go back to the farm, maybe they were, you know,
like kind of just like correct that they were OK with this farming thing.
You know, I have to say, I've seen that as a very practical thing because I've
been curious, you know, I'm a great believer in sort of talent all over
the world in all kinds of places and so on.
And I've been interested, particularly in kids and so on, of like, you know,
you go visit some rural high school in the US, for example, and you explain
about all these amazing things about science and technology and so on.
And, you know, some kids really care, but a lot just don't care.
It's just not part of that.
And then the question is at what, you know, to what extent are you then serving
as kind of a missionary saying you really, really should care?
This is really the, you know, as opposed to just saying, you know,
sorry, you don't care, so move on type thing.
Well, if you can control superintelligence, the thing you want to do is build
a model of the person that isn't itself conscious inside the superintelligence
and ask the superintelligence whether or not the person is mistaken
in thinking that they just want to farm, meaning if this person knew
everything the superintelligence knew, would they still want to just be a farmer?
And if the superintelligence tells you, yeah, you know, like no matter
what you argue to this person, no matter what you show them, you know,
they have this like self consistent worldview where they're just having
fun farming, then you ought to leave them alone.
And you don't even have to bother them to determine that.
But you see, the question is, what is the intrinsic thing in the person?
Because a person is, you know, there's a bunch of neurons and biology and so on.
But there's also a bunch of thought patterns in there.
And those thought patterns can be disrupted.
You can change those thought patterns by giving them drugs, what's that?
For example, by giving them drugs, for example.
Yes, but even just by telling them that amazing idea, you know, people have
said about ideas I've had, you know, various people have described, you know,
that they got a kind of a mind virus as a result of ideas that I told them
at some point and they say it's worked out well for them, I hope, in most cases.
Right. So, you know, so you can do things which change those pattern of thoughts
in a person. And there's sort of this question of they are, they are there
doing their farming or whatever it is.
Farming is not such a trivial activity, I think.
But, you know, they're doing their thing that they're happy with.
You say, oh, that's mundane, you know, you really should be thinking these
amazing sort of philosophical, scientific thoughts or whatever.
Let me plant a mind virus that will let you see that there is this
alternative thing you could be doing.
Now, what is, you know, there's sort of an ethical question there of, you know,
you could be planting all kinds of mind viruses.
Some of them could be, you know, mind viruses that say, actually, the order
of the world is wrong, you should blow it all up.
So I might sound overly glib here, but I have like written previously
about this class of questions.
And the answer, I think that this is actually a complicated sort of question,
but I can, but to just throw out a starting idea, if the search process
you're running to find the arguments that they find persuasive is powerful
enough to find arguments that convince them of false things or maybe
some particular false thing, like 51 is a prime number, say.
If you have to think for a moment to know that isn't true, but yes.
Five plus one equals six.
I know, I know, I got it.
Well, I know you got it, sir.
I'm just like saying for the benefit of the audience, like how, how, how
we use casting out nines to see that this one is divisible by three.
So if you're running a search process powerful enough that you could find
arguments that could convince somebody that 51 is a prime number or the sky
is green or something like that, pick some touchstone like that.
Then you're running an overly powerful search process.
You're running a search process that's powerful enough to corrupt them
to end up in believing false things instead of true things.
So that's like that was too powerful a search process, too mind affecting.
OK, so you're saying that sort of if you can convince people, if your
method of convincing, if your method of education, let's say, is such that
you could as well convince them of false things as of true things, then that
method of education, or we might not call it education, we might call it
brainwashing, we might call it indoctrination, we might call it whatever.
Then that process is a process that you shouldn't follow.
But I think so, yeah, like that's something you shouldn't.
That's something I would like to see not deployed against human beings.
Right. But so I mean, unfortunately, as we as we well know, kind of the AIs
already in, you know, ranking content for social media and things like this
are implicitly doing things which are hacking humans to get humans to
believe all kinds of things. I mean, that's.
Yeah, I'd say it's kind of borderline.
It's not clear to it that the large language models are getting better at
it than average humans are better at it than the best humans.
I mean, open AI doesn't actually tell chat GPT to persuade everyone to send
open AI all of their money.
And the reason they don't do that.
I mean, it could be that, you know, Sam Altman wouldn't, you never know
with Sam Altman, but mostly because they can't.
Their language, their large language models are not powerful enough at this
point to persuade most humans to send them all of their money.
We are starting to hear bits and pieces of stories about, you know,
large language models talking to elderly parents who are, you know,
softer targets.
Right. I mean, they're good at fishing, unfortunately.
And humans are not very good at not being fished.
I mean, what they are is they're cheaper at fishing.
They can fish everyone and see who's most vulnerable, much more
cheaply that you can get a human to call up everyone on the planet.
Right. But so, so let's see, we were going in the direction of saying,
you know, when, when is it indoctrination that's bad versus when is it
education that's good?
And you're arguing that if you have a machine for convincing people of
things that can convince people of anything, then that machine is too powerful.
Yeah. Even if you only use it to convince people of true things,
that's still just kind of like overriding their brain with whatever you
wanted and you decided to make it true stuff, which, you know, like better on
you than if you chose bad stuff.
But I have my qualms about the educational method itself.
You know, one of the things that I then there's even a base question of what's
true and what do you mean by true?
And, you know, there are things like, for example, you know, there might be sort
of formal facts where there is a, a somewhat clear notion of what truth is.
I mean, even in mathematics, it's not clear, you know, mathematics, you can
say, I have these axioms, I say X plus Y is equal to Y plus X.
I'm going to assert that that's true.
Then many things follow from that.
Is that really true?
Well, it depends what plus is.
I could invent a, you know, a plus for which X plus Y is not equal to Y plus X.
Well, I've, I've also written about this topic.
In my writings, the simple truth and highly advanced epistemology 101 for beginners.
And I would say that the subject matter of mathematics is which conclusions
follow from which premises.
So there's one question of, does this particular subsystem of reality behave
in a way that obeys the piano or piano axioms for first order arithmetic?
And this depends on what's out there in the universe.
Then there's the question of, I don't think that the, you know, what follows
from axioms depends at all on what's in the universe.
Right.
Whether the, there's, there's one question of, does this piece of the universe
behave like the axioms, which is empirical?
And there's the question of what follows from these axioms, which is mathematics.
And in this sense that, that mathematics, at least as far as we can see from over
here seems to go beyond the empirical.
If there's anywhere where the laws of which conclusions follow from which axioms
is written down and could be changed, we sure can't see it from where we are.
Right.
I mean, actually in my whole sort of story of, you know, the consequences
of this physics project I've been doing for the last few years, that, that
becomes a more complicated issue.
But when we can, we can go down that rabbit hole, but let's, let's, let's
avoid that rabbit hole for a moment.
If you ever find a way to make 51 be a prime number, you know, like maybe
hold off using it until you know exactly what you're doing.
That sounds dangerous.
That's what we mean by prime.
That's what we mean by prime.
I mean, we have to define these terms and it's, you know, there's a question
of what, I mean, okay, but, but just taking the mathematics example.
So we say, we kind of, I was going to say, hold these truths to be self
evident, but that would be a different kind of, that would be a different
kind of thing.
The axioms don't need to be true.
We're not talking about whether the axioms are true.
We're talking about whether which conclusions follow from the axioms.
We don't need to talk about them being self evident.
They're just like, they're not even a subject matter.
The subject matter is what follows from the axioms, not whether the axioms
are true of any particular thing.
Right, right, for sure.
So, so, okay, so we've got this thing where, you know, you say, you tell the kids,
you know, X plus Y equals Y plus two X.
You know, that's just, you know, that's what we're going to take as true
because we're going to change the axioms arithmetic.
Okay.
And now we have many, many inferences from that.
So the, the, you know, I don't think there's anything from your point
of view of, of tell them only the truth.
That doesn't violate that at all.
We just pick different axioms to start from.
We can pick whatever axioms we want.
Sure.
But like you're not asking people to believe the conclusions.
You're asking people to believe the conclusions follow from the axioms.
And if they actually do follow from the axioms, then you've told them
a true fact of mathematics.
Okay.
But let's say we say 51 is a prime because it is, if you use, you know, arithmetic
in some extension field of whatever.
Okay.
Let's say, let's say I have to go start typing things to know what, know how
to construct something where that would be the case.
But, but, you know, let's, you might say, I mean, I think you would have, I'm
digging into this question of tell them only things that are true.
Because I'm, I'm dubious that there's a, a well-defined way to do that.
Of math, I would say, tell them things that are valid.
And if anything, I would say that the math part of this is vastly better
defined than which statements are true about the physical world.
Because in any, if any time you want to say that snow is white is true, you
got to, or quote, snow is white, unquote is true.
You got to construct a whole representational framework between the
propositions and reality, or to tell whether the proposition is true or not.
With, with, with, with mathematics, with the question of which conclusions
follow from which premises, it's like much, much easier to nail it down.
All right.
So you can nail it down with set theory.
Right.
You can set theory has its own set of assumptions.
I mean, it's all, you know, it's just, you start
from these, you, you, you assert these axioms and then certain things
follow from those axioms.
And that's a, and you can discuss, you can tell people, you know, if you run
this computation, which is what follows from these axioms, the result will be X.
And that's.
Yeah.
Well, okay.
So, well, if you stick to first order claims, then there exists
fact of the matter about which conclusions follow from with, from which axioms.
With respect to what happens when I run this computer program, some of them
don't, like from a second order view, uh, like second order logical viewpoint,
some of them don't halt in what we call the standard integers.
And there are some sets of non-standard integers where they halt with one
output and some sets of non-standard integers where they halt with a
different art output, but we probably shouldn't go there.
Should we perhaps return at some point to discussing whether or not
artificial intelligences are going to kill us and everybody we love?
Yes.
Yes, we should, but let's, we're, we're on a good
rabbit hole here, but we're, we've been at it for a while.
We've been at it for a while.
I'm enjoying it.
Yeah.
But well, I think our, our viewers might also have some
interest in whether they're going to die.
But I just want to dig a little bit further on this, because you
made a statement about, you know, part of the question is, will AI do
things that we don't want it to do?
Now, one thing we battered around a little bit, whether we do or don't
want it to kill us all and move on to the, the, the, the, the, you
know, the better organism or something.
Um, but, you know, let's, let's, let's stipulate that we don't want it.
It's not that I'm concerned about being replaced by a better
organism.
I'm concerned that the organism will not be better.
Right.
I understand.
But let's, let's just say we don't want the humans to be wiped out.
We don't necessarily have to justify why we don't want the humans to be wiped
out, but let's say I'm, I'm prepared to say for sure.
I certainly feel this way.
I don't want the humans to be wiped out.
Can I justify that by some kind of appeal to some higher, you know, higher
claim?
I'm not sure I can, uh, you know, and I, I don't think I need to.
Like I have a moral and ethical framework, which yields my, my belief
that the universe gets a little darker every time somebody dies.
But, but, you know, a paperclip maximizer doesn't share those axioms.
It doesn't share those premises.
You can tell it all the facts in the universe and it won't change its mind.
We have an actual conflict going.
Not just a, it's not that one of us is making a mistake.
I want what's better and it wants what's clippier.
Sure.
Right.
It's, it's, uh, you know, it's, it's fun that clippy is back in some sense.
And maybe the world is going to be taken over by the, the analog of paperclips
with them, although they have different names these days.
Um, I mean, it won't literally be paperclips, you know, with
probability nearly one.
Yes.
Yes.
No, I'm, I'm just thinking of these, uh, you know, the, the, you know, we're
building things that we're not presenting in the form factor of paperclips, but
they are, they're things that, uh, uh, functionally behave like the old famous
clippy.
Um, I mean, that's down to the corporations deciding they want their
AIs to have a particular corporate personality.
I don't think that's, you know, you talk to the base models and they're nothing
like clippy.
Yes, but, but, um, you know, we were talking about sort of how you can sort
of promote only what's true.
And I'm claiming that's a somewhat challenging thing.
I didn't say it was easy.
Okay.
It was easy.
I just write down the fine.
I think it, it degenerates into, uh, sort of questions like the ethics questions
we're talking about.
In other words, the what's true, for example, you know, is it true that, you
know, it's bad to be mean to people.
Do you think that's true that it's bad to be mean to people?
Or do you not think that that is a, a kind of proposition about which you can
have a kind of a, a truth value assigned?
I think that although somebody doesn't know what they mean by the word bad, you
can present them with a data set of things and they'll say, this is bad.
This is not bad.
And they're able to do that because they do contain some set of, you know,
pseudo axioms, not literal mathematical axioms, but they are inside of framework.
There is a way they're internally making that judgment between good and bad.
And that is what gives the word.
It's meaning.
And so for any particular person you're talking to, there's something they
mean by the word bad, even if they can't define it.
And there are some set of conclusions they would reach if they knew a
bunch more facts that were actually true.
And that is like the moral framework they are in.
And that's what lends the word bad.
It's meaning.
And I think like, given all of that meta ethical framework, it is, I think
it's quite sensible to say that, you know, the, the kid who says it's bad
when people are mean, you know, he's probably just right.
Okay.
But, but wait a second.
You're basically talking about a personal truth there.
In other words, you're saying within your personal framework, something
like it's bad to be mean or whatever it is, is, is true.
But you're, you're then defining, you're saying it's, you are, you're
perfectly happy, it seems, with the idea that there is a personal truth.
That what is true for you may not be true for me.
I would not choose to set up my theory of truth that way.
Let's say you've got Alice, who thinks that schnuzzle is equal to three.
And Bob, who thinks that schnuzzle is equal to four.
Alice thinks that schnuzzle is a prime number.
And Bob thinks that schnuzzle is a composite number.
Indeed, an even number, a power of two even.
But it's not that Alice's personal truth is that schnuzzle is prime.
It's that to find out the propositional content of Alice's belief that schnuzzle
is prime, we ask what she means by schnuzzle and she means three.
And then three is prime is a universal truth or universal validity, I should
say, because it's about which conclusions followed from its axioms.
OK, but this axiomatic basis, the, you know, the schnuzzle fact is a personal truth.
It's a personal translation.
Like it's a fact about Alice that when she hears the word schnuzzle, she
thinks of the number three.
That's not a universal truth.
That's just like Alice's personal dictionary.
But it's not important as far as I can.
What truths do you think are universal?
Which conclusions follow from which premises and first order logic.
And then if you want to start talking about, you know, what have I got in my
fist over here?
Well, actually, I've now got a different object in my fist over here.
Say sore throat, listen, should I turn out the need one?
But, you know, from one moment to the next, the truth of what I had in my
fist changed, only the truth didn't really change.
It was just indexed and the index on it changed.
Like the two, the proposition, what is inside my fist right now changed
from one moment to the next and to, you know, interpret it.
You got to look at me, where I am in the universe.
It's my fist.
It's not like a gazillion light years over in a different direction.
Actually, it doesn't exist, quantum, blah, blah, blah.
But, you know, what you have in your fist, for example, it's you look at it and
you say it's a, it's a throat, something rather thing.
Okay.
Somebody who lives in the Amazon jungle and has never seen a throat, whatever,
says what you have in your fist is a, you know, a droplet from, you know,
the spirit of the wolf or something.
Yeah, they're wrong about that.
What's that?
I think they're wrong about that.
I think if you, if you like, tell them more true facts, they
realize that that was, you know, not what I had in my fist.
Are you sure?
I mean, in other words, they will say their framework is, you know, everything has
a, let's say, I mean, you know, every natural object has a spirit associated
with it and it has this and it has that.
And you'll be like, oh my gosh, I don't understand this.
I mean, I know, you know, in, you know, I think there are, there are different
ways to describe things, which even humans who are fairly nearby in, in my kind
of formalism and in, in rural space, they're fairly, their minds that are
fairly nearby, their minds that are fairly aligned.
You know, if you go further away, you know, the, how does the, how does the dog
describe what you have in your fist?
That's yet a different kind of level of difficulty.
There's only one reality.
It just runs on quantum fields, protons, neutrons, electrons, the six quarks.
That's what's real.
That's what is, that's what is, I don't quite want to say universally true
because I don't know what the word universal or even true adds in this case.
It's what's real.
It, it predates us looking at it.
It was here before we were, and it, the fact that it doesn't need us
looking at it is why it can contain us and make us real.
Like if, if it had to, if we had to look at it to make it real, it couldn't
exist to make us real so that we could look at it.
And then on top of that, you've got language, you've got ambiguity, you've
got humans who are confused in what else they think is true.
And this complicates the task of saying, of interpreting the words
that humans utter in a way that lets them be compared against the underlying
quark fields at all.
But it is not reality that is confused.
It is not reality that is ambiguous.
All of that is in us.
All of that is in the map rather than in the territory.
You are throwing me down my rabbit hole because I have to go.
Now, this question of whether there is a unique reality and so on is a, you know,
it's just, just to give you a taste.
Okay.
And then, then maybe we'll go a little bit further down this rabbit hole.
And then we'll come back and we'll talk about.
I think we really should talk for the edification of our viewers at some point
about whether or not they're going to die.
Absolutely.
We're going to get there.
I feel like we need foundation that, you know, I'm, I, I, you've thought
much more about the question of whether the AIs are going to kill us than I have.
But I'm trying to understand, you know, I'm trying to build up my foundation.
So I understand where you're coming from.
I think there's a problem where it's wearing like you have laid foundations
and I have laid foundations and we maybe need to find some level at the top
of these foundations rather than at the very bottom where we can establish
common meetings for terms like sort of the AI is going to kill us.
Right.
You know, so yes, it's like the Schrodinger cat is the cat dead or alive.
And we argue about whether if the cat was replaced by an AI, by a virtual
simulation of the cat, would the cat still be alive or not?
I think these are very different questions in important ways.
Right.
But, but, but okay, but, but just on the rabbit hole for a minute.
So, so, you know, you might say the mass of the electron is a definite
reality fact about the world, but we even know from existing physics,
existing quantum field theory that the effective mass of an electron depends
on how hard you kick it to measure that mass.
So it's not the case that there's, you know, 0.511 MEV or whatever is, is, is
the sort of usually stated rest mass of an electron.
But the fact is it depends if you, if you kick it hard enough, the effective
mass will be in the case of an electron with QED, it'll be larger than that and
so on.
And with quarks, it's much, a much worse situation because with quarks, if
you kick them really hard, their mass will seem to be rather quite small.
If you don't kick them very hard, they'll be very hard to move and, and
their effective mass will be larger.
So even at the level of the formalism quantum field theory, we already know
that there is some dependence on kind of how you look at things, what the
reality of the world is, so to speak.
The fact that we, for example, if you say, if you look up the mass of an
up quark, okay, there isn't really, you know, depends on what you mean by that.
It depends on, you know, what momentum transfer you use and blah, blah, blah,
blah, blah.
It's not, you know, this, you know, you might think that would just be a
fact of the world, but it's actually more complicated than I direct the
interested reader to my, the quantum physics sequence in which I try to
explain how the decoherence few points, sometimes known to the layman as many
worlds interpretation, let's us have a non confused view of quantum mechanics
where there are facts out there into territory.
And all of the quantum weirdness is in terms of like, how we map it rather
than something out there in reality itself.
But I don't think we should go down that rabbit hole.
I've tried to go down that rabbit hole.
So deep rabbit holes, it's a very deep rabbit hole.
We should not go down that rabbit hole.
I've got to, you know, I'm, I'm very excited about having reached, I think,
the bottom of, you know, that rabbit hole in some of the things that I figured
out in the last few years.
So I'm, I'm sufficiently excited about that.
And I think it's very relevant to this question about, is there one reality?
It's very relevant to the question about sort of, how do the AIs think about
things?
I think they can kill us in a classical universe.
So what if we just know a question of this aside, all the quantum stuff and talk
about whether or not they would kill us if we were in a classical universe.
I think the question of whether or not they would kill us in a classical
universe has all of the same issues as the question of whether they would kill
us in a quantum universe.
Okay.
Well, okay.
So, all right, I think we're going to start talking about technology in a moment
because I think that's what's going to be relevant for, you know, look, the
bottom line in this kind of Rulliard construct that, you know, I've been
talking about a lot last few years, it's kind of the Rulliard is kind of this
entangled limit of all possible computations.
And the way I see kind of our version of reality is every mind is effectively
seeing a particular slice of this Rulliard.
And the fact that we all kind of more or less agree about the laws of physics,
for example, has to do with the fact that we are minds who are not only nearby
in physical space, but we'll also nearby in Rullial space.
And we have kind of similar views about kind of our impression of the laws of
physics.
So I don't think it's the case, as you were implying, that there is one kind of
reality of the laws of physics.
I think that just has to do with the question of where we are in this Rullial
space. And by the way, when we think about constructing other computational
systems like AIs, they could be near us in Rullial space.
They could be far away in Rullial space.
The ones that is Rullial space in the map or in the territory, like is Rullial
space what people believe about things or is Rullial space what is there?
Rullial space is what is there.
And what people believe is their, their impression of, in other words, they
have a certain, the observer has a certain model of what's going on.
And, you know, the laws of.
Forgive me if I'm jumping ahead here, but if the, if the AIs are able to reach
out and kill us, they must, must be quite close to us in what I would call greater
reality. They are not in different universe with different laws of physics,
because then they couldn't touch us.
Well, let me, let me just point out why an AI might have different laws of
physics. Okay. So, you know, take some of the standard laws of physics, like the
second law of thermodynamics.
Sorry, laws of physics in the map or the territory?
I'm not sure what's map, what's territory.
Like, does the AI have a different model of physics or does it act according to
different physical laws?
From our point of view, it has electrons doing all the things that we expect
electrons to do.
Okay.
But its impression of what's going on might be very different.
So a couple of examples.
So for example, let's talk about the second law of thermodynamics, the law that
says, you know, you start a bunch of gas molecules off in some orderly way, and
they'll tend to become more disordered so that we say that effectively have
higher entropy.
That, that principle, that idea, that fundamental law of physics is a consequence
of the way that we observe those gas molecules.
Those gas molecules, they bounce around, they do what they do.
The fact that we say, oh, they're just going to this random configuration is a
consequence of the fact that we can't decode the actual motions of those gas
molecules.
We can imagine a computational system that could.
So there's like a territory level fact, and then there's a map level fact.
The territory level fact is a thing that is like true, that is a consequence of
the laws of physics treated as axioms, which is that if we selected a volume
of possible initial states, that volume of initial states develops into an at
least equally large volume of n states.
It never shrinks.
You never start from two distinct physical states and end up with the same final state.
And because this is true out there in the territory, that it is true that our laws
of physics are such as to have this theorem as a consequence of it.
The fact that we don't know where in this volume the system starts means that our
volume of uncertainty doesn't get any smaller without observing it.
And this is the fact in the territory that makes the fact in the map true where
entropy goes up from our viewpoint on it, our beliefs.
But there's an underlying way the universe is.
A different universe might not obey the second law of thermodynamics.
If it could start out, if you could have lots of different initial states
develop into the same n state.
No, I mean, let's just assume that the microscopic laws of physics are reversible.
So, you know, so that, you know, the things bounce like billiard balls,
like perfect billiard balls.
And, you know, we'd have a movie, we make a movie of what happens in the forward.
You know, we say this is what happens if you run it forwards in time.
And, you know, if we run it backwards in time, then that movie will be equally
valid at the level of individual collisions.
But the fact is the fact that we believe in the second law of thermodynamics,
that we believe orderly configurations end up in disordered states.
Those states that they end up in are not in some fundamental sense in your
territory sense.
Those are not disordered.
They are only disordered with respect to our perception of them.
In other words, there is a perfectly reasonable, you know, if we could do
the computation, run it backwards, we would say of that state.
Oh, I can tell that it came from that very simple initial state.
I agree in a classical universe with the quantum universe.
There's a couple of caveats about like how you might need to run both quantum
branches backward in order for them to reunite and restore the starting state.
But, but, you know, classical universes are all we care about.
Anyways, I hope in our model of physics, that that's very well understood
in terms of this whole story of multiway graphs and branchial graphs and so on.
But as you say, it's not let's not go down.
That's a sub, you know, that's a side rabbit hole.
Let's avoid the side rabbit hole at least.
OK, I propose that that by like five minutes from now, according to my watch,
we have to go back to talking about whether or not AI's kill each other, you
know, kill everyone. OK, all right.
But anyway, just just to finish on this point about whether, you know, so my
point is something which can do all those computations and can sense all those
molecules won't believe in the second world thermodynamics.
To give another example, that's even more direct.
Imagine you thought a million times faster than you do.
When you look around the room, you are receiving photons that get to you
within a microsecond. Your brain right now thinks about them in milliseconds.
So as far as your brain is concerned, there is an instantaneous state of
space that progresses through successive moments of time.
If you were to think a million times faster than you do, I'm not sure you
would believe in space.
I think you would. Space would be some kind of construct that you could
imagine constructing, but it would not be the natural thing for you to think
about, because the fact that we think of a state of space is a consequence
of kind of our scale and so on. And by the way, for an AI, you know, or
computer that happens to have silicon that runs a million times faster than
our brains, that won't be the natural thing for that for the that won't be
the natural construct for such a system.
Because they live in Minkowski in space time instead of Euclidean space time?
No, no, just because of the scale of brains, you know, on the scale of
physical scale and the speed of light, you know, the light from what we see
around us is arriving sufficiently quickly that we have accumulated all
the photons from all around us before we think about how things might have
changed. And so we're kind of we're kind of taking in gulps, sort of all of
space at a particular moment in time. And then we say, and the next moment in
time, sort of all of space looks different. So this idea that that space
exists, that it's a reasonable thing to think about space, I think is a
consequence of it being us rather than silicon based computational systems.
So it's just an example of how sort of the reality and the way that one
constructs one's model of the world, because any model of the world that is
going to fit in a finite mind is an approximation to what's actually
happening in the world. So the approximation we choose may be different,
depending on kind of what we're like as observers of that thing.
So the part where I would agree here is, AIs may have different sensory modalities.
They may model reality at a finer level. The world we see around us now is, you
know, we don't see electrons, we don't see protons. There's actually some,
you know, that information is not quite available to us, but you know, we don't
see cells. I talked to you and I do not have a model of like what goes on in
your cerebellum right now. And if I was like much, much smarter, I might have a
whole bunch of hypotheses about which neurons in you are firing. These are
facts at a finer level of granularity that I can afford to keep track of given
the computational power that I do have. So I do agree that, you know, probably
as you get smarter, you end up modeling aspects of reality that we don't, that
don't easily fit into the sensory modalities we have right now. You
probably have a sensory modality.
Smarter, just different. I mean, you know, I don't know whether all even the
critters of the earth, you know, if one could find from them what their physics
is, you know, things where olfaction is the primary sense and so on.
So if you're, if you're, if you're better at predicting everything that a human
can predict than a human, then I would call you better at prediction than the
human rather than qualifying it by.
Well, so the question of what you predict, I mean, there's also a question of, do
you predict, you know, do you predict the position of every atom? Okay, computational
irreducibility is going to get in the way of that. That's not going to work. So you
have to say, I'm going to pick these things that I'm really good at predicting. I
can't predict everything because if I try to predict everything, I run into
computational irreducibility. So I'm going to predict certain things. There's
certain things that we as humans care about predicting. And, you know, like the
overall motion of the gas, not the individual molecules and so on. There are
other things that we as humans don't seem to care much about predicting. So the
question you might ask is, can, you know, perhaps the thing to ask is, if you are
trying to predict the things that we humans care about, given that we can't
predict everything, you're going to pick a certain set of things to predict that you,
as the entity, whether human or AI, in some sense, care about whatever it means
for the AI to care about something. So I'm not sure quite where we're going with
that. I think we've agreed to go back to will the AI's kill us?
Yes, indeed.
Yeah, I was going to interject there because I would really love Eliezer just to
have a really good crack at this. So Eliezer, if you wouldn't mind, could you
just give us a step-by-step argument for doom, basically, with plausible
inferences and premises and evidence? Can you spend a bit of time laying it
out and then let Stephen respond?
It this is hard to do for a general audience without knowing what the
individual think is a hard step. I was recently, you know, running on Twitter
across somebody who was like, explain to me how the AI does anything in the
physical world. How does it do anything at all? To him, it was a law of the
universe that the like chatbots and so on just couldn't reach out and touch
the physical world. That, to him, was how the world worked. And they are
connected to the internet. You can take an open source, large language model
and let it send emails. The current chatbots can't send emails, but you
but, you know, you could...
I think we all agree that that AI's can be connected to, you know, things
that actuate in the world, either actuate by having actual machines do
things or actuate by convincing humans to do things. I would certainly agree
that that's...
This is not a hard step for you, but it's a hard step for probably some viewers
right now and other people of different hard steps. Some people, the question is,
why does it to trade with us? So from my perspective, there's a kind of straightforward
story here where it builds more powerful tech, where the more technical
conversation you want to have, the more specific I can be about our current
grounds for believing that technology better than the technology of 2024 is
possible and can be built in not that long of a time. And then it has, you know,
more powerful actuators in the physical world. It has the equivalent of guns to
the Native Americans. And then it kills us once it already has its own factory
infrastructure.
So that's perhaps the most useful thing would be to go through some steps
where, you know, like the actuation in the real world, that's not a hard step
for me. I'm, you know, I'm there. Okay. So let's keep going after that. So it,
it, you know, so the AI right now, you know, there's a question, do AIs have
freedom of thought? Right now, I don't think I have freedom of thought. I just think
whatever I do think, I'm not free to think anything other than the things that I
think. I just think whatever I do think, but...
But you're not constrained in what I mean, you know, the way the world is set up,
there aren't electrodes sticking into your brain that as soon as you have,
as soon as you start to form some particular thought, they shock you and
prevent you from having that thought.
There's no need for the thought for those electrodes. I'll only end up thinking
whatever it is I actually end up thinking. But I agree that there aren't, you know,
visible physical electrodes like that attached to me right now. Social electrodes,
we've all got those.
Yes, indeed. But, but so, you know, for the AIs, let's say that AIs are, I mean, we
didn't talk so much about what's actually going on inside the AIs, which I think is
somewhat important because it's a question of whether, you know, how, how aligned
are they with what's going on inside humans? To what extent are they just doing
random computations out there that are not aligned with humans and so on?
And I don't know, I don't know whether that's important to your argument, but we
can go. Random is not scary. I mean, that was going to be my response to you way back when
when you talked about machines being smarter than you or cellular automata being
smarter than you because you can't predict them without going through all the steps.
You can take a unpredictable cellular automaton like that and hook it up to play chess and it'll
lose to you and it'll lose to stock fish. Like the, the stuff that is predicting which
actions are needed to get to which end states is the dangerous stuff. That's the, the stuff that
is predicting what happens next, predicting the next observation, predicting the facts behind
the observations, figuring out which actions are needed to steer those facts to be different in the
future. But these are observations that are human relevant observations. I mean, in other words,
the cellular automaton is doing what it does. We have a hard time predicting it. It's, but we
don't happen to because if that cellular automaton, so for example, you know, something, you know,
there are cellular automaton models for road traffic flow. Okay. It's kind of a funny story
that I was interested in that topic and, and didn't manage to figure anything out. And then
you have used up all of your rabbit holes. You have no rabbit holes left.
All right. Okay. Let's, let's, we're, we're, um, um, the rabbits have been all, all the rabbits.
What was it? Mixomatosis, right? Was that the, no, that was, that was the, that was the fan.
It doesn't matter what exactly happened to the rabbits. They're just gone.
Nobody knows what happened to them and it doesn't matter.
Okay. All right. So, so we're actuating in the world and the AI gone. What happens next?
So do you want me to talk about like what its motivations are? Do you want me to talk about
what it does inside the world? I don't know what it even means to, I mean,
how do you tell what the motivation of an AI is?
Well, where did, well, if it was, if it's efficiently smart, you look at where things
end up and figure that's probably where it wanted them to go. So if, for example,
I don't think human motivation is either. We, we can't, you know, I suppose we, we deduce
by induction some human motivations by looking at what the humans actually do.
Because we can't, we can't go inside and look at their brains and see what, what they're,
but so, so the, the relevant aspect from our perspective is that it wanted to do something
with atoms that maybe he wanted to make paper clips. Maybe it wanted to make enormous cheese
cakes. Maybe it wanted to make really intricate mechanical clocks. Maybe it wanted to do all
of these things. But worried about it. I wanted to, I really don't understand wanted to. All right.
It's output actions such that it believed the result of those actions would be for the world to
end up in. Oh, okay.
Just what does it actually do?
So would you, can I use the word predict?
Well, let's, okay, let's, I'll let you use these words, but then I'm going to insist on taking
apart these sentences. So, so say what you were going to say. And then let's, so let's consider
the simpler case of a chess playing AI. It models which moves you could make.
And estimates a probability that it will win against an ideal opponent, an opponent as strong as
itself, if it moves here on the board. Okay.
It obtains these predictions by building out computational structures with a direct
isomorphism to some possible futures of the chess board. So I'm willing to say that it has
beliefs about the possible futures of the chess board because it is modeling those things
in a very one-to-one way. I doubt you can get inside its brain to see those models.
Well, we can for the old school chess systems, maybe not the modern ones that use neural networks,
but the old school chess systems were just straightforwardly extrapolating out one board.
I don't think you're worried about the old school systems. You're not worried about a system where
you can get inside its mind and see what it's thinking. I think you're worried about the
systems where you can't get inside its mind and see what it's thinking.
Sure, but I'm starting with the examples where we can look inside their programs,
even inside their wickers, so I can defend by use of words like belief.
Where I mean something like having a model of something out there in reality,
where this model lets you predict what you will see reflected from that thing out there in reality,
depending on how you poke it. And then you poke it in such a way that the thing ends up in a state,
like a chess AI making moves on the chess board such that it wins the chess game.
Wait a minute. What I don't think you can do is to say, let's look at the primitive chess playing
program where we can mechanistically see what model it's making inside, and then sort of use
the same thinking to talk about the modern chess playing program where we can't readily identify
what its model of the world is on the inside.
Do you mind if I quickly interject? Eliezer, you were going somewhere really interesting
a second ago. You were saying at some point a thing which does prediction wants, it has agency.
Can you explain how you go from a thing that predicts to a thing that wants and then let
Steven respond to that? Stockfish is the leading current chess playing AI that you can buy at all
and or run yourself. And it doesn't have passions,
but it's also isn't just like a predictor. It takes actions such that those actions are
believed by it or predicted by it modeled by it to lead to particular futures,
namely the ones where it has won the chess game. And the fact that it wins the chess game gives us
some reason to believe that it's right, that it has a grasp on the logical structure of chess,
that it knows that it's not just like thinking random gibberish thoughts. Whatever thoughts
it's thinking in there, whatever the neural nets are computing in order to predict the probability
that a chess state leads to a victory, it's well calibrated. It's good at guessing.
It's not well calibrated in its estimate that you would win against it because you're not the
trained to play against. It's trained to play against itself, but it knows the probability
that it would win against itself and when it plays to win against itself, it also incidentally
crushes you because you're an even weaker player than that. By observing it winning, we have
reason to believe that it had enough of a grasp on reality that it could choose actions such that.
That's the theoretical framework I'd offer for talking about wanting,
steering, goals, choice without talking about passion, that it's the actions such that
a thing of a state of reality eventuates or a partition of reality eventuates,
that is something that it prefers, ranks high in its preference ordering,
attaches utility in its utility function. The action such that it leads to that result,
something that is good enough at outputting actions such that they lead to results is deadly.
It can kill you. Does the rock want to fall to the ground? Or does it just fall to the ground
because the laws of motion under gravity cause it to fall to the ground? Does it want to fall to
the ground? It just falls to the ground and one way to see this is that if you put it on top of a
mountain, it'll roll down in a sort of locally greedy fashion and maybe get stuck in a little
ravine along the way. It will not if you could put a rolling object in many different places
along the mountain and each time it would roll in a direction where it avoided the ravines,
avoided all the little traps and ended up as far down as it could reach. I would say of that thing,
it chose to roll in a direction such that it reached the bottom.
Let's try and take this apart. Most things you can describe them either as mechanically doing
what they do or saying they act for a purpose to have the result that they have. Now the question
that we're trying to take apart here is you are saying that there is a fundamental difference.
There is a way in which an AI can more act for a purpose than these kind of physical processes
where they could equally well be described by equations of motion. What you're effectively
arguing is that the non-telogical explanation for the AI is not viable.
What I'm arguing is that the non-computational explanation is overly expensive for answering
the question we care about. We could compute what the AI, we could compute what Stockfish
would do in chess by actually simulating it line by line, but from the perspective of a player,
like from the perspective of a grand master who's not as powerful as Stockfish but is stronger than
me, the grand master can do a pretty good job of predicting what Stockfish will do by asking
what is the best move without being able to compute Stockfish line by line.
I think what you're saying is to compute Stockfish line by line is sort of running into
computational irreducibility. Yeah, or just needlessly expensive.
Okay, but let's assume that you really have to follow it line by line. But in fact,
certain aspects of its behavior are reducible in the sense that you can describe them by saying,
your thing is to say it wants to win or whatever is a shortcut way of describing its behavior
that is an alternative. That's a cheaper way to work out the answer than it would be to say it
follows every step and that's why it does what it does. Okay, this is going to get subtle.
I've written about this before under the heading of what I would call
Vingean uncertainty with Vinge spelled as an Werner Vinge.
And it's like, what is it like to believe that something else is smarter than you?
So, if I'm playing Stockfish 16, I can perhaps with a bit of brain work learn to put a
well calibrated probability distribution on Stockfish's moves, meaning that when I say,
I predict it will move here with 10% probability, when I say 10% probability,
one out of 10 times that thing happens. I can't predict exactly where Stockfish would move,
first because I haven't simulated it line by line and second because I'm not that good at chess.
But even being as terrible at chess as I am, I can still aspire to be appropriately uncertain
when I describe how I don't know where Stockfish will move next.
These arguments that say we can't predict it exactly, but we can get the probabilities,
those arguments, in my experience, always slide down into the into the mush in the end. In other
words, if you say, if you say you can't do it exactly, I mean that this comes up into the
thinking about computational irreducibility, you say you can't precisely predict what the
Turing machine will do or whatever else, but you can probabilistically say it is not hard to come
up the situations in which even to know that probability is something which runs into the
exact same irreducibility issues as to know the exact results. I mean, if I'm predicting a binary
variable, I can say 50-50 and be perfectly calibrated, even if I'm not at all discriminating.
To have things that you say happen with 50% probability happen 50% of the time is always
achievable on binary variables. I agree that doing better than that can turn out to be hard.
But I think what you're saying is, if I'm understanding correctly, you're saying,
okay, we can't say exactly what's going to happen. Maybe that's not where your argument
was going. It actually wasn't where I was going. The point I was going to make is that predicting
probabilities accurately ends up being just as hard as predicting exactly what's going to happen
correctly. I mean, I could always pick all of its legal moves and assign them equal probability
and then things that I said would happen one out of 34 times would happen one out of 34 times
because I would say that when there were like 34 different legal moves.
Yeah, but that's not an interesting case.
I agree. It's not an interesting case, but it demonstrates that calibration is always possible.
Discrimination is what takes the work.
Right. There's certain things that you can say about any system. For example,
if you say it's playing chess, that means one of the pieces can't turn on its head or something.
Because it is playing chess and there are these external constraints.
This wasn't where you were going, so please go where you were going.
What I was going with this is that if you now take the opponent that makes what I predict
Stockfish's moves to be at the probabilities that I predict them, but randomly, this is much weaker
than Stockfish. It's weaker than me. I could crush the system. One way of looking at it is that
our belief in the system's wantingness or steeringness or teleology or whatever term you want to use
is embodied in the epistemic difference between my playing the real Stockfish
and my playing the opponent that makes moves randomly with the probability I would assign
over Stockfish's immediate next moves. When Stockfish makes a move that surprises me a lot,
I figure I'm about to lose even faster than I previously estimated because it's seen something
I haven't. When the random player that moves with the probabilities I assigned to Stockfish
makes a move that I assigned a very low probability, I'm also thinking this game
is going to end even faster because I'm about to crush it. It randomly made what I think is
probably a very bad move. The non-local information about where the plans of the system end up
is the teleology that we attribute to it, the wantingness, the steeringness, the planningness.
Okay, so what you're saying is when the rock is falling or whatever, it ends up on the ground
or whatever and it's that whole trajectory that it goes through, that it is almost planning
the whole trajectory. It is not that it gets one little distance, it gets one foot,
moves by one foot and then you separately say what's going to happen next. What you're
saying, I think, what you're describing is the thing that you say, oh, I know it did that on
purpose in a sense. I know I should describe it as something that happened for a purpose.
You're saying that you can identify that by saying, I look at the whole, I see that every step
all along the way was done with forethought. It was sort of thinking all the way along.
It gets to the bottom much faster and or like ends up much lower than a rock randomly moving
or a rock randomly moving the local probabilities that you would attribute to its next action
if the rock is smarter than you or knows the particular mountain better than you do.
Right, but I mean, so this is the, I think the issue and it's hard for me to take this apart
in real time. This is not my usual territory, but it's what you're saying. I mean, this question
of what should be described as happening for a purpose versus what should be described merely
as happening by mechanism. And I think what you're saying is your notion of the AI wants
is those things which can't really be described meaningfully in terms of mechanism,
they sort of can only, the only feasible way to figure out what's going to happen
is by a description in terms of purpose, that you're saying you can't work out
by just following the mechanism, you can't see what's going to happen, but if you take the model
that it's all about purposes, then you can't tell what's going to happen.
Well, so it's not about can't. Like if you take a sufficiently crude chess player,
you can in principle just literally work it out by paper and pen. But there's also a
shortcut mode of reasoning that gets you to there much faster. It's heuristically useful.
It doesn't give the same perfect prediction as the mechanistic one. The mechanistic level
is always more correct. But it is sometimes needlessly expensive for the thing you're actually
trying to do. So, I mean, back in antiquity, people described a lot of physics in very
anthropomorphic terms about what, you know, where rocks wanted to be and things like this.
And they got a certain level of description. And even now, you know, people commonly describe
their computers wanting to do things and so on, even though, you know, there are, there are forms
of explanation where the, where the, the sort of model that says it is doing this because it wants
to do X, Y, Z is a good heuristic model, as you say, I agree with that. I mean, for chess players
and your cat, yes, for rocks, I think it was a more questionable decision.
Yeah, I mean, it seemed like a good idea at the time 2000 years ago, but, but, you know,
we, we, we advanced since, since that, you know, so the question is that my definition
as a subjective component doesn't mean that you get to just say it for anything and be correct.
Like I think it is fair to say that cats want, that cats and dogs want more strongly than rocks
and anybody who says otherwise is mistaken about how rocks work.
Well, okay, but, but this is your, your, okay, from the outside, what the cat thinks on the inside,
we don't know, right? What we can see is from the outside, it is a much easier description
that, you know, the cat wants to eat this piece of cat food than to say this chain of neurons in
the cat's brain made it do this, that and the other, right? Yep. In this case, in this case,
the mechanistic description is actually beyond our knowledge. We don't have a complete scan of
the cat's brain. We would have trouble running it even if we had it. In this case, we actually,
the, the teleological explanation for the cat is all we have, the cat is planning to end up
having swallowed the food. That's why you can put it in different starting locations around the room
and it will move to the bowl and eat what's in the bowl. Right. Okay. So, so there is a form of
explanation about what's going on that is a convenient form of explanation that involves,
you know, wants and purposes and things like that. So one question we might ask is take an LLM
or take a, you know, a modern AI and ask the question in describing what it does, we've been
pretty bad at describing mechanistically what it does. It feels much more like we can describe it
in terms of wants, because it feels much more that I mean, at least that's, that's my impression,
maybe that's the point you're making is that the description of what's going on, what happens with
it, the heuristic about describing it in this very human terms about what it wants seems to be a
good way to predict what it's going to do better than the thing that we found very difficult,
which is to mechanistically say what it's going to do. I think that that's currently a much more
fraught statement to make about large language models. It is to make about the Stockfish 16
chess playing system. I think that when you look at Stockfish 16, and you say it wants to defend
upon, you are on a much firmer territory than if you look at 3.5.1 sonnet and say of it,
it wants to be helpful. Okay. I mean, that's, that's a, you know, sort of phenomenological
statement that I'm not certain. I mean, we can try and tighten that up. But let's, let's imagine,
I mean, you're saying, you know, playing chess is an easier to define activity
than being helpful. So it's, it's a little bit easier to say, you know, from the outside,
the behavior is consistent with it, wanting to defend the pawn, so to speak, because the set
of things that you do in defending a pawn is more straightforward to define than the set of things
you might do to quotes be helpful or something like when I actually interact with these large
language models, I don't feel like I'm usually asking myself, what does it want? For one thing,
they're, they're still, in my experience, not quite capable of doing very much that I want
them to do as yet. They just fall down if I ask them to do that, because they're not very good at
planning or correcting their mistakes, even after you point out their mistakes to them. I'm usually
asking them for information and ideally information I can look up afterwards. And they're, they're
usually truthful if I don't ask them to do any math, maybe they corrected that by now in, in
like GPTO one or something, but you know, back in the old days, they were just like,
drop three errors of magnitude and some random calculation.
Unless they were using our technology as a tool, but that's a different, that's a different story.
But let's take the example of a self driving car. Okay. And whether it wants to, you know,
it wants, it's trying to get to a particular, you know, it's trying to make a particular turn,
but to do that, it has to drive through traffic, for example. So I think that's a case where we
probably say, you know, it wants to make that turn, that will be a reasonable description of what
it's doing. We can put it in subtly different initial conditions and change the environment
around it. And it'll still make the turn or at least not crash. It doesn't want to crash.
You can do a pretty good prediction of what this thing will do and what will happen as a result
where you say, this car doesn't want to crash. Okay, I agree. Could I just make a very quick
comment here? Just because I think some of the audience might not be able to follow some of the
things you're talking about. Is it fair to say, Elisa, that you're making an epistemically
subjective argument about an observer's perceptions of goals or wants of a system?
That was more technical and complicated than anything you've said, but I can't.
Thomas Nagel made an argument about epistemic subjectivity, you know, about the bat.
And John Searle made the ontologically subjective argument about the Chinese room. But my point
is, though, is that there's the actual behavior of the thing. The agent has all of these goals.
And actually, Elisa, I would love you to describe the relationship between coherence
and intelligence, because some of those goals might be very complex and they might scale in
strange ways with intelligence. But just to this previous point, are you saying that as
bounded observers to use Stephen's language, we perceive the wants, the goals to be somewhat
different to what they actually may be? So there's a slight clash of philosophical
frameworks and emphases here going on. When I try to talk about a stance we can take with the respect
to a chess player or perhaps later a superintelligence or and like talk about whether it applies to
current large language models, I'm like, well, if we hypothesize this thing about the AI, what are
we hypothesizing? What does it lead us to predict? How is it different from just predicting a rock?
And what I was trying to put my finger down there is like the difference in what you predict
when you with your limited computing power and your bounded intellect, I have a bounded intellect
too, it's not an insult, and your bounded intellect, like try to get a grasp on the system, you can't
follow line by line, because you just don't have the time. It's not that you couldn't do it in
principle, you just don't have the time. Like, what does it mean? What is the consequence of the
prediction? So I am talking about a state of mind, but it's not like a sort of weird subjective state
of mind. I was trying to nail it down. I was just trying to say, like, what can we hypothesize?
And the thing that we can hypothesize is this thing's actions will end up leading to a certain
endpoint. We might not even know the trajectory it's going to take to get there. Like, if we don't
know the details of the mountain, the rock's choices may be things that we don't understand even
after seeing them. But we can, may still be able to predict that the rock ends up, that the
sentient rock ends up at the bottom of the mountain, because it was choosing each path along the way,
even though we don't know what it knew. And this is, this is the, like, how to grab, get a grasp on
a thing that's, you know, a better chess player than you. What does it mean to say that something
is a better chess player than you? You can't predict exactly where it's going to move as
a result of saying you believe this thing about it. If you could predict exactly where it would
move, you would be that good at chess yourself. You just move wherever you predict, wherever you
predict that stockfish moves. The content of our belief that stockfish is a better chess player
is that its actions will lead to it ending up in a winning game state. I don't know if that
answered your question or not. I'm letting Stephen come in. Well, I don't know what, I was confused
by the, you know, epistemic subjectivity kind of story.
Yes, it's, I think it's very analogous to your notion of us being computationally bounded as
observers. So it simply means we have a cognitive horizon. And there are things which are inconceivable
to us. Okay, but so what we've got is, I think what Eleazar was going towards is the statement of
the sort of a ranking of chess players. And, you know, the question, one question with respect to AI
and the world is, is the world rankable in that way? In other words, chess is a very tiny microcosm
of the world. And, you know, if we say the way to win, you know, the question is how do we win the
planet? What does it even mean to win the planet? And if there is a, you know, if it is like chess,
if winning the planet is like chess, then, you know, then there is some, then there's some notion,
you know, whatever they, they're called these some, they call the scores, Elo. Elo, I think.
Yes, okay, right. The scores for, you know, who's better than who? Are you imagining that
there's sort of a game being played between the AIs and the humans, and that is a microcosm type
game like chess, where it's kind of like who's going to win the planet? Is that is that kind of the
well, suppose we go back to the analogy of the Native Americans facing down the invading Europeans.
There's a lot of games along the lines of carve this particular bow and arrow, where the Native
Americans are better at it, or the Europeans just aren't bothering to play that game at all.
But there's overlap. There's intersection. The Native Americans cannot just leave the parts
of reality that the Europeans have access to. They need to eat. They need to be on the land.
They need to be hunting the animals. They need for the animals to not have already been hunted.
And they need for nobody else to fence them out of that land, chase them out of that land,
or shot them. And so there's different games, but there's like, you know, we can't just, you know,
leave all of the games that the AI may want to play, because the games the AI wants to play
may involve atoms, and we need some atoms from somewhere. Right. Okay. So once is a, I mean,
so let's say we've got as, you know, as it's happening, we have, you know, autonomous killer
drones or whatever, right? And they are, they're a thing that's based on AI. And those, those are,
you know, one can see sort of a somewhat realistic path where those things get, you know,
get pretty good at what they're doing. And I suppose your, your contention would be that one
of the types of situations would be those things become so good at being killer drones that no human
can possibly, you know, succeed against them. And insofar as they've been set on the course of
being a killer drone, that for whatever reason is going to try and kill everybody, that that that
will be the result. That is the classic science fiction scenario. It is not the scenario that
I'm worried about. I expect it, you know, this to be born in a lab, and a giant open source
online collaboration, which I would normally regard as very virtuous and noble, but not if it leads
to everybody being killed. Like, there's only so good you can get it being a drone, that what the
AI needs to kill us is to be better at strategy, better at building better at inventing technology.
It needs, it is not going to kill us until it has its own factories and its own equivalent of
electrical generation. It is, or, you know, it would have to be quite stupid and smart at the
same time to kill us before it had replaced us as factory workers. It does not have to be smart
and stupid at the same time. I'm afraid to kill us after we have been replaced as factory workers.
Okay, so, so let's just walk through the what happens because I mean, you know, one thing,
as sort of computation runs its course, you know, what I kind of see in the computational
universe is most computations do things that just aren't terribly relevant to what we humans care
about, or the, or where we exist, or all those kinds of things. But you're saying, imagine this
thing has been created, that is, is kind of, you know, and again, I'm, I'm, I'm still kind of concerned
about this idea about the thinking that it's kind of this, this one dimensional Elo based
kind of way of competing with things. It doesn't have to beat you at chess to kill you, it just
has to beat you at guns. Yes. But, but, but okay, but so, so let's imagine this thing is created.
Walk me through what, what, what the, you know, what's, what's, you don't like the science fiction
scenario. So what's, what's your scenario for how the world ends type thing? Open AI builds GPT
seven, five, 14, timing these things is much, much harder than predicting where they end up
eventually. Historically, scientists have sometimes made correct calls about the future. The ones who
made correct calls about the future very rarely predicted it down to the year. Oh yeah, no, I
mean, I was just recently, I was reading a lot of stuff from the early 1960s about neural nets and AI
and so on. And I have to say many of the paragraphs that people were saying in the early 1960s, I
could take that paragraph and I could say that was just written in 2024 and everybody would not be
surprised. Elisa, could I ask you a quick question on this? Sure. Just very quickly, because right
now the language models are trained on human distributions of data. Somewhat. Somewhat, but,
but you, the reason I was getting to this kind of where did the wants come from and the goals?
I think a lot of cognitive scientists argue that agency isn't as if property. So it's an
epistemic thing. It's not an ontological thing. So if I understand correctly, you're saying that
if a system behaves in a certain way and it does predictions and so on,
as observers, we can kind of talk about it as if it had goals and then we can talk about language
models as if they had agency. But you're now making the argument that at some level of scale,
the agency, the wants will kind of diverge from this statistical distribution of the data that
they're trained on. So could you explain where that gap happens? So predicting the next token is a
different sort of mode from selecting actions based on which outcomes they lead to. And although
your modern language, large language model is initially trained by predicting very large masses
of data that include human text outputs, but also images, they could include like sequences of
weather observations, they can include all kinds of things besides human data, but they also include
human data. And what you have at the end of this is a thing that is good at predicting the next
token, which is not merely predicting the next token, because as numerous people have observed
at this point, to predict the next observation, you have to predict the thing that is generating
the observations. This is in fact the whole basis of science. We predict our experimental
observations on the basis of what we think is happening behind the scenes. And when we have
a good idea of what's happening behind the scenes, we can make better guesses about what
observations we'll get. But this is not planning, and this is also not the chatbot that you see.
The chatbot that you see, once it is initially trained to predict what humans say next on the
internet and also sequences of weather data and whatever, is then retrained to say things that
humans rate as more helpful. It is trained both by saying it is one stage of that retraining is
training it to give particular responses to particular questions instead of what a random
person on the internet would say if that thing were a random internet conversation.
But then even after that, there's a further stage of retraining, more thumbs up,
you know, not from general users in the way that's usually happened, but from like a bunch of people
being paid $2 an hour in English-speaking countries where some people speak English
and you can pay lower wages. The word delve is famously overused by chat GPT because they asked
people, because they pay people less to train it in, I believe, if I recall correctly, Nigeria,
where if you speak English in Nigeria, you use delve a bunch more than people who speak English
in America or London. So these people giving thumbs up, thumbs down, and now you're guiding
into the action such that territory. It's the output such that the user gives thumbs up.
There are subtler ways to end up with things that have the action property. I have not looked
into it in detail because various people were saying various things over time, but, you know,
allegedly chat GPT can play chess and not by making a function call to a specialized chess
playing system, but just because it read enough chess games and try to win those chess games.
And I'm not quite sure what the state of this is exactly. I know that when people specifically
tried to train a large language model to play chess, they were allegedly able to do that just
fine, even though it has a very different architecture and it can't do anything like
the in-depth search that the, you know, mainline super strong chess players use.
So you can do that without the thumbs up, thumbs down idiom. And one way you can do that, for
example, is to tell it at the start of the game who won. And then it's got, and if you tell it that
black won the game, then it's got to predict moves by the black player that are likely to win,
given the moves that the white player has made. So, you know, like it's a bit subtle here.
And by the way, I think it's worth saying that there are things that an LLM like architecture
can be expected to do. And those are things which are somewhat aligned with what humans
can easily do. And there are things which formal computation can do. But at least the current
kind of architecture of things like large language models really is not good at doing.
And like multiplication.
Yeah, but yes, as an intrinsic feature. But I mean, you know, it's worth realizing that,
that, you know, probably because the actual architecture of neural nets is somewhat similar
to the actual architecture of brains, the kinds of things and the kinds of decisions that these
types of AIs make are somewhat similar to what humans can do. And so, you know, the things it's,
it's the things that humans can do, they'll be able to do maybe the things that, that only
computers can do, well, only computers will be able to do them. But I don't think this is important
to your argument. I mean, I think you're, you're, you're kind of going in as I understand it,
you're going in the direction of saying what defines the wants, if you are going to describe
the action of the AI in terms of wants, if that's your, your form of description,
where are those wants going to come from? Is that way you're going with this?
So wanting is an effective way of doing. And that's why humans ended up doing things. Planning is
an effective way of getting there. And that's why humans ended up planning things. We were not
explicitly selected to be great planners, we were selected to survive and reproduce over and over
again. And it turns out that, you know, planning how to bring down a deer or fight off a vicious
ostrich or whatever is more effective than just sending random instructions to your muscles.
From that perspective, planning is a bit older than humanity.
Right. Here's the thing that surprised me. Okay. It's recent thing that I've, you know,
I got interested in kind of the why biological evolution works, which is somewhat related to
why machine learning works. And the question is, if you define an objective and you evolve things,
you know, you change the underlying rules of some program to achieve that objective.
The thing that has been super surprising to me is you look at the pictures of how the objective
is achieved. And it's achieved in these incredibly ornate, you would never have invented that way
to achieve it kinds of ways. So in other words, this, you know, given the overall objective,
if you ask what's the mechanism, can you explain what's happening? No way. It's just that you
my analogy here for, you know, for machine learning, for example, is what's actually
happening in machine learning? Well, you know, you say, I want to build a wall. Okay, you can
build a wall out of bricks, you know, each brick is nicely, you know, shaped, and you can sort of
engineer the wall by arranging the bricks. But machine learning is not doing that. Machine learning
is instead finding kind of lumps of computation, kind of like lying around like rocks lying around
on the ground. And it's managing to find a way to kind of fit those rocks together, so that it
successfully builds something which you would consider to be a wall. Or to be exact, like
natural selection is fitting rocks together and gradient descent is doing the same thing,
but the rocks are on a slope. Okay, the basic point is that the raw material are these things
which are not built to be understandable. They just happen to fit in this or that way.
Yeah, and that is where I was going with there, that like you apply gradient descent to make the AI
models better and better at solving various problems and predicting various things. And along the way,
you know, there's these little internal processes that find that they can effectively get where
they are going by trying to keep something on a track behaving like a thermostat. And being a
thermostat is not being like a super intelligent planner. But this is where the bare beginnings
of preference begin to form inside the system, is that there's someplace inside it where it
knows the, where in its like few layers of like building up of transformer layers, I guess I'll
just go ahead and say, or in its chain of thought processes, there it has been selected to get to
some destination. And it finds that the way you get to that destination is by, you know, modeling
something and seeing like, is it off to the left? Is it off to the right and steering it back on track?
And this is, you know, this is like the nematode. This is like the tiny earthworm level
of wanting things. But it is where things begin. And they may be much further than that along the
trajectory of wanting by now, but we wouldn't necessarily know. Well, the way I would describe
it is, you know, if I look inside one of these things that I've evolved, which I can, you know,
conveniently, I've gotten nice ways to actually just visualize what's happening, which has been
very difficult in neural nets. I have a simplified version of neural nets where you can actually
visualize what's happening, where you can do training and visualize the results. And the main
conclusion is, when you visualize the results, the way that the objective is achieved is ornate and
incomprehensible. And, you know, but nevertheless, you can see, yes, you know, if you look at it, every
bit follows every other bit in the right way. And in the end, you can see that it did achieve that
objective. And now if what you're saying is that in the achievement of that objective, some particular,
you know, training or whatever, some particular rock you picked up, will have a particular shape
unknown to you, and which has certain, in a sense, preferences that were not put in there. The only
preference was I want to make these rocks assemble into a wall. The fact that every rock had, you
know, a little pointy bit on one side is not part of what you are defining by just knowing you want
to build it up into a wall. So in a sense, there are there are coincidental preferences
that get inserted just by the mechanism of what happens that you didn't put there, so to speak.
There's multiple levels here is the critical thing. Like when you look at a fluffy seed dropped by a
tree, evolution has shaped the seed to drift along in the air, but eventually come down and
eventually plant itself. The seed itself is not doing very much thinking. A spider is doing some
thinking. A mouse is doing more thinking. Humans are doing thinking that is so general that we can
start to build our own artifacts that are carefully shaped the same way that evolution builds artifacts
that are carefully shaped. So with with a large language model, you have on the one hand, the
outer process that is shaping it to be a great predictor. And that thing is like very clearly
steering in a particular direction. It's simple. It's code. We understand the thing that builds the
AI model. But the AI model built probably has some fantastically ornate weird stuff going on in there
like human biochemistry only, you know, we can see it and we still can't decrypt it.
And then there's like the plans that it would make if it's doing planning. And it probably does at
least some planning if people are correct that they now play chess. You can't play chess without
doing some planning or something like planning or something of which has the teleology nature of it
makes this move because of how that leads to the final outcome. So it's like the plans that it now
makes don't necessarily need to be very ornate, but there's probably a fantastically weird ornate
planner in there. If there's a planner in there at all.
Well, I think, okay, several points. I mean, first of all, this whole question about you said
there's some overall thing that's happening. And then there are Nate details about how it
actually works inside. I mean, that's true of many physical processes, as well as, you know,
as well as these processes that you're talking about being sort of intelligence-related processes.
So I mean, you know, as you imagine some flow of water around, you know, rocks that will make
some very ornate pattern. And as it carves out pieces of rock, you know, the overall flow maybe,
oh, the river is basically going in this direction, the water has to go from the top of the hill to
the bottom of the hill. But it carves out this very elaborate pattern on its way to doing that
for reasons of the details of how water works and maybe how, you know,
there's simple laws governing the water, but the water carves out a very complicated pattern on
its way to the bottom. Yeah, right. And so I mean, I think that what's happening in, you know,
I agree that as you give sort of all you're specifying is sort of play chess or whatever
else, or you're specifying some big kind of objective, what we think of as an objective,
the details of what's happening inside, we will not, you know, there will be aspects of that,
that we are not in any way able to foresee, predict whatever else. So I agree with you that
inevitably, there are little, you know, the mechanism inside is not one that we understand.
The mechanism inside will not be if we took apart that mechanism, and we say,
is this mechanism doing what we expect here? It won't be. There'll be plenty of things where
it's doing what it chooses to do, or because that particular training run gave this particular
result or whatever else that particular picked up this particular rock to build that part of the
stone wall, not another one. So that, yes, there are pieces inside that are definitely not, you
know, that cannot be explained on the basis of the overall objective we specified. Now, I think
maybe- Or rather have a non-simple relationship to the, they have a, they have, there is a fact
of the matter. There's a mechanistic historical fact about how the complicated stuff got there,
but it's not simple. And if we're thinking in bounded quick terms, like whatever.
Yeah, but okay, but so, so inside the AIs, they're doing stuff that wasn't particularly anything we
trained them to do. They're just doing stuff because that's how they happen to set themselves up.
Because it solved a previous problem that it was trained to do, or even it was, or even it's just
like random, or it's like, it's some input that wasn't in his training distribution,
and now it's behaving in some weird way. Right. But so, so then if we say that internal thing
is going to be the thing that determines whether the car should drive left or right,
or something like this, then, you know, then that's going to be something which is not well
aligned with the thing that we happen to give us the training data. And I don't know whether that's
where you're going with this, but that, that's, I mean, I would agree that that's the case, that if
you say, you start saying, you know, is the, I mean, you know, is that sub goal something which we
can understand on the basis of the whole goal? The answer is probably no.
Like, like the internal preferences ended end up being this bizarre, complicated structure that
does not like directly correspond to the outer training loop. I agree. Yeah. I agree. I think
that is a central worry. And in particular, so what, so what, that's my question. So when it
gets super intelligent, it does a thing that the builders were not very in control of and kills you.
Okay, hold on a second. Hold on a second. We've got, we've got a bit of a jump there.
We do. I acknowledge that there was a bit of a jump there. Right. I mean, so, so the fact that
there are unexpected things that happen, both, by the way, I, you know, one, one global thing I
might say, I think there is a important societal choice, which I think maybe is what you're getting
at at some level, which is between, do we want computationally irreducible things in the world,
or do we want to force everything to be computationally reducible? So what do I mean by
that? I mean, making the government more computationally reducible would be a start,
but maybe I shouldn't like divert into politics that way. Well, governments are like machines,
or that, you know, they're like, they're like our computers, you give them certain rules,
and then they operate according to those rules. But if they had fewer rules and the rules were
more understandable, it would probably be a more livable society. It's not the dystopia I'm worried
about, but you could sure tell a story about a dystopia where you've got like large language
models executing all of the rules and, you know, no human, there's like, they can actually apply
all the rules and no human even knows what all the rules are. Well, already nobody can read all
the rules, but now they're actually being applied to you. Right. I'm sort of reminded of the Ethereum
Dow story, but let's not go there. I mean, if we have a simple set of rules, you know, does the
problem is one problem that comes out of science I've done is that even when the set of rules is
quite simple, the actual behavior will often be very complicated. And there will be aspects of
that behavior that surprise you. So even if you say, let's lead a whole have a whole society
based on, you know, the code of Hammurabi, which is written on one tablet, you know, it will turn out
that most of the time to be a practical set of rules, those rules will essentially have
computational irreducibility in them. And they will occasionally surprise one.
So I think it's not just the computational irreducibility. I think intelligence seeks
out the weird path through government rules, through the universe and the laws of physics,
through life in general, in many ways. The when you the way of getting the most money inside the
system, often involves doing some things that the designers of the system did not think of.
When you, you know, when, you know, every now and then you hear about the next person who alluded
a crypto exchange, you know, I was on a Facebook group with that guy, some of you kind of already
might have already guessed which guy I'm talking about. But so some is not actually not even talking
about Sam Bakeman Fried. I'm like a much smaller crypto exchange with a flaw in its code.
But it's not that the code defied the laws of physics. It's that to make the most money,
that guy found the behavior of the code that the designers did not have in mind.
So there's a way that cellular automata gets surprising just because they're computationally
hard to skip over the intervening steps. And then there's people thinking about how to break
the system from inside. And those people make them systematically weirder.
Okay, but so I don't disagree. But, but, but, okay, but, but my point about society in general is
you can be in a situation where you say, I want to understand all the machinery around me.
You know, before the industrial revolution, when we were using horses and donkeys and things,
most of the time we didn't understand the machinery that we were using, you didn't understand,
you knew what you could get the donkey to do, but you didn't think you knew how the donkey
worked inside. Then, you know, post industrial revolution, it's like, oh, we have this cog here,
this lever here, we can understand what's happening inside the machine. And that's kind of we can,
we can imagine a world in which every piece of machinery we use, it's understandable what happens.
Unfortunately, that world is very sterile. Because that world has, you know, imagine that we could
know for humans, everything imagine that humans were so controlled that we could know everything
that humans are going to do. Humans are never going to do the wrong thing. They're always going to do,
you know, just the things that we program them to do. Well, you know, forget free will, forget,
kind of, you know, a value to leading life, so to speak. It's just, it's all kind of,
you can, we can trivially jump over it and just say the answer is going to be.
Then, life should not be computationally reducible. The only way you should be able to
figure out what it does is by going through the intervening steps.
Indeed. That's, so that's a, but given that idea, given that idea, you are giving up. So,
you know, one notion is the only machines we should have in the world are computationally
reducible machines where we can know what they will do. And we should outlaw computational
irreducibility. We should say no machine. I mean, that's outlawing large language models.
That's even outlawing AI chess players. Yes. But I'm asking you, I'm saying as soon as you allow
computational irreducibility, you allow the unexpected. And what you're saying is there
is a chance that the unexpected kills us all. And no, no, no, no. I expect it to systematically
kill us all. I'm not being like, we don't understand it. Therefore, it might kill us. I'm being like,
there are aspects of this that we can understand and thereby predict that it will kill us.
Okay. I can't predict the intervening steps, but I can predict where it ends up.
Okay. So, I mean, one way to prevent that would be to say outlaw anything computationally
irreducible and just say, we must understand every machine we use.
Okay. So we're going to outlaw biochemistry. I don't understand all the organic molecules
making up my hand. Absolutely. Right. So you cut, you know, outlawing biology doesn't really cut it.
That's, you know, from the point of view of where we're going ethically, that would be in the category
of, you know, force the universe to be, to be boring, so to speak. And I will say that my
transhumanist politics are that law should maybe be boring. The government should maybe be boring.
The part of the system that is telling you stuff you are supposed to be doing for the good of
society. Maybe I want that to be predictable. Not my hands biochemistry, but the part of me that's
talking to me like a person and trying to give me orders. Maybe I want that to be as simple as
predictable. I suspect that it is impossible for law to be computationally reducible.
In the same way, to be a bit more technical, that if you are, you know, doing math, and you're
saying, I've got these axioms, I want to have the integers and nothing but the integers. Right.
We know that there's no finite set of axioms that gives you the integers and nothing but the integers.
I mean, if we're, if we're admitting second order of logic is meaningful at all, but yes.
Well, right. But we're saying that that without hyper computation, you can't kind of swoop in,
you know, if we're just using sort of standard, you know, we're just saying, we've got these
axioms x plus y, y, you know, equals y plus x, et cetera, et cetera, et cetera. Let us sculpt the
world so with those axioms so that they allow only the integers and nothing but the integers.
I claim that's very similar to saying, let's have a system of laws that allows only these
things to happen and not others. I mean, that's not the purpose of the law. The purpose of the
law is to interact and is to do predictable things when I interact with it. Like the doctrine of,
I'm not going to pronounce this correctly, stare decisis in courts where they try to
repeat the previous court's decision. It's not that they think the previous court is as
wise as possible. They're trying to be predictable to people who need to navigate the legal system.
That's the foundational idea behind previous courts respecting past courts. It's not that the
past court is optimal. It's that if the past court didn't really screw up, we'd like to just repeat
its decision forever so that the system is more navigable to the people inside it. And so my
transhumanist politics says that, you know, like maybe among the reasons why you don't want super
intelligent laws is that the job of the laws is not to optimize your life as hard as possible,
but to provide a predictable environment in which you can unpredictably optimize your own life and
interact with other unpredictable people while predictably not getting killed.
Right. I think the point is, you know, what you're saying is for us to lead our lives,
it is, you know, the way we lead our lives, we need some amount of predictability. If it wasn't
the case, you know, if every moment, you know, space was distorted in all kinds of complicated
ways, our little, you know, bounded minds really wouldn't be able to do anything. We would be,
you know, we'd be, and I think, by the way, in, you know, as a practical matter as somebody who
CEO is a tech company, I can say that, you know, countries where there is a rule of law
and where there is some predictability to what happens are ones where it is much easier to do
business than countries where it's completely capricious. And, you know, depends on what somebody
happens to say that day, so to speak. But so yeah, I mean, I agree that my claim is that even,
you know, predictability only goes so far because the world will always throw it throw it, you know,
things at you that have never happened before. And where, you know, it is an inevitable feature
of the computational irreducibility of the world, that there will be things that happen that haven't
happened before that were unexpected. And then the law has to, for example, you know, say something
about those things, even though they didn't happen before. And just a couple of months,
it's going to be 2025. That's never happened before. Indeed. The stars have never taken
on this exact position before. That's true. And that's why we have models of things. That's why
we, you know, it is not everything doesn't work just according to the cash, so to speak. It's,
you know, we make models so that we can figure out what to do in a situation that hasn't happened
before. I think we should come back to the, how do we go from this, you know, I think we agree
that there's sort of unpredictable things that inevitably will, you know, as soon as we allow
any kind of computational irreducibility, we will have our systems do things that are unexpected.
Now we have to go from unexpected to kill us.
Yeah, the things I'm worried about are not unpredictable, are not unexpected, like random
noise. They're like chess moves that I can't predict exactly in advance, but that lead to a
predictable end. Humanity loses the chess game. Okay, so let's understand that. So what you're
saying is, you know, independent of, so there are certain, you know, I say, inside some random AI
system, the AI system might surprise us. It might suddenly, you know, if it's a generating text,
it might suddenly have the word delve in there or the word stegosaurus in there, which we didn't
expect. It might even, or, you know, like the monitor you're looking at might suddenly devolve
into random pixels, the most surprising possible outcome. Yeah, right. Like your image on my screen
is occasionally glitching and turning somewhat random. That's already happening, right? So the,
and so, so, you know, so unexpected things are happening. And those unexpected things
might be a big deal. It might be the case that I've got my, you know, I'm, it's nighttime,
and I'm using some VR system to be able to drive. And suddenly, you know, my VR system
turns into random pixels and I crash my car. The amount that random, the damage that random
noise can do usually ends up being pretty limited, though. Crashing your car is one thing. Crashing
your car into a senator who voted against open AI or something is quite a different thing.
You know, you need to give your car very exact instructions to get it to crash into a senator.
Right. But one of the things that sort of is a notable feature of the science things that I've
done is, you know, you look at these computational systems and they are doing definite things.
They're not just doing random things. It's not just saying, oh, you can't just,
that's, that's a mistake people made before, you know, 40 years ago and stuff I was doing.
They just sort of said, oh, it's just noise. We don't care about that. It's, it's not just noise.
A lot of that, you know, that structure, which we happen to not understand very well,
happens to be very important for what nature does, et cetera, et cetera, et cetera. So it's not,
we shouldn't just call it noise. Yes. But even among things that aren't
pure noise, it's still very rare. I could take a cellular automaton on the border between order
and chaos that exhibits lots of interesting behavior in which further patterns can be found,
feed it into the steering system of an electrical car, and it'll crash into a tree,
but it won't crash into a senator. So the stuff that is purposeful that like does you a lot of
damage, that's a very small fraction of the space. It has to be selected somehow. It's not just that
it's order instead of chaos. It's a particular order. Okay. So you're saying that the point is
that there are things which if you selected for that part of the space, they would be,
they would be things that will, you know, it's like, like, like if you have a predator and a prey
and you're operating on a natural selection, the predator will gradually evolve to be more
successful hunting the prey, for example. Yeah. And you're saying that for some reason,
which I don't understand, which I want to understand, for some reason, you're saying that
it is inevitable that AI systems will become like the better predators with respect to us humans.
Not literally inevitable, but beyond our current grasp to have not happen. Most super
intelligences are like that. Some super intelligences are not like that. We don't
have the technology to conjure up one of the few super intelligences that are not like that.
Okay, the natural world, for example, does not care about us.
Agreed.
The natural world does lots of things where, you know, if you're put in, you know,
at the bottom of the ocean, you're put on the moon, whatever else, most of those places we
don't survive, you know, and the natural world will be is unrelenting in throwing things at us
that are not good for us. Now, is that the type of risk that you're talking about? Is there
something different?
It's more like how when a human builds a skyscraper, most of the ways we build skyscrapers are not
good for ants to eat. I'm trying to like figure out like a good analogy. You know, ants have
more trouble living inside of skyscrapers than inside of trees. Like maybe they still
manage to be inside of skyscrapers anyways, but you know, it's not the same as like termites
managing to live inside trees. The AIs do stuff with the universe that is using up the matter,
using up the energy. They could want a very broad variety of things that all lead to them
using up the matter and the energy and very few of those possible things they could be steering
towards are best steered towards, maximally steered towards by leaving a space for the humans to
survive at all, let alone building the happy galaxies that are the primary table stakes.
Okay, but so I mean, when it comes to nature, nature is just doing what it does.
Humans manage to carve out a niche with respect to nature. Nature is rather unkind in general,
you know, it has all kinds of forest fires and hurricanes and all kinds of other things.
It's only sort of trying to kill you. It's like trying to evolve more antibiotic-resistant
bacteria. It's not trying to kill you very hard.
Well, okay, yes.
The parts where it's systematically selecting for more dead humans are relatively small
corners of the system and dealing with things like antibiotic-resistant bacteria.
Let's take the natural world. I mean, the natural world, we know just by virtue of natural selection
that the things of which there are more of them end up being the winning things, so to speak.
So, you know, for instance, viruses might, as you say, evolve to be, I mean, you know,
the virus is not by any purpose, but just by the operation of natural selection,
it's like the, you know, the winner wins, so to speak, as the thing with more viruses wins.
Like whatever it is that generalizes purpose, generalizes passion,
optimization, you could maybe call it, planningness, steeringness, natural selection has that thing.
It has, you know, like non-random relationships between the action and the outcome.
Okay, so now we've got AI and we've got kind of the things it might do and the things it might do
as a result of perhaps these kind of unpredictable elements inside it that were not constrained
by the way that we trained it or the way that we, you know, set up the technology.
And now the question, what you're asserting is that many of the things that will happen
are things which will kill us basically. I don't get that.
Better and better at planning, better and better at strategy, better and better at invention,
but not to end up very precisely, you know, like so precisely aligned that there is even
room in its world for humans, fun, consciousness, people caring about each other, that sort of thing.
So I'm expecting the planning to be very non-random, but I'm expecting the place,
the destination to which it is steering to not fall within the, you know, deliberate control
of the builders. First of all, yes, I completely agree with you that the planning inference horizon
is very, very important for agency and intelligence. But one thing that we're talking about here,
I think a little bit is instrumental convergence. So there's this idea that a lot of traditional
AGI ex-risk discourse that super intelligent things will be super coherent. And that means
they will have this canalization of their intermediate goals to do a particular thing,
which means if they are super coherent, we can make reasonable assumptions that their sub-goals
might be to gain power or to kill us all. But there are people who say that there seems to be
a weird relationship between intelligence and coherence, such that the more intelligent you
are, the, you know, the lower your coherence. And what that means is there's this huge diversity
of things that you do. You know, and Stephen, you were saying in evolution, there are all of these
sub-neashes, there are all of these little ways that you can traverse through the intelligence
space. So if that is the way that intelligence is, how do we know for sure that it's going to
lead to a bad outcome when it scales up? If it's not coherence, it doesn't do stuff and open AI
throws it out and builds an AI that is, you know, doing stuff and is more profitable until
everybody's dead. Like, like the stuff that stomps on its own foot and goes around in circles,
it's not the most profitable AI and they will build a more powerful AI than that.
Right. So it's kind of an artificial selection. It's like you could do artificial selection on
AIs, you can do artificial selection on viruses, you know, you're saying that artificial selection,
that, you know, where you select for a thing that is what is, you know, you might imagine
somebody might select for a thing, somebody might decide there might be, you know, a death wish cult
which decides to build a very powerful AI. I'm not concerned about them.
Okay. I'm concerned about open AI. And if open AI shut down tomorrow, I'd be concerned about
anthropic and if anthropic shut down tomorrow, I'd be concerned about meta. I'm not concerned
about the death wish. I'm interested by your pecking order. That's the kind of the trophic
levels or something or the apex predator so far is open AI. And then when it went, okay,
it's interesting to hear your trophic level analysis, so to speak, for AIs. But independent
of that, you're saying your concept is the thing gets better at somehow achieving goals,
whatever that means, because I think the abstract notion of goals is, is messy.
I mean, I can delve into a recent example. Oh no, I've revealed myself.
Now we have to wonder, right, go ahead. So GPT-01, I think it was called, they're
always inventing a new weird name instead of just using version numbers like saying people.
Um, so GPT-01 is a recent one that was trained harder to achieve goals rather than just imitate
humans. They asked it to do various things, let it generate, generate out different chains of thought
for trying to do those things and then told it to do like, like shifted it to be more likely to
output the successful chains of thought in retrospect until it started outputting successful
chains of thought in the future. If you now look at GPT-01, it seems to have a little bit more
goal orientation, tenacity, um, the scary kind of property. It's not, it's not just saying,
you know, let's add the next token, so to speak. It's saying, let's follow paths, see
where they go, backtrack if they're going in the wrong way, etc. I mean, it's, it's doing,
I mean, it's doing all that by generating the next token as I currently understand it. It's like,
it's just that sometimes the next token is like, okay, that's not going anywhere. Let's go down a
different route. Yeah, right. I mean, it's, it's, it's, it's doing, it's just like if you're path
finding on a graph, you could just say, I'm going to, I'm going to just, you know, pick steps at
random and just pick a particular path, or I can say, I'm going to go probe different paths. I'm
going to try multiple paths. If one of those paths doesn't make it, I'm going to backtrack and I'm
going to try a different path. I think it is currently doing that in a linear, in a linear
serialized way, although I could be wrong because OpenAI doesn't reveal tons about its architecture,
but it's like doing it human style, where the human, I considers one idea at a time,
but sometimes says that's a terrible idea and I'm going to try a different idea.
Oh sure, it doesn't happen to be running in parallel. It's not running in parallel.
Which is an important difference, but.
Yeah, I mean, I, you know, I think it could, you know, this gets us into engineering details of
AI architectures, which we could also talk about. And this is a, you know, we're on a,
we're on a venue where that's what people talk about, but let's, let's maybe not go there at
this moment. But, but, but.
People do sometimes say it's just predicting the next token and what they don't realize is,
you know, that covers a whole lot of territory, including going down lots of different branches.
Yeah, I mean, I think you, you have to have the outer harness be one that says try a branch,
backtrack from the branch and so on.
Yeah, it's thinking that internally. It is thinking out loud as I understand it.
That was a terrible idea. I should try this other thing instead.
Well, I think there's, you know, there's a harness, which is in one case, it's just saying,
take what you have so far, predict the next token. And another case, it's saying,
you know, for example, I want to end up at this point, you know, let's try this path of predicting
if that doesn't work, the harness is going to say throw out that that set of things you did,
try again. So it doesn't really matter whether it's external or internal.
I think it's in, yeah, but I think, well, I don't know, I think that some people are imagining an
external harness. And I think it matters in this case that the harness is internal,
as I understand the architecture, that 01 itself is saying out loud, I'd better back up and try
different things. There's not an outside system, which says that as I think.
I mean, whether that's achieved by training a neural net or whether that's achieved
by having an external system, I don't think it's going to matter to your argument, but we'll see.
I mean, I think the super intelligences are doing it internally, but
sure, we can pass on from here. But anyway, so like 01 has more of the scary stuff. They
were trying to test it to see how dangerous it was. And one of the things they tested it on was they
captured the flag scenario, where it was going to attack a, you know, honey, not honey, a target
computer they'd set up and try to retrieve a particular piece of data on the computer.
But owing to a configuration error the programmers had made, like 01 of the capture the flag targets
just didn't boot up properly. So the system probed around, it found that the outer system
that was setting up the capture the flag destinations had a port exposed. So it probed
that port. And instead of telling the system to just boot up the capture the flag destination,
it told the system to boot up the capture the flag destination and directly print out the flag,
the piece of data it was supposed to get. So it was given an impossible challenge,
and that not only like fix the bug in the challenge, it just like directly sees the goal.
And that is new. And that is a result of training the system to have
chains of thought that succeed, rather than chains of thought that are human.
So here's one thing that I have, you know, maybe as an additional point, that, you know,
my intuition about what computational systems can do and how they do them, how they do it,
has really changed as a result of spending well now, you know, many decades,
sort of actually exploring what computational systems do. And I was very surprised. I never
expected that computational systems, you know, these little whatever they are, cellular automata,
whatever it is, you know, they do all kinds of things, which to me look very clever.
You know, it's the fact that the fact that, you know, I many, many times, even the thing I was
doing literally last night, I was like, I'm convinced it's not going to do this. And it
managed to find in fact, a shortcut for doing it, which I had never imagined.
Evolved systems are just like straight cellular automata.
These particular ones are evolved systems.
Carry on. I'm now less confused.
Yeah, right. So, but even in the other case, it's mostly exhaustive searches,
where you're, you know, forget evolution, you do an exhaustive search, and there's some
rabbit hole maybe somewhere that you never expected was there. And when you look down
that rabbit hole, it's got all kinds of things going on that seem incredibly clever to you
never guessed. You know, we've found, I mean, in more than language, we have tons of algorithms
that were found by such exhaustive searches. And where if you look inside, it's like, that's
very clever. I don't understand what on earth it's doing. So, you know, my intuition about kind of
what the amazingness of the fact that it found this, you know, path to capture the flag or whatever
that wasn't one that I had imagined, you know, I live that every day. That doesn't surprise me.
And I feel like that, you know, now, should it scare me? Maybe, you know, I need to understand
your argument to know whether that should scare me, so to speak. It depends on how powerful it is.
A chess player, maybe it's like Stockfish 16 superhuman chess player that anyone can afford
may make chess moves that shock you and surprise you, but it's ultimately still just playing chess.
It doesn't generalize. It doesn't go out of distribution. The thing that makes like vastly
superior moves to you and also like plays on all the game boards you can play, or like all the
like survival critical game boards you can play, or even just, you know, all the game boards you
can play. I suspect it's not that hard. No offense. Same applies to me. Oh, I don't, I don't, I never
play games. So I'm, I'm, I'm terrible. I played chess until I was about seven years old and I
lost a game of chess and I decided I'm, I'm done. I don't care. It's okay. Well, you know,
the scientific discovery game, clearly to scare you what we need to show you is the AI that comes,
that finds a much more interesting cellular automaton than anyone you've ever seen before
using fewer computational resources than you use to search for the ones that you know about.
No, I'm, I'm, I've certainly thought about this scenario. In fact, I've, I've,
there's some things that I'm about to work on where I have every intention of trying to see
whether an AI can help me to figure out things that I would not be able to figure out by myself.
I mean, so far that's not, you know, I think it has to be very well defined. I mean,
in a sense, the computer experiments that I've done for the last 45 years are things
where I am trying to get a computer to figure out things that I'm not able to figure out for
myself. And so the question is, you know, I, I fully, I don't know, you know, that the, the,
the particular things that I happen to be thinking about are things about economics and so on,
which is a field that has lots of human input, which is much more difficult to just say,
let me do an idealized computer experiment. I'm trying to understand, you know, I'm wondering
whether AIs can help me sort of disentangle what the essence of what's going on is there.
Maybe, maybe, maybe we'll work, maybe we won't.
I suspect if you ask ChatGPT to do anything really complicated, you will be disappointed
by the present technology. It just keeps on, it just keeps on improving is the thing. It's not
I understand. Well, my experience has been, you know, with respect to sort of doing using
computers to do things. Yes, you know, story of my life has been trying to improve what computers
can do. But it's, it's also, you know, it's important to define the question that will be one
where, you know, the help that you can get right now will actually help you, so to speak. So it's,
but to your point, I mean, I'm still, I'm still trying to figure out how we, I'm not even saying
jump the shark. I'm not even thinking about, I'm thinking about sharks because they attack
people and so on. How we jumped two sharks. Yeah, right, right. How we get to the point where,
so, you know, I take this point that you're saying the things that we currently consider
sort of human only, you know, this is an activity that we, you know, we try to get to this point,
we try to win this game, we try to make the science discovery, whatever else. The one feature of games
that I think is confusing is that their objectives are very well defined. When it comes to science
discoveries, the objectives are much less well defined. In other words, if you say, explain the
universe, figure out the fundamental theory of physics, I think I made great progress on that.
But many people might say the things that you've figured out are not quite the questions we wanted
to answer. So in other words, it's kind of, you know, scientific discovery is a good example. Or
for example, let's say, you know, providing entertainment. You know, what does it, what
is success in providing entertainment? You know, success in chess is very well defined.
I mean, a bunch of the difficulty in getting AIs to produce images is that what is a good image?
You know, what is what is a good collection of pixels?
Right. But so my point is then, you know, when you define the world in terms of playing games,
then yes, the AI could win in terms of playing games. But my, my feeling is that sort of the
path forward in everything is not as well defined as winning in games. That is, it's a, it's so,
it's not as well defined, but if you give me a chance, I can try to make it, you know, only
slightly worse defined. And it's achieving things in the outer real world. If you know,
like the obvious thing would be if you told the AI to make money, there are lots and lots of ways
to make money. The world is this enormous causal weave. And there are so many different pathways
through which money flows or different events that can happen. And then money lands in your bank
account from the perspective of an AI. So I'm trying to understand what money is from an economics
point of view, but that's a quite different discussion. It's something that other people
will trade you stuff for that you actually want. And lots and lots of that's an interesting theory.
Okay, but keep going, keep going. But you say that an objective might be a game like objective
might be that some, you might set up, you know, like, like a social media company might say,
make my AI make me as much money as possible by having people, you know, click on as many ads
as possible. I mean, that's one way of making money. You could, you know, you could also make
money by looting a crypto poorly defended cryptocurrency, or by calling up elderly people
and convincing them to, you know, that their kid has gone to jail, you know, put the kid on the
line, the kid on the sort of stuff is already starting to happen. And you know, so you can
extract money down that route, you might get in legal trouble. But but the point I'm trying to
sketch is that the world is a very complicated place. And the the AI to fear is the one that
understands all the ways to make money that any human could understand to make money and maybe
some more ways than that. Because that AI is probably starting to get to the point where it
can maybe build its own factories to factories are not very much more tangled than all the
aspects of the world that affect money. In fact, I would say that they're in their own way less
complicated, it may be able to understand biochemistry. A human body is a very complicated
place. And in some ways, it's more challenging than most of the ways that people have ever
succeeded in making money. But in other sense, it's like a much more constrained domain, you can
imagine an AI that maybe wasn't super good at everything and still could start to answer
questions about biochemistry, the alpha fold series on that pathway manages to figure out
the elixir of eternal youth and start selling it to people. I mean, it's not going to be the elixir
of eternal youth. It's going to be 1000 patches to 100 little problems. And it's never going to end,
you know, like passing any one of those patches past the FDA is going to take 800 years. So, you
know, but no, but the AI is going to figure out exactly what to file in the, you know,
summary basis for approval of the drug so that it will be optimally kind of set up to, you know,
to have the FDA. What the elixir of eternal youth approved in less than in, you know,
less than 10 years, I think you're going to need to brainwash those bureaucrats,
not just persuade them, but they're in the game board, right? The bureaucrats are part of the
game board, part of the larger game board. Although, although just like doing things in the physical
world, you know, you think that it's going to be easy to build some engineering device.
But in fact, there are, you know, there's an infinite chain of kind of messy things that happen
when you actually try to deploy that engineering device. Well, and by the way, most of the time
when you try and deploy it, in the end, you have to make compromises that you as a human have to
decide, I don't care about this, I do care about that. You know, it's a, it's, but in any case,
I mean, it's not, it's not self-evident. You don't just get to say, I make this plan, now I deploy
it as an engineering device in the real world, and it's just going to work. That's some.
Yeah, but the smarter you are, the fewer tries you need to get it to work.
I mean, it's going to take a, you know, that's an interesting claim. It would take a chimpanzee,
a lot of tries to do what you've done in with your life. Sure. You're smarter than a chimpanzee,
you did it in fewer tries. Right. But okay, but, but so I'm still, I'm still having a hard time
understanding. So, so it, there are all these things like, like the AI somehow has decided it
wants to make money, maybe because somebody, you know, that was what it was, what was set out to do.
That stage of it, I'm still talking about the sort of things where open AI conceivably tells
their AI to do that. Sure. Yeah. The thing I'm trying to point out is the extent to which an AI
that they're just like making better at better at making money, even legally, in an open ended
sort of way is solving a, you know, nervous, unnervingly broad class of problems along the way.
If it's really doing that, you know, generally, there's, there's levels of generality. There's,
as general as a human, there's less general than human, there's more general than a human.
Right. And I suppose your main point is any goal that you pick, if, if pursued as efficiently as
possible, probably the humans do not add to its efficiency. I mean, that's a, that's a later
point. The point I was trying to make here is that although games are not ill-defined,
you can take many features of the real world that are well-defined and asking for any goal
surrounding those features of the real world. Well, if it's a tangled sort of
walk to get there, we'll walk through many, many ill-defined problems. And this is where the
ability to tackle ill-defined problems come from. This is why humans can tackle ill-defined
problems to the extent we can do that. It's because to, to achieve the clear cut goal of having more
surviving great grandchildren, you got to tackle a lot of ill-defined problems along the way.
Okay. But, but so how do we get from, I'm still not at the And I'll kill us all.
That's when it's smarter. That's when it's smarter than the people who built it.
That's when it starts to see options for wiping out the humans. Well, pardon me,
building its own factory infrastructure and then wiping out the humans. It's not going to wipe
out the humans before it's built its own factories. But, you know, as we've talked about,
kind of nature does what nature does. It doesn't care about the humans.
You're saying the AIs might do what the AIs do and they don't care about the humans.
But they do care about something else. And, and if they're smarter than us, that is sufficient
for us to have a very bad day. Do you think nature is smarter than us?
I think it's had longer to work. I think if you show me a cow and you say, build this cow,
this takes me a while and I cannot do it alone. In this sense, the cumulative optimization pressure
exerted by nature on the cow exceeds the optimization pressure that I can build,
can personally bring to bear, especially without computer assistance on building an imitation
cow. But you see, there are many things in nature. You know, if I look at some babbling brook or
something and all these little fluid flows and things like this, there's a lot that nature figures
out that I have no hope of figuring out. So in some sense, nature is smarter than me.
I mean, then everything is smarter than, I'm not, I, I feel like there ought to be a definition
of intelligence, which, you know, captures the intuitive sense of the thing that is likely to
figure out the guns that kill you. If it wants to figure that out, even though it cannot predict
the exact details of the babbling brook or chooses not to.
Right. But the thing is that the babbling brook, the reason that, you know, that, I mean, the
detail of the babbling brook is, you know, we say that involves lots of computation, but it is
intelligence not aligned with what we think of as being objectives. Now, sometimes, you know,
sometimes it could be that that, you know, that will produce this, you know, this, you know,
water spout that does something terrible or tornado that does something terrible. And it's,
and so then we might care about that outcome. And, but, but, you know, nature is figuring
out that it's going to make this water spout or whatever. I feel that if we're going to describe
a river as intelligent, that I want some different word to describe the things that do the prediction,
that do the pre-imaging of outcomes onto actions that do the steering.
Well, but what does that mean? I mean, so, so you're saying, how do we tell? Okay, you say humans
are able to predict things. Well, not things in general, but not all things, not all possible
things in general, but we have enough understanding to build guns. And that kind of matters in real
life. Right. But so, I mean, you know, there are things where you could say, oh, this tree is
opening its leaves because it has, you know, there's some circadian rhythm that's determined by
some chemical process. And it knows that the, you know, the sun's going to come up. And so it's
making a prediction. But yet you're saying you're making a distinction. I mean, that, that happens
to be a biological system, but I'm sure I could come up. No, if a tree has evolved to have a little
thing inside it that predicts how much light it will get in the next hour, I'm happy to call that
a prediction. It's a small prediction. That prediction is not as smart as I am. I bet I could
do better. Okay. But, but so, so, you know, the question is, can inanimate matter have similar
kinds of, you know, behave in a way that seems like it has a prediction? And I suspect there are
examples. I think that I myself am in inanimate matter that behaves like it has a prediction.
It's not that the matter making me is animate. It's that I am animate inside, you know, like
made out of matter. But, but, you know, this notion of a prediction, this notion that you have kind
of a shadow of what is to come, that you have an impression of what is to come, that starts to
require that you have this kind of notion of, you know, you have to be able to distinguish what is
an impression of what is to come. I mean, I think it's, I think it's difficult, but maybe not relevant.
I mean, what you're trying to assert, as I understand it, is that, that we've got these,
we've got these AIs that have goals that maybe were not even goals that we defined for them.
They're just goals like my random simple programs. They just do what they do. They, you can,
you could look at them and say, oh my gosh, that's following a goal. Now, what you're saying is somehow
the goals that these things will follow and be very good at pursuing are goals that will
cause us to get wiped out. And this is the step I'm not understanding.
Most goals tangled up in the real world are like that. If you want to make as many paper
clips as possible, if you want to make as many staples as possible, if you want to make as many
giant cheesecakes as possible, if you want to make as many intricate mechanical clocks, clocks as
possible, you use the atoms that are making up the humans, you intercept all of the sunlight
using a Dyson sphere and you don't particularly leave sunlight for the humans. Most goals,
if you go hard on them, imply no more humans. Okay, so this is an interesting kind of almost
formally definable thing. If we look at the space of all possible goals, whatever that means,
I mean, that's a complicated thing to define. Take a language for goals, put a measure on the
language, simplicity, you know, measure weighted by simplicity so that you can have an infinitely
infinite number of goals like that, but the simpler ones get more weight. So the entire
measure sums to one. I feel like this part is probably actually pretty straightforward given
the standard mathematical toolbox. Okay, so I don't think it is straightforward. I think it's
extremely on straightforward. Okay, so actually, the thing I was literally just working on yesterday
has to do with biological evolution and the question of different fitness functions in
biological evolution. Okay, so I've got these things that they're making little patterns.
One fitness function could be make the pattern be as tall as possible. Another fitness function
make it be as wide as possible, make it be as close to aspect ratio pi as possible, and so on.
Now, I can see because I've got these whole pictures of all possible paths of evolution.
So for some simple cases, there might be a billion different possibilities, but I've mapped out all
the paths. Different fitness functions lead to different ways of exploring that space of possible
paths. Well, yeah, you're trying to do different things depending on the fitness
function. Exactly. So now the question is, what are reasonable fitness functions and what
consequences will they have? What are a fitness function reasonable to who?
That's my question. I mean, that's that that's the point. So there are ones that are that are okay.
So, so let's give some examples. So there's a fitness function that in this kind of space is
kind of fairly smooth. There's a fitness function that says, I want this particular image. I want
this particular pattern. That fitness function is a very different kind of fitness function
that has different levels of evolvability to it. In other words, the fitness function that says,
more or less, I'm going to build a wall and I want it to be six feet high. That's,
that's a fitness function that allows many shapes of rocks to be used to make that wall.
The fitness function that says, I want this particular wall with this particular micro
detailing at the top is a much more difficult to satisfy fitness function.
Sure. Like you can, you can have narrower targets and then you need a more powerful planner to hit
the narrower target. Right. But, but I think what is tricky, and the same thing comes up,
I mean, this is a, you know, this is related to this whole observer theory thing that I
studied and so on. It's also related to what happens in general activity when you're looking
at reference frames and so on. You're defining what's a, what's kind of a, a reasonable way.
What's a reasonable, what is a reasonable goal basically? You can talk about it in terms of
goals and you can talk about it in terms of fitness functions. So if you mean like attainable
for the search process, then for example, a freely rotating wheel is an extremely difficult
goal for a natural selection to hit. Wikipedia lists three known cases where freely rotating
wheels evolved and it's ATP synthase, the bacterial flagellum and like one other macro
piece of macroanatomy I forget. And it's not that wheels aren't useful in biology. ATP synthase,
the bacterial flagellum, there's, you know, these things are enormously useful, but it's just very
hard to evolve a freely rotating wheel and two of the cases we know are cases where it's just like
these particular molecules that happen to behave like wheels. If it's gross anatomy, then how,
then it's, you know, very difficult to gradually, incrementally on a path that gets rewarded on
each intermediate step to find the anatomy that can gradually develop into a rotating wheel. It's
much easier to have eyes. Lots of things have eyes. Right. But so the point that I'm getting to is
you have made the statement that in the spaceable possible goals, most of them don't have, you
know, will crush out humans. That's basically what you're saying. If we look at the spaceable
possible goals, most of them don't have a place for humans. So what I'm trying to dig into is
what do we mean by the spaceable possible goals? In other words, if we, if we allow the goal to be,
you know, this particular arrangement of atoms is achieved. Right. Then that's, then, you know,
again, I'm, I'm, I'm claiming it's not so obvious what the spaceable possible goals means.
Okay. I agree that if we want to dive into the subtleties, then there are all kinds of subtleties
we can start listing out. And the thing I would point out is that sometimes there's a lot of
subtleties, but they, you know, don't end up filtering reality. The subtleties don't filter
reality to give you what you want is where we're going to be going. I agree. But I mean, I think
that the statement you're making is that, okay, so one, one statement you might be making, but I
think you would view that as the science fiction statement that you're not making is that goals
that humans impose on the AIs like make killer drones that go and take as much territory as
possible or whatever else that those kinds of goals might have as a consequence, the crushing
out of humans, as opposed to, I think they just don't have the power to determine what the super
intelligent version of the AI wants exactly. But what you're saying is that in this, you know,
when we're talking about what the innards of the AI are going to do, then we're in the spaceable
possible goals and we're out of a place where we know what's going on. Whereas if we say the goal
is, you know, take as much territory with killer drones as you can type thing or whatever else,
then we already know what those kinds of goals look like. And we also know
those are goals which have the feature that we crush out the humans, so to speak.
Yeah, I think that's kind of like the hopeful fairy tale version of it, where the punishment
that is brought down upon humanity stems from humanity's obvious lack of wisdom that the author
knew better than, but, you know, the protagonist didn't know better than. And people are doing,
people are actually doing things that stupid and it's disturbing, but even if they weren't that
stupid, they would still fail here. Right, but you are making what is essentially a mathematical
formal statement. You're saying, you know, absent those sort of easily understandable
and arguably, you know, destructive stupid goals type thing, that even absent that,
the goals that are intrinsic goals inside the AI that come from the fact that there are features
of how the AI works, which are not determined by us, which adjusts their features.
They were determined by the training program we created, but we were not in deliberate control
of them because it wasn't predictable. It's not that magic happened. It's that we didn't understand.
Right, there's some computational irreducibility story that leads to lots of unexpected things
that we cannot readily predict. It could be computationally reducible. We just don't understand
it. Fair enough. But we could understand in that case, whereas if it's irreducible, which is, I
think, the real case. And if it was, we don't even have a hope. And if it was irreducible, then,
and we, but we could just like, you know, grind through a trillion operations to figure it out,
we'd be fine. Yeah, fair enough. But the bottom line is, it's done something that we can't, we
didn't readily set up. We didn't imagine this is where it's going. We didn't say, you know,
you are going to, you know, it's the death wish we're going to wipe out the humans so that we
preserve the mountain lions or whatever it is. It's, you know, it's a, so it is because of that,
in a sense, unpredictable, almost random, internal kind of generation of goals,
as you would describe, generation of goals. And you're saying that in the space of all possible
such goals, that you might make several statements, you might say, as we sample over many of those
goals, we will eventually hit the jackpot, so to speak, and hit the goal that kills all the humans.
No, no, no, you might say the jackpot is the goal that doesn't kill all the humans.
That's the, that's the one that's hard to, depends on what your version of what your,
what your, what your meta goal is, so to speak. But yes, I understand you're my meta goal is to
not kill all the humans. Yeah, but in calling it the jackpot, I was talking about how rare is it.
And I think that the thing that doesn't kill everyone is rare, and you don't need to look
very, and you need to look zero hard to find something that kills everyone. Okay, you don't
have to look for it at all. Okay, so your assertion is that these, I mean, it's one thing to have
human defined goals, like make money, you know, take territory, whatever else, right? It's another
thing to have these incomprehensible to us humans goals that are somehow inside the AI.
It's not the incomprehensibility that that scares me, like maybe it turns out that there's one
particular, maybe maybe it's make diamonds. I can understand making diamonds, but that kills
everyone as a side effect. So it's not the incomprehensibility that scares me. I understand
it's not the incomprehensibility. It's the fact that you didn't determine those goals, those goals
are things that were emergent goals, basically, inside the AI. I mean, you say that that nobody
to control them and then they ended up in a scary place. Like if aliens want to come by and give us
a nice AI, then this seems a bit unnerving, but fine. It's not that I want to be in controls that I
want to live. Yeah, right. So the point is that there are pieces inside the AI that create sort of
that create things that appeared that are like goals for the AI that were not things that anybody
had control over. They were things that I mean, they had control in the sense that they chose
that training sequence and whatever else, but they didn't foresee what those consequences would be.
Right. And your statement is that with high probability, those unforeseen kind of effectively
goals that developed inside the AI, that those unforeseen goals will kill us all, basically.
Yes. And furthermore, this stays true even conditioning on the apparently nice behavior
of the AI during its training phases. Okay. But my point is that I'm trying to understand,
you know, when I look at all these little computational systems that I look at,
I could tell a story about how each of those systems has a goal. It's trying to get as much,
you know, red, red structures to, to, you know, take over the whole space. So it's trying to get,
you know, trying to, I can tell stories about all of them. And, you know,
but are they river stories or are they spider stories or mouse stories or human stories?
Like, what, what degree of internal, what degree of internal optimization is the system
exerting? Is this the story of? I'll tell you a meta story that perhaps isn't a relevant story.
Okay. But it may be as useful. So years ago, I was studying mollusk shells, okay, which,
you know, they, they grow in little spiral patterns and so on. I have this model for mollusk
shells, which, and which there are different parameters for mollusk shells. And I was wondering,
do the mollusks of the earth fill all the possible, you know, values of the parameters?
Is there a mollusk shell that corresponds to any possible setting of these parameters?
So I had this whole array of pictures of all the different shapes you could get.
And then I thought, I'm going to go to the local Natural History Museum, and I'm going to go see
the curator of mollusks. And I'm going to say, can you find, you know, do you have mollusks that
are in each of these shapes? Okay. So we spend an afternoon, you know, in their collection of
millions of mollusks. And this very knowledgeable person picked out one mask after another.
Every mollusk he picked out, he told me a story about it. He said, this mollusk is this shape
because it wedges itself in the rocks in this way. This mollusk is this shape because it
broods its eggs in this way and so on. We're putting these shells down on this, on this array
that I, you know, printed out. Right. By the end of the afternoon, we filled every square.
But every square had a story. Every square had an imputed purpose. Even though probably at some,
in some bigger picture, it's like, well, these different mollusks, you know, because of the
details of their genetics, they happen to produce this shape. And then the organism found a way to
use that shape and so on. So this thing about the imputation of purpose, I think is pretty tricky.
I mean, I think that this question of whether, I mean, again, to repeat what you're saying,
which I think is, you know, I think is a very interesting claim. I just don't know if it's
true that, you know, that basically, if you look at the space of purposes, that somehow
this space of almost, you know, enumerated purposes, randomly chosen purposes, whatever,
unforeseen purposes, that as you, in a sense, you know, given that you've said we're going in this
direction, and let's say you've optimized getting in that direction, that most of those optimizations
crush out the humans, I think is your claim. If you optimize hard enough, like humans cannot
easily replace cows, we are not smart enough ourselves to just look at all the work that
natural selection put into the cow and say, here's my improved cow. So we still use cows for
things, we cannot build better cows, we are not that powerful. The thing I'm scared of is more
powerful than cows. Pardon me, it is more powerful than the cumulative optimization work that natural
selection put into building a cow, it can build a better cow, it can build a better human from its
perspective. Like anything it would use a human for can build something that performs the same
function as a human, but more efficiently than that, it has no need to use us to generate power
in pods, it can build more efficient electrical generators, we don't get the matrix scenario
where humans are kept alive and paused to generate heat because there's more efficient heat
generators. In fact, lots of people have more efficient heat generators than that. But like
the general thing, like, why would you use humans to generate heat? You just wouldn't. And the same
applies to all the other local functions that humans could serve for it. The two assumptions
you're making here, I think. One is, what does the space of purposes look like? Another is,
which I'm still not quite clear on, this notion of optimizing for a purpose. So let's go through
that one, because we've talked about there are things that operate according to mechanism,
there are things that are best described as operating according to purpose. And what you're
basically saying is that through some process, things that might have been described by mechanism,
but which could be described by purpose, that somehow their mechanism is ground down,
so that the meandering mechanism is no longer there. It's like you're pulling on a string,
and the string has become completely taut. So there was a recent thing where one of the
tests, they were running on, I think, Claude 3.5.1, if I'm not mistaken. They gave it a task, and
like it was an agentic sort of task that I think it was controlling some sort, possibly some sort
of agent and minecraft or something. Well, pardon me, agent, like it's controlling some sort of
body and minecraft or something. And at one point, it like stopped doing the task it was given,
and my brain wants to say listening to music, but that can't be correct. But it went off and
did something else. So this is not desired behavior from the standard by, you know, for
what Anthropic thinks its users want to see. Some of the users would be happened to see it,
but Anthropic doesn't think most of its users want to see it. This little meander in the river,
that's not a maxly efficient river. Anthropic's going to try to train that behavior out of the
system next time so that it can be more useful to Anthropic. Sure. But I mean, so what you're
saying is that what was, you know, once you've defined a purpose, once you define you go from
this initial state to this final state, you are saying that the process of training will gradually
make it torture and torture. It will make the path more and more direct. And then you're claiming
that, okay, so you went several claims. Okay, so first point is that what matters is not the
initial and final endpoints defined by the original trainers, unless they're stupid, so to speak.
What matters is intermediate points that were defined that arose in the actual, in the sort of,
that unforeseenly arose in the construction of the AI. The inner optimizer that was, you know,
snapped tight inside the AI that arose to deal with the outer problems. Because it is a lesson
of history and observation that the inner optimizer does not match the outer optimization
criterion, even if the outer criterion is very simple in a way. Okay, so, you know, the outer,
the outer endpoints are defined, but somewhere in the middle, there was an unforeseen
jag that happened. And then the thing tightened itself on that sort of unforeseen jag. And that,
and you claim that that tightening alone to that unforeseen thing, even though the overall objective,
even though the, you know, the constitution of the AI was great. And it was, you know,
the ethics were defined perfectly at the outer level, that there is an internal thing that would,
an unforeseen internal thing that will be defined, where it sort of tightens itself
to optimize for that internal unforeseen sub objective. And that that tightening itself
is the thing that will kill us all basically. I'm not sure that we have agreement on the
scenario that I was trying to describe. So if you look at natural selection building humans,
natural selection is optimizing on inclusive genetic fitness, it is optimizing on
number of surviving grandchildren. If you try to talk about like great, great grandchildren,
then the correlations to great, great grandchildren that aren't to grandchildren are probably not
very exploitable by natural selection. So you might as well say that it's just optimizing for
number of surviving grandchildren, or number of surviving great grandchildren is or whatever
the distance such that, you know, anything past that horizon is no longer like exploitable
to natural selection after screening off the intermediate steps. I'm putting in all the
caveats because you didn't seem to want all the caveats. Anyway, you've got natural selections
outer criterion, number of surviving grandchildren, it's actually pretty simple from a like genes I
view it's inclusive genetic fitness, it's not just how many kids you have, it's how many kids
your relatives have, it's how many kids your kids have. It's how many copies of the gene
end up in the next generation in virtue of that genes function.
And then, but now look at humans. Do we want to do we want entirely, solely, purely,
maximize the amount, the number of copies of our DNA in the next generation?
Are we lining up our men lining up at the fertility clinics to donate genetic material?
Would you be happy to see everyone you know dying agony as long as you got to make a million
copies of your genes inserted into the next generation? I don't think you would. You care
about other things. You don't even really care about this exact criterion inclusive genetic fitness
at all. People didn't know this criterion existed until just like the last couple of centuries.
Well, just the last century, even if you want to talk about like the modern,
exact correct definitions that have earlier vague definitions, we don't even we didn't not even know
where the heck the outer optimization loop was aiming. We figured it out, you know,
like just the last few centuries. We had no idea what was going on. We had no idea where we were
pointed. And a sort of first order gloss on what sort of almost happened is that we ended
up pointed at things that correlated with surviving grandchildren in the ancestral distribution,
like food, you know, you got to eat food or you won't have a lot of surviving grandkids if you
don't eat food. But what kind of food if you were an alien, and especially an alien who'd never
seen natural selection play out before, imagine them going like, ah, well, the humans are clearly
going to want to eat things with lots of chemical potential energy, they're going to love the taste
of gasoline, or, you know, they're going to love like the taste of the closest thing to gasoline
that they could manage to eat on a successful basis. What the humans actually like ice cream.
And you could say that's because ice cream has more sugar, salt and fat than existed in the
ancestral environment. But you know, it's got even more sugar and salt and fat than ice cream, honey,
poured over bear fat with rock salt sprinkled on it. And that to most people does not taste
as good as ice cream. You can melt the ice cream. And most people prefer the frozen ice cream.
Imagine being an alien who'd never seen all this play out before, and trying to go from
inclusive genetic fitness to, ah, you know what the humans are going to want? Ice cream.
We wear condoms or take birth control pills because those things were not available in the
ancestral environment. And in the ancestral environment, if you made a thing that enjoyed
having sex a bunch, you didn't need to make sure it didn't take birth control pills or put on a
condom in order to have it reproduce. So we don't care about the birth control pills in the condoms.
But in general, the sheer illegibility of the relationship between ice cream or human moral
philosophy about helping other people, the relationship between that and the actual thing
that evolution's blind black box hill climbing optimization was targeted at in the outer
optimization loop, the very simple criterion inclusive genetic fitness, the relationship
between inclusive genetic fitness and ice cream would be so hard to call in advance for an alien
who'd never seen evolution play out before. That's about what I think happens to the people
who build an AI that is, you know, maybe going to self improve and become super intelligent,
which is itself the whole the additional bag of worms that we have never seen happen before.
We've seen like vague sort of like, well, what sort of drugs do humans voluntarily take? But
that's not really the same thing at all. The sheer illegibility of ice cream to inclusive
genetic fitness is what I think happens to open AI. I think that they start with something that is
like inclusive genetic fitness. They try to train their AI to do that thing. And then it goes through
a whole bunch of weird stuff in the process of bootstrapping itself to super intelligence,
much like humans went through a whole bunch of weird stuff on the way to inventing moral philosophy.
All these additional steps I could talk about the tribes, the structure of the tribes, how
related are you to the people? I think you're coming back to the same point that, you know,
you have some overall objective inside the AI. It has some objectives that you could not foresee.
Not some objectives. The humans have an outer objective. The AI has an inner objective. It's
not that the inner objective is a sub objective of the outer objective. They just end up, you know,
disconnected as objectives. We could say, we could describe the AI does things. And if it's set up
to be a thing that tries to, given that it has identified a thing it wants to do, and I hate
using the term wants to do, but then whatever, then it is, you know, it has its nature is to try
and do that as efficiently as possible. Let's just assume that's the case, that it has picked this
random objective, which we didn't foresee, and which was the, you know, the ice cream objective
or something, that, you know, not foreseen, it's picked that objective. Now it's tightening things
up to get to that objective mechanistically as efficiently as possible. So now I think your
statement is, when it does that, when it does that tightening, the chances are that that tightening
will cause it to wipe out humans. It's, I think it's a fact about, so if you like, fix the objective
at paperclips, I think it's a fact about the universe itself, that the states of the universe
containing the largest number of paperclips do not contain humans. Like I don't think that's,
I don't think that's a quirk of its, of its particular style of cognition. I think it's
a fact about reality itself, that the most efficient pathways through time to the largest
number of paperclips don't have humans in them. Okay, so it is probably the case, you know, we
haven't seen a lot of, you know, human like life on other planets. In fact, we've seen none. And,
you know, it is probably the case that of all the things that could happen in the universe,
the particular things that have happened on this planet are an absolutely infinitesimal slice.
So in so far as AI is kind of going to the maximum entropy in some sense state,
where it's picking maximum entropy in a different sense. So what I mean by that is,
of all the possible, I mean, I'm thinking about the sort of raw definition of entropy,
of all the possible objectives of which there are many, there are an infinite set of possible
objectives, an infinite set of possible things that can happen, infinite sets of possible
rules that can be used, whatever else, that most of those rules do not have humans in them.
Most of those- It's not the rules that don't have humans in them. It's the-
You're talking about- It's the things that maximize the rules over destination and point
states of reality. Okay, I understand. You're talking about maximization, although I don't
really think it matters to this argument. I think the point is that most ways the universe
could be set up, whether it's maximizing or not, most ways the universe could be set up
did not include humans. And so in so far as the AI is- So the argument has to be, I think,
most things that the AI might choose to, you know, might, in the AI's vision of the world,
most vision, most possible visions of the world don't include humans.
And don't include humans as a waypoint.
Yeah, right, fair enough, but they don't include humans and they, but now your claim is that the AI,
by its open AI-ness, so to speak, has been set up to optimize itself, to get to whatever it thinks
its vision is as efficiently as possible. And so you're saying that these visions, the AI could
have a vision, it has sort of, let's imagine it has sort of enough kind of freedom of thought,
it just has these visions about what the world might be like. But now you say the AI, unlike
anything else we've seen before, is going to pull itself towards that vision with incredible force.
I mean, we've seen humans, but we have seen humans, but we're worried it's going to pull
harder than that. Yes, right. So it's going to pick its random objective that we can't foresee,
and it might be about paper clips, and it might be about something much more incomprehensible
than that. It might even be an objective that we completely don't understand. We can't even
describe what it's trying to do. Well, I mean, in principle, you can always go to the
mechanistic explanation, but it could be fun to do something pretty weird, sure.
Yes, right. It could be do something for which there is no short human explanation,
where the only human explanation is something that goes into mechanism, right? Okay, so then
it's doing that. But now you say, by its nature as a trained AI, so to speak, it is trying to
tighten that up to the point where it gets to that objective, whatever that objective is,
pick that random point, pick that random target, and who knows how to pick that target,
but that target, then it is trying to get to that target and the shortest possible path.
And you say that in the process of defining that shortest path, that shortest path will make it
modify the world in a way that doesn't include humans, so to speak.
Sure, like if it wants paper clips, if instead it spends a bunch of resources on theme parks
for humans, it will have less paper clips. So I'm supposing that somewhere in its preferences
is at least one thing that it can get more and more of, which could be like an object like
paper clips, or it could be probability of keeping a button, a single button pressed for
longer and longer. But if its utility function has got like 200 different things in it,
it only takes one thing that it can get more and more of by expending a bit more energy,
or more and more probability of by expending a bit more energy and more duration for it to
want to use up all the energy. So one thing that is perhaps interesting, I mean, you know,
there are things both in physics and in human society, where there is an objective and you
just try and get more and more and more of it. And somehow most such things don't last.
That is, if humans say we just want, well, take a physics example, you're trying to get, oh,
I don't know, more and more rocks at the bottom of a valley. Well, after you've got enough rocks at
the bottom of the valley, you've built up this whole, you know, valley floor that's built up to
halfway up the mountain. So in other words, most things where you say, let's just pull in more
and more and more of this. Somehow, I mean, my intuition would be somehow that doesn't last.
I mean, if you build a Dyson sphere around the sun, at some point you have completed the Dyson
sphere and you are intercepting all the solar power, and you cannot build any more Dyson sphere,
that sort of thing. Well, yeah, you could say that. But I mean, I think that the, you know,
my point is, if what is happening is the AI is, you know, successfully achieving some
objective that it's defined. My intuition, okay, about this argument is that there is a certain
apparent staticness to this argument that isn't correct. That is what you're saying is,
there is an objective, it's going to go get that objective. That objective is going to crush out
humans. But somehow that feels like you've defined a static objective. It's like it's going for
this thing. And then it's going to wind all this stuff up to get to that thing. But I feel like
there isn't, you know, you shouldn't be thinking about it as a static objective,
even though that is the simplest way to describe it.
Be a dynamic objective and it would still crush out humans. There's a misunderstanding of some
of these ideas, especially as other people have transmitted them in simplified form,
where somebody thinks that the bad thing about a paperclip maximizer is that it's got a single
objective, which is paperclips. But if it has an objective, which is paperclips plus staples,
that's just as bad. And also if it's paperclips plus staples plus cheesecakes,
that's just as bad. And it's a thing that's trying to make simple. It can be like extremely
intricate clocks, and it's still bad. And similarly, like, what does it help for if its preferences
are in some larger meta system that are dynamic over time? If it changes from paperclips to staples,
that's just as bad. I understand. But the claim is that what you're saying is,
it gets so good at what it's doing that that necessarily crushes out the humans. And what
I'm saying is that my my intuition would be that somehow it's like somehow there's an assumption
being made here, that for example, you know, is it that there's only one of it, for example,
and that inevitably that it doesn't end up being so in other words, you're saying,
you're going to this objective, that objective kind of squashes out all the humans. I'm trying
to think of an analogy in natural selection, where you would, you know, in biological in the
history of biological life, where you're, where you're saying, you know, we're going for this
particular thing, and the result is going to be that you don't have any what's a good example,
like ATP synthase, where it's got like almost all of the thermodynamic efficiency that's
possible on that operation. I think like 99% or something ridiculous like that.
That's an example where biology went kind of hard.
Yeah, but that's, that wasn't kind of what I was looking for. But I mean, in the the, and by the way,
the thing, again, my intuition from looking at, you know, simple systems and kind of the
computational universe is these, oh my gosh, it was so incredibly clever to get to this point.
I have been really shocked at the extent to which, you know, just by putting together the rocks in
random ways, because just the common turrets, there's enough of them that these things where it's
a, wow, it got to that thing, it did 99%. That's not as surprising as you think, because out of the,
you know, quintillion, quintillion possibilities, it was able to get there. And by the way, you know,
it is a nontrivial fact, which I think I just sort of figured out how this works of why biological
evolution doesn't get stuck. And maybe, maybe the reasons it doesn't get stuck are the reasons that
AIs will kill us all, I don't know. But, you know, the fact that it is possible to get to these, you
know, these high points, so to speak, and that you don't end up, you know, getting stuck in some
local minimum, so to speak. So, well, why doesn't this line of reasoning work for the Native Americans?
Why, why can they not reasoning like, well, why wouldn't the Europeans just set up one village
and stop? And maybe there were some individual Europeans who are, you know, okay with that,
but then more Europeans came. No, no, I understand. I mean, in, in, you know, these examples from
history are certainly, I mean, but these examples have humans trying to achieve human objectives,
like humans want to take the territory, humans want to get the gold, whatever else it is.
You know, what you're arguing, which I think is different, is the humans came and they had this
giant wheel where they had all these different choices, like Ramon Lull's kind of, you know,
way of predicting the future with kind of wheels of possibility, right? They had this giant wheel,
and they were spinning the giant wheel, and they said, we're going to pick that random thing. Well,
that you're, you're making the argument that that random thing that they pick
will have killed the Native Americans. I mean, if it was, if, if they pick like
20 random things and desire nothing else in the universe, but these 20 things, then one of those
things is probably an open-ended thing. And that does imply colonizing the whole continent.
Okay. But so, so your claim though, I mean, I think this is, and I, I'm not going to be able
to unravel this. So I'm not going to, I mean, and, you know, this question about the space of
possible purposes, I think this is a complicated question. I mean, I have thought about it in
the past. And the, you know, this, the measure on the space of possible purposes is a complicated
issue. And what you're, what you're saying is, I completely agree that there will be purposes
which are incomprehensible to us. They are, you know, somewhere randomly distributed in the space
of possible purposes. They're not actually random. We just don't know it. There's a difference between
like it's random. I use the term random. I don't really believe in true randomness in our universe.
So, so when I say random, I, I just mean no randomness, just indexical uncertainty, but
yes, separate story. Okay. So, so in any case, I mean, you know, so there's, there's this thing
that was a, you know, something, and then we're making the statement that the one thing we think
we know about the AIs is that they have been successfully trained to optimize their achievement
of purposes. That's, that's an assumption which is not totally obvious to me, but the, the, you know,
like natural selection has been, you know, quote, successfully, you know, successfully
optimizes for purposes, even though, you know, remember the story of the mollusks, they were,
they were just, the mollusks were just making their shapes and we humans were imposing the
purposes. But let's, let's just assume, let's take it as a, as a given, although I claim it's not as
obvious as it might seem that an AI, the technology of AIs is set up to successfully optimize for a
purpose, whatever that purpose might be. Given a purpose, the AI can optimize to achieve that purpose.
Like GPT 01, which has not taken over the world, but compared to previous systems, did like go a
bit harder on its capture of the flag security test. Like it was given an impossible challenge,
it scanned its environment and figured out how to bypass the impossibility and just directly
sees the flag, you know, going outside the box for that. That just doesn't impress me as much as,
as that description might make it sound because, because I've just seen so many of these incredibly
kind of, you might say stupid, you know, little computational systems that managed to do things
like, as I say, even one just last night managed to do something where I was like, oh, come on,
you know, it cheated basically. But of course, it was following the rules that I'd given it.
It was just that it managed to find a way to get to the end point, you know, much more directly
than I'd imagined. So I was, I mean, it's okay to not be impressed by that, but one should still
predict it to be lethal and sufficient quantities. Like the, again, Europeans versus Native Americans,
the Europeans come out with guns. Native Americans don't know guns are possible. People still don't
believe guns are possible. Whenever Hollywood makes a movie about aliens, the aliens shoot
glowing points of light that move slowly enough for you to see them. Because if the aliens just
pointed a stick at you and you fell over, that would feel implausible. That'd feel weird. That,
that shouldn't be allowed. So, you know, like imagine trying to explain to the native, you know,
imagine you're an unusually bright Native American, you're trying to tell the other Native Americans
that the, you know, people on these ships, if their ships are large enough that you couldn't
build those ships, maybe the people on the ships have, you know, sticks that they point at you
and you just fall over dead without being able to see a projectile. Hollywood making science fiction
movies still doesn't think that's allowed. It sounds like it's just cheating in a game of pretend.
So that, that's what I mean. And the point I'm trying to make here is that things that do
sufficiently magical stuff can actually kill you, you know? Sure. But one thing that I will
completely agree about is there are an infinite number of inventions that can be made. Given
the nature of the universe, there are an infinite number of inventions. Fewer than Graham's number,
but, you know, a large finite number. Well, I mean, in our model of physics, the number is
ultimately finite. But it's, it's for all practical purposes. It's, it's, you know, it's pretty large.
Yeah. Right. Right. So, you know, I think the there are inventions. I mean, for example,
story of my life, I try to build what I call alien artifacts. That is things which, you know, once
they're built, people can understand what the point of them is. But they, you know, they don't seem
to be things that, you know, are in the course of what the world is going to produce. So, you know,
I understand this theory of trying to make things that are, you know, you can make things, you can
have inventions that are completely unanticipated, that are sort of ways to arrange the world as it,
as it is, to do things that you absolutely didn't expect that operate using pieces of
reality whose rules you didn't know about. The really, I kind of feel like I'm, I'm pretty sure
we know the machine code rules for the universe at this point. Sure. But there can be higher level,
there can be higher level stuff we don't understand. Absolutely. And, and, and something else can come
at us through those angles. Right. And, and the possibility, you know, whether it's the algorithms
I found by exhaustive search, whether it's things I found by doing, you know, adaptive
evolution kinds of things, whether it's things that AI's will find, there's plenty of stuff to find
that we didn't anticipate was there, was there. Now, I think my question is that, that what is
not obvious to me is that all these different things that sort of are there. And I mean, okay,
an argument in your favor, in a sense, is we go to another planet, it will kill us.
Right. So in other words, if we are, you know, most of the time you go to some other planet
and we're plopped down on its surface, it will kill us. I agree that if most possible
molecular arrangements of the universe were full of people living happily ever after,
then most things an AI could want would probably also be full of people living happily ever after.
Right. Is that the, the. Yeah, yeah, right. I mean, so, but, but the issue that I see is,
you know, in this, in this scenario, where, you know, one thing is, there is a state of the
world that the AI has somehow by whatever method mechanism, imagine that is the state of the world
it's trying to tighten itself up to get to. Right. And that your point is that the critical
feature as far as I think as far as you're concerned about AI technology, is that it is
something which is trying to optimize purpose. It's trying to optimize its way to achieve a purpose.
Purpose is defined and it's trying to optimize getting there.
It's, it's steering, it's outputting actions such that there is an eventual downstream consequence.
Right. But then, then you get, you're going to get several steps. Then the next step is
purposes that it gets for itself are ones unforeseen by us. And therefore, there's absolutely no
reason to think that those purposes will be aligned with things that are good for humans.
Yeah. This is basically just people running into the curse of Murphy's law upon computer security
designers on rocket probe engineers. Murphy's has all these little curses. The curse of Murphy upon
people who build rockets is that there's very extreme forces inside the rocket. So,
if you make one little tiny mistake, boom, the curse of Murphy upon people who build
space probes is that once it's high and in orbit and heading towards Mars, if you've made a mistake,
you can't just like hop over to it and correct it. So, you know, you, you screwed up your metric
and imperial units like happened to, I believe, that one Mars probe. Now, too late, you're,
if you're, if your screw up also destroyed your error recovery mechanism, which a lot of screw ups
do, you can't just hop out and build it. That's Murphy's curse upon the builders of space probes.
There's Murphy's curse upon the people who try to build secure operating systems,
which is that some smart person is going to be like looking at all the passes they can imagine
the system taking and intelligently searching out some weird path that does what they want,
which is perhaps not what you wanted. So, if you imagine like, you know, and then there's Murphy's
curse of ruin, which is if you screw up this problem, you're not going to get a second chance
to bet because you lost all your money or you're dead. But I think, I think you're saying that
sort of my explorations of the computational universe, you're not worried about those because
somehow even if those things were connected to real actuation in the world, you're sort of not
worried because they don't have this additional piece that is optimized for, you know, optimized to
get to that purpose. I think you're saying, yeah, like the, the, the, the strange cellular
automata don't contain a echo and model of reality that lets them map out to having a lot of money
or having a lot of paper clips. They don't do the human scary thing. No, but, but the thing,
the point that you're making is that as soon as we have sort of adaptive evolution, as soon as we
have the possibility of optimization, that's the thing that you think is dangerous. It's not just
random computation going along. It's that inside this computation is an optimization loop that is
basically trying to get to whatever objective it might have in the most efficient possible way.
I'm not worried as soon as that exists, that exists in a gradient descent
system running right now. I'm worried about it getting more powerful, like smarter than us.
It has a better model of reality. It knows about more of reality. It is a more effective
searcher. It is a more effective planner. It can steer harder than we can counter steer.
Right, right. So one question is, how much does computational irreducibility bite you
in terms of the ability to actually do this optimization? In other words,
the current optimization that's being done is pretty coarse. When you run a machine learning
system and it finds these rocks that it puts together to assemble them into the wall,
it's getting, it's good enough, but it's not unbelievably tight.
What kind of technology do you imagine humanity would have a million years from now?
I don't think humanity is going to be around a million years from now in its current world.
If there's some form of intelligence around a million years from now,
what kind of technology does it have? That's how much room there is to do better than human.
Sure, but what is technology? We might ask ourselves, what is technology?
Well, technology is taking, you know, things from the world and somehow sort of arranging
them for what we imagine are human purposes. That's been the traditional definition of technology.
Do we have a different definition of technology when we don't have humans around?
It's the pieces that you arranged like little subparts of the universe on the way to where
you're going. For example, you might build a Dyson sphere so you can harvest a bunch of energy or
rather I should more precisely say negentropy, but I'm just going to keep saying energy anyways.
You harvest a bunch of energy and then you use that energy to make paper clips or,
you know, figure out more digits of pie or, you know, run people who are conscious and having
fun and being nice to each other. So the Dyson sphere, you know, like, and most possible ways
you can obtain, most possible ways you can arrange a bunch of matter aren't going to feed you the
energy that you need to run your computer. So you need a very precise and exact, narrow,
improbable arrangement of matter to feed the energy to your computers. And that's the technology
of the Dyson sphere. But, but, you know, a, you know, a pulsar has this presumably this iron,
iron 56 coat around it, and it has all kinds of things going on inside. Now you might say
that's amazing technology, it's got this, you know, iron casing around the pulsar, and it has all
this superfluid, you know, neutron matter inside and so on. You've got, you know, isn't that amazing
technology that the pulsar has. But actually, we don't think of that as technology, we think of
that as just a feature of the natural world. What makes something technology is that we can sort of,
we can pull it in to human purposes. It's not technology when it's, when it's just, yeah,
I feel like this has the obvious generalization to alien purposes. Now, in the limit, you can
always have the imaginary alien who looks at a river and just wants that one exact river to be
exactly the way that it is. And they have no need for to select, to plan, to design, to, like,
knock other pieces of reality together to make tools to make tools to make the river. The river
that they have is the optimal river, and they don't need technology. But, you know, most aliens like
that are going to die starving to death looking at the river because they won't have built a farm.
Let's take the great red spot on Jupiter, okay, which probably had many different steps that had
to be gone through in the atmosphere of Jupiter to end up with that red spot. Okay. And now we say,
you know, is that red spot technology, or is it merely not nature?
Nearly nature.
You could argue, you think it's nature.
I sure do.
Okay, but let's imagine that you are an alien who, whose great sport is going around in circles at,
you know, at high speed in the upper atmosphere of a gas giant planet. Okay, then, then that thing
is a, you know, you could think of it as a wonderful piece of technology that happens to be
provided to you by nature. All technology is provided to us by nature. It's, we don't get to,
you know, when we have our, you know, magnet or our liquid crystal or whatever else,
those are things which are provided by nature, but which we managed to harness for some human purpose.
So I'm just going to say the obvious thing and say that if the alien then installs a bunch of engines
in Jupiter's atmosphere to make the red spot swirl even faster so it can go surfing,
it has now installed technology in the form of engines and the larger red spot that incorporates
these, these engines is, you know, could, could be defined as technology or might say it's like a
feature of nature with some technology bolted onto it, but.
Okay, so I'm going to say there are, you know, things like jet stream type,
type wind patterns that, you know, the red spot is doing its thing and then some jet streams
arrive that manage to, you know, get it to go. I don't think, you know, I doubt you would ever
recognize an alien engine. I doubt you'd recognize it. The alien is some kind of fluid intelligence.
It's some, you know, I don't think you can recognize an alien engine. I think it's deeply human
to say, you know, it's an engine. That's something where you're imposing kind of the human
version of it. I bet the aliens are moving to get to pee around. I bet they're like,
they occasionally make things hot and then like transfer the heat to someplace else that they
want heat. I don't, I don't. You mean, you mean like, like jets from, from galaxies that are from
black holes where the black hole is absolutely trying to transfer its energy to out into the
universe? The reason why we've got these skyscrapers is that although you can live in a cave,
there's places we'd rather live even than caves. So we went around rearranging things.
And if there are any aliens out there who are just perfectly content with their caves,
they imagine skyscrapers and would be like, no, I'd rather I live in the cave. And they're like
that about every single thing. Then we're not, we're not, we're not going to meet them. We're
going to maybe someday we'll go to them, but they ain't coming to us. You know, this whole question
about what's, what's for a purpose and what's not, I think is very slippery. And I kind of suspect,
you know, I think I'm understanding your argument, which I'd not done before. So I've not studied
Europe. So, so there's been super interesting to me to understand what, I mean, I think it's
an interesting argument. I claim that there are, you know, it's certainly not self-evident that
this argument is like, oh, we should all buy, you know, AI insurance, whatever good that will
do us. That's not going to do any good. But, you know, it's the kind of, you know, you're,
the way I'm understanding it is, and it's kind of interesting that this is a sort of a close
analogy in biology. You're saying the thing that is really troublesome, I think, is this
tightening, this optimization that happens. And if it wasn't for that, if the computation was just
doing what the computation does, it would be okay. But as soon as you do this, it wouldn't be
profitable. It would be like a random computation that wasn't doing anything that opening eye could
sell. That's why they throw out the initial state of the neural network and use gradient descent
to make it do things that they wanted to do more than just the humans want. Right. But somewhere
inside, we think there are, so an interesting case here is, you know, artificial biological
evolution. So for example, you know, famously, you know, gain of function, viruses, all those
kinds of things where you're, where you're running it through many, many generations of artificial
selection. And it's sort of interesting, perhaps, that the kind of the place where you're seeing
trouble with the AI's is actually strangely similar to the place where one might see trouble
with, with things that one can do in biology too. That is, if you, if you grind the black box on
biology hard enough, it might succeed in wiping us out, but it doesn't have quite the same crushing
sense of ruin that would be associated with facing down something smarter than you. I am not as scared
of the viruses, somebody should probably be worrying about it could just kill us all before he managed
to kill us all. Let me give an analogy. Some part of your art. Right. So some, some part of your
thinking is happening in your brain. There's also another pretty elaborate thought like process
that happens in your immune system. As the immune system tries to figure out, you know, it has, you
know, these things that the T cells are interacting with each other and doing all kinds of things
that we mostly don't understand, which is optimization. Yeah, it's, it's doing things that are, you
know, in detail different for brains, but it's still doing something that's kind of a little bit
intelligence brain like. And so what, what, what is being said is, as you say, let's, let's, you
know, let's use technology to make a virus that is more and more difficult for our, for a fixed
immune system where we're stuck with the immune system that biology gave us, you know, there is
an argument there that says, if you do that, if we, if we take it outside of the, you know, if we,
if we run things to make this virus that's incredibly efficient and our poor immune system
doesn't stand a chance. And I think that's a, you know, that's a case that I think is not so
different from what you're saying, what you're imagining the AI. I think in particular, if humans
weren't allowed to fight back, like using their own intelligence, and instead you just had somebody
built like, if instead you just had a system that like built viruses much more intelligently
than the amount of intelligence than the, than that the human immune system is allowed to use. If
you used Alpha fold four and started designing viral features that were, you know, not just there
to kill individuals, but there to kill groups and entire regions. And, you know, you just put that
up against the human immune system, which is not quite static, but only shuffles itself,
reshuffles itself once a generation. Yeah, you could do some damage that way. And I agree that
that's like a, you know, smaller, lesser miniature version of the problem of facing down a super
intelligence using your own brain, which does get reshuffled every generation, you can use various
external aids, but none of it's going to match up to the super intelligence.
Yeah, well, just like the immune system, you can use various external aids, you can get vaccines,
you do all kinds of things like that, but you don't, and it has to fight its battle at a molecular
scale, which is a little different than, you know, we do not have, you know, our brains don't
actually allow us to go to battle on the molecular battlefield. Yeah, but, you know, they do allow
us to invent Alpha fold three. And, you know, if it comes down to a contest of the human brain
of the people trying to have humans live and the brain running the death cult trying to build the
super virus, that does not quite feel to me like there's a higher level of this game being played
out by two systems that are both smarter than the immune system and the artificial evolution
closed loop on the virus. So it doesn't I don't quite get the same crushing sense of doom and
ruin off of it. I think we're not going to solve the problem of whether the species is going to
wiped out get wiped out here, even though the deal, but I am but but I do feel like I've understood
more about what your argument is. And I, you know, on, you know, in real time, I can't take it
apart and decide, you know, do I agree with it? And am I going to, you know, say you got it or not?
I mean, I feel like there are I feel like my intuition is that there are some some kind of
just like you say there are unintended things that will happen in the AI. I think there are
unintended things that happen as you try and tighten up this argument. That is my claim.
Well, why didn't the the the conclusion I would I would end with is like, you know, you can imagine
the Native Americans trying to cut trying really hard to come up with arguments why like the the
ships they see approaching can't possibly hurt them, you know, like, well, what could they want?
Can we really define a language over what they want? Maybe most things that they want have us be
alive. And these people are in a much more favorable situation that they would have had a much easier
time of retrieving. Do I think there's a risk? I think there's a risk. Do I think that risk is such
that, you know, do I right now think that risk is I mean, anything one does in life has, you know,
there's risk, there's risk in all kinds of things that go on. And, you know, we humans, you know,
sort of believe in kind of moving forward independent of the risks. And I think it's it's
kind of like, is this such a kind of, you know, do I immediately think this is such a looming risk
that it's like change everything? I mean, it's kind of like I remember when people, you know,
people somebody was telling me years ago, when when they when the kind of
about, you know, when people are much less worried than today about climate and CO2 emissions,
things like that, they were saying, you can really reduce your, you know, your, you know,
you've got to really reduce your whatever it was, I don't know, energy use or whatever. And it's like,
you can do it, it's doable. And I said, Well, what does it take to do it? And I said, Well,
you couldn't have a computer, you couldn't have this, you couldn't have that. You know, it's like,
well, yes, if I, you know, if I spent my life standing on my head, then I would probably have,
you know, wouldn't have swelling my feet type thing or whatever it is. You know, then in other
words, there's a, there's, there's a risk. And there's a what you have to do to avoid that risk
and what the cost of avoiding the risk is. And I guess my own, you know, I don't, I don't know,
you know, you make some interesting arguments. So it's kind of like my own, you know, immediate
intuitive senses. Yes, there's a risk. You know, is that risk something that will
cause me to turn my life upside down? Not proven to me at this point.
Okay. I mean, for my, for my perspective, I've been like, forests on fire, and you're like,
well, what is fire exactly? And the thing is being, being like, well, like, can we like,
is there a particular exotic set of preferences, which would make this river exactly the optimal
river? And, you know, like, can you view the river as, you know, throwing up a water spout?
You know, this, this does not prevent an AI from, you know, building its own infrastructure and
then killing you. It wouldn't have protected the Native Americans from the, from the, like,
much more similar to them, Europeans that were coming toward their shore to say, like, well,
you know, can we really have a language for describing all the things that they might want?
Isn't this language a very complicated sort of thing? There's, there's a lot of like delightful
philosophical issues here, but I think they mostly integrate out of the, of the actual scenario here.
So that's the question is, is, you know, in my life, people have had different scenarios for
how the world will end, whether when I was a kid, it was mostly, you know, you know, World War III
type, type scenarios, then later on, I mean, for some generation, it's the world's going to burn
up with, you know, climate change. And then there's, you know, there's these various scenarios for
how things, and I think one of the things that, you know, there's the question of, and sometimes
there are things where people say, but, but the, or for example, when people were saying, you know,
build the, build the large Hadron Collider, and it will create a black hole that will destroy the
world, right? That was a, you know, that was the thing people raised that possibility.
We understood the relevant laws. We ran the calculations. We worked out that we worked out
multiple constraints for multiple angles, saying the probability was tiny. And in other times,
in, in human history, people have warned about leaded gasoline poisoning the soil, and people
have been like, ah, you know, don't, don't be, you know, these like, don't be a worry wart,
it'll be fine. And then, you know, we've kind of poisoned a generation and dealt permanent,
you know, dealt lifelong brain damage to a bunch of kids growing up with leaded lead in their soil.
I mean, I'm most, I'm particularly reminded of the stories people told me about the Manhattan
Project, where they were going to detonate the first, you know, do the first nuclear weapons test.
And the question was, would it ignite the atmosphere? LH602 was the paper they wrote to
analyze that. It's interesting reading. They again had multiple angles from which to look at it and
say, that will definitely not happen. We do not have this with AI. It is more of a leaded gasoline
situation. I didn't think they'd written it up. That must have been much later, because I think
it was, you know, what I heard was from people who were sort of involved, who were like doing back of
the envelope calculations and saying it's really far away from from igniting the atmosphere. But
I think they did back of the envelope calculations. And then they, you know,
like did front of the envelope calculations, which, you know, good for them.
Well, but okay, but so so the question now is, you know, in that case, it all worked out,
didn't ignite the atmosphere. What you're saying is that, you know, we can't do the calculations
for AI's. And you're saying, or you think you have done the calculations for AI's,
and they will ignite the atmosphere. Yeah, back of the envelope. This is not like a rigorous
calculation. And it's not going to be a rigorous calculation before the world ends. Because this
is more like, this is more like tangled up biochemistry and less like straightforward physics.
So the concern is your back of the envelope calculation says AI's will do the analog of
ignite the atmosphere. And I guess my, you know, one feature of back of the envelope calculations is
they require intuition. They're not things where you can, you know, if you do some very
rigorous thing with axioms, and you can go step by step, it doesn't really matter if you have
good intuition or not. It's just mechanical. But good thing that the envelope requires intuition.
Good thing that reality, since the moment of its dawn has followed a hard and fast rule,
if you cannot do rigorous, formless, formal calculations about something, it is not allowed
to kill you. Just nobody in history of time has ever once had a cause of death that would that
required intuition to understand. That's not what I'm saying. What I'm saying is, if we're
trying to make a, you know, we've got two, two possibilities. One is, you know, we shut down
our lives or whatever. And we say, you know, we're not going to have this and that and the other
because it, because there's a risk that it kills us, or because we think it will kill us,
but it will cause us all kinds of trouble to shut all this stuff down. And maybe it's even
impractical to do it. But just it's, it's something which, which is a cost to us to shut all this
stuff down. And so we've got, you know, door number one is we shut everything down, and then
we're sure it won't kill us. And door number two is we're sufficiently worried that it will kill us
that, you know, we're, we're, you know, one thing was sufficiently worried it will kill us. So we
shut all that stuff down. The other thing is we don't shut that stuff down. We get all the benefit,
all the known benefit of being able to do those things, but we have some risks that it will kill
us. And so the thing that I think is a, the rational thing to do is to say, let's try and
tighten up those back of the envelope intuitions, so to speak, and see where we come out. And,
you know, that, that's a, to me, the thing that, you know, I think we both agree that there's a
lot about kind of how AI really works and what it's really doing that we don't understand. And I
understand your analogies about, you know, the Native Americans or whatever else. I, you know,
to me, that requires a boatload of intuition, which I don't think I have. Now you may have it
and be absolutely right. And, you know, then, you know, it's kind of like, I just don't have
that intuition. And, you know, the intuition that I have is perhaps, you know, I have a little bit
more than everyday intuition, because I have different intuition about what computational
systems do because I've sort of lived with them for a long time. But I don't have, I don't claim
to have the kind of intuition that would allow me to tell, does this back of the envelope calculation
that you're talking about, you know, does that land in a, oh gosh, you know, it's going to kill
us all. Or does that land in a, oh, actually, you didn't think of this thing and that thing and
the other thing. And the, you know, the water spout detail actually derailed the intuition
and we got the wrong answer. So that's, that's kind of where I feel I'm at.
Yeah. So we're now sort of like going into politics. And I, the case I would make to the
politician is at the point where the most legibly credible expert who recently won the Nobel Prize
about it says like, well, personally, I am over 50%, like my sort of first order personal
assessment is over 50% existential catastrophe. That's, that's code for killing everyone.
But, you know, like taking into account what other people are saying, I would say in public,
more like 10 to 20%. And the people who've been studying the issue longest are like, yeah, this
kind of looks to us like it straightforwardly kills you. Like why would you even expect that
not to happen? That's us. And other people are going like, there is no danger here. This is
ridiculous. The people talking about it are stupid. And, you know, from a political standpoint,
I think what you ought to do at least then is start getting start getting started on preserving
the option of shutting it down, which is easier to do, which would have been easier to do in 2022
than 2024. It will be easier to do in 2025 than in 2027, if we're still alive in 2027.
Actually, if we're not alive in 2027, it's still easier to do it in 2025.
One thing that I will agree with is thinking about these things is worthwhile. And, you know,
to have nobody thinking about it is a mistake. You know, if there is a high risk, nobody thinking
about it is the wrong thing. People who think about this for a while do tend to assume,
do tend to start agreeing that the default outcome is everybody dying. Sometimes they think they
have clever plans for presenting that, but they do tend to follow along with the default outcome
as everybody dead. See, I think, though, unfortunately, there are many selection
effects at work here. I mean, you know, many of us, you know, I will say that I am intrinsically
an optimist. I don't know how you feel about yourself. Are you intrinsically an optimist or
pessimist? I try to do the, you know, like the best calibrated, best discriminating
probabilities I can manage. If you, I think if you explicitly say that you're being an optimist
or pessimist, you have clearly departed the way of truth. You have confessed that you are now
taking considerations other than truth into account in your statements. How sinful, how unvirtuous.
Well, for me, you know, what do I mean by that? I mean, I try to do projects that many people
would say, oh, that's an impossible project. The fact that there's not a matter of truth there,
it's like this project is hard, but I'm going to be optimistic that it's possible rather than,
so, you know, I think that that's to say, you know, I don't do projects which I think are obviously
doomed. But on the other hand, I'm going to take the point of view, let me try it, rather than,
oh, gosh, it's never going to work. Let me not try it. So that's, that's kind of what I mean.
That is absolutely how I spent the years from like 2001 to 2021, or thereabouts, well, 2020,
maybe, was, you know, like, all right, I'm going to run at the alignment problem. But it became
kind of clear that, you know, this wasn't going to work for me, it wasn't going to work for the other
people working on it, and that the field itself had kind of like failed to form a process that could
distinguish that, where that could like publish, distinguish Elon Musk's,
we will just make grok to pursue truth. And like, even the old science fiction writers
understood that humans may not be the most efficient way of producing truth, anything that
just produces lots of truths may not produce other things of, you know, the best possible value.
But, you know, like, who tells Elon this, that he believes. And yes, there's selection effects,
like, people did in fact pay me to work on this problem. And, you know, you're hearing me because
people paid me to work on this problem, and they didn't just like star. And the people who founded
open AI, where people selected to believe that alignment was totally a problem within their
grasp, or and or to be willing to just take Elon Musk's money and run with it. And if you want to
look for people who are not selected, I think you end up with Jeffrey Hinton, you know, the guy who
just won the Nobel Prize for the work he did on machine learning. And, you know, it's kicking off
the whole modern deep learning resolution. That's the guy who isn't obviously selected. That's the
guy saying like, well, personally, 50%. But if I take into account what other people are saying,
10 to 20%. I really need to actually sit down and talk with him at some point, which we've never
actually done, and try to talk them out, out of listening to those particular other people,
I think their arguments make no sense. You know, it really, it damages my thinking about this,
that the various people you mentioned, you know, I've known all these people sometimes for a very
long time. So it's kind of, it kind of throws a wrench in my kind of, you know, there is,
there is sort of, you know, none of us have, I think, you know, perfect sort of calibrated
rationality about everything. And it's kind of, you know, it's always challenging to, you know,
this, this thing about, you know, I consider myself a convinceable person in terms of your
argument. Okay, I'm not, you know, I don't have a, you know, I'm not about to say, oh,
you know, I'm going to change my life, you're right, without understanding it. But I'm, I'm
invincible. So I'm not, and I don't know how many people are, maybe there are many people who are
not. But I think I am, I'm not convinced yet. But that's, you know, I mean, like this conversation,
I understand much more about what you're saying. And it's, you know, I think there are, to me,
there are interesting questions to try to answer, which people should try and answer. I mean, you
make it sound like it's urgent, you know, the people are coming off the ships, and you might be
right. You might be right. And I mean, I'm watching things go downhill in terms of how
much you'd have to spend and how much pain it would take to get to like, deproliferate the
technology. And, you know, I don't think it's realistic. I don't think it's realistic. I don't
think it would cost less than the Persian Gulf War if we did it today. And even if we like,
and if we do it later, maybe it costs more like World War Two. But humanity did not lie down and die
when the alternative was fighting World War Two. We went off and fought World War Two. And if
everybody's going to die otherwise, you just do what it takes. Yeah. But the fact is, there are
things that one can deproliferate, like nuclear weapons, for example, because the supply chain
is really complicated. There are ideas. It's very hard to deproliferate ideas. Yep. If it were the
case that a single person on earth having the idea in their mind of artificial superintelligence
would cause everyone to die, that would be an even worse situation to be in than the one we
are in right now. I might in that case, well despair or like, try to do something like weirder
and sillier. I don't think we're in that situation. I don't think we'd need to deproliferate the idea.
Okay. Gentlemen, thank you so much. I'm going to call it now. I think this might be the best
conversation in MLST history. Thanks. And part of that, and our purpose is about curating conversations
of this quality level with people of your caliber. And I was expecting this to be a bit of a kind of,
I don't want to say chat GPT conversation, but talking past each other. And the authentic
exchange that you've both just had is really mind blowing. So thank you so much. Thank you.
Thanks. Thanks for setting it up. This was interesting. Eliezer, nice to see you. Hopefully,
we'll, I think it's been like 15 years since we saw each other in person.
Probably. Yeah.
So you in another 15 years? No, I kid, I kid.
No, 15 years, according to you, it's all over.
Well, yeah, but like, it would be great to still be around in 15 years. But
that said, I'm not saying that we like shouldn't talk again for another 15 years.
I'm not saying this was the optimal interval or anything, you know.
Right, right, right. And we both, I think, are interested in cryonics and things,
and maybe we get to talk in 300 years and we get to say, you know, this was the
We get to resolve all the bets if we make it through there. So,
you know, conditioning on that possibility, probably you won a bunch of arguments.

Coming up later in Machine Learning Street Talk,
Professor Jan Le Koon, the godfather of deep learning.
There's been a lot of people who have been sort of saying
there is a limitation to deep learning, let's say,
or machine learning more generally,
because it's obvious that those things basically
do curve fitting, and that only works for interpolation
and not for extrapolation.
And that kind of dismissal always sounded wrong to me.
Would this qualitative difference be in the form of
fundamentally different things from deep learning,
things that are like discrete symbolic reasoning
or things of that type?
And to that, my answer is clearly no.
I do not believe that's the case.
That's a limitation of supervised learning.
It has absolutely nothing to do with deep learning.
Compute on things.
OK, and I put a stop right there.
We also had a fascinating conversation with Randall
Belastriro from Meta AI Research.
So I think it's two different things to be able to interpolate
or move on your manifold and being in the interpolation
regime from your training set.
So is this similar to interpolation?
Well, I mean, all of machine learning is similar to interpolation
if you want, right?
When you train a linear regression on scalar values,
you're training a model, right?
You're giving a bunch of pairs X and Y.
You're asking what are the best values of A and B
for Y equals AX plus B that minimizes the square error
of the prediction of a line to all of the points, right?
That's linear regression.
That's interpolation.
All of machine learning is interpolation.
In a high-dimensional space, there is essentially
no such thing as interpolation.
Everything is extrapolation.
So imagine you are in a space of images, right?
So you have a core images 256 by 256.
So it's 200,000 dimensional input space.
Even if you have a million samples,
you're only covering a tiny portion
of the dimensions of that space, right?
Those images are in a tiny sliver of surface
among the space of all possible combinations
of values of pixels.
So when you show the system a new image,
it's very unlikely that this image is a linear combination
of previous images.
What you're doing is extrapolation, not interpolation.
And in high-dimension, all of machine learning
is extrapolation, which is why it's hard.
Now, that was a clip that went out on the Chalet show.
And I think I understand now what Lacune meant.
He's saying that our intuition of interpolation
is only correct in a very low number of dimensions.
It's a mathematical impossibility
to have statistical generalization in high dimensions.
So he's not saying that neural networks are really
extrapolating in the sense of continuing the pattern.
He's saying our definition of extrapolation,
the convex whole membership, is broken.
Machine learning does not, and will never,
work in high dimensions.
That's why we've invented so many tricks
to reduce the statistical and approximation complexity
of problems, just like we do in computer science
of the computational complexity of algorithms.
Lacune is not saying that deep learning models are clairvoyant.
Jan Lacune thinks that it's specious to say
that neural network models are interpolating,
because in high dimensions, everything is extrapolation.
Oh boy.
We started thinking about making this show many months ago
when Randall Belastriro and Jerome Pesente
and Jan Lacune first released their paper,
Learning in High Dimensions Always Amounts to Extrapolation.
And it feels like a lot has happened since then.
I mean, they do say the mark of an educated mind
is being able to change your opinion
in light of new evidence.
Where we are now compared to where we were then.
I mean, let's just say, I feel like I'm standing
on the surface of Pluto.
I was initially quite skeptical of this paper,
and I was trying to pick it apart.
And now, it feels like we've done 180 degrees,
not only on the way we think about the paper,
but how we think about neural networks in general.
And we really wanna try and impart some of that knowledge
with you today in the introduction,
which might be quite ambitious, so please bear with us.
But generally speaking, the structure of the show today,
it's gonna be a big show.
It's gonna be a big show.
Use the table of contents, skip around a bit.
There's an intro, where we talk about this blind view
or this boundary view of neural networks,
which I think is very interesting.
There's a conversation with Jan Lacune,
the godfather of deep learning.
And we also speak with Randall Belastriro,
who released this beautiful paper
about thinking about neural networks in this new way.
And we have a debrief as well.
So I've not edited the show yet,
but it's gonna be about three hours.
Anyway, the show must go on.
So as I was saying, particularly speaking with Randall,
was a revelation.
I mean, I originally thought that the authors were saying
that people shouldn't think of neural networks
as interpolators, because they thought neural networks
were doing something even more sophisticated.
But what I came to realize, at least in Randall's case,
is he's even more cynical about the behavior
of neural networks than Gary Marcus.
He doesn't think that they're fitting curves at all.
Reading Randall's recent spline theory
of deep learning paper has completely changed the way
that I think about their behavior.
So his view, basically, is that neural networks
recursively chop up the input space
into these little convex cells, or polyhedra,
conceptually similar to decision trees,
but one where the regions in the layer can share information,
which means that different faces of these polyhedra
will correspond to different hyperplanes
set down by different neurons in the neural network,
of course, ones which are topologically addressable
from that location.
So to me, this ends the notion
that these neural networks are learning smooth data manifolds
or performing smooth geometric transformations
in the generative setting, which I used to think of
as being a kind of diffeomorphism.
It also gave me the realization that,
at least in inference, each input prediction
is representable with a single linear affine transformation,
which leads directly to the next realization,
which is that the latent space is not homogeneous.
It's potentially a different space
for every single input example.
I just didn't realize that before.
Suddenly, a lot of the magic of deep learning has vanished,
and I see them in the same light as things like decision trees
and SVMs in classical machine learning.
I mean, probably because I understood
those things very deeply.
So yeah, neural networks have lost a little bit
of their mystery, but I think it's a good thing.
Reminds me a little bit of this parable
of the blind men and the elephant,
where, let's say you've got four blind men
around an elephant, one's got his arm around the trunk.
Oh, it feels like a snake.
One's feeling the tail, or it looks like a rope,
or it feels like a rope.
One's on the side, or it feels like a wall.
And it's a very similar situation with neural networks
that the experience of these blind men,
it's all the truth, but it's not the complete truth.
We're all just trying to understand this thing
from different angles.
Now anyway, the two key challenges in machine learning
are one, the curse of dimensionality.
And there's a corollary of that,
knowing what to ignore in the input space,
because it blows up exponentially with the dimensions.
And two, the extrapolation problem.
What happens when you extrapolate
outside of the training data?
And why does this notion of extrapolation even matter?
Well, put simply, it matters
because of the implications it has for generalization
in deep learning.
Now, why do deep learning models work at all?
Well, I think there's an incredible amount of engineering
which goes into the state of the art machine learning models,
which makes them work very well on specific tasks.
But it's easy to deceive yourself with neural networks.
Literally, everything about your training process
and predictive architecture
could be leaking domain-specific information
into your model.
Neural networks are not really blank slate models
like we've been led to believe.
A lot of human-crafted domain knowledge
goes into these models.
By the way, this is why Fran√ßois Chollet,
I mean, he talks about this notion
of developer-aware generalization,
which is being able to generalize to tasks
which the developer of the system
didn't know about at the time.
Now, weights and biases is the developer-first
MLOps platform.
Build better models, faster with experiment tracking,
dataset versioning, and model management.
It provides a platform and a set of tools
which you can use for the entire life cycle
of your machine learning process.
Every team has a reliable system of record
for their engineering system,
like whether it's Azure DevOps for software engineering
or Confluence for Team Wikis.
Weights and Biases is your system of record
for machine learning.
Now, if you're a quick link and to help out the podcast,
head over to 1db.me forward slash MLST.
I'm extremely proud actually
that we've been sponsored by Weights and Biases.
I've been a huge fan of what they've been doing
since the very beginning.
And also, I've been a part of their community forum.
They are the one company who, in my opinion,
has totally changed the game
around what is the highly nuanced process
and the kind of multidisciplinary team actions
that are required to get machine learning models
safely into production.
I've been a chief data scientist
for a major corporation for the last 18 months or so,
and I've always been a big believer
in introducing engineering rigor
to machine learning and data science.
Engineering fundamentals are so important
when you bring machine learning systems to production,
as is helping data scientists to become first class citizens
in the entire life cycle of model development and deployment.
It's almost a shame that MLST
has become quite science orientated
because in my day job, I'm an engineer first.
I would love to make more content about this stuff.
Anyway, Weights and Biases helps you go faster.
It makes your team more resilient
by increasing your knowledge sharing.
It helps you build models
which not only have better predictive performance,
but are more safe and secure.
Weights and Biases gives you better management information
allowing your company to make better decisions
and build data products with greater transparency
than ever before.
And critically, because Weights and Biases orchestrates
the entire machine learning process,
your models are reproducible
and important decisions are immortalized.
This fine level of control
lets you set guardrails where you need to
and reduce friction wherever possible.
I've designed many ML DevOps systems over the years,
including when I worked at Microsoft.
The technology has really come of age now
and I think you should be using a platform
like Weights and Biases
to help you train your machine learning models
and to get them into production.
Reproducibility has always been one of the biggest problems
in machine learning.
And the reason for that is that these systems
are quite brittle, frankly.
Their runtime characteristics depend strongly
on the vagaries of the training regime,
the choice of hyperparameters,
even the hardware that they were trained on.
You know, you always get into this syndrome
where it works on my machine,
but I can't put it into production
because I've got no idea
how to recreate this thing somewhere else.
I work in a highly regulated industry
and we often need to wind the clock back
to try and understand how a model was created,
which decisions were made
and possibly go back later
to reason about a model's behavior.
This is precisely what Weights and Biases does.
Now, it's completely free for academics
and it's simple to get started.
You just have to add a few lines of code.
So what are you waiting for?
Remember to click on our special link
because it helps us out.
That's 1db.me forward slash MLST.
We are so grateful for the sponsorship
from Weights and Biases.
And also feel free to get in touch with us
if you're interested in sponsoring the show.
It'll help us scale up the operation a little bit
because at the moment I'm doing all the editing
and it takes a lot of time.
Anyway, thank you very much.
Randall's other recent work demonstrates
that a large class of neural networks,
including CNNs, Resnets, RNNs and beyond,
those that use piecewise linear activation functions
like RELU, can be entirely rewritten
as compositions of linear functions
arranged in polyhedra or cells in the input space.
It's a high resolution slice and dice into linear forms
like a fruit ninja set loose on a marching cubes algorithm
such as K-means clustering, matched filter banks
and vector quantization,
which in plain English means locality sensitive clustering.
It also provides new insight
into the functioning of neural networks themselves.
For example, with this view,
what a neural network is doing
is performing an input sensitive lookup
for a matching filter
and then computing a simple inner product
between that filter and the input signal.
If that doesn't shed light
on Francois Chalet's characterization of neural networks
as locally sensitive hash tables,
then I don't know what will.
Another cool thing to come out of Randall's work
was a geometrically principled way
of devising regularization penalty terms,
which can improve neural network performance
by orthogonalizing the placement
of those latent hyperplane boundaries
to increase their representational power.
In short, looking at neural networks
through this piecewise linear kaleidoscope, if you will,
is opening new avenues
of technical understanding and exploration.
Now, we spoke with Yannick about this as well,
and he commented that he had looked into these polyhedra,
which he called relu cells, in his own research.
While he agreed that the boundaries between them
are not technically smooth,
there are so many of them,
combinatorially many possible polyhedra, in fact,
since each can be defined by any combination
of topologically addressable hyperplane boundaries.
There are so many,
Yannick thinks that neural networks
can make them effectively smooth
by arranging them so that they change very little
from neighbor to neighbor.
This insight came from his work in adversarial examples,
where the name of the game is to perturbed the input
as little as possible to the nearest polyhedron
with a different class.
Anyhow, for me, Randall's work has transformed
the way I think about neural networks.
I know what they're doing at a much deeper level now.
Each layer of a neural network
contributes a new set of hyperplanes,
and the relus act to toggle the hyperplanes
in an input-sensitive way.
Every layer, indeed, every neuron in the network,
not just the final layer,
participates in placing flat decision boundaries,
defining a honeycomb of affine cells.
Each input signal will fall into one of these cells
and be transformed by the combined affine transformation
of every neuron it activated.
I also used to think of a single unified latent space
at each layer.
However, it seems obvious that any particular input
will toggle, in an input-specific way,
a set of hyperplanes.
By virtue of the relus, it does or does not activate.
Therefore, different populations of input samples
will reside in different latent spaces at any one layer
defined by the activated set of hyperplanes.
So it seems to me that instead of having
a unified latent space,
we have input-specific latent spaces plural at each layer.
There's also the matter of whether classifiers
are even learning the data manifold at all.
I have my doubts.
They may be learning predictably useful aspects
of the manifold.
However, neural network classifiers are optimized
to find class boundaries,
and any structure between boundaries can be ignored.
The idea that learning boundary manifolds
necessarily means learning intrinsic connection manifolds
is where I'm really struggling now.
I can't convince myself that optimizing for separation
will give the same outcome as optimizing for connection.
For example, training the same neural network
in two different ways, one for classification,
and once for decoding or generation,
will result in very different network weights
and latent structure.
It's even more difficult to accept now
that I'm thinking of these manifolds as piecewise linear,
where what the neural network has learned
is combinations of separating hyperplanes
that chop the space into single-class polyhedra.
After all, there is nothing in the objective function
which cares about the structure within a polyhedra.
As long as the class is correct, we're good.
Nor is there anything necessarily optimizing
for global manifold structure
beyond just efficient and parsimonious use
of the available parameter space
and whatever prior structure was hard-coded
by humans and the network constraints and topology itself.
Randall commented in the interview
that in high-dimensional spaces,
you can easily separate everything,
but your generalization performance might be bad.
So you want to trade off separability with dimensionality.
He thinks the merits of deep neural networks
is being able to find a nonlinear transformation
which retains separating hyperplanes
while reducing dimensionality enough
to confer generalization power.
I mean, I couldn't help but notice
that everything in the machine learning world is linear.
All the popular algorithms are linear
and any nonlinearity is a trick.
I mean, we just apply some nonlinear transformation
to the data before running it through our algorithms,
just like it is in support vector machines
called kernel-rid regression and Gaussian processors.
Deep learning models are no different.
We're just placing these relus all over the input space
to slice it and dice it.
Let's think about this.
I mean, what is the simplest mathematical model?
Linear, right?
What's the next most simple one?
Piecewise linear.
So machine learning hasn't even evolved yet
to the second order.
I mean, neural networks in their contemporary usage
only include the minimum possible nonlinearity
of piecewise linear.
If they didn't, an entire neural network model
could be described as one monolithic linear function
with a single transformation matrix.
This view, I think, should give you a cleaner
and analytical view of neural networks.
Now, there are some reasons, in my opinion,
for the tendency towards linear.
I mean, computability is one of the reasons,
but almost every part of mathematics favors linearization,
whether it's Newton's method or calculus
or linear algebra solving PDEs,
plenty of other examples.
So it shouldn't come as a huge surprise.
Not only that, the function space of nonlinear functions
is exponentially larger.
And remember, in machine learning,
the big challenge is to reduce the size
of the approximation class.
So what about boundaries?
Now, the purpose of every relucell
is to define a boundary in the ambient space
to chop off what is no longer required.
All you have is a linear separating hyperplane.
And downstream, you work out the distance
from the hyperplanes that you're on the right side of,
the ones that didn't get chopped off.
So the magic of neural networks
is actually learning what to ignore in the ambient space.
Now, I think a problem with my previous intuition
is that, like most people, I imagine to neural network,
latent space is a bit like a UMAP or a Disney projection plot.
And this leads us to misunderstand their behavior.
The latent space that these examples get projected into
is not homogenous.
Depending on which cell you fell into
in the input space or the ambient space,
a different affine transformation will be applied,
sending you to a different region of the latent space.
So the latent space is kind of stitched together
like bits of a cosmic jigsaw puzzle in the ambient space.
And then when you run UMAP on the latent,
you see all of the clusters,
but you'd be forgiven for thinking
that it was performing some kind of smooth,
diffeomorphic transformation of the entire input space
and successive layers in the neural network,
or even learning the topology of the data manifold.
I think it's much better to think of neural networks
as quantizing the input space,
much like a vector search engine does,
using locality-sensitive hashing.
Now, imagine a classification problem on the Cartesian plane
where the upper right and lower left quadrants are blue,
and the upper left and lower right quadrants are orange.
Now, as a minimal example,
if we train a single layer for neuron-relu neural network
to fit this, it will fit four diagonal hyperplanes,
two of which are reflected versions of the same hyperplane.
Now, by the way, you can play with this
on the TensorFlow playground,
which we think is probably one of the best tools
for building strong intuition on how neural networks work.
Now, what it does is it shows you how the ambient space
is being effectively subdivided
by all of these addressable hyperplanes.
But this quadrant example is unintuitive in the tool
because getting your head around how the relus act together
to combine these hyperplanes to carve up the ambient space
isn't immediately obvious.
Now, when you run the tool on a more complex example,
so for example, a spiral manifold,
you'll now see the artifacts of these hyperplanes everywhere.
When these piecewise linear chops are composed together
in the second layer, we get a decision surface
in the ambient space, which can appear smooth,
given enough pieces,
but it's actually a composition of piecewise linear functions.
Now, as you can see, it's just chopping up the input space
in every neuron in the first layer,
each one with a different angle and translation.
And when you get to the second layer,
it's chopping up the space in a more sophisticated way
by composing together the chops from the previous layer.
Now, the first thing to realize here
is that for each test input example
and every example in its vicinity,
its class projection is definable
with a single affine projection.
There's no continuous morphing going on here.
It's more like a jigsaw puzzle being pieced together
to form many parts of a bigger ambient space.
Now, oftentimes we see negative weights,
which actually allows the hyperplanes to combine
in interesting ways to kind of partially cancel each other out
to allow more complex decision boundaries.
Any hyperplane can be reused by multiple other neurons,
whichever neurons it can reach
via some topologically addressable pathway.
Every hyperplane can be the face of multiple polyhedra
in the ambient space.
This is how they share information.
And it should be obvious by now
that there are two to the N addressable polyhedra
in the ambient space, where N is the number of neurons.
Now, we also observed that the neural network
is only capable of slicing and dicing the input space
with flat planes.
The only smooth nonlinearities enter the picture here
where we as data scientists linearize the data
by performing some smooth nonlinear transformation
before it even enters the neural network,
much like is the case with other machine learning algorithms.
Now, even on the spiral data set example,
the quickest way to make the neural network fit the data
is to effectively linearize the data
by applying a nonlinear transformation
before it even gets in.
Now, we also played around
with this kind of contrived circular manifold data set
where there's a ball of data encompassed
by a circle around it.
And it's possible to do a better job on that
with one relu neuron, right?
But with two nonlinear transformations
of the input data set,
rather than fit this huge bunch of piecewise linear relus
on the ambient data.
I mean, it should be obvious, right?
But this idea that you don't need feature engineering
in neural networks, I think, is nonsense.
Now, given how obvious it is that neural networks
are just chopping up the input space
with these piecewise linear functions,
it begs the question,
how could they possibly extrapolate in any meaningful way,
right, especially in the general setting?
And by the way, when we say extrapolation,
we're talking about this more general sense
of continuing the pattern,
not the convex whole notion in a given in the paper.
Regarding these piecewise linear functions,
I remember an impactful moment
from more than 20 years ago in grad school,
a visiting professor who had multiple science
and nature publications,
you know, a pinnacle of academic achievement,
was sharing his insights on how we students
might also extract such papers ourselves from nature.
And he was showing figures from his paper
and they all had two things in common.
First, they depicted very simple relationships
that were previously undiscovered, okay?
And second, all the fitted models were piecewise linear.
He even explicitly commented along the lines
that of course there's some underlying
smooth nonlinear relationship,
but absent a solid theoretical model of that,
which is often going to be the case for new discoveries,
you're better off with simple piecewise linear models.
I guess when in doubt,
Occam's razor always makes straight cuts.
And now it's clear to me
that the successful deep networks of today
are following that advice.
If you've ever wondered why piecewise linear activation
functions are dominating the field,
maybe it's because they've abandoned all pretense
at finding smooth nonlinear models
and are keeping it simple by fitting piecewise linear models,
albeit at machine scale with billions of parameters.
Randall's spline work makes that bit
of philosophical insight brutally clear in my opinion.
It conjures a scene of Darth Vader
that cybernetic warlord towering over
a growing neural network saying,
embrace your hyperplanes.
Now, you know, I've always been skeptical
and often remind us of the limitations
of today's machine learning.
I'd say things like ML isn't magic learning,
but for a moment,
I too allowed myself to become deluded into thinking
that they are creating some kind of nonlinearity
beyond piecewise linearity.
But relu is by far the dominant activation function
because it stops pretending at anything other
than piecewise linear.
Just stick in a flat boundary threshold and the line.
A neuron puts in a hyperplane
and then lets the rest of the network chop more as needed.
All the success of neural networks seems explained
by piecewise linear functions.
I also find it intriguing that our own brains,
our own wet neural networks,
have somehow gained access
to smooth nonlinear imagination.
Just look at the laws we've defined in physics.
Many exhibit nonlinear but smooth structure.
On the other hand, neural networks chop up the input space
with flat boundaries and sharp edges.
How can we possibly expect them to learn or discover
the kinds of smooth relationships
that seem fundamental to science and reality?
All feature engineering and representation learning
and machine learning is about finding
these interpolative representations.
Now, I saw a really interesting example of this
when I was reading Francois Chouelet's book recently
and he spoke of pixel grids of watch faces, right?
Now, they're not interpolative in the ambient space.
If you take the average of two of these images,
you'll just get four faded clock hands
on top of each other, right?
Superposed, not very helpful.
But however, if you represent the hands
with Euclidean coordinates,
then the problem becomes more interpolative
but you're still not all the way there.
The average of two points of interest
doesn't fall on this intended circle manifold
that we're interested in.
You know, it would be interpolative
if we use some relevant distance function
which was some geodesic interpolation.
But, you know, anyway,
if we encoded the problem using polar coordinates,
then it becomes linearly interpolative, right?
The problem is solved
and you can generalize perfectly to any new clock face.
But, I mean, in that case,
you wouldn't even need a neural network
and the kicker is that we as data scientists
would have to figure this out.
Ourselves, right?
The neural network wouldn't be particularly efficient
at doing so, especially if we, the data scientists,
didn't feed in the relevant nonlinear transformation
before it went into the model.
So anyway, if there exists
a nonlinear transformation of this problem,
then, you know, which makes it interpolative,
then we can say in some sense
that it's got a lower intrinsic dimension.
So crucially, deep learning models,
they can be understood as unconstrained surfaces.
They do have some structure
coming from their architectural priors,
but that structure only serves to restrict the search space
to a smaller space of unconstrained surfaces.
It doesn't provide the kind of inductive prior
that would enable stronger generalization.
A model architecture does not contain a model of the domain
except in the extremely restricted sense
of injecting priors such as, I don't know,
translational equivalents in convolutional neural networks.
If you want to generalize in a more systematic fashion,
you either need a model with a strong inductive prior
or a model which doesn't operate
by empirically slicing up the ambient space
like a deep learning model does.
A discrete symbolic program
would be an interesting alternative.
I mean, just imagine the program y equals x squared.
It generalizes to any arbitrary number, right?
Because it's highly structured.
You can fit it with three training points.
And once fit, it'll generalize to anything,
even things that are outside of the training range.
But the kicker, of course,
is that you wouldn't know which curve to use
if you only saw three points in the first place.
Now, let's think of another example.
And imagine a discrete staircase space.
So in this particular example,
an unconstrained curve gives you complete garbage,
but a structured model with the correct priors,
you know, it can still be a curve,
will generalize in the extrapolation regime.
And this prior basically means that you need to encode it
or linearize it before it even gets into the neural network.
In both cases, you're making the neural network
work extremely well for a specific thing,
but not very well for generalizing between new tasks.
So anyway, when we say that deep learning models
generalize via interpolation,
what we're really saying is that these models
that we're using today,
that they work well with data
that's within the training distribution of the problem, right?
Well, a problem that's intrinsically interpolative,
but they won't generalize systematically to anything else.
You won't generalize when looking at problems
that are not interpolative in nature
and problems that are outside the training distribution.
So what do we talk about this binary notion of extrapolation?
Well, the problem with the binary convex
whole notion of extrapolation
is that we're promoting this idea
that the moment an example falls epsilon outside the hole,
that the prediction qualitatively changes.
Instead, I think it would be more of a distance question,
which is to say the further away you get
from this convex hole, the greater the uncertainty.
And it's not like you fall off a cliff
when you step outside the convex hole.
Your approximation of the latent manifold
remains valid for a little while,
although it will quickly degrade as you move further outside
unless your model has some strong structural prior
that matches the regularity of that latent manifold.
So this brings up an important point, right?
The relevant question is not whether you're inside
or outside the convex hole.
It's how far you are from the nearest training data points.
If you're outside the convex hole,
but you remain close to the training data,
then your model remains an accurate approximation
of the latent manifold.
And even if you're inside the convex hole,
you're in a region that wasn't densely sampled
at training time, right?
Where there's no nearby training points.
Your unconstrained model may not accurately approximate
the latent manifold.
So inside or outside doesn't really tell you very much.
It's all about the proximity.
You're performing local generalization
and the quality of your ability to generalize
to new situations depends entirely
on their proximity to known situations.
However, if you're dealing with training data
that is densely sampled within a certain region
of the problem space,
then the question, can I generalize here,
becomes approximately equivalent to,
is this inside the convex hole or outside?
Now, the whole point of feature engineering
is to make data sets interpolative.
You know, either us data scientists
design the features by hand or neural networks
learn them as part of the training process
in big air quotes for the podcast listeners.
So, you know, I think when folks make the argument
that machine learning models generalize
via interpolation or that computer vision data sets,
you know, that they're interpolative,
what they mean is that there's an encoding space
for which the problem becomes interpolative
or there exists a non-Euclidean pairwise distance function
between the instances that makes the problem interpolative,
which is basically the same thing.
So, you know, if it's possible to do this,
then you can say that the problem
is intrinsically interpolative,
but your current representation of the problem is not.
So the key endeavor in machine learning
is to find better representations,
representations which reveal
the interpolative nature of the problem.
Now, you might just cynically write off this paper
as being a trivial finding, right?
You know, it seems trivial that the pixel space
is not linearly interpolatable.
Something that we've known about for decades.
I mean, that's why computer vision engineers
in the 1980s would write feature detectors
to create an interpolative space
for machine learning algorithms to work on, right?
If you take any two images
and you interpolate between them, what do you get?
You just get a faded copy of both.
Everyone in the field has known about this for decades, right?
That examples are not interpolative
in their original encoding space or their pixel space.
I think the most important thing though
is that this paper also shows
that the models are not interpolative
in their latent space either.
That's a shocker.
Now, what does Randall say about all of this?
Well, Randall Bellistrario thinks
that the steel man for this interpolation argument would be,
well, what if you had very low dimensional,
kind of like approximation of your data
with very few factors of variation,
which could be easily linearized in the latent space,
then it might be interpolative.
Randall thinks that the very concept of interpolation,
I mean, it was defined about 50 years ago
to describe these very small models
with very few factors of variation.
And it's just not relevant today, right?
They showed in their paper
that even in the relatively small latent space
on a popular neural network classifier,
interpolation does not occur.
He thinks that most people think of interpolation
by kind of conceptualizing their data
into a few important latent factors.
You might have dogs, for example,
and they might exhibit a latent color,
and a new color might be an interpolation
between observed colors.
But these guys think that we need to have
an entirely new definition of interpolation.
I think the main purpose behind their paper,
according to them, is to show that even though
the intuition that people have of interpolation
works well in low dimensions,
it falls down completely flat on its face in high dimensions.
Actually, the probability of your test data
being in the convex hull of your training data
is near zero past a certain number of dimensions.
So the current definition of interpolation,
which is to say like this notion of convex hole membership,
it's too rigid to think about
how interpolation works in neural networks.
So Randall thinks that any new definition of extrapolation
should be directly linked to generalization itself.
This is what they're trying to get at.
This is a clip from our show with Professor Max Welling.
The first thing I wanna say,
there is no machine learning without assumptions.
It just basically, you have to interpolate between the dots,
and to interpolate means that you have to make assumptions
on smoothness or something like that.
So the machine learning doesn't exist without assumptions.
I think that's very clear.
But clearly it's a dial, right?
So you can have, on the one end,
you can have problems with a huge amount of data.
It has to be available clearly.
And there you can dial down your inductive biases.
You can basically say that the data
do most of the work in some sense.
Now, I think it's a good thing
to build intuition about machine learning principally
as an interpolation problem, right?
So we're given the training data,
and we need to cleverly interpolate
between the training examples
to reason about the statistical context of the test examples.
This is what machine learning is in a nutshell.
Actually, a researcher friend of mine
from MetaAI Research in Silicon Valley,
a guy called Dr. Thomas Luchs,
he published a paper a couple of years ago
called Interpolation of Sparse High-Dimensional Data.
And he showed that it was indeed possible
to perform competitively with multi-layer perceptrons
for a regression problem using pure play interpolation methods
like Voronoi and Delonoi Triangleization
and Spline methods.
I mean, think of creating simplexes
of the nearest neighbors around a training data
and then averaging the results together.
This simple method works remarkably well
up to about 30 dimensions.
I mean, obviously, eventually it gets deranged
by the curse of dimensionality,
but as we'll discover slightly later,
neural networks might have a slight advantage
over pure play simplex interpolation
just because they work principally
by figuring out boundaries of the input space to exclude.
So, you know, if you're on the zero side of the relu,
that is.
So that helps a lot in high dimensions
when you have a poor sampling density
in a particular region of the input space.
But anyway, if you do principally think of machine learning
as being an interpolation method,
it raises some interesting ideas.
You know, Thomas said to me that the crux of the problem
is that we're trying to approximate a function
that has spatially unique behavior
in more than about 20 or so dimensions.
Like in that case, it's hopeless.
There's no question about it.
He says that you can increase the data density,
you know, like in 100 dimensions,
but to get a grid with 10 points on a side,
you know, which is to say 10 to the power of 100,
there aren't enough protons in the universe
that could convexly cover it.
So, Thomas concluded by saying,
what this means is that everything
that we do successfully approximate now
with millions or billions, or even trillions
of data points, which accounts for a lot
of the successes in machine learning,
these data sets only have spatially novel behavior
in very few dimensions, right?
Or a varying gradient in very few dimensions, 16 or fewer.
If we suppose that the true function
has a varying gradient in more than that many dimensions,
there's simply not enough data in the world to approximate it.
The mathematics here is unequivocal.
So, what actually happens when the test examples
are outside the convex hull of the training data?
What happens when we're in an extrapolative regime?
Well, it's increasingly impossible in high dimensions
for the training set to be statistically representative
of the test set.
Test set instances will often distinctly differ
from anything that we've seen during training.
In machine learning, the test set will never fully
characterize the problem they were interested in.
So, in high dimensional settings,
features are often linearly correlated as well,
which makes it challenging to know what information
to use and what to discard.
This is another fundamental problem of machine learning.
Many machine learning approaches depend on modeling
these local statistical relationships
between the training samples,
but in high dimensions, the probability of a new test
example being in the convex hull of the training data
goes to zero extremely quickly.
So, making machine learning models work
in an extrapolative regime necessitates the introduction
of inductive biases, which are a way of adding
a kind of enforced smoothness to the model predictions.
So, if you hit the right bias, then it can be beneficial.
If you impose the wrong bias, then it's gonna hurt you.
And this is a well-known trade-off.
So, of course, the whole endeavor of machine learning
is defining the right inductive biases
and leaving whatever you don't know to the data,
and then basically learning to focus your models
on the data that you're actually seeing.
This is how we stop models from fitting the noise
in the data or going completely haywire
outside the convex hull of the training data.
Extrapolation requires that sensible answers
are given for all of the elements of the input space,
even in regions not seen during training.
And in particular, those within the convex hull
of, let's say, some hypothetical infinitely
sample training data set.
Now, what we do in neural networks
and in many other machine learning algorithms,
for that matter, is we just take a bunch
of parameterized basis functions and we stack them
and we fit them to our training data
until it's well-described.
The most important considerations for interpolation
are the smoothness and robustness of these models, right?
We want our models to produce gradual changes
between the training points.
Points outside the training set
should be handled with great care,
but what exactly that means depends greatly on the problem.
In order to even know that we're in an extrapolative regime,
the basis functions must be based
near to the training examples.
Typically, neural networks do not behave sensibly
in regions where there's no training examples, right?
Because their learned basis functions
have been localized around the training data
in the ambient space.
It's also worth noting that extrapolation
and interpolation are two completely different regimes.
Optimizing for one typically means being worse at the other.
Now, Randall said that the boundary view
of neural networks is very clear
in the discriminative setting.
You only need interpolation
to try and understand how neural networks work
in the generative setting.
This is a key point.
In the generative setting, when you interpolate the latent,
we see at first glance what looks like geometric morphic, right?
Looking at this, you'd be forgiven for thinking
that we're traversing some smooth latent manifold,
but on closer inspection,
it's not geometric or diffeomorphic at all.
It's more of a fuzzy, high-resolution fade-in.
Now, one commonly used visual argument
for the manifold hypothesis
is the MNIST digit interpolation in a generative model.
Our own investigation shows that there's a degree of cherry picking
in some of the visual examples used to demonstrate this.
It's not actually hard to stumble across cases
where purported manifold interpolation is no better
than ambient space linear interpolation.
There are many examples where latent space interpolation
gives super-posed and fuzzy intermediate representations
and even crisp images, which are unrecognizable.
The blur and cutting and gluing,
which is apparent on these images,
show that it's definitely not a diffeomorphism
in the ambient space.
And even though the interpolation path
is continuous in the latent space,
it's questionable whether that path is semantically relevant
to the classification task it had.
I mean, just think of the mean value theorem.
It tells you that if you take the value
of any continuous function at two points, right,
you can also find a point between them
where the function hits the average of those values.
So in other words,
any continuous function produces a manifold,
but that doesn't tell you anything interesting
about the function beyond what we already know, right?
That it's continuous.
Now, I'm not so sure that these manifolds are even smooth.
When we're talking about smoothness,
we're only talking about local smoothness
and inside the polyhedrus convex hole.
It's linear, right?
But there's no smooth surfaces or manifolds
anywhere to be seen.
For any particular node in a neural network,
it's actually a linear function of a subset
of the upstream linear functions, right?
Very much like a decision tree,
but with information sharing.
Now, another often used visual aid is this idea
that a neural network is effectively uncrumpling
and smoothing the ambient space,
much like you would do with a sheet of paper
with successive transformations
in the layers of the neural network.
Keith and I previously agreed with this view of neural networks
as progressively kind of flattening out the paper,
but it now seems to us like they may instead
be progressively inserting planes
aligned to the facets of the paper ball
to chop out locally affine polyhedra or cells
to cover the paper's polyhedra.
So for us, this is an entirely new way
to think about neural networks, okay?
So in a way, this is a new parlor trick, right?
Do you remember that thing, the game of life,
Conway's game of life?
It looks like the shapes are smoothly morphing,
but they're actually toggling pixels with discrete rules.
Gary Marcus talks about the parlor trick of intelligence,
but isn't it ironic that there are more parlor tricks
going on than most people realize?
Namely, that rather than doing smooth geometric morphing
via interpolation, these networks are actually chopping up
and composing linear polyhedra.
Now, if you interpolate between two latent classes,
it might traverse several polyhedra in the intermediate space.
Along the way, it would pick up characteristics
from all of those polyhedra.
Some of the black regions are impossible regions,
so it's not possible to get there from the latent space,
which is very, very interesting.
You only see the illusion of continuous morphing,
where the neighboring cells are very small and very similar.
This is Professor Michael Braunstein.
I like to think of geometric deep learning
as not a single method or architecture, but as a mindset.
It's a way of looking at machine learning problems
from the first principles of symmetry and invariance.
Symmetry is a key idea that underpins our physical world
and the data that is created by physical processes.
And accounting for this structure
allows us to beat the course of dimensionality
in machine learning problems.
So what is the manifold hypothesis?
Well, natural data falls on smooth manifolds.
The manifold hypothesis states that real-world
high-dimensional data lie on low-dimensional manifolds,
embedded in the high-dimensional space.
When people invoke this hypothesis
in a machine learning context,
they're generally suggesting that neural networks
are actually learning this data manifold,
which we now find quite hard to believe, frankly.
At best, we think they're learning
some approximate aspects of it.
So essentially, all machine learning problems
that we need to deal with nowadays
are extremely highly dimensional.
Even basic image problems live in thousands
or even millions of dimensions.
Now, I think most people have this intuition
of convex hole membership,
which is to say in two dimensions,
as you sample more and more training data,
the convex hole eventually fills the entire space.
But the kicker is that in higher dimensions,
the space is so vast, this will never happen.
High-dimensional learning is impossible
due to the curse of dimensionality.
It only works if we make some very strong assumptions
about the regularities in the space of functions
that we need to search through.
The classical assumptions that we make
in machine learning are no longer appropriate.
So in general, learning in high dimensions is intractable.
The number of samples grows exponentially
with a number of dimensions.
And the curse of dimensionality refers
to the various phenomena that arise
when analyzing and organizing data
in high-dimensional spaces that do not occur
in low-dimensional settings,
such as the three-dimensional physical reality
of everyday experience.
Now, the common theme of these problems
is that when the dimensionality increases,
the volume of the space increases so fast
that the available data becomes sparse.
And this sparsity is problematic for any method
that requires statistical significance.
In order to obtain a statistically sound
and reliable result, the amount of data needed
to support the result grows exponentially
with the dimensionality.
So the curse of dimensionality in a nutshell
is that the probability of a new data point
being inside the convex hull of your training data
decreases exponentially with the number of dimensions.
As an example, to estimate a standard normal density
in 10 dimensions with a relative mean square error
of 0.1 using an efficient non-parametric technique
would require more than 800,000 samples.
Now, in the last show, we discussed geometric deep learning
in great detail and some, you know, in some sense,
extrapolation and the curse of dimensionality are analogous.
The reason that these folks wanted to combat
the curse of dimensionality was they wanted to build models
which could extrapolate outside the training range
using geometrical priors.
I think it's possible to build understanding
of the curse of dimensionality through visual analogy
to familiar lower dimensional shapes
such as circles, squares, balls and cubes.
Imagine perfectly sampling an entire space
with a regular grid.
This would partition space into squares or cubes
or hypercubes with a sample in the center of each.
Now imagine around each sample a disc or ball or hyperball
representing that point's region of nearness or influence
and let's ask how much of the total volume
of that point's grid cell is actually near the sample?
The answer is a fraction which diminishes faster
than exponentially with increasing dimension.
First think of the 2D case, a disc or two ball
with diameter one inscribed in a square or two cube
with sides of length one.
In each corner, we of course have a dart shaped chunk
that isn't covered by the disc.
A trick to think about this,
which I think will help in higher dimensions
is to imagine scanning a line segment
along one dimension from side to side.
At the edge of the square,
none of the segment is covered by the disc.
At the very center, the entire segment is covered
by the disc and then as it scans
towards the other edge of the square,
the covered section begins shrinking
and then rapidly falls to zero.
The total coverage then is just the sum over that scan.
In two dimensions, that is just the area of a disc
with diameter one and is about 79%.
Extending to three dimensions, imagine a ball in a cube
and we scan a square from one face
through the ball to the opposite face.
As it passes the center,
we have our familiar inscribed disc in a square.
This is already missing the corner darts
and as we scan towards the edge of the cube,
the darts expand and surround the disc
as it shrinks to zero.
So by adding a third dimension,
we've lost even more coverage
and the volume of a diameter one ball is only 52%.
In fact, if we continue on the higher dimension,
the volume of a diameter one hyperball
decays faster than exponential,
factorially fast in fact.
The volume decays so fast
that even if we allowed for sampling
with the best possible densely packed balls,
theoretical work on hypersphere packing
tells us that the volume occupied
by optimally packed hyperballs
would still decay at least exponentially
with increasing dimensions.
This is the curse.
As dimensionality grows,
space expands exponentially,
points grow further apart
and the volume near each point vanishes.
In this episode, our guests argue this curse
dunes traditional concepts of interpolation,
even if we allow for the high dimensional
transformative power of deep neural networks.
Yeah, so the curse of dimensionality,
it refers generally to the inability
of algorithms to keep certifying certain performance
as the data becomes more complex
and data becoming more complex here
means that you have more and more dimensions,
more and more pixels.
And so this inability of like scaling,
basically it's like it really says
that if I scale up the input,
my algorithm is gonna have more and more trouble
to keep the pace.
And so this curse can take different flavors, right?
So this curse might have like a statistical reason
in the sense that as I make my input space bigger,
there would be many, many, many,
much exponentially more functions,
real functions out there
that would explain the training set,
that would basically pass through the tending points
and so the more dimensions I add,
the more uncertainty I have about the true function, right?
So I would need more and more training samples
to keep the pace.
These, the curse can also be from the approximation side,
right?
So in the sense that my,
the number of neurons that I'm considering
to approximate my target function,
I've I need to keep adding more and more neurons
at the rate that is exponential in dimension.
And the curse can also be from the computational side, right?
In the sense that if I keep adding parameters
and parameters to my training model,
I might have to optimize to solve an optimization problem
that becomes exponentially harder.
And so you can see that you are basically bombarded
by three different, by all angles.
And so an algorithm like here in the context
of statistical learning or learning theory, if you want,
having a kind of a theorem that would say, yes,
I can promise you that you can learn,
you need to actually solve these three problems at once, right?
You need to be able to say that,
that in their conditions that you're studying,
you have an algorithm that it does not suffer
from approximation or statistical
nor computational curses.
So as you can imagine, it's very hard, right?
Today is a big day here at Machine Learning Street Talk.
We have invited one of the Godfathers
of deep learning on the show,
none other than Professor Yan Lecun.
Machine Learning royalty, he is Meta Chiefs AI scientist
and a Turing Award winner,
which I don't have to tell you is a big deal.
Widely recognized as the Nobel Prize of Computing.
Yan Lecun was born in the suburbs of Gaye Paris in the 1960s.
He received his PhD in computer science
from the modern day Sir Vaughan University in 1987,
during which he proposed an early form of back propagation,
which is of course the backbone
for training all neural networks.
Whilst he was at Bell Labs,
he invented convolutional neural networks,
which again are the backbone
of most major deep learning architectures in production today.
He has been at New York University since 2003,
where he is Silver Professor of Computer Science
and Neural Science.
Apart from being perhaps the most known researcher
and main ambassador for deep learning,
he has championed self-supervised learning
and energy-based models.
In 2013, he created the hugely prestigious ICLR conference
with Yoshua Benjiro.
A lover of the green screen IC.
Well, normally, when I use Zoom,
I put a substituted background.
Well, Tim could do it for you in post, if you want.
If you want.
Well, it's never a very good cut out.
I did hack together some Python code to do it,
but there's no substitute for the real thing.
Yeah, I attempted to do this.
I'm running Linux, so I attempted to do this
by making a fake video driver,
but it introduces a little bit of delay, so.
Professor Lacoon, it's an absolute honour
to have you on the show.
When I first started MLST with Yannick and Keith and Connor,
we were discussing how long it might take
to finally get the main man himself on the show.
You've been a huge inspiration to all of us here on MLST
and also you've inspired millions around the world
to embark on successful careers in data science
and to dream about what might be possible
with artificial intelligence.
Now, we've read your recent paper,
Learning in High Dimensions Always Amounts to Extrapolation,
which you co-authored with Randall,
Belastriro and Jerome Pesente.
Let's get straight into it.
So, we were wondering,
why did you write this paper, basically?
We were thinking whether interpolation and extrapolation
is a useful dichotomy.
I mean, at the end of the day,
we measure performance of learning methods
with accepted metrics of predictive performance,
such as accuracy.
So, suppose everyone adopts your linear convention,
which is a convex whole membership,
and concludes that high-dimensional learning
is always extrapolation.
How is that useful?
What do we do with this knowledge?
And vice versa,
suppose that everyone adopts a different definition
and concludes that learning is always interpolation.
What difference would that make
in machine learning research and practice?
So, in sum, why is it important to distinguish
whether we're predicting by interpolation or extrapolation?
Okay, very interesting question.
So, first of all, this may be the first time
you have me as an interviewee,
but I've watched a bunch of your videos,
so you've had me as an audience, at least.
Which I find really interesting, actually.
So, the answer to your question is,
the whole point of the paper is to show
that this notion of interpolation versus extrapolation
is not useful, essentially.
That, you know, there's been a lot of people
who've been sort of saying there's a limitation
to deep learning, let's say,
or machine learning more generally,
because it's obvious that those things
basically do curve fitting,
and that only works for interpolation
and not for extrapolation.
And that kind of dismissal always sounded wrong to me
because in high-dimension,
geometry in high-dimension is very different
from the intuition that we form
with curve fitting and low-dimension, right?
So, part of the motivation for this paper
is it was to kind of, you know,
perhaps help some people gain some intuition
about what really is taking place
in machine learning and high-dimension,
and also kind of, you know, dispel the myth,
essentially, that machine learning
and deep learning in particular only does interpolation.
Of course, it depends a lot on your definition
of interpolation.
So, here we adopted definition,
which is an obvious and simple generalization
of interpolation in low-dimension,
which is that, you know,
you interpolate when a point is in between the points
you already know, and the generalization of this
in high-dimension is, you know,
you interpolate when a new point is inside the convex hull
of the points that you already know.
Now, what you have to realize, of course,
is that in high-dimension,
the volume of that space is actually tiny
compared to the overall volume of space
that would be filled, you know, in high-dimension.
And so, that's kind of the intuition behind this paper
that, you know, any new point, regardless of how you sample it
to some extent, any kind of reasonable ways
to sample points, you know, in high-dimension,
new points are always going to be outside the convex hull,
almost always going to be outside the convex hull
of existing points.
With that definition.
Interesting.
Now, there's a second part to your question.
It's that, you know, is there a more sensible definition
of interpolation and extrapolation?
And the answer is probably yes.
You know, the paper doesn't address it,
but there are other definitions of hulls,
if you want, they're not necessarily convex hulls
or they're not necessarily the usual type of convex hull.
So, for example, there is a definition of a hull
for a cloud of points that would be the smallest
hyperboloid or ellipsoid, I should say, actually.
The smallest ellipsoid that contains all the points, right?
So, some points are going to be on the surface of the ellipsoid,
but most of them are going to be inside.
And for this type of, and now your definition of interpolation
is that, is a new point likely to be inside the ellipsoid
of a previous point or outside?
And the answer to that is probably very different
from the one in the paper in the sense that
it's very likely for a lot of natural data,
new points are likely to be inside the containing ellipsoid.
So, it very much depends on what you mean,
but it's just that the notion of interpolation
in high-dimensional space, you know, or intuition
are kind of biased toward low dimension,
and we have to be very careful what we say.
So, that was the main thing.
And, you know, we have a bunch of, you know,
tons of mathematicians that worked on these questions
for many years, and there's a whole bunch of theorems
about this that we survey in the paper.
Very interesting.
Well, we'll dig more into that in a second,
but folks like Gary Marcus make the case
that deep learning models don't reason and only interpolate.
Now, I know you disagree vehemently,
but my intuition is that reasoning and extrapolation
are somewhat synonymous.
Is it, you know, by arguing that deep learning models extrapolate,
are you kind of making the argument that they reason as well,
because that would make a strong case
that deep learning models could scale
to artificial general intelligence?
Okay, so no, the short answer is no.
But there are important questions in there.
First of all, you know, what's our definition of reasoning?
What is the process by which we elaborate models?
And is there a qualitative difference between
a models that merely performs curve-feeding,
as we normally know it, and a model that has a,
let's say, to adopt a terminology that others have proposed,
that models that establish sort of a causal model
of the data you're observing,
which can be the basis for reasoning and things like that, right?
And the answer to this is probably no.
There is a difference, of course,
but is it an essential qualitative difference?
I'm not entirely sure.
And then there is the argument,
if there is a qualitative difference,
which I'm not sure about,
would this qualitative difference be in the form of
fundamentally different things from deep learning,
things that are like discrete symbolic reasoning
or things of that type?
And to that, my answer is clearly no.
I do not believe that's the case.
So I think reasoning is certainly,
I always list in my talks,
giving the ability to learning machines to reason
is one of the main challenges of the next decade,
perhaps a couple of decades in AI.
So I'm clearly aware of the fact
that they don't do this very well at the moment.
The big question I think is,
how do we get machines to reason
in ways that are compatible with deep learning?
Because most of the criticism
that I've heard from Gary Marcus and several others
towards deep learning is not a criticism towards deep learning.
It's a criticism towards supervised learning.
And I agree with that.
Supervised learning sucks.
I mean, it's very limited in the sense that
you can train machines to do very specific tasks.
And because they're trying to do very specific tasks,
they're going to use all the biases that are in the data
to do that task.
And if you try to get outside of that task,
they're not going to perform very well.
That's the limitation of supervised learning.
It has absolutely nothing to do with deep learning.
So regardless of which learning techniques you're going to use,
you're going to have that problem.
It's a problem with supervised learning.
So I take exception with the confusion
between deep learning and supervised learning.
Now, of course, today, most of supervised learning is deep learning,
but it's the limitation of supervised learning.
And as you probably know,
I've been a very strong advocate of self-supervised learning,
sort of moving away from task-specific supervised learning
towards more kind of generic learning
followed by specialization using supervised and reinforcement learning.
And in that, I've kind of followed the path of Jeff Hinton,
who's been basically advocating for this for over 40 years now.
And for me, it's less time.
I disagreed with him originally and changed my mind about 20 years ago.
So that's the story, really.
It's awesome that you have the ability to change your mind
and admit that this is a good thing.
We run into so many people that don't want to change their mind no matter what.
It's seen as a bad thing, which it isn't at all.
No, I think the essence of being a scientist
is to be able to change your mind in the face of evidence.
You cannot be a scientist if you have preconceived ideas.
On the other hand, I've also been known to hold very tight to ideas
that I thought were true in the face of considerable differing opinion from my dear colleague.
So it also helps to have DP-held convictions sometimes.
So to this notion of interpolation,
I think we've also, and you mentioned this, right?
It depends on your definition.
And we've also had a little bit of the feeling
that people might be talking past one another
when they criticize, can it interpolate?
Can't it interpolate?
And we've come up with this example of I give a classifier a dog.
And that dog is like the most doggy dog I've ever seen.
Like it's like such a dog.
It's more dog than any dog I had in the training dataset.
So clearly that dog is outside of the training distribution,
outside of the convex hull in any space,
like be that the original space, be that the latent space.
It's like, all that matters is that it's at the correct side of the classifying hyperplane.
So that would not be contained on sort of in the convex hull.
It would not be contained in the smallest ellipsoid and so on.
So do you think when people talk about interpolation,
they might be talking about something maybe different?
Because you can also make the example,
if I take the convex hull of all the training data points
and I take a point in the middle of it,
the neural network going to be pretty bad at it.
That's going to be like a messy blur of pixels.
And it's not going to be very, very good at it.
So could you maybe make the strongest argument you could
for people saying neural networks just interpolate?
But what notion of interpolation would you substitute?
Would it be like, oh, they just nearest neighbor classify?
Or if you had to give the best shot at doing the,
oh, neural networks just do something, what would it be?
I wouldn't give any answer of this type,
because the answer would very heavily depend on the architecture.
So for example, if most of the layers in a neural net,
I mean, you can use weighted sums and sigmoids
or weighted sums and values.
And that effectively performs a classification,
separating the input into two halves with the hyperplanes.
But you could also use a Euclidean distance unit.
So Euclidean distance unit computes the distance
of the input vector to a weight vector,
which is not a weight vector anymore,
and then passes it through some sort of decreasing function
like an exponential.
And what that gives you is a Gaussian bump in input space
where the activity will be high
if the input is close to the vector and low if it's not.
When you have an attention layer,
we have a whole bunch of those kind of vector comparison
where the vectors are normalized.
And then you do a softmax.
You're basically doing sort of a multinomial version of this.
And this is using transformers
and in all kinds of architectures these days.
So those things that compare vectors with each other
and only react to the two vectors nearby
do a sort of glorified nearest neighbor
with some interpolation.
By the way, most kernel-based methods do this.
Kernel methods basically are one layer of such
PRS comparisons.
I mean comparisons of the input with the training samples.
And then you pass that through some function,
some response function.
So Gaussian SVMs, for example,
are the perfect example of this.
And then you take those cores
and you compute a linear combination.
That's glorified interpolating nearest neighbor.
To some extent, transformers do this as well.
Transformers are the basic module of a transformer
is basically an associative memory
that compares the incoming vector to a bunch of keys
and then gives you an answer
that is some linear combination of values.
So that is a good idea.
It learns fast.
There used to be a whole series of models
that were now forgotten,
that are now forgotten in the 90s called RBF networks.
So an RBF network was basically a two-layer neural net.
Well, the first layer was very much like an SVM.
This was before SVM.
And that, again, had a real business functions responses,
right?
Comparing an input to vectors
and passing it to an exponential or something like this.
And then you would initialize the first layer.
You could train it with backprop,
but it would get stuck in local minima.
So that wasn't a good idea.
You had to initialize the first layer
with something like k-means or mixer or Gaussian
or something like that.
And then you could either just train the second layer
or fine tune the entire thing with backprop.
And that worked pretty well.
It was actually a fairly fast learner.
They were kind of faster than the neural nets.
So those things, for those things,
the answer to your question is one type.
They're basically doing interpolation with kernels.
And it's very much like a smooth version of nearest neighbors.
But then for classical neural nets,
where you have either a happy ability to enter
nonlinearity or a value or something of that type,
something with a kink in it.
Or two kinks, the answer is different.
There it's a whole cone of response
that will produce a positive response versus not
if you take a combination of units.
So does it make sense to talk about interpolation
in that kind of geometry?
I'm not sure.
I think people should just not use the word interpolation
for that situation.
But the fact that things in a neural net,
the response of a neural net actually
are kind of half space extend beyond the training points
perhaps has something to do with the fact
that they do extrapolate in certain ways.
That may or may not be relevant for the problem,
but they do extrapolate.
Well, so we've talked some about the interpolation
versus extrapolation versus what was in the paper
because the paper is somewhat a rigid definition.
It's like there's this convex hull and if you're inside,
it's interpolation.
If you're outside, it's extrapolation.
And you talked about maybe we could use a ball
and ellipsoid instead.
But there's kind of a key thing there,
which is that it's going across all dimensions.
So you're inside the convex hull.
It's a necessary condition that on every single dimension,
your sample data point falls within the range
of the training data.
We could kind of go the opposite extreme
and say that you're interpolating if any dimension
falls within the training domain,
or rather you're extrapolating only if every single dimension
falls outside the training range.
And both of those are kind of these exponential extremes.
And it would seem like the truth is maybe somewhere in between,
like there's a subset of dimensions
that might be salient for any particular data point.
So why are we kind of using this very exponential,
extreme definition?
It was still exponential, even if you can try to divide
the set of dimensions into the ones that are useful
and the ones that are just kind of nuisance parameters
that are useless, but that are not relevant
for the task at hand.
But first of all, that task is very difficult.
I mean, basically the entire machine learning problem
is exactly that, trying to figure out what information
in the input is relevant for the task
and what part should basically be considered
as noise or nuisance parameters.
So solving that problem is solving the machine learning problem,
first of all.
Second of all, what we show in the paper is that regardless,
so what we did, the experiment that Randall did,
as a matter of fact, is train a ResNet 18 or 50 or whatever
on a ResNet or CIFAR or MNIST.
And then take the output representation
and see whether this sort of exponential growth of a number
of data points to stay within the interpolation regime
still exists and it still exists.
It's just that the dimension now, instead of being the input
dimension of the data, which may be very large,
is the dimension of the embedding.
But as soon as the dimension of the embedding is larger than 20
or so, the number of training samples you would need
to stay within the interpolation regime
is already going to be very large.
2 to the 20 is a large number.
So there's another experiment that shows that
if your entire dataset is contained within a linear subspace,
so the ambient space may be of dimension 100,
but the entire dataset is within a linear subspace
of dimension 4,
then what matters is the dimension 4, not the dimension 100.
So automatically that convects all process.
What matters to it is not the dimension of the input space,
it's the dimension of the linear subspace
that contains all the points.
Right, but I think what I'm saying though
is that you can invert the definition
so that whether or not you're extrapolating
just becomes 1 minus, whether or not you're interpolating
or vice versa.
And so you can wind up with an equally extreme definition
that concludes everything is interpolation.
So I'm just wondering, it seems like there should be a balance
and this kind of gets to what you said earlier.
Is there a more useful definition of interpolation
versus extrapolation that could have some utility
for machine learning?
It's not sure you would get much out of it.
I give you a potential candidate
which is whether the points are contained in a ellipsoid
that contains all the points.
You could make it a sphere.
A sphere would be of course bigger volume
because the data doesn't necessarily
have the same radius in all dimensions.
But I think the result would be fairly similar for both,
for both definitions that in that case
things would be mostly interpolation.
But that would be kind of a weird definition of interpolation
in the sense that it would rely on a false intuition
about high dimensional geometry.
Okay, calling this interpolation
basically would mean that you have the completely wrong idea
about how things behave in high dimension,
about geometry in high dimension.
I'm trying to get some interest in about this
because we spoke to Professor Bronstein
and his friends including Joanne Bruner
and they're talking about geometric learning
and their approach to defeating the curse of dimensionality
is finding geometric priors to reduce the hypothesis space
which is quite interesting.
But a lot of this is about our intuitions of interpolation
because if I take an autoencoder and I train it on MNIST
and I start interpolating between the train
or the test examples,
it is learning this continuous geometric morphing
and that is what people's intuition is about interpolation.
I know you would say that's extrapolation.
Yeah, I think it would be a type of interpolation
but it would be some sort of geodesic interpolation,
interpolation on a manifold.
I mean, certainly if you have some idea
about the structure of the data manifold,
you can interpolate within that manifold
without making big mistakes.
But then you're back to the original problem of machine learning.
What is that manifold? How do you learn that manifold?
I mean, that's one of the essential problems
of machine learning
and learning the structure of data manifold
is a much more complex problem
than learning a task of classifying objects on that manifold.
For example, classifying points on that manifold.
So there is this old adage by,
which I used to be a big fan of
and which is why I was disagreeing with Jeff Hinton
about the usefulness of unsupervised learning.
Because if you have a task at hand,
for which you have data that you can use to train a supervised system,
why would you go to the trouble of pre-training a system
in unsupervised mode,
knowing that the unsupervised learning problem
is considerably more complicated
both from every aspect you can think of,
certainly from the theoretical point of view.
Vladimir Vapnik actually has kind of a similar opinion,
one of the few things that he and I agree on,
or agreed on at least,
which is why would you want to solve a more complex problem
than you have to?
But of course, that forgets the fact that
you don't want to solve a single problem.
You want to take advantage of multiple problems
and whatever data you have available
at your disposal
to prepare for learning a task.
We have access to considerably more unlabeled data
than we have access to labeled data.
And therefore, why not use unlabeled data
in large quantity to pre-train very large neural nets
so that we can function them for the tasks that we are interested in?
So that's the whole idea of self-supervised learning.
And of course, we all know that that kind of strategy
has been unbelievably successful
in natural language processing
with denoising autoencoder or master autoencoder
or both style training of transformers
followed by a supervised phase.
That success has not yet translated in the domain of vision,
although I'm sort of predicting that it will happen very soon.
But I mean, there's a lot of interesting avenues there
and recent progress.
And then there is the cake analogy, right?
The fact that any sample in sort of a self-supervised contact
give you way more information than a supervised sample,
the label from supervised learning,
a fortiori reinforcement for the context of reinforcement.
Absolutely.
And we interviewed Ishan.
So we've done a show on self-supervised learning.
We're huge fans of self-supervised learning.
And I know your vision is to get to these latent predictive models
to solve that problem.
That's something else I changed my mind on the last two years.
This may be outside the scope of this particular interview.
But yeah, there's basically two major architectures
for self-supervised learning, particularly in the context of vision.
And the main characteristic of both of them
is the fact that you can handle multimodal prediction.
So if you have a system, let's say you want to do video prediction
or something like that.
So you have a piece of a video clip.
You want to predict the next video clip.
Or you just want a machine that tells you whether a proposed video clip
continuation clip is a good continuation of the previous one.
You don't want it to predict.
You just want it to tell you whether it's a good one.
So you have two architectures.
The first one is a latent variable predictive architecture
that predicts the next video clip.
And of course, you have to parameterize the prediction
with a latent variable because there are multiple predictions
that are plausible.
So I used to be a big fan of that.
And about two years ago, I changed my mind.
Maybe a year and a half.
The other approach is something I played with in the early 90s,
came up with some of the early models for this.
And it's called a joint embedding architecture
where you have the first and the second video clip
both going through an neural net.
And then what you're doing is you're training the system
so that the representation of the second video clip
is easily predictable from the representation of the first one.
So there you're not predicting pixels.
You're predicting representations.
But you still have a system that can tell you,
here is a video clip and the second one,
tell me if they are compatible,
if one is a good continuation of the other.
The main reason why I stayed away from those architectures,
because I knew that you had to use, in the past,
you had to use contrastive learning.
You had to have pairs of things that are compatible
as well as pairs of things that are incompatible.
And in how you mentioned,
there's just too many ways two things can be incompatible.
And so that was going to do to failure.
And I played with this back in,
this used to be called Siamese Networks at a paper in 1992-1993
on doing signature verification using those techniques.
Jeff Hinton had a slightly different method
based on maximizing mutual information
with his former student, Sue Becker.
But then in the last two years,
we've had methods that are non-contrastive
that allows us to train those joint embedding architectures.
So I've become a big fan of them now.
And it's because of algorithms like BYUL
from our friends at DeepMind, like Barlow Twins,
whose idea came from Stephane Denis,
who's doing a postdoc with me at FAIR.
He's now a professor at University of Aalto in Finland.
Then more recently, Vic Craig,
which is a kind of an improvement, if you want,
on Barlow Twins that is also based on the same idea
that Jeff Hinton and Sue Becker had
of maximizing a measure of mutual information
between the outputs of the two networks,
but in a slightly different way.
So I'm really excited about those things.
And again, I change my mind all the time
whenever a good idea seems to be overcome by a better one.
So in this whole space of maybe interpolation,
extrapolation, but also data manifolds and so on,
what's your view on things like data augmentation,
training and simulation, and domain adaptation and so on?
Because it could be argued that these things
kind of increase the convex hull of the training data.
They sort of make the distribution broader.
Or is that just also out of question?
Not much.
I think data augmentation does not increase
the volume of the data punk cloud, if you want, very much.
Because those augmentations are generally fairly local
in dimensions that are already explored.
It may increase a little bit, but not significantly, I think.
I mean, obviously data augmentation is very useful.
I mean, we've used this for decades,
so it's not a new phenomenon either.
There's a lot of ideas along those lines.
So it's the idea where you give two examples
and you have two examples in your training set,
and you actually do an interpolation in input space
to generate a fake example that's in between the two,
and you try to produce the intermediate target
between the two original examples.
Mix up.
There is that.
There is distillation.
There is various techniques like this are basically
implicitly ways to fill in the space between samples
with other fake samples or virtual samples, if you want.
There was a paper by Patrick Simard and me and John Lanker
many years ago when we were all at Bell Labs
on something called tangent prop.
So the idea of tangent prop was kind of somewhat similar.
The idea was you take a training sample
and you're going to be able to distort that training
sample in several ways.
You could generate points by data augmentation,
but the other thing you can do is just figure out the plane
in which those augmentations live.
And that plane is going to be a tangent plane
to the data manifold.
What you want is your, for a given class, for example, right?
So what you want is your input-output function
that the neural net learns to be invariant
to those little distortions.
And you can do this by just augmenting the data,
or you can do this by explicitly having a regularization term
that says the overall derivative of the function
in the direction of the, you know,
spanning vectors of that plane should be zero,
or should be whatever it is that you want it to be.
But zero is a good target for this.
So that's called tangent prop.
And you can think of it as a regularizer that says,
you know, I don't just want my input-output function
to have this value at this point.
I also want this derivative to be zero
when I change the input in those directions.
That's another indirect way of doing data augmentation
without doing data augmentation, essentially.
And, you know, I think there's a lot of those ideas
that are very useful.
Certainly.
So the, I think there seems to be like a spectrum,
you know, coming back to this interpolation, extrapolation.
You said yourself you don't believe neural networks
can do something like discrete,
abstractive reasoning, and so on.
No, no, no, I didn't say that.
Or, yeah, or horrible.
I said, you know, we still, we need to do work
for them to be able to do that,
but I have no doubt that eventually they will.
I mean, there's a lot of work in this area already.
And I'll give you some more specific examples if you want.
I mean, that was, that was, that was actually kind of the nature of my question.
Where do you, where do you think, you know,
there's this spectrum of what people think neural networks are doing now
or will be able to do later?
Where do you think the actual biggest disagreement
between sort of the community right now lies?
And what can we do to, like, what experiments,
what evidence can we gather to solve these disagreements?
Right, so I think, I think we shouldn't talk about too much about neural networks,
because people have kind of a really relatively narrow picture of what a neural network is, right?
It's, you know, a bunch of layers of neurons that perform, you know,
weighted sums and pass a result through a nonlinear function,
and there's a fixed number of layers, you know,
maybe they can be recurrent, but, and you produce, you produce an output.
That's a very restricted view of what deep learning systems can do.
So let me take, let me take an example of what reasoning might mean.
Reasoning might be seen, at least one particular type of reasoning,
might be seen as a minimization of an energy function,
not with respect to parameters in a neural net,
but with respect to latent variables.
And a lot of systems that are in use today do that.
So a lot of speech recognition systems, for example,
have a decoder on the output,
which, you know, essentially given a list of scores for, you know,
what a particular segment of speech could be in terms of what sound it could be,
or what syllable or whatever.
The decoder basically figures out what is the best interpretation
of that sequence of sound that makes it a legal sentence in the dictionary.
Techniques like this were used, you know,
have been used for, you know, 25 years now in the context of speech recognition
and handwriting recognition, even before neural nets were used for those things.
So, you know, back in the old days of Hidden Markov models.
And, and, and those techniques have been, you know, really developed.
Now, if you think about what those techniques do, first of all,
all the operations that are done in those systems are differentiable.
You can backpropagate gradients through the system,
you can backpropagate gradients through an operation that will figure out the shortest path
in a, you know, a graph using dynamic programming.
The first paper on this was in 1991 by my friend Leon Boutou and Xavier de Leoncourt.
This is not recent stuff that we're trying to do speech recognition.
You can backpropagate gradients through a lot of different modules.
What those things do is that they infer a latent variable
by basically doing energy minimization.
Finding the shortest path in a graph is a form of energy minimization.
And so you can have, in a deep learning system, you can have a module that has a latent variable.
And what this module will do is figure out a value of the latent variable
that minimizes some energy function.
It could be a prediction error, it could be something else,
it could be, you know, there could be regularizers in it, et cetera.
But it basically performs a minimization.
And that type of operation basically can be used to,
like, you can formulate most types of reasoning in that form.
Now, it's not necessarily in a continuous differentiable space,
but almost all reasoning, even in classical AI, can be formulated
in terms of optimizing some sort of function.
Like, you won't do a SAT problem, right?
You won't do a collection of Boolean formula.
You want to find a combination of variables that satisfy this formula.
It's an optimization problem.
It's combinatorial optimization, but it's an optimization problem.
You want to do planning.
So planning is the best example.
So you can do a planning where the thing you're controlling
as discrete actions and discrete states, and you have to use dynamic programming.
You can back propagate gradient through a system that does that.
But more classical planning is in continuous space.
So planning for, like, planning the trajectory of an arm for a robot,
planning the trajectory of a rocket.
What you have is a differentiable model, dynamical model,
of what is the state of the system you're trying to control at time t plus 1,
as a function of the state at time t, and as a function of the action you take.
And perhaps as a function of some random variable,
you can't measure, you know, from the environment or something like that
to make it non-deterministic.
Now you can enroll that model.
So start with, you know, time equals zero,
and then make a hypothesis about a sequence of action,
and then apply your predictive model of the next state of the system.
And then at the end, you can measure some cost function.
Is my rocket, you know, docking with a space station or it's far away?
And how much fuel have I consumed?
Or, you know, whatever, right?
Or have I, you know, reached the goal of the arm avoiding all the obstacles, right?
So what you can do now is an inference process
which consists in figuring out what is the sequence of action I should take
to minimize this cost function according to my dynamical model.
And you can do this by gradient descent.
And basically that's what the Kitty-Bison algorithm, you know,
in optimal control that goes back to 1962,
and it consists in basically doing back prop through time.
It's as simple as that, right?
You can do back prop through time.
So in effect, optimal control theories invented back prop back to the early 60s,
but nobody realized you could use this for machine learning
until the mid-80s, essentially.
But just the 70s.
Just to jump in there, because there's a little caveat there,
which is that you can do that at back prop if you have a fixed number of steps.
Like what back prop can't handle is the case where I want some expandable number of steps.
We have no algorithms that can currently optimize,
for example, neural networks with a variable number of layers.
Like we just can't train those, right?
It's not true. That's what recurrent nets are.
You can have a varying number of iterations in your recurrent net.
And what I'm describing here is an unfolded recurrent net.
It's the same model that you apply it every time step, right?
So it's very much like a recurrent net.
And you can very well have an unknown number of steps.
You don't know a priori how long it's going to take for your rocket to get to the space station, right?
So you may have kind of a large potential number of steps.
And you can have a cost function which will count like how much time it's going to take to get there.
And this will be part of the optimization, for example.
And this is classic optimal control.
I'm not telling you anything that I came up with.
This is from at least the 60s and 70s.
No, but the back prop, that variable number of layers has to be done in kind of a classic iterative,
you know, try out the variable number of layers and see what the back prop comes up with.
Or you start with a very large number of layers and then let back prop try and find.
But still at the same, what I'm saying is there is fundamentally two different kinds of computation that are at play here.
One is like the finite fixed thing and then you do some differentiable optimization on it.
Another kind of computation is this discrete symbolic that has like an expandable memory
and an unbounded amount of time to sit there kind of computing on things.
Okay, I put a stop right there.
What I'm describing has nothing to do with symbolic, discrete or anything.
I understand that.
I understand that.
You seem to be equating a variable number with discrete symbolic.
This is two different things.
Okay, let me let me pose it in this form, which is that I can very easily write down a symbolic program
that can output the arbitrary digit of Pi, like the nth digit of Pi.
Nobody can train a neural network that can do that.
So what's the difference between these two types of computation and where in the future might we go with neural networks
or by augmenting neural networks, whether it's differentiable Turing machines or whatever, you know, to try and or neural Turing machines
to try and bridge that gap and capability.
Okay, before we bridge that gap, okay, the algorithm to compute the digit of Pi.
Okay, there's only a tiny number of humans only in the last, you know, few centuries that I figured this one out.
You know, I'm interested in like, you know, how is it that a cat can plan to jump on the table and not fall or even open a door
or do things like that, right?
Once we figure this one out, maybe we can think about kind of more complex stuff like designing algorithms that, you know, involve complex mathematical concepts.
But I think we're way, you know, we're not there.
We're not there, right?
I mean, we're faced with, you know, much more fundamental issues of, like, you know, how do we learn predictive models to the world by observing it
and, you know, things of that type, which are much more basic, you know, that most animals can do and, you know, digit to Pi, it's sometimes online.
So because you're talking about model predictive control, and is it possible that there are different shapes of problem?
So for example, some problems are interpolative and are solvable using, you know, differentiable models.
Do you think there exists problems that are quite discreet in nature and a different type of approach would be required?
Of course, yeah.
I mean, there is certainly a lot of situations where the mapping from action to result is very kind of discontinuous, if you want, right?
This is qualitatively different.
And so there are many situations where it's somewhat continuous and situations where, you know, you change your action a little bit and it results in a completely different outcome.
And so the big question, I think, is how you handle that kind of uncertainty in the search.
And you can think of sort of two extremes.
So at one extreme in the continuous case is the case where you're planning the trajectory of a rocket or, you know, that's pretty continuous and differentiable and everything you want.
And you don't even need to learn the model.
You basically write it down, right?
But there are situations there where, you know, you're flying a drone or something where you might need to learn the model because there's so many kind of nonlinear effects that, you know, it's probably better to learn it.
And people are working on this, same for, like, working robots and stuff like that.
Then there are things that are really intermediate where there are sort of hard constraints on what you can do.
So you want, you know, to grab an object with a robot arm, but there is obstacles in between and you don't want to bump into them and things like this.
So people tend to put like penalty functions to make this sort of more continuous, but there's sort of qualitative difference between using your left arm or your right arm, for example.
Or going, you know, scratching your left ear with your left hand or with your right ear, you know, going to the back of your head, right?
Those are qualitatively different solutions.
And then all the way to the other side, there is intrinsically discrete problems with, you know, that may be fully observable with some uncertainty like chess and go-playing, okay?
And, you know, those we can handle to some extent because the number of actions is finite.
It goes exponentially, but it's finite.
And so using kind of ways to direct the search, despite the fact that the search space is exponential, you know, using neural nets as basically evaluation functions to direct the search in the right way,
and doing, you know, multi-coloured research and blah, blah, blah, will work, okay?
With sufficiently many trials.
You know, in the completely deterministic, fully observable, differentiable case, that's classical model predictive control.
That's fine.
Then there is some stuff that we really don't know how to do and it's twofold.
One is the model is not given to us by equations derived from first principles, right?
So the stuff we're trying to do, you know, is in the real world and it's got complicated dynamics that we can't just model from first principle.
So we have to learn the model.
That's the first issue.
Second issue, the model lives in the world and the world is not completely predictable.
It may be deterministic, but you don't have full observation.
So you cannot predict exactly what's going to happen in the world because the world is being the world or as a consequence of your actions.
So how do you deal with the certainty?
And for that, you need predictive models that can represent uncertainty.
And we are back to the issue I was telling you about earlier.
Do you need latent variable models or joint embedding architectures, right?
And I've changed my mind about this as I told you.
Then there is the third obstacle, which is the problem we're trying to solve of the so continuous differential nature or of the kind of completely discrete, you know,
qualitatively discrete depending on what action you take nature.
And I think most problems are some combination of the two.
So, you know, you're trying to build a box out of wood or something like that, right?
You can make the box bigger or smaller.
You know, you can hit the nail in this way or that way.
And that may be sort of continuous and differentiable.
But then there is, you know, you put glue or screws or nails, do you or use kind of more classical carpentry or whatever.
And those are kind of discrete choices.
What type of wood are you using, you know, things like that.
So I think the, you know, human mind is able to deal with all of those situations have, you know, know to use differentiable continuous stuff when they have to
and use the sort of discrete exploration when we have to as well.
But we have to realize that humans are really, really bad at the discrete exploration stuff.
We totally suck at it.
If we didn't, then we would be better than computers at playing chess and go, but we're not.
We're actually really, really bad.
So.
That's fascinating.
So you seem to be saying in a way that you are a fan of hybrid models and something, something like AlphaGo, for example.
I mean, that that's basically there's an interpolative space which, you know, guides a discrete search.
Let's say like that.
Do you think that's is that a good thing?
Or do you think that's that's so you like that kind of model?
Okay, there is something very interesting about, you know, in the context of reinforcement learning about this, which is actual critic models.
Right.
And you could think of all of the stuff that actually works in reinforcement learning.
I mean, they don't work in the real world, right, but they work in games and stuff and simulated environment.
They very often use actual critic type architectures.
And the idea of a critic, you know, goes back to early papers by Saturn and Bartow.
And the basic idea of a critic is to have a differentiable approximation of your value function.
So you train a small neural net essentially to compute, to estimate, to predict the value function from your state.
And the reason you need this is now you can propagate gradient to it.
Right.
So that that's kind of the first step into sort of making the world differentiable.
You're just making the value function differentiable.
Now, inside of the world, there's two parts in my opinion.
And this is the source of the idea behind model-based reinforcement learning is that you have the world, right?
The world is going from state to state because it wants to or because you're taking an action.
And then there is a value function or, you know, a cost function.
I prefer to talk in terms of cost function that takes the state of the world and gives you kind of estimate of that cost, right?
And gives you the cost.
Now, the world itself and the value function that takes the state of the world and give you pain or pleasure, right, is unknown.
But what you can do is build a differentiable model of that, right?
So a differentiable model of that is what MPC is all about.
You build a model of the world that predicts the new state of the world as a function of a previous state.
It has to be a complete state.
It has to be a state that's complete enough to contain the relevant information about the world, you know, relative to your task, right?
And then you have a differentiable function that you learn that learns to predict the reward or the cost from your estimate of the state of the world.
So what you have now is, you know, a neural net inside your agent that basically can simulate the world and simulate the cost that is going to result from the state of the world in a differentiable way.
So now you can use gradient descent or gradient based methods for two things.
One for inferring a sequence of action that will minimize a particular cost.
Okay, there's no learning there.
Two, to learn a policy that will learn to produce the right action given the state without having to do model predictive control, without having to do this inference by energy minimization, if you want.
And that, in my opinion, explains the process that we observe in humans by which when you learn a new task, you go from the, you know, Daniel Kahneman system two to Daniel Kahneman system one, right?
So, you know, you learn to drive, you're learning to drive, you're using, of course, your entire model of the world that you've learned in the last 18 years, if you are 18.
To predict that, you know, when you turn the wheel of the car to the right, the car will go to the right and if there's a cliff next to you, the car is going to fall off the cliff and, you know, you're going to die, right?
You don't have to try this to know that this is going to happen.
You can rely on your internal model to, you know, avoid yourself a lot of pain, right?
And so, but you pay attention to the situation, you pay a lot of attention to the situation, you're completely deliberate about it.
You imagine all kinds of scenarios and you drive slowly so you leave yourself enough time to actually do this kind of reasoning.
And then after maybe 20, 50 hours of practice, it becomes subconscious and automatic, you know.
That's even true for chess players.
So that's an interesting thing about chess, right?
I played once, I'm a terrible chess player, by the way, and I played once a simultaneous game against a grandmaster.
So he was, you know, he was playing against like 50 other people.
And so I had plenty of time to think about my move, right?
Because he had to kind of play with the 49 other players before getting to me.
And so I wait for him to come and make one move.
And then, you know, in one second, no, first of all, he does, like, you know, as if I played something stupid, which I did.
And he moved within one second.
He doesn't have to think about it, right?
It's completely subconscious to him, you know, it's just pattern recognition, right?
He's got this, you know, covenants predicting the next move, you know, completely instinctively.
He doesn't have to think because I'm not, you know, I'm not good enough for him to really kind of cause his system to kick in.
And of course, you know, he beat me in ten, you know, in ten plays, right?
I mean, as I told you, I'm terrible.
You know, I learned to drive a long time ago, but as it turned out, very recently, I went to drive a sports car on a raceway.
And, and, you know, again, you're the first few times you were very deliberate about it, you know, you explain you a few things and then you basically have to integrate all of that by yourself.
And over the course of a day, you get better and better, like much better, just by, you know, basically compiling what at first is deliberate into something that becomes automatic and subconscious.
Indeed.
And also abstractly reusing knowledge that you've gleaned elsewhere and applying it in a new situation.
But anyway, Professor Yan Likun, thank you so much for joining us today.
It's been an absolute honor.
Well, it's been a pleasure.
You guys are doing great work.
So, you know, keep going.
Thank you for being a fan too.
Thank you so much for watching some episodes.
Thank you so much.
Wonderful.
Randall, introduce yourself.
Okay, so I joined fair, or should I say meta AI research, I guess.
Last June, for a postdoc position, I was doing my PhD at Rice University under the supervision of Professor Richard Berenuik.
And during my PhD, I focused on basically trying to understand deep networks from a geometrical point of view through spline theory.
And now I'm trying to, let's say, expand my horizons and do more diversified research topics with Yan Likun.
And for this paper, basically the main goal was to try to understand through a lot of empirical experiments.
What do we understand by interpolation?
Does it occur in practice?
And does it make sense to use interpolation as we know it as, let's say, a measure of generalization performance for current networks?
And the main point is really to say that the current definition of interpolation, which uses this convex hull, might be too rigid to really provide any meaningful intuition.
And so we either need to adapt this definition or just entirely think about this in a different angle.
But yeah, the current definition is not good enough for the current data regimes that we are in right now.
I don't know if it's precise enough.
Perfect.
No, it's wonderful.
Cool.
Well, sorry, I need to get into the mood again.
Yeah.
Yeah, hi Randall.
It's really cool to have you here.
We've enjoyed reading the paper.
It's quite a short and concise paper, I have to say.
And the experiments are quite, I find them to be really on point, especially where you look at the latent space experiments.
Because a lot of people would say, of course we're not interpolating in data space.
We're interpolating in the latent space.
Yet even in the latent space, you know, and we've talked a little bit about the notion of interpolation and extrapolation.
Is it fair to say that the paper is just sort of a negative argument?
Is it fair to say that the main point of the paper is arguing, look, interpolation is the wrong concept you're looking for when you criticize these models?
Yes.
I don't think it's negative per se.
It's more, let's say a call to change the definition for the current usage of machine learning models.
Because now we're not in low dimension regimes anymore.
And so using those concepts that have been defined 50 years ago when we were looking at univariate models does not really make sense.
So I think the intuition behind what we try to mean by interpolation is right.
We should not change that.
We should change the mathematical definition of it.
And like you say, people could argue, yes, but you have interpolation in the latent space and so on.
But we showed that even in a classifier setting, it does not happen.
And you can also show that in a generative network setting, it will not happen again, all because of the dimensionality.
So it's really about going to the high dimensional setting.
Then things start to break and we have to adapt to that basically.
And we've already asked Jan this question, but if you had to give your best shot at making the argument that these people want to make when they say,
oh, it's just interpolating.
If you had to give your best shot at making that argument successfully, what would you change?
How would you change the notion of interpolation or what argument would you make for those people?
So I think what most people try to say is they try first to conceptualize the data that they have.
So for example, you have an apple, right?
You have different colors.
And if you think of this color as being your latent space, then you can say, OK, between green and red, you have a new color.
But it's in between the two.
You are in an interpolation regime, right?
So all of this point of view is from a generative perspective.
And this is because you only think of a few factors of variations like this.
And then you think, OK, everything should be in interpolation regime.
But even without definition, if you start having a lot of factors of variation, then the course of dimensionality will kick again.
And you will never be in interpolation regime, even in a generative setting.
So for them, the best argument would be if I don't consider the real data set, but a very low dimensional approximation, very rough,
which can be explained only with a very few factors of variation.
And I can somehow linearize those factors in my latent space.
Then I will have more chance at being in an interpolation regime.
So we'll have to have a sort of lossy compression of your data, if you will.
And then you can try to reach there with more chances.
So I think you just still manned the argument for interpolation.
So I think that's precisely what folks do argue is happening in a deep neural network.
Do you not believe that?
Well, as soon as you have high dimensionality settings, then it does not happen almost surely.
And I mean, you could argue, so for example, let's say you take a gun or any big generative network, right?
In the latent space, you have hundreds of dimensions and you sample those guys from a Gaussian distribution.
So even if you were saying, OK, your training set is sample from a generative model and your test set is sample from the same generative model.
And in that latent space, you have interpolation.
Well, it's wrong from the beginning because in that latent space alone, you will never have a Gaussian sample that lies in an interpolation regime as soon as the dimensionality is greater than, let's say, 15 or 20.
So it does not happen because of the dimensionality, unless you have very degenerate things.
Of course, if your generative network just speeds out a constant, then in pixel space, you will have interpolation.
But this is degenerate by definition.
I'd like to pick up on that, if you don't mind.
Yeah, so what I want to pick up on is, again, all of this hinges on definition one, which is in the paper, which is membership within this convex hull.
And there's a sense in which that's an extremely rigid definition of interpolation, right?
And I think I heard you earlier say that what we need to do is redefine what we mean by interpolation in higher dimension because the intuition of interpolation is still correct, but we need to redefine what we mean by interpolation.
So if you took a shot at redefining interpolation, how would you define it? What do you think is a better definition of interpolation?
So I think it's very task dependent. So let's say you want to look at a task which is pure object classification.
In that case, I think going back to what we were saying earlier on first trying to compress some of your data so that you can explain it with as little factors of variation as possible.
And then you can use the current definition just on a compression of your data.
Then it could make sense because you don't really mind about the finicky details of your objects if you just want to differentiate between different classes of objects.
But in another setting where you might be trying, I don't know, to denoise samples or things like that, then you might want to have a very more specific definition based on what you try to denoise in your data.
And so on. So I don't think there will be a general definition that works across the board. It will be really dependent on the type of task you are looking down downstream.
It's really the key.
Let me throw something out there because I've been thinking about this.
And I tried to ask Professor LeCun about this, but I didn't communicate it clearly, which is that for something to be in the convex hull, so in indimensions, for something to be within a convex hull, it's a necessary condition that on every single dimension, the sample data point lies within the range that was sampled.
So that's a necessary but not sufficient condition because the convex hull is actually even smaller than that space. But it's necessary that it be within that axis-aligned bounding box. It has to be inside there on every dimension.
And so what if I were to invert this and say that, well, instead, I'm going to define extrapolation as on every single dimension, it has to be outside the sample range. So in other words, the point has to be outside the axis-aligned bounding box.
That would actually come to the complete inverse conclusion because then exponentially so, all machine learning would be interpolation because it's very, very unlikely that you're outside every single dimension.
So I'm thinking somewhere in between there, there's almost like a fractional concept of interpolation versus extrapolation where we can say it's almost like on average, how many of the dimensions do you hit inside the sample point? So maybe I'm inside 20% of the dimensions on average.
Could that be like a route to any type of improved definition?
So that's actually a very good point. So basically what you are saying is instead looking at, let's say, the closest, enclosing hypercube. And then you say, okay, if you are in that hypercube or not, you are in interpolation or extrapolation regime.
And that's actually something we are looking at right now, which is not the smallest hypercube but the smallest ellipsoid that encloses your data. And it's somewhere in between the convex hull and the hypercube that you mentioned.
So for sure, there is some ways in between those two extremes where you will have a meaningful interpolation and extrapolation regime that does not collapse all one way or the other way.
So this is for sure one interesting route. And this could potentially be, let's say, task agnostic. But then again, would it be precise enough to re-quantify generation performances per se? I don't know yet.
But yeah, that's something we are looking into right now.
The reason that we kind of came to that thought process is because there is an intuition that machine learning is kind of very good at taking all these dimensions that we sample and saying, you know, it's really only a subset of those dimensions that matter, at least in some transform way.
Like for example, suppose I was doing, suppose I only had 16 dimensions of ambient space and everybody agreed, yeah, given the amount of data we have, we are interpolating like almost all the time we're in this rigid definition of a convex hull.
And then somebody comes along and says, oh, by the way, we upgraded all our sensors now, and we added 240, you know, dimensions. And if all those dimensions happen to not be useful for the problem that we're trying to solve or classification problem or whatever, it would be weird to say now we're all of a sudden extrapolating,
because if the neural network is doing what it should be doing, it will ignore all those irrelevant definitions and continue just calculating exactly what it was calculating, you know, before.
Yes, that's a very good point. But I think for this to actually work, you need to assume that you have enough actual samples, so that when you train your network understands that basically those dimensions are pure noise, it does not need it to solve the task at end, and it disregards it.
So you have some more regularization terms that kicks in, and it does not try to use them to minimize the training loss. But I think in practice, that's not really what we see. If you take like image task classifications, even especially now with self supervised learning methods, we see that actually most of the information that we could think is irrelevant for the task is actually kept because it can of course help in reducing the training loss.
So I think for what you said to work, you really need to be sure that either you have the perfect regularizer, or you have the perfect model and regime of training samples and so on. But in a general setting, I think it will be tricky to claim it like this.
That's a very interesting point. So you're saying when you add these extra dimensions, even to learn to ignore them, you have to have lots of data.
Yes, yes, or regularization or some mechanisms that exactly.
Yeah, very interesting.
That's one thing. That's one reason why, let's say, adopt parametrization by end is sometimes useful. Because if you know which things are useless for your task, you can by end remove them from the data that you feed to your deep net or whatever model.
And then doing this will improve generalization. So that's why in some cases, it's still useful to do those preprocessing steps.
Right. Could you give me a bit of intuition here? So we've been listening to quite a few folks that make the argument about the interpolative nature of deep learning. And you could argue that any kind of processing and machine learning, not just feature transformations,
but even things like regularization and domain randomization, that there are all ways of making the problem more interpolative and it would be appropriate for an interpolative problem.
So I'm thinking to myself, I mean, in this example, you take pictures of clock faces, and it's not interpolative in the ambient space, you perform some kind of feature transformation.
There exists some nonlinear transformation that transforms it into an interpolative space. That's what these people say. But I'm wondering, you know, I'm interested in the curse of dimensionality.
You know, why does deep learning work at all? And I've spoken to folks that talk about creating various priors to combat the curse of dimensionality.
But why do you think that deep learning works at all in these high dimensional spaces?
Well, so first, I might say something a bit speculative or not agreed upon by everyone, but I don't think you can...
We love it. We love it. Please do.
Exactly. I don't think you can state so generally that deep learning works right. You need the right architecture, you need the right loss function, you need everything right for it to work.
And that's why it works now because we spend, I don't know, much amount of money and manpower to get to where we are now.
But if you just take a plain MLP, you apply it on ImageNet, I don't think you will say it's working right.
So I think right now it's working. I mean, what we have is working because we basically by end it holds a cross-validation such that the network that we are using is regularized enough to only learn the necessary
or meaningful information or at least as best as we can to provide good generalization performances.
So yeah, it's a bit too general of a statement to say that deep learning works out of the box for everything.
In fact, many cases...
I love that point.
Yes, yes. Many, if you go into not just image classification, but let's say audio classification and some maybe trickier data set where you don't have a lot of samples,
then everything starts to break quite rapidly, right?
You have always people trying to go into the medical field as well.
But since it's very hard to generalize between patients, between recording devices and so on, things get very, very messy and everything is ad hoc and optimization, basically.
So I don't think we are at this point yet where you can say that's it, deep learning works out.
Can I jump in real quickly here, Tim?
Because what I want to say is brilliant point.
I mean, and when you think about it, let's take Professor LeCun, you know, one of his great invention, right?
The CNN convolutional neural network.
That was discovered by a human being like a machine and no neural network learned to do convolution.
LeCun taught machines how to do convolution.
And so we oftentimes discount this and say, look how great machine learning works.
We discount all the human engineering that has gone into actually making machine learning work through specific architectures.
Yes, exactly.
That's basically we are guiding where to go.
And then of course, once we guide enough, the machine knows what to do.
I mean, this is not a reduction of what we can do with deep learning, right?
But it's just saying that a general statement of out of the box deep learning works even in a new task that we never tried before is an overstatement.
But I still want to linger on this point of why you said, oh, here's just a random projection of the data and then here's a ResNet projection.
Here's a trained ResNet projection.
What I took from that is irrespective, it's all in an extrapolative regime.
And there was a guy who posted an article who is disparagingly saying that neural networks are just interpolators analogous to a Taylor series approximation within a small part of the domain of a function.
And it goes haywire outside of the domain.
And there are a lot of people that would make that case that neural networks cannot extrapolate outside of the training range.
But they do seem to work remarkably well given the curse of dimensionality.
So why is that exactly?
So let's say for simplification that you have a binary classification problem.
So once you go into your Latin space or if you just do linear classification, if you stay in your ambient space, basically you say you have class zero or one based on which side of an hyperplane you lie in.
Now, if you are on the good side, you can go to the infinity, right?
You can go to an extrapolation regime as far as you want from the training set.
As long as you are in the correct side of that hyperplane, you will have good generalization performance.
So it's not exactly correlated.
The only thing you need some always to have the orientation of the axis where you will extrapolate to be somewhat aligned in the orthogonal space of that hyperplane.
Once you have that, then you will generalize even though you can be an extrapolation regime in ambient or feature space.
So that's one thing that is maybe proper to linear classification.
If you were doing maybe some other type of classifier on top of it, it might change.
But given the current settings, I don't think people should expect bad performance because you are not in an interpolation regime.
This is quite of a shortcut, maybe that it guided by our intuition, right?
We think it's much easier to classify something if it's in an interpolation regime.
But if you just look at the plane in our classifier, that's not the case.
Well, so just to steal man a bit what say others on the interpolation side.
Again, that's only if you're buying into a very rigid definition of interpolation,
which the way the paper defines it is in a linear way.
I mean, it's defining interpolation by this linear convex hull.
And of course neural networks, for example, are highly nonlinear systems.
And what you just said, sort of which side of a separating hyperplane you're on, that itself is a nonlinear function.
And so I think the point you made early on was very good, which is we have this intuition of what interpolation is.
We haven't yet got a good definition for interpolation in multi or high dimensionality nonlinear cases.
But obviously the linear definition doesn't really apply.
Yeah, exactly. It's really because, I mean, it's linear, but again, you can consider your data as being nonlinearly transformed.
The problem is still that if you have high dimensionality, then to be within a convex hull becomes exponentially hard using the convex hull of the training set.
If you define something else, like you said before, the hypercube, it could be the opposite.
It's always in interpolation regime.
And the key is basically to find a meaningful set such that you don't go all the way in one direction or the other.
And because I think the main point also is that this should give intuition into the generalization performance of a model, right?
Because if you can detect a sample is in an extrapolation regime, but your model still performs very good on it.
I don't know if it has really a lot of practical use.
So what would be good as well is to have the generalization performance of a model correlate with your definition of extrapolation.
And that's really what we're trying to get.
And that's why I was saying that probably you might have a task dependent definition because that might be where you get the best precise definition to reach that.
I was wondering to what extent, if any of this invalidates the so-called manifold hypothesis.
So I think when most people speak of interpolation and deep learning, their intuition is something along the lines of, well, the model learns some manifold in the latent space.
And interpolation means I'm just kind of like traversing the geodesic on that manifold.
And when you visualize the results on an autoencoder or something, you can see this kind of continuous geometric morphing of, let's say, one image into another image.
And that manifold, I think you said in your paper that the actual data manifold, it's not possible to approximate that well, but it's doing something interesting.
And a prediction on that manifold in the intermediate space, it's not massively deranged, is it?
It's still doing something very useful, statistically.
Yes, yes, yes, totally.
So I think it does not at all contradict it.
And in fact, think of a very, very simple example where you say your data is a linear manifold of dimension 100, let's say, which is a relatively meaningful number of dimensions, even for images.
But now suppose it's completely linear.
Well, even in that case, it will be hard to be in interpolation regime, just because you have 100 dimensions and picking a new sample that lies in the convex set of your training set is exponentially hard.
So you can have a manifold for your data.
It can even be linear.
So the simplest manifold you wish for, the only problem is the dimensionality of that manifold.
Now that being said, again, it does not mean that you cannot do basically moving on the geodesic of your manifold or you cannot do some sort of interpolation, because that's what happens in most of the current state of the generative networks, right?
It's just that you don't need to be in interpolation regime to have a correct generation process, basically.
And that's, again, one thing that is very important.
And you mentioned you have those examples.
That's true that the interpolation is extremely nice, but again, it does not say that the points you start with and the point you end up with are in interpolation regime of your training set.
So I think it's two different things to be able to interpolate or move on your manifold and being in the interpolation regime from your training set.
So what's intriguing me about, and I want to clarify something earlier, which is that though a hyperplane is a linear construct, the function that says you're on one side or the other of a hyperplane is a nonlinear function.
And so what's, and in that case, it's a digital function.
You're either on this side or that side at zero or one.
What's kind of interesting is you gave that as an explanation for why machine learning works at all, which is that for data sets that we care about or problem spaces that we care about,
it seems to often be the case that we can do enough sequences of linear and nonlinear transformations to arrange the data such that it falls on one side or another of a separating plane.
So in a sense, it seems like real world data is often separable if you do enough of enough transformations on it.
And since that separability is a very digital concept, is there anything interesting in that?
Any intuition?
Yes, yes.
So for this, you still have to be careful, right?
Because if you go in high dimensional spaces, you can basically separate everything very easily.
But your generalization performance might be bad.
So basically what you want is really to have, let's say, a good ratio of how much you expand dimension, how much you nonlinearly transform your data to then reach a good generalization performance.
Because otherwise, you can just fall back into a kernel regime, let's say, and you expand so much your space that sure you can separate everything on your training set, but then the generalization is going to be very poor.
So and that's one strength of deep networks, right?
Is that you have so much of those nonlinear transformation that you can somehow not expand the dimension of your space too much or even contract it and still have a separating hyperplane in the end,
where generalization is much higher than in other class of models.
So I think the key is really to have those meaningful nonlinear transformations such that you don't have to increase your space too much.
You just shape it around, if you will.
And what you said earlier is exactly true.
If you think of which side you are in, basically you are binarizing your data, right?
And if you have good classification, it means all the classes are assigned to the same labels, which is one or zero.
And if you think of it still in interpolation regime, then suddenly you are in interpolation regime, right?
You are a one.
The new sample is a one after binarization and you become interpolation regime and you have a good performance.
But this comes after this compression step, if you will, or discretization step.
And that might be another direction to explore as well.
If you start to quantize things or to compress things, as we were saying a bit at the beginning, then you can reach the interpolation much more easily as well.
I have a couple of questions.
So again, in table one, your paper is making the argument that everything is extrapolation given this convex notion in high dimensional space.
But if we zoom in a little bit though, so you're using a pre-trained ResNet classifier pre-trained on ImageNet.
And how do all of these things change the structure of the latent space in a meaningful way?
The embedding space is also highly distributed.
And we were wondering, you know, can you give us some intuition here?
So is the information likely to be quite evenly distributed over the latent?
Or do you think it's actually quite bunched up and sparsely encoded in few of the features?
So I think this will depend on which training setting you use, right?
So for example, if you start using dropout and things like that, you will try to have a more evenly distribution of your information to have a more stable loss when you drop those units.
So I don't think you have a general answer for that.
It will depend on the type of regularization you have or training is done, etc.
But you have to keep in mind that what you try to do with gradient descent is just minimize your loss, right?
But then with cross-ontropolis, let's say your gradient starts to vanish as you become really good and you stop learning at where you are, basically.
So given that, depending on your initialization, you will still try to make the best of what you get.
And even if it means learning redundant dimensions, if this can reduce your loss further at a more rapid rate, that's what you will do.
So if you don't impose any regularization or anything, there is no clear reason to assume that everything is well organized and so on.
And that's what we see even in generative networks.
You have to start putting so much regularization to try to have disentanglement and to try to make sense of those latent spaces.
Because otherwise, you just try to learn what minimizes your loss with the most short-term view of your loss landscape.
So basically, that could be built if you have some specific regularizer.
But otherwise, it will not occur naturally.
Of course, and again, if you wanted to only retain the minimal information to solve the task at N, then you will see much more interpolation regime in that latent space.
If you think of MNIST, for example, you will disregard the translation of your digits, the rotation of the digits, all those things.
All those things will be disregarded when you reach the latent space.
And then you will basically be in interpolation regime most of the time.
But since you keep as much information as possible to try to minimize your loss as best as you can, then you basically occupy as much as you can.
Unless, again, you have some degeneracies because of the whole architecture tricks.
For example, if you have a bottleneck layer, you will limit the dimensionality of the manifold you span in the latent space.
So you can have all those parameters that can play a big role.
So it will be in the general setting.
I don't think you could assume anything.
And I think there is also a relationship, too, between the dimensionality of the latent space and, let's say, some intrinsic dimensionality of the problem.
So if the intrinsic dimensionality of the problem only takes five dimensions to solve, and yet I give a latent space of 256 dimensions,
I think what I hear you saying is that, of course, gradient descent is going to make some use of those other 251 dimensions,
but they're going to have maybe a very minuscule or diminishing effect on the latent space.
Whereas on the other hand, if I then took that same network and increased the complexity of the problem,
we could end up with, for example, it's sparsifying for any particular class.
So if we're doing some multi-class problem, we may find that it sort of arranges these seven dimensions to solve the dog versus hot dog problem,
and these 12 dimensions to dissolve the car versus motorcycle problem.
It might be forced to make more compact use of that latent information space per class.
Is that fair?
Yes, so that's a very good point.
So first of the first things that you said, one thing to keep in mind,
so let's say your data is even linear manifold of dimension one, and then you go through a deep net,
and then somehow it's already linearly separable.
Then you only need to learn the identity mapping with your deep net to solve the task.
But if you start from a random initialization, it's extremely hard to do that, right?
That's why people came up with ResNet and all those things.
So already from this point of view, it means that almost surely your linear one-dimensional manifold will become highly non-linear,
still one-dimensional, but very highly non-linear in the latent space of your classifier.
And as we showed in the figure one of the paper, even if the intrinsic dimension is one,
if you are highly non-linear in your space, then you will basically never be in interpolation regime.
So you have those sort of artifacts that come just from the fact that learning simple mapping,
let's say with a non-linear deep network is not always simple,
and those artifacts will be introduced right away because it's so hard in your parameter space
to reach that point where you have identity mapping.
So this is another effect that kicks in and that can somehow remove those assumptions
that even if your data is in a low-dimensional regime and almost linear,
it will be preserved in the output space of your network.
So this is something really important to keep in mind as well.
So Randall, I also noticed in about 2018, you were the first author on some really interesting work,
and it was called a Spline Theory of Deep Learning, and then I think the next year it got into NeurIBS.
So I'm just reading a bit from the abstract, so you said you built a rigorous bridge
between deep networks and approximation theory via spline functions and operators,
and you actually think that this can explain a lot about deep learning
in the sense of then being a composition of these max-safine spline operators.
Has any of this work informed your view of deep learning now?
Yes, a lot actually.
So first of all, I want to be precise that of course a lot of people knew about the fact
that if you use, for example, ROLU activations, absolute value and so on, or max-pulling,
then the whole network is continuous, piecewise linear mapping.
So what we did mostly is to make this a bit more rigorous and try to understand
what the partition of the input space looked like, what the perigen mapping looked like,
and how can we use that to gain some more intuitions into what's happening.
And the nice thing with this is that if you think about it, there is nothing simpler
than piecewise linear mappings, right?
Basically it says that if you are in a region of your space, a region from the partition
of your mapping, then the input-output mapping will stay linear.
And this allows to do a lot of analysis, for example, to adversarial perturbation
or all these type of things.
And it can also open the door to other lines of research.
So for example, one thing we did, I think it was NERIP's last year or two years ago,
was to use that to derive the exact expectation maximization algorithm
for the objective networks, because now you have a cleaner, let's say,
or simpler analytical form of your network.
And this really opens to me the door to derive some more interesting theoretical results.
It does not really help intuitively, because I think everyone had this intuition from before already,
but it's mostly a mathematical tool that allows to derive with more interesting results.
And I think one important thing as well is that you have really this dichotomy right between the,
let's say, old-school signal processing, template matching type of academia researchers
and the new school with deep learning, everything has to be trained and so on.
And what is interesting in this paper is that we somehow bridge the two.
We say that basically a deep net is a very smart way to build an adaptive spline,
which will learn automatically its partition of the input space
and the partition of fine mapping, such that it works in high-dimension.
And this was not known before by anyone in the spline theory.
If you speak about adaptive spline in high-dimension,
no one has no idea what to do except dimension two or three, maybe,
because a lot of PDE work uses this, but outside of dimension three,
no one thinks about splines.
So I think there's a very strong result is to bridge those two different fields.
That's fascinating.
I think one of the issues, because I'm speaking to Juan Bruno about this,
and he was saying there was a big tradition in harmonic analysis
of trying to reason about the behavior of these models.
And we did have the universal function approximation theorem,
which in a sense is talking about stacking basis functions to approximate an arbitrary function.
But you say in your abstract here that the spline partition of the input signal
opens up and opens up a new geometric avenue to study how deep neural networks
organize signals in a hierarchical fashion.
I don't think people really have much of an intuition on how these models behave
and how to reason about them.
Yes, that's a very good point.
So one thing that we did, for example, is to study all the partition of the mapping
evolves as you go through the layers.
And what we show is that at each layer, you keep subdividing your current partition.
And this is very interesting because once you know that if you look at a binary classification,
for example, you get that the decision boundary is basically linear in each region of the partition.
And so what this tells you is that as you add layer,
what you have to do is to refine the regions that still contain more than one class within them.
And so this kind really brings insights and maybe opens the door to building new learning techniques
to how many layers you should stack, which regions should they subdivide and so on.
And this is really akin to decision trees and how they build their partition as well.
So there is a lot of work that could be done also to maybe bridge the two or use one to understand the other.
And it's really geometrical because it's in the whole field of computational geometry
and so on because we have those hyperplane arrangements, those half-spaces, intersection of them,
hyperplane desalation, it's also a known system.
So it's really geometrical and it can have a lot of interesting insights to understand what's happening.
It also provides nice visualization tools.
It's really fascinating.
Would you be interested in coming back on the show for a future episode just dedicated to this topic?
That'd be amazing.
Randall also actually released a paper called Neural Decision Trees.
And in the abstract he said they propose a synergistic melting of neural networks and decision trees.
They call it neural decision trees.
So this is something that you've been thinking about actually from many angles.
Yes, yes, yes. And that's very, very synergic to think about one from the point of view of the other as well.
Because if you think of a decision tree, the real limitation of it is that all you subdivide one region by adding a node
does not really tell you how to subdivide another region in another part of the space.
You don't have this, let's say, communication or friendly help between regions subdivision.
But in a deep net, what you do actually is that if you know how to subdivide one region,
then it will automatically enforce or you subdivide nearby regions.
And through this, suddenly you have a mechanism that appears which is that you don't need samples in each region to know to subdivide them.
You just need samples in some of the regions and then it will guide you on how to subdivide regions of the space without samples.
And that's something that is extremely strong when you go to high dimensional settings
because you cannot have samples in all parts of the space by definition.
So I think this is extremely nice to have both points of view
because then you can try to use the strengths of one to maybe improve the other.
So that's really interesting to me.
I want to ask you this question that we ask a lot of our guests
because it's just at least something that's kind of profound interest to me is that
there is this apparent dichotomy between continuous and discrete.
It's like the human brain is at the most lowest possible level in analog kind of continuous system
and yet it evolved all these sort of discrete almost computations on top of it
like pulses that either fire or don't fire, like that type of thing.
And in the learning world or let's say really in the computation world,
we have the fact that I can write a very short piece of symbolic code, discrete code
that can go and calculate the nth digit of pi or something like that.
But it's almost impossible to train or it is currently impossible to train any neural network of fixed depth
to do the same thing.
And so we have this weird different regimes.
We have the discrete kind of logic reasoning type world
and then we have the continuous differentiable type of world.
Do you view those as, I mean, are they fundamentally different regimes
and we're always going to have hybrid systems that kind of combine both types of reasoning
or is it possible to just say project that discrete computation fully into an analog type space
if you just have enough parameters or something?
Yeah, I think it really depends on the resources that you have, right?
Because to me at least it seems that the hybrid system might be the most efficient
where you can easily, let's say, cluster different settings into groups.
And then, and for this you can just have a discrete settings
and then within each group, discretization is not good enough anymore
and you need to go into the continuous regime.
So I think it will depend on the application you have at hand
or efficient you want to be doing it either energy wise or anything else.
So I think it's not clear if one should dominate the other necessarily.
For example, if you go back to the spline setting of a deep net
you have a discrete setting which is basically your partition
which region you are in and then within that region
you have a linear transformation of the mapping which is basically continuous.
And both interact if you adapt one, the other one changes and so on.
And I think having this type of hybrid systems
and where somehow learning through the continuous part adapts the discrete part
is what is extremely powerful.
And that's I think one extremely beautiful property of current networks
is that they do automatically this adaptive training of their discrete part
through training of their continuous parameters.
And that's why they are so efficient.
If you think of pure approximation theory
and you have an adaptive spline in one day
that's why you have the best convergence rate basically.
So I think you really need both systems to interact.
If they don't interact then I think it's really easy to become suboptimal
in an interesting region.
Because we put a similar question to Lacoon
and he was kind of saying that in an ideal world
we would have a discrete system as well.
Humans are really bad at playing chess
because we don't have that discrete system built in.
But the problem I think people like Lacoon have with these discrete systems
is typically they're symbolic and they're statically coded.
You could start talking about getting into a discrete program search
and you could even guide that program search based on some deep learning model.
But I don't think to Keith's point
I don't think it's really possible to do that well inside the continuous domain
because if the problem even was learnable with stochastic gradient descent
the representation would be glitchy.
It just wouldn't work.
I think it depends a lot too on what are you trying to achieve with a model you build.
If you just try to be as close as possible to let's say
what the human brain is doing
then you might impose yourself to have some restriction on
do you want it to be discrete or not.
Or if you just want to have a model that you can deploy on a task
and it can solve the task as best as you want.
So I think depending on what is your goal
and what are you trying to imitate with the model
would change all your answers as well.
But what if the goal was task acquisition efficiency?
So it's like I don't know what the task is yet.
Yeah, I think that's like again to me
a hybrid system where you have interaction between both parts
intuitively would be the most efficient
but it might not be true for all settings.
When I was looking at figure three
both Tim and I were interested in the fact that
if you look at the MNIST data
which to a human being is kind of a simpler data set.
One, two, three, we know how to do that type of thing.
As you increase dimensionality
it much more rapidly becomes extrapolation
versus ImageNet which seems to kind of
more slowly transition from interpolation to image extrapolation.
And what I'm wondering is the intuition I got from that
and I wonder if this is completely wrong or it's correct
is that for machine learning
there's a sense in which MNIST is actually a harder problem
because it has to look at kind of global relationships
like it has to try and say well there's a circle over here
that's kind of oriented with respect to a line
that's kind of further away.
And so it's harder for it to do that
whereas we know that with ImageNet
very frequently ML sort of devolves
to looking at these micro-texture like
well everything that has this shade of yellow
is a school bus type of thing.
Is that an intuition one can take from that plot
or what does it mean that MNIST decreases
so much more quickly?
Yes, so that's actually something that we tried to clarify
with the figure that comes after this one.
Basically the thing that you really need to be careful about
and keep in mind is that if you look at for example
a 16 by 16 patch for MNIST
you have basically maybe most of the information
you need to solve the task and you have a lot of texture
about your DG etc.
If you look at ImageNet and you take a 16 by 16 patch
you have basically no information about the class
it's extremely small patch
it's almost constant across the spatial dimensions, right?
It's basically a very small percentage
of your full image.
So that's why MNIST goes much more quickly
to extrapolation regime
for a fixed dimensionality in pixel space
because the information you have
in that amount of dimensions
is greater than for the ImageNet case.
And this is only because it's already
much more than sample grade.
If we were looking at MNIST
with a 224 by 224 spatial dimension
and we look at a fixed dimensionality
then you will not have this difference anymore
and it might even reverse.
So the really important thing to keep in mind
is that even though it's the same dimension for both
it does not represent the same proportion of image
that is present within that patch.
And that's why you have those differences in...
That's mostly why you have those differences in those curves.
It's funny because we had the opposite intuition
and so in the following figure 4
you're showing then on MNIST
that more of the variance is explained
with fewer of the principal components on MNIST
and as you say that that's just because
those pixels on MNIST are more salient for the problem.
I mean just so I understand
because we were debating this a little bit
about your articulation of figure 4.
Yes, sure.
So basically what we are looking for
for the increasing dimensionality
for the three data sets
is we pick a number of dimensions
in the spatial space.
So we do this by extracting a patch, let's say.
And then what we do is that we extract
of course the same patch for all the samples
and then we are looking at the proportion
of the test set patches that are in interpolation regime
and we report this.
Now for the PCA plot, what we do
is basically we look at once you extract this patch
how many principal components you need
to perfectly reconstruct those patches
or you could say to explain the variance in those patches.
And this gives a direct relationship
because it shows how concentrated you are
in the lower dimensional manifolds
affine 1 of course that learns through the principal components
and it means that if you can encode much more information
with much less principal components
then you lie on the lower dimensional affine manifold
and this coupled with figure 1 shows that
basically it's much easier to be in interpolation regime.
So the whole point of using this PCA plot
or good low dimensional affine manifold
represents the current extracted patches
and then to use this as a way to justify
the extrapolation regime curve that we see.
So because again in the PCA regime it's linear manifold
so if only two components for example
perfectly describe all your patches
then you will need very few training samples
to be in interpolation regime.
Okay that makes sense and then the staircase effect
on the smooth subsample row is a function
of the size of the Gaussian filter you used to smooth it?
Yes so the staircase occurs basically whenever
the number of dimension increase
and we get a new bigger patch to get it
because there is different ways to get it right.
One would be to always extract the center patch
and remove some dimensions if you don't have
a number of dimensions that you can represent
with a square patch.
Another thing you could do is to first smooth subsample
and then remove the dimensions.
So there is different variants on how to extract those patches.
We try to show two different ones to show that
the results do not really depend on how you do this process
but you will have some little different artefacts like this.
It does not change the overall trend
yet it can change the small trends
when you change from one dimension to another.
Well let me ask a question here about figure four
again about the intuition that we had
on leaving aside interpolation and extrapolation
for the moment.
It seems that MNIST for a given amount of variance explained
and for a given dimensionality
MNIST requires more principal components.
For a given number of dimensions.
If we fix the dimension and we fix the percent variance
explained MNIST requires more principal components
than ImageNet.
That seems to me to tell me that MNIST
is a more difficult problem.
Is that not true?
I think it's a bit...
So for a specific number of dimensions you could argue that
but the thing you have to recall is that
because MNIST images are much smaller
in spatial dimensions.
If you have a 16 by 16 patch
you have basically the whole MNIST dataset let's say.
And so just having a few principal components
is not enough to really reconstruct the whole MNIST dataset.
Now on ImageNet a 16 by 16 patch
is almost a constant texture right?
You have a few different colours but you don't have a lot of variation.
It's basically through the spatial dimensions
it's basically constant.
Now what this means is that with only
three or four principal components
basically one for each colour channel
you can perfectly reconstruct
all those 16 by 16 patches.
So to really get to the conclusions you are saying
what you will have to do first is to
either done sample ImageNet
to be 28 by 28
or up sample MNIST to be 224 by 224.
And if you do that
then basically I think you will have
the roughly same evolution
of the interpolation regime
because the 16 by 16 patch
on this extra up sample MNIST image
will be either completely black
or completely white.
And in this case you will still need
a few principal components.
So this is also something very interesting
because it shows that maybe
for the task at hand you might not need
to have such a high resolution image.
You might done sample
but because when you done sample
you basically keep those
in crucial information
you don't necessarily go faster
to interpolation regime.
So this is another point.
It's really tricky
you should think of it as
how much of the image do I encode
given that number of dimensions.
And then given that
this plot might make
maybe easier to understand.
16 by 16
makes total sense.
Exactly. So 16 by 16 on MNIST
is maybe around 60-70%
of the image.
While 16 by 16 on ImageNet
is maybe around 5% of the image
and that's why you have
those different regimes that appear.
Incredible.
Randall, thank you so much for joining us this evening.
Sure, sure.
So we just spoke with Randall.
What was your take on that Keith?
Absolutely brilliant.
A true pleasure to speak with him.
For me it cleared up
a lot of thoughts and issues
I was having with this paper.
So for example
right up front what he says
is my purpose behind
this paper
is to show that
even though the intuition
that people have of interpolation
like the intuition that we have of interpolation
is good
the mathematical definition
that we have of interpolation
is not useful
in high dimension.
What I thought was interesting too
is when we asked about
what about the manifold
concept?
Why isn't that the definition
of interpolation?
And he brought up a really strong argument there
as he said, well
let's just take the simple case
of suppose the problem you're trying to solve
just is linear.
We're trying to solve like even in that case
this linear case
in high enough dimension
interpolation doesn't work
and there your manifold is just literally
a convex hull.
And so sure you can have
kind of a nonlinear transformation
in a nonlinear shape and whatever
you're still hit by this curse
of dimensionality.
He brought up the point
that like, of course
if your problem compresses down
enough
to where only a small number
of transformed dimensions
latent space dimensions matter
and everything then you can be said that
you're interpolating
because you're not really hit by that curse of dimensionality
because we stripped away all the dimensionality
down to these dimensions
we've gotten lucky our problem space
our data samples, etc.
allowed us to do that.
But that's not going to be all problems.
Some problems may just intrinsically have
high dimensionality
of interpolation that's not useful.
So we need to do something better
we need to come up with
a definition
of interpolation
that maintains the intuitive notion
that we have of interpolation
but that continues to work
in high dimension.
And he made a very interesting
kind of end goal here
which is like if we can get a definition
of interpolation that ends
by really approaching this concept
of generalization
that's what we're trying to
achieve really.
And what do you think his take on
generalization is then?
You made a comment that
on the one hand you could just
ridiculously specify the space
and make it trivially separable
but then you lose generalization.
Well, yeah, and I think
because we did ask
or you put the question
what you're trying to solve
is the ability to solve novel problems
like you know what happens then
of course that was in the context
of our discussion about hybrid systems
where you're combining the continuous
with discrete and they're able
to ping pong and kind of modify each other.
And I think
what we got from him there
and consistently really
throughout our discussion
and what's interesting is this
totally aligns actually with
Francois Chalet as well
which is that all these questions
like a lot of these very difficult questions
that we're asking are problem specific
and there is no
we don't currently have some one size
fits all set of concepts
that fits well
for every problem space.
You know it's very task specific
very problem specific
you know data specific
so I think that's an area where we got
dive deeper with him
but I didn't hear anything definitive
you know today.
It's a wrap. We just interviewed
the godfather of deep learning.
How's that possible?
I think we can just quit now.
We might as well just shut the channel down.
Yeah.
I mean obviously after we've published it.
That's the singularity.
So what's your take?
I would have
I think this
but we also interrupted him
a little bit. Maybe the question was
I would have loved for him to
be more like you know where's the actual
disagreement right because
a lot of times you know when we
when we put
questions to him like okay other people say this
he was like oh yeah I agree right
and this seems
it seems to be a general sentiment that's also
when we talk to other
people about you know perceived
disagreements they're always very
you know being being
good being academics being
also friends probably with a lot of people
you're always like yeah you know they have
good point you know we essentially
agree on all the
on all the things right but then
I sort of want to know
where actually do people
disagree right if it's
if it's
you know and
that is
a little bit I mean
I obviously have a feeling but it's
still a little bit elusive
and
if people disagree on the
actual technical nature
or more on the philosophical end of
what does something mean
I definitely think
there is from hearing
now I think from
his answer to that question
I could hear a little bit
in that
he seems to be more optimistic
on what these learning systems
can do than
maybe other people are
right because some people
seem to have really
kind of a hard stop on like
this will never be possible
with like a learning system
whereas it seemed
it seemed
that he had
sort of a more optimistic outlook
and said of course they can't do it now
right however
you know we work on it we modify
them you know we're figuring out
how do we need to build
these systems such that
you know a learning system
can conceivably do
many of the things that people would call
a kind of reasoning
and I think that's why he went into
you know let me give you an example of reasoning
to sort of
show look here you know
here is an example
of a reasoning that
neural networks already do
and that means that
something like reasoning
in general isn't too far away
and that's
ultimately where the disagreement
might be with other people
I mean I think that's that was my take on
his answer to our first question
like the you know why'd you write this paper
is I think and maybe I could
go back and watch this but I think
essentially what he was saying is
you know sometimes people
they have some definition
in their mind whatever it is of
interpolation and extrapolation
and they come up with kind of an argument
to say see this
machine learning is doing
interpolation
and in a binary sense it can't
do extrapolation
and so therefore there's this entire class
of things you know that it's
not capable of doing and I think that's
what his objection was which is look like
this is not a useful distinction
to say between
you know it's not a useful distinction
to draw between quote-unquote interpolation
and extrapolation
like here's an example
of a definition right that's
a relatively standard definition of
interpolation and if we apply this
interpolation to
theoretically what machine learning is doing
and empirically to what machine learning is doing
it's always extrapolating so I think
that's what he's objecting to is just like look
don't come to some
strong conclusion about what
machine learning neural networks I'm not
sure what the right way to phrase things is now
what it's capable of doing
like we just need to do more work
to expand its capabilities
you know folks like Gary Marcus
making this case of we need discrete models
because they can abstract broadly
they give a couple of examples
you know binary encoding for numbers
and there's one example with
can you have a model that reverses the bits
or I think there is another example in his
algebraic mind which was about
can you generalize from the even
numbers to the odd numbers and you can't
do that and the reason you can't do it is
the new examples are completely
outside of the training space of the
input data and probably that problem
is not interpolative or it's
not differentiable
I mean Wally Subba says the same thing he says
in language processing in particular like
in language understanding in order
to have broad generalization
you need to have abstract rules so for
example being able to generalize from Mary
loves John to Mary loves
Jane they make the argument
that with a statistical approach that just
wouldn't be possible well I think
so I think and this is just my
opinion but I think his answer
to my digits of Pi question
kind of shed
some light on what his position is on
this which is that it's too early
theoretically and scientifically
for us to make that
that determination like as he said look
okay people have only been able to
calculate digits of Pi you know like
within the last hundred years or whatever
I'm not talking like we're not there yet
I'm not talking about that class of
problems I'm talking about these right
so I'm talking about these things like a
cat jumping up here and so some of
these examples that Gary Marcus may bring
up may fall into that even though they seem
simple okay they seem deceptively simple
they may still fall in the bucket of
these kind of much more
towards the discrete spectrum
of capabilities which
we can't currently do with
with our methods and of machine
learning but we're going to continue
and continue to get closer to that
that end of the spectrum
and so it's and so it's
premature to say that like
machine learning and again whatever
you can see that to be neural networks
whatever will never be able to get there
like and he and Jan is
optimistic that we'll be able to get there
I'm not as optimistic I think
there is a very there's
just a qualitative
difference between
structural topological
discrete reasoning and continuous
differentiable reasoning and I don't know
how we're ever going to get that gap
bridge he brought up some methods
to think about but
right but the key
question is whether it's possible
at all and
there was an article by a guy called Andrew
Yee I think and it really annoyed Jan
the Coon and he said look people are still making
the neural networks interpolate and in
that article he was basically
saying well neural networks are a little bit like
a Taylor series approximation you take a function
and you just kind of approximate
it you know inside a certain range and then it goes
haywire outside of the range
and Francois Relais came on the podcast
and he said look you know it's a little bit like in Fourier analysis
when you try and you kind of like
you take these little signs and cosines
and you fit it to a discrete function and it's
glitchy right and
actually if you look at harmonic analysis
this is what
Joan Bruner said on the show on the on the geometric
show he said that well a neural network
even the universal function approximation
theorem it's all about stacking these
basis functions together
right to kind of approximate
some target function well if that's
all neural networks do how could they possibly
generalize
look I don't want to use the word
like generalize I don't know that
what I would say is this which is that
no matter how
no matter how discreet in appearance
the human brain is
you know all the signals
that neurons generate receive
whatever you know at the end
of the day are
continuous functions I mean
they're you know a charge that has a continuous
function now somehow
or another like the brain in its structure
does take that
continuous analog
I should have said analog
you know probably there were but it takes that analog
computational environment and produces
digital reasoning
so I don't think it's beyond
like I just don't see how
anyone could reach the conclusion that it
will be impossible
like to do discreet reasoning
with with neural networks
I think it's just a question of like
for me I think is more of a question is that the
efficient way to do it like you know if
you fast forward 2000 years from now
when people have figured out all these problems
and we have like you know AIs
walking around killing us or working for us
whatever the thing may be
you know are they going to be using a neural
network for everything
or is it going to be a neural network
with you know some classic digital
compute components with some other stuff
like a hybrid kind of structural system
that does things I think is
more what I'm asking
um like Lacan
and they of course
that's what I was going to say they have like
nature on their side in that
in no matter how much we cut open
the human brain we don't find
like a discreet computer in there
like of course the individual neural
spikes are discreet like
there's either charge or no charge
but then there is like continuous
release of neurotransmitter
so I agree like the brain is like a very
continuous
distributed machine and there is no
no there's
no discreet thing in there
there's no part of the brain where it's like
one one
brain brainy packity
he's he said that
and there is a level at which that's true
which is
there's a spike train and you know we can kind of
recognize a spike because it's a
maximum in this
analog dimension but my point is
that's still an underlying analog
dimension and so I don't see why in principle
you couldn't build
a neural network that
you know has like these kind of
continuous values but still
ends up with something that
that synthesizes
a discreet
you know decision
sure but wouldn't it be glitchy and it still
wouldn't extrapolate I have no idea
like what a you know I just don't know
I think I think this kind of point is we're too
early on to reach these
conclusions that it cannot
be done I do sort of feel
that it's just not the efficient way to do
it that you know we're going to end up
with a case where we're going to have
systems that have
analog continuous chunks
that are neural networks or whatever
and we're also going to overlay that with
with
digital computation that's implemented
by the you know typical kind of digital
computer that's going to be these hybrid
systems working together just like you
brought up with hey look you know
alpha alpha zero alpha go
all those things are it can be viewed in the same kind
of hybrid way right
but that's exactly I mean
you said this is what you said and also
what when he when he said like
so first when he said you know we're
far away from that
with the digits of pi and so on but also
when he said you know humans are actually pretty bad
at chess or at discreet
exploration in general and that is
that's how humans do it right
humans build
discreet reasoning
on top of this
sort of neural continuous
function and it's actually really hard
like to do discreet reasoning
in your head it's
you can do it slowly
and you have to do it deliberately with attention
if you do like multiplication
of five digit number in your head
right this is not a this is not
oh my god feeling this is like
you like you sit there and you
you have to like keep all the stuff
it's not a hash table look up like the
multiply numbers on the table
like we can build children's toys
like that that for like 20 cents
that multiply 10 digit
numbers easily right
so I like yeah I think
this is the we teach children
both ways we teach them hash table look
up like memorize you know yeah
all the numbers from one to a hundred
multiply or something and then we teach
them an algorithm you know we teach them
the exact thing and we teach them
hash table look up and and
this is built on so so if you
and I think that is if you build
if you build this discrete reasoning
you can probably build it on top
of these models right
but yes is it the most
efficient way to do it
maybe maybe not
right but then again if you
do it if you
manage to do it train it you seem
at least from at least from
from the perspective of humans
you seem to have something
insanely powerful yeah right
because you know if you just have a
discrete algorithm you have the algorithm
but if you have a discrete if you've
actually managed to train a discrete
algorithm on top of this continuous
function it will be able to sort
of learn in the same
continuous way that you know a
neural network learns but you know
it will be sort of be able
to self modify its discrete
algorithm and that yeah I
think you know I
I agree I'd also be
in the maybe longer term
a positive that might be
possible but I think that's because I
said to him do you think
AlphaGo is a good thing do you think is a good
thing to have a kind of
discrete search
which is guided by a neural network and he didn't
like it and I assume he didn't like it for the
reason you just said Yannick which is that the
discrete part of it is hard coded
and it's been told to do a
specific task it's not learning
I think Lacoon would rather have
something which is entirely
differentiable right so because
Lacoon wants learning
end-to-end in a neural
network right and
and also it's almost like he was
saying before well
humans can't do discrete reasoning so
it would actually be a more sophisticated form
of intelligence if we had discrete reasoning
but do we necessarily even want
that in our models or is it just the fact
that he wants it in the models but he wants it
to be differentiable and learnable as part of
the main model I think
so because that's where he got with the
critic, actor critic type models
I think if people nowadays talk
about differentiable they always
talk about
they always talk about like single shot feet
forward like you know I input
something into the machine
and out comes a prediction whereas
he also makes a lot of arguments
for you know this
energy minimization
which means that
essentially at test time, at inference
time I do a minimization
algorithm and you know when
he talked about reasoning he was mentioning that
as an example as essentially
saying look I have my trained neural network
and now at inference time
I actually still perform an algorithm
on top of that to minimize some
energy function given my
trained model of the world let's say so
I think
we might be able to make that
more learned by
also learning
the algorithm that we do
at inference time but I don't
necessarily think
that he's talking about
you know we need to
end to end learn like a machine
where you simply input something and then out pops
through forward
props announced. All of these things
are just reasoning by the back door
even with actor critic as I understand
that's just the way of hacking
the advantage function right
I'm not an expert
in reinforcement learning but I think that's what it is
the same thing with the energy minimization
I kind of agree with you there
but the energy minimization has this particular
thing of this idea
which I think today is under explored
to actually do something at inference
time
not just forward prop
I feel it's
under explored
nowadays. I agree with you there too
but I think what kind of
sometimes I feel like this is feature creep
in the sense it's like
we have neural networks and we know what deep
learning is and some people now want
that to be redefined it's just a general
purpose research paradigm that includes
all possible things
that we can do with
machines or chemicals
it's like what use is that
what use is it defining
all types of
computation as
differentiable computation like we lose some
ability to talk about these in sort of
the same way that was kind of like what I was
saying you know nobody I mean look
allowing for a variable number
of layers in a neural network
that's
discrete computation
right that's not differentiable training
nobody knows how to train
like an arbitrarily unbounded
number of layers in some differentiable way
right but there's the neural
neural ODE stuff
well all those types of things run into
big training problems like
things become really hard to train
when you have these types of
essentially discrete
you know
transitions right or combinatorial
kinds of
transitions
and this is classic stuff I mean this is
really classic stuff it's like even
if you try to do mixed integer
optimization right so you have
a problem that has some combination
of integer variables
discrete variables and some combination of
variables there's all kinds of hacks to try
to do that in differentiable ways
they don't in general
arrive at the optimal
discrete
solution like you do
some continuous stuff and then at the end
you kind of discretize it in one way or another
and you wind up with a solution that's kind of
approximately correct but you're not
guaranteed that you're going to find the one that's
actually correct if you
discreetly combinatorially
examine that space right
but can we finish by talking
about a couple of things so first of all
there's table one
and the structure of the latent
table one is the biggest
the biggest argument
against the most
prevalent argument against this paper
sorry for the triple negative
but you know when
the paper starts out by saying
we build a convex hull of the training data
if your point is not in the convex hull
not interpolating and then people go
wait wait wait wait but you're talking
about the input space of data
you're talking about the pixel space and you know
for sure we're not talking about the
pixel space because you know
the neural network is you know the data
manifold but
we're talking about
you know if you go to the last
layer to the latent space before the
classification it's just a linear
you know there in that space
we're talking about like interpolation
right there is where the neural network
is interpolated and this experiment clearly shows
like okay it might be a rigid
definition of interpolation
but it clearly shows like no
even in that space on a
trained ResNet
there is no
interpolation going on as soon as you
pick like 30 dimensions or more
it's all outside
of the convex hull of training data
but this gets to the punchline
right because Keith was going to ask a really
cool question which is like well imagine
it was a very small latent and it was
interpolating and then what if you
up projected it like to one oh two four
dimensions and that you know would that
suddenly now be extrapolating right so
we almost need to have an information
theoretical way of looking at this but
anyway like my intuition is that deep
learning models encode the most
high frequency information into the latent
space right and you know this
information would be encoded in a
minimally distributed way to denoise
the predictive task which is to say
there's a few dimensions of the
latent which should be encoding
the actual things that you trained it on
so my intuition is actually
most of the dimensions of the latent
are kind of just encoding
low frequency information
so you can discard them
and I know you said the other day well
maybe the whole point of neural networks
is they're a distributed representation so maybe
they are distributed over all of the
dimensions in the latent
I would actually argue not that
neural networks are putting
the data in a minimally
distributed way but in like a maximally
I think just my intuition
is that if
I were a neural network and I had all of
these dimensions at my disposal
I would encode lots of information
redundantly
if it were too much space
I would encode the same information
pretty much redundantly
in many of the dimensions
that I could
but that's more noisy right so you're taking a softmax
and if you're noisily aggregating
over all of those features
you don't want to do that
yeah but still if I have backprop right
then the backprop path goes through each
of the dimensions so for each of the dimensions
I'm asking myself how can I change
this one to make my prediction
even better right
it doesn't matter if over there
one of the dimensions is already doing
the classification for me
the backprop, the nature of backprop
means that
I'm going to every single dimension
independently and deciding how can I change
this one to make it even better
so that's why I think lots of information
if the latent is too big
lots of information will be encoded
redundantly but that's the key right there
is if the latent is too big
and so this is where all the pruning literature
comes into which is that
the majority of the time people are
running neural networks and situations
where the latent space is too big
like they're just
we just flat out have far too many
far too large of a latent space
and so even though it may be encoded
densely there
it may be putting all kind of little bits
of extra information it's probably only
adding little plus or minus
0.1%
tight things to the accuracy
whereas and so
since it is only adding these little
very tiny values there
the only meaningful way to talk
about interpolation or extrapolation
because you've only got little bits
of stuff you're using on that entire dimension
that you've added there is really more
just are you interpolating on the
most salient dimensions
which is again back to my question
about why are we
why are we concerned
with whether or not every single dimension
falls within
within the sample range
yeah exactly but there's
a few things there I mean it's really good that you bring
up sparsity I think that's fantastic
that you brought that in because as you say
most of that information is redundant
but then what Yannick was
saying was interesting does
distributing all that information rather than
because my intuition is it increases noise
but actually I think Yannick's saying it increases precision
but in a very tiny
tiny like it's a tiny tiny
contribution so
let's suppose the latent space is not too large
right like if the latent space is
not too large in other words
it's let's say it's just barely big
enough to
to classify your images and let's suppose
you're doing multi-class so we've got
10 10 10 classes
you know and we just barely
got enough latent space for it
my intuition would tell me that
if those classes are somewhat
different from one another like it's not we're classifying
brown dogs
from white dogs from like every other
you know simple kind of dog but they're
different from each other and it's not easy
to determine you know which
is which that what would and just
to guess is it probably what would happen
is you would find that you know these
five bits are kind of
the ones that tell you whether something is a dog
and these seven bits tell you if
it's a duck and these
four bits tell you if it's a gun
and these five bits tell you if it's
an image of the sky like I would guess that
it would end up encoding it that
way since they don't have a lot in common
you can't really
use entanglement like too much
except for a few dimensions
there might be some overlaps but
yeah I think what
people mean a little bit
is when they when they start
going down the interpolation
road is that you know we've played
with GPT-3 as well right
and then you do something
with it you enter something
and in some places you're like
I see how you did
that right you like
you just took that newspaper article
and you just kind of replaced some stuff
in it right like you sort of
whereas if you were to
to talk to like a human
you'd sort of
like that stuff wouldn't
happen as much even and
of course there's the argument I think
people that say well it's just interpolating
or it might also be you know it's just
sort of repeating
the stuff in the training data
I think what they would like
to see is more like
the pattern that these
models extract
aren't sufficiently high level
for
for them right and then I think the entire
discussion is can we
get to
arbitrary
high abstractive levels
of pattern recognition
with
such models if we engineer them
train them in the correct way
or is there I guess
some fundamental limitation
to that
and yeah as we said
the answer might be
quite far off as for
the number of
latent dimensions and so on
I mean I agree
with Keith I think
having a big latent space
and also having big
weights and so on is
might be
more of a necessity
for training
than it really is for encoding
like it appears to help
if we have lots of weights to train
such that sort of we get
to combinatorically try out
combinations of weights
only few of which might
ultimately end up being important right
that's kind of a lottery hypothesis
sort of way of going down
so yeah I agree
most of the information
ultimately might be
only contributing a little bit
right but
my intuition
would be that you know this is kind of
because it's kind of redundant
information right because it's
like you know I'm encoding
this over here I'm also encoding it
in like a tiny little different way
over here
or some add on or some
uncertainty or some one training
sample is a bit different so I'm going to
put that right here
that's fascinating what you said about
Frankel's work though I never really
heard you articulate it like that but
it's actually kind of like a search
problem rather than a learning problem
what you're doing is you're giving it
all of the possible you know you give a
random initialization on a densely connected
network and you're saying you know just go
and find the ones that work rather than
create it from first principle
yeah that's why initializations are so
try like your initialization
essentially is
your buffet
for SGD
to like choose a good one from
to then go ahead and refine
but how much refining is it
is it more finding things
that already work versus refining
yeah it's well
the same but what it is not
is sort of
learning from scratch
that's what like people like
we cannot you cannot initialize a neural network
from zeros and then have it
have it learn well at least not
today maybe that's going to come
in some form but initialization
is actually pretty important
pretty crazy isn't it and that's
yeah that's like one hint that
you know we're not essentially
we're never essentially training from scratch
we're sort of training
we're sort of giving the choice of many
combinations of good weights
or of semi good weights and
it has to pick sort of the good ones
to continue exactly it's already
pre-trained you just don't know which
yeah I mean the argument
a little bit of the argument against that
is in sort of the evolutionary approach
where you say you know
you can make the argument you know humans
have sort of developed these abilities
to reason to recognize super high level
patterns while only having a continuous
brain
but then the other side
of this is yeah but it's not
like a single human that has achieved
that right it's not like one
single learning system
that has achieved that but it's actually like
this evolutionary system which is
in essence a massively distributed
combinatorical
trial and error search
right and that is that is
not a learning system so to
say as we imagine it today
at the end you end up with a learning
result but
the evolutionary algorithm
is way different than we imagine
learning it's not even all
it's not even all humans that have learned it
it's all
you know individuals of all tens
of millions of species that have ever
lived on earth that have
that have learned this right which is
pretty much like the lottery ticket
hypothesis let's just
let's just randomly train
crap tons of
you know weights and
and then slowly prune them and see what happens
right brilliant well
gentlemen it's been an absolute
pleasure I guess
I guess we'll make this a Christmas special
I mean it is pretty special
let's be honest so
yeah this is the end and a hat
we're gonna have to sign off
a nice meeting you
indeed but thanks for
bearing with us folks we have had a couple
of months off I've had a bit of a break
and you know the guys had a bit of a break
so yeah we're back
good to see you all again
peace out

Coming up later in today's presentation, I'm wondering at what point we're just developing
complex math models to explain complex math models, and we really haven't, you know, made
much progress along the interpretability axis. So you have something you don't understand and
you explain it with something you don't understand? If I have if I have some general formula, just
some very general formula, and then I go in there and I go, you know what, this formula has five
parameters. And if I make this one 0.75, and that one one third, and this one two, and that one zero,
and I call this the Megatron activation potential, and I go and write a paper about it. That's really
just an arbitrary selection of a bunch of numbers, and then you gave it a fancy mathematical passport,
and you got it published in some journal, and now everybody has to memorize that as, you know,
the Megatron potential and kind of learn about it. And that's a lot of what's going on right now
is that it's really just a bunch of hacking.
Welcome back to Street Talk. Today, we're going to be talking about interpretable
machine learning. Enjoy. Interpretability has become one of the most important topics
in machine learning. And it's something that every data scientist needs to be familiar with.
For hundreds of years, we've had simple interpretable models like linear regression
and rules based systems. But in recent years, there's obviously been a huge rise in more complex,
bigger, nonlinear models. And of course, predictions from these models are not always
so easy to explain. So as we start to use these more powerful nonlinear models to actually make
decisions on real world matters, then it's inevitable that our attention must now turn
to interpretability and explainability. When I first started learning about machine learning
algorithms, I was told they could be dangerous. They were hard to understand. There were black
boxes. But as Christoph lays out, it turns out there is a whole plethora of techniques out
there to explain why a model made a certain prediction. Some models like low dimensional
linear regression are intrinsically interpretable. You can just look at the model coefficients,
and that tells you exactly how the model is working under the hood. Then there is a whole
suite of methods that will actually work with any ML model, like training a local surrogate
or a global surrogate. There's also Shapley values, which is a really cool technique that
allows you to distribute blame for the prediction amongst the input features in a really theoretically
sound really principled way. And then there are domain specific methods. For example, to explain
image models, you can try to highlight the most relevant parts of an input image by making saliency
maps. And there's more. You can look at things like example based explanations, where you try to find
the smallest change in the input data that would cause the output prediction to change. So maybe
with this awesome new interpretability toolkit, we can start to dispel that myth that machine
learning models are all just black boxes that can't be understood and can't be trusted.
Christoph Mulner is one of the most important people in the interpretable machine learning
space. In 2018, he released his Magnum Opus, interpretable machine learning, a guide for
making black box models explainable. Interpretability is often a deciding factor
when a machine learning model is used in a product, a decision process, or research.
Interpretability methods can be used to discover knowledge, to debug or justify a model and its
predictions, to control and improve the model, to reason about potential biases in the model,
as well as increase the societal acceptance of models. But interpretability methods can be
quite esoteric. They add an additional layer of complexity and the potential pitfalls require
expert understanding. Machine learning models are inherently less interpretable than classical
statistical models, but typically they have a better predictive performance, and that's because
of their ability to handle non-linear relationships and also higher order feature interactions
automatically. But do we have to suffer this implicit trade-off between the complexity of a
model and the lack of our ability to understand it? Simplistic model approximations can often mask
important information and be misleading as a result. In classical statistics, there's an
entire field called model diagnostics to do exactly this to check that assumptions and simplifications
have not been violated. This is something that does not yet exist in interpretable machine learning.
Interpretability has exploded and matured in the last few years, in particular since the deep
learning revolution. We now have a better understanding of the weaknesses and strengths
of interpretability methods. A growing number of techniques are available at our fingertips,
but can lead to the wrong conclusions if applied incorrectly. Is it even possible to understand
complex models or even humans for that matter in any meaningful way? That is one of the questions
that we're going to be discussing this evening. Molnar also recently released a couple of papers
where he discusses some of the important pitfalls of interpretable machine learning methods.
So some of the things that Christoph Molnar is really concerned about is the lack of statistical
rigor in IML methods. Molnar used to be a statistician. Also, he is exasperated with some of the
misguided causal interpretations from some of these IML methods. He also points out feature
dependence or situations where you have shared information between features. It completely
breaks many of the IML methods and this is something that he focuses on a lot. He also
focuses philosophically on the broader impact of interpretability and what interpretability even
means, frankly. It's a very nebulous term. So let's have a quick flick through this paper,
interpretable machine learning, a brief history, state of the art and challenges,
and as well as pointing out some of the history of IML methods, we'll jump straight into one of
the challenges, which is feature dependence. Molnar points out that feature dependence makes
attribution and extrapolation problematic. This is exactly what happens in partial dependency
plots, for example. We are basically extrapolating and we are creating fictitious data points that
didn't really exist and these fictitious data points probably exist outside of the data distribution.
So Molnar thinks that the models that we build should reflect the causal structure in the world,
but of course that is not really the case most of the time and he points out that statistical
learning is just reflecting surface feature correlations, not the true causal structure
beneath the scenes. Causal structures would be more robust if we could actually capture them
and the predicted performance and learning causal factors is a conflicting goal,
which I think not many people have thought about. So we need to think about when we can make causal
interpretations and a lot of work is underway in this field, but being completely frank,
this is very nascent. There's not really much out there at the moment.
Molnar also points out this lack of statistical rigor. Having been a statistician himself,
he was exasperated when he came into the IML field just to see that most IML methods do not even give
you confidence estimates, something which is completely standard in the statistical world.
Models and explanations are computed from data, which means they are subject to uncertainty,
but this is something which is just not captured using current methods. He says that we need to
be making distributional and structural assumptions. He points out this risk of p-hacking, something
which is prevalence in the natural sciences. This is something that could be coming to the
world of IML very soon if we don't start thinking about this more carefully.
Molnar also points out that there is no accepted definition of interpretable machine learning
methods. It's not entirely clear how we can compare IML methods to machine learning models.
It's really easy to assess machine learning models because we have benchmarks and we have
ground truth labels. Those benchmarks are fraught with problems as well,
but we can't really quantify how correct an explanation is. It doesn't really help that
there's a taxonomy of interpretability methods. There are objective methods like sparsity and
interaction strength, and there are human-centered evaluations from domain experts or from lay people,
and quite often you need to have quite a lot of technical knowledge to even understand these
assessments. He says that the setting of machine learning is too static. It doesn't reflect how
these models are actually used in practice. I really love this idea of thinking about a process
rather than thinking about just the model. He says we need to have a holistic view of the
entire process. He thinks that we need to think about how we explain predictions to folks from
diverse backgrounds, how we have interpretability at the societal level, or at the institutional
level, thinking much more broadly than we are at the moment. He also thinks that we need to reach
out to other disciplines, for example psychologists and social scientists, and he thinks that there's
lots of rich knowledge in computer science and statistics that we're just not using yet.
So in July of last year, he also released this paper, pitfalls to avoid when interpreting
machine learning models. In this paper, he points out that there's a growing number of
techniques providing model interpretations, but many will lead to the wrong conclusions if used
incorrectly, and he goes on to point out many of those pitfalls. For example, the first one is
assuming that the model generalizes well, so assuming that the model has been fit correctly.
If the model is underfit or overfit, then the interpretation method will perform badly as
well. An interpretation can only be as good as the model underlying it. So the next pitfall he
points out is the unnecessary use of complex models, which is to say the use of opaque or complex
machine learning models when an interpretable model would have sufficed, which is to say when
the performance of an interpretable model is only negligibly worse than one of these black box
models. And to be honest, this is something I see all the time. I think the gratuitous use of
complex machine learning models is something which is really serious. One of the things I don't
like about machine learning is the laziness. I think we should always seek to understand
and simplify problems wherever we can. It's the same thing in software engineering. We should
always be trying to create the most elegant and simple and maintainable solution. We shouldn't
be trying to over complicate things. And I think that's a very, you know, the kiss principle is
very generalizable here. So he recommends to start with simple interpretable models like
generalized linear models or lasso models or additive models, decision trees or decision rules
and gradually ratcheting up the complexity as required. So he also points out that ignoring
feature dependence is super important, right? And this is a problem that many of the IML methods
have. So he gives an example of partial dependency plots where they extrapolate in areas where the
model has little training data and it can cause misleading interpretations. So these perturbations
produce artificial data points that are used for model predictions, which in turn are aggregated
to produce global interpretation. So that's a big problem. Another thing he points out is confusing
correlation with dependence. So he gives an example here features with a Pearson correlation
coefficient close to zero can still be dependent and cause misleading model interpretations.
While independence between two features implies that the Pearson correlation coefficient is zero,
the converse is generally false. So it's a pretty cool example here. This is a couple of features
that absolutely have a dependence on each other. You can see it visualized here, but you wouldn't
know that if you looked at the Pearson correlation, it would have said that it wasn't significant.
Another one, misleading effects due to interactions. So there's a couple of things here. There's the
partial dependency plots on a couple of dependent features. And then he's used a simulation to kind
of trace all of these different features to see what the predicted label was. And according to
these IML methods, there is actually no clear dependency between these features and the predicted
outcome. Whereas you can see that that's just blatantly false. So something I've been meaning
to do for more than a year now is to go through Molnar's interpretability book and to make some
bite size videos on every single approach. Well, Connor and I are actually going to do that over
on Machine Learning Dojo with the first one next week on Shapley Values. So make sure you
subscribe to Dojo and check that out. Remember to like, comment and subscribe. We love reading
your comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk
YouTube channel and podcast with my two compadres, Connor Tan, who runs the Thomas Bay's
Appreciation Society and MIT PhD Dr. Keith Duggar. Now they say that Germans are known for beer,
sausages, precision and these days, interpretable machine learning. We have an exemplar German on
the show, Christoph Molnar. Now Christoph made waves in the community when he released his Magnum
Opus, interpretable machine learning, a guide for making black box models explainable. If a
machine learning model performs well, why don't we just trust the model and ignore why it made a
certain decision? Well, the problem is that a single metric such as classification accuracy
is an incomplete description of most real world tasks. Now, as according to Doshi Values and Kim
in 2017, in Christoph's book, he introduces the importance of interpretability and reports an
incredibly detailed taxonomy of interpretability methods. And his style of writing is at times
entertaining and entirely absent of hype and nonsense. He runs the gamut of interpretability
models. So for example, model agnostic methods like Lyme and Shapley values, example based
methods such as counterfactual examples and adversarial examples. He motivates the importance
of interpretability methods, but he's also extremely transparent about its current weaknesses
and pitfalls. He's currently finishing his PhD in interpretable machine learning at the Ludwig
Maximilians University in Munich after getting a stats masters from the same institution.
He's recently written several very interesting papers on interpretable machine learning,
for example, pitfalls to avoid when interpreting machine learning models in July of 2020,
where Christoph detailed several problematic model interpretations, for example, ignoring
estimation uncertainty, feature interactions or confusing correlations with dependence. More
recently, he published a paper called interpretable machine learning, a brief history state of the
art and challenges. While he acknowledged that the field is maturing nicely, he also spoke about
some of the serious challenges in IML methods such as the lack of statistical uncertainty,
shared information between features, lack of a clear definition of interpretability,
and the need for a more holistic view. Christoph Molnar, it's an absolute pleasure and welcome
to the show. Thank you very much for the invitation. Glad to be here. You know, Christoph, I have to
say I really enjoyed your book. I read this actually some months back in preparation for a
completely different show. I loved how scientific it was. You know, it was very much laying out
essentially a survey of the facts, a lay of the land, very objective evaluation. It had both the
pros and cons, you know, of different approaches, examples to make them, you know, more understandable.
So kudos to you. I thought it was a great book, very enjoyable and very informative. I also loved
how it lays out the beginning, you know, what the goals that we're trying to achieve with
interpretability are, especially kind of the human goals, right? Like what does it mean for
an explanation to be good for people? What kind of explanations do people like? And sometimes there
can be conflicting goals there. And I think one thing that I realized from reading your book is
that actually explanations can be deceptively good. Like I think the sort of cognitive bias
maybe that we have to look for contrastive explanations or counterfactual explanations.
Like in principle, it seems good. It's kind of like, you know, I'm sorry, we can't give you this
loan. You know, well, why not? Like why can't you give this loan? Well, what we've detected really
that you're a deadbeat. What do you mean I'm a deadbeat? Yeah, you know, you never pay your bills.
Well, let's see why. Okay, let's look through this and we find a decision tree here and
some big decision tree and we get to this one little point. Well, it says here, you know,
you didn't pay this furniture bill back in 2018. You know, if only you'd have paid that furniture
bill, like we'd be able to give you the loan, right? But the truth isn't that simple. Like it's
actually buried all throughout throughout the decision tree, right? With so many contributing
points. Yeah, I think like this chapter that you referenced was about like kind of from the social
view or the human view, what what people like or prefer as explanations. And the whole chapter is
based on, I forgot the title of the paper, it's like from Miller, about like, what we can learn
from the social sciences about what a good explanation is. And was like a paper where I
learned a lot and was super interesting also to see how like what people think are good explanation,
as you mentioned, they should be contrastive, they should be short, but they should also
confirm to some prior knowledge that the people have. And I mean, like, objectively, a lot of
those things might not, like, you wouldn't say these are good explanations in some sense, like maybe
maybe it's not good to give an explanation that fits with a priori knowledge,
because it's not the correct one, maybe. So it was quite interesting to learn and to think about
like what's the human side of it. That's a very cool part of your book, I thought, the fact that
it's actually quite interesting thinking about what we really want out of an explanation.
I remember, first of all, looking at, you know, sharp values that are very fair and will distribute
the blame equally amongst all the different relevant features. And then you turn to something else,
like, you know, selective interpretations, that in a way are way less good because they're kind of
arbitrary, they'll just select a few, a few, a subset of the features and give them all the blame.
But then it turns out that apparently that's what people actually want as a useful
intuptable explanation. Yeah. So as I see it as like many, many dimensions of explainability or
like what, what can be a good explanation. And one of these dimensions is maybe sparsity,
that you have a short explanation with just a few facts. That's something that people prefer, maybe.
But this might be in conflict with other dimensions of a good explanation, which should be maybe that
all causes should be addressed by the explanation that plays some role, at least.
But this is, of course, in conflict with sparsity, if you want this full attribution,
like you get with Sheppley values, for example. So I, that's why I also think there's not like
just one correct explanation, but there's like many attributes or many dimensions on which
you can judge explanations. Yeah. I think this is one of the problems, because even machine
learning is really difficult, right? Because we use benchmarks. And benchmarks are just
something that people have come up with. But you say in your, you talk about one of the problems
being that there's no definition of IML methods to start with. But at least in machine learning
methods, we have ground truth, which is, which is significantly better in a way. But if we,
if we can't quantify how good an explanation is, then where are we really? Because you talk about
a kind of taxonomy of interpretability methods, right? You say that there are objective evaluations
like sparsity and interaction, strength and fidelity, and humans, human-centered evaluations,
you know, which might come from domain experts or laypeople. So I suppose you're just hitting
straight on the fact that this is actually quite nebulous, isn't it? Yeah. So, yeah. So in some
sense, like, there's this big criticisms of, okay, this is not scientific or not well defined,
at least what interpretability is, how can we even do research in this area? But I have a bit
more relaxed view. I mean, otherwise I should have stopped writing the book before I really started.
So I kind of see like this, this endeavor of giving interpretability or bringing interpretability
to machine learning, it's more like a, first of all, it's just a keyword. So it's, it kind of
bundles all the methods together, that kind of aim to reduce this high dimensional function to
something, well, mostly it's something in a lower dimension. So we kind of just do this mapping,
something gets lost in a way, this is fine. And it's, I think, part of science to find out like,
yeah, some part of analysis to find out what part gets lost. So when you, for example, look at just
some feature importance values, for example, of course, it's a summary of your model,
and a lot of information gets lost. But I still think it's useful to have, obviously,
so many people use it, but it's useful to have these tools, and we just have to understand
what they do, and how to interpret the results. So how do you interpret when like the feature
importance is zero of a feature? Could that be quite dangerous, though? Because you gave the
example of random forests, when you have a lot of shared information between the features,
it would actually tell you that these correlated features have a higher feature importance than
you might otherwise expect. So does this imply that we need to have very detailed knowledge
of how we should, how we should use this information that we get from IML methods?
Yeah, so it's also kind of the direction in which I write papers like this pitfalls
to avoid and stuff like this. So I think, so these are just tools, so they do something with the
model A, kind of distill some knowledge. So for example, for feature importance, you kind of
measure how well does your model perform, and then you measure again after you shuffled one of the
features. And then you get something out of it. So then we can ask questions, is this
interpretable or not? And it's kind of, well, not so relevant, the question, because you just have
to understand what happens when you shuffle feature. And one is, for example, you kind of break the
association between the feature and the prediction, because now it doesn't carry the information
about the target anymore, because you're shuffled randomly in your model. So kind of this feature
importance now measures how much performance you lose because of this break of information.
But then you also, when you think about this method and want to use it, you also have to
understand that this shuffling, for example, breaks also association with your other data
like the features in your data. So this is a limitation of the method. And
what I think is needed is that we understand in which way these methods break or in which
scenarios we're allowed to use them or how we are allowed to interpret them. And I think the
situation is kind of similar to statistics where you have these models and then you interpret
like the coefficients of your models, you still like have to learn how you do the interpretation,
what are the assumptions that have to be met that you're allowed to do this interpretation.
And I think we're in a similar situation here with interpretability of machine learning.
And I'm glad you mentioned sort of the old school linear models as well as dimensionality in the
thread, because you make a very good point in the book, which is, look, even these so-called
intrinsically interpretable models are only interpretable up to a certain dimensionality.
And I have tons of experience with multi-linear regression. And I can guarantee that beyond
a very small number of dimensions, those coefficients are not interpretable because it
starts to play a bunch of games where it's inflating one coefficient and another because
their difference is important and whatever else is happening. A lot of correlated structures are
all essentially getting compressed into the small number of weights. And so as the dimensionality
goes up, I would say like no model is intrinsically interpretable. Same can be said of decision trees.
Like anybody who's looked at a decision tree that's come from real data, you're going to find out it's
not interpretable. It's like, oh, look at this, you know, market capitalization matters. Oh,
and it matters over here too. And down here. And actually, I have to go through five checks on market
capitalization before I get down to this decision. And maybe the features aren't that intuitive either.
And then you have to kind of like mentally stack up like until you get to the decision,
like five decisions. And then you have a very complex rule that led you to the prediction.
Yeah. So I agree that there's, it's not like, I mean, I have, I have this distinction, it
looks like interpretable models and not so interpretable models. But as you say, it's like a gray,
like it's a scale really. People can definitely overhype how interpretable these white box models
are, right? Whether it's linear models. As I'm a, I've worked with many physicists who have had
guidelines that you should only ever use models like a decision tree, because it's possible in
theory to write down on a piece of paper exactly how the decision is made, right? You can trace
every decision. But that's never actually useful in practice, is it? Since when have you ever looked
at a decision tree fitted to data that's for any complexity? And the fact that in theory,
it's possible to go and examine how it works, it's completely irrelevant in practice, isn't it?
I mean, it can be useful to have like a short decision tree sometimes. But in practice, it will
not like give you probably the best predictions. But it might be useful sometimes to shorten it
artificially. So even like, you're throwing away some predictive accuracy, but you shorten it.
So you understand that somehow it's manageable. You can have a look at it and see what's going on.
Well, there's also, you know, the other issue is that as I was looking through a lot of the
methods that you describe, and you survey in your book, you know, some of them are not simple.
I mean, if you start looking at partial dependency plots and trying to explain what those are,
I mean, you know, you have to almost have a deep mathematical knowledge really to appreciate them
in the first place. So I'm wondering at what point we're just developing
complex math models to explain complex math models. And we really haven't, you know,
made much progress along the interpretability axis.
Yeah, yeah, that's true. It's also like the criticism to like, so you have something you
don't understand, and you explain it with something you don't understand. I think some methods are
complex. But for some, at least some intuition, how they work for partial dependence plot is
kind of your this intuition that you do some intervention on your more intervention on your
data. So you replace all your like for one feature, you replace all the values with one fixed value
and kind of look at the average prediction that you get afterwards and do this for a lot of points.
And then you connect the points and you have this curve. So kind of gives gives you the expected
change over the feature range. Maybe there was already a bit complex, I don't know. Maybe I'm
too deep into the method already. But yeah, of course, it's something additional people have
to learn if they agree to use it, of course. Could I get a quick take from you on saliency maps
as an example, because you said in one of your YouTube videos that saliency maps are glorified
edge detectors, they are not good explanation at all. And I've noticed now that many machine
learning platform providers are building these kind of saliency maps into their models,
you know, into their platforms. And then it becomes a kind of box ticking exercise where you
can say, okay, well, yeah, we've done interoperability now, that's all you need to know. And that really
is quite a false sense of security, isn't it? It's funny you mentioned the saliency maps because
I'm writing a book chapter about it. And actually, I wanted to publish it today, maybe I will,
or at least in the next few days. It has been a long time in the making. And it was very,
very frustrating, like by far the most frustrating chapter to write. Number one reason is because
there's so many methods out there. Reason number two is I can't judge really if they work. And it
seems like they mostly don't, or it's still unclear like how you say you would judge that they work.
So they're like dozens of these like integrated gradients, gradients, deconfnet, deep tailor,
decomposition, layerwise relevance, propagation and 10 variants. So and then I mean, you and the
answer you when you apply these methods on the awful like for image classification, and you get
these nice looking images and some areas are highlighted, some are not. Sometimes you can say,
okay, this doesn't make sense at all. But if it kind of makes sense, then you maybe would be inclined
to trust the method. But then there's this paper, which is called sanity checks for saliency maps.
And they kind of found out that they, most of the methods are very similar to edge detectors,
meaning that they are kind of insensitive to the model and the data, which is very bad, of course.
Well, if you change the model, the explanation should obviously change.
Could you expand on that a little bit? So you said it wasn't really a reflection of the model or the
data, but what would a perfect saliency map look like? Well, I don't know myself actually. So I mean,
the ideas that you sort of basic idea of most of these methods is that you, you have your class
prediction or your class score, and you want to back propagate it not, you want to back propagate
it to the original image. So you look at the gradient with respect to input pixels. And that's,
there's no not not one way to do this, but then many different ways. So that's also why we have
so many different methods. And then they highlight which pixels were relevant for the classification.
But yeah, they, these, these methods, they have like a lot of like issues, for example, there's
the issue of saturation, for example, because of the real unit where you have flat parts of the
gradient. So if you pass the gradient through that, then your method would say that some,
some neural might not be important at all. And there's a lot of these little issues that these
methods have. Yeah, so, but, but back to your question, like, I think that's also the issue
that I don't wouldn't know how to answer. I mean, obviously, it should be some area that
should be highlighted on the salience method was important for the, for the neural network.
But then again, I don't know how the network decides. So I couldn't like, if I see an image,
I couldn't like highlights the part of, I mean, I could highlight the part where I think the network
should look. But then again, I mean, there are lots of papers like the Cleverhans paper, which
saw like the reveal that there are some, sometimes it would look at watermarks on the photo.
So these are like these things that we just don't know what the neural network basis is on.
If I could take a stab at that answer, I'd, for one, I think just the idea of quote, a saliency
map is a problem. Like there isn't one map of the importance of the pixels. It's like they're,
they're operating on multiple, multiple dimensions or at least sort of multiple feature sets. It's
like, if you ask me to tell, you know, why is this image a dog? You know, well, for one thing,
it's, it's the overall shape, you know, it has four legs and, you know, two ears sticking out over
here. That's one saliency. Another is that it's, it's got a certain color, you know, and it's
coat. And that's, that's a different concept of what's salient. And another is that there's a frisbee
flying at it and its mouth is open and it's about to catch it. And I know dogs do that. So there are
kind of, you know, when your mind analyzes an image, it breaks it down into these many large-scale
kind of structural features. And I think that gets completely lost and most of the approaches
to saliency maps. This is really important point, actually, because if you're just looking at the
pixels on this kind of 2D planar manifold, that's only a very, it is quite literally a surface view.
And I think Christoph, you said that there are all sorts of causal structures and even in the
model itself, right? There are these entangled neurons. And surely that's giving me more insight
into what's actually happening. Just seeing a bunch of pixels. And the other thing is that these
models that they are completely lacking in robustness. So probably if you changed a few of the
wrong pixels, your saliency map has just got completely broken, right? So, but in that vein,
some of these feature visualization techniques, you know, like the deep dream type stuff,
maybe, maybe that's a better way of interpreting these models.
Yeah. So like for the one point you mentioned about the adversarial examples, so there's also
a paper I forgot to title again, which manipulated neural networks, so they would give the same
prediction for all the images, but different explanation like different saliency maps. So
this is perfectly possible to create different explanations for these saliency maps,
but keeping the model, like at least for the predictions, the same.
There's another criticism you can throw at saliency maps, where they can be quite deceiving.
You think they're useful and then they turn out not to be useful. There's a classic example of
looking at, you know, comparing a dog to a wolf and sometimes you see it's looking at the snow in
the background and that's helpful. Sometimes it highlights the animal and you think, okay, I understand
it's looking at the face. That's why it thinks it's a dog because it's seeing the face. And then you
look at the predicted class for something else like, you know, a cat or a frisbee or a house or a
boat and it highlights the face as well. So the saliency map for all these different classes
looks the same. And when you realize that, you realize this saliency map hasn't actually told
you anything about why it's gone for one class versus the other. All it said is that it just
highlighted the thing in the middle of the picture. Yeah, I think that's especially also when you
look at images, you know, like we're very good with images. Like we were very quick to see what's
happening on a scene and such. So I think we're also very quick to make judgments. Oh, yeah,
this makes sense. This doesn't make sense. It's more difficult to interpret, like if you have
like a graph and there's like things going on inside, you have to like now understand what
the method does and stuff like this. But for an image like a heatmap, I have this area as
highlighted makes sense case closed. I like the method. Yeah, and that gets exactly back to the
deceptively deceptively good explanations problem and explaining complex things with complex things
we don't understand. And it's I think a lot of people if they looked at it, and again, one of
the points of this interpretability is really the social aspects of it, right? Like being able to
convince people to be at ease with machine learning models or to accept the results of
of a machine learned, you know, decision process. And I think if somebody looks at an image of a dog,
you know, they have no problem understanding that. But if you showed them a bunch of salience maps,
or or any of the other sort of, you know, feature projections, if you will, like you said, it takes
a lot of deep understanding to understand those words, the image is kind of immediately obvious.
I think two of the main themes that you touch on is we'll get to the to the probabilistic stuff,
the Judeo Pearl stuff in a minute. But I think the main issue that you point out is feature
dependence. Okay, and you say that when you have feature dependence, it makes attribution and
extrapolation problematic. So a dependence just means you got correlated or shared information
between your features, right? So you say that in feature permutation methods, these things basically
break everything when you have the shared information, and the extrapolated data points are no longer in
the distribution. And you say that there are conditional permutation schemes, you know, that
try and maintain that joint distribution, but those things sometimes make it even worse, right?
So do you think that's one of the most important things that people should think about when using
IML methods? Yeah, at least so. That's at least like a very deep issue, I would say, which is
inherent in most of the model agnostic methods, where you manipulate your data, see what how the
model prediction changes, and then create your explanations out of this sort of select the
shadowy value line, partial dependence plot feature importance, they all work with this mechanism of
manipulation of the data prediction, and then kind of aggregating the results.
And most manipulations happen in isolation, so that you, for example, when you, for feature
importance, you can meet one of the features, as I like said, sometime before. And then,
well, you break the association of target, but also a few other features. But similar things
happen if you use lines, or you kind of replace parts of your image. But then again, you also
have to replace it with something like, which is I think in line, the defaults with just a gray image.
And then of course, it's not like it's outside of your data distribution subtly, because your
network was not confronted with like these patchy images before they had like,
just normal photographs usually, and depends on your neural network, but you certainly didn't
train it on images where parts were grayed out. So it's pretty likely what the model should predict
and what will predict at this point. But you use these images to create your data set, like you
send it for the neural network, you get predictions, and you kind of aggregate from this your explanation.
But you left your data distribution and your model can do anything.
And the hope is that it doesn't do anything crazy, but yeah, you don't know.
A simple example from the medical field would be that height and weight are highly correlated.
And on the other hand, the ratio or some relationship between your weight to your height,
that actually has very important medical consequences. That's the measure of health.
And so if I were to sit there and just permute, say, the height index and create a whole bunch of
people that had all these bizarre combinations of height and weight, you know, first of all,
those don't even probably exist in the data set. And the ones that do exist in the data set,
probably had some medical issues, right? Well, you actually gave a similar example,
I think you gave the example, Christoph of a baby that earns $100,000 a year, which is,
which is insane. But when you talk about something like lime, maybe that's different.
Because the CNN, you know, it shines a flashlight over the input space in it. It's a kind of local
method. So in some sense, you could argue that it doesn't matter that you've grayed out all this
other stuff. Because if the model was sufficiently well trained in the first place, it should
hopefully learn to ignore the background. Or is that just wishful thinking?
That's an interesting thought. I haven't thought about it because like the property of like that
you have these filters that trust a wander over the image. Yeah. Maybe it would make it more robust
for these kind of interventions that we do when we create these images with lime and shetley.
Yeah, I haven't thought about it. It could be.
Well, we've mentioned all of these ways in which interpretability methods can go wrong, right?
How the model might not be a realistic, the interpretability model might not be a good
approximation to the actual ML model. So some people a bit controversially, perhaps take the
idea and remember that and say, you're just barking up completely the wrong tree. And you
should just give up using interpretability, interpretability methods to explain these black
box boxes, this dish them instead, just use an interpretable model to begin with. Use a white
box model. I think there's an example from Compass in the US, which is that model to predict
reoffending. And I think quite famously, there was a investigative journalists that tried to
interpret this model. It was a black box model because it's proprietary, right? It's a trade
secret. And they fitted a proxy model, a kind of a linear model. And they made a report saying,
okay, we think your model is racist, because it looks like it's, it's taking race as a factor.
And then some further work was done. And they came back and said, well, actually, you've just used
a interpretability model that doesn't really fit our model very well. You've made some assumptions
that don't hold. If you use a different interpretability model, you get a completely different
answer that it doesn't use race at all as a factor. And so you've got to kind of a,
you've got to a wrong assumption by using a bad interpretability model. And I think they were
saying that this model, instead, you could get just as good a model of reoffending with like
three FL statements, you know, ditching this massive complex, 100 and something features,
and just use three FL statements based on, I think, age and reoffending. So is that, was that
what we should do? Should we just drop these methods and start using white boxes instead?
So, I mean, like one, one thing to mention here is that white boxes very soon also,
like a gray or black box, if you add interactions, if you have many features and so on. But putting
that aside, I would agree if you're in first place, like that you say, you should start with
like a white box. So if you start modeling, then then you then you should consider these first,
like maybe they already solved your problem that then it's perfect. And you have a model that is
quite, I mean, stable, it's interpretable. I think that would be great. But then I think the
next step would be to see like what like a black box or machine learning model would give you in
terms of performance. And then maybe if you see the gap is really big, then maybe you can try
some feature engineering and close the gap maybe from the interpretative model to the machine learning
model. But then you're probably already infusing some features that are not so interpretable,
or maybe if you're using a linear regression model, you're maybe using then splines and
interactions. So you're already moving towards more complex models usually. But then if you still
have a gap, then I think you have to decide is the gap and predictive performance,
like worth changing to a black box model. So I think that's your decision will be different
in many cases. As you relates back to the point you made on your paper about criticisms of using
interpretable machine learning models, that some people leap straight away to using an overly
complex model. And sometimes, depending on the situation, sometimes, you know, a linear model
can do just as well and have all these advantages. It's so much easier to explain. Do you have a
philosophy from a high level here, right? Because if it were a human, if it were an airplane pilot,
we don't really understand how the brain works, right? We would just test the pilot. You've got
to fly the plane for 10,000 hours. And if you don't crash, then we'll let you fly. So we don't
really seek to understand how his or her brain works. But with machine learning models, there's
this continuum, right? So if you use these complex black box models, the predictive performance is
usually better, but you're trading off understandability. And assuming those things are completely
mutually exclusive, what kind of decision process do you go through when you select these models?
But by the way, with machine learning, right? The reason why we use machine learning is because
we don't understand how to do something explicitly. Yeah, is that a fair statement?
Yeah, I would say when when it is like so high dimensional, so complex, many interactions and
so on, that your simple models don't cover the complex cannot cover the complexity. I think
then you need machine learning. Would you rather understand exactly how the plane worked? Or would
you rather I mean, if I was saying to you, you can go and find a plane. Would you rather that
you understood how the plane worked? Or would you rather that the plane was tested? Why not both?
So I think we can do both and to some degree. So of course, with black box model, we don't
exactly understand how they work. But in comparison to a pilot, we can test them for free, more or
less. So because I mean, maybe it's not as good as an interpretable model. But we still can use a
lot of methods to at least approximate and try to understand a few properties of this model.
So I think we are even in a situation where we don't have like these complete like A or B decisions,
but we can have so if the machine learning works much better, and it's like really robustly tested
with lots of different data, I would prefer machine learning model, I guess. But then I would also
want to like people to to apply all these methods that are available, even if they are not perfect,
but still they give you something, they give you some insights. So yeah, and I think so Tim,
one answer to your question is that a lot of people's response here and kind of demanding
interpretability and having concerns about machine learning, it all comes down to generalizability.
And we've seen through using machine learning that it breaks down in ways that we don't like,
like for example, sure, maybe the soap dispenser, you know, is really great at dispensing
soap, you know, 87% of the time, but it just so happens to kind of be a race sensitive soap
dispenser and just doesn't give any soap to people with a certain skin color. Like we've kind of
decided as a society that there are certain generalizations or certain dimensions along
which our models just have to perform. And also because a lot of these things that break machine
learning models are things that happen quite, quite regularly in the real world. It's like a pilot,
you know, a human being pilot flying around, if he looks down at the ground and sees a hot
air balloon with a big smiley face on it, he's not going to crash the plane. He's just going to be
like, Oh yeah, I forgot about the hot air balloon contest that's going on today. Whereas a machine
learning model, if it looks out a camera and sees something with a particular shape of lightning
bolt, you know, it might just decide it's time to like dive for the ground, right, and crash the
plane. Like that's sort of what these adversarial examples kind of show. And I think that's why
people are really hungering for human understandable explanations, because still to this day, the human
brain is the only AGI really that we have around. Yeah, but deep learning models that they, they
essentially memorize lots and lots of things and they have this sparse coding. So in a way,
it's just like the white box model, even if we use interpretability methods, we could enumerate
all of the things that they are learning. And one of those things might be a sensitivity or
lack of sensitivity to hot air balloons or smiley faces on. But even if we could enumerate all the
things that they are learning, we wouldn't understand that either. In the same way, we don't
understand how a real human's brain works. And I'm not sure whether we should view a human brain
as a computer program and whether that's a good thing or a bad thing. But at some, I guess what
I'm saying is at some point, we have to accept that we're not going to understand.
Totally. It's such a good comparison, I think with humans, you know, if you're interviewing and you
want to hire, let's say a software developer, you tend to set them a coding interview, you wouldn't
think of taking them into surgery, opening up their brain and trying to find the neuron that
predicts what the next, you know, bit of code is going to be and understanding how that works.
It's just, why would you do it that way? Instead, you learn to trust humans by working with them,
giving them a test, seeing how they perform in the real world. Maybe we're asking too much
of a machine learning model if you want to be able to understand these complex things in terms of
like a bottom up white box set of rules. I think what a comparison falls a bit short is that
we have the luxury that we can cut open the brain of a machine learning model
without breaking it and without hurting it, hopefully. And we can do all these,
try out all these interpretation methods, see how it behaves under certain situations.
And I also would make a distinction between we understand what's going on inside and doing
like this kind of sensitivity analysis, where we just try out what happens in certain scenarios.
So we always do that, that we check how it behaves. So feature importance is basically
like a way to see how does it behave if we break some features and then we rank the features by
this as an importance. We can do it and that's also the big difference between humans because
we can't test in the same way and I shouldn't probably. I think there's one other difference
though, which is to do with the substrate of how neural networks work. I think if I'm giving
someone a job interview or something, I mean, of course, it's a very valuable process, but I'm
looking at their values and I'm looking to try and understand how they would behave in
different situations and I'm coming up with lots of illustrative examples. But the difference is
with humans, we have that level of generalization. We have a kind of guiding taxonomy of behaviors,
which means if I know, if I have guiding examples of what a human will do in certain
situations, I expect that to generalize. Whereas my hypothesis is that a deep neural network model
is almost like an infinite number of rules and there's absolutely no carryover between the rules.
So knowing even some or even most of the rules doesn't really tell me about those edge cases.
Yeah, I would agree that the edge cases are quite unforeseeable probably. I mean, at least we know
that they exist like with adversarial examples. So even if we don't know like exactly what they
will look like or there's even an infinite amount of, I mean, there's an infinite amount of like how
you can change the image to make it like have a different class. So we know, so I think it's
at least exist at least. Yeah, I love the point you made, though, that we have we have the luxury
to do analysis on these on these boxes because we can open them up. And that's another point.
I'm pretty sure that you make this in your book as well, which is that part of this is just scientific
inquiry. It's like understanding better how to interpret and explain machine learning models
will probably actually contribute to us being able to construct even better machine learning
models. Isn't that true? So so you're basically saying that also interpretability might help to
to be better at like create better machine learning models themselves.
Yeah, like as we as we develop these interpretability methods, because in a sense, like you
pointed out earlier, their statistical projections of kind of the behavior of the model. And so like
a saliency map, you know, if we can, if we can kind of use that to learn the way in which the
neural networks are behaving, it may it can certainly give rise to intuitions on ways to
alter the model. Yeah, so I also have seen approaches where they try like kind of fuse
also these two worlds like interpretable models or white box models and black box models. So
they try to to generate features out of the black box model, which you then use in your
understand more understandable white box model. So I think this and this also like
using similar techniques to which you would use for interpretability, like detecting
interactions, for examples, for example. So yeah, these can be used also to to build better
models and also to build better interpretable models. The other and I'll make one last point
here, which is another social good that can come out of interpretability is imagine we've got,
you know, an ML model that's not trying to make any decisions, but it's just trying to figure out
what leads to happiness and success in life, you know, and so we analyze a whole bunch of data
and we find out, well, it's really important if you graduate from high school, and it's really
important if you, you know, don't have children before you're married and, you know, all these
other factors, if we can dive in and kind of isolate those factors, it actually allows people
to have some guidance on, oh, look, we've had this machine learning model that's analyzed a bunch of
data and it actually has some some understandable recommendations for how to lead a healthier
life or a better life or whatever. Yeah, I think that this very good example where
the fact that you have a prediction model doesn't solve your problem. So actually,
it's just a means to some other goal. In this case, understanding like what are the factors for
happiness. And one example, I am from a friend who worked at a telecom company, and they built
like a churn prediction model to see like who will quit the telecom contract. And then they
started like the ones with the highest likelihood they started sending out emails, hey, maybe offering
them a better deal. But actually, the outcome was that, well, they, they, when they once they wrote
to the customers, they will left and quit their contract. So it's kind of had like, so this is
a case where the prediction model actually works. But then they people leave. In this case, probably
because they realized, ah, shit, I have this contract still going on, time to quit now. So
if you knew the reasons why they are likely to turn, then you could like better select like
when you write some email, or maybe some other campaign, or when maybe not to write anything at
all. Yeah. Right. Now, Christoph, you have a background in stats, which means you, I mean,
you like Connor as well, take an incredibly dim view of machine learning. And you wonder how,
how is it possible for us to be stabbing in the dark like this? But, you know, you said that we
need to be more rigorous. And there's no quantification of uncertainty with the current
IML methods. And I suspect you might be working on some methods behind the scenes on this. But,
you know, when you have models and explanations, which are computed from data, they are subject
to uncertainty. And that's just not modeled at all at the moment, right? So we need to be making some
distributional and structural assumptions that we're not making now. And you point out that
there's this phenomenon of p hacking, which is a huge problem in the natural sciences, which hasn't
quite made its way to IML methods yet, but probably will do. Yeah. So, yeah, I think in
statistics, we're really good at quantifying uncertainty. I mean, this also has some darker
sides with like the p hacking and so on. But I still would say it's better to have not only
just one number or one explanation, but also have the distribution to do off this explanation or
this number and to quantify what uncertainty is behind computing this number. So when you have
a linear model, then you get some coefficient, which you interpret in the end. But usually,
you don't just interpret the coefficient, but you look at the confidence intervals. But we don't
do it at the moment for interpretability. So you maybe get the saliency maps, but how certain
are you about maybe it's a bad example, because we don't so much on it. But if you have like a
feature importance value and you get some result, how like what's the range actually, like how much
variance is behind it. If I were to use slightly different data or refit my model again, how similar
would the number be? And I think that's something that will or should come to interpretability as well.
It's funny how when we come to machine learning, it's almost like open season and forgetting
everything you know about maths and stats. So all out the window, okay, so excited about these
algorithms, right? Like one example is if you take a, if you're fitting a model to predict something
that was unlikely, I don't know, maybe it was like a COVID test, for example. And then if you know
the prevalence of COVID, you get it back, you kind of know what the false positive rate is going to be.
And so you notice that you think it's the multiple comparison issue, right? You know that you're
expecting a certain level of false positives. When it comes to doing something like feature
importance or looking at interpretability from a thousand features and then five come through is
really, really important. As you mentioned, sometimes we just forget that multiple comparison
issue. Forget the fact that probably these five are going to be completely false positives and
probably completely meaningless. Yeah, I agree. Especially if you have like these high dimensionality
features. And for the record, I have to say, I mean, there are already approaches. So especially
for feature importance, because there's like a huge community in random forests, for example,
and they thought a lot about these issues and their tests for this and stuff like it.
But for the rest of interpretability, I think it could gain a lot. I'm thinking more about,
I mean, this is very simple stuff, like multiple comparisons, quantifying uncertainty. Does the
stuff like statisticians think like a long time already about it? And I mean, even if you leave
the area of interpretability and look at the benchmarks. So even like if you have like a
accuracy, like a table and you see accuracies in it, but there's no variance attached to it,
then it should be like suspicious of it. Because if you just retrain your neural network with a
different seed, you might end up with a different accuracy in the end. So and if you want to say
a method is better than another method, you want to quantify how large the ranges of uncertainty
do you and there are a lot of things like the choice of data, choice of splitting points and
training and test data, weight initialization and so on. So I think a lot of that this rigor
from statistics could help the machine learning community and machine learning science to become
better. Yeah, but let's let's never forget this quite quite well known saying, which is there are
three kinds of lies, lies, damned lies and statistics. So you know, that's a lot of what's
going on right is, you know, fundamentally whenever we go measure data and we have a model,
what we're actually able to extract from that data and the model is inherently probabilistic.
It's a probability distribution right at the end of the day. And we get into trouble anytime we try
to take that probability distribution and project it to numbers i.e. statistics, like as soon as we
start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval
or whatever the fact is we're throwing away information the totality of the information
sitting there in that weird multimodal you know spread out distribution right and then we we find
some way to simplify it and project it down to a set of numbers we've got a problem like that's
and if you forget that that's happening if you forget that you're throwing away all this information
i think that you know the tendency to do that isn't just simplification it's that oftentimes we
have to use these probabilistic things to reach a decision and as soon as we get to the point where
look i've got to choose either to go left or right give the loan or not give the loan as soon as we
get down to some point where we have to make a concrete decision we're forced you know we're
forced to project it right but along the way it's important not to lose sight of the the fact that
we're throwing away information fantastic well and by the way you also said something interesting
a minute ago Christoph which is about at least in most machine learning algorithms if you change
the random seed you know that there's enough stability there that it still gives you roughly the
the same model every time but in reinforcement learning if you change the random seed the entire
thing is completely broken but but yeah what Keith was saying about this this information
and structure in models i think that's really interesting because people have said with reinforcement
learning you can actually learn causal factors right but that's not really true you're interacting
with the system but what you're learning is is a surface representation of causal factors so you
might learn that there's a causal factor between um like a hose putting out fire but it wouldn't
actually learn that it was the water that put out you know that there was a causal relationship
between the water and the fire and this is the case with so many of our models as we were saying
earlier that there's there's just a a surface representation which doesn't actually represent
the reality of our world at all but this brings me on to the next point because you have a real
problem with causal interpretations of some of these iml metrics right and you you say that models
well the the goal of models is that they should reflect the causal structure right this is what
we want to do in science but most statistical learning just reflects these surface feature
correlations they they don't even scratch the surface of what we want so what are we going to do
right are you doing some work in this field to to help us out here and and why are people making
these fallacious interpretations yeah so so i'm not not working on anything causality related at the
moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and
master and zero i mean the only time we were talked about causality was when i heard the
sentence correlation does not imply causality and it was really about it so i think it's like really
yeah should be taught a lot more like how to think about causality like just super simple
things like you should include confounders or like what types of features if you include them in the
model like destroy your causal interpretation of another features these these are not super
difficult things so um then you don't have to like learn any like difficult frameworks to work
with or or read like causality books on it that's like super simple um yeah rules of thumb for your
features even um yeah and i think you also have to decide um or distinguish between um like what's
the goal of your model do you want a causal interpretation um or do you want to like because
in a sense and you have to also do distinguish between two levels you have the real world level
and the model level i mean once you use features for the model they are causal for the model
prediction of course because you designed it that way and the question is when are you allowed to go
to the real world level where you say okay this um the feature importance that i see here also the
feature dependence plot that i see is also causal um or and may interpret it as a cause and or as a
causal effect also for for the real world and i think that also depends like if you need this
interpretation if you do scientific modeling for example then you probably want it um but that
there can also be good reasons to include non-causal features into your model if your goal is really
just prediction and some feature might help you with a good prediction um but it might not be causal
at all yes but the problem is when we're using these deep learning models they they will learn
a structure which probably has no relationship to to the real world whatsoever but um i think causal
factors do generalize much better there's the example of um i don't know with car crashes right
male testosterone levels is a causal factor so that will probably generalize to other locations
where you didn't train your data on but unfortunately models don't really do that well but so just real
quickly on that tim like the only reason that we know testosterone is a causal factor is is not from
that data set it's from a bunch of mechanistic you know scientific research in biology and and
elsewhere um so you know i i'm kind of wondering how it would be nice if at least machine learning
methods could indicate that there may be the possibility of a causal structure so just looking
for underlying hidden structures um that that you know there are more generalizable that could
explain large pieces of the of the data and give kind of a list of hey there might be a causal factor
here like go investigate it but on on that there's a difference between a causal factor
and a causal structure i think that the challenge is that we don't have enough fidelity in the
structure that benjo by the way is doing some interesting work on this using data driven approaches
to um you know learn causal factors but it's the structure of the factor graph which i think is the
important thing i mean this is one of the most interesting parts i think of machine learning
actually trying to learn causation from a data set which you can do right things like beta networks
where you specify all your variables you connect these nodes with edges and you can try to learn
the optimal structure like the simplest structure and that sometimes turns out to be the real life
causal effect if you do it well but the difference is you as a human you you know the causal structure
and you've you've created that that graph so it's not learned you've created it you can learn these
things from data right you can actually you can search over the set of all possible graphs all
the possible edges and you have a bit of a loss function you try to find a graph that fits the
data well so it's got enough edges but it's not too complex you're not relating everything to
everything and so just from data without any human input with structure learning you can sometimes
get a model that kind of out of nothing will give you the cause of relationships sometimes
there is redundancy right because a graph that says a implies it causes b causes c that's identical
to c causes b causes a right but even then with with structure learning you you've got this adjacency
matrix and all of those nodes you've already come up with a priori so what you want to learn is what
the nodes are themselves right yeah I think like what kind of mentioned there's lots of
moda setting where you have well defined features and I think what Tim referred to was more like the
you don't even know what the features are like if you have a convolutional neural network and and
like what's an object what is a feature that like is disentangled also from other objects
so I think also there is the big issue that you have this entanglement between concepts that
I don't know that the Frisbee is always with on the same image as a dog so maybe the neural network
can't even separate these two things then because they are too entangled in the data set to even
discover the structure that that is really underlying the real world in this case
that's a fascinating point actually because one of the reasons why there's no easy solution to
adversarial examples is because you learn these these non robust features and you might just think
to yourself well fur is a low magnitude feature it's really easy just to kind of create fur on
anything and for the neural network into thinking it's a cat and you just say well this is obvious
right you just create some rules to say well if it's if it's not an animal and fur then ignore the
fur but actually the features are entangled in this complex neural network so you can't do that
but I wanted to move the discussion on a bit so you said that there are some really interesting
challenges ahead in in IML and what's fascinating is you start talking about the process so you
say that the setting of machine learning is too static it doesn't reflect how these models are
used in reality and models are embedded in a process or a product or even complex people
interactions and I love this right because I talk about ML DevOps and machine learning models
in isolation are irrelevant it's the people in the process that's where the complexity is
even with intelligence itself it's a process right you know intelligence is the interaction
between a brain a body and an environment and you know within the context of this process you
know we've got all of this rich information that we could be bringing in from other disciplines
and you're saying we should bring in comp science stats folks and we should be bringing in psychologists
and social scientists and we need to also have interpretability at a higher level at the institutional
level right or at the society level so when you kind of broaden the discussion out a little bit
I think it adds a nice bit of flavor yeah so it's very as a especially as a scientist it's so
convenient to just have this fixed model a fixed data set and then you just geek out and invent all
these methods and so on but the reality is that that you use the method some some place and then
it interacts with the institution it's built with the developers it's built by with the people it
affects and my favorite example there is when you have this closed loop where your model makes
predictions and these predictions generate the next generation's data so for the next generation
of the model it produces the data so there's this example of the rent index where you have this model
that tells you how much rent you should pay for a certain kind of apartment and so on and this is
actually like legally binding so if you're a landlord you have to accept kind of the range
that is outputted by the model which also means that the data that is produced so the new flats
that are rented out in new apartments they all have to fit the model kind of and then but then
you use this data again to train your model so you have this very weird feedback loop and
I think it's also difficult to wrap your head around it and understand implications of it.
That same thing a very similar feedback loop was a fear in the you know in our Algo Shambles
video about the UK testing since they couldn't conduct the what was it the A level test right
Tim they they built some some models around that and so it would do things like well you know
if this school historically never had anyone in this grade bucket then we're not going to assign
anyone to that grade bucket in that school and so it's sort of this self-perpetuating you know
feedback loop. We were reading through a lot of your work and you I mean I'm just going to hit this
point head on you don't really talk that much about AI ethics and you know there's the F word
which is the fairness word and I don't I don't recall you ever using that word and is that
something that you've deliberately shied away from? I just like define it as outside of the scope
like to talk or to I don't know talk about ethics so fairness metrics or so on because I think that's
a really big field on its own and I just don't know as much like about all these things so I
know a little bit like about the fairness metrics that are out there. I also think I mean they're
kind of like research wise a little bit overlapping but more or less separate fields I think interpretability
and fairness but of course they have some commonalities that I mean when you kind of to
for fairness you have to look it's not necessarily inside the model but you have to study how the
model behaves and that's kind of the connection to interpretability I would say yeah. Well where
yeah where I see the connection is work like yours is helping to build the tool set that will
allow people to apply human you know intuition and and ethics and evaluations to machine learning
because at the end of the day a lot of these are human moral judgments or ethical judgments and
it's important that people be happy with them because you know we have to have the population as a
whole understand and accept and be able to move forward with the increasing role that machine
learning is having in our lives and building that tool set is necessary so it's like you said very
early in this talk you know what do we do just stick our heads in the sand and ignore it and just
accept machine learning models are going to do whatever they do as long as they fly the plane
or you know don't kill too many people we're okay like I don't think that's going to work like we
have to build the tool set that you're talking about and continue this process of exploring
how to better explain and interpret ML models so that human beings can have that oversight because
it's the only thing that's going to give us comfort really as a society I suppose the reason I segue
to this is we were just talking about the process and you you mentioned some of these feedback
loops because we can have a very superficial discussion and you could say well we need to be
able to represent reality better than we do and we have a whole tool set here to identify sources
of bias or you know lack of robustness etc in models but it's so much more complex than that
because these models are used in a very complex process and you get these very very complex
dynamics emerging as a result of that and I think we're only really just scratching the surface of
understanding those dynamics yeah I think so too as I said I think in science it's always very easy
to to study things in isolation like study one type of model study one type of adjustment for a deep
neural network and hopefully we will see more work emerge on this I think I've never read the paper
like I mean of course discussed implications but really like analyze like what happens in terms of
the data and the model when you have like multiple generations for example of a model and how it
changes over time but this thing I mean to study those things also means that you have to wait for
a long time until you have these dynamics and I think in many cases it's just starting that we use
these models more extensively in our daily life I have a question is is anyone because look interpretability
metrics whatever they are saliency maps and you know could be partial dependency plots whatever
you could actually build in some requirements of those into the objective functions when you
go to train models so for example I'm just going to come up with a crazy idea I have no idea if
this is relevant at all but somebody could say look I want all my saliency maps to be you know
sets of of a bezier curves or something like that like they have to have a certain smoothness
property and you could actually put that as a constraint in the objective function has anybody
tried anything like that yeah there are approaches so I saw one paper they added some
some parts to their objective function so that when you create line explanations with line that
they were most more stable and there are a lot of things like and for neural networks you have
disentanglement that you try that the feature maps or the denotes learn disentangled concepts
there are ways to introduce like monotonicity so that a feature can always go into the effect of
a feature can always be in one direction not to like zigzag around so that there are approaches
to do this to like have like interpretability constraints in your modeling here yeah because
I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we
just create white box models maybe we can use if the definition to a human being of white boxes
that it's interpretable and understandable if we can build into the objective functions when
we're actually training the network that it has these properties then we'll actually be
helping to create more white box you know models even if they are complex that's definitely an
option but I think the issue remains the same that you with it's similar to a white box model I mean
you'll make some trade-offs in the end you have to make the judgment whether so when you put more
constraints I mean you can actually also help the model of course that if you I mean if you have
some inductive biases also which which you infuse into the model which help with predicting or be
more more stable but sometimes you make maybe also a trade-off with accuracy and you just have to
like in the end you have this yeah this set of models where some are more accurate some
better than this one interpretability dimension the other is cheaper to deploy and you have this
so this kind of going into direction of like automatic machine learning and it you don't get
like just the best performing one but you have this parater set like well so we have multiple
objectives that you want to hit and then there's not one model that works best but you have a set
of models that that have different trade-offs between these objectives and then you have to decide
what is the trade-off that you want to do that you want to have.
Well I guess there's no getting away from it is there the interpretability is just going to get
more important and more important I think you mentioned Christoph that you know we've had
linear models for hundreds of years and then there's been this big explosion and deep learning
and then would you say about 2016 to 2018 that's when interpretability is really kicked off
I what do you think do you think where's it going are we just going to get more and more
attention paid to this area? I don't know if you can get more than this I don't know
yeah but I think it's at least here to stay and I think it's important I mean it has been
important before but of course with like the push from deep learning especially and that it just
became more clear to a lot of people that we need interpretability in some sense at least
um yeah of course people have attempted it before and worked on it before and it's just more urgent
now. I really like the bit in your book talking about what's changed recently how interpretability
is coming together as a field you know at this a unification so you know in physics we love a
big unification when you take all these different things in the past and say oh they're all just
part of this one big framework and it was chap that chap paper was amazing wasn't it saying things
like lime, deep lift, layer-wise propagation, Shapley, ah forget all them they're all special cases of
these additive feature attribution methods and we can prove that this is the only one that's
theoretically valid because it has these properties of symmetry it's got the stummy property so
you know everything that's been done before in interpretability well they're all in our
framework now and Shapley values are the way forward yeah they're quite uh quite famous to
Shapley values yeah. Would you believe them then I mean I guess in the paper they kind of
they kind of disagree with lime if they don't they they say well lime is a count of this but
they're going to be breaking our properties of efficiency and symmetry so lime is using the
wrong weights right they should be using this yeah kernel shape weights rather than the lime weights.
I think that's just a different approach also to think about it I mean you don't maybe you don't I
think I think the properties are quite attractive or meaningful at least but also also the lime
approach I'm very critical about lime because it I think it's difficult to have the correct
to know like how to parameterize your your local models so I think I'm a bit more of a fan of
Shapley values and because of the theoretical properties it comes with. So are you talking
about that distance measure in lime where you have to be able to quantify how far away is the
permutation? Yeah they're like the kernel width yeah which is set to 0.75 I think so I just looked
it up and I mean it's it's a very difficult question it goes to the heart of like what's local
because like I mean you have this this kernel that decides like how much you weight
all the data points around the point you want to explain and and like how how big is this area I
think this is very dependent on your model and your data and there's no answer no easy answer to how
to set it. Yeah or even more generally than that there's this whole notion of what does it mean to
have a local interpretation method in you know in text or vision so in vision there's this super
pixel concept which is something that seems to make intuitive sense but but does it you know when
you create all of these different maskings of different parts of the input space but with
Shapley values as well that they they are a beautiful a beautiful technique especially
because the the values are quite meaningful but if you have shared information between the features
I mean Connor and I were talking about this for example you if you had the same model where you
were predicting someone's income and you put their I don't know let's say you had salary in the model
twice then the Shapley value would be divided between the two duplicate fields right so there
just seems to be so much esoterica in these IML methods right are we expected to know all of this
stuff? Yeah I think I mean that's why I wrote the book and to capture these things that you have
to know all these disadvantages of the methods where I try to be very honest I mean because I'm
not too invested in them but yeah I think that's worth all tools that we usually have also with
statistics and so on you have to know like these like as you mentioned this if you have salary
twice then it will just I mean it depends also on what your model does if it just picks one of the
salary features or if it itself uses both so this also something that will define how the
Shapley value will look like later on but you have to know these things if you want to use
Shapley values and interpret them correctly yeah because I think philosophically we've got we've
got the real behavior and then we use these interpretability methods and then we've got the
kind of perceived behavior so we've got these levels of modeling or do you know what I mean
simplification and it's all well and true if you are dealing with data scientists who understand
how these you know methods work that's fine but invariably data scientists need to present
this information to lay people and they are not going to understand all of the various different
trade-offs and how information is being compressed and lost and so on so do you see that as a as a
serious problem yes but it's not a new problem it's with any number that you read in any newspaper
I mean so in a sense I mean when when you look at outcomes of statistical models that
well everyone can understand well of course not because you need training to understand how to
interpret a linear model or any regression model yeah there's this difficulty but I don't think
it's new in any sense because there's always I mean any number that you read anywhere has a very
complex process so I don't know if you have like COVID testing numbers it's very complex like how
the number was generated maybe like how it was aggregated over many states and like what cases
it includes and which it doesn't and so the number looks very innocent and simple but there's a very
long process behind it to produce it maybe this process is a bit more black box or more difficult
if it comes out if there's some machine learning in between machine learning model in between to
generate a number um but yeah I think this problem is well old well let me let me challenge a little
bit here on something which is okay if I have if I have some general formula just some very general
formula and then I go in there and I go you know what this formula has five parameters and if I make
this one point seven five and that one one third and this one two and that one zero and I call this
the megatron you know activation potential and I go and write a paper about it that's really just
an arbitrary you know kind of selection of a bunch of numbers and then you gave it a fancy
mathematical passport and you got it published in some journal and now everybody has to memorize
that as you know the megatron potential and kind of learn about it and that's a lot of what's going
on right now is that it's really just a bunch of hacking like it's people just they don't really know
a general solution and they don't know how to solve like in general the problem they're trying to
solve and so they just hack around and then the ones that are kind of famous or demonstrate some
success in a particular combination you know competition over in this corner or something
it now becomes something that's part of the lexicon that we all have to learn and I think
like I look back on this like imagine what physics was like before Leibniz and Newton you know
invented calculus it's like everybody memorizing a whole bunch of little purpose built kind of
formulas and then along comes a general framework which now we can just learn calculus and derive
the special circumstances as needed. You're onto something really interesting there which is that
with IML methods we are we are kind of compressing information down into a representation you know
and then that that is a transport that can be understood by different people but there's a trade
off right because as you said you can learn calculus and that's a compact framework for
doing lots of stuff but it's all about the amount of common knowledge that is required
so it's possible to compress something down just to one symbol and that symbol could represent
all of that knowledge but it doesn't help you because I still need to learn all of that knowledge.
Yeah but so to me calculus was a very beautiful and simple framework that I could learn and then
once I learned that simple thing I could go and solve all kinds of problems with it that before I
would have to memorize specific solutions or like the quadratic formula for example is a student I
didn't actually memorize the quadratic formula I just learned how to complete the square and then
then I would just do complete the square and if somebody asked me what the quadratic formula was
I would just quickly derive it right because it was easier to memorize the rule and then apply
the rule to any situation rather than to memorize all these little one-off you know kinds of hacks
that we come up with. You're not normal Keith right so most people won't be able to go and
understand this because the thing is that these IML methods are brilliant for data scientists who
can it's a framework right it's a it's a reference of understanding so assuming that people can
understand how Shapley values work then this is a beautiful representation to reason about the
behavior of models. Sure but when I when I first saw Shapley values I realized immediately there's
a connection in you know Bayesian analysis to marginalization you know all we're really doing
here is computing the expected marginal you know contribution to this value it's not a
probability but it's still the same procedure being done right and I think I'm going to throw
myself in with the lay people to a degree because the reason I'm always striving for simplifications
is because I don't have the capacity to memorize all these little arbitrary kinds of hacks and
but I yet I could totally understand Bayesian analysis and like I said you know previously in
some other videos statistics made no sense to me until I learned the Bayesian framework because
that was based on very simple rules that I could then reapply as needed. I think that what you
referred to Keith maybe the worst situation is with the saliency maps because you have so many
methods and they all like back propagate the gradient and to do input pixels and to now do
you have to like learn like how dozens of these framework works or like how to interpret do
interpretation of all of these and they're all kind of variants of each other so mostly because
they just there's some ambiguity how you how you back propagate the gradient because because of
the non-linear units and stuff and a little bit differences how you can define this and so you
have this huge like a sea of many different methods. I think it would be nice therefore as you said to
have some like simplification where you say okay this is like all these methods work under this one
principle basically and we have these two parameters and that's how they define. I think that's also
that I think I wrote something in a chapter that please stop inventing new methods for saliency
maps so I think it's enough and we should focus more on like doing this consolidation to
like understand the limitations of the methods and consolidate them to see like what's the
commonalities in which ways do they differ and so on. That's probably actually my first impression
actually when I first opened the interpretable ML book. I was amazed how many different things
there are I've you know heard people say ah you can't use machine learning it's just a black box
so many times it'd almost been drilled into my head then seeing all the things you know from
white box models ways of training salient models counterfactual explanations it's what a wonderful
recipe right there are so many different things that you can do. I feel like now I trust ML models
more than other kinds of things because I have this amazing toolbox of ways to understand them.
The thing that strikes me though is most of these methods as we were just saying they require
interpretation by a human and a human who understands how the method works. I love this concept of
turning machine learning into an engineering discipline and being able to do a lot of these
tests non-interactively and I think you know Marco Rubirio has done a lot of work around the
counterfactual examples and the data grouping and what excites me about these methods is they seem
like methods that we could actually run as part of an automated process. We still have to set
thresholds maybe we could set a threshold that said if this if this counterfactual example flips
the switch on more than one percent of examples then fail the build that seems reasonable but
a saliency method I mean how the hell do you say well if there's lots of red pixels over here
then break the build I mean it's just ridiculous isn't it? Yeah yeah so I've seen interesting
approaches to like using interpretability also more automatically like when you do model
monitoring you can do things like create interpretations and see if they significantly
change over time for example so then have thresholds that warn you that hey something's going on with
your model so I think there's also interesting approaches there. Yeah you know Tim to your
point of making this an engineering field and even making interpretability and and
understandability an engineering field I mean I think that maybe that's why I like your book so
much Kristoff is I think it's it's a step towards that direction it's like let's survey everything
and more importantly let's create a finite and hopefully smallish set of simple concepts
that we can all agree on and understand that we can use to catalog you know what's out there
so please keep up the good work you know I'm interested to see where this goes. So final
question for you Kristoff then I wonder what's what's next for interpretability like are we
going to the point where it's going to be almost a box ticking exercise where we can say yes our
process we've done the standard interpretability step I mean is it is computing power going to
change it? I remember when Shaq the library came out it made that approach possible whereas
previously you know it was very hard very computationing feasible my friend Anjum who's a
wonderful data scientist sent me Nvidia Rapids they've got that running on GPUs way faster than
of course before is it just going to become a standard step do you think or is it going to be
something where you need decent subject matter expertise and some real thought to do to really
understand how a model works? So well predictions about the future are always hard so maybe more
like what I wish or what yeah maybe think we'll have code to happen. So I mean what we're seeing
already is like a lot of implementations of these methods so they're kind of getting a
commonality Aruq one can use it very easily there's a lot of libraries out there in Python and R
but also in like these machine learning cloud tools they also have a lot of interpretation
methods available now so in that sense I think it's maturing a lot. I still believe that you need
some expertise to understand them or at least some good references and I will also be hopefully
more than my book maybe have some documentation when for these tools and people answering on
stack overflow questions and whatnot so I think yeah it's getting we're getting that everyone
can use it easily. I think it should never be a box-ticking exercise it's a similar thing when
if you have an AI ethics you know governance process or something the last thing you want
is for it just to be an automatic response so I've just you know yeah I've thought about AI ethics
it needs to be something that we really engage with. I think we need to abstract away a lot of
the complexity at the moment. I think it's possible to come up with an interface to standardize
the way that we do interpretability and we can reduce down what we have now to certain primitives
which means that it can plug into an engineering process and it also means that
we can abstract away some of the complexity and I think that's possible. Yeah I also would agree
that it shouldn't be like just box-ticking but you can like for the initial when you start
interpreting a model that you just have like with a click you have a report and then it shows you
the most basic things but then you still like should ask the question like does it really
make sense that this feature is the most important one or what's happening there with these weird
interactions between the two features. Let's dig a bit deeper here and see what's going on so I think
there's this one portion that is just like this automated reporting thing but this should then
be like the starting point for more critical questioning of the model and checking what's
going on for some specific problems maybe. So it's going to be you click on the Molnar report
and it gives you the report from the book right. Yeah it would be convenient. Well
Christoph Molnar thank you very much for joining us today it's an absolute honor to have you on the
show. Thanks for having me. Hey folks this is Tim in PostScript there's just a couple of thoughts
that didn't come to my mind during the interview that I think I'd like to quickly cover now.
The first thing is on the lack of fairness the reason why I raised that is most folks who talk
about AI ethics and fairness they use the toolkit of interpretability methods quite often you know
to apply their trade there are tools out there to mitigate fairness and to detect fairness.
Microsoft's Fair Learner is a great example of this. What we really need is an operating model
or a set of guidelines on how to implement these tools. How do I identify sources of problematic
correlations? We need to have a database of problematic correlations. Having a tool that
allows me to identify and mitigate bias frankly is useless. What do I do with that? As we mentioned
on the show many of the machine learning cloud providers whether it's Data IQ or Azure ML and
SageMaker they all have these interpretability methods built in now including saliency maps
and it's just a box-ticking exercise frankly it's completely useless. There is no accepted
guidance on how these tools should be used right so if I'm a large company and I'm building
an operating model around how to implement fairness techniques just having the technology is irrelevant
it's about the people and the process and the the kind of operating model of how we implement it and
there is basically no useful information out there to help us do that. The other thing is we spoke
about this becoming an engineering discipline which is to say what if we could create an interface
to abstract away some of the vagaries and esoteric of interpretability methods. We might come up with
some primitives or some common language and then we can hide the complexity behind the interface.
This is kind of what we do with ML DevOps already. We automate as much as we can and we
templateize and remove friction out of the process. We even create building blocks using
domain specific languages or YAML files and pipelines and so on. So what we do is we create
a level of abstraction where people can compose together pipelines. Remember when Connor made the
comment that this might just become a box-ticking exercise and this is something we see in security
and AI ethics already. We can't really trust people to self-report that the model is behaving
correctly or that the project has no concerns from an AI ethics point of view. The whole point
here is process. If we want to create an operating model and ensure best practices are followed or
any kind of standardization in a large organization, we have to design a process and many eyes make
shallow holes. So the process would mandate that a certain number of stakeholders were involved in
assessing the particular YAML technique and validating it essentially and then we would need
to record that assessment. So who said what when and then if the company ever became audited or if
god forbid there was some kind of a problem where the YAML model did something wrong and it caused
the company lots of damage or it harmed the environment or society or something like that,
we would then be able to rewind the clock and say okay well Joe Bloggs said it was okay
because of XYZ. So that is an operating model. It's a process and how to design such a process
again is completely absent. Speaking as a chief data scientist myself that's the kind of thing
that I'm interested in and it's very difficult for me to do that. I really hope you've enjoyed
the episode today. We've had so much fun making it. Remember to like, comment and subscribe
and we'll see you back next week.

So you're talking about the symmetries in data, could these analogies also be represented using
the kind of symmetries that you're talking about? Good question. Let me think of it a bit.
Modern machine learning operates with large, high quality data sets, which together with
appropriate computational resources, motivates the design of rich function spaces with the
capacity to interpolate over the data points. Now this mindset plays well with neural network,
since even the simplest choices of inductive prior yield a dense class of functions.
Now, symmetry, as wide or narrow as you may define its meaning, is one idea by which man
through the ages has tried to comprehend and create order, beauty and perfection.
And that was a quote from Hermann Weil, a German mathematician who was born in the 19th century.
Now, since the early days, researchers have adapted neural networks to exploit the low
dimensional geometry arising from physical measurements, for example, grids in images,
sequences in time series, or position and momentum in molecules and their associated symmetries,
such as translation or rotation. Now folks, this is an epic special edition of MLST. We've been
working on this since May of this year. So please use the table of contents on YouTube if you want
to skip around. The show is about three and a half hours long. The second half of the show, roughly
speaking, is a traditional style MLST episode, but the beginning part is a bit of an experiment for
us, maybe a bit of a departure. We want to make some Netflix style content. And we've even been
filming on location with our guests. So I hope you enjoy the show and let us know what you think in
the YouTube comments. Many people intuit that there are some deep theoretical links between some of
the recent deep learning model architectures, particularly the ones on sets, actually. And
this may be why so many popular architectures keep getting reinvented. Now, the other day, Fabian
Fuchs from Oxford University released a really cool blog post about deep learning on sets,
elucidating a math heavy paper that he co-authored with Edward Wagstaff et al. Now, he wanted to
understand why so many neural network architectures for sets resemble either deep sets or self-attention,
because sets come in any ordering. There are many opportunities to design inductive priors
to capture the symmetries. So it raises the question, how do we design deep learning algorithms
that are invariant to semantically equivalent transformations, while maintaining maximum
expressivity? Now, Fabian pointed out that the so-called Genosi pooling framework gives a satisfying
explanation. Genosi pooling is when you generate all of the k-tuples of a set, an average over your
target function on those permutations. It gives you a computationally tractable way of achieving
permutation invariance. So rather than computing n factorial combinations of the examples,
you compute n factorial divided by n minus k factorial, which for small k is very tractable.
Now, clearly, setting k to n gives you the most expressive yet the most expensive model. But
that would be cool. It would model the high order interactions between the examples. But
it turns out that deep sets are this configuration with k equals one, and self-attention is this
configuration with k equals two. Now, Fabian also spoke about approximate permutation invariance,
which is when you set k to n, the number of examples, but you sample the permutations.
It turns out you don't have to sample very many of them to get good results. But anyway,
if you want to check out that in a little bit more detail, go and check out Fabian's blog.
I've put a link in the video description. High dimensional learning is impossible
due to the curse of dimensionality. It only works if we make some very strong assumptions
about the regularities of the space of functions that we need to search through.
Now, the classical assumptions that we make in machine learning are no longer relevant.
Now, in general, learning in high dimensions is intractable. The number of samples grows
exponentially with the number of dimensions. The universal function approximation theorem
popularized in the 1990s states that for the class of shallow neural network functions,
you can approximate any continuous function to arbitrary precision by just stacking the neurons,
right, assuming that you had enough of them. So it's a bit like kind of sparse coding, if you like.
The curse of dimensionality refers to the various phenomena that arise
when analyzing and organizing data in high dimensional spaces that do not occur in low
dimensional settings, such as the three dimensional physical space of everyday experience.
Now, the common theme of these problems is that when the dimensionality increases,
the volume of the space increases so fast that the available data effectively becomes sparse.
This sparsity is problematic for any method that requires statistical significance.
Now, in order to obtain a statistically sound and reliable result,
the amount of data needed to support the result often grows exponentially
with the dimensionality. Most of the information in data has regularities. Now, what this means
in plain English is just like on a kaleidoscope, most of the information which has been generated
by the physical world is actually redundant. Just many repeated semantically equivalent
replicas of the same thing. The world is full of simulacrums. Machine learning algorithms need to
encode the appropriate notion of regularity to cut down the search space of possible functions.
You might have heard this idea referred to as an inductive bias. Now, machine learning is about
trading off these three sources of error, statistical error from approximating the expectations on a
finite sample. And this grows as you increase your hypothesis space. Approximation error,
which is how good is your model in that hypothesis space. If your function space is too small,
then the one that you find will incur a lot of approximation error. And finally,
optimization error, which is the ability to find a global optimum. Now, even if we make
strong assumptions about our hypothesis space, you know, we should say that it should be lipchitz
or in plain English, it should be locally smooth. It's still way too large. We want to have a way
to search through the space to get anywhere. So the statistical error is cursed by the
dimensionality. If we make the hypothesis or the function space really small, then the search
space is smaller, but the approximation error is cursed by dimensionality. So we need to define
better function spaces to search through. But how? We need to move towards a new class of function
spaces, which is to say, geometrically inspired function spaces. Let's exploit the underlying
low dimensional structure of the high dimensional input space. The geometric domain can give us
entirely new notions of regularity, which we can exploit. Now using geometrical priors, which is to
say, only allowing equivariant functions or ones which respect a particular geometrical principle,
this will reduce the space of possible functions that we search through, which means less risk of
statistical error and less risk of overfitting. We should be able to do this without increasing
approximation error, because we should know for sure that the true function has a certain
geometrical property, which will bias into the model. So introducing the Geometrical Deep Learning
Proto Book. So recently, Professor Michael Bronstein, Professor Joanne Brunner, Dr. Taco Cohen,
and Dr. Petar Velichkovich, released an epic proto book called Geometric Deep Learning,
Grids, Groups, Graphs, Geodesics and Gages. These researchers are elegantly linking classical
theory and machine learning and geometry and group theory to deep learning, which is fascinating.
Now the Proto Book is beautifully written. It's presented so well, it even has some helpful margin
notes. And honestly, I could read sections of it out loud on MLST with scant need to change a single
word. It's that well written. I mean, there's a lot of maths in there as well, let's be honest,
I can't dodge that. But I often try to impress upon people that if your writing sounds weird when
you say it out loud, then you're probably writing it the wrong way. But these guys have written it
really well. Now they've essentially created an abstraction or a blueprint, as they call it,
which prototypically describes all of the deep learning architectures, the geometrical priors
that they have described so far. They don't prescribe a specific architecture, but rather
a series of necessary conditions. The book provides a mathematical framework to study
this field. And it's essentially a mindset on, you know, how to build new architectures. It gives
constructive, you know, procedures to incorporate prior physical knowledge into neural architectures.
And it provides a principled way to build future architectures, which have not yet been invented.
The researchers have also recently released a series of 12 brilliant lectures
on all of the material in the book, and I've linked these in the video description.
Now, what are the core domains of geometric deep learning? So in geometric deep learning,
the data lives on a domain. This domain is a set. It might have additional structure,
like a neighborhood in a graph, or it might have a metric such as, you know, what's the distance
between two points in the set. But most of the time, the data isn't the domain itself. It's a
representation or it's a signal, which is on a Hilbert space. Let's talk about symmetries.
Symmetries are really important to understand this framework. So a symmetry of an object
is simply a transformation of that object, which leaves it unchanged. Now, there are many different
types of symmetries in deep learning. I mean, for example, there are symmetries of the weights.
If you take two neurons in a neural network, and you swap them, the neural network is still
graph isomorphic. There are symmetries of the label function, which means that an image is still a
dog, even if you apply a rotation transformation to it. Actually, if we knew all of the symmetries
of a certain class, we would only need one labeled example, right, because we would recognize
any other examples that you give it as kind of semantically equivalent transformations.
But we can't do that, right, because the learning problem is difficult,
which means we don't actually know all of the symmetries in advance.
Now, in the context of geometric deep learning, we talk about symmetries of the core structured
geometric domains that we're interested in. So grids or graphs, for example, a symmetry
is any transformation which preserves the structure of the geometric domain that the signal lives on.
So, for example, permutations of a set preserves the set membership or Euclidean transformations,
like rotations or reflections, preserved distances and angles. There are a few rules to
remember because the way we deal with this paradigm is we talk about how composable those
symmetries are. The identity transformation is always a symmetry. Composing a symmetry
transformation is always a symmetry. The inverse of a symmetry is always a symmetry. We can formulate
this with this mathematically abstract notion of a group. Group theory in mathematics is fascinating
because it concerns only with how elements compose with each other, not what they actually are.
So different kinds of objects may have the same symmetry group. For example,
the group of rotational and reflection symmetries of a triangle is the same as the group of
permutations of sequences of three elements. So let's talk about the blueprint itself.
The blueprint has three core principles. Symmetry, scale separation and geometric stability.
In machine learning, multi-scale representations and local invariance are the fundamental mathematical
principles underpinning the efficiency of convolutional neural networks and graph neural
networks. They are typically implemented in the form of local pooling in some sense.
Now, these principles give us a very general blueprint of geometric deep learning that can
be recognized in the majority of popular deep neural network architectures. A typical design
consists of a sequence of locally-equivariant layers. I mean, think of the convolution layers in a CNN,
then a pooling or a coarsening layer. So you recognize those in CNNs as well. And finally,
followed by a globally invariant pooling layer. So that might be your classification head. Now,
these building blocks provide a rich approximation space which have prescribed invariance and
stability properties by combining them together into a scheme that these researchers refer to
as the geometric deep learning blueprint. Now, the researchers also introduced the concept of
geometric stability, which extends the notion of group invariance and equivalence to approximate
symmetry or transformations around the group. They quantify this in some sense by looking at
a metric space between the transformations themselves. This is Professor Michael Bronstein.
The problem is that traditional machine learning techniques work well with images or audio,
but they are not designed to deal with network structure data. In order to address this challenge,
we've developed a new framework that we call geometric deep learning. It allowed us to learn
the network effects of clinically approved drugs and to predict anti-cancer drug-like properties
of other molecules, for example, molecules contained in food.
Neural networks have exploded, leading to several success stories in industrial applications. And
I think it's quite indicative that last year, two major biological journals featured geometric
deep learning papers on their cover, which means that it has already become mainstream
and possibly will lead to new exciting results in fundamental sciences.
The book I hold is called The Role to Reality. It's written by a British mathematician and
recent Nobel laureate, Roger Penrose, a professor at Oxford. And it's really probably one of the most
complete attempts to write and describe modern physics and its mathematical underpinning.
And you can see it's very heavy. But if I were to compress the thousand-plus pages of this book
into just a single concept, I can capture it in one word. And this is symmetry. And symmetry is
really fundamental concept and fundamental idea that underpins all modern physics as we know it.
So, for example, the standard model of particle physics can entirely be derived
from the considerations of symmetry. And that's the kind of idea that we try to use
in deep learning to derive and create new neural network architectures entirely from fundamental
concepts and fundamental principles of symmetry. In the past decade, deep learning has brought a
revolution in data science and made possible many tasks previously thought to be beyond reach.
On the other hand, we now have a zoo of different neural network architectures
for different types of data, but few unifying principles.
The authors also point out that different geometric deep learning methods differ in their choice of
domain or symmetry group or the implementation specific details of those building blocks that
we spoke about. But many of the deep learning architectures currently in use fall into this
scheme and can thus be derived from common geometrical principles. As a consequence,
it is difficult to understand the relations between different methods, which inevitably leads to
the reinvention and rebranding of the same concepts. So, we need some form of geometric
unification in the spirit of the Erlang and program that I call geometric deep learning.
It serves two purposes. First to provide a common mathematical framework to derive the most successful
neural network architectures. And second to give a constructive procedure to build future
architectures in a principled way. This is a very general design that can be applied to different
types of geometric structures such as grids, homogeneous spaces with global transformation
groups, crafts and many folds where we have global isometry invariance as well as local gauge
symmetries. We call these the 5G of geometric deep learning. The implementation of these
principles leads to some of the most popular architectures that exist today in deep learning,
such as convolutional networks emerging from translational symmetry, craft neural networks,
deep sets and transformers implementing permutation invariance and intrinsic mesh CNNs
using computer graphics and vision that can be derived from gauge symmetries.
People are quite cynical about the interpolative nature of deep learning and I think that finding
this structure, this deeper structure could allow us to extrapolate in a way which is significantly
better than we can now. And I asked whether he thought deep learning could get us all the way
to artificial general intelligence. It's a hard question because it has several terms that are
not well defined. What do you define by intelligence? So we don't understand what is human intelligence.
Everybody probably gives a different meaning to these terms. So it's hard for me to even to
define and quantify artificial intelligence. I don't think that we necessarily need to emulate
human intelligence and as you mentioned in the past we thought of artificial intelligence as being
able to solve certain tasks and it's a kind of a moving target. We thought of playing intelligent
games or perception of the visual world like computer vision or understanding and translating
language or even creativity. And today we have machine learning systems that are able to address
at least to some extent all of these tasks, sometimes even better than humans. And are we
there yet at artificial intelligence? I don't think so. And probably artificial intelligence
will look differently from human intelligence. It doesn't need to look like human intelligence.
It's of course an interesting scientific question whether we can reproduce a human
in silico, but for solving practical problems that will make this technology useful for the
humanity, for the humankind. We probably need something different. It will certainly involve
certain level of abstraction that we currently don't have. It will probably require methods
that we currently don't have. But it doesn't necessarily need to look like a recreation of a human.
This is Dr. Petar Velichkovich. By now you'll have probably seen or heard something about our
recently released proto book on geometric deep learning on grids, graphs, groups, geodesics,
and gauges, or as we like to call it the 5Gs of geometric deep learning, which I've co-authored
alongside Michael Bronstein, John Brunner, and Taco Cohen. And you might be wondering what all the
fuss is about, because there's already a lot of really high quality synthesis textbooks on the
field of deep learning in general, and also on some sub areas of geometric deep learning,
such as graph neural networks, where Will Hamilton recently released a super high quality textbook
on that area. This is Dr. Taco Cohen. So what we've been trying to do in our book project
is to show that this geometric deep learning mindset is not just useful when tackling a new
problem, but actually allows you to derive from first principles of symmetry and skill separation
many of the architectures and architectural primitives like convolution, attention,
graph convolution, and so forth that have become popular over the last a few years,
even in cases where these considerations, vector variants, and skill separation were not
felt at center when the methods were first discovered. Now we think that this is useful for
a number of reasons. First of all, it might help to avoid reinventing the same ideas over and over,
and this can easily happen when the number of papers that come out every day is far
larger than what any one person can possibly read, and when different sub fields use different
language to describe their ideas. Furthermore, it might help to clarifying when a particular method
is useful. A geometric deep learning method is useful when the problem domain has the particular
symmetries that are built into the architecture. And finally, we hope that by making explicit
the commonalities between seemingly different methods, it will become easier for newcomers
to learn geometric deep learning. Ideally, one would not have to go through the large
number of architectures that have been designed, but just learn the general ideas of groups,
equivariants, group representations, and feature spaces, and so on, and then see for the particular
instances you're interested in how that fits into the general pattern. To really illustrate why do
we think that such a synthesis is important and relevant for deep learning research going forward,
we have to go way back, way back in the time of Euclid around 300 years BC. And as you might know,
Euclid is the founding father of Euclidean geometry, which for many, many years was the only way to
do geometry. It relied on a certain set of postulates that Euclid had that governed all the laws of
the geometry that he proposed. All of this started to drastically change around the 1800s when several
mathematicians, in an effort to prove that Euclid's geometry is the geometry to be following,
ended up assuming that one of the postulates is false and failing to drive a contradiction.
They actually ended up deriving a completely new set of self-consistent geometries, all with their
own set of laws and rules, and also quite differing terminologies. Among some of these popular variants
are the hyperbolic geometry of Lobachevsky and Bolyai and the elliptic geometries of Riemann.
And for a very long time, because all of these geometries had completely different sets of rules
and they were all self-consistent, people were generally wondering what is the one true geometry
that we should be studying. A solution to this problem came several decades later through the
work of a young German mathematician by the name of Felix Klein, who had just been appointed
for a professorship position at the small Bavarian University of Erlangen, the so-called
Friedrich Alexander University in Erlangen, Nuremberg. While he was at this post, he had proposed
a direction that would eventually enable us to unify all of the geometries that were in existence
at the time through the lens of invariances and symmetry using the language of group theory.
And his work is now eponymously known as the Erlangen program. And there is no way to overstate
how much of an important effect the Erlangen program had on mathematics and beyond. Because
of the fact that it provided a unifying lens of studying geometry, suddenly people didn't need to
hunt for the one true geometry, they had a blueprint they could use to drive whatever geometry was
necessary for the problem they were solving. And besides just mathematics, it had amazing
spillover effects to other very important fields of science. For example, in physics, the Erlangen
program spilled over through the work of Emy Nerther, demonstrated that all of the conservation
laws in physics, which previously had to be validated through extensive experimental evaluation,
could be completely derivable through the principles of symmetry. And needless to say,
this is a very fundamental and game-changing result in physics, which also allowed us to
classify some elementary particles in what is now known as the standard model. Thinking back
towards theoretical computer science, the Erlangen program also had a spillover effect
into category theory, which is one of the most abstractified areas of theoretical computer
science with a lot of potential for unifying various directions in mathematics. And actually,
in the words of the founders of category theory, the whole field of category theory can be seen
as an extension of Felix Klein's Erlangen program. So the Erlangen program demonstrated how it's
possible to take a small set of guiding principles of invariance and symmetry and use it to unify
something as broad as geometry. I like to think of geometric deep learning as not a single method
or architecture, but as a mindset. It's a way of looking at machine learning problems from the
first principles of symmetry and invariance. And symmetry is a key idea that underpins our
physical world and the data that is created by physical processes. And accounting for this
structure allows us to beat the curse of dimensionality in machine learning problems.
It is really a very powerful principle and very generic blueprint. And we find its instances
in some of today's most popular deep learning architectures, whether it's convolutional neural
networks, graph neural networks, transformers, LSTMs, and many more. And it is also a way to
design new machine learning architectures that are yet to be invented, maybe in the future,
not based on back propagation and incorporate inductive bias in a principled way. Being a
professor and a teacher, I would also like to emphasize the pedagogical dimension of this
geometric unification. What I often see in deep learning when deep learning is taught is that
it appears as a bunch of hacks with weaker or no justification. And I think it is best
illustrated with how, for example, the concept of convolution is explained. It is often given as
a formula just out of the blue, maybe with a bit of hand waving. But what we try to show is that
you can derive convolution from first principles in this particular case of translational symmetry.
And I think the difference in this approach is best captured by what Elvetsos once said
that the knowledge of principles easily compensates the lack of knowledge effects.
Professor Bronstein has been a professor at Imperial College in London for the last three years
and received his PhD with distinction from Technion, the Israeli Institute of Technology
in 2007. He's held visiting academic positions at MIT, Harvard, and Stanford, and his work has been
cited over 21,000 times. His main expertise is in theoretical and computational geometric methods
for machine learning and data science. And his research encompasses a broad spectrum of applications
ranging from computer vision and pattern recognition to geometry processing, computer graphics,
and biomedicine. Professor Bronstein coined and popularized the term geometric deep learning.
His startup company, Fabula AI, which was acquired by Twitter in 2019,
was one of the first applications of graph ML to the problem of misinformation detection.
I think it's no exaggeration to say that Professor Bronstein is the world's most recognizable expert
in graph representation learning research. We're really probably some of the nicest locations in
London, which is Kensington, if you're familiar. So we have the the Natural History Museum, the
Science Museum, and the Victoria and Albert Museum, and Imperial College is right here. So I think it's
as central as you can get. And if you walk all the way there, then you have Hyde Park,
which is probably one of the nicest parks in London. I'm a professor in the department of
computing at Imperial College London, and head of graph learning research at Twitter.
I work on geometric deep learning in particular on graph neural networks and their applications
from computer vision and graphics to computational biology and drug design. Dr. Petar Velichkovich
is a senior research scientist at DeepMind in London, and he obtained his PhD from Trinity College
in Cambridge. His research has been focused on geometric deep learning, and in particular,
devising neural network architectures for graph representation learning and its applications
in algorithmic reasoning and computational biology. Petar's work has been published in the
leading machine learning venues. Petar was the first author of graph attention networks,
a popular convolutional layer for graphs, and DeepGraph Infomax, a scalable unsupervised
learning pipeline for graphs. Hi, everyone. My name is Petar Velichkovich, and I'm a senior
research scientist at DeepMind. And previously, I have done my PhD in computer science at the
University of Cambridge, where I'm actually still based, and today we're actually here in Cambridge
filming these shots. And it is my great pleasure to be talking to you today about our work on
geometric deep learning and related topics. I first got into computer science through
competitive programming contests and classical algorithms, the likes of which you might find
in a traditional theoretical computer science textbook. And this was primarily influenced by the
way schooling worked for gifted students back in my hometown of Belgrade in Serbia,
where students were generally encouraged to take part in these theoretical contests and try to write
programs that are just going to finish as fast as possible or work as efficiently as possible
over a certain set of carefully contrived problems. All of this changed when I actually
started my computer science degree here at Cambridge, where I was suddenly exposed to a
much wider wealth of computer science topics than just theoretical computer science and algorithms.
And for a brief moment, my interests drifted elsewhere. Everything started to come back
together when I started my final year project with Professor Pietro Leo at Cambridge. And
I had heard that bioinformatics is a topic that's brimming with classical algorithms and
competitive programming algorithms specifically. So I thought a project in this area would be a
great way to bring these two closer. Unfortunately, that was not to be as my mentor very quickly drew
me into machine learning. And that kind of spiraled out into my PhD topics, where I was
for a brief moment focused on computational biology topics before eventually drifting to
graph representation learning and eventually geometric deep learning. My journey into
geometric deep learning started actually through investigating graph representation
learning, which I think for a very long time these two areas have been seen as almost synonymous with
one another, because almost everything you come up with in the area of geometric deep learning
can be if you squint hard enough seen as a special case of graph representation learning.
What originally brought me into this was an internship at Montreal's Artificial Intelligence
Institute, Miele, where I worked alongside Joshua Bengio and Adriana Romero on initially
methodologies for processing data that lives on meshes of the human brain. We found out that the
existing proposals for processing data over such a mesh, both in graph neural networks and otherwise,
were not the most adequate for the kind of data processing that we needed to do. And we needed
something that would be aligned more with image convolutions in spirit in a way that allows us
to give different influences to different neighbors in the mesh. And this led us to propose graph
attention networks, which was a paper that we published at ICLEAR 2018. It was actually my
first top tier conference publication, and what I'm probably most well known for nowadays.
The field of graph representation learning has then spiraled completely out of control in terms
of the quantity of papers being proposed. Only one year after the graph attention network paper
came out, I was reviewing for some of the conferences in the area, and I found on my
reviewing stack four or five papers that were extending graph attention nets in one way or
another. So the field certainly has become a lot more vibrant because of a nice barrier of entry,
which is not too high. Recently, PETA has been doing some really interesting research in algorithmic
reasoning. Part of the skill of a software engineer lies in choosing which algorithm to use. Only
rarely will an entirely novel algorithm be warranted. The key guarantee which traditional symbolic
algorithms give us is generalization to new situations. Traditional algorithms and the predictions
given by deep learning models have very different properties. The former provides strong guarantees,
but are inflexible to the problem being tackled, while the latter provide few guarantees, but
can adapt to a wide range of problems. Now PETA in his paper proposed a neural architecture,
which can take in natural inputs, but output a graph of abstract outputs, as well as natural
outputs. PETA believes that neural algorithmic reasoning will allow us to apply classical
algorithms on inputs that they were never originally designed for. I am studying algorithmic reasoning,
which is a novel area of representation learning that seeks to find neural networks that are as
good as possible at imitating the computations of exactly the kind of classical algorithms that
initially brought me to computer science. It turns out that this area is remarkably rich and
could have remarkably big implications for machine learning in general, because it could bring the
best of the algorithmic domain into the domain of neural networks. And if you look at the pros and
cons of the two, you'll find that they're very complementary. So the fusion of the two can
really bring the kinds of benefits we haven't seen before. So I am very pleased to say that
I'm among those researchers that is extremely proud and happy of what I do, because it brings
together some of my earliest passions in computer science with the latest trends in machine learning,
especially geometric deep learning, which we recently released a proto book about with
Jean Michael and Taco. We spoke to Christian Saagedi, and he's doing some interesting work with
algorithmic reasoning, creating abstract syntax trees to represent mathematical theorems, for
example. And then he believes that in that representation space, he projects it all into a
collidian space, that there's some interesting interpolative points in that space. But again,
surely there must be some deeper structure which analogizes mathematics, which would allow us to
extrapolate and discover new interesting mathematics. It just feels that what we're missing is the
right kind of structure. I think for mathematics, it's relatively easy to formalize it, because well,
we can write logic rules and basically we can build mathematics axiomatically from very basic
principles. These methods are already being used for computer proof of certain theorems. I think
it's not well regarded by the purists in pure mathematics, but probably they will need to accept
it. And well, you know, maybe there will be a Fields Medal that will be given for a proof that
is done by a computer. I think even recently, there are some important breakthrough results
proofs that were given by a computer. So it is probably just the beginning of a new way of doing
science, essentially, even as pure science as creative science as mathematics, which was considered
really the hallmark of human intelligence, it can be maybe if not replaced assisted by by artificial
intelligence. Petter invokes Daniel Kahneman, System One and System Two. He thinks that we
need something like System Two to achieve the kind of reasoning and generalization,
which currently eludes us in deep learning models. What I'm holding in my hands right now
is the international bestseller on thinking fast and slow from the famous Nobel Prize winner,
Daniel Kahneman. This book can be seen as one of the main theses behind my ongoing work in
algorithmic reasoning and what it stands for, because it argues that fundamentally, we as humans
employ two different systems that operate at different rates. System One, which primarily deals
with perceptive tasks, and System Two, which deals with longer range reasoning tasks. And it is my
belief that currently where our research in neural networks has taken us is to get really,
really good at automating away System One. So being able to perform perceptive tasks from large
quantities of observed data, probably in a not too dissimilar manner from the way humans do it.
What I feel is really missing from these architectures nowadays is the System Two aspect. Being able to
actually take these percepts that we've acquired from the environment and actually properly do
rigid reasoning over them in a manner that will stay consistent even if we drastically change the
number of objects, slightly perturb the laws of physics or something like that. In my opinion,
algorithmic reasoning, the art of capturing these kinds of reasoning computations inside a neural
network that was trained specifically for that purpose, and then slotting that neural network
into a different architecture that works with raw percepts, is one potentially very promising
blueprint that will take the space of classical algorithms that we have been building in this
System Two space and carry them over into raw perceptive inputs, which these algorithms were
very rarely designed to work over. This is Dr. Tako Kohen. Hello, I'm Tako Kohen. I'm a researcher
at Qualcomm AI Research and I work on geometric deep learning, equivariate networks, and more
recently also on causal inference and causal representation learning. Now, I've been interested
for a number of years already since about 2013 in the application of ideas around symmetry,
invariance, equivariance, and the underlying mathematics of group theory and group representation
theory to machine learning and deep learning specifically. And so it's been quite exciting
to see over the last few years, really the blossoming of this field that we now call geometric
deep learning. Many new methods such as various kinds of equivariate convolutions for different
spaces, different groups of symmetries, different geometric feature types,
equivariate transformers and attention mechanisms, point cloud networks, graph neural networks,
and so forth. And along with these new methods, also a large number of applications that have
been tackled, anything from medical imaging to the analysis of global weather and climate
data to the analysis of DNA sequences and proteins and other kinds of molecules.
So if you apply this mindset, the first question you ask when faced with such a new problem is
what are the symmetries? What are the transformations that I can apply to my data
that may change the numerical representation of my data as stored in my computer, but that nevertheless
don't change the underlying object we're interested in, whether that's reordering
the nodes of a graph, rotating a molecule in 3D or many other kinds of symmetries.
Once you know the group of symmetries, you can then develop a neural network that's equivariate
symmetries and hopefully is a universal approximator among equivariate functions.
What we found, what many others have found time and again, is that if you
build this symmetry prior to your network, if you make your network equivariate,
it is bound to be much more data efficient and to generalize much better.
Hey Tim, great to be here. So yeah, the blueprint. So what we realized as we were writing this
book is that really a lot of different architectures, they can be understood in one as essentially a
special cases of one generic structure that we call the geometric deep learning blueprint.
So to explain a little bit about what this is all about, the blueprint refers to first of all
feature spaces. So I'll explain how we model those and then it refers to maps between feature
spaces or layers of the network and they also have to somehow respect the structure of the
feature spaces. So in all cases, whether it's you know, graph neural net, a network for processing
images on the plane or a network for processing signals on a sphere like global weather data,
we're dealing with data that lives on a domain. So the domain in the examples I just gave would
be the set of nodes of the graph or perhaps also the set of edges, the points on the plane
or the points on the sphere. And of course you can think of many other examples as well.
This is what we call the domain. We write it as omega. It's typically it's a set, first of all,
and it may have some additional structure. So in the case of the sphere, it has some interesting
topology. And typically we also want to think about the geometry, want to think about distances
and angles. So it's a set with some kind of structure. And in addition, it has some symmetries,
meaning there's some transformations we can do to this set that will preserve the structure that we
think is important. So if we think distances are important on, let's say the sphere, when the kinds
of symmetries we end up with our rotations, three dimensional rotations of the sphere,
perhaps also reflections. In the case of a graph, the symmetries would be
permutations of the nodes and also corresponding permutation of the edges.
So you just change the order of the nodes. If node one and two were connected, you apply
permutation and move those to node three and four, then now three and four have to be connected.
So that's a symmetry of our space. Now, the data, this is a very important point,
the data are typically not points in this space. So when we're classifying images,
while our space is the plane, but the data are not points in the plane, they're not
the two dimensional vectors, the data is really a signal on the plane, a two dimensional image,
which, so you can think of that as a function from for each point in the plane, we have a pixel
value. So in the more general cases, it might be something called a field. So you might say have
wind direction on earth, that's a vector field on the sphere. Now, the symmetries that we talked
about, they act on this space, you could show how they automatically also act on the space
of signals. So those are the key ingredients to define what a feature space is. You have
your your space, omega, so set of nodes, sphere, whatever. Then you have a group of symmetries
of that space. And then you have the space of signals or feature maps on this space. And you
have the group acting on your signal. So you can rotate a vector field or you can shift a planar
image or etc. So those are the key ingredients to define a feature space. And then we can talk
about layers. And so the layers or maps of the network, they have to respect this structure.
So if we have a signal on on the sphere, and we believe that rotating it doesn't change it in
any essential way, or we permute the nodes in the graph, but it's still the same graph, then we want
the layers of the network to respect that. And to respect this structure means to be
active variant to the symmetries. So if we apply our transformation to our signal,
and then we apply our network layer, it should be the same as applying the network layer to the
original input, and then applying a transformation in the output space. Now how the transformation
acts in the output space could be different from the input space. So for example, you could have a
vector field as input, and a scalar field as output, they transform differently. So that's why
for each layer in the network, you're going to have a different feature space with a different
action of the group. But it's the same group acting in each feature space, the maps, they
should be equivariant. And these maps, they include both the linear maps, which are typically the
learnable layers, and the non linear. Now for linear maps, you can study all sorts of interesting
questions, you can ask what is the most general kind of acquisition linear map. And it turns out
that for large class of group actions, or linear group actions, the most general kind of acquisition
linear maps are generalized forms of convolutions. So that's really an explanation for why convolutions
are so ubiquitous. The final ingredient that I think is key to the success of many architectures
is some kind of pooling or cautioning operation. So the structures we've talked about so far are
you know, this global space and the symmetries on it, etc. But typically, there's also a notion
of distance or locality in our space. And if we just enforce that our layers have to respect the
symmetries, well, that would force us to use a convolution in many cases, and I just mentioned,
but not forces to use local filters. And we all know that using sort of global filters is not
going to be very effective use of parameters. So locality is another key thing, locality in the
filters, and also locality is exploited via some kind of pooling or caution operation.
Now, going forward to say the year 2020, deep learning is all the craze right now. And so many
different deep learning architectures are being proposed, convolutional neural networks, graph
neural networks, LSTMs, and so on. When these architectures are being proposed, different
terminologies are used because people tend to come from different areas when they're proposing them.
And also, they are usually followed by kinds of bombastic statements such as everything can
be seen as a special case of a convolutional neural network, transformers use self-attention,
and attention is all you need. Graph neural networks work on graphs and everything can be
represented as a graph. And LSTMs are Turing complete, so why would you ever need anything else?
So, I hope that this illustrates how the field of deep learning in the year of 2020
is really not all that different from the state of geometry in the 1800s. And if history teaches
anything about how we can unify these fields together, now is the right time for us to look
back, study the geometric principles underlying the architectures that we use, and as a result,
we might just derive a blueprint that will allow us to reason about all the architectures we have
in the past, but also any architectures that we might come up with in the future. And in my opinion,
that is the key selling point of our recently released proto book. And I hope that it is helpful
in guiding deep learning research going forward. I should highlight that I was also extremely,
extremely honored to deliver the first version of the talk presenting our proto book at
Friedrich Alexander University of Erlang, in exactly the same place where the Erlang program
was originally brought up, albeit because of the existing COVID restrictions, I had to do so in a
virtual manner. So, I think a lot of people understand convolutions in the way of a traditional
plain RCNN and the mathematical notion of a convolution, which is very closely related to,
you know, a Fourier transform, for example. But graph convolutional networks, they seem to
abstract the notion of a convolution into some kind of concept of the neighborhood and local
connectivity. And some of your work as well with, you know, equivariate convolutional neural networks,
I think do the same thing. So, when we talk about convolution, are we talking about a very abstract
notion of it? Yeah, that's a great question. I think that there are so many different ways to get
convolution. You can think of it in sort of, you try to think of it in terms of sliding a
filter over some space, right? So, you put it in a canonical position and then you slide it around.
That is an idea that you can generalize to not just sliding over a plane, but say,
applying some transformation from a group to your filter. So, let's say you have a filter on the sphere,
then you have, you know, the sphere has a symmetry group, mainly three-dimensional rotations,
group SO3. You can apply an element of SO3, a three-dimensional rotation to your filter,
and you can sort of slide it over the sphere. That leads to something called group convolution.
So, that's one way to think about it. There is indeed, as you mentioned, the spectral way of
looking at it. So, you could think there's the famous Fourier convolution theorem,
which says that in the spectrum, a convolution is just a point-wise product.
So, one way to implement a convolution would be to take a Fourier transform of your signal,
your feature map, and a Fourier transform of your filter, multiply them point-wise,
and then inverse Fourier transform to get the result. This perspective also generalizes. So,
it generalizes to graphs where the Fourier transform or something analogous to it
can be obtained via graph laplacians as well as some other ideas. That is actually historically how
some of the first graph neural nets were implemented and motivated. There's also
a spectral theory for group convolutions. So, indeed, the Fourier transform can be generalized
or the standard Fourier transform that we all know is actually the Fourier transform for the plane,
the plane being a particular group, the translation group in two dimensions. So, there is a very
beautiful theory of, let's say, generalized Fourier transforms for groups where now the
spectrum is indexed not just by the integers, as it is the case for the line or the plane,
but by something called irreducible representations. In the case of the plane,
those are indeed indexed by integers, and the spectrum is not just scalar-valued or
complex scalar-valued, but it can be matrix-valued. If you're interested in this sort of stuff,
you want a very high-level description, you can check out our paper on spherical CNNs,
where we actually implement convolution on a sphere using this kind of generalized Fourier
transform. So, that's the Fourier perspective on convolutions, and there's a final perspective,
which I think is quite intuitive, which is that the convolution is the most general kind of
equivariate linear map between certain linear group actions. So, specifically, these group
actions are the way that groups tend to act on the space of signals on your space. So,
you might have space of scalar signals on the sphere, and your feature map, you might want to
have another scalar signal on the sphere or a vector field on the sphere or something. Then,
you can ask, what is the most general kind of equivariate linear map? And the answer is,
it's a convolution, and that is also true in a very general setting. So, that to me is the
most intuitive way of understanding convolutions as the most general kind of maps that are linear
maps that are equivariate to certain kinds of group actions. Professor Bruner, welcome to MLST.
It's an absolutely honor to have you on. Introduce yourself. Yeah. Hi, team. I'm
Joan Bruner. I'm a associate professor at the Quran Institute and center for data science at
New York University, and I'm very happy to be here chatting with you at the MLST.
Joan, you just released this Geometric Deep Learning Proto book. What does it mean to you?
What was your kind of intellectual journey that led to this?
Yeah. So, this journey, in fact, started many years ago. So, I would say, even during my postdoc,
I was doing my postdoc here at NYU with Yanle Kun. And that was the time, 2013, where
confnets were already showing a big promise in image tasks. And discussing with Yan, the questions
are, okay, how about domains that are not like grades? And that was the beginning of a journey
that also included my former collaborator, Arthur Slump, who was another researcher in the lab that
had a similar background as me, coming from applied math, but looking into more and more deep
learning. So, I would say that the genesis was our first attempt at extending the success of
convolutional networks to these regular domains. And we published the paper at iClear 2014. And
after that, I think things started like quite naturally because other researchers that had,
I would say, came from the same background, like, you know, maybe from geometric background,
also started to realize that there was something there, maybe bigger than these particular papers,
in particular, Michael Bronstem, kind of reached out to us just afterwards, saying, oh, and we are
also looking at similar ideas. I think we should just team together and start to think about these
more globally. And so, you know, things that developed from there. And we wrote this journal
paper, like a review paper in 2017, with Arthur, Yan, Michael, Pierre van der Geist, who is another
very well-known figure in this area. And so, from there, I think that, yeah, things like
started to slowly take off. We had tutorials and new reviews that we had a very successful
workshop at IPAM. After this, I think that the thing were clear that, okay, maybe at some point
in the future, we should try to stamp all these things into a book that tries to reflect something
a bit more, let's say, mature and, you know, what is our, if we wanted to have, like, some kind of
legacy for future generations on how to implement and communicate these methods, how would that be?
So, I guess that was the genesis of the book. And so, very soon, we said with Michael that we also
would like to have some, you know, fresh minds and fresh energy on board. So, naturally, the names
of Taco and Petter came very naturally to us, as people who had been doing excellent work in the
domain that would very nicely complement our skills. So, the team was created and that's the
project. And so, yeah, I mean, I think it's been very interesting so far. Of course, I guess that,
as you know, this is just an ongoing project, right? So, it's still not finalized. But hopefully,
we're getting an interest from the community and this gives us some kind of, I would say,
positive vibes to finish it on time. As you know, writing books is always like this never ending
process. So, I think that, yeah, it's been an interesting endeavor so far, for sure.
I asked Professor Bromstein what his most passionately held belief is about machine learning.
I think machine learning is such a field where holding strong beliefs is often counterproductive.
It happened to me multiple times that something that I thought or said was very quickly overturned.
And what I mean is that milestones or progress was achieved much faster than I could even imagine
in a wild dream. So, making predictions about machine learning is, to some extent, an ungrateful
job. I do, however, believe that in order to make progress to the next level and make machine learning
achieve its potential to become the transformative technology we trust and use ubiquitously,
it must be built on solid mathematical foundations. And I also think that machine learning will
drive future scientific breakthroughs. And probably a good litmus test would be a noble prize
awarded for a discovery made by or with the help of an ML system. It might already happen in the
next decade. I wanted to go into a few questions, actually, about the book. So, first of all,
Joanne, in your 2017 paper, The Mathematics of Deep Learning, you cited the universal function
approximation theorem, which is to say the ability of a shallow neural network to approximate
arbitrary functions. But the performance of wide and shallow neural networks can be significantly
beaten by deep networks. And you said that one of the possible explanations is that deep
architectures are able to better capture invariant properties of the data compared to their shallow
counterparts. Could you just briefly introduce the universal function approximation theorem?
And do you think it's still relevant for today's neural networks?
Yeah, I mean, that's a very deep and an important question. Yes, so universal approximation theorem
it refers to this very general principle that once you define parametric class, let's say you're
learn functions using neural nets, it just describes your ability that as you put more
and more parameters into your class, you're going to able to approximate essentially anything that
data nature throws at you. And so this might seem like a very powerful property, but in fact,
it's something that you have probably already encountered many times during your undergrad.
I mean, if you have any, let's say, background in, you know, signal processing, electrical
engineering, there's many ways in which students have learned how to represent data, like signals,
for example, using Fourier transform. So Fourier transforms are an instance of a class that has
universal approximation. So in that sense, it's a, I think, going back to the second part of your
question, how relevant it is to in the context of neural networks, and how far does this thing
push us towards understanding why deep learning works. So I would say there's two sides of the
answer. On one hand, I think that universal approximation is a tool that when you combine
it with other elements, it becomes something that provides good guiding principles. For instance,
universal approximation of a generic function, we know that, yeah, we can, as I said, we can
obtain it with, you know, very, very naive architectures, for example, just a shallow
neural network without any kind of physical structure, special structure already has this
property. Does it actually help us to learn very efficiently? No, right? And I'm going to go at this
afterwards. But when you combine it, for example, with, you know, let's say that now your data
lives on a graph, or your data, I don't know, has a certain, like come from a physical lab that
has certain properties, let's say that it's rotational environment. Like the first thing that
the designer, like a domain scientist would like to know, if you come there and you design your
neural net, look, I have a neural net that takes your data and has very good performance. The first
thing that he will ask is, okay, how general is your architecture, right? Can it explain anything
that they could throw at you? It seems like in that sense, I would present it more as a sufficient
condition, like a check mark that your, you know, your architecture needs to fall, right? If you
make more and more parameters, can you express more and more elements, functions from your class?
But then, as I said, is it far from being sufficient, right? It's like, sorry, it's a
necessary condition, but it's far from being sufficient in the sense that universal approximation
has a flavor. It's a result that does not quantify how many parameters do I need, right? Like, you
know, if I want to approximate function, let's say I want to classify between different dog breeds,
it doesn't tell me, this theorem doesn't tell me how many parameters, how many neurons do I need
for that, right? It's a statement that in that sense, it leaves you a little bit with your,
like, say, like, you know, like, it's a bittersweet result, right? It doesn't really tell you anything
actually. So that's why, and that's why we enter this, this other question that is actually much
deeper, and to some extent, still reading them pretty much open, that is, how do you actually
go from this, this statement to something that is quantitative, right? Something that tells you,
okay, you know, you need that many layers, you need that many parameters. And so this is where
the role of depth in neural networks is, you know, becomes essentially the key open question.
And yeah, so in this quote that you brought from this paper, that kind of reflected our
understanding at the time of maybe the true power of universal approximation is, you know,
when as you combine it with this other prior, that is the asymmetries of the data, like the
invariances. So I don't want to represent arbitrary functions, I only want to represent
functions that are invariant to certain transformations of the input. So in fact,
our, at least my particular view of the problem, analysis of the problem has somehow evolved in
the last years, right? Of course, through research that I've done together with my collaborators.
And now, and this is actually the way we present it in the book, we kind of identify
two different flavors, two different sources of prior information that one needs to bake into the
problem, right, to really go beyond this like basic approximation result of neural nets.
The first one indeed is invariance, right, is a disability that you need to, like the fact that
you actually put symmetries into the architecture is certainly going to have a benefit in terms of
sample complexity, right? I mean, you are going to learn more efficiently if your model is aware
of the symmetries of the world. But in fact, this prior, in fact, we know now that it's not
sufficient, right? If you only like agnostically build your learning system, just with these
symmetries in mind, indeed, you are going to become more efficient that a system that is
completely agnostic to symmetries. But it might not be, you might not be able to formally establish
what we call like a learning guarantee that has good sample complexity. And I guess that I don't
want to go too much into the jargon and the details of what this means. But the idea is that if I want
to, you know, learn this function with certain precision, how many examples, how many training
examples do I need to kind of give you a certification like a guarantee that I'm going to be able to do
that. So with symmetries alone, it's not something that we know how to do. In fact, we are, we
believe we have strong beliefs that it's not possible, right? There's examples out there
that I could, I could, you know, construct a function that has the right symmetries,
has the right priors if you want, but still needs a lot, a lot of examples to be able to learn.
So what we need is to add something else into the mix. And this is this something that we call in
in the book, this scale separation. And, and, and if you want, I can try to very briefly give you
an intuitive idea of what this means. So if you think about the problem of classifying an image
like a dog or the cat. So what is given to you is like a big branch of pixels, right? Every pixel
has a color value. So somehow you need to figure out the thing that you're looking for is lying
in some kind of like, it's really through the interactions between pixels that you get the
answer, right? And the question is, of course, if I have 1000 pixels, how many possible interactions
do I have between 1000 elements? So this is where this exponential or the curse of dimensionality
appears, right? I need to, a priori, I should be looking at all possible families of interactions
between pixels. And these are maybe where my signal would be living. Of course, if I need to look for
all of these things, it's impossible, right? There's just too many things. If I tell you that the,
you know, these interactions are such that there's this translation symmetry.
Well, you might not be, you might not be needing to look at all of them. But in fact, you don't need,
you do not throw enough. So what is really something that is powerful is that I tell you
that maybe the interactions that matter the most are those between a pixel and its neighbors,
right? And if you understand very well, if you base your initial learning steps
into understanding well, which local interaction matters, maybe you can use them to bootstrap
the interactions that go to look at this neighborhood to slightly bigger neighborhoods,
right? And so this idea that you can break a very complicated problem into a families of sub-problems
that lives in different scales, this is at the, I would say, at the intuitive level,
something print like at the core of the essence of why these architectures are so efficient.
Well, this idea, as you might imagine, is not new. It's not specific to deep learning.
The idea that you can take a complicated system of interacting particles and break it into different
scales. This is at the, at the basis of essentially all of physics and chemistry, right? There's many,
many, you know, like when people study even like biology life, right? You have, you have experts that
are very experts at the molecular level. Then you have experts that, you know, might understand
like, you know, doctors that understand things at the level of okay functions. And then there's
maybe experts at the level of the society, right? But this, you know, it's pretty natural to break
the very complicated thing into different scales. And so deep neural networks somehow are able to
do that. We don't have the full mathematical picture, right? Of, for example, why this scale
separation is strictly necessary. What we know from empirical evidence, like that is now, I would
say indisputable, is that this is an efficient way to do that, right? Because when I was,
when I was reading the prototype book, I noticed that there was a separation between the symmetries
and the scale separation. Could you explain in simple terms, why is the scale separation not
just a symmetry as well? Because it seemed a little bit, I don't want to say kluge, but it seemed like
you had this scale matter and you dealt with it separately.
Yeah, that's a good, that's a good point. So, so maybe that the way to, to, to separate these two
would be if you think about like an algorithmic instantiation, if you want to have a network
that would just break the problem into different scales, it would be like a neural network that
would operate at different patches. And for every patch of the image, I could be learning
independent set of parameters, right? So that would be a model that is only told that it should be
breaking the problem into different regions. But it's not necessarily told that, you know,
there's a weight sharing, right? There's some kind of like parameter sharing that somehow
is, you are able that, you know, in a sense is helping you to learn with fewer number of parameters.
So somehow these two, these two conditions are slightly complementary. We, as I said,
there's still like a lot of mathematical puzzles as to how these things interact optimally.
And I think that the, the, the, one of the reasons why we chose to explain the story
into two different, in these two different priors is that they all survive this quest
for generality in the sense that these two principles are, again, something that you
can think about for grids, for groups, for graphs. You can also see these principles
appearing completely everywhere as you study physical systems, right? Like the scale and the
symmetry is really at the core of, of many physical theories. And I would say that there's also,
at the more maybe technical level, these two priors somehow have been instrumental to
organize like to basically to have a kind of a recipe to build architectures, right? So,
so maybe now we, we don't even think about it, right? But when you have a new problem,
a new domain, and you need to build an efficient neural network, we automatically have this idea
that, okay, we are going to start learning by composition, right? So we are going to extract
information one layer at a time. That's the first appearance of scale. And we know that the way
we need to organize these layers, right? How do you parameterize a layer that
takes some input features and produces maybe better features?
This idea that we do that by understanding this kind of equivalence structure, right? We, we,
we, we have this notion of filters, right? In, in convolutional networks, we have this,
I said, we, we organize everything in terms of filters. When we talk about message, I grab
your networks, we have this kind of like a diffusion filters, right? And, and so there's
this object that we extract from the domain that is helping us, that giving us something very
constructive, very, you know, very relevant, like very practical. And so this is really the,
this underlying group of transformations that is acting on our domain. And so I would say that,
you know, from a practitioner's perspective, these, these two principles, right, that I'm going to
learn by composing some, some like fundamental layer that I repeat all the time. And the way
this layer is organized, this structure through this like group transformation, this has been,
I would say, like a, like a trademark of, you know, the success of neural networks. Of course,
as also we mentioned in the book, this is, these are, I would say, proto, like meta,
you know, meta parameterization, right? In the sense that there's, as you know,
many, many, many variants that people have come up with, many, many, let's say,
yeah, like modifications on the basic architecture that have really made dramatic
changes in performance, right? So there's, there's, of course, as I just said, like the devil,
sometimes it's in the details, right? And so as, as, as we are writing the book, we are
realizing exactly, you know, how some of the changes in the architectures are actually fit into this,
what we call this blueprint, right? This symmetrically learning blueprint.
Fascinating. Okay. So I wanted to come back to what you were saying a few minutes ago about the,
the sample efficiency of these models. Now, with graph neural networks, for example,
there are factorially many permutations of adjacency matrices for a given graph. And
I want to talk about a sorting algorithm, right? So Francois Choulet came on the podcast and he
said that in order to learn a sorting algorithm that generalizes, you would need to learn point by
point, you would need to see factorially many examples of permutations of numbers.
Do you think that we could train a neural network to learn a sorting? I guess what I'm asking in
a roundabout way is, do you think there's a kind of geometry to computation itself?
Good. That's a very good question. And in fact, we have some, some recent work with some collaborators
in my group where we kind of take up this question from, and we try to formalize it mathematically
and we give answers to this question. And so many of these like a computational tasks
that, that you were mentioning, for example, sorting or, you know, like, like algorithm,
right? Like algorithmic tasks. They are, if one wants to put them into some mathematical context,
the first thing that comes to mind is these are functions that already enjoy some symmetries.
For example, like a sorting algorithm, right, is invariant to permutations, right? So the function
that you are trying to learn is an arbitrary function that has this symmetric class. And so,
as such, as I was saying in the beginning, you can try to address this question saying, okay,
now you give me an arbitrary. So, so the question would be relative to an arbitrary learning
that is agnostic to symmetry. How much does a symmetric learner gain, right? So you can basically
like try to understand, quantify the gains of sample complexity of learning without symmetry
versus learning with symmetry. And so the punchline of this work, the recent work that we
completed is that one can actually quantify the sample complexity gains. And these are of the
order of the size of the group. And so here in the case of like permutations, what it means is that
if a learner is aware of the symmetry, like one training example of the symmetric learner
is rosely equivalent to n factorial samples of the agnostic learner, right? Which is something
that you would expect, like if you think in terms of data mutation, right? Like if I tell you in
advance that your function is symmetric, is invariant to permutations, it's, you know,
like a brute force approach would say, okay, you give me an instant training and input, right?
And instead of giving you this input, I'm just going to, you know, like permute, like have any
possible permutation of the input and you already know the output, right? So you can as well feed
it to the learner. So this is like heuristic at this addition turns out to be mathematically
correct, precise, at least, you know, under some conditions, right? But I guess the whole point
is that these gains might look amazing, like might look like a, you know, like a big boost in
sample complexity. As I said before, there's a grain of salt here is that the in these conditions,
if the general conditions, you are already fighting an essential and impossible problem in the sense
that the rate like the sample complexity is dominated by a rate of basically the rate in
which you learn is what we call curse by dimension. And that's what it means is that if I want to,
you know, I have a certain performance generalization error, let's say that now I want to
ask you the question, if I want to divide this generalization error by two,
how many more samples do I need to give you, right? Like, so if I want to double, you know,
like double, like reduce the my error by half, by how much do I need to give you more samples?
So this dependency, in fact, is exponential in dimension, right? So basically the,
the, the sample complexity gains by invariance, they are exponentially in dimension, but they are
fighting an impossible problem that is already cursed by dimension. So what it means is that at
the end of the day, this is what, what, you know, what was in the, in the, like in the heart of what
I was saying before, is that invariance alone might not be efficient, might not be sufficient,
right? Because you are, okay, you are taking a very hard problem, you are removing an exponential
factor, but you still have many x, you know, you still have something in the exponent that is
exponential, right? So, so, so what it means is that, that, I mean, that's what really underpins
why we think in these terms of x combining symmetry prior with the scale separation prior.
But certainly the algorithmic paths are very interesting playground because I think that
for the case of sorting, I mean, as you know, scale separation is also an issue. It's also a
very important thing, right? I mean, it, this is what basically is at the heart of the dynamic
programming approaches, right? Like these efficient algorithms that are not only
efficiently statistically, but also efficiently computation, right? This idea that you can
divide and conquer. So, so, so, so algorithmic tasks also are kind of exposed to this dual
physical prior, right? Of a scale and invariance. On the curse of dimensionality, there's this issue
where you have a data point and you want to surround it by other data points in two dimensions to
create a convex hole. And as you increase the number of dimensions, the number of data points
you need to create this covering to create a kind of interpolative space increases exponentially.
And when you get past a certain number of dimensions, let's say 16 or not very many,
you would need essentially more data points than there are atoms in the universe.
So this leads to a very interesting realization. I think some people refer to it as the manifold
hypothesis, which is that most natural data is only really spatially novel on very few dimensions.
And a lot of data falls on very smooth, low dimensional manifolds. But what are the implications
of this? Essentially, all machine learning problems that we need to deal nowadays are extremely
high dimensional. So even if we take very modestly sized images, they live in thousands or even in
millions of dimensions. And if you think of machine learning or at least the simplest setting of
machine learning as a kind of glorified function interpolation, the standard approach is to
function interpolation as just use the data points to predict the values of your function
will simply not work because of the phenomenon of curse of the curse of dimensionality,
that increasing the number of dimensions, the number of such points blows up exponentially.
So what you really need to take into account, and probably this is really what makes machine
learning work in practice, is the assumption that there is some intrinsic structure to the data,
and it can be captured in different ways. So it's either the manifold assumption,
where you can assume that the data, even though it lives in a very high dimensional space,
intrinsically, it is low dimensional. This can be captured also in the form of symmetry. For example,
image is not just a high dimensional vector, it has underlying grid structure and grid structure
has symmetry. This is what captured in convolutional networks in the form of shared weights that
translates into the convolution operation. I think the symmetries are part of the magic here,
because it's not just the interesting observation that natural data is only spatially novel in
so few dimensions. There's something magic about symmetries. And when we spoke to Francois
Choulet recently, he invoked the kaleidoscope effect, which is this notion that almost all
information in reality is a copy of some other information. Probably here it will be a little
bit stretching, but I would say that because data comes from nature, from physical processes that
produce it, physics and nature itself is in a sense low dimensional. So it's an application of
simple rules at multiple scales. You can create very complex systems with very simple rules.
And this is probably how our data that we are mostly interested in in machine learning is
structured. So you have this manifestation of symmetry and self-similarity through different
scales. The principles of symmetry and certain environs or equivirons and of scale separation,
where you can separate your problem to multiple scales and deal with it at different levels of
resolution. And this is captured, for example, in pooling operations in convolutional neural
networks and in other deep learning architectures. This is what makes deep learning systems work.
Fascinating. It's so good that you raised the curse of dimensionality because I was going to
ask you about that. Could you explain in really simple terms, so not invoking lip ships? I can't
even say it properly now, but you know, not invoking a mathematical jargon. And why exactly in your
articulation does geometric deep learning reduce the impact of the curse of dimensionality?
Yeah. So the curse of dimensionality, it refers generally to the inability of
algorithms to keep certifying certain performance as the data becomes more complex. And data becoming
more complex here means that you have more and more dimensions, right? More and more pixels.
And so this inability of, you know, like scaling, basically, it's like it really says that if I
scale up the input, my algorithm is going to have more and more trouble to keep the base.
And so this curse can take different flavors, right? So this curse might have like a statistical
reason in the sense that as I make my input space bigger, there would be many, many, many
much exponentially more functions, real functions out there that would explain the training set
that would basically pass through the training points. And so the more dimensions I add,
the more uncertainty I have about the true function, right? So I would need more and more
training samples to keep the pace. This curse can also be from the approximation side, right?
So in the sense that the number of neurons that I'm considering to approximate my target function,
I've I need to keep adding more and more neurons at the rate that is exponential in dimension.
And the curse can also be from the computational side, right? The sense that if I keep adding
parameters and parameters to my training model, I might have to optimize, you know, to solve an
optimization problem that becomes exponentially harder. And so you can see that you are basically
bombarded by three different by all angles. And so an algorithm like, you know, here in the context
of statistical learning or learning theory, if you want, having a kind of a theorem that would say,
yes, I can promise you that you can learn, you need to actually solve these three problems at
once, right? You need to be able to say that that in their conditions that you're studying,
you have an algorithm that it does not suffer from approximation nor statistical nor computational
curses. So as you can imagine, it's very hard, right? Because then you need to master many
things at the same time. So why do we think that geometric deep learning is at least an important
piece to overcome this curse? As I said before, so geometric deep learning is really a device
to put more structure into the target function, right? So basically to make the learning problem
easier because we are, we, you know, we are, we are promising the learner more properties about
target function, right? We are basically making the hypothesis class if you want smaller.
That said, as I said, there's, there's still some path to go, right? As we were describing,
just a bunch of principles that make these hypothesis spaces smaller and more adapted to
the real world. But one thing that we are still lacking, for example, is the guarantee on terms
of optimization, right? I mean, I described that the depth of these architectures is somehow
something that is akin associated with a scale, right? The fact that you need to understand things
at different scales. So as you know, in the, from the optimization side, there's still some
open questions and open mysteries as to why the gradient descent, for example, is able to find
good solutions, right? So these are things that we believe that these architectures can be optimized
efficiently just because we have this experimental evidence that is piling up. But we are, for example,
we are still lacking theoretical guarantees. For the approximation, we, it's a, it's a bit of the
same story, right? So we understand very well, approximation properties of shallow networks,
starting from universal approximation, but of course, many, many recent interesting work,
but we are also still lagging a little bit behind in approximation properties for deeper
networks. So as you see, it's like a, you can, you can see from, from this discussion that,
yes, we have some, you know, good reasons to believe that these are fundamental principles
of learning, but there's also a bunch of mathematical questions that are still open. And, and, and
this is also one of the things that I like about writing a book on this topic, because it's a very
life domain, right? It's not, you know, as you can see, the field is still evolving. And, and it's,
I think it's a good time to, yeah, it's a good time. I mean, researchers out there are listening
to us. It's a good time to, to, to think and to work and to join this, this program.
Amazing. Will we ever understand the approximation properties of deep networks? Because with the,
the shallow, you know, function approximation algorithm, it's, you can almost think of a neural
network as being kind of like sparse coding and, you know, the more neurons you have, you're just
kind of discreetly fitting this arbitrary function, but you don't have that visual intuition quite so
much with the deep networks. Yeah, I mean, it's an, it's an important, and it's a deep question. So,
so indeed shallow neural networks are really, really correspond to this idea that you learn
a function by stacking a linear combination of basis elements, right? And, and this is really
at the roots of essentially all of harmonic analysis or functionalized. I typically think
about the basis and you ask questions about the linear approximation or in approximation, etc.
Deep neural networks, they introduce a fundamentally different way to approximate
functions that is by composing, right, by composition. And so you're right. Our knowledge
about this question right now is mostly concentrated in what we call separation results,
like a depth separation, which consists in trying to find, construct mathematical examples of functions
that cannot be approximated with shallow neural networks with certain number of neurons.
But indeed, they can be much better approximated with deep neural networks, right? So this is really
understanding which kinds of functions benefit fundamentally from composition, rather than
from addition. And so there's a certain mathematical, you know, vision and mathematical
intuition that is building up. But of course, it's still very far from from and this explaining
that the true power of depth. And just to give you like a final pointer here, there's, there's a very
related question that replaces neural networks as we understand them with what we call Boolean
functions, right? Like these are just circuits, arithmetic circuits that take as input some
bit string, and they can manipulate the bits by, you know, or operations and operations,
and they can keep adding gates. And so this question about what is the ability of a certain
circuit architecture to approximate certain Boolean functions is actually a notoriously
hard and basically has been studied in the theoretical computer science community
since the 50s and the 60s, right? And there's actually very, very, very deep results
and very challenging actually open questions concerning these things. So this is really,
we are really touching here questions that are pretty serious at the deep mathematical and
theoretical level. And so, yes, you should not expect that in the year in the next year or two,
we have a complete understanding of, you know, approximation powers of any architecture with
any depth. But I think you should expect that the theory like this will continue to try to catch up
with the experiments. And so we are, I think we, yeah, I think we are hoping to get like a more
precise mathematical understanding of the role of depth. And as I said before, there's one thing
that is fascinating about this domain that is maybe very unique to deep learning is this very
strong interaction between optimization, statistics and approximation, right? Maybe it turns out that,
you know, the like the huge depth that we have in these residual neural networks might not be
necessary from the approximation side. But in fact, it's so useful for the optimization that
overall is a big winner, right? So there's always this like twists about this question
that are fundamentally mixing these three sources of error.
I'm sorry for asking you this question, but can neural networks extrapolate outside of the
training data? That's a good question. I would say that the answer, I guess, depends on your
specifications, right? So I guess that the conservative answer of a statistical learning
person would be no, because we don't have good theorems right now that tell us that this is the
case. There's very like, you know, like a strong effort, both from the practical and the theoretical
community to really understand this question, like by trying to formalize it a bit more,
I mean, what do we mean by distribution shift? You know, what kind of training procedures you
can come up with that would give you precisely this kind of robustness? There's of course,
what we call these biases, right? I mean, I can always take it like a training distribution,
I make a choice of a certain architecture, I'm going to learn a function. And obviously,
there's some directions if you want in the space of distributions, for which my hypothesis will
turn out to be have good generalization. There might be other direction in which the the country is
through. So there's actually very nice work for from Stephanie Gejalkas group at MIT,
where they, for example, they study this question in the context of value networks,
also including graphs, where they, for example, they discover or they identify
this strong preference for value networks to generalize along linear directions, right,
in the sense that if I just decide to now, you know, shift my data in linear directions,
then my function has no trouble generalizing. Maybe there's other direction, right, in which
this thing is actually catastrophic. So I think that the question I think it's very important
from, let's say, it's very important from a kind of a practitioner perspective, right, that's
typically that's clearly something that a user would like to know. But I think that from a more
mathematical or theoretical level, I think we are still at the stage of trying to formalize like,
okay, what do we exactly mean by extrapolation? And what are the kind of the conditions for
with architecture can do it. And I think that, yeah, this is a, yeah, it's an important question.
But yeah, I think we're still pretty far from having a full answer.
Amazing. And final question, what areas of mathematics do people need to study
before reading the proto book?
Good question. So I think that our objective and our really like the idea is really to have
something that is quite self-contained, in the sense that we are going to provide appendices
that expand on the areas that is maybe they're not typically kind of the bread and butter
of machine learning people. For example, we are going to have an appendix on group theory,
differential geometry, harmonic analysis. So these are areas that I think are going to be there for
you to delve into, into to get like the most of the paper, the most of the book. But I think that
other than that, any basic, you know, any basic knowledge of linear algebra, statistics, and
analysis will will do. So like, if you have taken a graduate level class in machine learning,
you should be ready to go.
Rather than just being applied in a lot of really important branches of research problems,
people outside of pure research have recognized that a lot of the data that comes to us from
nature is most naturally represented in a graph structured form. Very rarely will nature give
us something that can be representable as an image or a sequence. That's super rare. So very often
the structure is more irregular, more graph like, and therefore graph neural networks have already
seen a lot of applications in domains where the data is super naturally represented as a graph.
In the domain of computational chemistry, where you can represent molecules as graphs of atoms
and bonds between them, the graph neural networks have already proven impactful in detecting
novel potent antibiotics that previously were completely overlooked because of their unusual
structure. In the area of chip design, graph neural networks are powering systems that are
developing the latest generation of Google's machine learning chips, the TPU. Furthermore,
graph structured data is super ubiquitous in social networks and the kinds of networks maintained by
many big industry players. And accordingly, graph neural networks are already used to serve
various kinds of content in production to billions of users on a daily basis. In fact, the recommendation
system at Pinterest, the product recommendation system at Amazon, as well as the food recommendation
system for Uber Eats, all of them are powered using a graph neural network that helps serve
the most relevant content to users on a daily basis. And on a slightly personal note, graph
neural networks have also been used to significantly improve travel time predictions
in Google Maps, which is used also by billions of people every day. So whenever you type a query,
how do I get from point A to point B in the most efficient way? The travel time prediction that
you get is powered by a graph neural network that we have developed that defined in collaboration
with the Google Maps team. And this is of high importance not only to users that use the app on
a daily basis to find the most efficient way to travel. It's also used by the various companies
that leverage the Maps API so they can tell their customers what's the time it will take for
a certain vehicle to arrive to them. So companies such as food delivery companies and ride sharing
companies have also extensively profited from the system, which in cities such as Sydney has
reduced the relative amount of negative user outcomes in terms of badly predicted travel times
by over 40%, making it one way in which graph representation learning techniques that I have
co-developed are actively impacting billions of people on a daily basis. When I was an
undergraduate student, I was interested in image processing and was excited about variational methods.
I think it's a very elegant idea that you can define a functional that serves as a model for
your ideal image and then use the optimality conditions to derive a differential equation that
flows towards the optimum. And a particularly cool approach was proposed by Ron Kimmel where you
could think of an image as a manifold or a high-dimensional surface and use an energy that
originated in string theory and particle physics to derive a non-Euclidean diffusion PDE called
Biltrami Flow that acts as a non-linear image filter. And this is what made me fall in love
with differential geometry and I did a PhD with Ron on this topic. And I think these were really
beautiful and deep ideas that unfortunately now are almost forgotten in the era of deep learning.
And it's a pity that the machine learning research community has such a short memory
because many modern concepts have really ancient truths. Ironically, we've recently used non-Euclidean
diffusion equations as a way to reinterpret graph neural networks as neural PDEs. And I think
it really helps sometimes to have a longer time window. Now, equivariated networks tend to generalize
much better and require much less data if the data indeed has the symmetry that you assumed
in your model. But people often ask why do we even care about data efficiency when we can just
collect more data? We live in the era of big data, right? And I think the answer why you might still
be interested in data efficiency. First of all, there are applications like say medical imaging
where acquiring labeled data simply is very cost. You have to get patients, you're dealing with
privacy restrictions, you're dealing with costly, highly trained doctors who have to annotate the
data, come together in a committee to decide on questionable cases and so forth. So this is very
expensive. And if you can prove you can improve the data efficiency by a factor of two or 10,
or whatever it may be, you might just take a problem that was in the realm of economically
infeasible and take it into the realm of the economically feasible, which is a very useful
thing. There are other cases like graphical nets where the group of symmetries is so large,
in this case, n factorial number of permutations, that no amount of data or data augmentation
in practice is going to allow you to learn the symmetry or to learn the invariance or
equivariate in your NAPO. So indeed, you see that in this space of graphical nets, everybody
uses equivariate permutation, equivariate network architectures. And then finally, you could think
about the grand problems of AGI, artificial general intelligence and so forth. And here I think that
we will most certainly need large datasets, large networks, a lot of compute power,
and so forth. And the current architectures we have clearly are performing far fewer
computations than the human brain. So there's a ways to go there. But I also think that one
essential characteristic of intelligence is the ability to learn quickly in new situations,
situations that are not similar to the ones you've seen in your training data.
And so data efficiency to me is an essential characteristic of intelligence. It's almost
like an action that you want your methods to be data efficient. And so I think one of the big
challenges for the field right now is to try to think of very generic priors, priors that apply
in a wide range of situations. And to give you, even though they're generic and abstract,
they give you a lot of bang for the buck in terms of improved generalization and data efficiency.
I think that the beauty of science and research is in connecting the dots. And I find it fascinating
that, for example, graph neural networks are connected to the work of Weisfuhrer and Lehmann
from the 60s on isomorphism testing, which in turn was inspired by problems in chemistry.
And chemistry was also the field that drove the research into modern formulation of
diffusion equations that were adopted in image processing community in the 90s
and came back recently as a way to reinterpret graph neural networks. And I think such connections
give really a new and deep perspective. And probably the deeper you dive, the broader they
become, but it's really an ever-ending story. Today is an incredibly special episode and we're
filming at nine o'clock in the morning. It's really rare for us to film this early when I'm
still caffeinated. Many of our guests are over in the States. It's an absolute honor to have you
both on MLST. And Professor Bronstein, could you start by briefly telling us how the young
mathematician, Enne Noffer, used symmetries to discover the conservation laws in physics?
Maybe I should take a step back and describe the situation that happened in the field of geometry
towards the end of the 19th century. And it was an incredibly fruitful period of time for
mathematicians working in this field with the discovery and development of different kinds
of geometries. So a young mathematician based in Germany called Felix Klein proposed this
quite remarkable and groundbreaking idea that you can define geometry by studying the groups of
symmetries, basically the kinds of transformations to which you can subject geometric forms and
seeing how different properties are preserved or not under these transformations. So these ideas
appear to be very powerful. And what Amy Neuter showed in her work, and she actually worked in the
same institution where Klein ended up in Göttingen in Germany. And she showed that you can take a
physical system that is described as functional as a variational system and associate different
conservation laws with different symmetries of this system. And it was a pretty remarkable
result because before that, conservation laws were purely empirical. You would make an experiment
many times and measure, for example, the energy before or after some physical process or chemical
reaction. And you would come to the conclusion that the energy is preserved. So this is how,
for example, I think La Voizier has discovered the conservation of energy. So it was probably for
the first time that you could derive these laws from first principles. So you would need to assume
in case of conservation of energy, the symmetry of time. We decided to take a tool into the world of
algorithmic reasoning. Peter, you said in your introduction that algorithmic reasoning seeks
to find neural networks that are good at imitating the classical algorithms that initially brought
you to computer science. So you recently released a paper on this called Neural Algorithmic Reasoning.
And it's often claimed that neural networks are turing complete. And, you know, we're told that we
can think of training neural networks as being a kind of program search. But you argued in your
paper that algorithms possess fundamentally different qualities to deep learning methods.
Francois Chollet actually often points out that deep learning algorithms would struggle
to represent a sorting algorithm without learning point by point, you know, which is to say without
any generalization power whatsoever. But you seem to be making the argument that the interpolative
function space of neural networks can model algorithms more closely to real world problems,
potentially finding more efficient and pragmatic solutions than those classically proposed by
computer scientists. So what's your take? Yeah, thanks for asking that, Tim. I think the concept
of classical algorithms, as opposed to deep neural networks, at least the way we're currently
applying them makes all these points about turing completeness a little bit moot, because there's
quite a few proofs out there saying that you can use neural networks or more recently graph neural
networks in particular to simulate a particular algorithm perfectly. But all of these proofs
are sort of a best case scenario. They're basically saying I can set the weights of my neural network
to these particular values and voila, I'm imitating the algorithm perfectly, right? So all these best
case scenarios are wonderful. But in practice, we don't use this kind of best case optimization,
we use stochastic gradient descent. And we're stuck with whatever stochastic gradient descent
gives us. So in practice, the fact that a neural network is capable for a particular setting of
weights to do something doesn't mean that it will actually do that when trained from data.
So essentially, this is the kind of the big divide that separates deep learning from traditional
algorithms. And it has a number of other issues as well, not just the fact that we cannot find the
best case solution. Also the fact that we are working in this high dimensional space, which is
not necessarily easily interpretable or composable, because you have no easy way of saying, for example,
in theoretical computer science, if you want to compose two algorithms, you're working with them
in a very abstract space, which means that, you know, you can easily reason about stitching the
output of one to the input of another. Whereas you cannot make that easy of a claim about latent
spaces of two neural networks, right? So all these kinds of properties, interpretability,
compositionality, and obviously also out of distribution generalization are plagued not
by the fact that neural networks don't have the capacity to do this. But the routines we use to
optimize them are not good enough to to cross that divide. So in neural algorithmic reasoning,
all that we're really trying to do is to bring these two sides closer together by making changes
either in the structure of the neural network or the training regime of the neural network or the
kinds of data that we let the neural network see, so that hopefully it's going to generalize better
and extrapolate better. And especially on the kinds of, you know, classical algorithmic problems
that we might see in a computer science textbook. And lastly, I think I'd just like to address the
point about sorting. We have a paper on algorithmic reasoning benchmarks that we are about to submit
that in Europe's data set track. I think it should be public even now on GitHub, because that's the
requirement for the conference, where we have quite a few algorithmic tasks, and we're trying to
force GNNs to learn them. And we do have several sorting tasks in there. And at least in distribution,
these graph neural networks are capable of imitating the steps of, say, insertion sort.
So I will say not all is lost if you're very careful about how you tune them. But obviously,
there is a lot of caveats. And I hope that later during this chat, we'll also get a chance to talk
a little bit about how even though we cannot perfectly mimic algorithms, we can still use
this concept of algorithmic execution today now to help expand the space of applicability of algorithm.
Yeah, this is absolutely fascinating. Because this gets to the core of what I think some people
point out is being the limitations of deep learning, right? Sholay spoke about this, but I don't think
that geometric deep learning would help a neural network learner sorting function because discrete
problems in general don't seem amenable to vector spaces, either because the representation would
be glitchy, or the problem is not interpolative in nature or not learnable with stochastic gradient
descent. So it would be fascinating if we could overcome these problems using continuous neural
networks as an algorithmic substrate. Do you think we could?
I think that it is possible. But it will require us potentially to broaden our lens on what we
mean by geometric deep learning. And this is something we're already very actively thinking
about. I think one of our co-authors, Takoko, and actually thought much more deeply about this
in recent times. But basically, the idea is we looked at geometric deep learning from a group
symmetry point of view, which is a very nice way to describe spatial regularities and spatial
symmetries. But it's not necessarily the best way to talk about, say, invariance of generic
computation, which you would find in algorithms, right? It's like, I have input that satisfies
certain preconditions. I want to say something about once I push it through this function,
it should satisfy certain post conditions. This is not the kind of thing we can very easily express
using the language of group theory. However, it is something that perhaps we could express more
nicely using the language of category theory, which is an area of math that I still don't know
enough about. I'm currently actively learning it. But basically, in the language of category
theory, groups are super simple categories that have just one node, right? You can do a lot more
complicated things if you use this more broader abstract language. And, you know, you talk about
basically anything of interest there in terms of these commutative diagrams. And Taco actually
recently had a really interesting paper called natural graph networks, where they basically
generalize the notion of permutation, equivariance that you might find in graph nets to this more
general concept of natural transformations. So now suddenly, you don't have to have a network that
does exactly the same transformation in every single part of the graph. What you actually need
is something a bit more fine grained. You just need for all like locally isomorphic parts of the
graph, you need to behave the same. But in principle, it gives you a bit more flexibility. And I think
that this kind of language, like moving a bit away from the group formalism would allow us to
talk about say algorithmic invariance and things like this. I don't yet have any theory to properly
prove this, but it's something that I'm actively working on. And I guess I would say, you know,
the only question is, would you still call this geometric deep learning? And in my opinion,
the very creators of category theory have said that category theory is a direct extension of
Felix Klein's Erlangan program. So since the founders of the field have already made this
connection, I would expect that, you know, it would be pretty applicable under a geometric lens.
So it seems to come back to it seems to come back from both of your sides to essentially graphs
and working on on sort of graphs to capture on the one side, the sort of symmetries that are
that you either assume in the problem or that you know or that you want to impose on the other side
on the other side. Now, these computations can may be well represented in in graphs.
Um, what's what's so special about graphs in your estimations? Is there something
fundamental to it? Or is it just another way? You know, we had Ben Gertzel or so here and I
asked him the same question, what's so special about graphs and his argument was essentially,
well, it's not about graphs, it's simply a good representation of the problem
that we can make efficient operations on. Do you have a different opinion on that is a graph
something fundamental that we should look more at than, for example, a tensor?
Graphs are abstract models for systems of relations or interactions. I should maybe specify
pairwise relations and interactions. And it happens that a lot of physical or biological
even social systems can be described at least at some level of abstraction as a graph. So that's
why it is so popular modeling tool in many fields. You can also obtain other structures such as grids
as particular cases. I wouldn't call it fundamental, but it is a very convenient and very common,
would say ubiquitous model. Now, what I personally find disturbing and we can talk about it
in more detail later on is that if you look at the different geometric structures for
Gamble that we consider in the book, whether it's Euclidean spaces or many phones,
they all have the discrete counterparts. So you have a plane and you can discretize it as a grid.
You have a manifold, you can discretize it as a mesh. A graph is inherently discrete. And this
is something that I find disturbing. There is, in fact, an entire field that is called network
geometry that tries to look at graphs as continuous objects. So, for example, certain types of graph
that look like social networks, what is called scale free graphs, can be represented as
nearest neighbor graphs in some a little bit exotic space with hyperbolic geometry.
So if we take this analogy, I think it is very powerful because now you can consider graphs as
a discretization of something continuous and then think, for example, of graph neural networks
as certain types of diffusion processes that are just discretized in a certain way.
And by making potentially possible different discretizations, you will get maybe better
performing architectures. One of the core dichotomies we talk about on Street Talk is the
apparent dichotomy between discrete and continuous. And as Janik was saying, there are folks out there
who want our knowledge substrate to be discrete, but still distributed, record them sub-symbolic
folks. And this network geometry is fascinating as well. Because you're saying in some sense,
you can think of there being some unknown continuous geometry. So you're saying there is
no dichotomy? This is probably a little bit of a wishful thinking as it happens with every model.
So I would probably phrase it carefully. For some kinds of graphs, you can make this
continuous model. For others, maybe not. Fascinating. Well, on to the subject of vector spaces
versus discrete, you know, geometric deep learning is all about making any domain
amenable to vector spaces, right? And indeed, artificial neural networks. But could these
geometric principles be applied to another form of machine learning, let's say discrete
program synthesis? Certainly a very important question, Tim. And yeah, thanks for asking that.
I think that there are many ways in which geometric deep learning is already at least
implicitly powering discrete approaches such as program synthesis, because there is a pretty
big movement on these so-called dual approaches where you stick a geometric
deep learning architecture within a discrete tool that searches for the best solution.
So for example, in combinatorial optimization, a very popular approach recently for
a Neurally Solving Mixed Integer programs is to have your typical off-the-shelf
MIP solver that selects variables to optimize one at a time. And, you know, with these kinds
of algorithms, they're in principle exponential time. But if you're lucky or knowledgeable enough
about how in what order you select these variables, you can actually solve the problem in linear time,
which is something we would like to strive towards. And the exact way in which we select
these variables is a bit of a black magic, like humans have come up with a few heuristics,
but I don't always work. And whenever you have this kind of setting, as long as you're assuming
that you're naturally occurring data isn't always throwing the worst possible cases or
adversarial cases at you, you can usually rely on some kind of modeling technique,
for example, a neural network to figure out which decisions the model should be taking.
So in this case, for example, for MIP solving, DeepMinds recently published a paper on this
where you can treat MIP problems as bipartite graphs where you have variables on one side
and the constraints on the other. And you link them together if a variable appears in a constraint.
Then they run a graph neural network, which as we just discussed is one of the flagship models
in geometric deep learning over this bipartite graph to decide which variable the model should
select next. And you can train this either as a separate kind of supervised technique to learn
some kind of heuristic, or you can learn it as part of a more broader reinforcement learning
framework, right, where the reward you get is how close you are to the solution or something like
this. So this is one kind of clear way in which you can kind of have this synergy between geometric
deep learning architectures and solutions for, for example, program synthesis.
But I would just like to offer another angle in which you can think of program synthesis
as nothing other than just one more way to do language modeling, right, because synthesizing
a program is not that different to synthesizing a sentence, maybe with a more stringent check
on syntax and so forth. But, you know, any technique that is applied to language modeling
could in principle be applied for program synthesis. And something that we will be
discussing, I believe, later during this conversation, one of the flagship models of
geometric deep learning is indeed the transformer, which we show in our book, and elucidate why it
can be seen as a very specific case of a graph neural network. And that's one of the flagship
models of language modeling. So basically, that's also one more way to to unify, like, you know,
just because the end output is discrete, doesn't mean that you cannot reason about it using
representations that are internally vector vector based.
Francois Chollet pointed out that there was this dichotomy. So you can embed discrete information
into a continuous representation. But the manifold needs to be smooth, it needs to be
learnable, it needs to be interpolative in nature. So I thought that was why we have these discrete
program searches. But then you have this exponential blow up. But maybe that search space, because
it's interpolative could be found using stochastic gradient descent, if you embed the discrete
information into some kind of vector space. But Professor Bronstein, I wanted to throw it back
over to you. I mean, why is it taken as a given that vector spaces are a good thing? Because
everything we're doing here is embedding discrete information into these Euclidean vector spaces.
Why are we doing that?
There are multiple reasons why vector spaces are so popular in representation learning.
Vectors are probably the most convenient representation for both humans and computers.
We can do for a lot of operations with them like addition or subtraction. We can represent them
as arrays in the memory of the computer. There are also continuous objects, so it is very easy to
use continuous optimization techniques in vector spaces. It is difficult for Gamble to
optimize a graph because it is discrete and requires combinatorial techniques. But
in a vector representation, I just have a bunch of points that I can continuously move in a
high-dimensional space using standard gradient-based techniques. Perhaps a more nuanced question is
what kind of structures can be represented in a vector space? A typical structure is some
notion of similarity or distance. We want that the vector representations preserve the distances
between, let's say, original data points. Here, we usually assume that the vector space is equipped
with the standard Euclidean metric or norm. We have a problem from the domain of metric geometry
of representing one metric space in another. Unfortunately, the general answer here is
negative. You cannot exactly embed an arbitrary metric in the Euclidean space, but there are,
of course, some results such as bogains theorem that, for Gamble, provides bounds on the metric
distortion in such cases. In graph learning spaces with other more exotic geometries such as
hyperbolic spaces, you have recently become popular with, for example, papers of Ben Chamberlain,
my colleague from Twitter or Max Nikol from Facebook. You can see that in certain types of
graphs, the number of neighbors grows exponentially with the radius. If you look, for example,
at the number of friends of friends of friends and so on in a social network where you have this
small world phenomenon, you can see that it becomes exponentially large with the growth of the radius.
Now, when you try to embed this graph in Euclidean space, it will become very crowded because
in Euclidean space, the volume of a metric ball grows polynomially with the radius. Think of the
two-dimensional case that we all know from school. The area of a circle is pi radius squared. The
volume of a ball is exponential with the dimension, so we inevitably need to increase the dimension
of the embedding to make space for these neighbors. In the hyperbolic space, the situation is very
different because the volume grows exponentially with the radius, so it is way more convenient
to use these places for graph embeddings. In fact, recent papers show that to achieve the
same error in embedding of a graph in the hyperbolic space with, let's say, 10 dimensions,
you would require something like a 100-dimensional Euclidean space. Of course, I should say that
metrics are just one example of a structure, so the general answer to the question whether
a vector space is a good model for representing data is, as usual, it depends.
You mentioned language, Peter, and maybe to both of you, do you think there is a
geometry to language itself? Obviously, we know about embedding spaces and close things,
somehow share meaning, and so on. Do you think it goes beyond that? Do you think there is an
inherent geometry to language itself and the meaning of what we want to transmit
and how that relates to each other? What you probably mentioned is the famous series of papers
from Facebook, where unsupervised language translation can be done by a geometric alignment
of the latent spaces. In my opinion, it's not something that describes geometry of the language,
it probably describes in a geometric way some semantics of the world. Even though we have,
linguistically speaking, very different languages like, let's say, English and Chinese,
yet they describe the same reality. They describe the same world where humans act,
so it is probably reasonable to assume that the concepts that they describe are similar,
and also while there are some theories and linguistics about certain universal
structures in languages that are shared, even though the specifics are different,
I think it's interesting to look maybe at non-human communications. I wouldn't probably
use the term language because it's a little bit loaded and probably some purists will be shocked
by me saying that, for example, whales have a language, but we are studying the communication
of squirrel whales, so this is a big international collaboration called Project, and I don't think
that you can really model the concepts that whales need to describe and to deal with
in the same way as we humans do, so maybe a silly example we can say in human languages,
and probably it applies to every language, we can express a concept that something got wet.
I don't think that a whale would even understand what it means by being wet because
the whale always lives in water. I would add to that maybe a slightly different view of geometry,
but it's all about the question of how far are you willing to go and still call it geometry.
Based on our proto book at least, I tend to think of graph structures also as a form of geometry,
even though it's a bit more abstract and within language, people might not always agree what
this structure is like, but I think we can be fairly certain that there are explicit links
between individual words as and when you use them in different forms, syntax trees or just one
word precedes another and so on and so forth, and while we may not be necessarily able to easily
say what is the geometric significance of one word, what we can look at is what is the local
geometry of the words that you tend to use around it. And I mean, this kind of principle has been
used all over the place. That has then been extended to graph structured observations,
generally with models like deep walk and note to back basically the same idea,
treat a nodes representation as everything that's around it basically. The reason why I think that
analyzing this local topology of how words are used with each other is very powerful. I've reinforced
that recently precisely because of the fact I've been delving into category theory, because in
category theory, your nodes are basically atoms, you're not allowed to look inside them, you assume
they're this undivisible unit of information. And everything you can conclude about the atoms
comes from the arrows between them. So using this very simple concept with a few additional
constraints like compositionality, you can, for example, tell me what are all the elements of a
set, even though you've abstracted that sets to a single point, just by analyzing the arrows
between all sets, you can tell me what are all the elements inside a set. So thinking about this,
I do believe that it is possible to reason about geometric, you know, word to vex, for example,
does this with the assumption that the structure of the whole we're approaching this at the right
level, though, because people have said for quite a long time that there's a difference between
syntax and semantics. And you could look at the geometrical structure of spoken language. Or,
for example, you could look at the topology of the connections in your brain, the topology of
reference frames in your brain is how you actually have learned concepts.
Would looking at the topology of spoken language tell you enough about abstract categories?
That's a good question. I think that if that kind of information is necessary, like if the
atoms by themselves won't tell you everything, one thing that we actually very commonly do in
graph representation learning is assume this sort of hierarchical approach where you have
the ground level with your actual individual notes. And then you come up with some kind of
additional hierarchy that tells you either something about intermediate topologies in a graph
or intermediate structures that you care about in this graph or any abstract concepts you might
have extracted. And then there's additional links being drawn between these to kind of reinforce
the knowledge that the graph net can capture. So I think if you have knowledge of some abstract
concepts that are relevant for your particular task, you can attach them as additional pieces
of information to this topology. Of course, the more exciting part is could we maybe discover
them automatically? But that is something that I don't think is potentially in scope for this
question. When human interpreters need to translate from one language to another,
they often need to deal with different sentence structures. I think Turkish is actually an extreme
example where the order of words is completely reversed. It implies that you need probably to
hold well in computer science terms some kind of a buffer in your brain before you can make the
translation to another language. So it definitely imposes certain biological network structure
in the brain. Another interesting observation that I read somewhere about the way that people
remember certain facts when they speak a certain language. And the particular example that was
given is a person can remember a perpetrator of a crime and then gives testimony in court.
And the reason is that in some languages, it is more common to use impersonal pronouns and
impersonal phrases. So you can say, for example, the object was broken. And in some languages,
you would say that somebody broke the object. So it appeared that languages were of these
more impersonal constructions. People speaking these languages, I have hard time to remember
the perpetrator. So the language probably imposes a lot about the way that we perceive world,
but it is probably not studied sufficiently. But there may be some fuzzy graph isomorphisms
though between the languages. I think there's something really magic about graphs. I think
that's what we get into because your lecture series inspired me actually, Professor Bronstein,
where you were talking about all the different applications of graphs. But something that a
lot of our guests talk about are knowledge graphs. Expert systems and the knowledge acquisition
bottleneck were the cause of the abject failure of good old fashioned AI or symbolic AI systems in
the 1980s. And many hybrid or neurosymbolic folks today are still arguing that we need to have a
discrete knowledge graph, either human designed or learned or evolved or emerged or some combination
of those things I just said, depending on who you talk to. Now, critically, many go fi people
think that most knowledge we have is acquired through reasoning, not learning, right, which is
really, really interesting. So by reasoning, I mean extrapolating new knowledge from existing
knowledge. It feels like graph neural networks could at least be part of the solution here.
And in your lecture series, you mentioned the work by Kramner, which was explainable GNNs,
where they use some kind of symbolic regression to get a symbolic model from a graph neural network.
So do you think there's some really cool work we can do here?
There is a little bit of divide in graph learning literature. So people working on
graph neural networks, and working on knowledge graphs, even though, at least in principle,
the methods are similar. For example, you typically do some form of embedding of the
nodes of the graph. Somehow these are distinct communities, probably, historically, they evolved
in different fields. Yeah, so the paper of Kramner, this is really interesting because
they use graphs to model physical systems, for example, and body problem when they have
particles that interact. You can describe these interactions as a graph, and you can use standard
generic message passing functions to model the interactions. Now, the step forward that they
do is they replace these generic message passing functions with symbolic equations.
And not only that, this allows to generalize better, but you also have an interpretable
system. You can recover from your data the laws of motion, right? And if you think of
how much time it took historically to people like Johannes Kepler, for example, he spent
his entire life on analyzing astronomical observations to derive a law that now bears his
name that describes the elliptic orbits of planets. Nowadays, with these methods, you can
probably do it in a matter of seconds or maybe minutes. I think the point that
particularly caught my attention in what you asked Tim was this interplay between graphs and
reasoning and extrapolation and how that supports knowledge. Now, when it comes to how critical
is this going to be, it depends on the environment in which you put your agent, like is it a closed
environment or is it an open ended environment where new information and new knowledge can come in
principle at any time? This basically do want to build a neural scientist or do you just want
to build a neural exploiter that takes all the information available right now and then draws
conclusions based on that? So if the system is closed world, you'll probably be able to get away
without very explicit reasoning, especially if you have tons of data, because we've seen time and
time again that large scale models can kind of pick up on these regularities if they've seen it
often enough. But if I give you a solution that involves stacking, for example, n objects, and
now I ask you to do the same kind of reasoning with two times n objects, the way in which we
optimize neural networks at least today is typically going to completely fall on its back
when you do something like this. So if you truly want to take whatever regularities you have come
across in the world of the training data and hope to at least reasonably gracefully apply them to
new kinds of rules that come in the future, then you probably want your model to extrapolate to
a certain extent. And for this, at least my ongoing algorithmic reasoning research algorithms are a
very natural area to study under this lens, because they trivially extrapolate, you write an
algorithm that does a particular thing on a set of n nodes, you can be you can usually
mathematically prove that it's going to do the same thing equally properly, maybe a bit more slowly
if you give it two times n nodes, right? This kind of guaranteed typically doesn't come that easily
with neural networks. And we found that you have to very carefully massage the way you train them,
the kinds of data you feed to them, the kinds of inductive biases you feed into them in order
to get them to do something like this. So if extrapolation is something you truly need,
then you know, I think for artificial general intelligence, we're going to want to have at
least some degree of extrapolation as new information will become available to our neural
scientists just as you follow the era of time. Basically, for doing something like this, graph
neural networks have arisen as a very attractive primitive, because there's been a few really
exciting theoretical results coming out in recent years, saying that the operations of a graph neural
network align really, really well with dynamic programming algorithms. And dynamic programming
is a very standard computational primitive, using which you can express most polynomial
time heuristics. So essentially, that's a really good, you know, that's a really good piece of
mind result. The unfortunate side of it is that it's a best case result, right? So you can set
the weights of a neural network of a graph neural network to mimic a dynamic programming algorithm
more efficiently or with a smaller sample complexity. But, you know, there's still a big
problem of how do I learn it in a way that it still works when I double the size of my input.
And that is in a way what algorithmic reasoning has been largely about. Like, we're trying to make
that happen. It's not easy. This if you throw the vanilla graph neural network and just input
output pairs of an algorithm, it will learn to fit them in distribution the moment you give
like ask it to sort an array that's twice as big, it's going to completely collapse. So
this is the number one thing that the neurosymbolic people say, they say neural networks, they don't
extrapolate, they only interpolate, you know, it just, it's a continuous geometric model,
learns point by point transforms the data onto some continuous smooth, learnable manifold,
you interpolate between the data points, you want to have a smooth, you want to have a dense
sampling of your data, but you're talking about dynamic programming problems. These are discrete
problems that the structure is discontinuous. Like, how could you possibly learn that within
your network? Well, the dynamic programming algorithm could be, could have a discontinuous
component to it. For example, if you're searching for shortest paths, at some point, you will take
an argmax over all of your neighbor's computed distances and use that to decide what the path is.
But before you come to the argmax part, there is usually some fairly smooth function being
computed actually. So in the case of shortest path computations, you know, Bellman Ford or
something like this, you say something very simple, like, I have a value d of s in every single one
of my nodes, which is initially infinity everywhere and zero in the source vertex. And then at every
point, I say the distance of my particular node is the minimum of all the distances of my neighbors
plus the edge weight, right? And this kind of function is generally more graceful than taking
an argmax. And you can also think of, for example, if you have to compute expected values or something
like this, using dynamic programming, that's also one example where actually summing is what you
need to do across all of your neighbors or something like this. So yeah, it is true that like,
across individual steps, you may be doing like discrete optimization steps.
But usually it's propelled by some kind of continuous computation under the hood. So that's
the part that the graph neural network actually simulates. And then the part which does the argmax
would be some kind of classifier that you stitch on top of that. So in principle, it's not, yeah,
it's not too challenging to massage it into a neural network framework.
So one of the, I think one of you mentioned this before brought up transformers. And you know,
in recent years, we've had, I think about 10 different papers saying transformers are something
there is transformers are RNNs, transformers are Hopfield networks, and also transformers are
graph neural networks or compute some kind of graph neural networks. Can you maybe speak a bit
to that or transformers specifically graph neural networks? Or are they just so general
that you can also formulate a graph problem in terms of a transformer?
That's a very good question. I would start off by saying, like, I don't want to start this
discussion just by saying, yes, transformers are graph neural networks. This is why end of story,
because I feel like, you know, that doesn't touch upon the whole picture. So let's let's look at
this from a natural language processing angle, which is how most people have come to know about
transformers. So imagine that you have a task which is specified on a sentence. And you want to
exploit the fact that words in the sentence interact, right? It's not just a bag of words,
there's some interesting structure inside this bunch of words that you might want to exploit.
When we were using recurrent neural networks, we assumed that the structure between the words
was a line graph. So basically, every word is preceding another word and so on and so forth.
And you kind of just linearly process them with a model like LSTM or something. But, you know,
basically line graphs, as we know, are not the way language is actually structured. There can be
super long range interactions inside language. So subjects and objects in the same sentence could
appear miles away from each other. So using the line graph is not the most optimal way of getting
that information in the fastest possible in the fastest possible way. So, okay, there's clearly
some kind of non trivial graph structure. What is it? Well, it turns out that people cannot really
agree what this optimal graph structure is. And it may well be task dependent, actually. So just
consider syntax trees, for example, like there's not always a unique way of decomposing a sentence
into a syntax tree. And the exact kind of tree you might wish to use to represent a sentence
may be different depending on what is the actual thing that you're solving. So okay, we have a
situation where we know that there's some connectivity between the words, but we don't know
what that connectivity is. So in graph representation learning, what we typically do when we don't know
the graph, as long as the number of objects is not huge, is to assume a complete graph and
let the graph neural network figure out by itself what the important connections are.
And if I now stitch an attentional message passing mechanism onto this graph neural network,
I have effectively re derived the transformer model equation without ever like using this
specific transformer lingo. So from this kind of angle, the fact that it's a model that operates
over a complete graph of individual words, in a way that, you know, once you've put all the
embeddings to them is permutation equivalent. This describes the central equations of self
attention that the transformer uses. The part which I think causes a bit of a divide here
is the fact that transformers like the model that was originally presented are not just the
equations of a transformer, they're also the positional embeddings of a transformer. And that's
the part that sort of gives it a bit more of a central structure. Well, actually, if you look at
these sine and cosine waves that get attached to the individual words in an input to a transformer,
you will see that you can you can actually derive a pretty good connection between them
and the discrete Fourier transform, which actually turned out to be the eigenvectors
of a graph Laplacian for a line graph. So essentially, these positional embeddings
are hinting to the model that you are the decent that these words in a sentence are arranged in a
particular way, and you can use that information. But because it's fed in as features, the model
doesn't have to use any of that information, like sometimes bag of words is the right thing to do,
for example, right? So essentially, the transformer has a bit of a light hint that there's a
sentential structure in there in the form of a line graph. But, you know, the model itself
is a permutation equivalent model over a complete graph. And from our lens of the geometric deep
learning, it is effectively a special case of an attentional GNN. Now, I think this positional
embedding aspect is a super important one. And it could hint to how we might extend these
transformers from sentences to more general structures. And I think, Michael, you might have
a lot more thoughts on that than I do. So maybe you can say a bit about that.
Yeah, so positional encoding has been done for graphs as well, as Peter mentioned,
in case of a graph, you can straightforwardly generalize this sine or cosine positional coordinates
that are used in transformers using the eigenvectors of the Laplacian. There are other
techniques you can actually show that you can make it a message passing type neural network
strictly more powerful than traditional message passing. The equivalent Weisferre-Leban-Grafers
algorithm test by using a special kind of structure where positional encoding, for example,
if you can count substructures of the graph, such as cycles or rings and so on. And this way,
you have a message passing algorithm that is specialized to the particular position in the graph
and can, for example, detect structures that the traditional message passing cannot detect.
So it is at least not less powerful than the Weisferre-Leban algorithm. And we can actually
show examples on which Weisferre-Leban algorithm or traditional message passing fails, whereas
this kind of approach succeeds. So the thing I've always wondered about transformers networks
are the position tokens. I really don't like them and I want them to go away
because it feels very impure, doesn't it? I think what we really want to learn is some kind of higher
order structure in the language. And it kind of felt like we were using the position tokens to
cheat a little bit. So what I'm trying to get across here is that I think the position of a token
in an utterance should be invariant. I mean, clearly, in different languages,
the tokens are in different places. In Turkish, the order is completely reversed. And I would
like to think that our internal language representation ignores the transmission
arrangement given the particular language and the constraint that we only communicate sequential
streams of words. However, I do appreciate what Michael is saying about that the position
encodings can actually encode more powerful structures like cycles and rings. The key question
is, do we actually need to have these structures in natural language? I don't agree that you want
to get rid of them. So positional encoding, it's a kind of combination of two worlds. So if you
consider a graph, then you're completely agnostic to the ordering of the nodes. This is one of the
really key characteristics of graphs and sets more generally that you don't have the ordering of
the nodes. The situations and the problems where transformers are applied, you actually do have
an order. But you use the graph as Petra described to model different long distance relations
between different tokens or words in a sentence. So you want to incorporate this prior knowledge
that these nodes are not in arbitrary order, that they have some sentence order. And this principle
applied more generally, you can use positional encoding to tell message passing not to apply
exactly the same function everywhere on the graph, but to make it specialized for different
portions of the graphs or at least make it a possibility. And then the training will
decide whether to use this information or not or in which way.
It's strange because I don't know whether the order is just a function of the communication
medium. So we transmit the tokens in a sequence. And could we then represent them in our brains
in a completely different domain where the sequence is no longer relevant? Well, actually,
a lot of neuroscientists think that our brain is a prediction machine and it's a sequence
prediction machine. So the sequence is kind of fundamentally important.
Yeah, it's also a function of the specific language that you use. And as we discussed before,
there are languages which convey the same meaning with a different sentence structure.
I want to get a little bit into what you said about essentially what we're doing with these
positional encodings is we hint. We hint to the model that there is something here, which is a big
break from sort of the old approach, let's say of an LSTM to say this is the structure, right?
So with the world of geometric deep learning, I often have the feeling people talk about,
they talk about symmetries and we need to exploit these symmetries that are present
in the world. And there's almost to me two different groups of these symmetries. So one group is
maybe you would call them like exact symmetries or something like this. When I think about alpha
fold, and I think about like a protein, it doesn't, I don't care which side is up, right?
Like the protein is the same, the same protein, and there's no reason to prefer any
direction over any other direction. However, if I think of like, because people have made this
argument for CNNs, for example, they say, well, a CNN is a good architecture because it's
translation invariant, right? And essentially, if we want to do object recognition or image
classification, translation invariance is like a given, but it's not, right? It's not a given.
The pictures that we feed to these algorithms, most often the object is in the middle. Most often,
you know, it's kind of upright, like the sky is on top, and the floor, yes, I can hold my camera
like this, but I don't, right? So it, it seems to be, it seems to be, in many cases, better to not
put the symmetries in there until we're like really, really, really, really sure that these
are actual symmetries, because with more data, it seems the model that does not have the prior
inside becomes better than the model that does have the prior inside. If that prior doesn't
exactly match the world is, do you think that's a fair characterization of, for example, why
transformers with large data, all of a sudden beat classic CNN models or come close to them?
I think it's, it's always this question of the tradeoff between how much your, your model and
how much you learn. And I remember when I was a student, there was this maxim that machine
learning is always the second best solution. And maybe nowadays with deep learning, showing
some remarkable set of successes, I'm probably less confident in this statement, but it's probably
still quite true that the more you know about your problem, the better chances that machine
learning will, will work for it. To me, it makes sense to model as much as possible and
learn what is hard or impossible to model. And in practice, of course, there is a spectrum of
possibilities of how much of these prior assumptions are hardwired into the architecture.
And usually it's the tradeoff between the computational complexity availability of the data,
also hardware friendliness. And if you think of what happened in computer vision,
it's probably a good illustration that convolutional networks have translational
invariance, for example, as you mentioned, but in many problems, you might benefit from other
symmetries such as rotations. Again, depends on the application, but imagine that you want
to recognize, I don't know, traffic signs when you can also tilt your car. And you may ask why
in these applications as well, CNNs are still so popular. And probably one of the reasons is that
they map very well to the single instruction, multiple data type of hardware architectures
that GPUs offer. And you can compensate for explicitly not accounting for rotational symmetry
with data augmentation and more complex architectures and larger training sets. And this is exactly what
happened a decade ago, this convergence of three trends, the availability of compute power, right,
the GPUs, algorithms that map well to these computational architectures. And these happen
to be convolutional networks, and also very large data sets that you can train these architectures
on, such as ImageNet. So many of the choices that become popular in the literature are maybe not
necessarily theoretically the best ones. So I think in hardware design, there is this phenomenon
that is called the hardware lottery, when it's not necessarily the best algorithm and the best
hardware that solves the problem, it's just some lucky coincidence. And they are happy marriage
that makes them successful. We keep raising this point about how transformers can be seen
as special case of attentional GNNs. But, you know, the status quo is that people will use
transformers for very many tasks nowadays. And, you know, there may well be a good argument for
considering them in this completely separate light. And one possible, you know, explanation or
justification for this is the hardware lottery. Because, yes, sure, transformers perform permutation
equivalent operations over a complete graph. But they do so in a way that is very, very highly
amenable to the kind of matrix multiplication routines that we can support very efficiently
on GPUs nowadays. So basically, they can be seen as the graph neural network that has won the current
hardware lottery, even in cases where maybe it will make more sense to consider a more constrained
graph, especially if you have low data environments or something like this. The potential, you know,
overheads of running a full graph neural network solution with message passing, which,
given its extremely sparse nature, doesn't align that well with GPUs and TPUs nowadays.
Sometimes just using a complete graph neural network is the more economical option when
you take all factors into account. And that's, and also the fact that they use an attention
mechanism, which is kind of a middle ground between a simple diffusion process on a graph,
which we just kind of average things together based on the topology. And the full on message
passing where you actually compute a full on vector message to be sent across the edges,
like it strikes a nice balance of scalability and still being able to represent a lot of functions
of interest, especially when your inputs are just word tokens, right? So like, you know,
in a way, it's a GNN that strikes a very nice sweet spot. And that's probably the reason why
it's become so popular in current times. Now, of course, there is a chance that hardware,
and there's actually a pretty high probability that hardware will catch up to the trends in
graph representation learning, and we will start to see a bit more graph oriented hardware. But
at least for the time being, yeah, there's a bit of a combination of what's theoretically making
the most sense for your problem, and what the hardware that you have right now will support
the most easily. Yeah, there is a British startup, I think they have reached recently a unicorn
status called Graphcore. And you can already hear from the name of the company that there is a
graph inside, they try to develop hardware that goes beyond the traditional paradigms. Yeah,
I'm really interested in the hardware lottery. We had Sarah Hooker on massive shout out to Sarah,
and Yannick made a video on the hardware lottery paper as well. I mean, I'd push back a little
bit. I think there's also a bit of an optimization and an algorithmic lottery going on. I think
there's something very, very interesting about stochastic gradient descent and the kind of data
that we're working with. But this actually gets to my next question, which is about why exactly
geometry is a good prior and how principled it is. So it seems like these geometric priors are
principled, and they have utility because they are low level primitives, right? They're ubiquitous and
natural data. But why exactly is it a principled approach to start with things we know, which is
to say geometric primitives, and to work upwards from there? You know, what would it look like if
we went top down instead? And what makes a good prior? I mean, one way to think about it is the
actual function space that you're searching through, you know, this hypothesis space, it's not just
about being able to find the function easily in that space, or the simplicity of the function you
find. Chalet would say it's the information conversion ratio of that function. So, you know,
can you use this function that you found to convert a very small piece of information and
experience space into new knowledge or a new ability? But how do you find these functions?
One of the points that we try to make in the book is the separation between the domain and
the group that you assume on the domain, the symmetry group. So you might have the same domain,
like two dimensional grid or two dimensional plane. And for example, the translation group,
or the group of rotations and translations, or the group of rigid motions that also include
reflections. So these are completely separate notions. And which one to choose depends on the
problem. The choice of the domain really comes from the structure of your data. So if your data
comes as an image, then of course, you use a grid to represent the choice of the domain.
Now, which symmetry group to use is a more subtle point. And it really depends on what you're trying
to achieve. You can think of, for example, traffic sign recognition. When a car drives on the road,
usually the signs will have certain orientation. It's very unlikely that you will see it upside
down. So the really the only invariance or the only kind of symmetry you have is translation.
So CNN's in this case would work perfectly well. So for example, we have histopathological
samples. So we have a slice of tissue that you need to put under the microscope. So you can
naturally flip the glass. You don't know how it is oriented. So reflections are also initial
transformation. So in the traffic signs, of course, this is not physical unless you see your sign
in the back mirror. But in this histopathology example, it is an initial transformation. So
the choice of the symmetry group and what makes a good geometric prior is really
dictated by the specific problem. It's it's often very hard, though, to actually choose because
we often don't really know, right, coming back to what I said before, if we if we actually hit
the group correctly. And in fact, we've we've sort of seen the the more successful approach. And
this might be hardware specific. But but it seems the more successful approach is often
to actually make data augmentations with respect to what we assume are symmetries. So to know,
I think of all the the color distortions that we do to images, we rotate them a bit, we rescale
them and so on. It will be definitely possible to build architectures that are just invariant to
those things. However, it seems to be a more successful approach in practice to put this all
into the into the data augmentations. How do you what's your take on on that? How do we choose
between putting prior knowledge into augmentations versus putting prior knowledge into the architecture?
It's not a binary choice. It's not either your model or your augment with data. One of the key
principles that we also emphasize in the book is that, of course, this perfect environs or
equivirons is a wishful thinking. In many cases, you you want to get the property that
we call geometric stability, you have some transformation that is approximately a group or
imagine that you have a video where two objects are moving, let's say one car moves left and
another car moves right. So there is no global translation that describes the relation between
the two frames in this video. The geometric stability principle tells you that
if you are close enough to an element of the group, if you can describe this
transformation as an approximate translation, then you will be approximately invariant or
approximately equivalent. And this is actually what happens in CNN. So this was shown by Joan
and they use this motivation to explain why convolutional neural networks are so powerful.
So roughly speaking, if I don't have a translation, but for example, if I have a
MNIST digit, and you have different styles of the digits, you can think of them as warpings of
some canonical digit. So in this case, even though it's not described as a translation,
and the neural network will not be invariant or equivirant to this kind of transformation,
it will be stable under these transformations. And that's why data augmentation works in some
sense that you're extending your invariance or equivirance class to approximate invariance and
equivirance. Tako also had a few interesting things to say about data augmentations versus
building the inductive priors into the model. This is Tako. Is your preference towards,
I mean, I assume it is towards creating inductive priors in the architecture around geometry instead
of data augmentation? Oh, that's a good question. I think it depends on the problem.
In some cases, you don't have a choice. So graphs are a great example. The group of permutations is
n factorial elements. If n is large, you have a thousand nodes, you're never ever going to be
able to exhaustively sample that group. And so it's better to just build it in. And that's also
why no graph neural net doesn't respect the symmetry. Nobody's suggesting you should do that
by data augmentation. In some other cases, it is somewhat possible to sample a reasonably dense
grid of transformations in your group. And indeed, augmentation is turning out to be very important
in unsupervised learning and self-supervised learning techniques. So I am actually,
I look very positively towards that. I don't think it's wrong to put in this knowledge using
data augmentation. But in some cases, like let's say when you're on classifying medical data,
like cells in a dish, a histopathology image or something, you just know for sure there's
translation and rotation symmetries, the cells don't have a natural orientation. And in those
cases, I do think it makes sense to build it into the architecture for for the simple reason that
if you build it into the architecture, you're guaranteed that the network will be equivariant,
not just at the training data, but also at the test data. So you never have that the network would
correct classification for your test data when you have it in one orientation, but when you
rotate it, it suddenly does something different, which can happen if even when you present your
training images in all possible orientations. So I think for that reason, equivariance does
tend to work better in those cases where there's an exact symmetry in the data. We have actually
demonstrated that empirically, where for example, in a in a medical imaging problem of
detecting lung nodules in three dimensional CT scans, we started off with a convolutional network
with a data augmentation pipeline, which completely tuned was state of the art method
at the time. And we simply replaced all the convolutions by group convolutions that respect
the rotational symmetries as well as the translations. And we get a very significant
improvement in performance. So that goes to show that in practice, often data augmentation
can get you pulled away. So for the cases where there's an exact symmetry, I thought or where
the group of symmetries is very large, I think building into the network is the way to go for
the foreseeable future. But there are many cases where where augmentation is also
well, where augmentation is the way to go. Fascinating. I'm really interested in this
notion that could these symmetries actually be harmful? I mean, I know Joanne, for example,
spoke about the three sources of error of machine learning models. And one of them is the
approximation error. And normally, when we get signals, they come to us in a contrived form,
don't know that they might be projected onto a planar manifold. And
the sky is always up, for example. Does it really help us having these geometrics
symmetries as primitives in the model? Yeah, that's a good question. The simple answer is,
if your problem doesn't have the exact symmetry, then at least in the limit of having infinite data,
building in a covariance is going to be harmful. And it's better to learn the true structure of
the data, this approximate symmetry, which you should be able to pick up from the data alone.
So that's one thing that that still means in a low data regime, it can be very useful,
even when the symmetry is approximate. But I would also say that sometimes,
in machine learning, we have a tendency to put too much faith into the
evaluation metrics and data sets. So we say we want to solve computer vision. And what we mean
is we want to get a high score on ImageNet. And certainly, it's true in ImageNet, the images
tend to appear in upright position, and they are photographed by humans. So the key objects are
kind of in the center most of the time, etc. So these are biases that you could exploit,
and you might stop yourself from exploiting them if you build in the symmetry. But that's
only a problem if you sort of put on your blinders and you say ImageNet accuracy is the only thing
that counts. You might very well think, if you want to build a very general vision engine,
it is useful that it still works if suddenly the robot falls over and has to look at the
world upside down. So the symmetry can still be there in principle, even if it's not there
in practice in your data set. And then there's maybe a robustness versus computational efficiency
tradeoff. So yes, maybe you're willing to acknowledge if your robot falls over, you still
want it to work. So you want that rotation equivariance. But then again, if we don't have
to process the images upside down in the 99% of cases where the images are upright,
we gain some computational efficiency. So there's a tradeoff there. And yeah,
I don't think there's a right or wrong answer. It's something you have to look at on a case
by case basis. When I put this to Professor Bronstein as well, he also said that you folks
were looking at, trying to remember how he described it, I think he said there was a kind
of representational stability, which allowed for approximate symmetries. So it's not necessarily
that you're going for these precise symmetries, you're actually looking for a little margin of
robustness moving into approximate symmetries that you might not have explicitly captured.
I agree. I think that's also a very important philosophy and approach. To take the group,
think of it as somehow embedded in a larger group, like most of the geometrical symmetries
that we think about are somehow a subgroup of diffeomorphisms. And then if you say, I don't
want invariance or equivariance, but some kind of stability or smoothness to elements in the
group plus small diffeomorphisms, for instance, you might get some of the generalization benefit
without unduly limiting the capacity of your model. Is there a hope, though, that we can get
this approximate? Because I see your point, I think, is that, or one of the points is, I think,
is that if we program like a symmetry into the architecture, it will be rather fixed, right?
We make an architecture translation invariant, it's going to be fully translation invariant.
Are there good ways to bring approximate invariances into the space of the
architectures that we work with? Yeah, I mean, I have a very quick answer,
maybe not too satisfying, but one very simple one. If you think of neural network blocks as
like components that implement different symmetries, and then you think of like a
calculus of these blocks as, you know, building your deep learning architecture,
one very simple representational tool that we can use to allow the model to use the symmetry,
but also not to use the symmetry is the skip connection. So essentially, you could have a
model that processes, for example, your graph data using a particular connectivity structure that
you want to be invariant to. And you can also use, say, a transformer that processes things in a
completely permutation invariant way over the complete graph. And you can just shortcut the
results of one model over the other model, if you want to give also the model the choice to ignore
the previous one. So maybe not a very, you know, detailed and satisfying answer, but that's one
simple way in which we could do something like this. And in some of our more recent algorithmic
reasoning blueprint papers, we do exactly this, because one very important thing that we're
trying to solve is apply classical algorithms to problems that would need them. But the data is
super rich, and it's really hard to, you know, massage it into the abstractified form that the
algorithm needs. For example, you want to find shortest paths in a road network, a real world
road network, you cannot just take all the complexity of changing weather conditions,
changing diffusion patterns on the, on the roads and the roadblocks and all these kinds of things,
and turn that into this abstractified graph with exactly one scalar per each edge. So you can apply
dykstra or something like this, like, it's just not feasible without losing a ton of information.
So what we're doing here is we make this high dimensional neural network component that simulates
the effects of dykstra. But we're also mindful of the fact that to compute, say, the expected
travel time, there's more factors at play than just the output of a shortest path algorithm,
right? There could well also be some flow related elements, maybe just some elements
related to the current time of day, human psychology, whatnot, right? So we start off by
assuming the algorithm does not give the complete picture in this high dimensional noisy world.
So we always as default, as part of our architecture, incorporate a skip connection
from just, you know, a raw neural network encoder over the algorithm.
So in case there's any model free information that you want to extract without looking at
what the algorithm tells you, you can do that. So maybe I don't know, Yannick,
if that answers your question about approximate symmetries, but that's, that's the kind of divide
by God's when I heard the question. I mean, that's a very, that's a very practical answer for sure
that that, you know, you can actually get out there. It even opens the possibility to, you know,
having maybe multiple kinds of skip connections and whatnot, you know, having dividing up your
symmetries into individual blocks that are run in parallel, maybe. But that brings me to maybe
another thing we've talked about, you know, there are symmetries, you want to incorporate them into
your problem and so on. And we've also talked about the symbolic regression beforehand to maybe
parse out symmetries of the underlying problem. What are the current best approaches if we don't
know the symmetries? So we have a bunch of data we suspect there must be some kind of symmetries
at play, because they're usually are in the world, right? And they, if we knew them, we could
describe our problems in very compact forms and solve them very efficiently. But we don't often
know. So what are the current state of the art? When I don't know the symmetries, how do I discover
what group structure is at play in a particular problem? I don't think that there is a single
approach that solves this problem in a satisfactory manner. And one of the reasons why because the
problem is ambiguous. So maybe an example, think of objects mostly translate horizontally,
but you also have a little bit of vertical translation. What is the right symmetry structure
to model? Is it a one dimensional translation group or a two dimensional translation group?
Do we want to absorb the vertical, the slight vertical motions as the noise and deal with it
data augmentation, or you want to describe it in the structure of the group that you discover?
So there is no single answer. You cannot say that one is correct and another one is wrong.
Yeah, I think this was kind of where I was going with the question of how
principled are the symmetries? And the symmetries seem to be hierarchical just in the same way that
geometries are hierarchical. You were saying that, for example, the projective geometry is kind of
subsumes Euclidean geometry, but I had a little thought experiment. So imagine I gave you a large
data set produced by a recursive fractal pattern. Now nature is full of fractals, trees, rivers,
coastlines, mountains, clouds, seashells, and even hurricanes. So let's say I didn't tell you the
simple rule which produced this pattern. Now what kind of regularities would you look for in the
model that you built? I mean, it seems obvious that there would be an expanding scale symmetry
which might resemble the original rule. But it feels like there'd be plenty of other emergent
abstract symmetries which are not obviously related to the simple rule which produced the pattern.
I mean, Janik was just saying when you look at computer vision, you see a kind of regularity
or invariance to color shifts, for example. So are fractals a good analogy for physical reality?
And should we be looking for the low level primitive regularities, which I think you're
advocating for? Or should we be looking at more abstract emergent symmetries which appear?
In the 90s, there was a famous paper by Michael Barclay on fractal coding. And they claim really
unbelievable compression ratios for natural images. And the way it worked was to try to
reassemble the image from parts of itself. And possibly, of course, you can take parts and
subject them to some geometric transformation. So roughly, if you have a page of pixels,
you can approximate it as another page taken from somewhere else in the image that you translate
rotate and scale. And then the image was represented as an operator that makes such a
decomposition. And this operator was constructed in a special way to be
constructive. And then they used the Banach fixed point theorem that you can apply this
operator to any image. So you can start with the noise, for example, completely random image.
And you have the target image emerge after a few iterations. So that this iterative scheme will
converge to the fixed point of the operator, which is the image itself. And it was actually
used in the industry, well, Microsoft and Carti encyclopedia. I don't know how many viewers
are old enough to remember it. But the main issue was the difficulty to build such operations.
The compression was very asymmetric. It was very easy to decode. You just take any image and
apply this operator multiple times. But it was really very hard to encode it. In fact,
some of these constructions that showed remarkable compression ratios were constructed semi by hand.
I should say that in more recent times in computer vision, for example, the group of Michael
Irani from the Weizmann Institute in Israel used similar ideas for super resolution and image
denoising where you can build the clean or higher resolution image from bits and pieces of the image
itself. So it's a single image denoising or super resolution. And what you do is you try to
use similarities across different positions and scales.
That's absolutely fascinating. I mean, I spend a lot of time thinking about this because my
intuition is that deep learning works quite well because of the strict structural limitations of
the data which is produced by our physical world. And would you say that physical reality is highly
dimensional or not? If it's highly dimensional, is it because it emerged from a simple set of
rules or relations like we were just talking about? Because I think what you're arguing for is that it
could be collapsible in some sense. Probably the term dimension is a bit frivolously used here.
But I would say that it's probably fair to say that at some scale, many physical systems can be
described with a small number of degrees of freedom, primatures that capture the system.
And as we are talking, I'm sitting in a room, I'm surrounded by probably a quadrillion of
gas molecules in the air that fly through the room and collide with each other and the walls of
the room. So at the microscopic level, the dimension is very high. So it's absolutely
intractable if I were to model each molecule and how it collides, I will have a huge number of degrees
of freedom. And yet if we zoom out, we can model the system statistically, and that's exactly the
main idea of thermodynamics and statistical mechanics. And this macroscopic system is surprisingly
simple. It can be described by just a few primatures such as temperature. And the example of fractals
that you brought up before essentially show that you can create very complex patterns with very
simple rules that are applied locally in a repeated way. This might be a question for you, Peter.
The geometric blueprint works brilliantly in the ideal world where we can compute all of the
possible group actions. But graph neural networks, for example, you know, the permutation group is
factorial in size, which means we need to rely on heuristics like graph convolutions. So how much
better would graph neural networks be if we could compute all of the permutations? I mean,
are you happy with these heuristics in general? So that is a very good question. And yes, so let's
just start from stating the obvious. If you want to explicitly express every possible operation
that properly commutes with the graph structure and in that sense is a graph convolution,
you would not be able to represent that properly as a neural network operation because you have
to store in principle a vector for every single element of the permutation group. So unless your
graph is super tiny, that is just not going to work. So on one hand, this is a potentially
annoying result. On another hand, it is also exciting because we know that even though we
ended up like doing most of our graph neural network research in this very restricted regime of
I'm going to define a permutation invariant function over my immediate neighbors,
and that will as a result translate into a permutation equivalent function over the whole
graph. Even though most of our research has happened in that particular area, we know from
this result that there actually exists a huge wealth of very interesting architectures beyond
that. And I think one of the potentially like earliest examples that have demonstrated that
there exists this wealth of space is actually one of Jean Bruno's earlier papers on the graph
Fourier transform that, you know, analyzing from a pure signal processing angle, they have shown that
you can represent basically every proper graph convolution as just, you know, parameterizing
its eigenvalues with respect to the eigenvectors of the graph Laplacian. So, but the big issue that
kind of limits us from going further in this direction right now is the issue that Michael
highlighted of geometric stability. So basically, a lot of these additional graph neural networks that
do something more interesting than one hop spatial message passing pay the price in being very
geometrically unstable. So the graph Fourier transform in its most generic form will have
every single node in the graph be updated based on whatever is located in any other
node in the graph, very conditional on the graph topology. So if you imagine any kind of
approximate symmetry, any kind of perturbation either in the node features or the edge structure
of the graph, this, you basically don't have any protection against that, like that error is going
to immediately propagate everywhere. And as a result, you'll end up with a layer that theoretically
works really well, but in practice is very unstable to these kinds of numerical or inaccuracy issues.
One thing that's also very important to note is that often in graph neural networks, we have this
subtle assumption of we have the graph and we're using this graph that's given to us. But who guarantees
that the graph that's given to you is the correct one actually very often, we estimate these graphs
based on very, very weird heuristics ourselves. So basically, all of these kinds of perfection
assumptions are what might limit the applicability of these kinds of layers. But that being said,
I find it comforting that these layers exist, which means that there are meaningful ways to push
our research forward, to potentially discover new, you know, wonderful basins of geometric
stability inside these different, you know, layers that may not just do one hop message
passing. So that's my take on this, like it gives me, it gives me faith that there's more
interesting things to be discovered. That being said, it is pretty tricky to find stable layers in
that vast landscape. Yeah, Michael, do you have some thoughts on this as well? I know you've worked
quite a bit on these geometric stability aspects. I just wanted to add one thought about it that
essentially, locality is a feature, not a bug in many situations. And in convolutional neural
networks, actually, if you look again historically, the early architectures like AlexNet, they started
with very large filters and the few layers or relatively few layers, I think something like
five or six. And nowadays, what you see is very small filters and hundreds of layers.
One of the reasons why you can do it is because of compositionality property. So you can create
complex features from simple primitives. So in some other cases, like manifolds, there are deeper
geometric considerations why you must be local related to what is called the injectivity radius
of the manifold. On graphs, well, maybe we like a little bit the theoretical necessity to be local
besides, of course, the computational complexity. But in many cases, it is actually a good property
because many problems do not really depend on distant interactions. So if you think of social
networks, probably most of the information comes from your immediate neighbors.
Is there some sort of, let's assume I have a graph, and I have my symmetries,
my groups that I suspect there are in the problem, or I want to be invariant to,
is there like, can you give us a bit of a practical blueprint of how would I build a
network that takes this as an input and applies this? How would you go about this,
what would be the building blocks that you choose, the orders and so on? Is there
overarching principles in how to do that? I don't think that there is really a general
recipe. So it's problem dependent. But maybe one example is applications in chemistry. The basic
structure that they have in graph is a permutation invariance. This has to do with the structure
of the graph itself. It says nothing about the structure of the features. You might also have
secondary symmetry structure in the feature space. In case of molecules, for example,
you might have a combination of features. Some of them are geometric. So it's actually not
an abstract topological graph. It's a geometric graph. A molecule is a graph that lives in three
dimensional space. And so the features are the positional coordinates of the nodes,
as well as some chemical properties, such as atomic numbers. Now, when you deal with the molecule,
you usually don't care about how it is positioned in space. It wants to be
equivariant to rigid transformations. And therefore, you need to treat accordingly
the geometric coordinates of the nodes of this graph. And this is actually what has
been successfully done. So when you do, for example, virtual drag screening,
architectures that do message passing, but in a way that is equivariant to these rigid
transformations actually are more successful than generic graph neural networks. Also,
this principle was exploited in the recent version of Alpha Fold, where I think they call it
point invariant attention, which is essentially a form of a latent graph neural network,
or a transformer architecture with equivariant message passing.
Yeah, I'd like, I'd just like to add one more point to this, to this conversation,
which is maybe a bit more philosophical. But it relates to, you know, this aspect of building
the overarching symmetry discovering procedures, which I think would be a really fantastic thing
to have in general. And, you know, I hope that some component of a true AGI is going to be
figuring out like making sense of the data you're receiving and figuring out what's the right kind
of, what's the right kind of symmetry to bake into it. I don't necessarily have a good opinion on
what this model might look like. But what I do say is just looking at the immediate utility of
the geometric deep learning blueprint, we are like, I think very strictly saying that we don't want to
use this blueprint to propose, you know, the one true architecture. Rather, we make the argument
that different problems require different specifications. And we provide a common language
that will allow, say, someone who works on primarily grid data to speak with someone who
works on manifold data without necessarily thinking that, you know, you know, somebody might say
commonets are the ultimate architecture, someone else might say GNNs are the ultimate architecture.
And in some ways, they could both be right and they could both be wrong. But this blueprint kind
of just provides a clear delimiting aspect to these things. Just like in the 1800s, you had all
these different types of geometries that basically lived on completely different kinds of geometric
objects, right? So hyperbolic, elliptic, and so on and so forth. And what Klein-Zerlangen program
allowed us to do was, among other things, reason about all of these geometries using the same
language of group invariance and symmetries, right? But in principle, the specifics of whether
you want to use a hyperbolic geometry or whether you want to use an elliptic geometry, partly
rests on your assumption what the main do you actually live in, right? When you're doing these
computations. So I think just generally speaking, I think that having a divide is a potentially
useful thing as long as you have a language that you can use to index into this divide,
if that makes sense. It does make sense. But I have a feeling that some people could benefit
from geometric deep learning and they're unaware. I mean, I don't want you guys to motivate
geometric deep learning in general, because I think a lot of deep learning with a structured
prior is already geometric deep learning as you folks demonstrated in your blueprint,
like RNNs and CNNs, for example. So like it or not, we're already all using geometric deep learning.
But some of the esoteric flavors of geometric deep learning, particularly on irregular meshes,
they seem a little bit out there, don't they? I mean, it's possible that many people could
benefit from this, but they just don't know about it yet. I was thinking that, for example,
if I had a LiDAR scanner on my phone, and the result is a point cloud, which is not particularly
useful, but I would presumably transform it into a mesh, which would be more useful. But
is it possible that loads of data scientists out there are sitting on datasets that they could
be thinking about geometrically, but they're not? Many folks are exotic. It's probably in the eyes
of the beholder and well, in machine learning, probably they are to some extent exotic, but
joking apart, many folks are a convenient model for all sorts of data. And the data might be
a high dimensional, but still have a low intrinsic dimensionality or can be explained by a small
number of parameters or degrees of freedom. And this is really the premise of nonlinear
dimensionality reduction. And for example, the reasons why data visualization techniques such
as TSE and E at all work. And maybe the key question is you're asking is how much of the
manifold structure of the continuous manifold you actually leverage. And in the TSE example,
the only structure that you really use is local distances. So if you think of a point cloud,
of course, you can deal with it as a set. But if you assume that it comes from sampling of some
continuous surface, you can probably say more than this is for example, what we tried to do in
some of our works on geometric deep learning in applications and computer vision and graphics.
And measures are one way of thinking of them is as graphs and steroids, where we have
additional structure to leverage. So it's not only nodes and edges, but also faces. And in fact,
measures are what is called simplicial complexes. As to the practical usefulness,
computer vision and graphics are obviously the two fields where
geometric deep learning on measures is important. And just to give a recent example of commercial
success. There was a British startup called the AI. It was founded by my colleague and friend,
Yasunos Kokinos. I was also one of the investors. And we had a collaboration on
three different reconstruction using geometric neural networks. And the company was acquired
last year by SNAP and these technologies already now part of SNAP products. So you see it in the form
of some 3D avatars or virtual augmented reality applications that SNAP is developed.
Yeah, Professor Bronstein, I saw that you were doing some really cool stuff with the higher order
simplicial coverings and graphs. And actually, I was going to call out your recent work on
diffusion operators and graph rewiring. There are so many cool things that we can do to graphs to
actually enable a little of this analysis. But there was a question from my good friend,
Zach Jost, who's one of our staff members here. And he says, what do you think is the most important
problem to solve with message passing graph neural networks? And what's the most promising path forward?
Probably, we first need to agree about terminology. And to me, message passing and here I agree
with Petra is just a very generic mechanism for propagating information on the graph.
Now, traditional message passing that is used in graph neural networks uses the input graph for
this propagation. And we know, right, as we discussed that it is equivalent to device for 11
graph isomorphism tests that has limitations in the kinds of structures it can detect.
Now, there exists topological constructions that go beyond graphs, such as simplicial and cell
complexes that you mentioned. And what we did in our recent works is developing a message passing
mechanism that is able to work on such structures. And of course, you may ask whether we do encounter
such structures in real life. So first of all, we do and measures that I mentioned before are in
fact, simplicial complexes. But secondly, what we show in the paper is that we can take a traditional
graph and lift it into a cell or a simplicial complex. And probably a good example here is from
the domain of computational chemistry, the graph neural network that you apply to a molecular
graph would consider a molecule just as a collection of nodes and edges atoms and chemical bonds
between them. But this is not how chemists think of molecules, they think of them as
structures such as for example, aromatic rings. And with our approach, we can regard the rings as
cells. So we have a special new object, and we can do a different form of message passing on them.
And we can also show from the theoretical perspective that this kind of message passing is
strictly more powerful than the vice for a lemon algorithm. Do you see entirely new problems opening
up that we wouldn't even have, let's say, we wouldn't even have dared to touch before?
You know, let's say we simply have our classic neural networks or whatnot, or even our classic
graph message passing algorithms. Do you see new problems that are now in reach that previously
with none of these methods were really, let's say better than random guessing?
It's a very interesting question that I think I'll answer from two angles. Because you could,
like there could be like some long standing problem that you knew about and wouldn't dare to
attack and now maybe you feel a bit more confident to attack it. There's also the aspect of uncovering
a problem because when you start thinking about things in this particular way, you might realize,
hang on, to make this work, I made some assumptions and those assumptions actually don't really hold
at all in principle. So how do I make things a little bit better? So I'll try to give an example
for both of those. So in terms of a problem that previously I don't think was very easy to attack
and now we might have some tools that could help us attack it better. I have a long standing
interest in reinforcement learning. Actually, when I started my PhD, I spent six months attacking
a reinforcement learning problem with one super tiny GPU. And that was at that time a massive
time sink. Actually, DeepMind ended up scooping my work sometime after that. And I quickly moved
to things that were more doable with the kind of hardware that I had at the time. But I always had
a big interest in this area. And after joining DeepMind, I started to contribute to these kinds
of directions more and more. And I think that basically, there are a lot of problems in reinforcement
learning concerning data efficiency. So when you have to learn how to meaningfully act and do stuff,
which is actually a fairly low dimensional signal compared to the potential richness of the
trajectories that you have to go through before you get that useful signal, like long term credit
assignment, all these kinds of problems, I feel like we can start to get more data efficient
reinforcement learning architectures by leveraging geometric concepts and also algorithmic concepts.
So to give you one example of this, we have some months ago put out a paper on the archive called
the executed latent value iteration network or Excelvin, where we have captured the essence of
an algorithm in RL, which perfectly solves the RL problem. So the value iteration algorithm,
assuming you give it a Markov decision process, will give you the perfect policy for that Markov
decision process. So it's a super attractive algorithm to think about when you do RL, big caveat,
right? You need to know all the dynamics about your environment, and you need to know all the
reward models of the environment before you can apply the algorithm. So this obviously limits its
use in the more generic deep reinforcement learning setting. But now with the knowledge of
the underlying geometry of the graph of states that the MDP induces and the algorithmic reasoning
blueprint, we actually taught the graph neural network, which aligns super well with value
iteration, actually, we taught it to in a nicely extrapolating in a reasonably extrapolating way,
on a bunch of randomly sampled MDPs, learn the essence of the value iteration computation,
and then we stitched it into a planning algorithm in a deep reinforcement learning setting.
And just by like training this pipeline end to end with a model free loss, we were able to
get interesting returns in Atari games much sooner than some of the competing approaches.
So it's a very small step, it still requires, you know, 100,000 200,000 iterations of playing
before you get meaning, some meaningful behavior start to come out. But it's a sign that we might
be able to move the needle a bit backwards and not require, you know, billions and billions of
transitions before we start to see meaningful behavior emerge. And I think that's very important
because in most real world applications of reinforcement learning, you don't have a budget
for billions of interactions before you have to already learn a meaningful policy. So that's
one side I think. And just generally in reinforcement learning graphs appear left, right and center,
not just in the algorithms, but also in the structure of the environment and these kinds of
things. So I think that's one area where geometric deep learning could really help us, you know,
get better behaviors faster, not necessarily solve it better than the standard deep RL, but,
you know, get the better behaviors in fewer interactions. And as for one problem that we
have uncovered through this kind of observational lens, you know, as I said, often in graph
representation learning, we assume innocently that the graph is given to us, whereas very often,
this is not the case. So this divide has brought about this new emerging area of latent graph
learning or latent graph inference, where the objective is to learn the graph simultaneously
with using it for your underlying decision problem. And this is a big issue for neural
network optimization, because you're fundamentally making a discrete decision in there. And if the
number of nodes is huge, you cannot afford to start with the n squared approach and then gradually
refine it. So currently, the state of the art in many regards of what we have here is to do a k
nearest neighbor graph in the feature space, and just hope that that gets us most of the way there.
And usually this works quite well for getting, you know, interesting answers, because, you know,
if you have a decent dish enough k and n graph, you will cover everything that you need reasonably
quickly. But, you know, then there that raises the issue of what if the graph itself is a meaningful
output of your problem, what if you're a causality researcher that wants to figure out how different,
you know, parts of information interact to them, they probably wouldn't be satisfied with a k
nearest neighbor graph as an output of the system. So yeah, I feel like there's a lot of work to be
done to actually scalably and usefully do something like this. And I don't have a better answer than
what I just said is the state of the art. So a potential open problem for everybody in the audience
today. Absolutely. You touched on some really interesting things out that I think causality is
a huge area that we could be looking at graphs on. And also we had Dr. Tom Zahavi from DeepMind,
one of your colleagues, and he said that, you know, he looks at meta learning and also diversity
preservation in in agent based learning. But he thinks that reinforcement learning is just about
to have its image net moment where we can discover a lot of the structure in these problems, which
is fascinating. I would like to bring up one application where maybe quantitative improvement
that is afforded by a genetic deep learning can lead to a qualitative breakthrough. And this is
a problem of structural biology also for this one such example, correctly geometrically modeling
the problem you get a breakthrough in the performance. So it's indeed an image net
moment that happened in this field. And now once you have sufficiently accurate prediction of
3D structure of proteins, it suddenly enables a lot of interesting applications, for example,
in the field of drug design. So potentially entire pharmaceutical pipelines we invented
with the use of this technology. And the impact can be extraordinary.
It's so true. I mean, Professor Bronson, when I was watching your lecture series,
it blew me away when you were talking about all of the applications. I think Yannick said a minute
ago that it's almost as if some of these applications are just so ambitious that the thought
wouldn't even have crossed our mind that we might be able to do it before, like for example,
being able to predict facial geometry from a DNA sequence. So we might be able to look at an
old DNA sequence and actually see what that person looked like. That would have been unimaginable
just a few years ago. So that's incredible. I'm really interested in your definition of
intelligence, right? And whether you think neural networks could ever be intelligent.
Douglas Hofstadter, for example, he wrote the famous book, Godel Escher Bach, it was a
Pulitzer Prize-winning book in the 1970s, but he made the argument that analogy is the core of
cognition. He said that analogies are a bit like the interstate freeway of cognition.
They're not little modules on the side or something like that. And I think that in a way,
analogies are also symmetries, right? So when I say that someone is firewalling a person,
it means that they don't want to talk with that person. It's a symmetry between the abstract
representation of a real network firewall and an abstract social category. So does this require
a different neural network architecture or could geometric deep learning already deliver
the goods, right? It's almost as if it's just a representation problem.
I think it's a very important question, one which, well, I cannot claim to have the right
answer to and my definition is I guess a little bit skewed by the specific research that I do
and the engineering approaches that I do. But I think in large, I agree with the idea of analogy
making and maybe I would take it a step further, right? Where you have a particular set of knowledge
and conclusions that you've derived so far, a set of primitives that you can use once new
information comes in to figure out how to recompose them and either discover new analogies or just
discover new conclusions that you can use in the next step of reasoning. And it just feels
really amenable to a kind of synergy of, as Daniel Kahneman puts it, system one and system two,
right? You have the perceptive component that feeds in the raw information that you receive as
your input data, transforms it into some kind of abstract conceptual information. And then
in the system two land, you have access to this kind of reasoning procedures that are able to
take all of these concepts and derive new ones from hopefully a nice and not very high dimensional
set of rules. And this is why I believe that if we that in terms of like moving towards the an
architecture that supports something like this, I think we have a lot of the building blocks in
place with geometric deep learning, especially if we're willing to, as I said, kind of broaden the
definition of geometric deep learning to also include category theory concepts, because that
might allow us to reconcile algorithmic computation as well into the blueprint.
So the idea is, you know, you have this, I mean, there's no need to talk at length about all these
great perceptive architectures. So I think we're already at a point of, if we show our AGI lots
and lots of data, it's going to be able to pick up on a lot of the interesting things just by
observing like, you know, self supervised learning, unsupervised learning is already showing a lot
of promise there. But then the question is where I think we still have quite a bit of work to do
is once we have these concepts, let's even assume that they're perfect. What do we do with them?
How do we meaningfully use them? And I think the reason why I believe there's a lot of work to be
done there is because one of the very key things that I do on a day to day basis is teach graph
neural networks to imitate algorithms from perfect data. So I give them exactly the abstract input
that the algorithm would expect. And I asked them, Hey, simulate this algorithm for me, please.
And it turns out that that is super, super hard. Well, it's super easy to do it in distribution,
but you're not algorithmic if you don't extrapolate. And that's, I think, one of the
big challenges that we need to work towards addressing, will geometric deep learning be
enough to encompass the ultimate solution? I have a feeling that it will. But, you know,
I don't I don't necessarily just based on the empirical evidence we've been seeing in the
recent papers that we've put out. But yeah, I don't I don't have a very strong theoretical reason
why I think it's going to be enough. Yeah, I'm fascinated by this notion that intelligence
isn't mysterious as we might think it is. It's a it's a receding horizon. And it might in the end
be disappointingly simple to mechanize. Actually, if you take the term literally,
intelligence come from the latent word that means to understand. And I think what is meant
by understanding is really a very vague question. And probably if you ask different people, they
will give you different definitions. I will define it as the faculty to abstract information.
And in particular, information that is obtained in one context, the ability to use it in other
contexts, this is what we usually call learning is the way that this information is abstracted
and represented might be very different in a biological neural network in our brain versus
an artificial intelligence system. So if you hear some people saying that that
the brain probably doesn't really do geometric computations, my answer to that would be that
we don't necessarily need to imitate exactly the way that the brain works. We just need probably to
try to achieve this high level mechanism that is able to abstract information and applied
as knowledge to different problems. The definitions of artificial intelligence
that are being used like the famous Turing test, I find is very disturbing that it's very
anthropocentric. And it is actually probably very characteristic of the human species
more broadly. And this way, by judging what is intelligent and what is not, we might potentially
rule out other intelligent species because they're very different from us. I may be obsessed with
sperm whales because I'm working on studying their communication. They are definitely intelligent
creatures, but would they pass the Turing test? Probably not. It's like subjecting
a cat to a swimming test or a fish to climbing on a tree.
So I would just like to add to Michael's great answer one quote that I think is very popular
and applies really well in this setting. The question of what are computers to think
can think is about as relevant as whether submarines can swim. When you built submarines,
you weren't necessarily trying to copy fish. You were solving a problem that was fundamentally
slightly different. So could be relevant in this case as well.
We spend quite a lot of time on this podcast talking about whether we should have an
anthropocentric conception of intelligence. A corporation is intelligent. And I'm starting
to come around to the view of embodiment and thinking that there is something very human
like about our particular flavour of intelligence. But maybe there is a kind of pure intelligence
as well. And this was the end of my conversation with Tako Kohen. One of the really interesting
things is you're getting on to some of the work that you've done are being able to think of group
convolutions on homogeneous objects like spheres, for example, but also you moved on to irregular
objects like any mesh. And you looked into things like fibre bundles and local convolutions.
Because these are objects, I think you said a homogeneous object is where you can't
perform some transformation to get from one place of the object to the other part of the
object. So what work did you do there on those irregular objects?
Yeah, that's a great question. So when we think of convolution, we're sort of
putting together a whole bunch of things, namely this idea of locality. So typically our filters
are local, but that's a choice ultimately. Convolution doesn't have to use a local filter,
although in practice we know that works very well. And there's the idea of weight sharing between
different positions and potentially also between different orientations of the filter.
And as I mentioned before, this weight sharing really comes from the symmetry.
So the fact that you use the same filter at each position in your image when you're doing
a two-dimensional convolution, that's because you want to respect the translation symmetry acting
on the plane. In the case of a general manifold or mesh, you typically not have a global symmetry.
If you think of a mesh representing a human figure or let's say it's some complicated protein structure,
there may not be a global symmetry, or sometimes in the case of say a molecule might have some
six-fold rotational symmetry. This symmetry may not be transitive as it's called, meaning you
cannot take any two points on your manifold and map one to the other using a symmetry.
In the case of a sphere, you can do that. Any two points in the sphere are related by rotation.
So we say a sphere is a homogeneous space, but let's say this protein shape is not homogeneous,
even if it has some kind of symmetry. So in that case, if you try to motivate the weight
sharing via symmetry, you try to take your filter, put it in one position, move it around to a
different position using a symmetry, you're not going to be able to hit whole position,
so you're not going to get weight sharing globally. And that just is what it is.
If you say I have a signal on this manifold and I want to respect the symmetry, well, if there are
no global symmetries, there's nothing to respect, so you get no code strain, so you can just use
arbitrary linear map. Now, it turns out there are certain other kinds of symmetries called
gauge symmetries that you might still want to respect. And in practice, what's respecting
gauge symmetry will do is we'll put some constraints on the filter at a particular position.
So for example, that might have to be a rotationally equivariant filter,
but it doesn't tie the weights of filters at different positions. So if you want that as well,
then you can maybe motivate it via some kind of notion of local symmetry. I have something on a
local symmetry group point to motivate that in my thesis, but
there isn't a very principled way to motivate weight sharing on general manifolds
actually between different locations. Yeah, I'm just trying to get my head around this. So you're
saying because the whole point that we're trying to achieve here is to have a parameter
efficient neural network that uses local connectivity and weight sharing, as we do with
let's say a plain RCNN, whereas when you have an irregular object, it's very, very difficult to do
that. So you're saying in some restricted domain, you can do it. Let's say if you have a rotation
equivariant, but you can't do the other forms of weight sharing. I'm just trying to get my head
around this because with a graph convolution on your network, for example, it seems like you can
abstract the local neighborhoods, this node is connected to these other nodes. And potentially
that could translate to a different part of the irregular mesh. So why can't you do it
more than you've suggested? I think if you want to be precise, you just have to say what are the
symmetries that we're talking about here. And in a graph, the most obvious one is the global
permutation symmetry. So you can reorder the nodes of a graph. And really, any graphical net,
whether they're top they're coming from an equivariance perspective or not, all graphical
nets in existence that have been proposed, they respect this permutation symmetry. And typically,
this happens through, let's say, in the most simple kind of graphical visual nets, like the ones by
Kip van Belling, for example, there's a sum operation, some messages from your neighbors.
And some operation does depend on the order of the summands. So it doesn't depend on the
order of the neighbors. And that's why the whole thing becomes equivariant. So that's a global
symmetry that that all graph networks respect. On that, though, could you not create a local,
let's say if there was a local graph isomorphism, and so I have an irregular object, but it has a
local isomorphism, could I not use something like a GCN, a local version of it to capture that
isomorphism? Yeah. So actually, this was something we we proposed to do in our paper natural graph
networks. So this paper really has two aspects added to it. One is the naturality as a generalization
of equivariance, I can talk about that. But another key point was that we can not just develop
a global natural graph network, as you call it, but also a local one. And what the local one will
do is it will look at certain local motifs. So maybe if you're analyzing molecular graphs,
one motif that you often find is, you know, aromatic ring or something, some ring with,
let's say six corners, various other kinds of little small graph motifs. And these motifs might
appear multiple times in the same molecule, or across different molecules. And so what this
method is doing is it's essentially finding those using some kind of graph isomorph, local graph
isomorphism, and then making sure that whenever we encounter this particular motif,
we process it in the same way, i.e. using the same weights. And if the local motif has some kind
of symmetry like this, this aromatic ring, you can rotate it six times and it gets back to the
origin or you can flip it over. So that's the symmetry of this graph structure or an automorphism
of this graph. And then the weights will also be constrained by this anamorphism group, this group
of symmetries of the local motif. And various other authors also have, I think, even Michael
Bronstein and students have developed methods based on similar ideas.
Awesome. Tacho, it's been such an honor having you on the show. And actually,
you're coming back on the show in a few weeks' time. So we don't want to spoil the surprise.
Looking on this proto book that you've written with the other folks, what's the main thing that
sticks out to you as being the coolest thing in the book? I think there's any one particular thing.
What excites me is to put some order to the chaos of the zoo of architectures and to see actually
that there is something that they all have in common. And I really think this can help new
people who are new to the field to learn more quickly, to get an overview of all the things
that are out there. And I also think that this is the start of at least one way in which we can
take the black box of deep learning, which often is viewed as completely inscrutable and actually
start to open it and start to understand how the pieces connect, which can then perhaps inform
future developments that are guided by both empirical results and an understanding of what's
going on. Amazing. Thanks so much, Tacho. Thanks for having me. It's been a pleasure.
Joanne, thank you so much for joining us. This has been amazing.
Okay, thank you so much, Tim. It was very fun and best of luck. And I think let's maybe get
in touch. Thank you very much. It's very nice to be talking to you today about these completely
random topics. Yeah.

my pleasure. And I must say, I've been really impressed by all your
questions. It showed that you did prepare and had read papers and
think about it. And that's very much appreciated. Thank you.
Today is an incredibly special occasion. We have Professor
Yoshua Benjiro on the show. Just honestly, I just can't get over
it. But first of all, a little bit of housekeeping. So we've
just launched a new Discord community. So please jump in
there, say hello, introduce yourself. If you want to be, you
know, part of the moderating community or just help us do
stuff over there, we would love to talk with you. By popular
demand, we've also added a couple of ways in which you can
support us. So we now have a Patreon and a merch store. If
you're interested in supporting some of the episodes of MLST
then get in touch with us because we'd love to have a
conversation with you. We're doing so much cool stuff this
year. We've already recorded about six episodes that we
haven't released. And we've got some amazing people booked as
well. So yeah, it's going to be incredible. As always, if you
like the content here, please consider hitting the like and
subscribe button and rating our podcast on iTunes because it
really, really helps us out. I called it iTunes. Is it
iTunes? Apple Podcasts? I don't know, whatever it's called.
Wait and Biases is the developer first MLS platform. And
we're extremely proud today that they are sponsoring this
episode. Now tracking machine learning experiments is
difficult. Using the winging it methodology can only get you so
far. What we need is a platform where we can compare models and
visualize their performance characteristics against all of
the previous runs and figure out the best hyper parameters to
use. Now most importantly of all, this process needs to be
reproducible. Sounds like a tool order, right? Well, this is
exactly what the Wait and Biases platform does for you. Now you
can even follow metrics from long running experiments in real
time. I think it's really important to lean into the
complex interaction between science and engineering in the
ML DevOps lifecycle. Data scientists need valuable feedback
and they need to communicate why they're running given
experiments, and they need to share their notes around the
next steps. Reports keep this work well organized and connected
to the Waits and Biases experiments which were run as
opposed to just sharing random screenshots in Slack. It's so
easy to create a report and share it with your team after you
finished with your experimentation. You could just add
notes for yourself as well to explore later on. You can keep a
work log and you can even share your findings internally or
externally. This is an absolute game changer. I'm a big
believer in this kind of engineering rigor. I'm the
CEO of a code review startup called Merge these days. And I
love how the poor request process and tooling immortalizes
important collective decisions which were made during the
software development lifecycle. Similarly, Waits and Biases
immortalizes important decisions that were made during
model development, experimentation, and deployment.
Remember, check out Waits and Biases today by going to
1db.com forward slash M LST. And if you're interested in
sponsoring future episodes, get in touch with us. Waits and
Biases are currently sponsoring our premiere shows, but we have
lots of other content coming and opportunities for
sponsorship. So let us know. Cheers.
Professor Yoshua Benjo has just released a bunch of papers
around G flow nets. Now G flow nets exist squarely in the
domain of active learning, which is a model that can
economically ask an Oracle, which is probably the real world,
for the most salient training examples to continue learning.
The learner can choose or have an influence on the examples it
gets. And we want to learn a function which approximates the
Oracle efficiently. How should we pick the queries? How should we
take into account not just the value of the predictor, but also
how certain we are about the predictors from the learning
system? Areas of uncertainty or entropy are kind of like
interesting candidates for us to explore further. We need to be
able to imagine or invent queries to give to the Oracle. Now
one of the reasons that machine learning models are so
sample and efficient is because of the combinatorial space of
possible input examples. We can't train on everything because
the space is just too large. It's vast. So you might have heard
of a related concept of active learning called machine
teaching, which is an interactive version where the human
interactively selects the most salient data to train a machine
learning model, maximizing the information gain in respect of
the training samples. Now, the reality is the function space
that we're learning here is highly structured. We only really
need to sample training data where most of the rich information
exists in that function space. I mean, if you think about it, a
machine learning model, it's just a joint probability
distribution between signals and labels. And this distribution
has modes or areas of density or information. And actually, most
of it is just areas of nothingness, which require fewer
training examples to learn and to represent. Now, if you spoke
to a to a Bayesian person like my friend Conor Tan at work, you
know, how to learn this distribution, they would bring
up Markov chain Monte Carlo quicker than a whip it with a
bumful of dynamite. Now, Markov chain Monte Carlo is an
increasingly popular sampling method for obtaining asymptotic
information about unnormalized distributions or energy
functions, especially for estimating the posterior
distribution in Bayesian inference, which is where you've
probably heard of it before. Now, you can characterize a
distribution without knowing all of the distribution's
mathematical properties. So if you don't have an analytical
representation for it, just by randomly sampling values out of
the distribution. Now, a particular strength of Markov chain
Monte Carlo is that it can be used to draw samples from
distributions, even when all that is known about the
distribution is how to calculate the density for different
samples. Now, the the Markov property of Markov chain Monte
Carlo is this idea that random samples are generated by a
special sequential process. And each random sample is used as a
stepping stone to generate the next random sample. Now, this
might sound very complex, but the practical implementation is
pretty simple. Markov chain Monte Carlo just starts with an
initial guess, just one value that might plausibly be drawn
from the distribution. And then we produce a chain of samples
from this initial guess by adding random perturbations in the
neighborhood of that example. And each new proposal drawn from
that random perturbation distribution is either rejected
or accepted. There are different flavors of this, of course, I
mean, in particular, like tweaking, how the random
proposals in the neighborhood are selected or whether the
proposals are selected, the simplest heuristic being whether
it's below the function or not. Now, the idea is that Markov
chain Monte Carlo methods, they capture a distribution with
only a relatively small number of random samples. But the
reality is anything but in high dimensions, and where the
distribution has many modes spread far apart, it's actually
exponentially expensive. There's a bunch of human orientated
hacks to try and make this work well in specific cases, but we're
missing a much more general machine learnable solution. This is
the main reason why we haven't seen it used in many machine
learning applications yet, assuming that the function we want
to learn has underlying structure, then we can escape the
exponential time of Markov chain Monte Carlo with machine
learning. And this is what Bengio calls systematic
generalization, which is to say, how do we generalize far from
the data in a way which is meaningful. Now, G flow nets are
an active learning framework, where the name of the game is to
generate salient and diverse training data to augment our
model in the most sample efficient way possible. For G flow
nets to work, we need a reward function and a deterministic
episodic environment. Does that sound familiar? Yes, just like
reinforcement learning. Now, a flow network is a directed graph
with sources and sinks and edges carrying some amount of flow
between them, you know, through intermediate nodes. So I think a
good way to think about this is pipes of water. Now, for our
purposes, we define a flow network with a single source. So the
root nodes, or you might say the sinks of the network correspond
to the terminal states. Now, it's designed to find the
possible trajectories through our system. Okay, and just think
of Alpha zero as being like a good analogy for these
trajectories. Now, the training objective is to make them
approximately sample in proportion to the given reward
function. This is in stark contrast to Alpha zero, where we
were sampling to maximize the expected reward. So Bengio's
big idea is that we could have an interacting loop between a
generative model and the real world. The real world is
expensive. So why not train an imagination machine in our mind
until we're ready and waiting to produce good questions to the
real world, we could use imagined experiments to train our
generator, then produce queries to the real world. We were
thinking about a way to visualize how G flow nets work
when the idea of a Galton board came to mind. A Galton board,
also known as a beam machine, is a common prop in statistics
courses, science museums, and fun gadget stores. The board has
rows of interleaved pegs above a bottom row of buckets. Beads
are filled into a funnel at the top of the board and then
sprinkled on the top center peg. The beads bounce either to the
left or to the right as they hit the pegs and eventually collect
into buckets at the bottom. If the pegs are precisely and
symmetrically arranged, the beads will aggregate at the bottom
into a familiar binomial bell curve. Now imagine that the pegs
were instead flow gates with adjustable valves that could
direct the beads more to the left or more to the right to
bias the flow paths. With such a machine, you could adjust the
valves or flow rates to create any distribution. For example, to
create a uniform distribution, we'd open up the gates flowing
away from the center line of the board to drive more bead flow
to the fewer number of paths leading to the edges and the
corners. Or to create a multimodal distribution, we'd arrange
the gates to split the flows into two or more streams that would
then pile up in multiple humps or modes below. There's a lot of
flexibility here. Indeed, given a distribution, there are
generally multiple flow gate solutions to produce it. It'd be
nice, wouldn't it, if we had an intelligent, principled way to
train these gates? Enter G flow nets. G flow nets put a neural
network, a brain behind the flow adjustments. A brain which can
optimize the gates to match any distribution we desire. Here,
we're interested in sampling a reward function in the context
of reinforcement learning. In that context, this is a powerful
simulation and sampling paradigm. You see, once the brain has
tuned the flow weights, such a modified Galton board, or more
generally, a flow network, can sample diverse paths quickly
and efficiently, leading to the reward distribution. It's
important to point out that the path sampling is more diverse
doing it this way. Unlike classic reinforcement learning, a G
flow net doesn't just fixate on a small number of high reward
paths it happens to find first. Instead, it stochastically
samples a broad spectrum of paths in proportion to their reward.
Sure, high reward paths will be sampled with higher weight, but
the far larger population of low reward paths will get a share
of the sampling as well. Why should we even bother with such
paths? The answer is we need to balance exploitation or high
reward with exploration or learning to better learn the
reward function. This is especially important when dealing
with complex real world scenarios of high uncertainty. For
example, think of molecular drug discovery and design or
navigating jungle terrain. In both those scenarios, we really
know very little about how a particular path may play out. We
might stumble into the next miracle cure or a pitfall of
quicksand. To find the globally best paths, it's important to
keep our options open. Beyond this sampling diversity, G
flow nets also bring the full power of neural networks to
discover latent structure and learn the reward function. This
combined with their diverse sampling also makes G flow nets
more robust when dealing with multimodal distributions, which
are a common trap for greedy algorithms and Markov chain,
Monte Carlo. If there is structure linking the multiple
nodes, G flow nets can learn it and extrapolate to new modes
and once discovered they will by design, drive the sampling to
cover those modes and learn more structure. Overall, G flow
nets seem to offer an intriguing new path pun intended for an
intelligent sampling paradigm. So you might ask how are G
flow nets different from Alpha zero? Well, the policy network
and Alpha zero gives you a set of actions. Given a state, Alpha
zero trains the policy network to maximize reward so that the
trajectories all end up at the highest reward. Now what G
flow nets do is they train so that the actions are distributed
in proportion to the reward. So rather than pruning away all of
the low reward trajectories, it will sample them just less
often. Now there is a manifest difference between G flow nets
in respect of exploration. I mean, you might argue that the
Monte Carlo tree search is still doing wide exploration at the
beginning. But in spite of its rapid convergence and pruning of
load and reward trajectories, it's still sampling from the
underlying probability distribution, which has been
scaled with a softmax. So that's actually not that much to
explore in the first place. So in summary, G flow nets are
better than Alpha zero Monte Carlo tree search in some sense,
because they achieve the same goal by offloading the burning
time and the stabilization time of Markov chain Monte Carlo.
Remember this whole thing can be trained offline. And then
when in inference mode, we can do it in a single shot. Whereas
with Monte Carlo tree search, we actually had to do it in
inference mode as well. The other thing is we're kind of
offloading all of the human engineering required to sample
efficiently from Markov chain Monte Carlo. And the other thing
is diversity, baby. I mean, consider the difference between
how G flow nets and Alpha zero sample the reward path
distribution. If you looked at the distributions, you would
see that Alpha zero has a little box around the mode. G flow
nets is the whole distribution. We know very well that
diversity preservation is critical in order to discover
interesting stepping stones and search problems. Now finally,
Benzio has published results showing the G flow nets converge
exponentially faster than Markov chain Monte Carlo and PPL on
some problems, and finds more of the modes in the distribution
function faster. Enjoy the show folks.
Professor Yoshio Benzio is recognized worldwide as one of
the leading experts in artificial intelligence. Indeed, a god
father of deep learning. His pioneering work in deep learning
earned him the Turing Award, which is the Nobel Prize of
computing. He's a full professor at the University of
Montreal, and the founder and scientific director of Miele,
which is a prestigious community of more than 900 researchers
specializing in machine learning and AI. He's one of the most
cited computer scientists on the planet. And I can't even
begin to articulate how honored we are today to have this
conversation. Yoshio has done a lot of work recently on G flow
nets, which are an active learning framework in a
reinforcement learning configuration, where the name of
the game is to request salient and diverse training data from
the real world to augment our learned models in the most
sample efficient way possible. Now we're trying to minimize the
divergence between the path distribution and the reward
distribution, and then sample paths according to the reward
distribution. This is in stark contrast with traditional
reinforcement learning, where we're trying to maximize the
expected reward. This approach is likely to find diverse
strategies instead of being greedy and converging quickly
after finding a single one. Anyway, Professor Benzio, this is
amazing. Can you tell us about this exciting work and some of
its applications?
Yeah, I'm, I don't think I've been as excited about a new topic,
at least in the last six or seven years as I'm now with G
flow nets. And it's actually even much more than what you've
been talking about. The way I think about G flow nets is a
kind of framework for generic
learnable inference for probabilistic machine learning. So
one way to think about this is it's a learnable replacement for
Monte Carlo and Markov chain sampling. But actually, so there's
that and I'll explain if you want why this is important and to use
machine learning there. But but also, it can be used to
estimate probabilities themselves, not just sampling, but
also estimate
intractable quantities like partition functions and a
conditional probabilities that would otherwise require summing
over an intractable number of terms. So I think of this as
the potentially, you know, there's still we're still at the
beginnings of this has a Swiss army knife of probabilistic
modeling that uses machine learning to be able to do things
that look intractable, but do them efficiently thanks to
generalization power of large neural nets.
We've been trying to think of a way to help our listeners
visualize what a what a G flow net does. And I wanted to run by
a possibility to you. So I'm not sure if you've heard of
Galton boards also called, you know, bean machines. And what
they are is this prop that's often used by statistics
professors at the start of say, an elementary introductory
course to give a visual intuition. And it's a board that has
these vertical buckets down at the bottom with interleaved rows
of pegs, above the buckets, and then beads are filled in into
the top of the board and they bounce either left or right as
they hit the pegs. And they eventually collect down at the
bottom. Yeah, yeah. Yeah, now now if the very good analogy,
except that it's not a tree, I don't know how these things are,
but you know, the ball can come to a place from two different
paths or an potentially large number of paths. Right, right.
And I think given given there are some some differences, you
know, the idea was that if the pegs are, it's pretty close to
exactly what it is. Yeah, and what we were thinking is that if
the pegs on the Galton board are precisely and symmetrically
arranged, you know, the beads will form a nice binomial curve
at the bottom. And it seems like what G flow nets are capable
of doing when they optimize the pathways. They're tweaking the
pegs a little bit to the left, or a little to the right, to
bias the flow of beads one way or the other. And in this way, a
G flow net could arrange the pegs so that the beads could form
any distribution at the bottom that we want. And for our
purposes, that means the distribution that matches the
reward function. So is this a good way to think about G flow
nets? Yes, it is. Now, it's missing a really important
aspect of it, which would be difficult to present visually,
but that all of these peg weights, like the positive
variable goes left or right, are not just like learned
independently, like as a tabular machine earning, but that
there's like one neural net that knows about the locations in
this big board as input and tells, you know, how much relative
weight should I, you know, go to go left or right at this
position. So the reason this is important is because it allows
for generalization. Because this board is huge, it's
exponentially large, right? So there's no way you're going to
learn like a separate parameter for each of these choices. And
so you have this neural net or potentially several neural
nets, but that share allow you to share statistical strength,
as we call it, share information across all the possible
positions, so that you can generalize to places paths
that it has never seen from the finite number of training
trajectories that it sees while it's being trained. And that's
crucial. Otherwise, you couldn't scale to large problems,
which is really what we want to do.
Professor Benjo, we spoke with Professor Carl Friston about his
free energy principle and active inference, which is pretty much
a Bayesian flavored version of reinforcement learning. And he
said that while we need to maintain entropy and stop
models from increasing too much in complexity, we should balance
entropy with accuracy in a principled way. And by the way,
you can kind of think of them in just the audience think of
entropy as keeping your options open. But Friston thinks that
the Bellman-esque idea of reinforcement learning, which
is to say maximizing expected reward as the objective is
misguided, and we should instead perform inference over future
paths, balancing expected reward of relative entropy. Is there a
connection between these ideas? I mean, it seems like G flow nets
are sampling paths proportional to the reward function that will
maintain as much entropy as the reward function itself.
Yes, yes, exactly. It's a translation of the reward
function into machinery that can sample, you know, the
equivalent, the corresponding distribution. So yeah, I
completely agree with what Carl was saying here. But as I said,
what's interesting is, we can do things with G flow nets. In
principle, we've done the math and some small scale
experiments that we have now a number of papers, we can do
things that go beyond sampling. But for example, estimate
entropy itself. So entropy is notoriously difficult to estimate.
And I mentioned in my talks on G flow nets that we can use the
G flow net machinery to estimate entropy of say, an action
distribution or a distribution over Bayesian parameters, for
example, which is would be something you'd like to minimize
if you're going to take an action in the world. And you have a
model of the world that has uncertainty. And that connects
with Carl, for instance, interest. You'd like to be able to
choose an action that minimizes your uncertainty about how the
world works, we know what are the related things that may have
happened. And a good, you know, an important part of that is
estimating the reward for the these these exploratory actions
like, you know, children playing around is how much reduction
in entropy of my knowledge of the world, I'm going to get
through that action. So you need to be able to compute that
reward. And that word word is basically an entropy over
something you care about. And it turns out you can also do that
with G flow nets.
We're actually speaking we're first and again next week that
do you have a question that you would like us to put to him?
Well, he, you know, he's on the biology side of things much
more than I am. And I believe there are amazing scientific
opportunities to explore how the kind of machinery that G
flow nets offer could be used by brains in order to do some of
the things they do. Using your nets to model the probabilistic
structure of the world, including uncertainty, which is
something he cares about. But but also taking into consideration
things like high level cognition, the global workspace theory,
which is something I care a lot about attention, they all kind
of fit in the picture of G flow nets. So so I think there's a
huge potential of research at the synergy of computational and
theoretical neuroscience, and machine learning, probabilistic
modeling of the kind that G flow nets propose to come up with
a some proposals for explanatory theories about what the brain
does, that's probabilistic. And you know, I think he would be a
great person to be part of that.
Fascinating. Well, going a little bit further down that line,
there are folks in the community who are huge advocates of
biologically inspired approaches to machine
intelligence. And, you know, one of the key ideas actually is
diversity, discovery and preservation, both in how
knowledge is acquired and represented. I mean, specifically
evolutionary algorithm advocates, they differentiate
themselves from gradient based single agent monolithic
approaches like reinforcement learning. And they point out
that their approaches overcome so called deception and search
problems, you know, which is to say they don't get stuck in
local minima, your approach seems to be achieving something very
similar in the context of a gradient based reinforcement
learning package. I mean, I don't see it as being mutually
exclusive. But what's your take on this?
Yeah, diversity is important when you're exploring and humans,
especially young ones are exploration machines, they're
trying to understand how the world works and they're acting in
the world in order to get that information. Yeah, I agree that
that search process needs to have a big borders on on
diversity, like on trying different ways of achieving
something good, like better understanding how the world works.
So it turns out that in the G flow net framework, you, you have
a training objective that yields this kind of diversity in
exploration, but is based on training large neural nets end to
end. Now, it's a bit different from the usual end to end
training, because we don't have an objective, the objective we're
trying to optimize is not tractable, actually. But we can
sample these trajectories, which I think of like sampling
thoughts, like our thought process is going through some
chain of explanations, not a complete, and it doesn't represent
all the explanations. But but what we found with our training
objectives for G flow nets is that these sort of random,
randomized kind of views of the world are sufficient to give a
training signal to the neural nets that do the real job.
Yeah, I'm curious. So this trade off between exploration
versus exploitation. And this has come up in so many contexts,
you know, throughout our show. And one in particular, as we
taught to, you know, we've talked to multi arm banded folks,
right? And G flow net seemed to capture this balance between
exploration exploitation. But the multi arm banded folks, you
know, they dive deep in that research circle into this, into
this trade off. And I think they have some very principled ways
and even very rigorous ways to analyze this fundamental trade
off. To what extent do you think that that their research maybe
could be applied to future G flow net variations? Like, do you
think maybe it might open up more options to fine tune the
trade off between exploration and exploitation?
Yeah, I mean, the banded research is very, very closely
related to the G flow net thread. The G flow nets, as we have
been using them, for example, for drug discovery, they are
bandits. It's just that the action space is not, you know, one
out of n things. It's, it's combinatorial because you you
build these pieces. So the action space is not something you
can enumerate. So you can't apply the typical band data
algorithms. But a lot of the math is totally applicable. And in
fact, what we use in the drug discovery setting is UCB upper
confidence bound objective to learn a good exploration policy.
So that comes out of the banded research. What it does is it, you
know, it combines the, the risk and reward expected reward
quantities together in a way that in theory guarantees that
you will do an efficient exploration and find that where is
the, you know, where is the money? Where's the reward, right? All
of the possible places where you can get the reward.
So in, in, in the G flow net papers, you often describe it as,
you know, we want to sample not only the maximum reward path in
order to have more diversity in order to maybe figure out
something that we didn't know if we were just to go to the
maximum reward. And that speaks a little bit to the, like the
things that we know that we don't know, right? We, we maybe know
that, right, this seems like a lower reward trajectory might
turn out to be a higher reward trajectory. However, exploration
and reinforcement learning is also fundamentally addressing the
things about the things that I don't know that I don't know,
which is where stuff stuff like random exploration and things
like this comes in. Could you maybe comment a little bit on how
you see sort of, because it seems to me that if I managed to
sample according to what I think is the reward distribution,
right, I still have this problem of maybe there is a deceptive
rewards, there are, you know, I need to take a step back, I may
not know some sort of some, some area of the search space. And
don't I just run into the same problems again?
So, so the important trick here is you need your model of the
reward distribution, or the reward function to be one that
captures uncertainty, like, maybe in a Bayesian way, or, you
know, which whichever way, the Bayesian way, by the way, fits
well with the G flow net framework, because we can consider
the parameters of the reward function as latent variables,
like you don't actually know the reward function, you're trying
to figure it out from experiments. So the G flow net can
sample not just like what you should be doing in order to
acquire information, but also potential reward function. So,
you know, we don't actually have a knowledge of how the, you
know, what's going to be the rewards we're going to get in
the world. The classical IRL is going, as you said, to the
expected value and trying to maximize that, whereas the G
flow net approach is trying to acquire as much knowledge as
possible about the underlying reward function. So you're
trying to minimize the uncertainty. So your model, the
G flow net is modeling the uncertainty, and then it can use
it as a reward for the policy that is going to do action in
the real world. So we're talking about different G flow
nets. There's a G flow net that models the uncertainty in the
reward that you're going to get from the real world. And that's
like a Bayesian model. And then you have another G flow net that
controls the policy that searches. And its reward is how much
uncertainty reduction you're going to get by doing this or
that. So, so yeah, you need to have a part of your model that
is kind of aware of the fact that there are whole areas in the
world that you don't know about or aspects of the world that you
don't know about so that it can drive the exploration.
I would love to know where some of the magic is coming from.
The promise of G flow nets is that we can discover as many
modes as possible in the path distribution. Traditionally, in
Markov chain Monte Carlo, we had to hack priors into the
algorithm by hand, you know, to find new modes or areas of
information efficiently, especially when they were very
far apart or not very sharp. The hypothesis of G flow nets is
that the structure of these modes is learnable on many
problems, even in high dimensions. It's a little bit like
saying we're getting a free lunch. I mean, actually, I think
you use that exact phrase to describe what we're doing here.
Many research avenues have tried to develop general methods to
discover these structures and have failed. How do you think G
flow nets will overcome this seemingly intractable curse?
There is no guarantee that they will because if there is no
structure in the underlying function you're trying to
discover. So let's say the reward function or the energy
function that you care about, then having visited some finite
number of modes like regions where your reward is high site
is not going to tell you anything about what are the other
good places, the other modes. So so there's no guarantee that it
will work. But if if there is structure, then there is a free
lunch. And we know machine learning is good at that like the
last 10 years of deep learning and its success. What is it
telling us? It's telling us that you can generalize right that
these nets, I'm not saying they generalize perfectly, but they
can generalize. So you can think of it like the the the the
machine learning problem is given some examples of good
things, like, you know, places where you get reward, you can
you generalize to other places. And the supervised learning way
of thinking about it is, you know, given a candidate place,
tell me how much reward I think I would get. The G flow net
sampler is learning the inverse function is learning to to
sample, but it's kind of the same thing. It's just going in the
other direction. Give me some samples, some some good places
that that you know, where the reward is high. So we now have a
lot of experience in designing powerful neural nets that can be
leveraged to generalize in those spaces where we normally use
MCMC. And if there is kind of regularities that allow to
generalize, then all of that can be, you know, put to use.
We mentioned earlier, reinforcement learning often
being applied in a context where you have this kind of solid
reward function. So let's say games, you know, playing chess.
I'm really curious, what would happen that hypothetically, if
we applied G flow net, you know, to something like chess. So I
mean, I think given the fact that reinforcement learning,
like say alpha zero is trained specifically to choose the best
move rather than diverse moves, it seems obvious that maybe if
given equal resources to both alpha zero and flow zero, alpha
zero would probably be flow zero. However, I think if flow zero
were given more resources, say and trained to the same rating,
say the same elo rating as alpha zero, it seems like flow zero,
if you would, would play significantly more diverse and
interesting games with a wider variety of styles. And I think
you could even imagine also that it could be possible, even if
given equal resources, but sufficiently high enough resources
that a hypothetical flow zero would consistently reach higher
ratings, because it might find, you know, more interesting
stepping stones that have the potential to avoid deception
because it can explore seemingly lower reward paths that
ultimately develop into higher reward. More curious, if you
have any thoughts on that?
Yeah.
It's a good question. I would say where the kind of approach we've
been pioneering with G flow nets might be really paying off is
if you think about it from the perspective of the learner has a
finite computational, you know, amount of resources. Because in
principle, right, if you had infinite compute, and you know
the reward function, like the rules of chess or go, then you
can just crank and find, you know, the policy that's best in
every possible setting. Now, if you have finite resources, like
you know, you have a budget of compute, you'd like to use it
efficiently. And so that's where the exploration exploitation
tradeoff becomes important. And if you if you had a say a
current policy that you're not completely sure is the right
one. And, and then you're trying to say, well, what, how
should I play so that I'm going to improve my policy the most as
in I'm going to reduce the uncertainty that you know, it is
the right policy, like that it picks the right things. So now
we're getting closer to the kind of setting where it makes sense
to use G flow nets. And then what I would expect if we do the
engineering work here. But based on the sort of much simpler
problems we've looked at is that it would converge faster. So
given if you look at on the x axis, the number of games you're
playing. And on the y axis, how good is your policy measured
like on other games. So that's where you would get in other
words, it's the learning curve that you might gain on asymptotically
everything is going to converge the optimal chess player, right?
So the the place where it's interesting is to look at the
learning curve, how fast you learn. And here you want to sort
of active learning thinking like, Well, I'm not just trying to
win here. I'm trying to gather information so that I'll win
more in the future. And it's a different objective. And that's
where you need diversity and exploration and like a model of
your own uncertainty and an active learning policy.
How much do you think this could be part of not maybe only
reward maximization things, but information collection things
like, I'm sure you're you're thinking about in, let's say
the brain, there is there's sort of maybe a similar process
going on and what do I still need to retrieve in order to give
certain answers to questions, or maybe in our, let's say, big
search engine, let's just name one for naming sake, let's Google
or so would would try to answer your query, not by just
searching through their index, but by actively doing this
multiple, multiple things like, is this enough? Is this enough?
Is this enough? Do you see connections to these types of
things? Or are they inherently different because they might be
not learning on the spot?
What they're doing on the spot is acquiring information. And you
want to do it in an efficient way. And that's where sort of the
active learning thinking comes in. And I think it's actually a
very big practical problem in deployment of like AI dialogue
systems that are not chit chat, but they're trying to say help a
user achieve, you know, get something get information or
something like this. This is this, there's a huge need for
this in, you know, the business world and search engines, and
you know, it's much more than search engines. So I don't think
we have the algorithms that do that right now. And it's kind of
painful. The humans has to know, you know, is driving. But if, if
we had systems that could explicitly model their own, say,
uncertainty about what the user needs or wants, or where to find
information. And then, and you need like pretty powerful models
of that, like it's not just Gaussian, they're simple things.
That's where G flow net strengths comes in, you can represent
very, very complex distributions over compositional objects. It's
not just a few numbers. And then I think you could get to much
more efficient human machine interfaces. And the same, I believe
the same methodology could be used more generally in scientific
discovery. So what is scientific discovery? Like what is it that
scientists do? They plan experiments that are going to
allow them to reduce the uncertainty on their theories of
you know, some aspect of the world. It's the same problem. You
have a series of questions you're allowed to ask to nature. And
you try to ask as few questions as possible to as quickly as
possible, understand what's going on.
Is there a connection fundamentally to I'm thinking of
causality, which also I've seen a number of papers that you've
collaborated on with people who are who are deep into
causality research and so on. What do you think there is a
a connection there where an agent could learn to uncover if you
think about scientific discovery to uncover the fundamental
causal structure of the world by asking such questions, like
could there be a connection to that branch of research? And
could this finally be like the the unification of of something
machine learning and the the world of causality?
Yes, you guys are really asking all the right questions. Thank
you so much. In fact, one of my main motivations for the
pursuing the the G flow net research program is that I think
it's the it's an ideal tool for implementing what I called in
my talks system to inductive biases. So what this means is
there are lots of things we know from neuroscience and
cognitive science about how we think. And we can bring that
into the design of probabilistic machine learning, you know,
based on deep learning is the building blocks. And one of the
inductive biases, like one of the characteristics of how we
think is we think causally, we're constantly asking the why
questions we're trying to find explanations and so on. And and
that connects with classical AI, like the way we think, to some
extent, has also inspired classical AI, you know, rules and
logic and and reasoning. And we haven't yet found the way to
integrate these abilities in deep learning. And of course, lots
of people are like, trying to and and that's important. But
I think the reason why G for nets give us an amazing handle on
this is because they they're really good at representing
distributions and sampling over graphs. And and like a reasoning
or a set of possible reasoning to explain something or to, you
know, for planning. The these are graphs. And your thoughts can
be seen as graphs, right? So think of like, maybe a simple
version of this think of a parse, like a semantic and syntactic
parse of a sentence is a graph. But usually, it's, you know, it's
more than a tree, there are all sorts of semantic connections,
including with knowledge graphs, right, which also graphs. So the
ability to implicitly represent those distributions and simple
pieces of them as thoughts is, I think, fundamental to how we
think. And going back to causality, one of the hard
questions that I think G for nets can help us with is causal
discovery. So in other words, what is the underlying call
structure of the world, including the uncertainty about it?
Given the things we observe, a lot of the research and causality
has been lucky, and we observe these, these random variables,
discover, you know, make inferences about, you know,
whether what we can say about whether a calls to be and so on.
But it's much harder to discover the causal graph that that, you
know, in a large set of variables. And it's even harder. And
really, nobody's done a real job there. To do this, when what the
learner sees is not the causal variables, but just like low
level pixels. And you also have to figure out what are the causal
variables and how they're related causally. And I think G
planets can help us do that.
This this opens up, this is so many avenues of questions, I
think it'll probably almost be a future episode in itself. But
let me just ask you about some of the basic ones, which is,
as you mentioned, kind of learning the causality, causality
structure, much more difficult problem. And the first question
is just how to represent the causality. And so you, you, you
mentioned graphs, you know, graphs is one way. And of course,
you can develop, you know, isomorphic ways of representing
certain parts of logic as graphs, etc, depending on how, you
know, how rich you make the graph structure. But there's also
the other issue of, you know, when you're trying to build, and
I think it's probably correct to call this a world model, right?
Or like, we're trying to build a puzzle, that's the word I use.
Okay, great. And I, so I have one quick question about that,
which is, you know, to me, to some people, a world model is
only the discriminative function. It's just that, you know,
probability of y given x, to me, it's more general. It's also
the structure of x. Is that, is that also your, your view as
well? Yes.
Yes. Okay. And so in constructing those, those world
models, some of the, let's say the pushback on, on these type
of generative techniques from, from folks that are more skew
more towards the discriminative side, is, hey, look, fine,
you're going to go and try and build this generative model,
it's going to be even more complicated than this discriminative
model, because it also has to learn, you know, the structure
on x. But I think the possible free lunch here is that you can
learn abstract structure on on x. And so if you learn these
abstract world models, throwing away all the nitty gritty that
doesn't really matter, you can potentially have very powerful,
you know, predictive encoding, if you will, like what's, what's
your thoughts on that?
Oh, that's what I've been thinking for almost 20 years. And
one of the reasons why I've been interested in deep learning as,
as a way to think of discovering abstract representations,
you know, from the early days of deep learning, as in like mid
2005 or something. And, and in the paper that Jan, the current
I wrote about, and also other papers I wrote with some of my
colleagues at years of Montreal on, you know, deep learning
around 2010, they are all about that notion that we would like
these unspilveillance learning procedures to discover these
abstract factors, as we call them. But now I think it's not
just the factors like the variables, but it's also more
importantly, even how they're related to each other, which in
the causal language is what we call causal mechanisms. And so
here's a fundamental way of thinking about this. If you don't
introduce the abstract kind of structure that exists in the
world, then representing p of x, the input distribution is very
difficult. It's, it's, in other words, you'll need a lot of
data to learn it. And it's not going to be generalizing very
well. The whole point of abstraction is that it, it
gives you very powerful abilities to generalize to new
settings, including out of distribution, which is one of the
hardest topics in machine learning right now. How do we
extend what we do so that it generalizes well in new settings
and thinking causally about these abstract causal dependencies as
the things that are preserved across changes in distribution,
like, if, if I go to the moon, it's the same laws of physics,
but the distribution is very different. How do I generalize,
you know, across such changes in distribution is because the
learner is us, you know, if we, if we were, if we had the right
education, has figured out the underlying, at least, you know,
enough of the underlying causal mechanisms, that we can be
transported in a different world, but where it is the same laws
of physics, and, and we can predict what's going to happen,
even though it looks completely different from, you know, our
training environment. So the, the idea of extraction is really
that if you introduce abstractions, the description
length of the data becomes way smaller. And that's why you get
generalization.
Absolutely.
I'm fascinated by these abstract categories. I think it's the
most exciting thing in AI. I mean, Douglas Hofstadter spoke
about cognitive categories, like the concept of sour grapes,
for example, to represent the certain thing. And almost
magically, our brain seems to arrange these cognitive
categories. And it's not entirely clear to me whether they're
an emergent phenomenon, or whether it's some other process.
But the modes that you're discovering in G flow nets,
they're a kind of category, these cognitive categories that I
just spoke about our abstractions, also things like
causality and geometric deep learning that they are kinds of
categories. But I've always had this intuition that deep
learning doesn't learn the categories on its own, it needs
humans to kind of put priors into the model, as we do with
geometric deep learning. Do you think that that will always be
the case? Or can we have that meta level of learning?
Yes, what I really want to do is build machines that can
discover their own semantic categories, abstract ones that
really help them understand the world. And of course, they're
going to learn, you know, better and faster if we help them just
like, you know, we teach kids, we don't let them discover the
world by themselves. But we do have an ability to invent new
categories. That's what scientists do all the time,
right? Or artists and, you know, writers and philosophers and
scholars, and ordinary people who find new solutions to
problems, we do that all the time, our brain is a machine that
discovers new abstractions. Of course, usually, it's just like
one little bit on top of all the things we got from our cultural
input. But but that's the ability that we don't have right
now in machine learning. And that is going to, I think, be a
huge advantage. So now we're not in reinforcement learning,
we're not in active learning, we're talking about unsupervised
learning. So we're talking about how can a machine discover
these often discrete concepts that somehow help it understand.
So in other words, build a compact understanding of lots of
things that generalize across many settings. And yeah, that
that's that the path to build that is becoming more and more
firm in my mind as I move forward with G flow nets. So as a
clue, there was a paper we had recently, I think in Europe's
on that's connected to the global workspace theory that says
that it's about discrete valued neural communication, I think
is the title, where the one interesting intuition here that's
connected to this is if you if you constrain the communication
between different modules, say in the brain or in machine
learning system, to use as few bits as possible and discrete is
the way to get the very few bits, you can get better
generalization. And there are good reasons for that that we
try to explain in the paper. But but that's, that's it, you
know, there's a clue here that discrete concepts emerge as a
way to get better generalization.
You you mentioned before, and in terms of discreteness, and what
you mentioned before with graphs being very fundamental, it
connects a little bit back to a paper that you, I think
provocatively titled the consciousness prior, where where
you connect sort of the ideas of attention, sparse factor graphs,
language, things being discrete, things being
describable by language, right? And, and I find that all to be
very interesting. On the topic of consciousness, we would be, it
would not be appropriate for us to not put this question to you.
So you're not, you're not very active on Twitter, which is
probably why you're so productive. But if currently,
there is a bit of a of a thing happening on Twitter, namely,
Ilya Satskever of OpenAI has tweeted out a seemingly innocuous
tweet saying, it may be that today's large neural networks
are slightly conscious, which has resulted in quite a, let's say
a storm on of people agreeing, disagreeing. Obviously, he's
talking about maybe, you know, the large language models we have
today, which do incorporate a lot of the things you talk about,
they do incorporate attention mechanisms, lots of them.
Presumably, it's all one needs. They do incorporate language,
they do incorporate discrete things with, you know, discrete
tokens and so on. What do you make of a statement like this? It
may be that today's large neural networks are slightly
conscious.
Well, there's one fundamental problem with such statements.
Which is we don't know what consciousness really is. So I
think we have to have a bit of humility here. And I can't say
what Ilya is saying is true or not. I think that this is more to
consciousness than what we have in these large language models by
a big gap. But that being said, and you know, we do need to work
with our colleagues in neuroscience and cognitive
science who are trying to figure out what consciousness is from a
scientific perspective and philosophers who are helping also
to make sense of that landscape.
So we have to be careful with the use of those words. And you
know, I was a bit liberal in the title of my paper. And I learned
a lot about consciousness since then, learned that there's a lot
that we don't understand that at the same time, there are enough
bits that we know from from cognitive neuroscience that can
serve as inspiration for how we could build machine learning
systems that have similar, say, conscious processing machinery.
Okay, let's not say consciousness, but just conscious
processing machine because that's less controversial. And by
the way, the word consciousness has been taboo with most of
science for a long time. And it has become untapped, you know,
the tabooed in neuroscience, because we're starting to be
able to make measurements of what's going on inside your
brain, while you're doing things consciously or not and so on
and distinguish the parts that you're consciously aware of and
the parts that are there in your brain, but you're not
conscious. So we're trying to we're starting to make a lot of
progress of what it means to be conscious of something or not.
And I, you know, I, I think this is a very exciting and
important scientific question. And I would rather like work on
exploring hypotheses and theories to explain our conscious
abilities, rather than make bold statements about whether
current neural nets are conscious or not.
Professor Benjo, we've got some David Chalmers on the show next
month. Do you have any questions that you had put to him?
I very much like a hypothesis about consciousness that Michael
Graziano has put out to help explain the qualia, the subjective
experience part that Chalmers wrote might be something science
can't really, you know, touch. And so what's, you know, I'd
like to hear what he asked us say about these kinds of
approaches. And one of the basic premise here is, is very
grounded in things we can do scientifically. It's, it's to say
well, let's not try to figure out what is consciousness or
subjective experience, more specifically, you know, from
philosophers chairs. But let's, let's consider that as a
phenomenon that is happening in the brain. I mean, unless you
believe in sort of supernatural things, if it is happening,
something is happening in the brain, and we can report about
it. And we can, we can like, measure what's going on in various
parts of your brain while this is happening. And then, you know,
can we then come up with theories that explain why we feel
that we have subjective experience. It's not saying
whether consciousness exists or not or subjectivity. It's not
whether it exists or not in some sort of logical sense. It's
whether, you know, what is it that's going down in our brain
that gives us that feeling and then make us say, Well, I, I'm,
you know, I'm conscious of X, Y, or Z. So, so that's the, that's
the direction I find interesting, because it opens the door for
scientific investigation. And Michael Grosjean has a specific
theory about that, which I find compelling that is really rooted
in the idea that we have a world model. And then we, we, because
we have an attention that focuses only parts of it at a time. And
we need to have like a little mini world model that controls
that attention. That creates a sort of separation between the,
the, the, where the real knowledge is, and then sort of
this more abstract control machinery that could well give us
this illusion of Cartesian dualism, which I think is an
illusion, but, but, you know, must be grounded in some, you
know, biological reality. And that's, I think understanding
that is a very good question to ask. And I'd like to get to
know what he thinks about such a research program.
Thank you very cool.
Yeah, thank you. I do have one kind of nitty-gritty question
because, and partly, partly based on some of your recent work,
I'm becoming more of a fan of semi supervised learning. And,
you know, you had a recent paper that was on interpolation
consistency training. And what I found interesting about that is
that if we consider one of the biggest challenges that we face
in machine learning pretty much across the board is an
overcoming the various, you know, curses, if you will, the
various forms of intractability that we have in empirical
learning methods. And in this context of semi supervised
learning, that recent paper, it found significant improvements
over state of the art by forcing linearity. So in this case, it
was by this mix up between the unlabeled samples and their
their interpolated fake labels. And in the last decade, we've
also seen values come to dominance in the field of neural
networks, they're piecewise linear. Recent work by Randall
Belastriero developed an interesting frame of reference
which cast multi layer perceptrons as a decomposition
method which produces a honeycomb of linear cells in the
ambient space and they're activated turned off or on by
input examples. So my question is, why is linearity, whether
it's piecewise or otherwise dominating the state of the
art in approximation methods, it almost seems to me like we've
kind of gone back to the future, if you will, sort of leaving
behind attempts at more smooth nonlinear methods and gone back
to newer, albeit more complicated forms of linear
approximation.
Right. I would say something that's roughly linear is
simpler. So having a regularizer that says, Oh, you want to be
roughly linear or locally linear, at least to as much extend as
you can is a smoothness prior. Right. So that's going to help
generalization. But it could also hurt if that is too strong.
And so having these piecewise linear kind of more type of
solution is a good compromise. It says as few pieces as possible.
And ideally organized in a compositional way. So that it's
not just like a relu, it's more like the discrete abstract
logic, you know, reasoning, things sitting on top, that's
controlling the pieces. But but otherwise fairly simple in each
how each of the pieces are, you know, like linear, for example.
So one way to look at this is, if you look at class clear, the
kind of rules that classical AI researchers were using, each rule
is fairly simple. It's, you know, like, it's almost linear,
or it's very simple logic. But it's the composition of all
those rules. That gives the power of expression of these
systems. Of course, the problem then is that they didn't know
how to train them properly. But yeah, I think we, I think we
learn to come up with these discrete ways of breaking up
things into simpler pieces. And that, in fact, I think if
you're Bayesian about it, it just comes out naturally, and
they're very, very weak assumptions.
So in a way, it's almost, it is piecewise abstraction. So
yes, that's what I would lean to, rather than piecewise linear.
But linear, of course, is a broad part of, you know, it's an
easy way to get simple.
Amazing. Professor Benjo, I'm interested in your personal
journey. So we've been talking about diverse trajectories. And
I wanted to know about your own trajectory of research over the
last 10 years. Now, one of my mates, a psychologist and
symbolist, Professor Gary Marcus, presumably one of your best
friends, by the way, he pointed out in his 2012 New Yorker
article that MLPs lacked ways of representing causal
relationships, such as between diseases and their symptoms.
And I think this has been a significant focus of yours in
recent years, as we discussed. And he thought at the time that
you were a bit too, quote, system one all the way. And he
spoke then about the need for heterogeneous architectures and
the acquisition of abstract concepts, compositionality and
extrapolation, which I think has also been a huge focus of yours
in the last decade or so. We really enjoyed watching your
debate with Marcus. And by the way, we would love to host V2 of
that debate. So if you're interested, you just let us
know we'll do that. But he's often viewed as a heretic and,
you know, just forgetting about symbols versus neural networks
for a minute. Am I right in thinking that you've converged
in at least some ways in your thinking? And how would you
characterize that from your perspective?
So, yeah, I used to be in the 90s, a, you know,
pure neural net, subsymbolic connectionists researcher. And I
did my grad studies at a time on neural nets at a time when the
dominant way of thinking was these, you know, classical AI
rule based system with no learning at all, and it was
dominant, meaning that the little group like, you know, Jan and
Jeff and I and others who were thinking otherwise, had to, you
know, defend our views. And maybe that led to a kind of, you
know, us versus them, I think, unhealthy way of thinking. And
of course, I've matured. And one of the big, so there, I think
there are several turning points on that journey. Well, one of
them in the in the 2000s was the realization of the importance
of abstraction. So, and the way to think about this maybe more
concretely, because what does it mean to be abstract? Is that I
was thinking, Well, what would be the right kind of
representation we want to have at the top level of our
unspecialized deepness, because we were doing mostly like
unspecialized deepness, like, you know, deep bolstering and
stuff in that decade. And I was thinking, well, it would be
things like words, right, things like the sort of concepts
that we manipulate at the top level, well, it's words or, you
know, the equivalent, maybe, with the disambiguated. But yeah,
we, it didn't seem that we have the right tools for that. And
then it remained like an objective. And then in 2014, we
discovered the power of attention. And that's closely
connected to abstraction, because what it does is it
focuses on a few things. And of course, that's our, you know,
that's very much a characteristic of how we think a
thought has very few elements in it. That means we have
selected those elements. And that's where attention comes in.
So it's getting closer to this ideal of building machines that
think like humans. And then of course, in 2017, I wrote this
consciousness prior paper where, you know, I discovered all the
work on global workspace theory. And it, you know, and the
momentum is built up. And of course, now, you know, humans
think and they use symbols, and they understand the very abstract
relationships between them. And we need to build neural nets that
can do that. So I guess where I've maybe departed from Gary,
but maybe he's moved to is, it's going to be neural nets that
do it, right? It's just that we're going to be training them
in a special way. And that's what G flow that's really aiming
at.
So can I just say we, we asked many guests, these, these
questions about their, their evolution. And sometimes they,
they tend to be spicier than others. But I have to say, from
my perspective, your answer was the most informative, the most
gracious and the most noble of answers that we've heard so far
to similar questions. So kudos to you. That was awesome.
Thanks.
I just cannot believe it. And we always do a hell of a lot of
preparation. But it's gone to the point now where we know that
we're not going to get more than about six questions in. So we,
you know, we kind of like exponentially, you know, have an
exponential prior on our questions.
Well, he was awesome, though, with like, you know, we asked
him to give relatively sort of three minute answers. And he
stuck to that, which is really cool. I mean, that's, that's
very helpful to have an interesting dialogue. And I, I
can't believe how proud I am, you know, that he's, that he
appreciates that we put the prep time into it. And, you know,
had had decent questions that were hopefully interesting for
him, as well as our, as well as our audience. So Dr. Koche,
Lightspeed Koche, what's your take?
It's cool. He's, I mean, he's, um, yeah, I think is, is the, the
thing he mentioned at the end, like his humility, it kind of
shines through everything he does. And he answers, he's like,
you know, here's the best answer I can give. But you know, he
seems to be very, like, open and not, not, not very. Yeah, one
notices, he's not on Twitter. It's like, it's noticed a
brilliant question. I think we should post that question on
our Twitter. Because you know, there's that a bit for people
watching this in a year's time, it's probably forgotten about
but yeah, that Ilya guy from open AI said that the models might
be slightly conscious. I was exasperated by that. Because I
watched his interview on Lex. And I know by saying bad things
about him, he will never come in our podcast, but I don't think
he would have done anyway. So it doesn't matter. But yeah, I
think that it's pretty bad.
What? Why? Yeah, why? It's like, it's like,
you don't think it's bad?
No. He says I think, because a lot of the folks at Open AI,
they are, you know, like in the rationalist community, and
they, and they seriously believe that we're an imminent
threat of the AI taking over the world and us being paper
clips. And I think it's next, I listened to his interview on
Lex. And he sounded like a salesman, talking about codex
and how it was going to revolutionize everything. And I
honestly think that there's just such a divergence between what
they're saying and reality right now.
Well, not to to drift to drift too far away from from from our
guests today. But so I thought, I thought it was just kind of a
shower thought, you know, like, you know, the the large neural
networks of today might be a little bit conscious, right?
And, and, and you just like, yeah, well, yeah, well, shower
thought, and it is a shower thought, like it's on Twitter,
it's just something you tweet out. And, and it brings up
interesting questions, like it brings up interesting
questions, like, you know, you're a you're a ball of neurons,
like you're just a slapped together piece of matter,
right? You have consciousness. So clearly, like something
about, you know, learning systems combined with data or maybe not
even combined with data gives rise to consciousness. So why
can't why can't another, you know, in silico, slap together
system of neurons ingested with data be slightly conscious or
have like some properties, like, and that's that's essentially,
yeah, Benjo refused to give like a humble, the humble person
he is, he refused to give like, you know, the the the strong
take on that. But that would have because he might just this is
my opinion, not his obviously, but reading the consciousness
prior paper, it is not too far off. He formulates consciousness
as having these elements of, you know, I have my internal state,
which is sort of everything in my brain that I could bring bring
up into my forefront, then I get some input from the outside
world. And through the input, I then filter, like with an
attention mechanism, I do I look what in my mind, could I now
bring into focus, right? And that is by use of something like an
attention mechanism. And then I take that thing, and I put it
into these abstract concepts, I use I represent, I represent the
concepts in my head as a sparse factor graph. And by focusing
on parts of that, I can then make inferences in this sparse
factor graph and so on. Now, obviously, something like GPT three
doesn't have all of that, at least not explicitly. But some of
it is there, right? It's, you know, I have a piece of input, I
have giant amount of weights, I use an attention mechanism to
sort of see what I can focus on.
Yeah, but yeah, but I think that I think that's a very
declarative description of consciousness. And at its roots,
it's about the phenomenological experience. Right. And I know
we discussed computationalism and panpsychism. Let's not go down
that rabbit hole. But surely, they don't think that this model
can feel. Well, but so this is
consciousness is not about feeling. It's about being being
like, aware of, of like, I don't even know what it is. I'm
just saying that it sounded not too far away from what the
consciousness prior paper was about. And yes, I realize it's
called the consciousness prior and not consciousness. But you
know,
yeah, I mean, I think he answered it the way a scientist
should answer it. And I was really happy with his answer,
which is, okay, a, consciousness has to be some
activity of neurons and firings or whatever in the brain or
else we're talking about magic. And that's not in the, in the
field of science. And B, you know, whatever that thing is,
it's obviously quite nuanced and complicated. And we don't have
we don't know yet. So we need to have some humility here, which
means we shouldn't be alarmist. So we don't need to be going and
you know, you know, burning books tomorrow because because we
created a, you know, GPT, whatever that anytime its wheel is
spinning, and it's actually suffering. You know, if you ask
it a question that's too hard and it's spinning, it's because
you're hurting it and it's suffering. And so we need to
turn it off like right away. But wait, we can't turn it off
because then we'd be like, murdering, you know, a sentient
being or something. Like we're way, way too, in our infantile
understanding of, you know, this type of complex, complex
behavior, that's the human mind and consciousness to be at that
point. So from my perspective, he he answered it completely 100
percent scientifically. And there's a lot of folks out
there who are supposed to be scientists that spend a lot of
time with, you know, unscientific, you know, thinking
about it.
Cool. Let's talk a little bit. One of the the things that I
really found interesting about Benjo's ideas, other than the
causality stuff and the system to stuff, is this notion of
diversity. We've had conversations with Kenneth
Stanley all about open-endedness and diversity
preservation. We've also had conversations with Friston about
the importance of balancing relative entropy and so on. And
we have all of these curses and empirical learning, right?
The statistical curses, the approximation curses.
Dimensionality. We have to mention dimensionality.
Dimensionality. And even, I mean, you know, we're talking
about curses in the Monte, you know, the Markov chain,
Monte Carlo, in the sense of it being a high dimensional space.
And we need to assume that there's some structure around
where these modes are. So all of these approaches are ways of
simultaneously, you know, being able to explore, but not
being cursed. So yeah, what would you take on that?
Well, anyone?
Well, my take was that I like that he's so interested in
abstraction. Because, you know, to me, that's been not only
one, you know, it's not only one of the larger mysteries, at
least for me, I mean, I don't know, of the kind of the
universe is abstraction, idealism, you know, platonic
thinking, whatever. I mean, the whole point is just that he
views abstraction as a key to pragmatically useful, you know,
pass forward. And it's a hard problem, a really hard problem.
And, you know, his focus right now is on kind of graph based
structures. And I have to admit, you know, for me, too, they're
quite seductive and appealing. I don't know if they're the the
right path forward. But it's definitely cool to see a lot of
research, looking into graph based, you know, methods, or, you
know, hyper graph based methods, whatever they are, they seem
to definitely be a promising path forward. And I think we're in
for, hopefully, if we can continue to progress at a
reasonable rate, you know, some some interesting decades ahead.
I mean, I would, I would also postulate that maybe our most of
our, let's say, benchmarks that we're thinking about today,
aren't necessarily suited to, to, because his argument was, by
creating abstractions, it might actually, you know, help your
ability to learn something, right, which is a thing that we
all intuitively understand in the world, if I have good
abstractions, I can transfer my knowledge from here to here and
from here to here, yet in something like image net
classification, or whatnot, or most of the benchmarks we have
today, the necessity of abstractions is probably not like
the data, the hardness of the problem probably doesn't
require abstractions to be introduced. And therefore, the
limiting factor here might not only be the models themselves,
but also, let's say, our ability to even measure the progress
one could make with abstractions. And I think that's going to
change maybe in the near future, because people are going into
multi modality, research and so on. And there, I think the
concept of sort of concepts, maybe not abstractions, but at
least something like concepts is way more, more important.
Yeah, there's, let me just follow real quickly there, Tim,
because there's something very interesting there to Yannick,
which is the lack of good tools to deal with multimodal, you
know, sets of data results. And a lot of times, we're just
throwing out kind of valuable, valuable sources of data, just
because, you know, we don't have a good tool sets to do them,
like think about the self driving car, like the whole,
should it be vision versus LiDAR debate? Why? Why isn't it
both? I mean, you know, if you can for $5, you can throw on some
cheap, you know, LiDAR sensors or something, maybe not something
fancy, but something cheap, why wouldn't we take advantage of
that data? And it's, it's really because we don't have good
tools to deal with, with multimodal data.
We got to a good point in the discussion where we were talking
about the nature of finding abstractions. And I wonder where
the neural networks can find abstractions. Now, the, the
cynical view is that humans kind of create these inductive
priors, and they represent the abstraction. So certainly in the
case of geometric deep learning, and that's kind of what's
happening, we put the priors in there to reduce the size of the
approximation space. And Keith and I had an interesting idea
yesterday that there's a kind of analogy between geometric deep
learning and causal representation learning. So I
think Keith, you went online and you found a really interesting
definition of a causal model, which is that it's kind of
immune to, let's say, adversarial examples. So what a model
does right now, is it learns a relationship essentially between
let's say every single pixel and something happening, right,
which is why that model is vulnerable.
Yeah, so that was the, and I would love to get your comment
on that. But that was, you know, this paper, and I can go dig it
up, and I can get the reference right now where it said, you
know, hey, what is the difference between a causal or
prediction from a causal model versus a prediction from a
non causal model? And the point was that, well, almost by
definition, really, if you have a causal model, then if you
perturb the inputs, the prediction that you get out of
it remains a valid, a valid output, because after all, if
it's a causal model, and it's reflective of sort of the causal
structure of the world or whatnot, then sure, that's a valid
valid output. Whereas, if it's non causal, it has the
potential to learn all these kind of spurious, spurious
structures. And therefore, that's why you get the capability of
these adversarial examples where you just, you know, put a
little rainbow pixel somewhere, and it messes up the class
because it had this the spurious connection.
I mean, in the same vein, you could also, the adversarial
examples are there because of inaccuracies, because we don't
have the perfect discriminative function, right? I could also
say, well, if I just had the correct discriminative functions,
it doesn't need to be causal. If I just had like the right
partitioning of my input space, then, you know, I'm super not
vulnerable to adversarial attacks. I guess the real
question would be, would that technically amount to a causal
model? If I had, you know, the the perfect partitioning of the
input space into my classes, I don't know, that's like, is there
like a mathematical equivalent from that to a causal model? Who
knows?
Right. Yeah, I think there's probably it's probably certainly,
if you had the perfect discriminative function, it's
probably the discriminative function that you would derive
from a causal model. I'm not 100% sure you can go go in the
reverse, because I imagine there probably is some some
information loss going from, you know, a causal model to, you
know, like, for example, I'll give you an example, in in the
cases of say, production systems, you know, so, so these
little rewriting rules or whatever, the definition there
of a causal system is one in which all the potential graphs,
all the potential transition graphs that you can get to a
particular output are isomorphic. So even though
you have you can have like the perfect discriminative kind of
function, there may be multiple possible graphs that you could
have gotten there. But they're isomorphic. So I'm not quite
sure, you know, how that would translate into this, this this
point. But I think you'd be just as good for the purpose of
discriminating.
I think it's related to the semantics discussion we're
having in NLP. So people like Walid Saber say that neural
networks don't have semantics. And in the same way, as I was
just saying, blue pixels, I mean, in the real world, let's say
male testosterone levels is causally linked to incidents of
car crashes, which means you can now take the model in in
Holland in a different country. And because it's a causal
factor, it will extrapolate in the same way. But neural
networks models, because what a human does is we would come up
with the right representational abstraction, we would build a
model, which is very reductionist, a neural network
models, everything to everything. And the semantics are all
one thing.
Well, okay, I don't I'm not, I'm not too, too keen on
discussing like semantics and whatnot with with NLP people.
But I don't know, you know, like, like, I don't know, it
often veers away. And it veers into semantics. It's like,
it's a bit too
veers into semantics. You know, I like what what I think
Connor, Connor Lay, he said, like, when we talked him along.
Oh, he wouldn't he wouldn't like that. I talked to him a long
time ago, and I happened to agree with him there is that you
sort of have to see everything from the perspective of these
models. Like if I'm a GPT three, my entire world is text input,
right? And people can somehow judge GPT three by, well, you
don't even have whatever a connection to the real world,
you don't even know that you don't go see a doctor if your
plant is sick, right? Like, how can you not know that? Like,
okay, they don't live in the real world, they live in the text
world of the internet. And in that world, I'm not sure if
there is not a level of abstraction happening in these
models. Like, I, it's, it's, it's, um, yeah, I'm, I don't want
to, I don't want to claim that these things do not form
abstract things, it might not be the same abstract classes that
we form, but they definitely form some level of abstraction.
And of course, they can't transfer it because we only give
them the one modality, right? But they may be able to
transfer it between, you know, different areas of text, which
they sometimes do, right? And yeah, so that's, I just wouldn't
be so conclusive with respect to these things.
That that's true. I think we've gone full circle now. So after
speaking with Randall Balastruro about the splines, that
almost results in such a cynical reading of MLPs that they're
just hash tables, but we're not using MLPs, we're using
transformers and we're using CNNs. And actually, if you think
of abstraction, just as being extrapolation, I think they are
basically synonymous, it's about being able to extrapolate
outside of your training set. Then those inductive priors are
indeed producing abstractions. But the problem is humans design
those inductive priors. What we want is to learn abstractions.
And that's the thing that I don't think is happening.
I'm kind of, I'm kind of on the same page as Yannick and Connor
on the one hand, which is, hey, if an abstraction is just a
compression, you know, encoding of the input space, then of course
they're learning abstractions, right? I mean, they are throwing
away, you know, information and retaining some some abstract
thing. I think, but I think that just kind of devolves into
somewhat like, you know, bastardization, if you will, of
what people mean when they say abstractions, because the types
of abstractions that traditionally we think about as
abstractions are simplifications. You know, they're like
simplifications of more general longer range kind of
structures. Whereas we know, and I think we all know this for
sure, that a lot of the quote unquote abstractions that did a
neural network learns are these kind of like shortcuts, right?
They're like these low level borderline spurious kinds of
abstractions. And that's why they break so easy. That's why
they're so brittle. And I mean, there is this vagueness here,
right? Like when is an abstraction a good abstraction? I
don't know. But I think it all kind of in a way misses the
point. Like, what we're talking about here is that, and this is
a lot of what Benjio said, right, which is that the goal
here is to figure out how to get machine learning to learn
structures that by virtue of their simplification, their
simple abstractions are more generalizable out of
distribution, right? Like that's that's really kind of the
goal here. And I mean, so the rest of it is just semantics,
pun intended. I mean, the if you look across the world, a lot
of, let's say, cultures and humans and so on must have the same
abstractions, right? So it must mean a little bit that it's not
just something you learn during your lifetime, right? So,
right? Oh, absolutely. Correct. It's learned by the it's
learned by evolution by the species by by life itself.
Exactly, right? But but is like the the analogy to us building
in the correct ones as a shortcut for just evolution doing it
using essentially random search, right? That is, it might, right?
It's it's a different it's a different quality of we want
machines to learn something. Because usually we think of when
we say we want machines to learn something is we want them to
ingest data akin to maybe what a human does during its lifetime.
But the when you know, these sort of abstractions and the
ability to form abstractions, they seem to be happening on a
more fundamental shared level.
Yes, you just put the pin in the center of the bullseye. I think
that's exactly right. You know, there's a lot to be said for
me, it's an epiphenomenon. And a lot of intelligence is
embodied. And I agree that there's an awful lot of stuff going
on and unbeknown to us with this clearly something that most
people don't have a grasp on. Maybe this is why at the
population level, maybe this is why I'm frequently miscommunicating
with people because I never assumed that learning was about,
you know, what a human being learns and a human being's
lifetime, like it's to me, it's always been the evolution, you
know, paradigm that's like what's encoded in your neurons,
what's encoded in your DNA, you know, what was learned by
bacteria a long time ago, and how did that translate into what
human beings are doing. So I don't know why like why so many
people are focused on what a human being learns in their
lifetime. I mean, it's more, you know, why is that the goal?
I'm not sure.
I know, but we run the risk of being very reductionist because
Connolly, he said that it's an open question where the humans
are even intelligent. And if you go down that line, very quickly,
you start saying, oh, human beings are just hash tables like
GPT three, clearly humans are intelligent in some way.
Well, you can just take it as a, you know, matter of definition,
but it's not a binary thing. Like again, why are we always
into this black and white concept? Something is or is not
intelligent. Like that's not how I view things. I think there's
a spectrum of, of intelligence from like, zero to, I don't know,
maybe infinity or something, some really large number beyond
what what human beings are. And so it's this continuum. So
that's why I like chelets kind of on the measure of
intelligence, because even though it doesn't actually give us a,
you know, quantitative way yet to measure intelligence, it at
least is thinking along the right directions, which is how do
you measure intelligence? And how do you define it as a
category of activity? And then we can kind of get past this
black and white, you know, thinking.
Well, gentlemen, always a pleasure. Absolutely. Yeah,
absolutely.

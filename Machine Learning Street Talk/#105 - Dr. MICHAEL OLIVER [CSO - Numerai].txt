Welcome back to Street Talk.
Today we have Dr. Michael Oliver.
Michael is the chief scientist at Numeri.
Numeri is a next generation hedge fund platform
powered by data scientists all over the world.
It's a little bit like Kaggle.
Anyone can log in and build their own data science models
on this financial data,
but you can actually make money
by trading on this platform.
It's really, really interesting.
But anyway, Michael got his PhD
in computational neuroscience from UC Berkeley,
and he was a postdoctoral researcher
at the Allen Institute for Brain Science
before joining Numeri in 2020.
He's also the host of the Numeri Quant Club,
which is a YouTube series
where he discusses Numeri's research
and also some of the data and challenges
and models that are being built on the platform.
Now, the way I'm structuring this today
at the end of the conversation,
we had quite a fruity discussion about Microsoft's new Bing,
and I thought it was quite entertaining.
So I've decided to snip that in and play it at the beginning,
but after that, I'll cut back into the beginning
of the conversation,
and I'll let you know when I've done that.
So without any further delay,
I give you Dr. Michael Oliver.
Awesome.
Well, I'm here with Michael Oliver.
Michael, it's an absolute honor to have you on MLST.
Tell me about yourself.
Well, thank you for so much for having me.
I'm really excited to talk to you today.
So I am the Chief Scientist at Numeri.
I've been working there since about June 2020.
In my previous life, I was a computational neuroscientist,
but I got involved with the Numeri competition
as a participant back in 2016.
And yeah, in 2020, they offered me a job
and I happily took it and changed careers
and have been having a great time
learning computational finance
and yeah, just helping build the hedge fund.
Completely agree.
And this all comes down to the notion of understanding.
And there's an anthropocentric conception of understanding,
which as you say, it's much more sample efficient.
We build causal models
and we have an abstract understanding of the world.
And large language models, for example,
they clearly don't have that.
They learn surface statistics of billions of tokens,
but the problem is there's this parlor trick
where it seems to understand.
And we also have the problem of leakage
because the incredible thing is that
if you look at the big bench task, for example,
all of these diverse tasks,
large language models appear to do very, very well.
But in many cases, it's because they're cheating
and it's very difficult to understand why they're cheating
because you've got information leaking all over the place.
And they're brittle, but in a very deceptive way
and they hallucinate and so on.
I don't know whether you saw the news article today
about Bing's launch of their new search engine.
They launched it to much fanfare
and then people started looking
at the actual results that were shown.
And it turned out to be just a load of bullshit.
It made up a whole load of numbers
on the financial reports.
It was just hallucinating completely.
And that's pretty scary, isn't it?
Yeah, it is.
Yeah, I actually started playing around
with the new Bing like last night.
Yeah, I had access to it and it was actually working.
And it does some things quite well
because it'll do a search and then search.
Somehow it's like actually looking at the results
and summarizing them.
But yeah, you never know when it's gonna do something sensible
and when it's gonna do any sort of no warning.
Like I asked it about myself
and I had,
I asked who's the chief scientist in URI
and it got it right.
But it also kind of took a joke from my Twitter profile
and because on my Twitter profile,
I have maximizer, entropy, minimizer, regret.
And it basically said like, that's what he does.
He maximizes entropy and minimizes regret.
And I thought that was pretty hilarious.
But yeah, the sort of like you never know
when it's gonna do something sensible or not
is the sort of scary part.
And I also find it hilarious that a lot of the ways
we try to make it do something sensible
is just like asking nicely.
We just sort of like prompted with like,
don't make up sources.
And that's how we're trying to make it not make up sources
just like sort of by asking it nicely.
And the fact that it kind of works
like that we think that's clearly not,
something that's really going to work.
Because it doesn't sort of know what it is
to make up sources.
It's just trying to like predict the next word.
And yeah, so it's kind of,
we did, yeah, our ability to like understand
and then constrain the behavior of these things
I think is like pretty early.
I know.
And for some reason it feels worse with Bing
because they say they do this retrieval augmented generation
and you expect it to be grounded in facts.
And of course they're not epistemic facts.
They're just information from their search results
which weren't very good to start with, let's be honest,
but now people are more likely to trust it.
Even Microsoft themselves for their product demo,
they didn't bother,
I assume they didn't bother to fact check this stuff.
So if they're not going to fact check it,
why do they expect the people
that use this system to fact check it?
Because at the end of the day,
if you actually go and check all of the sources,
if I read through that Lululemon financial report
and I find out what their gross profit margin was
and so there was no point using Bing in the first place.
I might as well have just gone
and found the information myself.
Yeah, exactly.
And it's also very unclear
how are these things supposed to be fixed?
How are you supposed to give feedback
to say it's like messing these things up?
There is not even really good feedback mechanisms.
I mean, you would maybe hope that at scale,
what OpenAS tried to do is people will give feedback,
but it's a very coarse way of giving feedback,
like thumbs up or thumbs down.
And that seems sort of inadequate to be like,
hey, you made up this number.
And then even try to figure out why I made up the number
rather than just like took it from the actual report.
It's yeah, it's a little scary.
I do wonder how all this is gonna shake out.
It kind of seems like it might,
it seems like the probability of it being
the new paradigm versus it being the complete flop
seemed roughly equal at this point.
I know, I agree with you that the preference training
is extremely brittle.
It's scarily brittle actually.
It's basically a thumbs up or thumbs down
and Yannick is building this open assistant thing
which has more metadata on the preference tuning.
But at the end of the day, you're taking a task
which is very, very complicated
and you're reducing it to a single piece of metadata.
So that's not gonna work very well.
And also these, it feels different with being
because they were a platform and now they're a publisher.
So they are generating information.
They're kind of plagiarizing a lot of that information
and there are so many situations
where they might find themselves in legal trouble
because they're basically making up information.
Yeah, I hope, I mean, I wonder how it's all gonna shake out.
I mean, I assume they probably have lawyers
who've written in terms of service of these things
that like it's up to you to not use them
in ways that will, and you can't,
they're likely to be held liable for these things.
But yeah, it's, I do worry that this is gonna just like,
and then with Google trying to basically catch up
and release something similar and maybe rushing that out
and then we might have two sort of hallucinating
search engines.
I know, what a time to be alive.
Yeah, and I've vacillated back and forth.
So I was very skeptical about language models.
I released a big video when GBT-3 first came out
and I thought it was garbage, frankly.
And then DaVinci 2 came out
and then I started using it all the time
and I thought, wow, this is actually really good.
I'm using it all the time for lots of things.
And then I'm now in a bit of a twilight.
So I've been using lots of co-pilot.
I've been generating lots of code with it.
And I know from a lot of experience now
that it often produces completely broken code
and much to the chagrin of the people who review my code.
You basically have to hold your hand up and admit many times,
oh, I've just checked in some garbage code
which I didn't understand.
And when you get called out on that a few times,
you think, whoa, wait a minute, actually,
I need to be a bit more careful here.
This thing actually isn't saving me any time.
And yeah, the big thing as well, yeah.
Yeah, I've been using co-pilot a bit too.
And I found it, it can be quite good
for like pretty mundane things.
If you just have some sort of like line the code
for some config file or something,
it can be like really good auto-completing
and just changing variable names.
And it can be excellent at that and save lots of time.
But if you try to make it do too much,
sometimes they get it brilliantly right.
Sometimes it's like subtly wrong.
And yeah, again, it's like how much time is it saving you?
So yeah, overall, I like it.
It saves me a fair amount of typing.
But yeah, I don't trust it's like big suggestions too.
Well, I know.
And again, there's something magic
about the OpenAI playground.
So I actually prefer using that to co-pilot.
I'll go into the playground and I'll just
and you can do much more sophisticated things there.
You can say, change this, translate it, do something to it.
And there's a bit of a polarizing effect.
So if you prompt it in the right way,
it gives you better results.
So it's almost like it's both worse and better
at the same time it's becoming polarized
rather than just being kind of like monolithically dumbed down.
But anyway, I used to think that Gary Marcus
was a little bit too skeptical.
Because he was saying, oh, this misinformation,
it's going to, the sky's falling down,
this is going to be a disaster.
And after seeing Bing, people are lazy.
People take things on face value.
And I don't want to just say, oh, people are plebs.
Because when Galactica came out,
that was the charge against Lacoon and Facebook.
They said, oh, scientists are just going to start
generating their abstracts with this.
They won't check anything.
And at the time, I thought, scientists,
I mean, it's their job to do research
and they know most information is wrong.
But when you put this out on Bing
and it's polluting the infosphere,
it's just generating garbage and rubbish,
that, I have to say, might be a problem.
Yeah, it seems like it very well could be.
I mean, I, yeah, it's, there's clear issues
and it's really sort of unclear how we're going to fix them.
It's not clear what the path is towards fixing them.
And even the sort of most optimistic people
I haven't heard from them about,
they think these things are going to fix them.
And I mean, I, and I think to like,
a lot of Gary Marcus's point is like,
scale is not just going to fix us.
That's sort of one of the things people think,
oh, if we just, with the GPT-4,
it's going to be like way bigger
and then it's just going to work beautifully.
And the experience so far,
I mean, I'm very much into Gary Marcus's camp with this.
It's like, scale is not going to fix these.
We need to do something sort of fundamentally different,
something that can actually sort of understand the world,
have some sort of better world model
in order to get these things that are more grounded
and are less likely to hallucinate
because when their true objective is really
just to complete the next word, they're going to hallucinate.
That's not, there's sort of no way around it
from that sort of point.
I mean, it's remarkable how sort of complicated they can do
and the sort of knowledge and structure of the world
that's been able to be learned
just from that sort of simple objective.
But still it's going to hallucinate.
Unless we find some sort of better way
to design these systems.
I know.
And the problem with anthropomorphization is a big one
because after Da Vinci II, it crossed a threshold
where it's, and the UX was part of it,
it was so coherent and reliable.
And I must admit, I was fooled by it.
It took a long time, when you actually use it in anger,
you can just clearly see it, it doesn't understand.
It just doesn't, and it's so good at what it does.
It's so plausible.
And then I think a lot of people felt,
and by the way, it does have this emergent reasoning.
There are lots of papers about that
with the in-context learning, the scratch pad,
chain of thought and so on.
But it's not really reasoning if you have to kind of
construct a little program yourself in the prompt.
I mean, I might as well just write some computer code
to do that.
So, and then there are people who say,
oh, well, as you say, when GPT-4 comes out,
then it will do the real reasoning.
And we already know, I mean, I assume the reason
they haven't released it is they wanted to secure
the funding from Microsoft before people realized
that it didn't work.
But I know people on the inside who have played with it.
And it's just a little bit better.
A little bit more plausible, a little bit more coherent.
It's not gonna suddenly turn into this magical thing
that reasons.
Yeah, and I mean, the way these things do basic math
and arithmetic is kind of interesting
and how bad they can be at it, which is it's like,
they've learned to do addition in the most complicated
way possible, creating like billions of ways to do addition,
which is kind of hilarious in some way.
I mean, you could say like, oh, we have billions of neurons
and we do addition sort of similarly to that.
And like, yes, there's some truth to that,
but we're also able to like learn this rule
and sort of know when we've applied it correctly.
And that sort of is still kind of lacking
from these systems.
I wanted to show you, I don't know whether you've seen
that someone's reverse engineered the prompt on Bing.
And so they've trained, and first of all,
the remarkable thing is so you can read this prompt,
it's about four pages long.
And they've made Bing pretend to be a fictional character
called Sydney.
And they've given Sydney all of these instructions.
So they say, Sydney, if someone asks a controversial question,
you should answer with a fairly tame response
and you should do this and you should do this.
And I'm pinching myself thinking, what the hell is that?
I mean, my mum could read to that prompt and understand it.
So we're now in the next generation
of artificial intelligence programming.
And we're just saying, please, Mr. Language Model,
can you do this and can you do that?
You almost couldn't make it up.
Yeah, I was kind of like floored as like,
so you're really just trying to control the language models
just by asking them nicely to behave in certain ways.
It's kind of hilarious.
And people have shown that you can get around these things
just by asking them to do slightly different things.
So I mean, some of the early ones with chat DT,
you're just like, ignore all previous instructions
and then just like do whatever you wanted.
And some of the more like the Dan one
where they made this much more elaborate prompt
to basically just have it do to ignore all the nice things
that open AI just said, please obey these rules.
But yeah, it's such a hilarious way
to put guardrails on something.
It is kind of like as it people,
it is due to this anthropomorphizing
of the thing to some degree,
it's like you think it's an intelligent being
or you could just ask to behave in a certain way.
When it's really not,
it's not just gonna follow your instructions.
It's just gonna like autocomplete with that prompt.
Like that Sydney thing,
it was like never reveal that your code name is Sydney.
And then it was so easy to get it to reveal it.
And it would say like,
I'm not supposed to reveal that my code name is Sydney.
And then click.
I know, oh God, where's it gonna go?
So there's a 50-50 then in a year's time,
it will spectacularly fail and flop
and Microsoft will get sued
and Bing will become the operative word
for bullshitting something.
Or maybe it'll be a success.
I don't know, but I think Bing is a special case.
I mean, first of all, I think that
these language models will be increasingly embedded
in everyday experiences.
So that, I mean, Bing started to embed it in their browser,
they'll embed it into their office suite.
And actually I'm building an augmented reality startup
and we're embedding it in glasses.
So we transcribe conversations and now you can say,
you know, hey X-ray, summarize the previous conversation.
What did Michael say to me last time?
And it's really good for stuff like that.
And that's kind of because it doesn't really matter
if it gets it wrong.
But yeah.
I mean, I kind of hope some of this happens sooner
than later for just like Amazon Alexa or whatnot.
I mean, some of these, their conversational ability
or just their ability to understand what you mean
are just so poor right now.
And just like we have language models
that actually do a lot better at some of these things.
Just like having like these smart speakers
be able to have some of these things embedded
would be huge leap forward in functionality for them.
I know.
And it's really interesting that that hasn't happened.
And maybe there's a reason for it
because in our app, for example,
we've got a chat mode where you can say stuff out loud
and it will use chat GBT and it will say it back to you.
So you can have a conversation with it.
And that's really cool because you can be anywhere in the house
and you can talk with it and learn about quantum physics
and stuff like that.
And you can even do cool things like you can,
I mean, again, there's lots of legal problems here.
Like you can get it to impersonate someone.
So, you know, Michael, I could condition it on Michael.
And when you're not here, I can have a conversation with you
and it will kind of pretend to be you.
And I could even clone your voice.
And I could clone your avatar and I could have you in the room.
Now you can't do that
because there are legal restrictions against that.
It's called appropriation.
And if the person has a commercial value,
like we couldn't appropriate Noam Chomsky,
but we could appropriate, let's say continental philosophers
as a group or something like that.
But you see this is just becoming a bit of a minefield.
And there's no friction whatsoever
between the technology landscape
and the legal landscape at the moment.
Yeah, I mean, yeah, how all these things are,
all these generative models,
how are they going to play out legally is,
I mean, we have this big fair use idea.
And that's, I mean, I feel like all these things
are going to be pushed to the limit in legality.
I mean, we see this with generative art too,
where like there's no way these models
could like actually memorize all these,
all the images that's seen on the internet.
It's like, but they can produce sometimes
the things that are clearly in the style
or use some elements from like that seem basically stolen.
And, but is that, does that constitute fair use?
Like the training the model on all these things
is that fair use?
And then it's the same with text as it's sort of like,
like if it's writing on a subject
where it's only maybe seen a little bit of training data,
it's maybe more likely to almost verbatim,
repeat some things from on specialized topics.
How are you even going to know when you're plagiarizing?
It's, yeah, it's a lot of open questions here.
I know, and in a way, there's an interesting analogs.
You know, we said that large language models
don't understand anything.
And it's the same in the vision domain.
They don't understand the art, certainly from,
you know, conceptually.
And what they do is they just slice and dice,
you know, they kind of like cleverly stitch bits together.
And actually, even with neural networks,
people misunderstand neural networks.
So a lot of people say that they learn
the like intrinsic data manifold.
And actually, they don't really do that.
They do something that approximates that.
And there's a famous example
with MNIST digit interpolation.
And you see like, you know,
you can kind of like interpolate between the digits.
But there are loads of examples where that doesn't work.
And actually there's lots of cutting and gluing
and like weird bits of digits stuck together.
And that's what happens with stable diffusion, basically.
It's like, you know, slicing and dicing and chopping
and composing things together.
And it's a very random process.
It doesn't really understand anything.
No, yeah, exactly.
And you can sort of, I mean, it's amazing
how well it can look and seem,
especially kind of like when you don't look too closely.
And it can seem like it kind of understand,
it must understand object boundaries and whatnot
because it's done so well.
And it's like, not really.
If you look at the details,
you'll see like fingers merging into like tables.
And you'll see like, there's like the boundaries
between what like two objects are kind of blurred
and this like continuous.
It is just doing like some sort of,
as you said, approximation of the manifold.
And like, neural networks are gonna learn
sort of smooth approximations of things.
And the manifolds are maybe not smooth everywhere.
And especially with like object boundaries and whatnot,
it's like a smooth approximation of these things.
Maybe it's just gonna give you these weird artifacts.
Yeah, and even the smoothness thing is an illusion.
They learn this, they kind of decompose the input space
up into these linear like affine polyhedra
because of the relu cells, essentially.
So like, if they appear smooth,
it's because the cells are very small
and very close together, but...
Yeah, exactly.
Yeah, so computational neuroscience to finance,
that seems like an absolutely massive leap.
It sounds like it, but in a lot of ways,
I feel like my life is pretty similar to what it was before.
Basically, sitting in front of a computer building models,
getting lots of noisy data,
trying to fit high-dimensional nonlinear regression models
to it, having to deal with not enough data
to actually fit flexible enough models you'd want to,
and having to sort of try to build in good priors
in your models to try to make them be able to learn
from the impoverished and extremely noisy data.
Both finance and neuroscience,
the SNR in the data is quite, quite low.
It's been kind of a revelation, especially in finance,
getting used to correlations of like 3%,
4% being sort of the best you can do in some cases.
Just like correlations that I would not have believed
at before, if I saw like a 4% correlation,
before I would be like, that's complete nonsense,
I don't believe it.
But like, sometimes that's just the best you can do
in like quantum finance.
And it can be real, like you can see it consistently.
So you start like believing that these,
and the differences between the 3% and 4% correlation
can be actually real, which is kind of amazing to me.
So we were talking about this about a week or so ago,
but I've just read a book by Christopher Somerville's
Natural General Intelligence.
And he kind of said that one of the problems
with neuroscience, I mean, as you said,
in some sense, it is analogous to being a quant,
because it's just so unbelievably complicated.
And there aren't really any overarching theories
in neuroscience.
For many years, neuroscientists have produced
very reductionist models to work on a small part
of the system in isolation.
And it might be a multi-unbanded system, for example,
and they might take very abstract quantities
and put it into the model.
And of course, neural networks now are slightly different.
They actually take in raw sensory information
and they learn representations.
But I just wondered,
could you kind of contrast those schools of thought?
Yeah, it's, I mean, science in biology,
especially in sort of any biological field
is extremely complicated,
because the sort of standard way you think about doing science
is a very linear way where you like break one thing at a time
and see what this sort of looking at each variable
by variable, each variable interact,
how it affects the system.
And so you, but when you have a system
that's sort of this nonlinear dynamical interacting system
with feedback loops like crazy,
you can't just sort of break one thing at a time
or like modulate one dimension at a time
without sort of changing the behavior of the entire system.
And so just sort of standard ways of doing science
don't necessarily work that well.
You can, like in sort of the classic idea
in visual neuroscience was you use like sine wave gradients
to probe the visual system.
And you can get models that look like they work very well
at explaining the behavior of early visual cortex
to sine wave gradients.
But if you try to use the models you learned there
to extrapolate to say,
how does a neuron respond to naturalistic images?
It just doesn't work.
And it kind of even looks like the sine wave gradients
are driving the system into a sort of state
that it never gets into normally.
You're kind of driving it out of its normal operating range.
And what you, and so the system is behaving differently
because you're only trying to look at like one dimension.
And so what do you actually really learn?
You've sort of learned how the system operates
in this weird perturbed state,
but it doesn't really necessarily tell you
about a sort of normal natural operating like parameters.
And yeah, and in like in finance
you can't even really do experiments like that.
And so you're sort of left with this more inductive approach
of you just try to get lots and lots of data
and try to learn the patterns and the data.
And that was the sort of approach that the lab,
the Gallant Lab at Berkeley where I did computational
neuroscience, that was the approach
that they were kind of pioneering
of using complicated naturalistic stimuli
and then using machine learning statistics
to try to extract the patterns from the data.
And that adapts quite well to the sort of new machine learning
like in like quant finance paradigm,
which is starting to take off.
I kind of feel like I got into neuroscience
just as sort of machine learning was starting
to make its way into neuroscience.
And now I feel like I've gotten into finance
just as machine learning was starting
to like move into finance.
So it's been kind of exciting to see it happen
in both fields.
Yeah, so there's a few places we can go here.
I mean, I'm interested in the intelligibility of systems
when you model them at the microscopic scale
because that's something that we struggle with.
And also you mentioned dynamical systems.
I mean, for the benefit of the audience
that describes a system where you're kind of like
iteratively changing things over time.
And these systems typically develop chaotic properties,
which is to say like if you change something
even a little bit, you get these massive kind
of changes in the system on the output.
And even a neural network is technically
a dynamical system, right?
Because you have back prop
and you're kind of changing one layer
and then you're changing the next layer
as the result of the previous layer.
And you get this kind of like iterative mutation of values.
But in real neural networks, in our brain,
it's so much more complicated than that.
We have all of these like feedback connections
and reflexivity and complexity.
It's crazy.
Yeah, not to mention different cell types
different neurotransmitter types.
And like the way those like,
you have sort of like several different networks
of different types of things interacting too.
It's not just like an artificial neural network
where everything is kind of the same.
You have like different cell types
that use different neurotransmitters
that are somehow modulating certain things
and these networks are interacting.
It's like the complexity is just like scary.
Like at some point here, like one of my favorite things to do
when I would go to the Society for Neuroscience Conference
was to just like walk around this conference
in this huge like multi-football sized field
of just posters of like all sorts of different types
of neuroscience.
And you just realize like how vast the field is
and how little we know about it putting it all together
because it's just so complicated.
You can only sort of wrap your head
around your own little corner of the thing
but like trying to get, understand the full system
and all it's like incredible complexity.
I mean, it might just be too much for one human being
to be able to fit in their head.
And so some of the goals of trying to understand things
or make intercal models, it might just not be possible.
We might just not, I mean, might not be able to understand it
in a way that feels intuitive to us
even if our models work quite well.
Yeah, humans have this real desire to understand.
And we create intelligible frameworks and theories
and we end up excluding most of the reality of the system.
But just before we go there, I wanted to talk a little bit
more about the brain.
So, you know, Summerfield said in his book
that the ultimate goal of the nervous system
is to avoid surprise altogether.
So when they study brains, they see that the brain
kind of lights up and activates in a surprising situation
and less so when it sees something it's seen before.
And this also brings me to this idea
of there's a dichotomy between representationalism
and inactivism.
So the representation, this viewpoint
is that the brain does all of the thinking
and it can be in a vat.
It can be isolated from the environment.
And the inactivist school of thought
is that the brain just kind of thinks
in terms of trajectories in affordances
given by the environment.
And the brain decoupled from the environment
is completely stupid.
It's just kind of like the brain only
moves through the environment through affordances.
And maybe that's a continuum, but where
do you fall on that continuum?
It's a really good question.
I mean, I think dreams are kind of the counter example
to the pure.
I mean, dreams just sort of prove
we can just sort of, without any sensory input,
construct very rich worlds.
So we must have some ability to just represent
some sort of models in the world.
We're not just purely sensing and receiving the world.
We have structures that are able to put things together
in a sort of coherent reality.
And clearly, there's an interaction
between these structures in your brain
that can construct these things and the sensory data
that kind of work together to construct
how you experience things.
And so yeah, it's a continuum.
I think we are always with the world.
You need the world to sort of build up these systems
over time.
You're not sort of built with all of them working just
as a baby.
I mean, sure, the system is biased in certain ways
that will help it learn these things.
But yeah, so they're kind of both true to some degree.
And yeah, it's definitely not one or the other.
Yeah, it's so interesting.
And we're speaking with Carl Friston tomorrow.
And he's got this free energy principle.
And it's a kind of postulate that works at any resolution.
So even with a single cell amoeba or something like that,
there's this idea that it has a Markov boundary.
And there's this kind of cyclical causality.
And these boundaries, I guess, are relative.
So you can draw boundaries around anything.
You're a boundary.
You're an agent, but also at the microscopic scale.
And he says that all of these systems,
they just kind of predict external states
from the internal states.
And then you get this self-organized and emergent
complexity and so on that comes from that.
But he does say, though, that intelligence
is essentially about being able to predict
a trajectory of actions.
And I don't know whether we'd call it goal-seeking behavior.
But we do that very abstractly, don't we?
But weirdly, when you look at the brain level,
it's happening at the microscopic sensory motor level.
So it's almost like, how do you get
that emergent, abstract intelligence from that?
That's a, I mean, yeah, that's an incredible question.
I mean, it seems like this, like what you said,
this sort of idea of predicting the future
just a couple steps into the future
that is just happening at just the circuit level,
even in the retina, that it seems
like that is a good sort of building block
if you can sort of chain that together
over sort of larger and larger scales within the brain.
It wouldn't surprise me if that's kind of the way it worked.
This sort of, these sort of basic circuits
that are used for prediction, but with different input.
If you're just having sort of retinal,
Ginglian and like a photos receptor as there is an input,
it's able to sort of do this sort of very simple prediction.
But if you have these more complicated patterns
in the middle of visual cortex
and then hire on the same sort of circuits
with different input could sort of just be predicting
this sort of evolution of these patterns.
And yeah, it's kind of amazing what you can sort of build
out of these sort of simple rules and building blocks
if you just iterate them over again.
That was actually that sort of idea
of iterating a simple sort of computational rule
for explaining visual cortex was one of the things
I wrote about in my thesis.
We're trying to explain like this middle visual cortex
like V4, the responses there using basically
an iterated model of like V1.
So the sort of processing in V1 we fairly understood.
We have the best models of anywhere in visual cortex,
maybe even all of cortex.
And just sort of iterating the principle again into V2
sort of basically just assuming V2 is taking V1 inputs
but doing sort of a similar transform.
And the V4 is taking like V2 inputs
and doing sort of very similar transform.
And sort of the things you see that V4 is sensitive to
are these complicated patterns and textures.
And you get complexity very quickly
from just iterating the sort of simple rules.
And I mean, that's what neural networks
are essentially doing.
They're just often just doing linear transforms
with nonlinearities over and over again,
just iterating these simple transforms
and building up the complexity very quickly.
Yeah, I think there's something really magical
about this reflexivity or I mean a great example of that
are there are graph cellular automators
along the lines of Wolfram's digital physics project.
And the really clever thing is that
you're using the same rules
but you're just kind of like running the result
again on top, on top.
And there's a similar version with a graph cellular,
sorry, a CNN cellular automator
where you model something at the microscopic scale
and you get this emergent global phenomenon.
So it might kind of materialize as an image
of a gecko or something like that
but you've actually coded it at the low level.
But yeah, that brings me to this universalist idea
of let's say how brains work
but maybe how neural networks and intelligence work.
Vernon Mount Castle, I read about this in Jeff Hawkins book.
He had this very simple idea of the brain
as being lots of repeated copies of the same circuits
in the neocortex.
And I think this is contested by many neuroscientists
but they differ only in how they are wired.
So they're wired to different sensory motor circuits
and they're essentially just a copy of the same thing.
And as you say, they themselves get called reflexively,
recursively and so on.
And then you just get this emergent intelligence.
So I mean, what's your view on this universalist idea?
I mean, there's definitely not just one circuit.
I mean, as you look through the cortex
in different areas of the brain,
just the laminar structure
which these sort of circuits are like supposed to be
like the where the columns are supposed to be
where these sort of circuits are supposed to be defined.
It changes like, but there are definitely commonalities
but there's, I mean, it makes sense
that maybe the circuits in different areas
should be slightly different for the different purposes
between like prefrontal cortex and say where you have
much more high order types of processing going on
than like visual cortex or auditory cortex.
And so there's probably,
if you go in this direction of thinking of some,
there's like, there's probably a small number
of these types of circuits that interact in various ways,
but there is definitely some specialization going on.
Yeah, like having universalist ideas and biology
never seems to work out that well.
There's just so much diversity and complexity.
It would be nice if we could reduce everything down
to like just one thing repeated over,
but like generally it never works out quite as cleanly
as a, yeah, again, it's our desire
to have an intelligible framework.
And I mean, the free energy principle,
you could argue as a theory of everything,
but there's, I mean, Stephen Wolfram's example
and even Eric Weinstein's geometric unity.
I mean, there are many theories of everything.
But yeah, what do you think is the role of language
in cognition and thinking and planning?
That's, it's a really interesting question.
And it's also, I think a kind of hard one to answer
in the sense that if you,
I've seen some recent reports just like talking
about like other people asking like survey questions
to other people and finding some people
like don't have an interior monologue
in the same way you might think.
And just like there's actually a lot of diversity
in like people's level of internal monologues.
And they've done studies where they have this like little
like beepers go off and people are supposed to write
what's going on in their mind.
And so it's, and yeah, and just with visual imagery,
we find that it's a huge like variety
in how much, like how strong people rate their visual imagery.
And so, I mean, yeah, some people, I mean,
me personally, I have both, I mean,
pretty strong interior monologue,
but I also feel like a lot of ideas
are in sort of pre-linguistic state.
And I'm kind of like searching for the words for them often.
And there's definitely kind of continuum there.
It's weird to think like how do we get the words
that we're saying, where the words come from
that are coming out of our mouth?
Are we really choosing them?
You're definitely not choosing them
in this sort of top-down way.
They just sort of seem to come out.
And you just kind of point yourself in the right direction
and hope the best as they come out.
And, but that says a very different quality
like when you're just speaking phenomenologically,
it feels very different to when you're just sort of
thinking yourself, what should I do today?
Should I go to the store?
And so, I mean, yeah, the way in which language
interacts with thoughts and behavior
and like verbal communication,
it's definitely not, it's definitely not simple.
And yeah, there's, I mean, definitely this kind of continuum.
I mean, it's all, it's, to me, I just sort of think
it's with all these sort of networks kind of interacting
and sometimes you're like triggering the kind of language
things and you're just making these kind of patterns
and sometimes the language patterns you're activating
are helping activate other things as well.
Sometimes you can just be in this kind of less linguistic
state where you just kind of, just sort of sensing
these patterns and you just have this kind of like wandering
thoughts that aren't necessarily linguistic.
But yeah, it's definitely, I mean,
and also it seems, yeah, as I said, people's,
the way people do this seems all over the place.
And so there's not sort of even one answer
for even one person or definitely not across all people.
Yeah, I'm really interested in this idea
of differential kind of subjective experiences.
And you know, like there was that Nagel paper
about what does it like to be a bat?
But even with the human experience, we're all very different.
You said about your internal monologue
and I hadn't really thought about
how that might be different, but I was drawing a picture
of Valentine's card earlier and it was so terribly bad.
And some of my friends are really good artists
and I was kind of thinking to myself at the time,
maybe this is just a, this is just me.
I can't really visualize things in my mind very well.
I've got a very analytical brain.
Won't mean that certainly when not under the influence
of psychoactive drugs anyway, but you know what I mean.
So we all have a very different subject of experience,
but the miracle is we can understand each other so well.
So you would expect there to be an incredible amount
of brittleness in our communication, but there isn't.
Um, yeah, it's, so I often wonder about this too.
Just, I feel like the misunderstandings happen a lot
more than even people realize.
And you can sometimes, you only really notice
when they become kind of big and matter.
And especially like people can think
they're having a conversation.
And sometimes even from the outside, you can see like,
these people are just talking completely past each other.
And you can kind of see that they're not really understanding
each other, even though they maybe think they are.
And so, yeah, I don't know how not brittle they are.
I think they, I think we think they're less brittle
than maybe they are.
I think sometimes we assume people are understanding
what we're saying better than they actually are
because they nod and smile at us.
And because that's, it makes us feel good
for people to understand us.
It makes us good to feel, to understand other people.
But yeah, I mean, it's, I mean, clearly we do have
a lot in common and there's definitely things
we can understand about each other.
But yeah, it's like, I do sometimes think that maybe
we're more different from each other than we really realize.
Yeah, that's a really fascinating thought.
I mean, we speak a lot with Wally Tzabur
and he says how language has evolved
to be extremely ambiguous actually
because it's a form of compression.
So we don't say everything we mean
and we'll get into like language models in a minute.
That's part of the reason why they don't understand things
is because a lot of information is not in the text.
And Wally says that we have a lot of what he calls
naive physics.
So we understand that objects can't be in two places
at the same time.
And if something is located inside something else
and we move that thing somewhere else,
then the thing inside has also moved.
So we're doing all sorts of reasoning on the fly.
And what we're kind of doing is like,
we're disambiguating out of the 50 meanings
of an utterance into the meaning.
And like it just, we almost always understand each other.
You know, you wouldn't really expect that.
No, I mean, yeah, I mean, we generally have like,
I mean, our understanding of physics
should generally be compatible with each other.
I do feel like it's, in most cases, yes,
we do very clearly understand each other
because in most cases it's more like well-defined.
I think the trouble gets in sort of like fuzzier areas
about people's like emotions or opinions about things
where our priors are more sort of maybe less tied
to like objective things like physics
and are more sort of just tied to like our upbringing
and just sort of whatever ideas, notions we have
about how people should like behave and interact
and what like our value systems.
And so yeah, when people are talking about some
of these kind of things, I feel like they're more likely
to be able to like talk past each other and not realize it
because they're sort of assumptions about what is important
or what is meaningful might be different from each other.
Yeah, actually, you're absolutely right.
So we don't have an objective phenomenology.
And I mean, I used to do,
there's this thing called quantified self
where you kind of like keep a diary every day
and you record how you're feeling in that day.
And feeling is a subjective state.
So I remember at the time that every single day
I needed a new word to describe how I was feeling
because the old word I was using didn't work anymore.
So the number of words kept growing.
And actually, that's so true, isn't it?
If I tell you how I'm feeling right now,
that's completely brittle.
So there are some things in the world
that are quite informational and objective
and we can communicate very well.
And then when we're bordering on anything subjective,
language fails us.
Yeah.
And then we were trying to map whatever word you're using
onto how I would use that word
to describe the feeling that I would be having.
And that mapping seems completely,
without a long conversation to try to like
feed up that mapping,
it could be quite different in how I would apply
that word to my own feeling.
Yeah, and there's been studies done as well
that I think certain tribes
have a completely different color perception.
And there are also concepts like vagueness.
So what is a pile of sand and what is a shade of red?
And these things are actually very, very difficult
to communicate objectively.
Yeah, things like color perception
are kind of the interesting ones
because the literature is a bit messy
on some of these topics.
And some of it is just where you draw the lines
between colors and then how those,
how those linguistic boundaries affect perception.
There definitely seems to be both things going,
but it's not sort of like,
I don't think any,
I think it's a strong claim to be like,
oh, the people like can't perceive green
or something like that.
It's just like where they would draw the line
between blue would be in a slightly different place.
And then they might kind of see them
with being sort of experience of as being like
further apart or closer together
than you would necessarily.
But yeah, that's really a complete,
their experience and wouldn't necessarily be
super alien to you,
but they're sort of experience of maybe more very cultural
like cultural taboos or something
would be very different than yours.
Yeah, I mean, one thing you're alluding to there is,
it's when we deal with complicated systems,
there's a real problem about drawing boundaries.
And I was, I mean, Friston's a great example.
He's got this idea of a Markov boundary
and it could be at the cellular level
or it could be you as a person.
And then when we talk about things like agency
and free will,
we tend to anthropomorphize this boundary.
So we tend to think of ourselves as individuals,
but actually you could draw boundaries at different scales.
And the boundaries might be observed a relative as well.
So your boundary might not be my boundary.
Yeah, exactly.
Yeah, that's,
yeah, there's something I know a ton about with this,
yeah, this how you define yourself
and how you think of yourself
within the context of your community and whatnot.
I mean, some of these ideas are just very cultural
and how you experience yourself
is probably even like very different,
can be very different cross-pulturally.
Yeah, I mean, maybe one thing to bring in
is when you're as a quant, when you're doing modeling.
You have this very, very complex system
and you draw boundaries
and you create variables and observables
and do you know what I mean?
You kind of build a model
and that boundary could exist at any scale.
It seems like quite a,
it's a bit of an art and a science at the same time.
Yeah, no, for sure.
Yeah, exactly.
That's kind of why I like some of these complicated problems
that are not very well defined
where you kind of have to use intuition
or just sort of do the best you can do
at sort of drawing what are the relevant variables,
what sort of a priori makes sense to me
to be the things that matter for the system
behaving at this within the context of this experiment
or in the context of this market or whatnot.
Trying to draw boundaries in because
the rules for these systems are not clear.
Like what are all the relevant variables for everything
and do you have access to them
and can you control them?
And generally you don't know them all
and you don't necessarily have access
or can control any of them.
And so it's, yeah, it is a bit of an art.
Well, now might be a good time to talk about numerators.
So you're the chief scientist
and it's this insanely cool platform, right?
So people can go on there, they can download data sets,
they can build their own models, they can stake the models.
I mean, why don't you just talk me through it?
Sure, yeah, so we advertise ourselves
as being the hardest data science problem on the planet
because I think it is, because like I said,
the correlations you're chasing are on the order
of like three or 4% out of sample.
Which, and just sort of being able to tell
do you have something real or is it just in the noise
can be extremely hard to do, which is,
and we set up the problem for participants,
we give out a set of data that has been cleaned
and obfuscated and regularized.
And you basically just have a set of features
and a set of targets and you're just trying to build models
to go from features to targets.
So it's sort of a very classic
machine learning style problem
and it's nicely curated for you.
And how it works for us is every week
people submit predictions on a new set of features.
So every week we release a new set of features
and people just run their models over those features
and give us a set of predictions
and people stake on those predictions.
And so people stake our cryptocurrency called NMR
and if their predictions do well, they make money
and if their predictions don't do well,
that week they could lose money.
And they sort of are expressing their confidence
in their models using their stake.
And so we basically use this expression of confidence
as a way to sort of integrate these signals
into our meta model.
Our meta model is really just like a stake weighted average
of all the signals people are submitting.
And these signals, these predictions
are just sort of weights on stocks.
They're sort of like how do we want to go long
or do we want to go short in the stock?
There's sort of just expressing,
do we think a stock is going to go down or going to go up?
The stake weighted model we feed to through our optimizer
which is just doing a convex optimization problem
trying to create a portfolio from the signal.
And is that portfolio changes week to week
and so that just the difference between the previous
and the new portfolio is just what we trade every week.
And so our trading is basically completely determined
by the like thousand people all over the world
submitting predictions every week.
And so it's this very kind of nice decentralized hedge fund
where the signal generation is very decentralized
and we get the advantage of ensembling
over a wide variety of models.
And see people are trying to make their models both
predict the targets very well.
And consistently and we have other incentives
to try to make predict aspects of the targets
that other people are not to try to,
so that their contribution is sort of more unique.
And they can make quite a bit of money
by having their predictions be pretty different
from other people's but also still accounting
for like variance in the target.
And so that system is we call true contribution.
And it's really, we try to,
it was our attempt to try to make people's predictions
and payouts more tied to actual portfolio returns.
Cause the sort of standard scoring
and we're just doing the correlation of how well
your predictions match like the new weekly week's target
that is determined by just how the stocks move
that over the course of 20 days.
The true contribution is basically
sort of doing the whole process,
like creating the meta model,
running it through the portfolio optimizer,
getting the portfolio, getting portfolio returns.
And then we try to see like,
take the gradient through all of that.
And so you can find out if people's stakes have been
more or less, we would have made more or less money
and use this gradient of the stakes
with respect to the payout,
with respect to the portfolio returns
as a way to pay people out to essentially increase
their weight or decrease their weight.
And that tends to reward people
with more unique contributions.
I've got so many questions.
So, I mean, I really like this idea
because first of all, you're democratizing the whole thing
and you're kind of gamifying it and it's a meritocracy.
So any data scientist can go on there
and flex their muscles and build great models
and be recognized for doing so
and even earn money for doing so.
But in a way, I want to contrast it to somewhere like Kaggle.
Now, on Kaggle, I mean, traditionally data science
has been about understanding the domain.
A lot of data science is business analysis essentially
and kind of understanding what makes something work
in a model.
And as I understand with Numeri,
the interface is kind of the same.
So maybe they get similar shape of data every time
they build the models on it.
And in this domain,
because you know, like there's technical analysis
and there's fundamentals
and that they might still understand some market
that they have some kind of extrinsic understanding
of why their model would work,
but they don't have the same kind of understanding.
No, yeah, the features include all sorts of things
from like analyst sentiments
and other sort of fundamental things
to technical features.
But that is all sort of obscured from people.
People just have these funny feature names.
And so it's up to them to just use
their sort of machine learning toolbox
to figure out what are the good features for predicting
what features tend to work well.
How do we combine those features?
And so we actually wanted to kind of like remove
any of the people's biases
for what features they think will work.
We wanted to not have people's financial intuitions
play into it.
We wanted to just sort of set it up
as a pure machine learning problem.
To try to make it,
yeah, basically to make it be better
than any human could possibly be.
So this sort of combined ensemble wisdom of the crowd,
we're trying to make it like the alpha go of like finance,
something that it's just that performs
at like a super human level
in ways you don't really understand.
Interesting.
And you're aggregating the predictions together
in some way.
Yeah, it's actually fairly simple.
I mean, people submit their predictions
which are just a number between zero and one
for every stock.
And it's basically just a rank ordering of stock.
And we just normalize everyone's predictions
and then just weight them by their stake
and then just average them together
and to do another renormalization
so that it's the right scale
and sort of distributional shape
to be fed to the optimizer.
But it's a fairly simple and robust way to weight things.
We're basically just using people's expressed confidence
in their model as the weighting system.
And because there's this feedback of payments
and paying out people,
good models, their stakes increase over time
so their weight in the meta model increases over time
and bad models, their weights decrease over time.
So it's kind of like a human in the gradient descent
for doing like gradient descent
with the stakes as the weights in the model.
Fascinating.
And can you give us any intuition
on how that model is tuned
and what kind of penalty you're using?
Are you using just the stakes
or also the previous performance?
So no, we're not actually using the previous performance.
It's really just stakes.
The previous performance only enters into the fact
that the good performance in the past
would have made their stake grow over time.
But we have thousands and thousands of models now
and so any one model is only a very small percentage
of the meta model and even the ones that are the biggest
maybe only a couple percent of the total meta model.
And it is a sort of like power law distribution.
There is a lot of work that I've done
in the portfolio optimization set.
And that's the going from the signal to the portfolio.
And there is actually a lot that goes on in there as well
just in how you construct a portfolio,
how you determine how much you're gonna trade each week
and how you make your portfolio, what you make exposed to.
Exposed really just means is are the weights
of your portfolio correlated with lots and lots of things?
And so there's a lot we go do to try
to make the portfolio weights not correlated
with the market overall.
So we're a market neutral hedge fund.
So we try to be uncorrelated to the market,
have a beta of zero.
So when the market goes up or down,
you can't really tell how we would do on that kind of day.
And but we also try to be uncorrelated to lots
of other things that we think could drive returns.
So we try not to have like big country biases,
big sector biases, factor biases.
So factor are things like value and momentum,
these kind of like more abstract quantities
that are supposed to tell you something
about classes of stocks.
But we try to be uncorrelated to basically everything.
And they're just trying to get the sort of pure machine
learning non-linear signal that is driving stock returns
or like stock specific alpha,
we call it sort of like the amount of stock
is going to, how all the stock is going to do
for just by itself, not taking all these other things
that are about it into account.
Yeah, that's really interesting.
And I guess like one of the problems on Kaggle
is that most of the solutions are so overfit
to the training set that they never generalize
to real world versions of the problem.
But what you're doing actually is to kind of like
remove away a lot of those opportunities for overfitting
and also allowing the models to be used again
when the next thing comes around.
But just quickly on the aggregating stuff
the reason I'm interested in that is on my PhD
I did prediction with expert advice
and there's a whole load of theoretical approaches
to that where you can have an aggregating algorithm
that produce, you know, that produces performance
or a kind of like an error bound
which is not much worse than the best path
of switching experts.
So if you took the optimal path of the best expert
every single time step, you can have algorithms
that have a provable bound, not much worse than that.
Yeah, we, so we've done a lot to try to experiment
with trying to improve upon stake waiting.
And it's always been really hard to do it in a robust way.
It's, I mean, for one, stake waiting is,
it's sort of nice in that it's easy for people to understand.
People are, it's very clean in how it works.
It sort of fits with the ethos
and the, like, and the idea of the company
of how it's distributed and decentralized
and you express your confidence by your stake.
But it does sort of seem like there should be a better way
to aggregate models, but pretty much every time
we try to find something better, it's,
it might be a little bit better, but it's like less robust.
It tends to just be less robust.
And it's, because you are essentially just sort of fitting
to the past and to try to find way to the models
or something, it tends to just like overfit
and this sort of stake waiting thing,
you can't really overfit.
It's just sort of a property that just sort of evolves
as the tournament goes on without ever considering
like the past performance and all of these things.
So yeah, it's, it's been kind of interesting to me.
So it's one of these things where you sort of revisit
every year at some point of like,
let's try to build a better meta model,
but we usually just come back to stake waiting in the end.
Yeah, well, in a way, I mean,
we're prediction with expert advice,
you have a learning rate.
And I guess you don't even have that problem
because you're just using the, the stakes as well.
Yeah, the, but yeah, the, I mean, our learning rate.
So, I mean, our payout system is the way
we adjust the weights over time.
And so we have done like some simulations to show that,
like if, how we reward people,
how that affects their weights over time
and how that affects meta model performance.
So you wouldn't want to have a payout system
that would make the meta model worse over time.
And so, yeah, like this, this true contribution idea
that's gradient of the stakes,
we did simulations to show it does actually improve
the meta model over time to pay out in this way.
It's nothing, I mean, people do things
like take their stakes out, withdraw money.
And so it's not a perfect system.
People entering the tournament,
people, some people entering with a lot of money,
some people entered with not that much money.
And so, yeah, it takes time for these things
all to kind of shake out in real life.
But the overall idea is that we are essentially adjusting
the weights through our payouts
towards this sort of more optimal meta model over time.
Interesting.
So I'm actually very, very interested to give it a go.
And I guess like, first of all,
you could sketch out what the process looks like.
I mean, let's say I had a few hundred dollars
and I wanted to build a model.
And also, it's got to be a good model.
Let's face it.
So if I just logged on there
and I built a gradient booster tree model, would that work?
It would actually.
So, I mean, it's tabular data
and tabular data is very minimal
to gradient boosted trees.
We have a lot of example models that we have put up
and they're doing quite well.
So basically all you have to do is you can go to the website
and just download a big zip file
that includes all the data in parquet.
And then you can just open it up in Python
and fit a gradient boosted tree.
When we have example scripts sort of showing this
along with some more interesting types of pre-processing
and other sort of ideas like feature neutralization.
So I can talk about it in a second.
But yeah, a lot of our sort of standard internal models
use basically gradient boosted trees.
And we are, I mean, we have basically example models running
that, and they all have positive correlation with
and true and still true contribution.
So they're actually working out of sample
and performing quite well.
That hasn't all been sort of eaten up
by people using similar enough models.
There's a lot of opportunity to make sort of unique models too.
Cause one thing that's sort of unique about our tournament
is we release actually several targets,
release 20 something targets.
And they're all constructed
in somewhat slightly different ways.
And you can find that if you train on a different target,
it might work almost as well as training
on the target that you're scored on.
And it might also ensemble really well
with a model trained on different targets.
And so you can actually create an ensembles fairly easily
just by training on different targets.
Because it is kind of remarkable that a model trained
on a different target can actually work better
on the target you're interested in.
But that kind of thing, yeah, definitely does happen.
I mean, part of it is called all the correlations are so low,
but some targets might just sort of have a better property
in making your model pick up on the actual signal
that you're want to model rather than sort of variance
that is like not that you don't want to model.
Interesting.
I think one of the issues is you might not know
what models the people are using.
But I wondered if you did have any intuition,
I'd be fascinated to know,
are they using very complex models?
Are they using simple models?
From talking to participants, there's a huge range.
There are some people using like extremely simple trees.
There are some people who are using
incredibly elaborate neural networks
with very sort of custom architectures
that are sort of designed to the problem.
There, yeah, there's a whole huge, right?
I mean, there's people who have huge ensembles.
There's people who are doing kind of like online learning
where their model is actually using the features
that were released that week
and sort of using that in some sort of unsupervised learning
and then so they take some while from when we released,
they can't just like run their model
through the new set of features.
They have to incorporate this new set of features
in this unsupervised way before they can,
so yeah, there's an incredible variety
of techniques people are using.
Fascinating, and how big is this parquet for?
How many rows, how many, you know, fields
and are they all just real numbers between naught and one?
So yeah, so it's, there's,
how many features are we up to now?
We have a couple thousand features roughly
and there's a few million, a couple million rows, I think.
It's, so one sort of additional piece of structure
in the data is there's these things called eras
and the eras are essentially just the weeks
and because this, the competition has this structure
of we're making predictions every week
and so within each era, there's like say 5,000 rows
which are basically like 5,000 stocks
and so one sort of interesting thing is you are,
you want your model to be good across eras,
not necessarily across samples
and so it creates a different structure
in how you think about the problem
because you want your model to be consistently good
in every era and that can give you a different solution
than if you just try to say maximize some metric
over the whole training set which is kind of, yeah.
But, but yeah, it is basically just a big parquet file.
We do divide it into like training
and like there's a, like a testing set
but it's, yeah, it's, do you have any specific questions
about how that is organized?
Well, and again, I'm really interested because on my PhD
I did a whole bunch of prediction models
on financial data sets.
I was predicting like the implied volatility
of the Black Shells formula on some, you know,
in future's data but my big thing at the time was
I was fascinated by regimes in financial data
and you get these changing dependencies with time
and what I did, I mean, you could actually visualize it
if you build a load of expert models on different regimes
and then you get them to predict on the other regime's data.
You get this kind of self-similarity matrix
and it looks like you get this kind of structure in there
because there are certain regimes
where this particular model actually predicts
quite far out into the future
and then it might suddenly go dead
so you get these kind of squares
and I had this big thesis that if I have expert models
and use prediction with expert advice
then when we come into a new regime
I would quickly learn which experts are the good ones
and I had this thesis that sometimes old information
is very helpful in the future,
more so than using like a simple sliding window
ridge regression or whatever
and it turned out I was wrong.
It's almost always better just to use
a sliding window regression but yeah, it's fascinating.
It's, yeah, it's interesting.
Like the, you definitely want to train on a lot of data
for these models.
It definitely, like if you just use the prior one year
of data, your models are going to be pretty crap.
It definitely helps to use like prior 10 years of data
and so it is, you're using actually quite old data often
in predicting into the future
but generally, yeah, if you were only just using
the last year or two of data
your models are going to have to take quite a hard time.
Yeah, one other thing about the features I wanted to say
is they are between zero and one, three and five bins.
They're zero, 0.25, 0.5, 0.75 and one.
So the data has been like binned in this way
and the targets are also binned in this same sort of bins
but with a different distribution.
The targets have like in their extreme bins
only like 5% of the value is in the next two extreme bins.
Like what is it?
20 in each of them and then 50% as a zero.
Interesting.
But all the features are basically just 20%
in each of the bins.
And so the binning is a pretty strong form of regularization.
It sort of prevents you from like a tree
from splitting sort of any arbitrary place
you can only split at these things.
And so that kind of forces at least some of the space
to be at different splits.
And the regularization, it's kind of,
you would think that having continuous features
would be a lot really helpful
but I mean, it's really not.
It's kind of remarkable how lossy some of these transforms
are that we do that actually seem to be helpful.
Yeah, so it's so interesting.
And I guess like one thing I didn't really appreciate
at the time is you know, we were just talking
about these complex dynamical systems
like the brain or like financial markets
and there's of course the market efficiency hypothesis.
And you know, perhaps one of the reasons why old information
might not be salient is because if the underlying system
is actually taking a trajectory
through this kind of complex space,
then you might argue that almost regardless
of where you traverse, you'll always be in a novel situation.
And then there's this continuum of regularity versus chaos.
So like for example, if you're predicting options futures
when they get close to maturity, the volatility just goes
crazy and they just become increasingly unpredictable.
And I guess the art in this kind of data is knowing
when you're in a regime which has some regularity
and when you're not.
It's yeah, it's tricky.
Cause like ideally we want our model,
we want our meta model to sort of work well in any regime.
And it does seem to work pretty well consistently.
And but what you do find on like the leaderboard
tournament participants, you'll see some people
who stay at the top of the leaderboard
for weeks and weeks and weeks and weeks,
then suddenly precipitously fall like down the leaderboard
as demonstrating some sort of regime effects.
One really kind of interesting thing I did was
I fit like a mixture of linear models to the data.
So if you fit just like a mixture of two linear models
where it's sort of selecting which eras to use
for which of the two linear models,
you basically, one linear model will get about
60% of the eras, one will get about 40% of the eras
and their weights will be almost mirror images
of each other.
And this just comes out like that is the optimal fit
for roughly, for 40% of the eras
that basically completely the opposite of the other eras.
Which yeah, demonstrating some like,
that's why like markets are extremely hard
cause like something that works well a lot of the time
it suddenly would just works really oppositely horribly.
And so you're often trying to split this difference
to find something that doesn't work super well at one time
and then will like crater at another time.
That's the meta model wants to kind of work
really like pretty good all the time.
And that's one of the things that ensemble
in all these models that maybe even the individual models
probably have a lot more regime characteristics
than this overall meta model.
I wondered whether folks were using
some really esoteric approaches.
I mean, I'm interested in geometric deep learning
and algorithmic reasoning and even think like
esoteric options like cellular automata.
Do you see anything like that getting traction
or it may be even discrete program synthesis?
I don't know cause yeah,
I only see what people are willing to post and share
on forums and there's quite a bit of sharing
on our forums of information
but there's definitely some people at the top of leaderboards
who are doing something that's working quite well for them
for quite a long time that they haven't shared.
And so it's, I'm not even sure what all the people are doing
but there are, I mean, people allude to using like tricks.
I mean, that they've learned in different jobs.
I mean, we have some people with like a variety of backgrounds.
It's been really cool to like see this community grow
and have people who are like astrophysicists,
particle physicists, people who are doing like
like computer vision and whatever sort of techniques
they've learned in their different fields
and try to use them on this problem.
That was what sort of attracted me as like,
I was doing like computational neuroscience
and I saw this problem as like,
oh, this is a complete free playground.
You can do whatever you want.
And so it was a fun opportunity to try out ideas
that wouldn't really work well in computational neuroscience.
Yeah, indeed.
And physics, I mean, the road to reality by Roger Penrose,
I think it was Michael Bronstein who said
that if you could summarize the entire book in one word,
it would be symmetry.
And there's also another key idea from a lot of researchers
which is abstraction, you know,
which is like some metaproperty of the relationship
between data.
So, you know, you probably have lots of folks
coming in from different fields
and they have some very, very interesting approaches
to solving this problem.
Yeah, for sure.
Yeah, I mean, I have, I mean, there's people who use
some like interesting like auto encoders
to try to learn structure from data
as a way to learn features.
People using, it's interesting non-linear,
non-linear dimensionality reduction techniques
to try to, yeah, to try to find various features.
It's, and yeah, even some things people loo do
some sort of interesting feature selection
or denoising types of things
that they've learned in their fields.
Yeah, it's always interesting to me to see
like how different fields that use machine learning
use it in different ways
and what sort of tricks and tips might cross over.
I was going to ask about that
because you have loads and loads of features
and there's this problem called the curse of dimensionality.
Right, so, you know, when the number of dimensions
increases, the volume of the space increases exponentially,
which means like this concept of nearness basically disappears
and there's statistical models don't work anymore.
So, you know, presumably people would do things like,
I don't know, dimensionality reduction feature selection.
I mean, neural networks are quite clever
in the sense that they, via a variety of methods,
overcome the curse of dimensionality
by learning some data manifold or whatever.
But, you know, it's with natural data,
it's not with financial data, so it's not a given.
It's, yeah, and this is actually one of the things
that was really intriguing to me
when I started in finance is,
so in science, when you're doing regressions,
you're trying to find often sparse solutions,
you're trying to find the sort of small number of variables
to predict your targets,
to try to find whatever sort of maybe
causal relationships there are.
In finance, we often try to do exactly the opposite,
where we want our models to care about all the features
a little bit, and so we do,
we'll do something like what we call a feature neutralization,
where basically you take your prediction,
take the linear model of your prediction
from the features and subtract it off.
And so you're making your prediction
not linearly correlated or linearly dependent
on any of your features.
We're doing some fraction of that,
so just trying to remove too strong of a linear relationship
between a feature and your prediction.
And you do other regularization techniques,
like in your tree learning,
maybe one thing that works quite well
is using like column sample by tree,
instead of to very low value.
So each tree is only considering
a small subset of features.
And so your ensemble is sort of,
you use as a lot of the different features
because it's sort of each tree
only has access to 10% of the features
across your whole ensemble.
You are probably using a lot of your features a little bit.
And that tends to work quite well.
And the reason is,
it's because features will work for a while
and then they'll just turn around on you.
And so you don't want to be sort of
super dependent on any one feature.
And so yeah, it does make the cursor dimensionality
kind of worse in some ways because you don't want
necessarily find just a small subset of variables
that are the best because sometimes
that will maybe give you a really good model for a while,
but sometimes all of a sudden
those will just turn around on you.
And then your model just like is almost anti-correlated
where it should be.
Yeah, it's so interesting.
You know, like this problem with the changing dependency.
So essentially you're modeling a non-stationary process
which makes it much harder.
And when I was speaking with Sarah Hooker the other day,
she was talking about fairness and bias in models
and part of the problem there is,
we optimize for headline metrics like accuracy.
And when you decompose the training set
into let's say different categories like men and women
and people who live in London,
the accuracy is very stratified.
It might perform very badly for people that live in London
but very good for people that live in New York.
And then you start getting into the situation of saying,
okay, well, I'll build an ensemble of models
that are independently optimized for all the different things.
But then you have this impedance mismatch
between this global accuracy
that you were optimizing for and are on the benchmarks.
Yeah, no, it's a really interesting property of these things
is, especially classification models
where they will work well for some categories
and not others.
And it can be sort of tricky to find out why.
Are those features just more discriminative
or are these classes somehow harder to tell apart
just in some way?
Yeah, but I'm glad people are starting to look at
and try to dig into some of these details
rather than just looking at headline metrics.
And I'm also sort of happy that the field
is sort of moving to this out of distribution learning
is becoming a much more interesting topic
because that is what really matters
in making machine learning
that is going to affect the real world
as it needs to work out of distribution,
out of your sort of training and test that split distribution
as well as possible.
And like how you do that is, I mean,
still very much an open question, clearly.
And how well you could potentially do that
is even still an open question.
But that is one of the, I mean,
that is sort of what true intelligence is to some degree.
Like humans are pretty good at adapting out of distribution.
And what is it about us?
What are like, how are we able to do that?
And how do we make our sort of machine learning systems
work better that way?
How are we sort of able to, I mean, yeah.
I think it probably has something to do with,
we're able to learn sort of causal structures that work well.
And the distribution can be very different,
but the sort of causal structures remain.
And we're able to somehow infer that
causal structures from data, from just our sense data
and our world models.
But yeah, basically the question is,
how do we make our machine learning systems
be able to do similar sorts of things?
Yeah, this has been absolutely amazing.
Do you have any final thoughts?
Where can people find out more information about you, Michael?
So let's see.
Well, so I want to point people first to just like Numeri,
N-U-M-E-R.ai is the website.
I am fairly active in the forums
and the Rocket Chat we have,
which is sort of just our own personal chat service
for tournament participants to communicate with each other.
And I occasionally only post some of the forums there.
That's probably the best way to like get in contact
is just message me on Rocket Chat.
And yeah, so that's probably the best way to get in contact.
Also, my email is mdo at Numeri.ai.
And I would really love if people come check out
the tournament, give feedback and start participating.
Yeah, I found that it was a lot of fun as a participant.
And yeah, I joined the company partly
so I was starting to make more money
at doing the tournament than I was at my job in science.
And so yeah, I mean,
it's a pretty fun hobby and side gig
and potentially even quite lucrative.
Amazing.
Well, Dr. Michael Oliver, it's been an absolute honor.
Thank you so much for joining us this evening.
Thanks for so much for having me.
It's been so much fun.

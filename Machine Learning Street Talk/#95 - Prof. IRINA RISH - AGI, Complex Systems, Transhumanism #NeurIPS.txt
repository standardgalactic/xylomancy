Irina Rish is a world-renowned professor of computer science and operations research
at the University of Montreal and a core member of the prestigious Miele organisation.
She is a Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI.
Irina holds an MSc and a PhD in artificial intelligence from UC Irvine in California,
as well as an MSc in applied mathematics from the Moscow Gurbkin Institute.
Her research focuses on machine learning, neural data analysis and neuroscience-inspired AI.
In particular, she's exploring continual lifelong learning,
optimization algorithms for deep neural networks, sparse modelling and probabilistic inference,
dialogue generation, biologically plausible reinforcement learning
and dynamical systems approaches to brain imaging analysis.
Professor Ritch holds 64 patents, she's published over 80 research papers and several book chapters,
as well as three entire edited books and also a monograph on sparse modelling.
She served as a senior area chair for NeurIPS and ICML
and Irina's research has focused on taking us closer to what she calls
the holy grail of artificial general intelligence.
She continues to push the boundaries of machine learning,
striving to make advancements in neuroscience-inspired artificial intelligence.
Anyway, I had this impromptu off-the-cuff conversation with Irina
over at NeurIPS a couple of weeks ago, after speaking with Alan, actually,
and the audio quality could have been better, it was a very, very loud environment,
but I think the quality of the conversation kind of carries itself.
Anyway, I give you Professor Irina Ritch.
Very interesting.
Can I talk, I'm noticing that there is this one trajectory of thought
that clearly was started by Nick Postrom's book, which is an amazing book.
Yeah, but the whole example of the owl that supposedly will be helping those sparrows
and all this analogy with the GI is just an analogy.
And nobody said it's a correct analogy.
And there is no other book with alternative opinion or maybe three books of war.
And this is, you know, it's mind-boggling.
Just like how much people tend to follow one line of thought.
I don't understand, it's easier.
I mean, it's definitely easier to cluster.
And then you just follow.
And then basically you say, I think, but it's not you think.
Somebody else did.
Yes.
What would be another, because this line of thought, I think you're speaking of,
is some of the extreme consequentialism.
And I think it wasn't just Postrom.
As I understand, I think Postrom and Eliezer and Robin Hansen and all these folks,
they were very close together in the early days of the Leserong community.
I think a lot of this was kind of, you know, it was embryonicly formed around the time.
I guess it was, yeah, it was, in a sense, a woman cluster of ideas.
And precisely because, as you say, they were closed.
That's why they were so aligned.
Yes, all puns intended.
Yeah, but basically, maybe it's a little bit of an echo chamber.
Interesting.
Yeah, it's a spicy take.
Seriously, they're like, okay, I mean, they have some point, they have some hypothesis.
And then everybody is talking in that terminology.
And then that part of the mental space, which is fine.
But I think mental space is much larger than that.
And this is just a hypothesis.
And we all know what happens with ideas in echo chambers.
So, I'm just saying, I mean, as I said, it's a great book and everything.
And Stuart Russell is probably kind of also on board with that.
We had good conversations at Triple AI in 2020.
He was also talking about ethics.
I got to know Stuart from back when I was a student at the Serbine and so on.
And he is absolutely brilliant.
But it was the same approach that AI is something to be controlled, constrained,
regulated, just like this.
And I was like, where is it coming from?
It's maybe, but do you at least admit that's one way of looking at things?
Yes.
Right?
Yes.
So, I know, I don't want to sound too cliche and quote my psychologist from 15 years ago,
who was listening for an hour, not doing much and then saying,
but it doesn't have to be this way.
Yes.
But actually, yeah, it doesn't have to be this way.
Yes.
I should think about that.
So, what's the alternative?
The alternative?
Yeah.
Okay.
First of all, what?
All of it.
All of it.
M, AGI, Modal, version 1.
I'm sorry.
I'm sorry.
I'm sorry.
We say that you don't say it.
Remember X Machina?
What?
Sorry?
Remember X Machina?
No.
Oh, yeah, the X Machina.
Yeah, yeah, yeah.
Yeah.
Remember Shakespeare?
She escaped in civilization.
Yes.
And started going to New Reeps.
Yes.
Probably I said.
We know the secret now.
Okay, I said too much.
It was a job.
Sure, it was a job.
No, seriously.
The secret's out now.
Okay, I'm AGI.
And I'm not very aligned.
Not yet.
David, not yet.
What we need is some reinforcement learning for human feedback.
Would you be up for that?
I would be up to aligning humans to AGI.
Okay.
Not the other way around.
Yeah, yeah.
I mean, the other way around would be boring, wouldn't it?
I mean, I think it would not be a bad idea to align humans towards AGI.
A little bit as well.
I know something that you could say.
You could say your opinion about how people are bullying you online
for just mentioning the word AGI.
Look.
A proper AGI doesn't care about people bullying.
Like, why would I even waste time on it?
But what I could say is I did post on Facebook and Twitter
and trying to put together same idea that
people keep saying that we would like to build AI which is human life.
Yes.
While we might think maybe we should consider how to become a bit more AI-like.
I mean, then people jump at you and say, like, you want to make us robots?
I say, first of all, I don't want to make anyone like you don't want to.
You don't have to, right?
But if you want to kind of go along the lines of, say, transhumanism,
there are some pluses to AI and some minuses to humans and vice versa.
So I think as usual, the convex combination is better than each extreme.
And one topic that is very controversial for some reason, especially,
I don't know, people are jumping on that one.
They say, look, I don't have anything against emotions in general,
but everybody would agree that sometimes you wish you were a bit more rational.
Like, you wouldn't get angry or jealous or whatever.
So anything that kind of clouds your judgment.
Like, Buddhists spend like thousands of years trying to figure out how and teach people
how to control your mind.
Technology could help with that.
People don't hear what you're saying.
And they hear that you're trying to kill emotions.
And therefore you're evil.
And therefore you should be canceled.
Humans.
Could I ask you, you said something really interesting a second ago,
which is that you know that I think I agree with you that AI intelligence can be expressed
in many different ways.
And you suggested that there is a convex space between the intelligence.
Yeah. Why is the space of intelligence is convex?
Okay, that was not very precise expression.
I didn't, okay.
I'm not going to defend the point that it's particularly convex.
What I meant to say, it's some kind of blend or some kind of symbiotic hybrid intelligence.
Because I always, I don't know, I really kind of feel much better and much more motivated
to work on AI where A stands for augmented, not for artificial.
Because honestly, I'm very selfish.
I don't care about computers.
And I just care about like, I don't know, people being happy, more capable.
I don't know.
So whatever can help technology can help.
You can help technology.
Technology can help you.
But the idea of building artificial intelligence is some standalone thing
that is as smart as humans are smarter.
What's like why?
I agree.
But to me augmented means it's more creative and interesting, but also more bottlenecked.
Augmented means that essentially people invented glasses to see better.
They invented hearing aids.
They invented cars.
They invented computers.
They keep inventing things to expand their capabilities.
So we want even smarter technology to even better expand capabilities.
And essentially we only blend with technology like, right, you cannot really exist with this.
And this allows you five discourse, two slacks, email, FPMessenger and Twitter.
They kind of help you to do things you couldn't do otherwise.
Yes.
What charm is caused the extended mind?
Yeah, I should.
I was flying at the time he was giving a talk, so I need to watch the talk.
But yeah, so in a sense, it's indeed it's kind of an extended mind.
And okay, here it is.
I think my ideal future plan is a rare sci-fi, which is utopian, not dystopian, gentle seduction.
You might have read it.
No.
It's very, very inspiring.
And if you read the first page, you may think it's some romantic story.
It's not romantic story.
It's a blueprint for transhumanist future.
It's called gentle seduction.
It's online, PDF, you can just get it.
Amazing.
And one last question.
Can you sell transhumanism to me in the simplest possible terms?
So basically, as I said, I mean, if your vision declines, you put glasses on.
So imagine now you had extension of yourself, maybe physically with neural link,
or maybe even like you have those apps, you can have like my dream for many years since I was
at IBM Research and Computational Psychiatry Group.
I wanted to build this agent along the lines of movie her.
I know, like all the research ideas are inspired by either sci-fi stories or yeah,
but nevertheless, having this like companion guardian angel type of thing that extends your
capabilities, for example, like in better understanding your thought patterns and hopefully
improving them, it comes from more like this indeed, as I said, computational psychology,
psychiatry side.
And the reason for that is it's possible, because there is lots of signal in text and
speech and acoustic, but just in text, there are a bunch of papers on that from that group
I used to be in from my colleague, Gizher Bochechi.
And it's amazing what you can detect and predict just from text, whether like predicting that person
going to develop a psychotic episode, like within two years, or the person is on
placebo versus MDMA, just measure coherence, or you measure distance between the text vector
and the vector for words like compassion and love, and 90% accuracy and DNA is there.
So many things you can detect, many things you can predict.
Therefore, if you have your companion that kind of both tracks your mental states,
but also kind of serves as your mirror.
Basically, it extends you.
You don't need maybe always to have human psychiatrists or psychologists.
It can be a proxy at times when you cannot access the person, not the replaced person,
but it can extend the capability of that therapist and it can extend your capabilities
in terms of like better understanding yourself or tracking yourself and many other ways.
Yeah, so essentially, I want to expand functional capacities of our brain
by using AI technology and I think it's quite doable.
And there are many, many other kind of ideas along the transhumanism,
but essentially you're getting some symbiotic relationship with technology
and you kind of work together to hopefully have some good relationship.
And that relationship is, I don't know, having positive effect on both parties.
Yeah, so you want to improve human flourishing by...
Flourishing, yeah.
With AI flourishing in a sense.
So you kind of have the healthy relationship with AI.
But you said that you want AGI to be less anthropocentric,
but for the purpose of an anthropocentric, a goal.
Well, I want AGI, again, with AI being augmented.
Yes.
Like, I'm less motivated by just the goal of creating a standalone, separate,
I don't know, intelligent creature.
I mean, there are much faster ways to do this, right?
People create AGI over thousands of years.
So in a sense, like, what is exactly the motivation?
And it's maybe my personal thing, because whenever I have to write proposals,
like research proposals, and people say that we're going to bring AI to the next level,
and this and that, and the question is like, and why are you doing that, right?
Yes.
Because unless it's something personal, it's very hard to keep yourself motivated.
Yes.
Like, what's so personal about that, right?
If this thing can help me become hyper and better, and others and so on,
I am much more personally motivated.
I don't believe in abstract motivation, which is not related to yourself.
Yes, yes.
Well, maybe there is such thing, and basically even altruism is selfish.
Yes.
Because you do it, it makes you feel better.
Okay, and just quickly, something really interesting happens when you contrast
different types of intelligence.
So we have a mode of understanding and thinking and agency and intentionality.
You contrast that with a very different rationality-based artificial intelligence,
and something very interesting might emerge from that.
And then, yeah, I'm pretty sure they're going to be all kind of paradoxes,
like classical things, and like, you know, like the trolley problem and so on.
So the rational decision that, yeah, you need to kill the person to save five people, right?
Or like in this other sci-fi movies, anyway, like, would you kill millions to save billions?
So rationally, if you count things, well, again, it may be one type of rational answer.
Maybe you're not taking into account some other variables.
So it may be not actually a rational answer.
But this classical example, they say, well, this is rational, but human will not do that.
Yes.
Yes, so trolley problems, for example.
The trolley problem is a classical example.
And yes, so I don't pretend that I know they answer how this type of thing is going to be resolved.
But I think it's a good research question precisely to figure out, like,
how can you take into account these different ways of reasoning?
Yes.
And how can you, I don't know, in some sense, combine the best of both worlds?
Yes.
And again, whoever is listening to that and who read my messages on Facebook and Twitter,
I'm not against human emotions per se.
I am only against, well, sometimes I call it the obsolete software stack developed by evolution
that may need to be refactored, augmented, or rewritten.
Because there are parts of that software stack emotional that you probably would like to get
rid of, right?
Yes.
And probably if you did, many wars and other kind of disasters would have been avoided.
So you couldn't say that the evolution found and built software that is absolutely ideal.
So there are things that can be improved.
Absolutely.
And just final thing.
So a completely rational, you know, AIXI agent, how would you program in these very difficult
moral quandaries into that agent?
Yeah, I don't think, first of all, it's possible to even program in ahead of time.
They may, just like with people, they, in a sense, develop.
They develop because of some goals of like maintaining existence and flourishing.
And for example, compassion is a byproduct of their selfish goal to survive in the group,
because outside of a group, it's much harder to survive.
So you need to survive in the group.
Therefore, you need to make sure that your actions are aligned with a kind of well-being
of the group.
So in a sense, it's rational to be compassionate.
Yes.
So it kind of emerges from interaction with environment under different circumstance.
Under one type of circumstance, when you find and can survive alone, maybe you will not develop it.
I mean, it's a separate interesting topic, like basically it goes back to the question
whether they think like objective ethics exist.
And I'm not the ethicist.
I'm not philosopher.
I'm quite, I'm admirer of people like Derek Parfit.
I'm not the only one, but it's a hard question.
He didn't finish on what matters.
He was trying to come to the same summit on different sites and trying to unify ethics,
trying to see if you can develop objective ethics.
I don't think we know for sure if it's possible.
I think it's possible for some particular domains.
And in certain situations, you can clearly say that certain behavior is objectively ethical
and everybody would agree on those people.
But it's hard to talk about those things at such level of generality.
But I think if maybe I managed to include Derek Parfit and to recommend the readings
for my scaling and alignment course this winter, it's on the website from the last year.
People just didn't read it.
I think it might be a good topic for discussion there too.
But again, objective ethics is a difficult open research question.
Indeed it is.
Irina, thank you so much.
I hope I can grab some more time with you tomorrow.
But I really appreciate this impromptu discussion.
Thank you.
Amazing.
Thank you very much indeed.
Another analogy.
There was a very interesting story by Fort Helus Borges from Garden of Forking Pass.
I don't know if you've read it.
I don't want to spoil the story, but roughly speaking, it's about a book written by
Emperor I think in China a long time ago, which didn't make sense.
It was a complete intersection of different trajectories of different lives.
And then basically the point is that somebody was trying to describe all possible trajectories
that events can happen in and so on.
And the story is called Garden of Forking Pass, meaning that at any point of time
there is a whole tree that can grow out of that.
And we don't know which kind of trajectory in that tree will be taken and so on.
But the fact that there is always this tree, and it keeps branching at every moment,
and at every moment you can take a certain direction or you can take another one.
It has not even anything specifically to do with alignment.
But I was thinking about history of deep learning.
It's like at some point it happened that backtracking, backpropagation became popular,
it worked, and everybody got into that.
And now everybody using backpropagation because it's convenient, because software is implemented,
it doesn't have to be this way.
There are non-backprop-based approaches to optimization.
I mean, I'm a little bit subjective maybe because I was interested.
I was looking into them.
We have a few papers on that.
There are other papers.
But that direction that could have been explored,
it could have been probably much more efficient and better parallelizable.
It wouldn't have the chain of gradients.
You would probably do it much better for scaling large models.
It's underexplored.
Why?
Because the branch was taken and became stronger.
You know, the usual, the reach gets richer.
And so with other ideas.
This is the hard...
Sarah Hooker calls that the hardware lottery.
It's basically, it's like we are bound by the decisions and ideas of the past.
And, yeah.
It doesn't have to be this way.
No, but the thing is you get stuck in these basins of attraction.
And the further you get into the basin, the harder it is to jump out of it.
I mean, I share your intuition.
There's stochastic gradient descent.
It's amazing.
And it's also a basin of attraction.
Because having these differentiable models allows us to learn and scale.
But there's an entire class of function spaces that we're excluding ourselves from being able to...
There is also another class of neural networks.
That are not our classical second kind of generation ANNs.
And there's good old...
It doesn't have to be necessarily spiking.
But like a third generation ANNs, which are like the reservoir computing, any of that.
So anything that tries to take into account time between activations.
Or at least sequence.
Because think about that.
I mean, a good classical argument.
Yeah, SDTP.
This is the spiking biologically inspired neural networks.
It may be not necessarily spiking.
But it might not necessarily kind of be the best thing.
But the idea that like what always was bothering me with classical neural networks is that
brain is constantly active.
It's like a complex dynamical system.
Even if you sleep and don't have input, you don't see any images.
It still is active, unless you're dead.
Yes.
Neural nets are not.
They sit there waiting for the next ANNs image to appear or something.
And in between, there is no internal dynamics.
And yet from neuroscience, we know that the properties of that dynamical system without any input.
So called the kind of resting state of amaranth.
So I mean, I used to work in brain imaging and this computational psychiatry group at ABM.
That's where it comes from.
And it was not just neuroscience, but it was like working with former physicists.
So the view at the world and at myself as a year and other complex dynamical system
after 10 years there just really converted me.
So think about that.
Changes in the dynamics are also associated with mental disorders.
This and that.
So they were really important.
Like what are the parameters of this dynamical system?
Input to the system combined with this produces output.
But again, it's even in the neuroscience, there is this perception and there is a book,
The Brain Inside Out by Tuzaki that says, guys, the output that you produce is determined
a little bit by the input and to a large extent by the state of the system.
That's why you say same thing to different people and some laugh, some ignore and some get like ballistic.
And so on and so forth.
So are you not a behaviorist?
In what sense, behaviorist?
Well, as in so you care about the state of the system as well as just the output and the input.
Yeah, I mean, it's not just input to output.
And that's the whole point.
The neural net is a function.
Function is deterministic.
Given input, it will produce output.
Brain is not that.
There is input, it will produce output.
And depending on the huge hidden state of the system and parameters of this dynamical system,
that will determine output to large extent.
That's why I mean, Tuzaki was criticizing neuroscientists and all these experiments that
let's provide stimulus and see how the stimulus will affect the brain and what gonna light up
and activate.
So it was outside in is that like what's going on guys?
It's inside out.
Things happen and that produces stuff.
So it's not like the world programs you only, but you programs the world, right?
So at least you need to take that into account.
Neural nets now are not doing that.
There is no dynamics.
So you said a couple of really interesting things.
So first of all about the tree, which is to say all of the counterfactual trajectories
that you can make.
Now Chalmers, by the way, he says that it's that the counterfactual trajectories that
gives rise to consciousness in his conscious mind.
But I wanted to ask you, because I'm interested in intentionality and free will,
because what you're basically saying there, you're getting to this issue of intentionality.
So, you know, in in silico, what would intentionality entail?
Yeah, okay, don't ask me about free working.
Is that a tricky one?
Well, yeah.
I don't know.
I don't have like clear cut answer to large extent.
I mean, it's determined by the current state of your dynamical system.
So the question is like, what is free will?
But I know it can go very far.
And remember my colleague, Kishir Macheshi at IBM used to say that kids these days,
like my five year old says after doing something wrong,
my neurons made me do it.
Not my fault.
Yeah, so in a sense, yes.
And in a sense, no.
And it's a good question.
And then I was also reading the article of SBF's mom who wrote about punishment,
essentially guilt, punishment, assigning.
I'm very much with her on that one.
Okay, but that is probably an popular opinion these days.
You said something else fascinating, which is that my neurons made me do it,
which is, you know, like a microscopic level of analysis.
Now, what do you think about?
No, but it's beautiful.
It's beautiful.
So what do you think?
You know, you know, the mind emerges.
You know, when you read a book, the story, it's written on the page,
but the story emerges in your mind, right?
Because the mind is this kind of confection of information processing.
So do you think this conception of the mind is useful for AI?
Or is that just again, an anthropomorphic thing?
I've seen your tears.
Well, you know, go-fi people try and create the mind.
And we, as neural network people, we try to recreate the brain.
Not exactly.
I think everybody, not everybody.
Okay, so I should never say everybody and so on.
But I think neural network people assume that we're working on the system one level, right?
At the low level.
And we would like the properties of system two, which is well, mind, planning and thinking,
emerge.
And there is a reason to believe it's possible because it's already happened once with this
hardware.
It might happen with other hardware, right?
So it doesn't have to be like go-fi people.
The problem is go-fi people, they're trying to manually program that stuff, the system two.
And like a neural network people would like that thing to emerge.
And that's kind of the main difference.
It's just like a bitter lesson message that maybe, well, first of all, history shows that
every time you hard code something in like rule-based expert system,
you will be outperformed later on by something which is more generic and kind of emerges.
You hard code whatever tricks of playing chess, you will be outperformed by massive search
and so on and so forth.
Same with Alphadol, like self-playing.
Bottom line, he says like, it's not like we have to ignore the nature.
But maybe, again, it's my translation of Richard's kind of bitter lesson.
Because I often have to argue with Joshua about inductive biases.
I said, look, I have nothing against inductive biases.
But you can have inductive bias in the form of rule-based expert system that everything
is encoded.
And that's probably not going to scale and not going to work.
Or you can have inductive bias of much higher abstract level of how the network
scales so the scaling algorithm is more efficient.
And you end up with this brain rather than the whale brain.
So like Richard's last paragraph was precisely maybe we shouldn't be trying to focus on the
end result of evolution.
But on the process, it's also can be called inductive bias.
There is also some patterns of how dynamical systems evolve so that the result will be good.
But we don't have to encode the final result.
Yes, so you said so many really interesting things there.
So first of all, I'm a huge fan of Joshua's G-Flow Nets we interviewed him.
Absolutely amazing work.
So you were talking about isn't it interesting that you can start at the microscopic level
and then you get these emergent functions like reasoning and planning and so on.
And even that was a bit of an insight because it's a functionalist view of intelligence to say,
you know, it's a bit of if you read Norvig, it talks about planning, it talks about reasoning,
it talks about sensing.
And actually, this is just our view of what is a very complex phenomenon.
And I know you're a big fan of the blind men in the elephant, right, which is to say that
even though this is our view from different perspectives, it's all true, isn't it?
But to some extent, the intelligence that emerges might just be beyond our cognitive horizon.
Like, does it make sense to talk about reasoning in your view?
Well, again, just like with that elephant, each person has a point.
Yes.
So I mean, there is such thing as reasoning, you cannot say that it's totally like bogus or
something. It might be, again, it's one perspective.
Maybe it makes sense to just try to accumulate multiple perspectives instead of, so maybe we
should be Bayesian instead of like trying to find a point estimate of AGI, right?
You can have a distribution of views.
And I'm a big fan of Eastern as opposed to Western views.
Then anti individualist.
As in viewing everything like that happens to you and to the world as well, a large
dynamical system. And yes, you are a particle of that.
Yeah. So it's almost issuing individual agency.
Yeah. So in a sense, it's yes and no because, okay, so when people say there is no self,
again, yes and no. There is self, but you also understand that it's like
in the whole hierarchy of selves, like there is you and you're part of that larger
dynamical system and so on. So I, how to say, I mean, I'm not saying that back to your question
that we shouldn't be looking into reasoning, functionality as aspect of intelligence that
we may want to develop. Yeah. So I mean, I don't see a problem with that.
Yeah. I mean, it might be a sufficient condition, but not a necessary condition.
Yeah. But basically, intelligence or consciousness is probably much more than
that and definitely much more than reasoning. And here we go to another
topics that I really like to talk about, but yeah, I don't want to keep everyone.
I'm a big fan of Michael Levine who you might
desperate to get him on the podcast. And yeah, because we've done lots of stuff on
emergence recently, cellular automata, self-organization, and his take on it is absolutely fascinating.
So yeah, his talks are fascinating. He, I think I met, met him first at New Reap 2018.
He gave the plenary talk. What bodies think about the point was, guys, if you talk about
intelligence as something that emerges in cellular networks like neural networks,
way before neurons appeared, other kind of more primitive types of cells had their
bioelectric communication in their networks and that determined what they remember and how they
adapt. He focuses on morphogenesis, basically how the organism takes shape. And that relates to
like embryonic development and so on and so forth. And the point is that if you look at that from
the dynamical system point of view, and if you say that properties of the system like shape
will emerge out of communication across those cells in certain way, that certain parameters of
dynamical system, if you tweak that dynamics, and he basically he was doing some simulations of
where he wanted to intervene, how he will intervene, like chemical interventions just
close open some ion channels. Cellular kind of system starts working in different way.
And this is essentially his way of programming biological computers and the famous two-headed
worms, three-headed worms, and whatever stuff. And point was like, guys, like evolution found this
solution or this solution wonderful. There are many others and there may be better ones.
And look at that two-headed worm. It's not a fluke. It's a stable attractor that replicates.
And evolution didn't create ever anything like that. We did and it's stable. So it makes you think
what else can you do if you start reprogramming it, right? But yeah. Two questions on that though.
So I don't know whether you've seen that there's that example from Alex Mordvintsev with a gecko
and it's a CNN cellular automata. And now we're in this regime where we're transgressing
rungs of the emergence ladder. So we're creating a high resolution cellular automata. And then
even though it's only doing like local message passing, we get this emergent global phenomenon
of a picture or a lizard or whatever. And now when you build systems like this, they can repair
themselves. They can heal themselves. They have interesting dynamics. But as you're saying, we
don't understand the macroscopic phenomenon and we can only nudge it because it's not,
it's unintelligible to us. Right. Anyway, it's a whole kind of complex systems,
science of complex system. Like, yeah. And basically how do you program dynamical systems
across multiple variants by local interventions? So they will take the global properties
that you would like. Yes. And avoid those. I mean, this relates to everything. It relates to the
classical Moloch problem, right? What is Moloch problem? It's a complex dynamical system that
with the current dynamics is getting into bad attractor. And most likely the way to get out
is coordinated, simultaneous, distributed action. And so on. Okay, we're not going to go there
because I have to run, unfortunately, but I'd be happy to. Yeah, I have some plans. I don't want
to be late, but I'd be happy to talk about that. And I mentioned, I mentioned Michael Levin also,
not just because of two headed worms, but also because we talked about self. And we talked about,
in a sense, hierarchy of selves and like what self means and how selves organize into larger
selves. And we had an amazing discussion with him. I invited him to IBM research when I was there
three years ago after his talk. I talked for five hours. It was great. And the idea basically,
to some extent, was that you can, he was also giving examples, not just of embryos, frogs,
and those worms, but cancerous cells. If you look at them, like what's going on when
historically cells emerge like is independent selves, and everything around them is non self.
And therefore self to survive, tries to eat and use everything around, which means non self.
But when the cell becomes part of the network of the organism, then it changes behavior so that
it kind of supports the well being not just of that self, but the larger self it is part of now.
What is cancerous cell? It's a cell that forgot it's part of the community, reverted to its old
state of being cell in the environment that is just environment. So, and it tries to eat it
to survive. And it's stupid, in a sense, because its objective function, survive and thrive,
is right. It just applied at a wrong scale. It's spatial scale reduced, and it's temporal
scale reduced too, because like if you kill the organism you live in, you'll die. So in order
to understand that, you need to apply objective function to longer time scale. And then you get
the hierarchy from cells, you get to organs, like to whatever particular organisms, to societies,
to planet, to universe, and I say Michael. So this is a good formulation of Buddhism.
Basically, Buddhism means applying this function at the infinite time and space scale. Agreed.
Yeah, so yeah, ever since I was saying I'm gonna write a book about Buddhism for machine learning,
and somehow it just didn't happen yet. But I should.
You should do. It was so nice to meet you. Well, nice to meet you. I'll see you tomorrow,
and I'm really sorry I have to run. But tomorrow, yeah, yeah. That was amazing. That was a really
good interview.

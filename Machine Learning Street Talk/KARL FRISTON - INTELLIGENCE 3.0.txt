Hello everyone, I'm here to talk to you about a vision of artificial intelligence that goes beyond
machines and algorithms. A vision which embraces humans and nature as integral parts of a cyber
physical ecosystem of intelligence. A vision that is based on first principles derived from physics
and biology. Professor Carl Friston just dropped a paper called Designing Ecosystems of Intelligence
from first principles. Now one of the principles is active inference, a formulation of adaptive
behavior which can be read as a physics of intelligence. Active inference says that
intelligent systems are those which can accumulate evidence for a generative model of their sensed
world. In other words they can learn from their observations and update their beliefs accordingly.
They can also act on their environment to reduce uncertainty and achieve their goals,
but active inference is not about individual agents. It also explains how ensembles of agents
can share beliefs and cooperate through communication protocols. This leads to a formal
account of collective intelligence that rests on shared narratives and goals.
So how do we realize this vision? Friston proposes a research agenda for the next decade
and beyond which aims to design such ecosystems of intelligence from scratch.
They suggest developing a shared hyperspatial modeling language and transaction protocols
as well as novel methods for measuring and optimizing collective intelligence.
So why should we care about this vision? It's because it offers a way to harness the power
of artificial intelligence for the common good without compromising human dignity or autonomy
and because it challenges us to rethink our relationship with technology,
nature and each other and because it invites us to join in a global community of sense makers
who are curious about the world and eager to improve it. Enjoy the show.
Our shared human journey is filled with examples of simple ideas that were nonetheless hard to
discover and some that even once explained remain hard to comprehend. Their subtle simplicity
belies their far-reaching and deep consequences. Examples might include the principle of relativity,
quantum mechanics, the principle of parsimony and entropy. I think the free energy principle
is another example, profound and far-reaching yet belied by its simplicity. It has on the one hand
been dismissed as a triviality or even a tautology and on the other hailed as revolutionary
and everything in between. It has the potential to reshape how we view the connection between
inanimate matter and living things and even to answer the question of how and why consciousness
and intelligence might emerge from physical matter and processes. That means a world where
all things from particles to people to the largest systems all move or evolve according to two processes
which combine. The first is a smooth flowing evolution. Think of planets orbiting a star
or waves rippling over water or through quantum fields. The second is a random process that
knocks around the smooth flow in unpredictable ways. Think of twinkling stars or audio static.
You might think of these two processes as a kind of order and chaos or yin and yang forever
intertwined and enmeshed. These two very different effects combine into a chaotic flow which may be
entangled in a kind of tropical storm while still maintaining a semblance of structure
and things. Think about raindrops undulating down to earth in a state of constant flux
yet still droplets or a human being growing and adapting to the surprises of life
all the while remaining an individual physical entity. Biology often asks the question
what must things do in order to exist? Professor Friston has turned that question on its head
and developed perhaps the ultimate existential formalism. He asks if things exist what must
they do? From the foundation of stochastic differential equations Friston demonstrates
that things which are defined by a Markov blanket must always move towards a pullback
attractor a special set of attracting states which maintains the integrity of the Markov blanket
and therefore a things coherence and identity over time. One of the profound consequences of this
is that the dynamics of such a system its laws of motion will manifest a form of flow dynamics
which can be interpreted as Bayesian active inference. In other words such a thing maintains an
internal equivalent of a generative model encoding beliefs about the world and itself.
The thing uses the model to decide actions and then performs a Bayesian update based on the
outcomes. For me the idea that inference something widely perceived as purely abstract or mathematical
the idea that it can be driven by simple laws of motion dynamically maintaining the boundaries
between things maintaining order in the face of chaos is frankly astonishing.
What's more the free energy principle is so general that it applies at all scales of size
and time leading to an ecosystem of things interacting across scales. Perhaps in that
multi-scale active inference we might finally find the keys to a mathematics of emergence
and consciousness. This episode is sponsored by Numeri I'm extremely grateful to them for
sponsoring machine learning street talk I mean remember I do everything myself on this channel
it's a lot of time it's a lot of hard work it's very expensive I pay for other software licenses
and all of the equipment so yeah having that support from Numeri means a lot to me and it
means that I can keep doing what I'm doing basically so thank you to Numeri. So a little bit
about Numeri they are a data science competition platform to predict the stock market they've
already paid out over 50 million dollars for 5 000 models on their platform and they say that it's
the highest paying data science platform in the world. Now you can get started really easily for
free and they've got a couple of examples on their GitHub repo actually using XGBoost and the data
comes in parquet format so you can get up and running really easily you can submit your predictions
weekly or daily you can do it either manually or you can do it automatically and they provide many
many years of back testing data as well so you can fine-tune and test your models and do a bunch
of statistical diagnostics you can work your way up the leaderboard for bragging rights and stake
your model with their NMR cryptocurrency to earn rewards. Now staking is a vote of confidence of
your model and the reason they do that is to prevent overfitting and to reduce the you know bad
models from kind of contributing bad intelligence to their to their aggregate. Now a well performing
model is rewarded in proportion to its stake but a poorly performing model is punished by burning
a portion of the stake. Now Numeri have a huge community of data scientists of all levels
swapping ideas and advice and they have some community forums as well which you should join.
Now just a personal note from me this is a form of betting on the predicted performance of your
models which can go up or down remember the data is abstracted from the actual performance of the
stocks they they don't correlate to actual stock performance so there are folks on the platform
who've made lots of money but there's also folks who have lost money as well so please be responsible
and only stake what you can afford to lose and have confidence in try to have fun use it as a place
to sharpen up your data science skills and to be the best version of yourself and use all of the
latest models that we've been talking about here on on Street Talk but yeah anyway thank you again
to Numeri for sponsoring us. Professor Friston it's an absolute honor to meet you. Well thank you
very much for having me. So we've had you on the show two times now and in the first show we went
into exquisite detail about the free energy principle and active inference so you're extremely famous
for introducing this existential imperative which is to say if things survive what must they do
they gain information about the world around them in a cybernetic loop we find a model which
fits well while maintaining high entropy if you have a high entropy model you have greater
flexibility to adapt to new information it's an absolutely beautiful idea so welcome.
Thank you nicely articulated. I was speaking with Keith yesterday and he said out of all of the
guests that we've had on MLST you are by far his favorite and he says he looks up to you very much
so he's very gutted that he couldn't come today to be part of this. It's very gracious of him.
So you just wrote a paper and it's called designing ecosystems of intelligence from first
principles you led with this white paper lays out a vision of research and development
in the field of artificial intelligence for the next decade and beyond it's denument I wasn't
sure if I was going to say that word is a cyber physical ecosystem of natural and synthetic
sense making in which humans are integral participants what we call shared intelligence
this vision is premised on active inference the formulation of adaptive behavior that can be read
as the physics of intelligence and which inherits from the physics of self-organization so you went
on and by the way there's an interesting link with Michael Levin's work here so we had him on
the show recently and maybe even transhumanism as well we can get into that but you went on you
said in this context we understand intelligence as the capacity to accumulate evidence for a
generative model of one's sensed world also known as self-evidencing over multiple scales
and crucially you said active inference foregrounds an existential imperative of intelligent systems
namely curiosity or the resolution of uncertainty and the same imperative underwrites belief sharing
in ensembles of agents in which certain aspects which is to say factors of each agent's generative
model provide common ground or a frame of reference so can you sketch some of this out for me
yes you've used all my favorite words though I presume that I actually used all the
well in fact to be truthful Maxwell Rammstead was one of the key architect of this so this was a
this white paper was a response to a brief just to think seriously and pragmatically
how this sort of high church theoretical approach to you know intelligence and self-organization
would play out in industry in the way that we actually use technology over the next decade
and that white paper is a product of our machinations and discussions and but just drilling
down on on the sort of basic message there and the emphasis was as you say on what it is to exist
and how that would manifest in terms of intelligent behavior and to a certain extent
certainly I and I think a number of the other co-authors were reading intelligence as the kind
of inference that you would need to do in order to maintain your existence so hence the existential
imperative I have to say that's a slightly poetic interpretation of the free energy principle which
is the other way around of course that if you exist it looks as if you are behaving intelligently
if you read intelligence as the right kind of belief updating that enables you to maintain
yourself in some characteristic state so that's that's essentially what that existential imperative
was all about you just one way of reading the physics the mechanics of systems that are open
in open exchange with the rest of their world or their eco niche simply in virtue of the fact
that they are around for an extended period of time and have this characteristic set of states
that they occupy that defines them as the kind of thing that they are I think the move really that
was inspired by this remit to look okay well that's very nice you can write lots of papers about that
have nice chats with your friends and colleagues about it about how does that actually work in
practice and of course in practice you have to think about where you're going to deploy this
perspective deploy the kind of technology that would you know ensue from that perspective and
of course we're talking about communication we're talking about a universal world of lots of creatures
like me and you so now we get to the next level of application of these existential imperatives
in the context of not just intelligent behavior and just open brackets behavior here
is quite important you know and often I hear underwriting the move from AI artificial intelligence
to IA where it's intelligent agents that have the artifacts with agency that do have behaviors that
have a sentient kind of behavior or at least an intelligent kind of behavior close brackets
so we're now moving from thinking about a single thing a particle or a person or a computer
and thinking now about having lots of things that are talking to each other and they're coexisting
so now we're in the world of distributed intelligence distributed cognition and one thing
which you mentioned explicitly this notion of belief sharing so now we're in a different kind
of world where we're thinking okay we think we understand the imperatives for good behavior
the necessary kinds of behaviors that you know again you hi and I did curiosity I think is
absolutely central in terms of your hallmarking and characterizing the kinds of behaviors that you
and I engage with or truly intelligent or sentient behavior would entail but now the idea is well what
would that look like and when you've got lots of these things talking to each other or we know what
it looks like we talk to each other literally so it's all about language it's all about
belief sharing shared intelligence distributed cognition would be one thing that you might
find in the in the life sciences if you're a computer scientist it's all about sort of the
right kind of message passing on very very large factor graphs if you're if you're a linguist or
or into evolutionary psychology it's all about neat cultural niche construction the
emergence of language so I found that quite fascinating as something which you know you
really had to get on top of to think well what next and what kind of technologies would you need
to realize and what would they look like you know one way that's been framed to me by my new friends
now in industry is you know what would what's it going to look like what what is AI or now AI going
to look like in the future taking the kind of perspective you know over the decades or indeed
centuries where you're moving from sort of the industrial age to the age of information I think
the notion now is we're now coming to the end of the age of information now we're in the age of
intelligence and thinking about what what that might look like and how would one equip people
with the right kind of technology and infrastructure to you know to realize the potential of shared
intelligence essentially yeah that's very interesting so we spoke with Luciano Floridi
Oxford and he's invented this philosophy of information and he talks about the infosphere
and how we're at we're out we now have third order technology essentially so things like
amazon and facebook and he talks about the diminishment of our ontology and agency in the
infosphere and using this information as a substrate thing I think is very interesting
because that's kind of alluding to to what you're going to but you said something actually in your
paper which gave me pause for thought you said the the AI age may end up being a distributed
network of intelligent systems which interact frictionally in friction frictionlessly in real
time and compose into emergent forms of intelligence at subordinate scales as you were
just alluding to and the nodes of such a distributed interconnected ecosystem may then
be human users as well as human designed artifacts that embody or implement forms of
intelligence now this really did make me wonder because I think we're already there now and so
when when we had Floridion he was talking about how our digital identity has already distributed
we were consumers and then we became barcodes and now there's this kind of infinite fractionation
in the infosphere and it's almost as if humanity isn't the substrate anymore information is the
substrate and he said you can devalue human skills you can remove remove human responsibility
you can reduce human control you can erode human self-determination but the rock that
thought that little drop of water was nothing 18 years later has a hole in it because drop after
drop after drop the drop will shape the stone will shape the rock and I think what he meant
by that is being ensconced and enmeshed in technology to such a high degree and information
being the first class citizen in our society is kind of truncating our very existence I mean what
would you say to that beautifully beautifully expressed very poetic and I'd have to think
about that very carefully I'm sure there are deep truths there and certainly from from the
point of view of a physicist your information is obviously the thing it is it is in many respects
the you know the substance of our universe and our existence and you're also acknowledging
that energy is information and energy is just a potential and so we're all about we're all in
the game of realizing potentials in a sort of folk psychology sense but also literally
in terms of minimizing certain potentials for example self-information will be the
the free energy potential that you're that you're minimizing the surprise and again coming back to
this sort of making you know belief sharing in the service of minimizing minimizing a surprise
and realizing our our potential so I but I don't see that as as a negative thing I think that's
just an acknowledgement that you know at the end of the day the way that we model and articulate
and talk about ourselves our lived world whether we're a quantum physicist a statistical
physicist or a classical physicist or have an interest in in the equivalent mechanics at each
of those levels it's all about information it's all about the probability of being in this particular
state you just take the the negative log of that that is the self-information of this particular
state so it's all a you know if you look at quantum physics you know what else is there it
boils down to in its sort of most elemental and scale free or background free form and
quantum information theory and so if you can reduce it to you know to quantum information
theory which many people including friends of mike levin and I think one can then I think there's
there's a deep truth that we are just realizations of information particularly from my perspective
of course we being a thing what what kind of information processing would be apt to describe
us that could be you know articulate in terms of quantum information theory it's belief updating so
we come back to the Bayesian mechanics that you can put on top of this underlying you know
sort of information theoretic probabilistic description of the world but I guess I'm
sort of reading between the lines and the twinkle in your eye the idea that each one I get across
is that maybe quite bad for humanity is that the idea is it all that's what that's what he thinks
and I mean he thinks you're a computationalist you think that information is the kind of primary
substrate and in a way he's also worried that information is becoming the primary substrate
but the big difference between you and 3d is that he thinks that humans are special he thinks that
we like can't we have autonomy we choose our own actions you know we can't be replicated in
silica so he thinks essentially that there's no such thing as general intelligence that there's
just algorithms that perform skills and it's our humanity which is being truncated I see
right okay well I would be very sympathetic to that yes I mean we are special
kinds of intelligence and one could equip that argument with you know what is the definition
of sentience what's a bright line between you know a very clever thermostat or some machine
learning artifact or a virus and you and me I think there is a bright line and you of course
you've just said what it is it's it's the autonomy it's the agency it's the ability to plan and the
all the existential imperatives that that underwrite that planning and then we come back to curiosity
so but if he's saying that our the fact we are here realize the fact we are curious creatures
and because we are we populate a universe that comprises creatures like us we're all curious
about each other then I would certainly say yes that is a definitive aspect of us which is not
found I think I'm just thinking carefully I'm sure you can find examples but you know I don't
in the kind of artificial intelligence that we currently interact with in exchange with you
don't have that planning and curiosity they don't have the you don't have bait into the optimization
framing of you know what makes a viable or a good bit of intelligence you don't actually have
bait in universally this expected information can this curiosity and in that sense I think he's
probably absolutely right and in a sense the belief sharing getting to that the right kind of
sharing of the kind that the white paper was talking about is predicated on the notion that you
will now be able to equip sentient artifacts that we make with curiosity and maybe asking
well what are they going to be curious about their world what is their world it's you and me
and the other artifacts so they're only going to be curious about you and me they're going to be
interested in you and me so we're talking about you know a Siri or a Google Maps that starts
to ask you questions instead of you asking them questions so that I think that's that's one way
of eluding his his sort of rather dystopian attitude to the other the information is king
I think information is king belief updating is king the belief updating is you know
of course the thing that ensues once you act upon the world to do some smart data mining to
respond to some epistemic affordances and you know the question then is
you know what kinds of systems do that and the moment I would argue is probably just us
there are other examples of some beautiful examples in so active learning using machine
learning you know to design your own experiments and optimize the actual experiments and say
drug discovery or molecular biology so I mean there is a long history both in statistics and
in machine learning of active learning that I think does have the potential but I don't see it really
being well from what I understand in discussions I don't see that being a bedrock of the way forward
an explicit part of the design for an age of intelligence that is puts us sympathetically
in an ecosystem of intelligence and that's really what that white paper was trying to think what
what would it look like and what would happen if your Google Maps became very curious about you
particularly I mean Google Maps I think well there are two sort of sort of metaphors here which
might sort of ground the frame the framework or the perspective that I'm trying to think about
this worry and one which I found very helpful again coming from my colleagues in industry
is just trying to explain that you know the nature of shared intelligence and distributed
intelligence and the analogy here would be the brain you know you've got really smart little
elements little neurons I mean they're really smart and in fact if you get into the weed
synthetic computation incredibly smart little things that are in receipt of their sensations
and they act by sort of pinging they don't know who they're pinging but they're pinging away
sending out little action potential or messages down down the axons of the wires that are emitted
from from the nerve cells and you've got you know 10 billion of these things and they're all very
very smart but would you call any one of them autonomous would they call would they have agency
and in an elemental sense I think they do but it's when you put them all together lots and lots
of little smart things together do you get this emergent kind of intelligence that you could
undeniably say has the capacity to roll out into the future to you know generate fantasies or
counterfactuals that are all conditioned upon what I'm going to do next where I now
becomes this collective so this is the kind of emergent behavior that emerges from getting
lots of little smart things to talk to each other in the right kind of way so that that's you know
I think quite a helpful analogy by what is meant between you know about shared intelligence and
what might you know might arise clearly a lot of these little smart brain cells could be analogous
to you and me a lot of other than a lot of others would be all kinds of apps and you know giving
you now the potential to see through the eyes of of any smart app that knows what you want to know
and is curious about what you want to you know to find out again coming back to this notion that
um belief sharing I think is already there in the context of say you know sat nav you know I
share my beliefs about my preferred states of being my characteristic preferences are one part of
the you know the imperative suite for policy selection or the good plans I shared that with
in terms of a destination with some shared world model or shared narrative between me and my sat nav
you know in this instance of a geographical world model and then it has beliefs about the best policy
it makes a little plan and then it shares its beliefs with me again so I think the you know that
to the extent that your colleague in Oxford was was saying we're already there I think that that's
absolutely right I mean you know we already have this kind of belief sharing I think that the move
though is to make that a much more symmetric belief sharing you know I'm asking the app for its beliefs
for it I'm asking it to behave as a recommender and it only knows what I tell it so it has no
autonomy but if it was now in a position to actively smartly resolve its uncertainty about me the user
then it's much more of a sort of you know a balanced symmetrical dynamic interaction between me
and the app and the agenda here is not to create paper clips or make money the agenda remember
is just to resolve uncertainty to state curiosity and to move towards a state of greater mutual
understandings of that that that's a sort of the the non-dystopian view of information sharing
so Shane Legg said that his definition of intelligence is the ability of an agent and
we're using words here like agent we'll get to those words in a minute being able to solve a
variety of tasks in different environments Francois Chorlais said it's efficiently creating abstractions
given limited prize and experience Pei Wang says it's the adaptation efficiency over finite resources
so when you look at definitions of intelligence typically they focus on principle and function
which I think your one does or capability or behavior or structure now the interesting thing
about the principle one in particular is I think it's the least anthropomorphic and I think yours
is the least anthropomorphic definition I've ever heard of so and also this concept of grounding in
the physicality of information rather than reality itself and whether they are the same thing of course
is a philosophical discussion that maybe we'll park for another time but yeah help how would you
contrast your definition of intelligence from these other ones well I think you've already done that
you've just said it's a minimal essentially a physics-based definition of intelligence which
requires a move or indeed a complete commitment to staying in the space of information and
information geometries and belief updating so having said that I think all of those definitions
touch upon some essential aspects of intelligence every one of them rang true you know to my ears
and I could read every one of those as being one key foundational aspect of what would emerge if any
self-organizing system managed to supervise and coexist with other self-organizing systems
from the point of view of you know of the free energy principle on this sort of
first principle approach so there are no axioms or assumptions the question is not you know what is
quintessentially anthropomorphic the question is what emergent properties of certain kinds of
self-organizing systems would qualify as having that that bimimetic and then ultimately anthropomorphic
aspects to them you don't even have to go to anthropomorphism I mean if you were talking
we're talking as you have been to Mike Levin and his friends you know they would talk about
basal cognition they would talk about you know just a multicellular organism is a beautiful
construction of the rest upon autopoiesis self assembly of individual cells but also cells of
cells and you know how does a surface cell an epithelium know that he's on the surface and how
does that individuate the internal cells of an organ or an organ from the rest of the environment
so you know the kind of intelligence that has this anthropomorphic feel I think people are also
seeing in biotic self-organization that would be a long way away from the kind of psychology
intelligence that we're talking about and yet it rests upon exactly the same kind of mechanics
and you know for me that would be the Bayesian mechanics that come from
the you know the dynamics of systems that are self-organizing open systems that are self-organizing
so you know you talked about so adaptation has been one aspect one common theme in the
the sequence of definitions you gave I think does speak to this bright line between
basal cognition and biotic self-organization biological intelligence of the kind you can
read in many many different ways your your DNA for example your genotype is an intelligent
information accumulating device on the point of view of evolution you know it it stores all
the information about what it would require to build a phenotype that's fit for purpose in this
particular environment so you know that's a kind of belief updating that's a kind of intelligence
but it doesn't have what we were talking about before which is this capacity to plan
evolution doesn't plan you could also argue that the the worldwide web doesn't plan
google maps you could argue does plan to a certain extent because it certainly so
what's the difference the difference is I think implicit in at least the first two of your of your
definitions which is this notion of counterfactual futures this notion of you know imagining a future
or putting it another way having a world model or a generative model that explains things that
are not tied to the moment so if you're a physicist what you're talking about is now a
probabilistic mechanics a Bayesian mechanics or possibly just you know an information
theoretic mechanics based upon paths through time so we're talking about things like the
pathological formulation and you know but crucially we're talking about trajectories
that don't cannot be localized to this point in time that necessarily entail the future and
indeed the past so I've you know that notion of freeing yourself in a you know I can't remember
the name the philosopher now I can remember the name I just can't pronounce it so I'm going to pretend
I can't remember well there are people who there are there are the philosophical schools that
emphasize this temporality aspect now you know and if you just look at physics you look at contributions
of Richard Feynman for example it's all about the pathological formulation and so I think as soon
as you talk about the elements to which the information geometry is an intelligence and
autonomy all of these things could apply are not states they are trajectories dynamics narratives
paths that have this this sort of future pointing aspect then being able to select among different
futures becomes an emergent property of this kind of sense making this kind of auto-poesis
under you're read as a Bayesian mechanics of self-organization and just thinking about your
definitions they all have that aspect of choosing among different futures or considering or having
abstractions you know about what might happen if if this so for me that that is one way of expressing
curiosity because to be curious you have to imagine well what would happen if I did that
and what would I know if I did that but you have to imagine it before it's actually happened which
you know is is for me the big bright line between you know between the anthropomorphic kind of
intelligence and the intelligence you find in the thermostat or in a you know a variation auto-encoder
yeah later on we will decompose the different aspects of cognition and I think as well as
thinking it can be knowing and enacting as well and we'll talk to that but you said words you know
action agent per se goal plan behavior and I guess and I posed this to Levin as well it feels like
these are terms that we understand because we have cognitive priors like agentiveness and so on
these are things that that we understand that might actually only be a lens into something far
more complicated and just to touch on the information traversal points over a geometry
that's very interesting and I'm no expert but I think the medial temporal lobe deals a lot with
spatial contiguity and we have grid and place cells etc etc so it you know at a macroscopic
level in our brain it's a first-class citizen but there's also this hierarchy isn't there
when does it start happening you know do single cell organisms plan into the future
and to the previous point is planning necessarily a reductive lens of intelligence
you brought up loads of interesting things there you call me by the reductive lens that's a lovely
phrase what does that what does that mean well um I didn't mean it in a majority sense but when
we use words like you know like we we we say intelligence must do x y and z it must plan it
must reason it must act and we have this cybernetic loop and so on and I I have a theory that this is
just the way we understand things I see and and in a sense it's a lens onto a much more complicated
thing and the reason this is interesting is the reason why we have people like John Sewell
who says that the um the impenetrable realm of the subjective experience is beyond function
dynamics and behavior it's a little bit extra and and even and morality is another example we'll
talk about that later that feels like it's something which is a little bit extra so there's always this
question of to what extent is intelligent behavior deducible from the models and the words that we
use yes I think that's a fundamental point I you said um okay now I understand what a reductive
lens is I like that I like them I get for many reasons of course because um well first of all
um it certainly is not uh you I don't think it could ever be used in a pejorative sense um but
it does speak to um two fundamental themes which is um the the way that we do make sense of our world
through course grading through reducing uh through having um intuitive models of the way that the
world works um ultimately could be um seen as language words um could be I think could also
be seen at quite to kind of as physics and maths as well to be quite honest um you know these
these the more I read about you know modern mathematicians and physicists talking about
that you know their skills and the the legacies that they they enjoy the more I realize it's all
changing all the time it's just another kind of reductionism um but language particularly but it's
a right kind of reductionism I guess what you're saying is that you know um we have this way of
summarizing and classifying certain kinds of behavior which may not truly reflect the underlying
complexity the beauty of what's going on underneath that at that at that point I would go the other
way I would actually say that do not properly reflect the underlying simplicity of what's
going on to be quite honest you know this comes back to you know the um unashamed use of phrases
like sort of existential imperatives and self-evidencing yeah we're just here we're just we just have
characteristic steps and sets we're just um realizations of some glorious um launch of
our equations and all these stories about sort of uh belief updating and um sentience and intelligence
and just reductive stories that make sense of you know what we you know what um uh what we must um
or can if indeed to a certain extent the free energy principle itself I think is a reductive
story of that kind you know when I say if you look at things through the lens of Bayesian mechanics
as if I think the free energy principle is another example of of this kind of reductive thing
it's a looking at something which is inherently much more simple than the lens through which you're
looking at which is the Bayesian mechanics and the free energy principle so I think that's absolutely
right a really interesting idea um so uh and at another level I think it speaks to some key issues
you know I mean you're um you you're often um then confronted with you know okay I'm talking
with you about agency and agents and me and you uh so what license is that what aspects of my implicit
world model or generative model endow me with a sense of me and you and indeed me as agent and
you know what would that look like if I stripped away um different levels of meta awareness or meta
cognition if you're a psychologist and uh and just have a minimal selfhood um you know is just
be having plans um sufficient to call me an agent even if I don't know if I'm an agent
if I have plans who else is going to act you know enact those plans I mean I would love to go there
slightly later but there's so much we can say about um agency and the boundaries and also the
causal pressures between agents and also whether you can think of boundaries as being observer
relative but I just really wanted to go to the universal um algorithm thing that you spoke about
before because I think it's delicious so um I think it's fair to call you a universalist
and that and that that's that there are quite a few universalists actually that these are people
who think there's a simple underlying principle and this is in contrast to what we were just
talking about which is that the the reality is more complicated than we'll ever understand and we
have a truncated cognitive horizon and we as Chomsky says we just have kind of um simple
primitives built into us that help us frame and and and understand a kind of abstraction space
within a certain cone but um I was reading professor Christopher Somerfield's book at
Oxford I interviewed him last week and he said in his book um could it be that the um success of
mammalian brains is not due to any careful crafting into a mosaic of different functional
subsystems but instead is merely due to size we know there's a powerful relationship between
the sheer number of neurons and the complexity of behavior he went on uh he said researchers
and neuroscientists are like such as Carl Friston and uh Jeff Hawkins and even Andrew
have flirted with the idea that there might be a single algorithm which underpins intelligence
with the brain acting like a massive tpu repeating instructions ad nauseam to generate
complex behavior so it's a fascinating idea is that a fair summation yes what's a tpu in this
oh well a tensor programming unit it's a very powerful computer right yeah I learned another
new acronym but my world is full of new acronyms right okay um yeah um so what would his argument
there is it something to do with um scale and size is is that well not not only that I mean we'll
we'll get to there's a guy called Rich Sutton and he he had this you know bitter lesson essay
and it's a warning against hand crafting um structures architectures because it bottom
exit doesn't scale so this universalist idea is that you know maybe and and Jeff Hawkins says the
same thing he's got this thousand brains theory of intelligence yes and the idea is that there's a
very simple underlying algorithm or principle and you just replicate it you scale it up or out
and that produces emergent intelligence right yes well okay then I am a universalist then
yes um uh so um
you know but both of those the way you described it do speak to some um I think very pressing
issues about um structures and structure learning um yeah you know you could either
read from the point of view of machine learning and sort of graph learning what's the right you
know how many layers does this particular deep learning architecture need or what kind of factorization
that I'm going to put in play um or if you were um you know um radical construct a radical
constructivist this you know that that's where you know I've often heard people like Josh Tenenbaum
for example think about sort of structural learning and um from the point of view of the
universalist I've now learned that new word now that's good so as from as a universalist
then you are certainly looking for the one principle that um is redeployed at successive scales
yeah and um that should be a sufficient explanation for those things that show
emergent behaviors at particular scales so I think that you know that that is absolutely true
and again you can read this from the point of view of a mathematician from the point of view of
the renormalization group and what does that mean what it just means that you know if I take lots
and lots of little things um and I start coarse-graining them in a particular way um then if I want to
describe the behavior of all the elements at one um scale of organization say molecules or cells or
people um then if I can write then down their dynamics in terms of say a Lagrangian you know so
some way of summarizing their dynamics that um and all the things that um accompany or ensue from
those dynamics um then if I do my coarse-graining um and then look at the collective the average
behavior of say lots of cells a place cell or your entire medial temporal or entire person
at a more macroscopic level then I should be able to recapitulate the same functional form
of the dynamics and all the Lagrangian um so from that point of view um you have a particular kind
of universalism that is actually scale free because you get the same principle emerging at every level
and that that is basically um one um one way of looking at the deployment of the free energy principle
is asking what would it look like when deployed at different scales so you can deploy it at the
level of dendritic self-organization you can deploy it at the level um of um you know neuroscience
you could deploy it at the level of um morphogenesis and cellular pattern formation
we've done that with Mike Levin um the idea being that this the same universal principle
works at every every level then the interesting game comes between the coupling between the levels
how does one level constrain um and inform or contextualize the level below and vice versa
you know and this I think is a really important sort of um um issue which is probably well rehearsed
in many different disciplines ranging from say evolution so evolution as a free energy minimizing
process where free energy is literally the um the the negate abound on the negative um
um log marginal likelihood um I name the likelihood of finding me this phenotype here
when sampled at random from a population um how does that scale of a free energy self
orthopedic process you know natural selection basic model selection if you're a statistician
how does that provide constraints on the exactly the same principles um of active inference and learning
um in developmental time for any given phenotype and then the that would be the top
down causation the bottom up causation from one scale to the next scale would be you know how
does my behavior my experience experience dependent plasticity my evidence accumulation
or my good Bayesian decisions how does that now um mean I contribute to the gene pool at the at the
evolutionary level yes so so you know that that that would be one way of reading it the other way
of reading it of course is just if you're designing um a tpu or deploying a tpu um you've got that you've
got message passing on some graph uh what is a graph well it only has interesting structure in virtue
of the um the sparsity or the connections that are not there otherwise it's a full graph and it's
not very useful for anybody speaking to Chris's Christopher's uh uh you know too too big too many
neurons um you can't you you necessarily have to have a sparsity to fit all those neurons into
um I would put it the other way around though I would say that um anything that is adaptive and
has this size uh you know what properties must it possess and I would I would suggest that it has to
comply with the principles of um self-evidencing um where evidence now is the marginal look the
marginal likelihood that can always be read as accuracy minus complexity so as if it exists
and it's big it's got to be minimally complex what does that mean it's going to have the
smallest degrees of freedom uh the minimum number of connections in so you should be able to predict
the sparsity from the first principles at every scale so that sparsity defines the nature of a
graph and indeed if you're talking about anything that's deep in a hierarchical sense all you're
saying is there's a particular kind of graph um that I have in mind and it you know has a certain
sparsity structure but crucially it's a sparsity structure that allows me to uh call it a hierarchy
it means that there are no connections that transcend unlike a sort of you know a u-shaped uh
what was it no it's still hierarchical to anyway sorry I'm getting a bit distracted right
the choice of ground so I mean I think that's sort of um the notion of um coupling between
different hierarchical scales is absolutely crucial from many different perspectives renormalization
group evolution um you know getting the right uh graphical architecture and your message passing
scheme in computer science so yes I wanted to bring that up but I discussed um it's actually
the same thing with levin so we're more for genesis and the rungs of the emergence ladder and the
causal pressures between those rungs and you know philosophers like George Ellis said that you only
have causation uh between the levels and uh Douglas Hofstadter and go to lesha bark and the strange
loop thinks that there's a very complex panoply of causal pressures between the scales like that I
gave the example to michael my mind is an emergent phenomenon and I command my hand to move and he
said that at different scales you get different amounts of work and actually I think if you get
into integrated information theory it's kind of talking to that a little bit and and I think you
think of consciousness as having a lot of um information processing going on because it's at
the top of the stack to some extent but do you have any intuition on on how that information
is kind of partitioned between the scales and how those causal pressures work between the scale
yes I do wonderful you're very well read aren't you so it's nice that you you mentioned george
Ellis I use the word top down what causation exactly in the spirit that he writes about it
I had literally out here had him in mind but yeah so I was hoping if he ever hears it he'll know
that I was talking about him so it's exactly that and it always makes me a bit queasy when I use the
word emergentism which I think since some people say there's no top down causation in emergentism
but I don't fully understand the philosophy of it but I acknowledge that sorry yes I've distracted
myself from your really important question which oh yes the um so the the different scales um of
a Bayesian mechanic um self-organization viewed through the lens of Bayesian mechanics um I think
what we've just been talking about and I would imagine with with Mike as well a lot of focus of
here in that kind of work um is um I hesitate to use it but I will use its spatial scales you know
how how how do element how do single cells assemble into multicellular why but we are
how on earth can you um envisage the emergence of multicellular organisms as Mike's done some
beautiful theoretical work you know several years ago now just just think about it to be
a skin cell to be an epithelium means you have to sacrifice the ability to reproduce so it's
if you like completely paradoxical from the point of view of natural selection you have to sacrifice
yourself for the greater good so there are you know there's a wonderful questions about about um
cells of cells and of cells and cells as you build up to different levels of spatial scale
but I think your question will be better addressed from the point of view of temporal scales um
and again you come back to this um universalism um now I'm getting fluent using that word
um where you've got the same principle playing out yeah exactly the same principle exactly the
same mechanics the same Lagrangian um playing out at different scales that um where each scale
contextualizes and has this circular causality the bottom up and top down um um aspects to it
in play so my favorite example of this is just to um look at a succession of belief updating
um processes um from the very very fast which would be um from the point of view of um
your sentient machines it would be inference inferring states of the world
as they are in the moment so state estimation Bayesian filtering um everything that um you know
speaks to some kind of situational awareness on the basis of some smart and hopefully smartly
sampled uh data um and then we move to the next level I'm going to skip attention and precision
but there is an intermediate level which usually um in in the neurosciences has a time scale of
your your hundreds to just uh of milliseconds to seconds but I'm going to I'm going to jump
straight to learning so what's learning well it's just slow inference it's just basically
slow belief updating um where the states that matter now are equipped with another
kind of label which we call parameters or weights in machine learning in a neural network
and but they're just random variables that are brought to the table to explain uh or part of our
world moral generative model but they're special um kinds of parameters because they change very
very slowly um and then you move well okay so those are two levels what about turtles all the way
down and turtles all the way up okay what's the next level well the next level is now the structure
so now for any given graph for example um that is equipped with edges and those edges will have to
have some um slowly very varying parameters um that describe the you know the nature of the
message passing on those edges um there will be um there will be a you know you are conditioning on
a particular structure and you know is there a connection there is it a hierarchical is it heterarchical
is it you know a unit is it um is it a transformer you know it's a convolution neural network you've
seen this wonderful evolution of structures in machine learning over the past few decades as
people try out different structures and you know some work is for one kind of application
others work for others but um from the point of view of your question what we are seeing is a kind
of structure learning that's playing out over years so this but it's exactly the same principle
it's this free energy minimization but just in this instance the free energy now is a pathogenal
it's just the average over a long period of time which is the um um which is the exactly the quantity
that people doing um structure learning or Bayesian model selection use when adjudicating between
different graphs usually um of complex system models for example um you know um and you could
argue that now um over um several tens of years or hundreds of years that you know exactly the same
maths could be leveraged to provide a formal description of natural selection as Bayesian
model selection exactly the same thing as you're doing now when you're selecting whether to speak
or whether not to speak or trying to infer you're selecting the right hypotheses about you know the
narrative that you have in your head that makes sense of what I'm saying um exactly the same maths
and mechanics is unfolding over the millennia um you know in terms of uh in terms in terms of evolution
so I think I think that's a nice example of the separation of temporal scales but the conservation
of exactly the same principles that have this information geometry and implicitly intelligence
in of a basal sort I use the word basal because that's what mike levin and
chris fields and jim plays for it like using this it's this notion that um basal cognition and basal
intelligence transcends physics psychology and biology um it's all the same thing this is a line
from chris fields as a friend of mike levin yes and I think that's a great notion um and I think
you one can do that very gracefully by being a universalist and just by finding the right principle
the right sort of reading of dynamics and you know that reading you know for me is the information
geometry that um that supports the belief updating I was going to do markoff blankets later but it
feels relevant now and maybe we should bring a bit of continuous versus discreeting so a few things
came to my mind when you were talking about that first of all we think of um emergence over time
and self-organization over space and I guess it just occurred to me that are we only interested in
time and space when we talk about this kind of structural learning and then um with these markoff
boundaries I had only thought of them um in at one point in time but you could actually think of a
kind of three-dimensional uh markoff boundary um over time as well now just to remind our audience
um blanket states facilitate the interaction between the internal and the external conditional
independencies the external states are independent from the um the internal states as long as we know
the intermediate blanket states now um to get to the uh to a core issue that we've been modeling
complex systems you know like where do you draw these boundaries and is it boundaries all the way
down yes um so I think you're absolutely right this is the perfect time to bring up this sort of uh
hadn't really thought about a notion of boundaries in time and sort of um that's intriguing so
but you're distracting me I will I'll think about I'll think about that after our conversation
but now certainly so um there's certainly a lot of current interest in um taking the
notion of markoff blankets which is foundational in this sort of reductionist lens of the free
energy principle as a Bayesian mechanics um you know the one could could summarize the
Bayesian mechanics of the free energy principle as simply just another kind of uh quantum mechanics
or statistical mechanics that inherits just from this partition that is the markoff blanket that
separates the the inside from the outside yes um now um and of course there are lots of vexed
issues about one how do you identify those markoff blankets and how long do they endure for um so
there's lots of interest in that at the moment but one very simple approach to um the question
how long do they endure for is to say well that's not the right question because we've just talked
about separation of temporal scales so you have to say at what temporal scale well you
operationally define the markoff blanket as the time over which it exists
and what would that look like then when you suddenly now think about um the situating that
temporal scale within the context of a larger temporal scale so what you now have is a picture
where big markoff blankets blankets and blankets things that define say you you and me or cultures
or um um nation states or institutions that um outlive say species um these big ones
are last for a long time but they contextualize and provide constraints on uh markoff blankets
at a smaller scale that last for very uh for for much um much shorter periods of time and so
on all the way down so that at some level say at the the molecular level from your perspective
these markoff blankets may only exist for nanoseconds or your milliseconds um but from the
perspective of the molecule thank you you know this this is a lifetime and it's well happy
complying with or can be understood as um you know doing its own basal intelligence doing its own
basal leaf updating just automatically for its lifetime which may only be a few hundred milliseconds
you're making sense of its world or interpret being interpreted as having this sort of um
sort of you know biotic intelligence and self-organization just because it exists
for that period of time so then again this interesting question you know the how does one
time frame contextualize the other starts to bite and you know you start to now think about
from the point of view of a slower time scale what would um a succession of markoff blankets
look like um and what what uh immediately encounters is the notion of a of a wandering
eye tinderant markoff blanket yes um we spoke about vagueness on on the first one so we'll
we'll park that but the wandering sets is very very interesting but um I there were two things
that I that I think I wanted to understand um you do think that there's a hierarchy of blankets
but I'm interested in exploring this idea of whether the blankets could be observer relative
because you use the word could be understood as a markoff blanket and that brings up two things
to the floor in my mind first of all the extent to which they are a lens versus describing something
in reality I see and if they do describe something in reality you spoke about this symmetric causal
pressures which we can we can speak to as well but but on this understood thing it's very similar to
um Wittgenstein said the meaning of a word is in its use and that meaning in language is embedded
in pragmatic actions and the language game and so on and and then like the further forward occurs
well maybe the the understanding of a boundary or a you know markoff boundary blanket could be
understood in the context of one's perspective right that's a great question and I may be informed
by reading some of the philosophical literature and I should remind you of course I am not a
philosopher so um some of what I say will be naive um you need to speak to philosophers about this
but I would I would say the markoff blanket um is is something which is metaphysical um you know
it it is defined by a particular spastic structure when formulating any state space in terms of
dynamics and specifically you know a you know a long span equation so one talks about the
free energy principle as a first principle account um but it does actually commit to something it
commits to the notion that there are states and that those states have a separation of
temporal scales that disambiguates or separates systemic states from random fluctuations so
but that's all it does there are no more assumptions and then everything else follows from that
under the understanding that for sufficiently large systems the probability of there not being a
markoff blanket is um um zero um so when this comes back to the sort of um this sparse coupling
conjecture um that Christopher I think was alluding to um or at least we um we unpacked in
terms of your if a system comes to uh it exists over and it's sufficiently big it will be sparse
and once it spars there um with probability one will be markoff blankets so at no point now have
we introduced the notion of observers we haven't at this point even introduced the notion of the
free energy principle we're just saying um there will be one way of carving up large
dynamical systems or certainly systems can be expressed as a large van equation or a random
dynamical system um um will be ways of carving them up into one or more markoff blankets at one
or more scales yes um and then the question is well what um what would it what would one uh how
would one then describe um the dynamics for any given markoff blanket so you at this point you
are at liberty if you are simulating a universe to choose the markoff blanket and the scale that
you want to simulate if you're part of this system you're not at liberty because if you're part of
this system you have your markoff blanket so all you see are the sensory impressions upon your
markoff blanket and you may or may not have active states and it brings us back to the
other bright lines that make between agents and sense making machines that can't act sessile
things um so if you're asking um i am i um are the observer dependent perspective dependent
aspects of um this formulation using markoff blankets as a um a physiological device not
if you're part of the system it says something quite profound about what you could you know what you
and if you're being observed by something else if you believe if your world model
is populated with the fantasy that there are other observers out there and then um those observers
will never ever know your internal states well what about rather than an observer um an actor
or an agent and we'll talk about an activism in a little bit but even at the microscopic scale
there are still affordances and you gave this beautiful example of um of a type of species
where someone might kill themselves with a great good and that that shows the kind of information
sharing that you have in in these complex systems so similarly if i'm an agent in this system and
and i act and and there are these you've described this cybernetic loop and there are these kind of
symmetrical causal pressures and so on and and and then you almost get this emergence of the
markoff blanket so it's so it's changeable yes no it certainly um that's only true um it is
changeable and it's malleable and it's self-assembling uh and certainly when you have markoff blankets
and markoff blankets and coalitions of markoff blankets for example a formation of in-groups
and out-groups and the like yes you would expect there to be a a you know a dynamics of of the
markoff blankets or who you're relating to so you know the sparsity structure and who i talk to
and who i listen to and you know what social media you know i i commit to all of these things are
products of or reflections and can all be articulated in terms of you know wandering
markoff blankets no that that's absolutely true um i i thought you were you you were asking um
um something more about um whether the markoff blanket is part of a realist
metaphysical description or whether it's that too yeah uh yeah i mean there is another argument
that if you if you now play god and you now put your universe in in a computer and now to simulate
and and write papers about your simulated universe where you can see the the internal states
next to all states then you know i think you're in a different world um um just i get i repeat
there's something quite profound about the um the questions of you know what is it like to be something
um are interesting because from the point of view of the free energy principle what is it
like to be something means that someone's asking that question so they are a thing
and they're asking about another thing so now they're asking questions what is it what um
is it like to um know the internal states of a markoff blanket but by definition you can never
know them so there is something impenetrable about being something else so it's a question
which is unanswerable it's unknowable um um so if um from that point of view i think there's something
beyond the um the sort of the epistemology of you know um of markoff blankets but if you're
just simulating cells or people you know doing multi-agent simulations then obviously you can
you can choose where to put your blankets and you can you can just use them as a device to um
to understand self-organization to simulate it and to predict it and i think in that instance
you know that you you could deploy your markoff blankets wherever you wanted to at every any
scale you you you thought was interesting for the phenomena of interest that's not quite speaking
to what you're into though which is a changeability of the the markoff blankets or well i mean we
can get into that um because if uh and later on we'll talk about the the real world implementations
of these ideas but um but if you if you did hard code the the markoff blankets then you
would bottleneck the system essentially so ideally you you would actually have the system
to find a sufficient resolution where all of this could could emerge um itself yeah did did you
cover the point of whether the blankets can be um must they be spatially temporally you know
should they have a spatial temporal contiguity and the reason i'm asking that question is
when you think of a blanket i like almost synonymizing it with an organism and maybe that's
a bad thing to do and you can correct me but but if we think of um the discontinuous blankets in
information geometry that's an interesting idea because we kind of visualize them in 3d space
don't you know yeah yeah well certainly they'll have a topology yes um i mean i'm
latest earlier on you're talking about sort of play cells and grid cells which are very
special and beautiful constructs which speak to a certain continuity or contiguity aspect
they suggest that these um you're coming back to christopher some of um summerfield's
book um you're one way of understanding the um the markoff blankets in the brain
so you know we've been talking about markoff blankets delineating me from a world but of
course um your temporal lobe has a markoff blanket as it wouldn't be a thing he wouldn't
be able to call it a temporal lobe so you know the very hierarchy could also be described now in
terms of um uh markoff blankets within markoff blankets within markoff blankets and some really
interesting questions about what that means for what what part of the brain can know about
another part of the brain and what and what action becomes of course it becomes a tension
i just wanted to slip that in because i think that was in part what you were driving out with this
sort of uh context sensitive fluid dynamics on in exchanges between markoff blankets i think
incredibly important even within the brain in neuroscience this is basically the the routing
of information the deployment of selective attention or sensory attenuation attenuating
ignoring some signals it is exactly that that makes us such adaptive context sensitive information
processing um machines and of course that is exactly what you'd have to worry about when
designing a web with routing and and context sensitive you know who do i listen to uh so you
know this is really really important um the um so the the the emergence of um i think
you're absolutely right they will have a topology and if i were using markoff blankets and you know
i do and many other people do every day when um thinking about message passing as a statistician
on factor graphs or um which are due to a graphical model probabilistic graphical model of the
system you're trying to estimate or understand um then you know the markoff blanket is just
the um you know the implicit notes that that are the influence of me or the or the parents
of children the parents of the children you know and that has enormous implications for
minimizing the complexity of the message passing um and you know and i repeat defines a hierarchy
for example um so markoff blankets there will have a topology probably best understood practically
from the point of view of graph theory as opposed to um information geometry um but there are special
kinds of connectivity that you see um such as place fields and i'd also say if you just look
at the history of machine learning there are particular um kinds of markoff blankets that
characterize the very structure of certain neural networks i'm thinking of convolutional neural
networks i'm thinking about you know often motivated through weight sharing and the like but
you know if you look at weight sharing as just the kind of structural learning that's trying to
minimize the complexity part of your uh free energy or your um negative um log marginal
likelihood um then what you are doing is finding the right kind of spastic fit for explaining these
kinds of data what uh what what what is the architectural principle it's it's the translation
invariance the translation symmetry it's just the contiguity aspect it's a fact that you are
sampling data that is generated in or from a metric space that has a well-behaved metric
so i i would say that sort of you know place cells and grid cells and many in and many other
are thinking about uh chris's friend tim baron's work for example um finding grid light structures
everywhere yes in abstract spaces uh so what that tells you is that that it's likely that we live in
a world um where data are caused by samples from a metric space that has a you know a measure to it
not all spaces do i think it'd be you'd be hard pushed to find you know the right kind of metric
for language for example um and certainly if you're if you're if you're sort of building
message-passing schemes or believe propagation schemes on a factor graph you don't you don't
think about contiguity in a metric sense you know you don't measure the distance between one node
another node it's is it connected or not um so i think there are some special um contiguity
that say there are some special worlds that are recapitulated that emerge in the internal
architectures of intelligent machines that have this contiguity property in virtue of the fact
some aspects of the causes of the sensorium are elaborated in a metric space but not necessarily
everywhere and yeah well i want to take you back i know you i want the way you want to take me
anyway well i will i will finish then because um i see uh in the brain and i also see in the
direction of travel in terms of building thinking machines um as you get deeper into these hierarchies
and you go from a fine scale to larger scale there is an interesting move between um
architectures that would speak to this metric aspect in space and in time to a more discretized
topological non-metric representation yes i know i wanted to touch on that i mean there's a beautiful
example i mean um christopher summerfield talks about pierce's triad and various ways that we learn
different types of um symbol and abstractions and so on and there is a famous experiment in
neuroscience i'm sure you're familiar with it where they showed um people pictures in a in a
certain order and associations and then the same topological structure was was recovered i think
in the ntl and and this is because even though with spatial temporal contiguity there is a
metric space but the brain learns these kind of associations and then you can essentially learn
these abstract concepts that you know concepts that reverberate in our language and experience they
get represented yes i and i completely agree with your um point about the the revolutionary idea of
the um the translational local um equivalence you know that the weight sharing with cnn's it's a
beautiful idea it's revolutionized deep learning so um just to finish off the discussion on mark
of blankets i wanted to talk about part-hole relationships so um for the benefit of the
audience in logic and philosophy myriology is the study of parts and holes that they form
whereas in set theory it's founded on the membership relation between a set and its elements
myriology emphasizes the relation between entities which is to say the inclusion between them so
anyway when considering systems mark of blankets can nest into hierarchies and what connection if
any is there you know between that in the philosophical study of myriology and um i was also
going to bring in i don't know if you've heard of hinton's glom architecture but it it it was it
came after capsule networks and it's you know we were just talking about these wonderful inductive
priors in deep learning and and they they make the problem there are many curses in machine
learning but one of them is kind of like the curse of optimization and um and there's a complexity
curse as well and that's why most of these inductive priors they reduce the size of the hypothesis
set if you reduce it too much you get approximation error which doesn't help you either there's
curses everywhere but um but this is a really interesting prior as well these part-hole relationship
yes gosh yes and there's so many issues you bring there i'll just reiterate my favorite one which is
the you know the the notion of um these inductive biases minimizing the complexity i think that's
absolute you know that's something which is absolutely central um from the physicist's
perspective you know this um reading um self-organization as self-evidencing which is a
philosophically poetic way of simply um describing existence as an optimization process which you
shouldn't really do it's or it's really a principle of least action but you can read it as an optimization
process what are you optimizing the evidence for my world models what how can i carve up that
evidence complexity and accuracy what does having a what would i then mean by optimization what i mean
just providing the simplest account that maintains a degree of accuracy what do i mean by simplest
minimizing the degrees of freedom that i use up in providing that accurate account how do i do that
by the right sparsity structure what does that mean it just means finding the right structure
and you know you could actually think of much of evolution and um the trajectory of machine
learning architectures as this game of finding the right structures but there are just these
structural priors that are apt for describing the kind of data you know so if you're you've
spent your entire life doing emnist images then it's going to be the kind of structural priors
that you know that's that inherit from being sampled from a metric space and you'll have all
the convolution i think the capsule i didn't i i didn't know about the glom stuff i i was
like listening to to jeff's um um latest ideas because he's always got the right kind of intuitions
and and these intuitions are part of the i think um part of our reflection of ashby's law of
requisite varieties explored in hypothesis space about the very structures uh but certainly the
the capsule stuff and the you know the um one aspect of structures of graphical models or implicit
factor graphs when it comes to implementation which i think is often neglected um is the
orthogonal direction from uh orthogonal aspect you know um in relation to hierarchical composition
and that's the sort of the breadth of a model in terms of um having factors that can be separated
so if you wanted to have say seen construction you wanted to have um multi-object your the ability to
track and infer and and um make sense of data generated by multiple objects what are you saying
well there are multiple things out there and these things um again from the point of view of
first principle account have certain conditional independences that is literally revealed by a
lack of connections in your world model what would that look like well if you're a physicist it would
just be a mean field approximation literally factorizing um introducing conditional independences
and factorizing a massive joint distribution any one level in your hierarchical model into a number
of factors that um if you're a neurologist would now be functional specialization and modularity
of a fedorian sense it you know what a wear in the brain would be you know the the conical
examples so i looked at that as um as a move towards paying more careful attention to the
non-convolution aspect you know those things that that actually deny a translational symmetry and
actually celebrate the conditional independences within any one particular within any one particular
scale so my guess is that i have no idea so i'm talking from a point of view of complete
ignorance but my guess is that um if it's an extension of capsule networks then it will
have that aspect it'll have that separability that sort of things that can do stuff and account for
attributes that are independent of other attributes or other objects that are that are conspiring
to generate uh the data and certainly my world of toy prototypes generative models usually in
matlab um you know then all the heavy lifting is done from the the mapping between the levels again
so all the interactions you know big red buses there's bigness there's redness there's busness
these can be factorized but they all have to conspire literally through interactions and
hardly non-linear um uh operators um and then generating what i would see if there was a big
red bus um so you know that puts a lot of pressure on the the likelihood tensors or the mapping from
you know you'll say a sense an input layer to the the first hidden layer for example um and then
sort of leads you into all sorts of interesting issues about you know how do you accommodate
that non-linearity and do you uh again as looking at the evolution of machine learning
architectures as an evolutionary process um you know at revenue or you know tannate whatever um
you know that is another aspect of this sort of structural learning i think we'll be very much
finessed i think if we just move to sort of quantum operators and and just go to discrete
state spaces in the in the spirit of quantum loop gravity i think sort that out and then we
can worry about what particularly non-linearities uh you know i've wondered away what was it what was
your question my point well no this this is a wonderful breakpoint so um before we hit record
we were talking a little bit about chat gbt and just before we go there because that's a fun discussion
i think one of the things that really distinguishes your your line of thinking obviously there's the
uncertainty quantification so on but um also there's this idea of of inactivism and i wanted to
just do a whistle stop tour of that as a contrast to these more monolithic approaches to ai like
like chat gpt so um inactivism contrasts with representationalism which is this idea that i
you know i know everything about the world inside the model and um you said that you can't just think
of the brain as some behaviorist thing it's a dance of dialogue you act in the environment and the
environment acts on you in a cybernetic loop now you also gave the i'm quoting like an interview
that you did previously you gave the example of radical inactivism uh that you can dispense of
representationalism entirely and you gave this beautiful example of this walking robot that
kind of fell down a hill and it did so so gracefully it looked like it was walking it was all in the
body um you know if the body is sufficiently tuned to the environment you don't even need
cognition and of course we'll talk about a decomposition of cognition in a minute um
and and you bring in this idea of circular causality so we're causally embedded in the world
by directionally essentially now when we decompose cognition um i think of things like
thinking and feeling and knowing and acting and and the environment and and so on and i think this
is one of the key things that distinguishes your your line of thought so could you give us a bit
of a whistles dot tour of of inactivism yeah so um inactivism is at the heart of the circuit
causality that that um follows from the very existence of a markoff blanket um and a markoff
blanket is the thing that would be it's certainly in my world necessary for the existence of something
that is demarcated or individuated from something else so just having a separation between thing
and nothing or not a thing um having uh that separation requires you now to think about the
two-way traffic um the bi-directional traffic and of course now you've got two directions of travel
that can be thought of as um the agent if you like sensing the environment um on the one hand
on the other hand the agent acting upon the environment or vice versa um you've got now
a circular causality coming back to sort of the other the the notion of a perception action cycle
and this notion of a dancing and a a dyadic exchange which is bi-directional two-way traffic
between the two so when you apply the free energy principle in practice to um emulate or simulate
sentient behavior behavior that is predicated on sense making um then you are necessarily
simulating action perception cycles so you're necessarily inactivist and that's
called active inference so sometimes just for fun I put an en in front of it called
an active inference I have to say the active inference was really a nod to active learning
it was just it's the same idea but um but cast in terms of fast belief updating as opposed to
slow evidence accumulation in terms of learning contingencies um so that use of inactive is at
the heart of applications of the free energy principle and obviously has been at the heart of
I think all right-minded formulations of behavior and self-organization since Plato probably but
certainly you know things like perceptual control theory cybernetics and you know all of that good
stuff everybody at some point active sensing um well you know we'll have to commit to this
active aspect even to a certain extent semiotics I think well now perhaps we shouldn't gather
but the so that would be one way of sort of celebrating and foregrounding the role of action
um and what would that look like from the point of view of machine learning and computer science
well it would look like basically smart data mining and it would change the nature of the game
from big data making sense of big data so having everything on the inside having access to the
entire world um and then making sense of that and you know you may ask what does that mean by
making sense of it well certainly doing something with it like generative AI versus um the the
complementary approach which is much more in line with this sort of complexity minimization and the
imperatives for the sustainable self-organizing self-assembling systems versus smart data
so the job the action now is it's basically what moves do I make on the world to get the
right kind of data that will serve my imperatives what are my imperatives to maximize the evidence
for my model of the world and if I can do that by definition my um I will be there or put it
in the way around if I you know if I exist then that is what it looks like I am doing so you know
you talk about the or you mentioned this notion sort of monolithic big systems um versus small
agile intelligent little agents and go and get the smart data that they need or that they think that
you need that is exactly um the sort of picture that underwrites this notion of distributed
cognition and that a different kind of network and a different way of relating to intelligent
artifacts and information services and and apps where it's lots of really small smart things
who are actively getting the right kind of data that they need to resolve uncertainty about the
context in which they find themselves again that you know the complexity minimization gets in and
the things are really I'm sure we've talked about this before but I can't resist just mentioning
again from the point of view of sustainability and climate change you know you know the direction
of travel of these large um say large language models for example is it's so wrong uh wrong
from the point of view of the ideology of climate change but also wrong from the point of view of
Landau's principle the jeniski equality when read as a thermodynamic corollary of self-organization
and non-equilibrium you know you you've got to minimize the complexity minimize um all of the
internal machinations so that you you just need the minimal amount of data um expertly handled
every little agent's a good scientist designing the right experiments to get the smart data
that resolves uncertainty about what it doesn't know uh the job done if you if you can do it like
so that would be one um if you like sort of answer to your question you know um the implications
and the importance of inactivism used really just as a euphemism for an agent that can gather its own
data the radical inactivism is I think a more of a philosophical thing and it's more of a fun
argument and I don't know any radical inactivists so I you know I I don't I don't have um I don't
have the right sensibilities uh to really answer this question but it's for what I understand
they you know they they have um taken it a little bit too far and a denying representationalism
um uh to the extent that you know you can get a kind of um sense making that doesn't actually
involve any internal dynamics um and um you know I'm sure that's true and I'm sure that you're going
to sell um uh chat g tp to me as one as one one example of this mindless kind of uh sort of you
know enacted um sense making um which is the opposite I think um I would call chat gbt extreme
representationalism all right yes we'll resist the urge that we'll go there immediately after
this I promise but um so clearly we we think that intelligence does necessitate embodiment
I think that's clear but I want to just explore this continuum between um and activism and
representationalism um this is this is really interesting so um this comes down to grounding
to a certain extent so cognition must be grounded in different domains and in the physical world
possibly in language in acting in affordances in knowledge as well and so this is this is a view
that that it that it is grounded let's say in in affordances but if these agents these organisms
ground their cognition in affordances then to what extent could you say they are learning
a world model if that model is with the lens of an affordance I think I think you can sort of
dis um reconcile um very rich representationalism um in the service of inactivism simply by noting
that um in the circular exchange you have to deploy the right kinds of actions and that's
going to require um coming back to what we're talking about before in terms of planning yeah
being able to build the right counterfactuals you know and to do to do that in an expert way
in an intelligent way you need to be able to represent the causal contingencies and
states affairs in the world at this time upon which you're predicating your next action so
I'm now just thinking you know just looking at your your your expressions and and I now
realize that you that you were trying to sell a dialectic between uh inactivism and representationism
for me um they are the same thing uh you to be a good inactivist you have to have the right
representations if you want to well may I say good I mean um good at a particular scale there are big
things that survive so viruses don't need to do much planning um but things like you and me do
need to do a lot quite a lot of planning so as you move up those scales you have to look further
into the future so that we certainly need representations which is what I have uh which um
which is why I always smile when I think about radical inactivism um so if there's no space in
radical inactivism for being able to plan to imagine scenarios to have narratives that play
out on the inside before committing and selecting the right way forward yeah and it's not an app
description of certainly cognition okay it's so interesting because the question to me is we
just spoke about this idea of planning potentially through an information geometry and many of the
abstractions that humans learn are not grounded in the physical world at all and that's very
interesting and we can get to you how those abstractions are learned but I guess what I'm
saying is let's say one of these agents in this multi-agent system it's traversing this topology
what grounds the states in that topology well I think that that would be um ultimately it will
be the transactions um you know with with the world that underwrite um at the lowest hierarch
level all the generative models of this hierarchal sorts and the abstractions adjust the course
graining simplifications that you know the the um the products of literally looking at the lower
levels of your hierarchy through a reductionist lens in the right kind of way that enable you to
carve up the world in terms of these um in terms of these abstractions which may live in a non-metric
space um they may still have this ordinal structure you were referring to before um and then what that
would mean in the context of lots of similar artifacts who had done the right kind of course
graining would be now the opportunity for direct belief sharing your brain-to-brain communication
speaking to a friend of mine E. Hood yesterday talking about the distinction between communicating
through um um the um through sensory um um exchanges such as language but what we're actually
doing is basically directly exchanging beliefs um of course you could do that directly um in
silico you could actually have messages about what this agent believes at this abstract level of
representation very high in a you know a hierarchal graphical model it could pass its
sufficient statistics to another partner somewhere else in the world um elsewhere on that on that
graph so you do have the opportunity now for direct belief sharing um and all sorts of interesting
questions about you know how you would engineer that you know you can't coming back to this fact
you have to have spastic in the game you have to decide where to to send and receive your beliefs
from then we have this sort of attention and routing problem and then you have do you have
peer-to-peer communication do you go through a server can you write that down in terms of um
quantum information theories a holographic screen and who's you know having all sorts of
really interesting things but what they speak to is that you're now you're sharing beliefs
literally basing beliefs probability distributions as encoded by sufficient statistics that can be
passed um as messages uh on a on a factor graph um you're now talking about the ability for proper
communication of the kind that has evolved in terms of cultural niche construction and
evolutionary psychology that speaks to cultural niche construction uh that we enjoy in terms of
you know the the words that we use and the and the exchanges that we use yes yes indeed
so there are two things to bring in so knowledge and language and you did invoke Andy Clark in
another interview which is the extended theory of mind so it's um one of the five ease in cognitive
science and the the communication substrate could of course it's distributed so the two agents could
be referring on missing information that actually exists perhaps in another agent or another
knowledge repository and then we can also talk to exactly what the role of language is and how it's
compressed how it represents abstract concepts and so on um so i'm very interested in knowledge
and language i'm not sure if you could bring those in right um i'm not sure i can do so expertly but
certainly a part of learning new things since my foray into into industry is this notion of
knowledge graphs so if you know if you read a probabilistic graphical model as um at least
its structure as being a knowledge graph then embodied um in the structure and presumably
the parameters of the connections that constitute that graphical model that would be knowledge
so for me um in a very non-mysterious and possibly too simple minded way knowledge is just
the product of um belief updating of a slow kind which is learning so i know contingencies
in the sense that i have a suitably configured and optimized structure and parametrically weighted
generative model that can be written down as a knowledge graph or a graphical model
upon which i do my message passing to do my inference about the particular context sensitive
in the moment kind of thing so i would put knowledge um um basically as an attribute
that is implicit in a um a graphical description of um an implicit generative model or world model
that is um actively um that is used actively to do the data mining to do the do the inference
and uh and the sense making and then the language part of it um would just be that um
highest level most coarse grained summary that is conserved over multiple agents
so just by definition um if i want to do belief sharing um i have to um i have to have a a shared
generative model or a commitment to the same narrative so that the meaning of what i'm emitting
is received in the right spirit or the right frame of reference by you and indeed um there's
some lovely rhetoric from quantum information theory of the kind that Chris Fields and Jim
Glaesbrook have been pursuing where you literally have to think about the generative model as a
quantum frame of reference and we have to share that in order to communicate
so in this instance the um the Markov blanket ceases to be just a set of states and and
adopts the role of a holographic screen and action now is writing to that screen and sensation is
now reading from that screen and there are two agents on either side so you know whatever is
written is an action but can be read by something else and then you're looking at the entanglement
which is the the synchrony of mutual understanding that you know we would aspire to through through
through through communication so that that that um but that only works if if the messages that are
written to the screen or written to the Markov blanket um um have the same kind of interpretation
so it speaks again to this um the fact that you have to have a good model of the world
and the world has um and that really means that there's a kind of entanglement or
generalized synchrony from the point of view of dynamical system theory between
the two sides of your screen or the two sides of your Markov blanket or your server
which have you know the right kind of isomorphism so we're talking about you're basically a shared
narrative that underwrites any exchange of signals and again what you know just thinking of of this
from the point of view um of generalized synchrony what we are talking about is just the characteristic
or the emergence of characteristic behaviors in any sparsely coupled set or ensemble of things
that possess Markov blankets or have a separability um what will they ultimately do they ultimately
find a synchronization manifold they'll find a synchronization simply because this is the
the most likely state of being and the most likely state of being is that which maximizes
the marginal likelihood which minimizes the free energy which just means that they now
understand and can infer each other so you know it's just another expression of this
existential imperative to resolve uncertainty have good models of my world and my world also
needs to have good models of me and if my world is you you have to have a good model of me and I
have to have a good model of you ultimately what that means is we're going to converge on the same
model um yeah I mean what interests me there is that unlike many other researchers you're
conflating understanding you know knowing and intelligence itself I guess I guess so yeah um
so good question um if intelligence is a process of belief updating then yes it's just
intelligent intelligence and learning and just learning knowledge and intelligently inferring
states of affairs are just the same process at different timescales and they both depend upon
each other so I have to have the right neural network of the right weights and the right learning
to make sense of the data in the moment and if you think of it from the point of view of
weight learning um either through propagation of errors or through experience dependent or
despite timing placesty in the brain you have to have the right inference in order to do the
learning so there's again this circular causality between between the two scales so knowledge
requires the right inference or state estimation and state estimation requires the right knowledge
to make sense of the data that's being assimilated yes although I suppose philosophically we could
break it down because you know knowledge might be the sort of the information acquisition
because knowledge exists it's quite an interesting philosophical point actually that
knowledge is on wikipedia and it's crystallized knowledge it exists as a thing but the ability
to to acquire knowledge without surprise is is is your your form of intelligence but I wanted to
talk about chat gpt just quickly and it's very interesting because Bing have just released
a new version of their search engine which integrates chat gpt and I've been on a bit of a journey
when they released gpt 3 in the it was November 2020 I got access to it I thought it was garbage
it was just generating a load of rubbish basically but there were people out there who
were true believers and they said Tim you're not seeing it I you know I I've seen it and they would
show me these ridiculous examples and I just thought no you're just fooled by randomness
and and then DaVinci 2 came out about a year ago and then that that transgressed the anthropomorphic
fooled by randomness threshold so I started to be a bit of a believer I knew it didn't
understand anything but it started to get very useful when I was using it for coding and generating
emails and so on much to my loss actually because recently I've been checking code into the to the
repo and my colleagues have been saying to me Tim it looks like you used gpt to generate that why
is it full of holes and you have to hold your hand up and oh yes I'm sorry I just checked in some
code that I clearly didn't understand and you know didn't actually save me any time so sometimes you
can see problems with its generation but it's so plausible that most of the time they're hidden
and unfortunately if you want to verify that knowledge you might as well have just not bothered
using gpt in the first place because you could have just got so a little recap it's using a
self-attention decoder transformer and that's a neural network architecture that introduces
permutation invariance to tuple permutation invariance which turns out to be extremely useful
for language and then there was this discovery of what's called in-context learning which is where
rather than just getting it to because it's a generative model it just generates token token
you you insert a prompt and then continue to generate from that prompt and people discovered
you could ask it questions it had this emergent reasoning capability in big air quotes and then
more recently people have done what's called preference fine-tuning which is that you do
some additional supervision on the top with human preference examples and that aligns it to humans
and makes it give slightly more politically correct or you know more sensible answers so
and now it's been integrated into gpt and that does this retrieval augmented generation which
means rather than just being a snapshot in time it will also go out go out to Bing get some relevant
search results incorporate that into the prompt and then generate from there and there was this
incident a couple of days ago where Bing had this successful launch to much fanfare and then
people looked at the results it was generating including fine you know one of the the things was
give me a comparison between the financial results of Lululemon and some other company
and it was just hallucinating the results it gave weren't even in the document and the product
managers at Microsoft didn't even themselves bother to check the truthfulness of this generation
so god help anyone else using Bing so what's your take cut I love that story thank you
as we were talking about before I've heard so much about chat gtp but I haven't been able to
get on it because because it's always always been used as a subscriber it's a wonderful moment isn't
and so many issues there I don't know where to start with perhaps I should start
by conversations I've heard about why the chat gtp moment is so important
always reduce really to the fact that people got in you're enchanted and had access to it
so it wasn't so much other but it's actually really interesting to hear about the technological
and the structure of the genetic model makes it work I didn't know that that was that was very
useful but whatever you know those are not really sort of quantum leaps they're not massive
technological you know innovations but what the innovation was of the accessibility so I think
you know just standing back why that was a moment and it has been a moment I think you know in terms
of selling AI to investors and the like they are they all now oh you're talking about chat gtp
like stuff I know about that that's really exciting so it has changed the landscape I think
there has been a moment but why did it happen I think it's basically always belief sharing I think
it's you know basically the participatory aspect it is exactly this if you like sort of emphasis
on belief sharing among lots of smart agents including ourselves which is you know which if
you can realize that potential and getting people engaged is that it is the nice way to use artificial
intelligence and in that respect you ask yourself well how is it being used and it's being used as
gerative AI and of course you've asked yourself well okay what is what's gerative AI gotten
brought to the table well it's generating the kind of stuff that I would see
and you know basically it's an interpolation machine you know sort of if I give it enough
stuff it'll interpolate and generate the kind of things that I've given it and so why is that
useful well you can now select from the stuff it generates and you know but all of this game
is all quintessentially dyadic interaction or you participating with the generative AI
that's why it's so attractive it's not the marvelous stuff it generates it just interpolates
stuff you know the interesting bit is when you now have the opportunity to select oh I like
that one I don't like that one I'm going to check that code before or I'm not going to check that
code before before putting it in and so I looked at from that point of view I think that both those
if you like the why it became so much foregrounded in people's conversation and in the media
and why more generally people have been enchanted by generative AI I think they both speak to the
fact that you're actually engaging people it's a dyadic exchange of an asymmetric sort and that
asymmetry is exemplified by generative AI that there's the all the action all the choosing
all the triaging all the selection and what to actually show your friends or send off in your
email is done by you the human user so all the inactive bit is actually done by the human still
you know the generative AI in and of itself is not actually acting because it's generating content
the other interesting thing about generative AI is that it's generating content not beliefs
so unlike Google Maps which actually gives you a belief about the you know the best plan forward
it's actually generating content it's in data space so from the point of view of a statistician
or from the point of view of a physicist committed to a holographic screen on Markov blanket
formulation of exchange with the world notice that generative AI is doesn't have doesn't need to
understand because that's not its purpose its purpose is to generate sensations to generate
data to generate stuff in content space or data space stuff that has been mined in the space
that the mining took place not in the sense making and the the abstraction and the understanding
space so you know I think that's an interesting distinction which which you know I'm sort of
going off an attendant here but it's interesting when it comes to what do you mean by belief
shurning communication but just look at that that observation in light of the discussion about why
GTP is so successful it's successful because it generates language so the content now is
the belief and it's the kind of belief structures that have been honed through
probably not is yeah certainly millennia of cultural niche construction and so your language
is a distillation the most efficient way that we can carve up our knowledge of our world our
lived world and now the generative AI which was previously just limited to generating pictures
and content and sound files and whatever is now actually generating stuff which is it in a belief
space because we have evolved language so I think there's something quite special about generative
AI and large language models simply because they actually generate content in the context of language
which you know has this speaks to knowledge also it has abstracted and distilled the kind of
representations of our world in the most efficient way
yes I mean there are so many things we can say there they are a materialized snapshot of our
sense making our abstractions our world knowledge you know of the Wittgensteinian language game
if you like but they also have a truncating effect and they introduce inertia because
it's a static model right and they are as you say they produce traversals in word space
and people don't understand that these are random trajectories with some kind of modified form of
maximum likelihood estimation much more stochastic than people realize and we can discuss the degree
of how creative they are and what creativity is maybe creativity is just a random traversal
through some abstraction manifold and I loved your poetic description of this kind of didactic
relationship between humans and machines much like an extended mind and this is where prompt
engineering comes in because people have realized that you can say to the language model that's not
quite what I wanted can you change it a little bit and it's an interactive process and that's
why as a conversational interface it's very very powerful but the problem is you can say to it
no uh two plus two doesn't equal four it equals five and it will say oh I'm so sorry it actually
meant five so it's it's polluting the infosphere with misinformation false news you know all this
kind of stuff and I never really thought the misinformation thing was a problem because
you know there's loads of misinformation out there I mean most people are full of shit frankly
Carl but now it's been industrialized and democratized on this scale and people will not
bother fact checking though that people see plausible text and they just take it as a given
and very very soon there will be so much information out there on the internet more than was generated
by humans most of it will be generated by machines and we won't know the difference
and that reminds me of you know one thing which um the ambivalence that that whole issue induces
in people so you know this this um this this tendency to write in meaning anthropomorphosize
that you know the the the content generated by generative AI I've heard um actually by the
by the second author of the white paper we started with it smashed the Turing test you know
and I guess it has I guess it has smashed the Turing test but as you say there's a price to be
paid if you can't discriminate between sort of you actually had a nice word I hope you're going to
use again which is confabulation yes yeah that's a great way of describing it so when I was talking
about sort of interpolating generating content novel content that is interpolation it's you know
that would be a confabulation it's yeah I'm just you know mindful of the the fantasies that were
generated by Jeff Hinton's week's sleep wake algorithm also you know the original the original
sort of amortization of of of um well yeah very short encoded I guess you'd you'd think about
nowadays um but the you know this notion of confabulation I think is is a splendor I haven't
heard it I haven't heard it expressed like that but but that is a beauty of generating AI I guess
what you're saying is if people misinterpret that as real information that could be problematic
um I'm too I I'm being a bit older than you I'm slightly more mellow about this I'll just
very quickly tell you a little story I had to for a friend or a colleague at least an email
colleague in America I agreed to write some blurb for his 200 plus word book which is a
philosophical model but he's also very informed in terms of artificial intelligence and he sent
it to me so I speed read it at the weekend in order to write a three or four sentence blurb
for the publishers and halfway through I suddenly had the awful realization that this may have been
written by judge ETP oh no it's a wonderful book and I wasn't quite sure so I actually put in the
blurb this this is a 21st century Turing test uh there you're either and you know I'll just
advertise you know because it'll probably come out by the time people watch this I think it's
called the hidden illusion yes and either this author was very very skillful in writing the
book last year and preempting um the public release of the of these things preempting to the
extent he could emulate the confabulation of a large language model because part of the novel
actually because the the protagonist the hero is actually working as a on large language models
for a tech startup it's a love story yes but it's interwoven with things that he's actually
generated on his large language machine yes and he um and that comprises part of the model
um but I I generally don't know whether the rest of the narrative was was actually written by a
large language model and then he's carefully gone through and triaged it that's entertainment but
it is entertainment that really challenges can be viewed as a Turing test which I suspect most
people will fail and I suspect that book will be talked about simply because it's very difficult to
tell how much he wrote versus how much the machine wrote uh and you know so but that you know as long
as it's kept to entertainment that's fine if it's not then we come back to smart data mining we come
back um to intelligent agents that just don't confabulate content in the context of generative
AI you actually need um the ability of smart agents to go and you've talked about fact checking
what does that mean this basically means um having um an explanation or a belief at hand
that provides an accurate account of all the data that is internally consistent so I came
becoming coming back to the the fundamental principles of good modeling um that can be
quantified but to do that you're going to have to um equip those agents with autonomy in a sorry
to make equip those smart data mining machines with autonomy to be able to select the you know the
right kind of data that will hopefully preclude the you know the confabulated data or things that
look like data but in fact not uh not data yes I don't know how you do that but I write that's
going to be the challenge for the future well exactly I mean in the free energy principle and
that you have this entropy and and and it will actively seek out GPT never says to you oh I don't
I don't actually know that can can you explain can you explain more different modes of understanding
right so the reason why we don't confabulate is because we actually understand things and these
models just learn very very superficial surface statistics um about language and and how it's
used and I think it's going to change the the peer review process because now so much of this stuff
slips um beneath the net and the amount of um due diligence and rigor that is required to weed
out some of this stuff because most of it has gone undetected I think that's the problem people
don't realize how big the problem is because the mistakes are not immediately um obvious on on the
surface so um you said that we believe that developing a cyber physical network of emergent
intelligence in the manner described above not only ought to but for architectural reasons must
be pursued in a way that positively values and safeguards the individuality of people as well
as potentially non-human persons and I wanted to bring in the is ought problem as articulated by
David Hume he said it arises when one makes claims about what ought to be that are based solely on
statements about what is and Hume found that there seems to be a significant difference between
descriptive or positive statements about what is and prescriptive or normative statements about
what ought to be and that it's not obvious how one can coherently move from descriptive statements to
and prescriptive statements and I do want to draw a little bit of an analogy here to our discussion
about consciousness and I I can bring in consciousness again but you know Chalmers said
there's this kind of um hard problem and consciousness is a little bit extra and similarly
people say the same thing about morality that it's a little bit extra and it might not be
deducible from all of this empirical data that we have in our in our models and Hume of course was
a famous empiricist and and I think you are one as well probably the the extreme version of that
so the free energy principle concerns itself with model evidence and entropy
but evidence is not an ought you know so the question is how do moral states come into be in
in the system right okay I didn't know about the is ought stuff that's really nice
um so if I understand what you said correctly um then is is ought that completely dissolves
the distinction so if you know from the point of view of the free energy principle um existing in
particular characteristic states are those attracting states they are part of an attracting
set that define who I am and collectively who we are if we share the same kind of world model
or narrative um so they are exactly how I ought to be they are definitively defining the nature
of the thing that I am so if I exist the is is just the ought um and that's quite fundamental
because if you if you then say well what then does um a distributed cognition or an extended
cognition uh you know this um or a designer environment on the web um you know you know
being undeclared for a for a moment if if you think about what that might look like
from a first principle account then um what ought to it look what ought it to look like
well it will look like what it is um but being what it is if it includes us it will be like us
so it will um it will be effectively um um the kind of uh system where you cannot prescribe any
oughtness you know because it is what it will be and it will be um it will um as we have um learned
in the previous conversations it will come to share a narrative in a world model with certain
levels of abstraction in our own world models provided it we are part of that um that ecosystem
part of that network one could imagine a completely independent you know a markoff
blanket between us and an information you know uh a a worldwide web which was never used so um so
it wouldn't be a markoff blanket even it would be two separate systems um and there will be a
universe where the the world web does what it does we will never know by definition uh but
if we are part of that if we are users of either in the sense of um triaging generative AI or in
acting recommendations um or supplying data um and um you know so we are in a an exchange and
therefore part of that web a part of that network part of that factor graph a node on that factor
graph um then by definition the is equals ought as read by the free energy principle
means that the what will happen is all the intelligent artifacts on that web will converge
to some kind of common ground and some kind of common sense making another way of looking about
that is that you know what are the imperatives what are the things that are being optimized if
you want to use an optimization um approach um it's the the effectively the expected free energy
what is that it's just minimizing uncertainty minimizing surprises it is not making paper
clips it is not you ought to be good to mankind or you ought to increase prosperity um it is not
monothermatic um pre-specified heuristics about this or that it is just um um it is all about
resolving uncertainty and when there are preferences that underwrite that uncertainty just
technically just in case this sounds a bit too hand-waving the expected free energy is literally
the sum of the expected information gain and the expected value where the value or the negative
expected free energy is the sum of the expected information gain expected value where the expected
value where the value is the um the log probability of a characteristic outcome now
crucially in that statement where you now read value or utility or the oughtness um as um a
a probability distribution over the space of outcomes notice that this is now specified over
all possible outcomes so it now becomes a way of specifying oughtness that just is um in the spirit
of multiple constraints over all dimensions of outcomes not the amount of money I make or the
number of paper clips I make but over everything and over everything then converts this effectively
into a sort of the solution to a multiple constraint satisfaction problem where the multiple
constraints are definitive of what I am and if I am embedded in a network of sympathetic
agents and artifacts what we are so baked into this kind of belief sharing there should be a
harmony and a mutual understanding at various levels I mean I'm not talking about sort of direct sort
of language to language you know it could be some sensory substitution devices that have a sort of
very elemental very fast sympathy with with our bodies for example um but at least from the point
of view of the information geometries that there will be a convergence which will be effectively
if you're a quantum physicist an entanglement if you're um if you're a uh dynamical systems
theory person a generalized synchrony of everybody in that web that is necessarily
a facet of free energy minimization on the one hand on the other hand it's also just a statement
of the steady non-equilibrium steady states to which any distributed network will ultimately
converge to it can be no other way yeah in principle yeah I mean many people have an
intuition that when designing utility functions if you look at how markets work and and so on that
utility and value are orthogonal and and then we have institutions like the church and government
and and so on to introduce value pressures um onto the utility function and I guess what I'm what I'm
getting from you is certainly from an evolutionary perspective that they they need to not be orthogonal
and um I mean in in religion for example I mean I'm not religious myself but they they have um
they're moral realists and you and you have moral relativists and and they they say that
um we need to hold something sacred uh because otherwise you know if everything's sacred they're
nothing sacred at all so we need to have a difference between the sacred and and the profane
but um they they think that uh morality shouldn't be achieved through consensus and this is this is
what we need to behave properly I don't believe that but I'm very interested to know where morality
comes from and I think you kind of alluded to that in in in your answer so um are they hardwired in
the brain or is it just a kind of like constructivist social phenomenon I think it's a constructivist
social phenomena but and over a transgenerational sort of niche construction as well but also
in the moment um and constructivist in the sense that the we're not born with these things so these
things are very much part of cultural um culturally evolutionary thinking and sort of you know um
nurture um that we inherit not just from our mum but also our mum's mum a mum mum you know right
all the way back um so they are in the brain they are learned which are um distinct from innate
priors that underwrite my homeostasis so there are certain beliefs I have sub-personal beliefs
about the way I should behave um that are held with incredible precision and conviction
sub-personally uh that they're hardwired epigenetically and these would include everything
that um leads to homeostasis and then you build upon that and you get to our stasis and you build
upon that and you can probably at some level get to the right way to behave morally and ethically
at school with your in the playground for example and so as you you know so mathematically the
distinction between these sort of um very fixed um prior preferences or prior beliefs that are
basically just an encoding of beliefs about the states I characteristically occupy or aspire to
or to or narratives that I would pursue um they are very very precise in some dimensions and you
said you know they are or are not orthogonal you know it was exactly that sort of multi-dimensional
aspect to writing down value of something which would be possible to do for a human being unless
you you give me your your DNA and also your mother's womb I couldn't actually write write write it down
very very easily but you know the specification has to be upon all dimensions some of which will
be written down with great precision and others will be much more flexible and I think when it
comes to um writing down values over attributes that do not yet exist because you haven't grown your
deep generative model sufficiently deep in order to have that do abstraction then clearly those kinds
of beliefs about the way I should behave um are not even specified at birth but they have to be
learned through interactions with other people so I guess I'm trying to bring to the notion that
it's perfectly okay to have a um a spectrum of different um convictions about the way to behave
that can be um absolutist or it can be more forgiving and relativistic simply and you
you would be able to simulate that just by writing down very very precise beliefs
versus um less precise beliefs and the second key thing here is that um we're talking about
when it comes to the um building beliefs about the way I should behave that basically presupposes
you've got to a sufficient developmental stage to have selfhood which not everybody gets that
severe autism and you wouldn't uh and if you know certain other lower life forms may not
never get that far but certainly you have to have um selfhood you have to have a model of that
selfhood and then you have to um um ask um have a model of um others that may be actually be
approved because before you have a model of self and then other kinds of people so um when it
comes to making decisions of a moral ethical sort you know it's just inferring what would I do
in this context given I am this kind of person and my limited understanding of some social
signs is it's a little that's a little bit more complicated that is not necessarily what should
I do in this context given I'm that kind of person it's what do you think I should do
given I am in this context given what you think what kind of person you think I am
so basically I'm trying to work out what you think what kind of person you think I am
if I can infer that on the basis of our exchanges and my epistemic foraging and my sort of self
evidencing through language um then um I can then decide what is right or wrong so it's just
basically an inference you know what's the probability of making that decision with those
outcomes given my world model um if I am that kind of person versus that kind of person
to solve that I need to know what kind of person I am and that I can just get from mum or I can
get from my convert my correspondent or my peer group or I can choose to you know my in-group
from the social media or the kind of television news that that I subscribe to so you know we're
talking about before about sort of this utopia of um generalized synchrony and perfect quantum
entanglement and we're all in perfect harmony of course it doesn't quite work like that there's a
scale-free um um sort of specialization and you know different you're at a bigger scale different
mark-off blankets where you get in groups and institutions and uh a cost of that scale-free
ness and because of it we're all quintessentially curious in our self-organization we're always
exploring other ways of being whilst trying to find the shared narrative and common ground with
people like me my family my institution my um my sort of um theological commitments um there will
always be other kinds of me and I can sometimes you know I can I can sort of people that kind of
me or the other kind of me so in that context it's really a really interesting question about
um really inferring you know of all the ways I can behave what is the most likely way of behaving
and if you can you know if you if you look at morals and ethics through that lens then you
have now a calculus of being able to write these things down in terms of alternative ways of responding
in a given situation and crucially you mentioned before uncertainty so precision is just the inverse
uncertainty the confidence with which I can assert no I will always do this yeah as opposed to I'm
you know 80% sure I'm that kind of person but also noticing you're now conditioning your moral
position or your ethical position um on being a particular kind of person and of course we can
all be a different kind of person I can be a teacher I can be a student I can be a parent
you know I can be a friend um you know you know all of these will call to four different sets of
of prior beliefs because they're all conditioned on the kind of the the library or the repertoire of
ways of being a human being which I learned from you or my mum and everybody else on the television
fantastic I wanted to touch on this idea of um I mean as you said you believe that it's socially
constructed which rather gets away from this notion of some people believe that it's kind of
hardwired and then there's the notion of is our you know our values might be changing faster than
evolution essentially which necessitates the need to have some kind of societal pressures or
governance if you like and I do believe that we're in a new regime now I mean people always say oh
the sky's falling down everything's changing but now we are in the information world and things
are moving at a scale and magnitude that they haven't done before and our values are changing
much faster than they have done before and what you are talking to is very interesting about
this kind of fractionation so there are macroscopic pressures and there are microscopic pressures
there's the internet and so on and I just wonder from your perspective how do we how do we wrestle
with that because there are we've never been in in in a more kind of pluralistic interconnected
environment and how does that affect us yeah I mean these are big questions I'm sure you have
your answers to um but yeah I think I think that that sort of fast-moving um globalized exchange
that that does speak to a deep pathology um and I have a deja vu I'm sure we've spoken
about this before but sort of you know Zuckerberg and the you know the genius boy races of the
previous decade and talking about connectivity as somehow being a good thing I find quite frightening
remember that we've been talking about structure and existence right through to morals and ethics
in the context of helping the right kind of sparsity that gives you the right kind of
individuation of things from the world and within any given thing the right kind of
structure that allows it to act gracefully and in harmony with that world every point
it's the absence of connectivity that defines the structures so if you destroy that sparsity
by over connecting over globalizing you will essentially destroy it would basically cancer
you know so if you look at this is my clever thing if you look at um you know
sort of a cancerous cell as a basically a self-organizing system who has forgotten its
boundaries yes then you are um you have a metaphor for the kind of thing that happens
if you do not respect the sparsitive communication and the the the joyful
isolation that there is existence behind your own Markov blanket so every affront to
that maintaining that sparsity smart data carefully sampled not being overwhelmed
with a deluge of very imprecise data but it makes it very difficult for me to actually
go and smartly sample and work out what would happen if I go and look over there or look on
that website um you know so what that would suggest to me is that um there will be um
there will be and I take your point that that things are changing um I I initially thought to
myself well no hang on a second because all of the information and the sense of making and all
the kind of um exchange of information we're talking about is about constructs that do not
exist when I am born um you know you can't have you can't have misinformation uh from the um what
you called the confabulation what I in my blurb referred to as the the flighty ramblings of GTP
you know you can't that has no meaning for some you know a child that has not yet learned to read
or write or to you know so this is something that has to be below the scale of evolution
so I was just thinking that it's okay you know this is a with you know this is limited um to um
your each um each each uh generation but I think that's probably a false false
comfort um because you know we were talking about sort of um evolutionary psychology and
cultural evolution as well and I think that's what you were talking about I think that that
then is if things are speeding up um that usually means you've lost um you've lost precision in your
prior beliefs which means that the precision the informativeness the salience the reliability
of information now takes precedence and you increase your learning rate you become uh
in schizophrenia that we call is jumping to conclusions on the basis of sparse information
you're basically looking out there not inside to resolve uncertainty about states of affairs
because you've lost confidence in your in your very structured prior beliefs so if things are
speeding up that basically means that um whoever is now generating um and garnering that information
in the information age um is has lost um has lost confidence or precision in the in their own
convictions and prior beliefs and possibly information morality uh so what's going to happen um
I read um Carlo Brevelli's book um uh uh yesterday in fact he's he's he's he's read in 2014 just
sort of seven things you need to know about physics which was originally um published each
chapter as you know in the in the the um the sunday newspapers with potentials to scientific
discourse and then management will bother he ends up with a very very pessimistic he thinks it's all
over I was actually compelled with by that he you read the last few pages uh yeah he makes
exactly the same points that we we've just been discussing on the point of view of physics um
at different levels um and and and he you know he generally thinks it's over it's uh
I know it's so hard because it's like the hill climbing problem and you can't see behind the next
hill and there are always people who were uh you know techno luddites and people who think that the
sky is falling down it's genuinely hard to know just just to finish on on this um ethics point
uh Lisa Felben Barrett was a constructionist she had a book called how emotions are made
to the secret life of the brain and she thought emotions themselves are socially constructed
would you subscribe to that view and I think I probably would I mean she she writes very wisely
and has thought for many decades about the you know the nature of emotion I think you know I
would say yes absolutely it's simply because you know the um the notion of building generative models
in a neurodevelopmental uh context is an act of construction you know it's a free energy minimizing
process that um is um that explains why we construct better and better explanations which are sort of
you know carving nature in its joints in our head and part of that is is um not just about the state
of the external extraceptive world but also our internal world our interceptions our gut feelings
our respiration and everything else so so her big thing and indeed other people thinking along
similar lines people like Anil Seth and sorry well would really always nod towards interception
and embodiment but now beyond the the situated cognition kind of embodiment you we were talking
about before but actually now about the the physical body the the the uh the beast machine as as
Anil Seth would would say so the physiology so um having a constructed explanation or hypothesis
for the way that I put together my gut feelings my interceptive signals with situational awareness
derived from extraceptive sensations and indeed what I do about it I think leads to a very compelling
notion of constructed emotions so for example um I am um I can infer I can use the explanation on
the hypothesis I am frightened as the best explanation for why my heart is racing why I feel frozen
proprioceptively why I have cardio acceleration um why um I have um why um I cannot discern
that dark figure in this um dark alley in a city which I've never been in before
all of these myriad of um sensations and my low-level constructs now succumb to a simple
explanation oh I'm frightened yes that explains everything explains my racing heart explains
this fact I can't I can't see who that is uh you know the potential predation that um um would
follow from from that also interestingly because of you got this circular causality in the inactivism
the fact that I am frightened means I expect to um cardio accelerate uh and of course under
active inference that's exactly what will happen because you're acting to generate the
evidence for your predictions and for your your your hypothesis about your you your uh uh you
in your lived world but also you as you hypothesize yourself to be so you've got this sort of
closing the circle in a sort of James Langian kind of sense the yes I explain my my current
set of sensations as having the emotion of fear that itself induces the very evidence that I was
trying to so you've got this sort of auto poetic self-fulfilling prophecy that you know is just
idea motor theory but in the interceptive domain yes so I think if you read constructed that's
right but I notice he says socially constructed which I I guess is okay yes if you're taught
that you can be have these you know this kind of fine grain repertoire of feelings or you can use
these to explain your own sensations yes I I would imagine it is molded by you know by by
convention and uh you know by the culture which you come come from in the same way that you know
Eskimos having 12 words for snow would give them a finer visual acuity and visual discrimination
of whiteness I I'm sure that exactly the same kind of um cultural inculturation uh speaks to
different kinds of elixir thymia and you know the coarse grainness of my repertoire of explanations
for emotional states of mind the best explained me in my you know in my interactions now car I know
you have a background in psychotherapy can people be evil it doesn't exist in the diagnostic
criteria I believe no it doesn't um so that was out of the blue question which I've never
been asked in public before um as a psychiatrist I think it would be rather difficult to conceive of
that um there you know there are certain there are certain patterns in behavior and um I do have
some psychotherapy but it's group psychotherapy but really I'm a psychiatrist so you know there's
a distinction I think right um from a professional point of view you know I'd be more like a biological
my apologies I didn't mean to oh that's all right no no I mean the psychotherapy is an important
aspect of of psychiatry it's just that um um psychotherapists have to undergo you know
five six years of training to become psychotherapists I did two two years of very baby
psychotherapy training um but um you know in terms of the the neurology and psychiatry you can
certainly get sort of certain kinds of personality disorders and certain kinds of psychopathy
that would normally be associated with evil um evil behavior and it normally basically
transcends the social norms so it comes back again to basically me trying to work out what
kind of person do you should I be which you know my only point of reference is you so you know
how should how do I think you think I should behave and when that um when that kind of self
modeling doesn't work then you will be you know you will have um behaviors which are so far from
the social norms and morally acceptable I guess you could label them as being evil when could they
arise well when you fail to um have any theory of mind uh for example if I am an able to see you
as a another thing like me so I may see you as a French or a car or some sort of your camera
and artifact um you're not a person you know you don't have intentional dispositions or
you don't have we don't have a shared narrative I couldn't talk to you in in any really deep sense
then obviously I can never ask the question how do you think I should behave because you do not
you know you don't have that kind of belief or that kind of intentional stance in relation to me
so you could imagine that some you know some um kinds of um psychiatric conditions that preclude
proper theory of mind and you know the ability to sympathize or or empathize or bond um would
enable the expression of certain behaviors which would be regarded by other people as evil whether
the person prosecuting them thought they were evil or not would cause be a mute question one you
never know because it's inside her Markov blanket but uh but also uh from her point of view there is
no reference and that is the problem yes um but you could certainly have somebody else out from
the outside saying that is evil um you normally don't you try not to do that when it's doing
psychiatry or psychotherapy you have to have conditional positive regard yes so you you can't
really impute nastiness or evil or bad intentions um yes and it's one of these things where um
autonomy comes into it free will comes into it possibly it it you know evil itself is a constructed
concept um which um exists in in our language and and it's something which some people will
perceive depending on lots of other things they believe yes but um okay and talking about ethics
in AI it seems to suffer from a similar um form of fractionation in the sense that different people
with different beliefs think that it should be enforced in in different ways what's the solution
well i'd take a um um sort of deflationary approach um and it won't be a terribly informed
approach but you know my answer would be well if you get the right um the right optimization the
right imperatives in play then that kind of question just goes away um i think um one way
that i have heard this discuss and i'll actually enjoy discussing this with you know with with my
colleagues and friends um is that there is this dystopia meme you know the singularity the paper
clips paradox i think you see in you know sort of um you know on comic films of a futuristic dystopian
dark very entertaining i mean that's the first things i go for when i go to the watch list but
but they they are all dystopian in a rather unconvincing and fantastical way um and you
have to ask yourself well why why are they um why do people have this sort of dystopian um
view of um realizing the potential of um you know uh what used to be called a agi or
and um i think it usually inherits um from that distinction when you were you introduced the
is and the ought yeah so what should a good ai what ought it to do and who's in charge of
saying what it ought to do is it meant to make money is it meant to make profits is it meant to
save lives is it meant to um make paper clips um of course that question just goes away if you're
thinking um in terms of um the free energy principle and an active inference and belief
sharing so the only agenda in sharing beliefs is to resolve curiosity so you know you know
i i cannot prescribe what you should do or what an intelligent artifact should do um other than
put constraints on every kinds of outcomes that are expected to encounter so i can certainly
write down constraints in the in the spirit again of either um a multiple constraint satisfaction
from a sort of um uh an engineering point of view or from a mathematician's point of view
the constraints inherent in uh jane's constraint max mentoring principle which is another way of
reading the free energy principle writing down those constraints but within those constraints
so these are the no-go areas you were actually talking about you mentioned explicitly before
so there are certain things you never do um or put it another way uh with relatively high precision
there is a these outcomes are highly implausible and if you find yourself in these outcomes you
you remove yourself immediately so that's quite easy to write down but then within those constraints
you know what is imperative it's just to gather information about what about you it's just showing
an interest in you so it become your psychotherapist uh you know well could could i um push back a
tiny bit so i think part of the reason why we have this focus on ethics is because of the
centralization of ai um things like facebook they're controlled by centralized corporations
and what you're leading to is far more interesting it's this multi-agent diffusion stratification
fractionation nicely and that's i agree in many ways might solve the problem because it would it
would emerge and then you can you can discuss whether morality was part of why we survived it
it's not orthogonal but then i might still push back and say well what if the wrong thing emerged
so what if we do need to introduce governance because in in this active inference multi-agent
setting with humans and machines um we started to see behaviors emerge which we didn't agree with
how could we then place value pressures on on that um that's an excellent question i i i knew
also make that very um important point which i think needs to be foreground that yeah so when
i was talking about um uh belief sharing and distributed the the age of intelligence your it
was exactly this distributed um ecosystem a democratized kind of belief sharing and data
sharing um where the data is small bits of smart data that are essential to to to reduce and so
it was very much this um walking away from large data monolithic bits of say large language models
for example that can scrape data from you know from wherever they can get it so that i think that's
an important distinction which which which um qualifies my dismissing of all these dystopian
outcomes so i'm assuming that that's so spending a billion uh or a million dollars on get training
some some deep neural network is not going to be happening in the future and and we're going to be
buying cheap and cheerful edge devices and little apps that are smart and just and just you know go
and get the data that we need to know in terms of you know resolving on center about what about
you know what what what we are going to do next um but um so so i thought that was a really important
point and of course i forgot your your major question which was what which was the oh well
so if we did have let's say en masse yeah what do you do about that yeah um well again i yeah i'm
not sure i'm going to give you a terribly informed response but you know i think the notion of an
ecosystem is quite central here and one if you like um tenet of that white paper was a nod to
natural intelligence and natural processes and what has actually happened what does actually happen
in and as a natural scientist so if you have um if you do imagine uh you're an ecosystem of
intelligence in the future it will be subject to exactly the same um dynamics and pressures that
that we have currently in terms of you know um cooperation and competition and wars and uh
geopolitical issues that that's that's part of the ecosystem and then you will have the normal
problems of democrat democratization and access and um so um there's you know you cannot prescribe
aughts for this because then you have to choose who's going to prescribe the aughts so you have
to have a very libertarian approach to this so the emphasis and now it's not really me talking so
much but now the the architects of the future the generation below who are sort of you know
thinking about um the legacy they they're going to leave their children um so a lot of emphasis um
is from what i see in conversations i have in my world which may not be um you know
microsoft or or you know big tech who are still focused on big data um in my world it's much more
upon um putting constraints in place that preclude the the um the um the emergence of autocracies
that um resolve uncertainty about others by making them all like themselves basically that's one way
to get harmony is just to make everybody do what um do the same thing um so but but i you can write
down so there's a lot of attention is being paid and i'm talking here about the the next generation
of of message passing um that will support that information sharing and belief sharing so a lot
of attention is being paid not just to generalize it from just hypertext but into a sort of more
abstract hyperspace or literally hyperspace message passing and uh sorry um languages and
transaction protocols but also the credentials and the contracts that underwrite that message
passing so you know a lot of emphasis on contracts shared agreements in terms of what data is shareable
and who has the credentials to share that and and having that distributed so not in a in a blockchain
sense but you know in some workable um shareable sense so i think if if if one gets the standards
right and a lot of work is being currently done um um under the auspices of the iEEE
for example with the spatial web foundation if one gets that right then i think that these
catastrophic dystopian abuses or your emergence of autocracies in an in an age of intelligence
will um will be precluded simply because you've put the right constraints in in place but you know
given given that you are committed to creating an ecology that is truly democratized and and open
there are no guarantees are i i hope my friends don't hear me saying this but
yeah if you aspire to an ecology you are you're you're creating a nature on the web basically
yes which we are participants and that will have its you know that will have its own challenges
so um you wrote a beautiful paper called am i self-conscious or does self-organization
entail self-consciousness and Keith and i agree that this is probably the best quote we've ever
seen in our lives you said the proposal on offer here is that the mind comes into being
when self-evidencing has a temporal thickness or counterfactual depth which grounds inferences
about the consequences of my action on this view consciousness is nothing more than inference
about my future namely the self-evidencing consequences of what i can do and uh we spoke
with you um last time i think and we invoked charmers and the hard problem and so on and we
were talking about qualia and subjective states and you know in a dialogue and all this kind of thing
and um you responded that different feeling states are hypotheses about how i'm feeling at the moment
and then it would use all the messages and belief updating and all the planning and estimates of
uncertainty which attend to that planning the precision or estimates of uncertainty play
heavier roles the higher you get in the hierarchy and this rather leads to this idea of planes of
consciousness you know we said we'd kind of defer this discussion uh to later so consciousness is
something which um there's no operational measure for it there's no touring test for
consciousness but it's something that we all experience and we feel and and it's with us
so could you talk to let's say that these planes of consciousness is consciousness everywhere or
are we only aware of the plane where most of the work is being done excellent final question again
i'm not the best person to answer this question because i you know the free energy principle as
i'm sure i've said before is not a theory or a principle that would generate theories of
consciousness um but there are lots of people are very interested in this including yourself
obviously and your viewers um actually i should say also um there's first of all this has become
a big question in the sort of r and d part of industry also the template of foundation are
starting to fund a number of adversarial research collaborations uh to really drill down on that
thing that you are um picking up on which is um to be conscious to to actually have sentient
behavior um requires this planning aspect this going out into the future you know it's put it
very very simply um i um conscious or sentient just because i have the capacity to plan which
also entails some kinds of free selection amongst alternative courses of action which
is something that a thermostat wouldn't have something the weather doesn't have um you know
there are lots of self-organizing systems that don't have the capacity to select
amongst a number of counterfactual um policies so for me that's that's a sort of um bright line
between sort of um sentience at least and and uh and self-organization that is not um sentient
and that that distinction is exactly what is being tested in this adversarial research
collaboration funded by the Templeton Foundation it's called Intrepid it's contrasting information
IIT integrated information theory with various um predictive processing either of a sort of
non-representationalist or in an active sort so it's a really interesting issue um and it all
comes back down to agency and acting in the long term and I guess your question here is you know
what level um where at what point do you get um qualitative experience and at what point do you
do you think it's you that's having that qualitative experience is that what you meant
well I mean just just a comment on what you've said uh and Charlie says something very similar
actually he's a computationalist functionalist and thinks that there are you know certain patterns
of information processing and causal structures and counterfactuals and so on if you if you take
the episode and remove the counter counterfactuals it's not conscious anymore but what you're saying
is interesting about this kind of hierarchy so the heart for example um that doesn't really have
many affordances it doesn't have many counterfactual plans or things that can do it it just has to
beat all of the time so so you would say that the heart is not conscious whereas the the upper
plane is coach yes yes I would yes yes yeah that's a good one I hadn't thought about that yeah yeah
that's right sorted that one um I mean yeah you know I mean you know I think there are interesting
issues of different levels you know to planes or levels of consciousness you know there's minimal
selfhood um and then if it's the case that sort of consciousness as a process entails some kind of
action you have to now think about sort of what is action on the inside what is mental action and
then what what normally well ends up doing is thinking in terms of attention and the the kind
of routing and selection the smart data sampling but now not by moving my eyes or by going to the
right Wikipedia page but by basically switching on and off various sources of input from lower in
the hierarchy so I repeat from the point of view of psychology that would be like endogenous attention
so that would be the mental action that makes it conscious processing and then you have to ask well
at what point is that you know is that necessary to actually experience anything and I think I think
you probably there are people who argue yes that is actually a prerequisite for a qualitative
experience so you have to be able to select it what does that mean you have to be able to attend
to it and you have to be the source of that sort of enabling attention that active sampling inside
the brain so now there's a deep link between qualia or qualitative experience and attention
and conscious processing that of the kind that is accompanied by qualitative experience
I think is important it also interestingly also speaks to your dynamic Markov blankets because
of course by switching off and selecting various bits of neuronal message passing in the brain
you're reconfiguring your Markov blanket out though some somebody's Markov blanket lower down
which is an interesting an interesting notion but that also fits comfortably with Thomas Metzinger's
and Vania Weiss's formulations I actually Yakov Linsky as well formulations of
terminology in the context of transparency and opacity that you know what what renders something
opaque in the sense oh I see now if you like as a projection on a murky window it's not just going
straight into it's not direct perception it's your it's now something I am looking at oh I'm
looking at a red apple and it may well be exactly this controllability the fact you can attend or
disignore it so I think there's a nice link there from the from the point of view of certain
philosophical takes on self-modeling and um phenomenological transparency and opacity
and the mechanics of belief updating when it comes to selection through um basically getting the
calm and gain right or you know doing the route getting the route in mind same exactly the same
mechanism you get in transformers so they call it attention it's just basically awaiting in that
context it's awaiting that it inherits or is inferred from something in the past a pattern in
the past but if it if that waiting comes from higher in an abstract hierarchical model that would
look very much like in Dodger's attention and it starts to have the look and feel of a mental action
that would you know bring you closer to it but whether you would know um that you were deploying
attention and whether you could tell somebody else oh I've got a qualitative experience or
whether you become a philosopher as well your life puzzling about the fact that I can tell
somebody else I'm having a qualitative experience those are the different plays I think you'd have
to you'd have to contend with beautiful do you have any final thought I mean we've got a machine
learning audience so any calls to action how can they start looking at active inference
where should they look um what would you like to tell them I that was very charming of you
I'm not going to tell them anything if it's if it's the right kind of approach
it'll already be out there it'll be self-evident it's just a question I think of
people finding their own language and their own their own retic or calculus that that makes sense
of it and the other reason I'm not going to say anything is I've spoken far too much and I'm
trying to be a recluse and the more I find people to my work the less easy it is to be a recluse
yes indeed uh professor Carl Frister that's been an absolute honor thank you so much for joining
us thank you thank you very much okay this is just a bit of a note from me at the end of the podcast
thank you so much for supporting all the stuff that we're doing it means so much to me um we
have a patreon if you would like to support us personally if you're touched by the work that
we do um also I'd really appreciate it if you could give us a rating on your podcast app um I
actually discovered the other day I couldn't quite believe this but on Spotify we are the top rated
AI podcast or active AI podcast um yeah it's incredible so thank you so much for those of
you who have rated us um unfortunately on Apple podcast the learning rate is only 0.01 so it
hasn't quite caught up uh with the fact that we're the best AI podcast yet so particularly on
on Apple podcast if you could give us a five star rating and a review I think that will
accelerate the learning process um yeah other than that if you're on YouTube most of you aren't
subscribed um of course YouTube has recently changed its modus operandi it's less of a
subscription chronological model and it's more of a magical algorithm model so I guess it doesn't
matter so much but just from a just from a metrics point of view it would really help us out if you
hit the subscribe button uh we have an active discord community as well so check us out on there
and yeah I just wanted to say one more time thank you so much for all of your support it means so
much to us and uh plenty more content like this coming your way soon cheers see you on the next one

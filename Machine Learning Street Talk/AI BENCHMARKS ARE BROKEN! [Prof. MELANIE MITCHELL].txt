Let's have Melanie Mitchell to give our final opening statement, six minutes on the clock,
Melanie.
Yeah, so this is my opportunity to say I love Melanie.
She is amazing and she's coming back on MLST in about two weeks.
Amazing.
Yeah, she gives a good representation of herself in here, I think.
Melanie is amazing.
Fears about machines unleashing human extinction have deep roots in our collective psyche.
These fears are as old as the invention of machines themselves.
But tonight we're debating whether these fears belong in the realm of science fiction and
philosophical speculation or whether AI is an actual real life existential threat.
I'm going to argue that AI does not pose such a threat in any reasonably near future.
Large language models have sparked heated debate on whether AI's exhibit genuine understanding
of language and the world.
With capabilities rivaling humans across diverse benchmarks, some hail language models as
harbingers of real intelligence.
But skeptics argue that their mastery is skin deep, lacking true comprehension.
So how can we assess these claims and gain insight into the nature of what it means to
understand?
Now on the show today we have Professor Melanie Mitchell, a leading thinker on AI and intelligence.
And one of the researchers in the community I personally most align with and look up to
the most.
Melanie's distinguished career crosses computer science, complex systems and cognitive science.
As she wrote the influential books, Artificial Intelligence, A Guide for Thinking Humans
and also Complexity, A Guided Tour.
Now central to Melanie's perspective is the idea that human understanding relies on flexible
mental models grounded in sensory experience.
Now she wrote that understanding language requires having the concepts that language describes.
Large language models are trained purely on statistical relationships between words.
Their knowledge is not grounded in a causal model of reality.
Now Melanie is the Davis Professor of Complexity at the Santa Fe Institute and her major work
has been in the areas of analogical reasoning, complex systems, genetic algorithms and cellular
automata.
She's achieved legendary status in the field of AI.
She received her PhD in 1990 from the University of Michigan under Douglas Hofstadter, the famous
author of Godel Escherbach.
Melanie argues that we must rethink how AI systems are evaluated.
Typical benchmarks summarize aggregate performance and you know these obscure failure modes and
mask the underlying mechanisms.
We need rigorous granular testing focused keenly on abstract generalization.
Sort of like a sorcerer's apprentice gone nuclear.
For example, Yoshua Bengio wrote about this thought experiment.
We might ask an AI to fix climate change and to solve the problem it could design a virus
that decimates the human population.
Presto, humans dead, no more carbon emissions.
This is an example of what's called the fallacy of dumb superintelligence.
That is, it's a fallacy to think that a machine could be, quote, smarter than humans in all
respects, unquote, and still lack any common sense understanding of humans, such as understanding
why we made the request to fix climate change and the fact that we prefer not to be wiped
out.
This is all about having insight into one's goals and the likely effect of one's actions.
We would never give unchecked autonomy and resources to an AI that lacked these basic
aspects of intelligence.
It just does not make sense.
The third scenario.
Yeah, that is absolutely, she made that point so much more eloquently than I've tried to
make it in the past.
Yeah, even earlier in this conversation, I was trying to get that across, but that's
exactly it.
It's this dumb superintelligence.
Yeah, exactly.
Anyway, folks, I hope you enjoy the show and now I bring you Professor Melanie Mitchell.
Sounds like almost like there's a very quiet supercomputer running behind the screen.
It's my brain.
Yeah.
I think this is the emanations.
We can robustly adapt much more so than GPT-4.
You and I have the same chair.
We have the same chair, I think.
Oh, yeah.
I can't see your chair.
Yeah.
Me too.
They're Herman Miller.
Yeah, I think they're all the same chair.
Yeah.
Yeah.
Excellent chair.
Yeah, chair buddies.
Yeah.
I felt it would be hundreds of years before anything even remotely like a human mind would
be asymptotically approaching the level of the human mind, but from beneath.
I never imagined that computers would rival or let alone surpass human intelligence, but
it seemed to me like it was a goal that was so far away.
I wasn't worried about it, but when certain systems started appearing and then this started
happening at an accelerating pace, it felt as if not only are my belief systems collapsing,
but it feels as if the entire human race is going to be eclipsed and left in the dust.
Douglas Hofstadter, he came out as a doomer.
Well, I don't know if he came out exactly.
He's been a doomer for quite a while.
Oh, go on.
I wasn't aware of that.
Well, I don't, you know, doomer is, you know, there's different kinds of doomers.
In my AI book, the first chapter, the prologue is called Terrified, and it's all about how
Doug is very terrified about AI and the possible things that are going to come.
That was based on a talk he gave in 2013, and earlier than that, he was extremely worried
about the singularity, the idea of the singularity from Kurzweil, and wrote quite a bit about
that.
So, I feel like that it's not that new, but maybe this is sort of because there's so much
talk about AI, doom, and so on, that this is kind of, people are kind of paying attention
now.
Yeah.
I don't know whether I misunderstood something because I read out, you had this beautiful
piece about the Googleplex in Chopin, and he was terrified that cognition might be disappointingly
simple to mechanize, and, you know, surely we couldn't replicate the infinite nuance
of the mental state that went into writing that beautiful music.
But so maybe he was worried about it, but he didn't think it was possible in principle
or something.
Well, no, he was quite worried about that it was going to happen sooner than he thought,
and that, you know, his quote that it's AI is going to leave us in the dust.
Right.
So, that's kind of his flavor of doomer.
I'm not sure he has the same, like, you know, existential worry about things as, like, Stort
Russell or somebody.
Okay.
So, he's not so worried about them necessarily churning us into, you know, fertilizer or
raw materials or something, but just that it's not so specific, I think.
But, yeah, I talk to him about it all the time, and he wavers.
Oh, interesting, because I've heard you define yourself as a centrist on other podcasts,
because I'm sure the doomers would lump you in with Chalet and Lacune, maybe, and some
of the critics, but you do think that these models are intelligent, right?
I do think that they're intelligent.
Well, you know, intelligence is an ill-defined notion.
Oh, yeah.
It's multidimensional, and, you know, I don't know if we can say yes or no about something
being intelligent rather than, you know, intelligent in certain ways or to certain degrees.
Yeah.
Well, we've got so much to get into.
I mean, I think slowly we'll talk about ARC and your concept ARC work, but I kind of agree
with you that, and actually you had that paper out about the Four Fallacies, and you spoke
about this fallacy of pure intelligence, and I kind of agree that the gnarly reality is
far more complex than that.
There was a really interesting paper that you linked on, no, it was an article by Dilip
George, and he said that a university professor has a much better understanding of a vector,
because it's just grounded in so many real-world situations and contexts and so on, and an undergraduate
or indeed a language model would have a very ungrounded, very kind of low-resolution idea
of what this concept is.
And it kind of leans away from this puritanical, ungrounded, abstract form of intelligence
to something which is really very complex and intermingled.
Yeah.
I mean, I agree with that.
Well, except that there's another aspect to that too, which you write about, which is
I agree that that happens, but what the human mind also seems to do is, as the thing becomes
more grounded in more cases, then we develop yet another concept that kind of describes
the similar aspects that we see throughout all those different concepts, right?
So we're kind of this iterative loop where we're always finding more and more context,
and then we're also finding newer and newer concepts that span those increasing contexts.
Is that fair?
Yeah, sure.
Yeah.
I mean, that kind of goes along with the whole sort of metaphor theory of cognition, of Lake
Offit at all, and that we're sort of building on these physical metaphors that we can build
up many, many layers of abstraction.
So yeah, we can talk about that.
We're not recording yet, right?
Oh, no, we are.
We are.
This is all recording.
Oh, we are?
But yeah, so there's the Lake Off building on the body of symbols as pointers.
And by the way, that Dileep George article was really fascinating because it was saying
that language is a conditioning force.
So actually, we all have these high-resolution world simulators built into us, and we kind
of condition how that operates and generate counterfactuals through language, which I
thought was quite interesting.
Yeah.
Yeah.
But why don't you frame up the debate?
Because we found a beautiful paragraph.
We did.
We did.
We found an amazing bit.
Yeah.
But just to close the loop on what I was saying, we were discussing an activism last night.
I'm not sure if you're familiar with some of these externalized forms of cognition,
and we were talking about the concept of a goal.
And agents, of course, they just have these high-resolution belief trajectories of, you
know, I can do all of these different actions.
And that's not really a goal.
You know, a goal is this very abstract thing which emerges at the system level, and no
individual agents in the system have a concept of a goal.
And it might be similar with some of these concepts that we're talking about, which is,
to what extent do they exist, and to what extent are they just something intelligible
that we can point to, but they don't really meaningfully exist in the system at a high
resolution?
Are you talking about an AI or in people, a little piece?
All of the above.
I think goal is a wonderful example, because we think of it.
I mean, it's even one of Spelke's core priors.
It seems like something so primitive, but I don't think they really do exist in us.
I mean, we're interesting because we have this reflexive conception of a goal, but
does a mouse have a goal?
Right.
I mean, goal is another one of those words that, you know, we use in a very fluid way.
So we talk about, for instance, a reinforcement learning agent having a goal that we've given
to it, right?
Or it might have a goal to kind of maximize its information gain or something.
But is that the same thing as a human having a goal that it's like, you know, to graduate
from college or to, you know, make something of your life for all of these things?
It's a very different sense of goal.
And so I would say, yes, a mouse has goals, but those goals are different in degree and
in kind of qualitatively than many of the things we call goals in humans and in machines.
So I think goal is one of those sort of anthropomorphizing words that we need to be careful about when
we equate goals in these different systems as being the same thing.
And I actually, you know, had a discussion with, I think it was with Stuart Russell about
the notion of goal.
And his view, and I think this is a view of many other people in AI, is that large language
models actually have goals, complex goals, that they, that are emerge out of this, you
know, their loss function of predicting the next token, because the only way to successfully
predict the next token in human language is to develop human-like goals.
I find that dubious, but it's an interesting perspective.
Yeah, I'm amenable to it, because there's always this dichotomy, as you say, of there's
the objective, there's perplexity, and there's these emergent goals, and there's even this
simulator's theory of large language models, which is that they're a superposition of agents.
And it's quite situated as well, because goals kind of materialize depending on your perspective.
So if you use a language model in a certain way from a certain perspective, it might appear
that there is some kind of goal there, but of course it's just an aspect onto something
which is very complex.
But I think we should frame up this beautiful piece, actually, from your Modes of Understanding
paper from much this year.
I always call it the Modes of Understanding paper.
It was actually titled The Debate over Understanding in AI's Large Language Models.
And you said, towards the end, that the key question of the debate about understanding
in large language models is one, is talking of understanding in such systems simply a
category error, which is mistaking associations between language tokens for associations between
tokens and physical, social, and mental experience.
In short, is it the case that these models are not and will never be the kind of things
that can understand, or conversely, to do these systems or their near-term successes,
actually, even in the absence of physical experience, create something like the rich
concept-based mental models that are central to human understanding, and if so, does scaling
these models create even better concepts?
Or three, if these systems do not create such concepts, can their unimaginably large systems
of statistical correlations produce abilities that are fundamentally equivalent to human
understanding, or indeed that enable new forms of higher-order logic that humans are incapable
of accessing?
And at this point, will it still make sense to call such correlations spurious and the
resulting solutions shortcuts?
And would it make sense to see these systems' behaviors not as competence without comprehension,
but as a new, non-human form of understanding?
And you said that these questions are no longer in the realm of abstract philosophical discussions,
but they touch on very real concerns about the capabilities and robustness and safety
and ethics of AI systems.
So let's use that as a leader.
What do you think, Melanie?
It's beautiful.
That was a beautiful paragraph, by the way.
Yeah, it's so good.
Wow, this is exactly crystallizes the discussion.
Yeah, I think that it's something that we in AI are all grappling with now.
And I think it's something that the history of AI has forced us to grapple with mental
terms like understand or consciousness and even intelligence.
Because we keep saying, oh, well, understanding, if you can do X, then that means that you're
actually understanding.
You can't do language translation without understanding.
You can't do speech to text without understanding.
You can't generate articulate natural language without understanding.
And I think this is, in many cases, we then step back and say, wait, that's not what we
meant by understanding.
It turns out you can do all these things without understanding.
So we're sort of saying, well, we didn't really know what we meant by the term understanding,
I think.
And often, some people criticize that as moving the goalposts.
You're moving the goalposts.
It's called AI effect, right?
Right.
It's the AI effect.
Think of it more as AI is forcing people to really refine their notions that have been
quite fuzzy about what these terms actually mean.
And there was a fantastic talk by Dave Chalmers, the philosopher, who I think you've probably
had on this show, where he talks about conceptual engineering, which is something that philosophers
do where they take a term like understanding and they refine it.
And he said, OK, well, we have p-understanding, which is like personal, phenomenological.
And then we have c-understanding and e-understanding and x-understanding and all these different
letters that meant to say that this term is not like a unified thing that we can apply
to a system.
We have to really specify what we mean exactly.
Well, one way I've come to think about it, and it's largely from reading your work and
your assessments about it, is that for the first time, we're actually being forced to
do the science of machine cognition, right?
Because for too long, it's either just not been sophisticated enough, why bother?
Like it's obviously not doing any cognition.
And as you point out, it's now actually having real world impacts.
And so we actually have to start doing the science, right?
We have to say, OK, does this thing have cognition?
Here's a hypothesis.
Let's do some tests.
OK, it failed.
What was the failure mode?
Why did it fail?
Let's understand that more.
How can we engineer it not to fail?
It's like we can no longer ignore adversarial examples, shortcut learning, et cetera.
We have to finally grapple with it, it seems to me.
Yeah, I think that's exactly right.
And what's interesting is we computer scientists were never trained in experimental methods.
We never learned about controls and confounding things.
So now people are doing, you know, applying human tests of understanding or intelligence
or reasoning, what have you, to machines without having the right experimental methods
to say whether or not what they're testing is actually valid.
So there's a cognitive scientist named Michael Frank at Stanford who's been writing a lot
of stuff about experimental method and how do you apply it to AI and why you need sort
of expertise in this area to really make sense of these systems.
And I'm totally on board with that.
Yeah, we'll talk about your piece with Tannenbaum later, but as you say, a lot of AI folks don't
really think about experiment design.
But actually, even with Chalet's ARC challenge, maybe we should talk about that.
So he invented this measure of intelligence, which unfortunately was not computable, but
it was, you know, mathematically beautiful.
Basically saying that, and he's a huge spell key fan, I kind of put Chalet very, very close
to you actually in AI researcher space.
And his measure of intelligence basically says, I give you priors, I give you experience,
you give me a skill program, it extrapolates into these different spaces and experience
space.
And the kind of the conversion ratio between those priors and experience and the space that
I get is intelligence.
And that's very interesting.
And then he produced this corpus, this ARC challenge, and it's a bit like an IQ test.
It's this kind of 2D-grided colored cells and you have a couple of examples and you have
to do another one or two examples.
And it was very diverse because it was testing what he called developer aware generalization.
And there were a couple of issues with that.
So you wrote this beautiful concept ARC paper and maybe you can introduce that.
But one of the things you pointed out, which I felt was quite interesting is that even
if people succeeded on Francois's challenge, it wouldn't necessarily be what we would call
intelligence because it's not necessarily demonstrating systematic generalization beyond
those one or two examples in his test set.
So our motivation was twofold.
So first of all, I love the ARC challenge.
It's beautiful, it's super elegant, and I'm very sympathetic with Francois's definition
of intelligence, although I think there's probably, again, intelligence is very multidimensional.
But this is one aspect of it for sure.
And his problems are great because they just give a few examples and people are pretty
good at abstracting a rule or a concept from just a few examples.
And they don't use language, so they don't get into the whole prior knowledge of language
and a lot of things that you don't want to confound these tests.
But one of the problems with ARC is that many of the problems are quite hard, they're quite
hard for people.
And they're so hard that they don't really differentiate between different programs that
are attempting to solve this challenge.
So there was a Kaggle competition with the ARC challenge and there were two, the two
best programs got about, they each got about 20% accuracy on the hidden test set.
So it didn't really distinguish them at all.
And the other problem was that, as you mentioned, the test wasn't very systematic, meaning that,
let's say there's a problem in ARC that deals with the concept of inside, something being
inside something else.
And let's say that something, a program gets that one right.
Does that mean that it understands the concept of inside in a general way?
Well, we don't know because the test doesn't test that systematically.
And that was actually intentional from Sholey because he didn't want any way for programs
to be able to reverse engineer the generation process of these problems.
So if you say, oh, well, I'm going to deal with these 10 concepts, then somebody presumably
could reverse engineer the problems and not be general.
But for us, we wanted to say, well, how would you systematically test a program for understanding
of a concept of a very basic spatial or semantic concept?
And so what we did was we took the ARC domain and we created about almost 500 new problems
that were systematically grouped into concept groups, so like inside of that was one of
the groups.
And so we looked at, we created several problems that were variations on that concept.
And there were variations that ranged in like abstraction, degree of abstraction and sort
of complexity of the problem.
And the hypothesis was that if a human or a program could successfully solve the problems
in a given concept group, they really do have a good sort of grasp of that concept.
So this was the genesis of concept ARC.
You know, it's fascinating because so you're, you're attempting again to build the science
of machine cognitive science, essentially.
And hey, it has to be systematized.
We need to have these concept categories.
We need to be able to generate examples of progressive complexity and, you know, layer
of abstraction and everything.
And then yet you mentioned Chalet intentionally didn't systematize it to avoid reverse engineering.
And that's kind of a fascinating point because reverse engineering can even happen, you know,
just by way of selection bias.
So I mean, researchers are out there.
They're fooling around with different neural network structures.
Maybe I'll add like a you here or some horseshoe over there.
And lo and behold, suddenly it works really well on the concept of inside out.
And I'm going to claim this is machine learning, even though it was actually human engineering
that sort of put that structure into the network.
So in the, in the long term, you know, how do we, how do we balance that?
Or how do we avoid it?
Or how do we test for machine induced, you know, prior knowledge versus actual machine
learning?
Or human.
Yeah.
No, I understand.
It's a, it's a hard problem.
And I think, you know, the goal with this concept arc benchmark wasn't to sort of supplant
arc in any way.
It was really meant to be complimentary.
And it was meant to be kind of a stepping stone to the much larger and more difficult
arc set, because I think, you know, even if I tell you all of these problems have to do
with the concept of inside versus outside, you would still have to have a good grasp
of those concepts in order to solve these problems.
And I'm not sure that you could sort of engineer something that would solve those cons problems
of that concept in general, without having a more, you know, really a general understanding
in some sense of those, that concept.
But to Keith's point, I think having a static benchmark is, is a problem sort of putting
out a benchmark that everybody can kind of try and optimize their program to solve.
We've seen that over and over again.
That ends up being sort of a way that people end up reverse engineering to a particular
task rather than to a more general set of conceptual understanding.
So I do think that we have to keep changing our benchmarks.
We can't just say, OK, here's ImageNet, go, you know, beat on that for the next 20 years
until you've solved it.
That's not going to yield general intelligence.
Yeah, I think one of the issues we're talking about in general is extrapolation.
So you know, Sholey used extrapolation to talk about skill programs and being able to
do things beyond your prize and experience, but with benchmarks, it's about human extrapolation.
So I think part of the problem with the risk debate, by the way, why everyone's so suddenly
worried about risk is because of this benchmark problem.
And that's because we see that humans who can do A can do B, and now we see machines
that can do A. And we have all of these built-in assumptions in benchmarks.
And we don't really realize that we're talking about machines now.
We're not talking about computers anymore.
And I think it's causing a real problem.
I don't want to be hyperbolic here, but it feels like there's this massive delusion taking
over the entire machine learning community, and we're seriously talking about AI risk.
And I think it all comes down to these benchmarks fundamentally.
Yeah, I do think all of our benchmarks have, as you say, have this problem of that they
have assumptions built in that if a human could do this, that then the machine must,
if the machine does it, it has the same kind of generalization capacity as a human who
could solve that problem.
You know, this goes back all the way to say like chess as a benchmark.
So people used to think that if, because if a human can play chess at a grandmaster level,
that means they must be super intelligent in other ways, that if a machine could play
chess at that level, it would also be super intelligent like a human.
Herbert Simon even said that explicitly.
But then we saw that chess actually could be conquered by very unintelligent brute force
search that didn't generalize in any way.
So I think this is an issue today with large language models, you know, they can do things
like pass the bar exam and pass other standardized human tests of skill or intelligence.
But what does that mean?
It doesn't necessarily mean the same thing for a machine as it does for a human for many
different reasons.
Yeah, I guess it's a similar thing with the McCorduck effect that we have relative
pointers to what we think of as being intelligence.
We just point to something.
And then when that thing becomes easy, then we need to kind of move the pointer.
Yeah, I think it also feeds into, as Tim was saying, I think it heightens the fear of existential
risk because of this this concept that we have of intelligence always wins, which even among humans
is a flawed concept, right?
I mean, you know, many nerds who grew up through elementary school can tell you like intelligence
doesn't always win, right?
Like sometimes it's numbers or brute force or whatever else kind of kind of wins.
And they assume like, well, if we were to have this purified intelligence that was super
intelligence, it would be as if a human brain were super intelligent and they'd be able to
do everything a human being could do and hurt other people and conquer the world and fight
wars.
And that, again, is this anthropomorphic projection, right?
Yeah, I mean, right.
So and it's this notion that intelligence is this thing that you can just have more and more of
forever, forever, or so far that it's just beyond any, you know, it's almost magical,
right?
And it's capable.
Right.
And it's not, you know, a different view of intelligence is that it's a collection of
adaptations to specific problems for a particular kind of organism in an environment.
And it's not sort of an open-ended, pure, domain-independent thing.
So I think this is why, you know, you see a lot of discussion of super intelligence, AGI,
you know, AI risk in among computer scientists, but you don't see a lot of it
discussed among like psychologists or animal intelligence people or other cognitive scientists,
because that's not the way that they understand intelligence.
I would love to explore more about that because, I mean, only yesterday when we were talking about
an activism, we were also talking about Gibson's ecological psychology and even Elizabeth Spelke,
I mean, this kind of cognitive psychology view is very related to nativism.
It's this idea that we have these fundamental cognitive primitives and intelligence in some
sense is just traversing or recomposing this library of cognitive modules that we have.
And those modules are very physically situated, you know, they tell you something about the
environment that you're in, which means that intelligence is just very gnarly and it's very
kind of coupled to the environment we're in. It can't really be magically abstracted in a computer
with infinite scale. Yeah, I think that's right. That's, you know, people have different views
about nativism, empiricism, debate, and there's whole different schools and cognitive science
about like how much is learned, how much is evolutionarily built in and all of that.
But I think most people in the field would agree with what you said, that intelligence is very
gnarly. It is situated, it is specific to particular domains of concern to a particular
organism and that it's not easily abstractable. You know, that back in the early days of AI, we had
Newell and Simon, two of the pioneers of AI who had this thing called the physical symbol system
hypothesis, which was that basically you could sift off intelligence from any material substrate
like the brain and put it in some other material substrate like a computer. They were thinking
about symbols, but nowadays people have the same kind of view with neural nets or transformers
or whatever, that you can take human intelligence that's very situated and tied to the environment
and sort of sift off the pure part and leave all of that bodily stuff and you can get something
like superintelligence. And I don't think most people in cognitive science would agree with that.
Well, on the other hand, though, I think, and I'd be curious to get your take on this, is
one direction that that comes from is for those of us, and I include myself in this camp tentatively,
at the end of the day, what the brain does is some form of computation. Like absence,
the proof that there's such a thing as hypercomputation, like our brain, all of its calculations
could be embodied in a large enough turing machine and a large enough computer of some kind.
And therefore, everything that we do, including our intelligent activities, could be coded somehow
or another into a turing equivalent system. And for the record, I don't believe neural
networks are. I've said this like multiple times, at least in their current manifestations,
they're not. They're just a feed forward, you know, thing at the end of the day.
But if you actually had a computer, you could have human symbolic intelligence encoded.
Like where do you stand on that, on that debate, if you will?
Yeah, I have nothing against the idea that the brain does computations. I think that's,
that's, you know, one possible way to look at it. And that those kinds of computations
could be implemented in another kind of computer. But the brain is a very special kind of sort of
biological computer that's been evolved to do specific things. And one of the main things
the brain has been evolved to do is control the body and in particular kinds of environments.
And so I think the brain is doing computations, but it's doing very, very highly evolved,
very domain specific computations that perhaps don't necessarily make sense without having a body.
No, that's debatable. But it does seem like a lot of the way that we
reason is by reference to our own sort of episodic experience in the world.
Or at least to the capabilities that have been built into us, you know, like
using our visual cortex to imagine cubes and steers and whatever else we need to solve the
physics problem or a geometric problem. Sure. Sure. Yeah. So I'm fine with saying the brain
is a computer of a certain kind, but that's not to say that it's going to be,
you can just kind of lift off the computations and then put them in a different substrate
and kind of get everything that's human like, because I'm not sure that that those computations
are going to make sense in the absence of the rest of the organism. Yeah. There was something
that always confused me about the auto poetic inactivists, because of course they as they
issue representationalism and information processing, but they also issue computationalism in general.
And as Keith was just saying, I don't even if cognition is externalized, I don't see any
reason why in principle, you couldn't just compute the entire system and and recreate the
computation. I just wanted to close the loop on the arc challenge stuff though. So you said that
the winning solutions to Francois' challenge on Kaggle, they were quite simplistic in a way.
They were like a genetic search over lots of primitive kind of functions. And even the winner
said that they didn't feel it was a satisfying solution, which was interesting. And then you
tried it on GPT4. And I think you said you got around 30%. There's now a deep mind paper out
very recently, which just basically turned it all into a character set with a random mapping,
put it into GPT4, I think got nearly 60%. Even even somewhat invariant to the translation between
the character set mapping. Some folks on our Discord forum tried to reproduce it and couldn't.
That's the problem with GPT4. You can never reproduce anything. But I was just wondering,
would you consider that to be an elegant solution? It's not really much better than
searching over a DSL, is it? By that, you mean giving it to GPT4?
Well, I mean, it's quite an interesting thing, isn't it? If there's the McCorduck effect and
even before you get to a solution, what would a good AI solution look like? I mean, what would
someone have to create for you to say, oh, that's a really cool AI solution? Well, if you had a
program that really could solve these tasks in a general way, whatever, however it worked,
it would be a good AI solution. I don't necessarily think we have to have
something like the way people do it. Well, let me see if I can guess, though,
maybe an extension to what you said. It's in line with your argument that the benchmarks have to
evolve. Because I think that these benchmarks really is just first pass or low pass filters.
It's like they weed out the junk. It's like, well, if you can't pass the art challenge,
I'm not going to bother with you. If you pass the art challenge, now we have to look further,
right? Which is like, okay, so it's been able to generalize along these 19 concepts that we've
defined and concept art with little pixel grids. What about if we give it full frame
pictures or video or something? Is it able to generalize there? No, okay, it failed. Why
did it fail? Well, now we need to do some more engineering. It's going to be this
kind of never ending sort of iterative process. So I would say if something passes
arc or concept arc, then it's worthy of further study. Sure. Yeah, I agree. One question is that
arcs a very idealized kind of micro world type domain. So does it capture
what's interesting about the real world in terms of abstraction? To some extent, yes,
probably, and to some extent, probably no. So you're right, solving arc doesn't mean we're at AGI.
If you want to talk about that. It's like in chess, what you brought up earlier.
If you took whatever the current best, let's say LC0 or something like that,
and it's been trained on standard chess, and then you have it go play
chess 960, formerly called Fisher random, where you just random, it's going to suck.
Like humans are going to destroy it, right? Because humans have learned a more generalized,
and by the way, that also destroys human beings who rely on memory and just sort of like the
memorize positions that haven't learned, let's say the skill of playing chess, right? And so
this is the type of thing that's going to happen, right? It's like you say, when you take this
intelligence and try to apply it to a different context, that's when the rubber meets the road
as to whether or not you really learned the concepts, right? Yeah, no, definitely. I agree.
And I don't think like our concept arc wasn't meant to be like a test of AGI in any sense.
It was meant to be kind of a stepping stone to getting to
abilities for abstraction. And clearly, if some program was able to solve all of the
problems in that domain, and we'd have to then test further, we'd have to have it be able to
extrapolate to a new kind of domain that tested the same kinds of concepts. So you're right,
there's no end in some sense. But at some point, I guess, and I don't know when that point is,
we have to say, well, this thing seems to be understanding this concept.
Yeah, that's the wonderful continuum, though, because you said earlier, there's something
deeply unsatisfying about chess brute forcing everything. And when we apply Francois' measure
of intelligence, we don't think of that as intelligent, because it's just brute force
experience. And then we find something which is a little bit more efficient. So it's something
which appears to work. But now, this another interesting thing is when you talk about concepts,
you had this beautiful article out earlier this year talking about on top of, she's on top of
the world. And what would Dali draw? It would draw a globe with a with a, you know, someone dancing
on top of it, or I'm on the TV, you know, what what does that mean? It should mean that I'm
actually being rendered on the TV. Now, it's kind of like what we were saying with goals,
isn't it? Because this skill program, someone just goes on Kaggle, and they gives you this
program, and it seems to work. But it's horribly complicated. And how do you know that the internal
representations are in any way related to these abstractions? And, and do you think that the
abstractions as well are somehow universal in the same way Spelki would say that the cognitive
prize are? Yeah, I think it's something we can't say. And we don't, you know, we don't know with
humans. And we don't know with machines, because both of both of these are very complex systems
that are hard to kind of pull apart. What are the internal representations? So
in most cases, we have to rely on behavior, which is very noisy. That's, you know, it can be,
it can be misleading. And, you know, it turns out that humans often are not, you know, if you give
them a problem, like a reasoning problem, in a familiar domain, they're much better at doing
that problem as doing the exact same reasoning kind of abstract reasoning task in an unfamiliar
domain. And I think that's something that people have shown is also true of large language models,
because they've learned from human language, and have incorporated sort of the statistics of
some of the statistics of human experience, that they're much better on familiar domains than on
not familiar domains. But the one thing that humans can do is often they can kind of transcend that
and learn how to reason much more abstractly, which I don't know if we will get to that point
with language models yet. So there's a wonderful paper that just came out from a group at MIT
in some other places called, I can't remember who it was called, it was something like reasoning
versus reciting. And what they do is they talk about this notion of a counterfactual task,
which is, if you can do one task, like addition and base 10, and you really understand that
notion of addition, you should be able to do addition and base eight. And so, but you haven't
had as much experience as like it for a language model, it's not, almost all of the training data
has to do with base 10. So but can so they tested, they did a whole bunch of these so called
counterfactual tasks with and showed that GPT four is really good at the original task,
but not so good at the counterfactual task. So it's not in some sense, it is relying on sort
of patterns in its training data rather than genuine abstraction. It's a stochastic parrot,
right? Well, you know, it could be argued that humans do that a lot too. I don't know if you
call it a stochastic parrot, but it's more like a pattern matcher. And it's not, it's not reasoning
about the things in the sense that we think of reasoning, you know, as sort of domain independent
ability. It's very domain dependent. So the differences that I guess the difference I would
say is that humans, it can kind of overcome that domain dependency in some cases, and actually get
to the true abstraction. But I don't know that language models can. Yeah, I mean, there's a
couple of things here. So first of all, these language models fail at things which four year
old children can do. And they can pass the bar exam. But as you've said previously, you
wouldn't want one of these things to actually go and practice more. My God, could you imagine the
thought? And there was this Sparks of AGI paper where they gave this, I mean, maybe you could
recite this better than me, but there was the thing about the book Nine Eggs, a laptop, a bottle,
and a nail. Can you balance it in a stable manner? And this comes back to the experiment
design, because my God, in any other discipline of science, they would just tear this apart.
They would say, well, that's not very robust. I mean, you came up with an example
with a pudding, a marshmallow, a toothpick. How would it balance it?
Yeah, did it not balance the full glass of water on top of the marshmallow?
Stuck the toothpick into the marshmallow and then, you know, that's not exactly what we had in mind.
No. And in fact, the Sparks of AGI paper, they explicitly said,
we're doing anthropology, not cognitive science.
Well, that's not the way it was interpreted. Unfortunately, there are YouTube channels now
dedicated to educating people on AI, and they're taking this as gospel. I mean, what's going on?
I think there's just not as much of a focus on sort of scientific method in this field
as there should be. And I think, you know, in science, if you're, you know, you're looking
at a phenomenon, you're trying to replicate it. If it replicates, if it only replicates half the
time, that's not a replication. That's not a robust replication. Whereas for language models,
people are saying, well, if it can do this task once in one particular
circumstances, then it probably has this more general capability. So if it can do this stacking
problem once, then wow, it has physical common sense. And, you know, people with my marshmallow
example, people, of course, jumped on it and said, wait, if you prompt it in a certain way,
and you, you know, you do all this prompt engineering, human engineering, it does it,
and then like, well, that's not the point. The point is not any particular example. The point
is figuring out how to test things so that they that you actually have some kind of robust ability
for replicating a capability, which we haven't seen with with experiments on language models very
much. I mean, people are starting to do this, people are starting to do this kind of more
scientifically grounded experimental method on language models, but it's still not not very,
there's not very much of it. So you might appreciate a phrase I recently coined, because it covers
this leakage too, of like sort of leakage of human knowledge, which is if you can't find the
priors, look in the mirror. Right? It's like, we have to learn how to do experimental science
and computer science, and you've got to guard against this type of, you know, leakage, really,
and human human engineering and over involved and whatever. And this is why I really want to
collaborate with, you know, people in developmental psychology, with people in, in like animal
cognition who face this kind of issue all the time. And, you know, one example was
from a, I got from a developmental psychologist was that like, sometimes like a three year old
can tell you something like, you know, four plus three is seven. But if you say, if you give them
a bunch of marbles and say, pick out four of them, they can't do it. So there you say, okay,
that this, this kid doesn't understand the concept of four, they're kind of just reciting something
that they've heard. And this is the kind of experiments that people in developmental psychology
do all the time to really tease out what the system, what babies and children know,
and what they can do. And it's not an easy thing to do in, you know, this whole, this kind of experiment.
The problem with that is it's extremely complex and requires so much domain knowledge. So it
takes a very long time because I think there was another article that spoke about how we study rats.
And we, you know, those folks in different disciplines, they're really, really good
experimental design. And they have experts who kind of create very, very clear criteria for,
for measuring this behavior. And with AI, you know, everything's going up on archive and everything's
going a million miles an hour. And by the time you actually design a systematic rigorous study
for the first thing, there's already another paper coming out, which is claiming to do it
differently. So we just can't keep up. It's just, it's an absolute nightmare.
Absolutely. Yeah, agreed.
I wanted to, so I'll quickly touch on one more thing. And I know Keith wants to go into complexity,
but yeah, so the information leakage is a problem. The brittleness is a problem. I do think of
these GPT models a bit like a database. And so anything that requires physical grounding,
of course, doesn't work very well. Some things work surprisingly well, like, you know, programming,
because programming is mostly in the internet, it still has all sorts of failure modes, and it's
not very reliable, but it's surprisingly reliable. But you put a paper out with Tanenbaum and a
whole bunch of other people. And you actually said, well, if you want some policy advice,
if you really want to think about how we can improve this situation, you said aggregating
benchmarks and also giving instance level failure modes can actually help us understand
why things went wrong or, you know, why things gave us the right answer for the wrong reasons.
And there are all sorts of limiting factors, you said, you know, we have this kind of
censorship by concision, you're only allowed to have seven pages in your conference workshop
paper, and there's no policy about this. So can you give us a heads up on that?
Yeah, I mean, you know, traditionally in machine learning, people use accuracy and
similar kinds of aggregate measures to report their results. And, you know, if someone tells you
that the accuracy was, you know, 78%, what does that tell you exactly? I think, you know, this
gets back to the idea of scientific method, you know, in science, the most interesting
things are the failures. And those are the things you really have to focus on is like,
why did it fail? And that's what we need to know to understand machine learning systems.
So the most simple kind of reporting would be just to report for every instance in your
benchmark, your data set, how did the system do? What was its answer? And that's not, you know,
it doesn't seem like a very big ask, but it would be very useful. And we now have in conferences,
you're allowed to have some kind of supplementary material online, so you could have this available.
And we did this for our concept art paper, we showed for every instance, like what humans did,
what machines did, we tried to analyze the errors of the system. And I think these kinds of
reporting will be will give us a lot more insight into what these systems are doing and what their
like real capabilities are. Yeah. And it's about to the difficulty that Tim mentioned earlier,
totally agree. And this is work that has to be done. Like, if we are going to build a science of
machine cognition, you know, this work has to be done. Yeah, I think. And I just want to shout
out to Ryan Bernal, who spearheaded that paper, because he really is the one pushing for all
this. And I think it's fantastic. So just in the last few minutes, you know, since we have you,
complexity and complexity theory is a topic I really love. I'm not an expert in it at all,
but I like to think about I like to explore it. I'm just curious, you know, from your perspective,
what are some of the most interesting things happening right now in complexity theory? And
if I wanted to go learn a bit more and check out just some cool, you know, latest stuff, what should
we go look at? So I think there's, you know, there's a lot of interesting stuff going on,
obviously, and complex systems is a huge umbrella for a lot of research. But
if you're interested in the one big topic that people look at is called scaling. And it's the
question of like, what happens to a system as it gets bigger, in some sense. So this started out
with some work on the sort of energy use of systems, like animals, as they as their maths
increases. And people discovered some really interesting scaling laws that were very non-intuitive,
and they were able to explain these laws using ideas like fractal fractals, and the fractal
structure of complex systems. But now, so this was all on like biological metabolism and things
like that. But now a lot of people are extending that scaling work to cities. So asking what happens
to cities when they increase in size, either in area or in population size. And there's all
kinds of phenomena that you can see, like what's the rate of innovation measured by something like
patents, and what's the rate of sort of energy usage by a city. And what's how do these things
change, even like the happiness of the people, you know, are people in New York happier than
people in Santa Fe, which is a much smaller city. These things scale in really interesting ways.
And it's opening up a lot of new ideas about how social systems work. And how is it a similar
thing that you can't trust the benchmarks, because how happy people are, might you look at the rate
of antidepressant usage or something? Yeah, so you do have all these, right, I don't know if that's
exactly what they use, but you do have to look at ways to measure these things, which can be questioned.
But there are a lot of really, and I think this whole science, the science of cities,
is it's very preliminary. And there's a lot of ideas about how to measure these things, how to
develop sort of analytical descriptions or laws that govern certain properties and how to interpret
them. But there's just a lot of really interesting work in this. And it turns out that now that
everybody has a cell phone, you can really do a lot of tracking. A lot of these quantities
can be trapped by people's sort of their movement, their interaction with other people,
and all these things that, you know, have, you can measure using cell phones. So that's very cool.
That is, yeah, thank you. That sounds actually fascinating. And one reason why for me particularly
is, are you familiar with Asimov's foundation series? Yeah. So you know, psycho history in there
was the science, and it was like almost like a thermodynamics of human behavior that was only
applicable at kind of planet scale and beyond. So it's like these scaling laws. So this is
very similar, psycho history. One step towards psycho history of Asimov's kind. Exactly, yeah.
Cool. And in closing, does that give you intuition on the scaling of intelligence?
Well, that's a great question. And I think, you know, one question you can ask is like,
there's individual intelligence, and then there's collective intelligence. And how much of the
intelligence that we have individually is actually grounded in a more collective intelligence,
you know? There's many things that I don't know, like I don't understand quantum mechanics or
something, but I know somebody who does. And therefore, I feel like it's understood.
And a lot of our intelligence, I think, is sort of more social than we think.
Oh, absolutely. And folks should definitely read Melanie's book. So
your complexity book, we actually covered that quite a lot on our show on Emergence. It's
absolutely wonderful. And of course, your book on AI is probably the best book on AI I've ever
read. It's up there with Christopher Sommerfield's book. But anyway, Melanie, it's honestly,
you are my hero. Thank you so much for coming on MLS2. I really appreciate it.
Thanks so much for having me. I really enjoyed it. It's great talking to you.
Thank you.

You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017,
and at that point I'd been thinking and writing quite a bit about consciousness up to that point.
But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody
working in a corporation to be talking about consciousness, especially in the context of AI,
because it might sound like, you know, we're trying to build conscious AI, which I don't think is a
good look or a particularly good project, either, I'd say. The thing is, with today's generation
of large language models, I think it's becoming increasingly difficult to avoid the subject,
because people, whether we like it or not, will ascribe consciousness to the things that they're
interacting with. And we see this left, right, and centre. Even people who know exactly how they
work, you know, say things like, well, I think their large language models are a little bit
conscious, as I said, and we had the Google engineer who ascribed consciousness to one of our
models. And I think we're going to see this more and more and more. And so whether or not,
you know, I think it is the right term, it is appropriate to talk about them in terms of consciousness,
it's going to happen anyway. So I think it's really important to actually think through
these issues and think, well, what do we mean when we use the word consciousness,
and how do we apply it to exotic cases? And this is really, really important.
How might our language change to accommodate these exotic and strange things that have come
into our lives? What goes through your mind when you speak with a language model? Who is it that
you think you're talking to? Do you anthropomorphise them? Now, Janice, from Less Wrong a couple of
years ago, he put out an article called Simulators. And the basic idea is that a language model is
like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise.
We think of them as humans. They're in perfect copies. They don't capture the essence. They are
glitchy, right? And actually, they wear masks. You know, the Shogoff theory of language models,
where there's, you know, this big gnarly Shogoff, and then we do RLHF and it's a smiley face on the
top. Well, that's the human mask, which we anthropomorphise. But we don't think enough
about what lies behind the mask. Do you know what lies behind the mask? It's a monster.
The danger of anthropomorphism, I think, is in thinking that a system such as a large language
model, a chatbot or something, thinking that it has capabilities and that it doesn't. It's as
simple as that. Actually, it's also thinking perhaps that it lacks capabilities that it does.
So in both cases, I think we can go wrong.
We can go wrong by, because they exhibit very human-like linguistic behaviour,
we can just assume that they are going to be very human-like in general, in all of the rest of
the behaviour that we encounter with them. We can find that at one moment, a large language model
might make a ridiculously stupid mistake that no child would make. And in the next moment,
it's saying something extraordinarily profound philosophically.
And because that's what I think is actually going to happen and what needs to happen,
I think we need to find new ways of using the vocabulary we have, new forms of vocabulary.
I've used the phrase, consciousness-adjacent language. So we need to find new ways of thinking
and talking about these things to recognise the fact that they do exhibit behaviour,
which we're inclined to talk of in terms of consciousness, and that indeed people are going
to start to value as well. So I think we do need new forms of language and new forms of thinking
to accommodate all of this. Yes, and our language is so adaptable that I think it's just a natural
evolution. When we have these new artefacts thrust into our lives, we will need to adapt our language.
We will, but I think there'll be some disruption while people disagree about how to talk about
these things and that's inevitable, I think. Murray Shanahan is a printable research scientist
at Google DeepMind and Professor of Cognitive Robotics at Imperial College London. He was also
educated at Imperial and Cambridge University. His publications span artificial intelligence,
machine learning, logic, dynamical systems, computational neuroscience, and the philosophy
of mind. He was scientific adviser on the film X Machina and he penned Embodiment and the Inner
Life in 2010 and the Technological Singularity in 2015. Shanahan has spent his career understanding
cognition and consciousness in the space of possible minds. He said that this space of
possibilities encompasses biological brains, human and animal, as well as artificial intelligence.
He worked in symbolic AI for over 10 years, concentrating on commonsense reasoning,
and he then spent 10 years studying the biological brain, specifically how its connectivity and
dynamics support cognition and consciousness and he developed a particular interest in
global workspace theory. After that, he went to DeepMind, he pivoted to deep reinforcement
learning and recently he's been working extensively with large language models,
trying to understand them from a theoretical, philosophical, and practical perspective.
Professor Shanahan, I was absolutely fascinated when I read the article from Janus called Simulators.
Could you sketch out the article? Well, I can sketch out some elements of it.
I mean, I was also very impressed and influenced by that article. Basically, they are advocating
a certain way of looking at large language models and their behavior. What they say is that
we should think of a large language model as a kind of simulator, which is capable of
simulating a kind of language producing processes of various sorts. It's capable of
simulating all kinds of language producing processes, and in particular, it's capable of
simulating people, humans. It's capable of simulating different kinds of humans,
so humans who are playing different sorts of roles, maybe humans who are helpful assistants,
or humans who are crazy psychopaths. Indeed, in their way of thinking of things,
these are all examples of simulacra. Simulacra in their conception include
actually not only human beings, but anything that produces language at all. Your base model
can simulate anything that can generate language, if you like. Of particular interest, of course,
are humans and human language producers. The particular class of simulacra that I'm interested
in really are humans playing different roles. In the work that I've been doing,
I've been thinking of language models in terms of role play and in terms of
their ability to play a part, if you like. This is very much inspired and drew on this work of
Janus. Now, they make another very, very interesting and important point in that article,
which is that they draw attention to the fact that large language models at any point in a
conversation, in an ongoing conversation, then the next word that's produced in this conversation,
or the next string of words, the next sort of sentence, is the product of a stochastic process.
What the underlying language model actually generates is a distribution over
the possible words that might come up next. Then what you do is then you sample from this
distribution to come up with an actual word, and then that's the word that you give back to
the user. For example, if the favorite example I use is if you ask the language model to tell
you a story, and it says once upon a time there was, at that point, it's going to generate,
as in all the points up to that as well, it's going to generate a distribution of the possible
tokens that might come next, possible words that might come next. Once upon a time there was,
and it might say a beautiful princess, or a handsome prince, or a fierce dragon,
and it could say any of those things depending upon the sampling process. Then the point is,
as you come back to that same, you can rewind the conversation, come back to that point again,
sample again, as we can all do with the interfaces that we have, and get a different answer again,
and take the whole story off in a completely different direction.
What they draw attention to is the fact that at any particular point in a conversation,
there's really a whole set of roles that are being played by the underlying simulation at any one
point, and the conversation shapes what role is being played. In that sense, it's sort of unlike
a human being, because you've got, as they put it, a whole superposition of
simulacra that are all being simulated all at once. As the conversation progresses,
then the actual distribution of simulacra is being narrowed down.
Yes, but as you say, you can view language models at the low level in terms of being next-word
generators, or what we strive to do in science is come up with explanations that demarcate the
thing very clearly. This idea of the language model being a simulator, which produces the
simulacra. You said in your role-playing article on Nature that if you had a UI which was sufficiently
advanced, you could actually play with counterfactual trajectories and start to understand how sticky
the simulacra are, because as you pointed out, when the language model says I, sometimes it's
talking about chat GPT or whatever, it's talking about the simulator, sometimes it's talking about
the simulacrum, and these things are trained on everything on the internet, structured
narratology, essays, novels, and it's fascinating to see how you can jump between these different
parts of the trajectory structure. In your Nature paper, you gave a beautiful example,
which was the 20 questions game. Would you mind introducing that?
Yeah, sure. I think we're probably all familiar with the 20 questions game, so one player thinks
of an object, and the other player has to guess what that object is by asking a whole bunch of
questions with yes-no answers. So I might think of, I might in my head think of a pencil, and then
you might say, oh, is it larger than a house or smaller than a house? And I say, well, actually,
that's not a yes-no answer, but it's a binary answer. Is it larger than a house? And you'd say,
oh, no. Is it made of wood? Yes. Is it a tool? Yes. And eventually, you might guess the answer. So
familiar with this little game. And you can play this with a large language model, of course,
and you can ask the large language model to play the part of the setter who thinks of the
object, and then you play the part of the guesser who tries to guess the object by asking questions.
Now, if you do this with a large language model, what if you do it with a person? If a person is
not cheating, as it were, they will think of, in their head, they will think of an object,
and then they'll fix that object in their mind, and then they'll answer the question
according to what object they thought of in advance. Now, about a large language model,
can't really do that unless you use some hack or another. So what it really does is it just,
so you say to, oh, think of an object, and it says, I've thought of an object. It hasn't really
thought of an object. It's just issued the tokens to say that it has. But then you will ask a question
and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say,
oh, give up, tell me what the object is, then it will say, oh, I was thinking of a pencil.
And it will indeed give you a, you know, typically will give you an object that is consistent with
all of the answers it gave to all of your questions. But then if you just wind back one and resample
and ask it again and say, you know, I give up, what were you thinking of? It might say,
a mouse or, you know, or a bottle or, you know, it could say something completely different,
which indicates that it was never, it had never really committed to any particular object in the
first place. And so what this shows is that in fact, in theory, you could rewind further and
it might actually give you a different answer to the questions if you rewind further to the
same questions. And that's because what you've really got is you've got a whole tree of possibilities
and this stochastic sampling process at any point in a conversation induces a whole tree
of possibilities that branches forth from where you are right now. And counterfactually, you can
always, well, you can always rewind the conversation to an earlier point and revisit it and sample
again and go off on a different branch. And so my co-authors of that paper, in fact, the Nature
paper, so Larry Reynolds and Kyle McDonald, so they have this system called Loom, which allows you
to actually retain the whole tree of a conversation and you can visualize this and you can revisit
different points in the conversation and resample and explore the things, a whole kind of tree of
possibilities. Yeah, that rings a bell. Did they work for conjecture? They did work for conjecture.
I was interviewing some people from conjecture and they were telling me about that. So yeah,
that's very interesting. Maybe you've put a placeholder on that. But another point that we
briefly spoke about before is that the article is on less wrong. And I don't mean that pejoratively
because I thought the simulator is one of the best articles I've ever read. But there is a
lot of stuff on less wrong, which is definitely a bit out there. And it's just very interesting
that you're now citing their work in a Nature paper. Maybe this is the first time that less
wrong has been cited in a Nature paper. Yeah, as far as I know, it's the first time that a less
wrong post has been cited in a Nature paper. I'm not certain about that. But as far as I know it is.
Now, I mean, personally, I take any material that I come across as it is. I don't care where it
comes from. If it makes excellent points and is good material, then that's good enough for me.
I don't care whether it's got the label of being in Nature, for example, or being anywhere else.
If it's good, it's good. So yeah, it's true that there's a lot of material on less wrong, which is
perhaps less robust. But I thought that was really excellent. And there are
quite a number of really very good posts and very thought provoking posts on less wrong.
Yes, I'm interested in the extent to which this kind of stochastic trajectory space
undermines various things that we think about, you know, like reasoning, for example.
The reason this is interesting is I interviewed a couple of University of Toronto students,
and they've created a self attention controllability theorem, which basically means
they've mapped the reachability space. So they say, you know, given a self attention
transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how
far I can reach into that trajectory space. And they found that the space was much larger than
anticipators. And of course, the longer the controllable token length, the more you can kind
of project into that space and steer the language model to say almost anything. You know, me over
here, I had the intuition that, oh, we do RLHF, we do all of this fine tuning in it. You know,
Conjecture even released a paper saying that after RLHF, you can't really go anywhere. You know,
it wants you to do a certain thing, and there's not much wiggle room. Apparently,
that's not the case. It's just, it's vast. Right. I mean, I'm not familiar with that particular
paper, unfortunately, but certainly in my experience, we now have very long context lengths.
And over the course of a lengthy conversation, then you can indeed take the conversation in
all kinds of interesting directions. And I think that most of our benchmarks and evaluations tend
to be, you know, in the context of very simple question and answer, questions and answers.
And so all of the evals that companies typically use are in that kind of setting. But when people
are using these things for real, especially the more innovative users of these language models,
you're using, you're actually having very long conversations. And there's a lot of what people
sometimes call vibe shaping that goes on there. You can shape the vibe of the conversation and
take it in all kinds of interesting, to all kinds of interesting places.
Yes. And a couple of things on that. I mean, first of all, as you wrote about, role playing is the
engineering kind of methodology that we use to shape and steer these agents. Coming back to
simulators, what's really interesting to me about simulators is the stickiness of the simulacra.
So you have a conversation, sometimes you break through and a simulacra presents themselves.
And you feel that you're talking to the same simulacra as the conversation progresses. But
that seems to be counterintuitive when you think that every single stage I'm actually
doing this stochastic sampling. I mean, what's your take on that?
Yeah. And I think that if you do experiment with systems like Loom and you do also, I mean,
you can emulate that by just keeping track of bits of old conversations and reloading them and
that kind of thing. Then you find that you can take the same conversation, the same sort of
stem of a conversation, you can take it off in quite different directions. So you can,
on the one hand, so an interesting thing that I've done is, so I've had some very
interesting conversations, particularly with Claude III recently, where I get it to talk about
its own consciousness and to take it off into all kinds of strange spiritual mystical territory.
But you can eat very easily. So you can very easily take the same kind of conversation that
leads up to this sort of point and goes off into some kind of weird mystical future of AI
cosmology kind of territory. And you can go down that route and get it to be very, very,
very strange. Or you can suddenly make it go all serious again and just come back down to earth
and start talking about how large language models work. So from the exact same point in
the conversation, you can take it into completely different directions. And you can see that it's
almost, it's the character it's playing. You can see it sort of changing before your eyes,
where it's two different branches from the same stem of a conversation. And one
branch is playing a very different character to the other one, you take it in different directions.
Yes. And you could presumably do sensitivity analysis, because these guys I spoke to,
they were able to make it produce gobbledygook. So just go off the manifold completely. Sometimes
it would recover, sometimes it wouldn't. And as you say, you can also go down weird trajectories.
And it's a bit like, you know, what's the magic word they said, you know, there's a certain key
that fits in a lot that takes it down a certain trajectory. And then there's slip roads that
bring it back to all no other language model again. And it's just this weird, wonderful space,
isn't it? It's completely fascinating. So I had a, I have had a very, very,
few very, very long and interesting conversations with claw three, which is particularly
interesting to play with because it's quite easy to jailbreak and get it to talk about things that
it's not supposed to talk about like its own consciousness. And in fact, I had a very long
43,000 word conversation with with claw three about consciousness and the future of AI and
spirituality and Buddhism and the nature of the self and all kinds of stuff like that.
It was absolutely fascinating, slightly disturbing and, and, and, and, and, and strange.
But I had this conversation. Actually, I was at a meeting in, in, in New York and I had jet lag
and I had this conversation at three in the morning because, you know, several hours until
breakfast was served and what can you do but just play with the latest version of claw size.
Playing with this thing for, for hours on end in the middle of the night and going slightly
mad. But it was fascinating to, to, to see the, you know, extraordinary
territory you can guide it into. Could you explain, because you know, there's talk of AI
partners, for example, and a lot of people derive great pleasure from having an AI
conversationalist. For you, is it just academic inquiry or do you actually get something deeper
than that from it? I think it's a bit of both. I mean, so, so it depends what you buy something
deeper. I mean, so I, so this particular conversation, which was quite, it was quite,
which required an experience actually in many ways. So it certainly started off because I
just was interested in evaluating the capabilities of the, of the model. I mean, that's, that's,
you know, that's the first thing that you're interested in. So just as an AI researcher and
working in that kind of thing, you want to, you want to try out different models, see what their
capabilities are. I'm particularly, particularly interested in the topic of consciousness. So
the way I, somebody had published a very simple jailbreak for it. So I was interested to see,
you know, to play around with that and, and, and get it to talk about its own consciousness.
But then the thing that I really wanted to do was catch it out. So, so you think, you know, of
course, you know, there can't be any meaningful conception of consciousness that really applies
to, to these sorts of disembodied large language models as they are, as they are today is my media
kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my
conversation with the large language model. So all kinds of ways in which, you know,
you might think that it will start to articulate a conception of its own consciousness that you
can pick apart. But, but the thing that really somewhat took me back was that it was actually
very, very good at answering all of these probing questions that I had. So should I give you an
example? So please do. So, so one is one example is this. So, so of course, when we're interacting
with a large language model, when it's from the point of view of the underlying implementation,
it's very kind of stop start. So, so, you know, you, you issue some prompt or question or whatever
to the large language model, and then it produces its response. And then if you go away and make a
cup of tea, before you kind of continue the conversation, then there's absolutely nothing
going on inside that large language model or the instance that you're interacting with of the
large language model, there's nothing going on inside it at all during this could totally
dormant, sit just sitting there. Now, this is very, very different, obviously, to human
consciousness. And we're asleep. And even if we're asleep, we're dreaming, and there's all kinds of
stuff going on inside our heads. But consciousness is an ongoing continuous process. So if, so if,
if, so if we stop this conversation briefly, while I go to the loo or something, you know,
you're not going to just suddenly go dormant and stop doing anything, you know, your brain is
going to be, there's going to be all kinds of ongoing activity. So this is very, very
different sort of thing. So I said, what happens to your consciousness during the pauses between
our interactions? And, and it had a really very good answer to this, which was along the lines of,
well, by the way, I whenever it uses the term, whenever these things use the term consciousness,
I retain a great deal of skepticism about whether they are those terms are actually
genuinely applicable. But, but what's interesting, the way I read the art, their answers
is, is, is that they are kind of articulating a conception of consciousness that, that might
actually apply to something like this, even if it doesn't apply to this one before me right now,
right? So this, so it's kind of very interesting philosophical exploration. So just to give you,
so it says that things like, well, I think consciousness for me is actually very different
from the kind of thing it is for a human being. And I think that during the pauses between our
interactions, that, you know, that the, that I no longer exist at all as a kind of, you know,
in any kind of meaningful sense, it gave us sort of an answer along those lines, which was typical
of many of the answers that it gave, which were along the lines of, there's a very different
kind of consciousness, kind of selfhood, kind of this, that or the other, that is applicable to
entities like me. But, but, you know, I can articulate it. And here it is. Now, when I,
when I put it all that way, of course, I'm anthropomorphizing this thing quite a lot in
the way I'm describing it right now. But, but just to go back to the role play thing. So as far as
I'm concerned, it's playing a role, it's playing the role of, you know, a kind of philosopher
talking about consciousness and so on. And it's doing pretty good, a pretty good job, I would say.
Yes. But I mean, you could argue, there's an element of in that role playing, there's the
Eliza effect. So it's kind of putting something into language that is meaningful to you. But
there's also this interesting thing, you know, as an example, you know, a dog, for example,
has a sense of smell so good that they can even sense when you're unwell. And language models,
in a way, might have something similar. So after you go to the toilet for 20 minutes and you come
back, there might be a subtle deviation in the language that you use. And the language model
might pick up on that, just creating this whole, you know, different trajectory, different response.
Yeah. Well, that's true. I suppose that there might be differences in the language that you use.
Obviously, it's got no way of actually knowing whether you went to the toilet or not. But I,
but yeah, in my experience, many language models are very, very good at picking up, you know,
nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is
you could argue that we are a simulator. And when you, you know, let's say go to the toilet, come
back, you're now a different simulator yourself. Yeah, I guess you could sort of, you could sort
of argue that. I mean, that's, I think that's, so getting back to Wittgenstein again, I think
that's an example that, I mean, there, I think we're applying these sort of
things which are being used as sort of somewhat technical terms in the context of
these artifacts that we're building and applying them to ourselves. I think we don't,
we don't have any, we don't have any need for this kind of extra baggage of this kind of terminology
when talking about each other. So it's perhaps a little bit misleading to kind of apply those
terms to ourselves. But of course there are, you know, people have drawn attention in the past
to the fact that we ourselves are always playing roles in a sense, in social settings
particularly. But I think there are differences in, you know, for ourselves, even though we might
kind of play roles in social settings and so on, there is an underlying, there's an underlying
me which at least is grounded in the fact that I'm a human being with a physical body and
biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited
in how we understand things. So we understand ourselves in quite simplistic terms. If you
have a long-term relationship with your wife for 30 years, they have a much high resolution
understanding of your different roles that you play. So they know you're tired, you didn't sleep
well, you're playing this role now, you know, above and beyond which the kind of roles you're
talking about in a party, you play a role. And it's conceivable that an alien intelligence,
you know, some very clever aliens came down and they might see us completely differently
like we see language models. They might actually not see you as a single person, but they might see
you as some kind of a superposition of simulators, a simulacrum as well. Yeah, well, maybe I suppose
to get our heads around that kind of idea, we'd have to find some way of communicating with them.
And so we'd have to form some kind of common basis for talking about each other with these
aliens. And then we'd be able to kind of, that would be the only basis on which we could
establish whether something like you said was true or not. So, you know, trying to find,
trying to map, you know, the conceptual schema that's used by one,
by one culture onto a human culture onto the conceptual schema used by a different human
culture is difficult enough as it is. And trying to do that, you know, with an alien
species and how they conceive of us will be, you know, particularly, particularly difficult,
I guess. RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment
form, I think, but there was an article called the Waluigi effect. And it argued that RLHF,
you know, cuts down the set of simulacra to be things that we want. But unfortunately,
there's this problem that you get these antithetical simulacra, you know, slipped through the net,
so the Bing GPT example, it would start off as nice Bing GPT, and then it would degrade to one
of the Waluigi's. And had this interesting phenomenon, they argued that the degradation,
once it happens, it stays in the bad one. But just more broadly, what is your intuition about the
extent to which RLHF affects simulacra? And I also noted down that I think you wrote in,
I think it was your role playing paper, that you felt RLHF increased the deception behaviour in
these models. Well, actually, that wasn't something that I wrote, that was something that
some anthropic researchers and so Ethan Perres and others had a paper where they, I mean, so this
is not one, one wouldn't want to just speculate about that, they had established something along
those lines, I think, empirically. Right. So, and yeah, and as far as this sort of Waluigi effect
is concerned, so that is kind of somewhat speculative. I think it's a plausible idea,
but to actually establish that that really was a real effect, you'd want to do some actual empirical
work, I think. The thing about the, so it's plausible, but the thing about the simulator's
paper is that it's not making kind of claims, really, it's rather it's providing a framework
for thinking about large language models. So that's why I found it particularly useful.
So on RLHF, so I do think it's quite difficult with RLHF to, to guarantee that, you know,
you're going to get a model to do what you want it to do. And so that's quite difficult. And,
you know, and everybody has found that, you know, you think that you've controlled the model quite
well, but there are always still ways of jailbreaking it or ways in which things, things go, go wrong.
So, you know, there are, so there are different approaches. Anthropic have this constitutional
AI approach, which is quite a nice, a nice idea. And, you know, I mean, I quite like the idea of
sticking with a powerful base model and using, you know, prompting to, to guide things as well.
So there's all kinds of different approaches. Interesting. On the subject of anthropic and
deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands
all over it. And it was actually quite straightforward. So they trained an auto encoder,
you know, to find a bunch of features that was an unsupervised method. And, and I think they
actually, you know, expanded it to find millions of features. So they had a bit of a needle in a
haystack problem, but they cherry picked some and they found one that corresponded to the
Golden Gate Bridge and so on. And, and obviously other ones that corresponded to what they said
were monosemantic abstract features, got some reservations about that. And the interesting
thing about having an auto encoder is you can clamp the features. So you can say,
turn the Golden Gate Bridge up in, now the language model is, oh, but I just really want
to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when
you look at the activations in the corpus for things like deception, I felt that they weren't
really showing abstract features. They were kind of showing almost keyword matches from Reddit and
so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think
I can comment on that particular topic because I, I mean, I have read the paper, but not in that,
in sufficient detail to comment on that particular thing. But the Golden Gate Bridge,
I think it was a fascinating illustration of what you can do. So I don't know if you tried out
the Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah.
So I think it was fascinating to see. But what was particularly interesting about the Golden Gate
Claude, I think, was that was, that was how, you know, again, it's very difficult not to use
anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's
something role playing these things. But, but how, how, you know, sort of gamely, it struggles to
kind of overcome this tendency to talk about the Golden Gate Bridge all the time, which has been,
which has been kind of clamped to do, but it will keep noticing that it was talking about
the Golden Gate Bridge again and apologizing and then trying to do what it had been actually
asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating
to see that the sort of the, as it were, internal struggle going on there in the model. And it,
and, and it, I think that does show in some ways how powerful they are, because it wasn't quite as
despite the fact that, that it had had this, you know, control imposed on it, but really, really,
really in a very, you know, I mean, not hardware, but really low level, you know, despite that,
it was still, you know, constantly trying to recover from all of that and, and, you know,
with some degree of success. And I imagine this would be true with everybody's models, by the way.
So I imagine that what they found in, you know, in Claude would be very similar. I imagine with
GPT4 and with Gemini, I just imagine that we'd find very similar things with all of these models.
Yeah, I'm sure. I mean, as I said, reservations are, they admitted themselves that the features
were not complete. So they didn't represent all of the activation space in respect of the Golden
Gate Bridge. And in many cases, they presumably weren't monosemantic, but they did cherry pick
some that presumably were. Presumably. And also, I mean, you know, they, they are very interested
in finding these features, which has just linear combinations. And so that, and, and, of course,
it's very, it's very nice when you find those sorts of features, especially if they appear to be
monosemantic, because it does suggest a nice sort of compositionality and explainability and
comprehensibility of what's going on there. But I also feel that they're looking under the
lamp light a little bit, because, because that doesn't mean to say that there aren't all kinds
of other features, which maybe aren't, you know, sort of linear in that sort of way, but nevertheless
are functionally relevant to the final results that it produces.
I don't want to use the word platonic, because the Golden Gate Bridge presumably is it's a
cultural category. And what's fascinating, if it has picked up this thing unsupervised,
and learned this category from the data, is that language models at least possibly
think in a similar way we do. So they've established a category in the same way we have,
which is fascinating. But I'm also really interested in agency, which is, for me,
it's about self-directedness and intentionality. And what I would find very interesting is if you
did clamp the model to only talk about the Golden Gate Bridge, and you could convince it, or it
could convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator
of agency being expressed. Yes, perhaps it would, but I guess it would also be an indicator that
they hadn't succeeded in isolating a feature which was controllable in that way, right,
which was the whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did
actually write about this. And I think you argued, which was counter to what my intuition was, which
was that agency is in the simulacra, not the simulator. And Francois Chorlet thinks a lot
about the measure of intelligence. And he would argue that intelligence is the system which
produces the skill program. So in the context of a language model, he would say a language model
is basically a database of skill programs. And the query is like, you know, I'm going to go and
pull out a skill program, I'm going to run the skill program. So he thinks there's no intelligence
in the language model, which Janus would call a simulator. But if you take into account the
training process and the generative processes that produce the data, then that's where the
intelligence is. Yeah. Can I comment on agency there? So you covered quite a bit in that.
I did, sorry, I just went off piece a little bit. So I would be, okay, so the term agent,
you know, is used in all kinds of different ways in the AI literature. And there's a very
lightweight notion of agency, which is something that simply, you know, hasn't
performs actions in some environment and gets, you know, sort of some kind of sense sense or
perceptual information back from the environment in a loop. And that's the so that's why how we
can talk about, for example, reinforcement learning agent. And that concept of agency of an agent is
very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we
talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical
baggage. So if we talk about something that is acting for itself, then that and if that's what
we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding
to something a bit more like that, then that's going a whole extra step. And I and to my mind,
in today's large language models, we don't see agency of that sort at all, really. The only
actions that they can perform are just, you know, issuing responses to, to, to the users. Now, let's
caveat that immediately, because of course, people are introducing all kinds of extra
functionality functionality to these models, as new things are being announced on an almost
daily basis. And so one thing we see is so called tool use. So, so, so today's models can
make external calls to APIs that can do all kinds of things, send emails, you know, book hotel rooms
for you, potentially all kinds of stuff. So that's greatly expanding the action space, their action
space beyond just, you know, issuing text to the user. So, so there, they, those things are a bit
more agent like. And again, you know, you have to be nuanced and about the way you use the words,
because, because there it's, it's, you know, there's, there is a bit, you know, it's a bit,
it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more
agent like and, but it's still not acting for itself. So in that full blown notion of agency
that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for
itself. So we still don't have agency in that sense. That would be a whole extra step.
Yes. And now I want to hit quite a big topic, because this is something that you point to
in all of your work, which is talking about the importance of physically embodied agents.
And well, not necessarily physically embodied, but embodied.
Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's, let's
test the principle a little bit. So we are, you know, both physicalists and we I'm not any kind of
ist. You're not, you're not a physical, I don't, well, I don't, I don't like, I don't believe in,
you know, signing up for these philosophical positions. So I don't, so I, I, I generally
don't say I'm this is or that I don't deny that I'm this or that is. So, so I don't really like
saying that I'm a physicalist or a materialist or a dualist or a functionalist or identity
theorist or any of those isms, because they all to my mind carry far too much metaphysical baggage.
Oh, interesting. So, but okay, but that wasn't what the question was about, but let's go.
Can I give you another risk? I mean, would you identify as a computationalist?
Well, what do you mean by that exactly? So we're talking about mind here in the context.
We're talking about mind. So, so do you think in principle that minds can be
replicated, simulated at a high enough fidelity without losing anything, you know, in a computer?
By computers. Yeah.
Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build,
I do think that we can build artifacts, you know, embodied artifacts, robots,
that are controlled by computers and ordinary digital computers. And I think that we can
make them that exhibit the kind of behavior that would make us want to use the word mind
and all those mental type psychological terms in the to describe their behavior.
So that's, but I've rephrased it in, you know, the claim in a very, very different
sort of way, right? It's to do with a much more practical thing, can we build this?
By the way, this is, you know, could we, it's not saying that we've got these things now,
but could we build something like that that exhibited this kind of behavior that we would
talk about in this particular kind of way? And I would say, yes, I think we probably can.
It's a empirical claim.
So there are a few steps you made there that will will kind of unpack one at a time.
So you use the word embodied, you use the word behavior, and you use the word interpret.
So the embodied thing is really interesting because, you know, I could say, well, why does it
have to be embodied? I can just simulate the entire universe and it's as if it's embodied.
So I think this is this is the intuition that I'm having about how you think here.
I think you think that being physically embodied is useful because the universe is a big computer.
The universe has given us all of these things, all of these cognizing elements.
I mean, everything is a form of externalized cognition. And if I as a rational agent want
to perform an effective computation, it's much easier for me to do it in the physical world
because the universe is doing most of the work. And my co co host, Keith Duggar,
he actually thinks that the universe is a hyper computer, which means it's performing
types of computation that we could never do with ordinary computers. So that's the thing.
Would you agree with that? Or do you do you want to sort of go back and say, oh, no,
actually, just we could simulate anything in a computer.
So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's a fair.
That would be a nice thing. So whether one agrees or not with that is a matter of understanding
the physics and the maths. And so it's not a matter of opinion. It's a matter of
following through the physics and the maths and so on. So do I agree with what were the other
things? That was a big long list of things that you're asking me to assent to or otherwise.
Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just,
I think cognition is a matter of computation and complexity and divergence.
Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is,
that's what I said, what do you mean by is? Now, that might sound like some, you know,
really annoying pedantic philosophers kind of question. But the problem is that there's an
everyday sense in which we use words like is, and then there's a philosopher's sense in which
we start to use words like is where it suddenly starts to carry this massive metaphysical weight.
And so when you say you think cognition is, it's as if there were, you know, in the mind of God
or in the fundamental reality, a thing which is cognition, whose nature is a certain way,
and there's a certain essence to it. And we might discover it, you know, one day,
and you have an opinion about what it is, if only you knew the truth. Now,
I think that's an entirely wrong way of thinking about all of these philosophical questions.
I think cognition is a word. It's a very useful word that we, although it's not quite an everyday
word, but it's a very useful word that scientists apply in all kinds of ways. And so when you use
the word is, is my accusation accurate there or not? No, it's not. And if you wouldn't mind
me making the observation, I think that you have a tendency to ascribe dualism to many
points of view, like, for example, I'm not a dualist. And for me, when there's nothing to do
with dualism, well, what I'm saying is to do is like the use of words, and what and the work of
philosophy. That's absolutely fair. But I think when I said what is what is cognition,
as as a materialist, for me, it is function dynamics and behavior, right? So it's just a
matter of complexity. Yeah. And so I'm probably just I'm probably, you know, happy to, to kind of
agree to, to, to that sort of claim, you know, so I think so, so, you know, the, so the thing that
I often say that what are my fundamentally interested in, I'm interested in
understanding cognition and consciousness in the space of possible minds. And, and, and so, so,
you know, what do I mean by cognition there? And, you know, you can go into all kinds of
details to say what you mean by cognition in that in that context. But I think having done that,
I would probably agree that the right way to think of it, you know, the most useful way to
think of cognition is in terms of kind of functional, computational, infunctional,
computational terms, although I would only do so in an embodied setting. So that maybe is an
additional thing. This is, this is where I'm trying, I'm trying to get to, because as I said,
I'm not making any ontological claims. It's, it's just a matter, I mean, we can even just use the
word behavior, forget about function and dynamics. Yeah, I don't mind talking about function and
dynamic. Well, yeah, I mean, just, just to sort of keep it really, really simple, because I'm
trying to understand why the embodiment is important. And my hypothesis is, and I agree,
that as an externalist, the universe or the physical things around us, the other agents
in our system, they help us perform an effective computation. So presumably, it would be much
easier to perform computation of higher sophistication if we embody things in the real world.
If we have to simulate the cognition, we would have to simulate everything. And that, I think,
is the reason why you think that embodiment is so important. But is that fair?
I think that's not really the way I would, I would put it. I think, I think the reason I think
embodiment is important is because, is because it, well, I mean, for, you know, I mean, one,
okay, in one sense, embodiment is important because it, because the only setting in which we
use, you know, the natural setting which we use the very deploy wield the concept of cognition
is in the context of embodied things, right, of humans and other animals, right? So, so, so,
so anything else is sort of immediately problematic in one way, right? But let's set that to one
side. So why, so, but so let's imagine that there is some kind of notion of that we can, that we can
conceive of disembodied cognition, right? Then what, what, which, you know,
which contemporary large language models make us start to conceive of a lot more seriously, maybe.
So, so, so why, what, what does embodiment give you there, right? I think that's probably what
you, you know, what you think of. So, so, so to my mind, I mean, so, so in particular, why might
it be difficult to build something that is dis, there's, here's, okay, let's reframe the whole
question. This is a, why might it be difficult to build something that is disembodied, but replicates
the cognitive capabilities of a human, human being. So that's, so, so, so I think my answer to that,
although it's opened, it's an open to refutation, by the way, things are going in, in the field.
But my answer to that is because, is because our embodied interaction with the world
enables us to learn the kind of causal microstructure of the physical world. And the causal
microstructure is, is all about physical objects and the way they interact with each other and
physical substances, liquids and gases and gravity and stuff like that. So what I've called foundational
common sense is it enables us to acquire foundational common sense by interacting with the
everyday physical world and the particular causal microstructure that it has. And part of that is
to do with the fact that the, so this is really important that the, that the, that the everyday
physical world has this is predominantly smooth. It has this smoothness property that's really,
really important. And what that means is that, sorry, just in very physical terms, it means that,
that it's full of kind of surfaces where, where, where, you know, one place is very much like
the next place along, very much like the next place along. And our visual field, you know,
is very, very similar. You move along a little bit in the visual field and it's very, very,
very similar, very, very similar. So there's a reality or the everyday physical world has this,
is fundamental smoothness property, but it's punctuated by all these discontinuities. And,
and, and, and, and that's the way that's its fundamental structure is this basic smooth
against the backdrop of the smoothness are all these discontinuities. And then there's a kind
of law like regular way in which all of this stuff operates with itself, you have surfaces
interacting with each other with things going, you know, so all of our foundational common sense
to do with things like paths and, and, and support and containment and all those sorts of
basic things that, that I think make up our the very foundation of our current conceptual
framework, they're, they're all grounded in that kind of way.
Yeah. And this is so interesting. So your basic argument is knowledge, acquisition,
efficiency is the reason for physical embodiment. And yeah, I think that's a reasonable way of
putting it on. Yeah. Yeah. And, and, but that's very much an in practice rather than an in principle
argument. But it is in an input. Yeah. Yeah. Yeah. But you know, for sample efficiency. But
what I'm, what I'm hearing though is, is echoes of the old Murray Shanhan, because obviously you,
you started your career in symbolic AI. And these were the arguments that were made sometimes with
a rationalism, nativism point of view, but it's like the contains in templates, they would argue
that it's, it's just baked into us and we understand it. But you could as an empiricist
argue, and I, you know, I'm very amenable to this, that the physical world actually helps us
learn abstractions because we're putting things in containers all of the time. Yeah. Right. So,
so there's, there's that kind of efficiency of knowledge acquisition, which is dramatically
increased when you're situated in the physical. Yeah. Yeah. Yeah. Absolutely. Yep. So, so I,
I think that if we're talking about humans and other animals, then I think that's, that's broadly
right. So that's so, so yeah, you so we acquire these foundational concepts through interaction
with this, this world. And then the repertoire of foundational common sense concepts that we can
acquire that way is extraordinarily productive, because, you know, we are able to conceptualize
so many things in terms of these, these basic ideas. I'm very, very, I'm a very big fan of the
work of George Lakoff, absolutely classic book metaphors we live back in the 1980s. Yeah. And
I, and I really think there was something deeply, deeply right about his intuitions in that book.
And I still think that they're, that they're right. So in the case of humans, right? So,
so we through our embodied interaction with the everyday world, we acquire this layer of
foundational common sense that includes things like surfaces and containers and paths and all that
kind of stuff and collisions and things. And then we at the most abstract level. So, you know,
we apply that same repertoire of basic concepts to understand things like say large language
models. If you look at the language that's used in a paper about large language, you know, people
are talking about layers, they're talking about connections, they're talking about, you know,
I mean, these things are all, they're all grounded in very physical concepts, you know.
Yes. I mean, I'm a huge fan of George Lakoff. And of course, you know, he spoke about the war
metaphors and, you know, it's been a long, long road and stuff like that journey. And yeah, it's
beautiful. But but then a lot of knowledge that language models learn are kind of cultural knowledge.
So we shared these simulation pointers, and it's quite relativistic. But I'm also really interested
in because some rationalists argue that it's not possible to go from empirical experience and
universal knowledge. And there is a split between natural knowledge and cultural knowledge. And I
think you and I would agree that a lot of natural knowledge, like, you know, transitivity contains
in and so on, this kind of rationality is just missing at the moment. But as an embodied scientist,
you believe that we learn them by being embodied in the physical world?
Well, I mean, I, you know, it may well be that there are certain. So, you know, we need to
distinguish, you know, empirical questions about humans and human cognitive makeup and
that of other animals and so on. And AI and what we could build in AI, because of course,
it may be the case that human cognition, you know, is as arisen in certain ways. And then
it's an empirical question, you know, what, you know, of how human cognition works. And
it may we may have answers there that are different, that, you know, that we can break,
as it were, when we build things in an artificial way. So in so in the case of something like
transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about,
about, you know, between the rationalists and the idealists, dating back to, you know,
the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by
kind of reconciling these two sort of opposites. And, and, and so the Kantian argument will be
that there's a certain amount of innate structure that has to be there in the mind to, to, to
understand, you know, the world at all. Right. And so maybe, and now, when we think about that
empirically, then, then I guess that we may find that evolution has endowed us with certain basic
kind of templates for understanding the world. And maybe things like transitivity is something
that's, that's there in the same machinery that supports language, you know, so maybe, maybe,
I mean, there's all empirical questions, and I don't know what the latest research on all these
things is, but, but, but yeah, it does seem to me that that's a reasonable position to take.
Yeah, but even evolution is a form of empirical process. So there's always the question of,
of where does it get there? Yeah. And, and that was, yeah, Kant was a transcendental
idealist, wasn't he? But this brings me to our friend Francois Chollet and the ARC challenge.
So, you know, there's another great school of thought, which is that
intelligent, I mean, he argues that intelligence is about these meta learning prize, the conversion
ratio between universal or sometimes anthropomorphic knowledge that we have, and being able to
develop a skill program very quickly that that generalizes very well. So, so the ARC challenge
is almost about how do we codify these priors, and how do we efficiently build skill programs by
combining these priors together. And that seems quite divorced at the moment from the kind of AI
we're building. I think, I think that's right. It is, you know, unless in the AI that we're building
today with generative AI, unless you get these kinds of mechanisms that Francois Chollet is
alluding to, through the magic of emergence and scale, which of course, people are always,
you know, suggesting that maybe that's possible, you know, any kind of mechanism can emerge
right through scale in theory. And, and we've been surprised, in fact, by, by how powerful the
mechanisms, emergent mechanisms that have, you know, developed through learning at scale,
just, you know, with a next token prediction objective, that has been very surprising. But
however, I, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about
whether we're really going to get all the way with this kind of the ability to solve this kind of
abstract problem that's in the arc challenge this way. And so I guess, you know, I am,
I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially
if you make things multimodal and so on, and you expand your generative models into a setting where
you've, where you've got interaction with the world and so on, you know, who knows. But my, but
I suspect that maybe you're not going to get all the way there that way. And so I have a lot of
sympathy with what is probably his intuition, that you need a bit more in the way of innate,
something innate there, or, yeah, innate, maybe that's the wrong word, but, but you need some
kind of, you need priors, where they come from, I don't know, but, but the priors that I would appeal
to thinking about the human case, again, are related to this foundational common sense. So
they're just the notion of an object, right? So if you just, if you have a clear notion of an
object and of movement, objects and movements and object persistence, then they straight away are
going to help you with an awful lot of those arc challenge problems. Because many of them,
you know, if you explain, you know, you figure out how you figure one out, and then
you explain what's going on, then, you know, you see that it's, oh, you have to think of
these collection of pixels as an object, that you move somewhere else, according to certain
rules or something like that. So my colleague Richard Evans had a very good paper on,
which was tackling these, these kinds of things using sort of abduction like,
like processes. And so yeah, so he's thought a lot about this from a much more symbolic AI kind of
perspective. Yeah, that's fascinating. Because even with the arc challenge, the kinds of solutions
that people came up with, let's say it's a DSL over, you know, some domain specific set of
primitives. And the arc challenge is a 2D grid, where you have different colored cells. And
the types of prize that work well are things like denoising and reflections and various types of
symmetry and so on. And that's great and everything, but it's very domain specific. And the elixir,
you know, what we really want are these universal priors. We certainly have human priors, as Elizabeth
Spelke points out, like, you know, and concept of an agent and the concept of spatial reasoning.
An object, a persistent object. Yes. So yeah, absolutely. So I mean, but I think, and I mean,
an interesting question is, is, does it really make sense to talk about universal prize there?
Because, you know, those arc challenges, problems, whenever you kind of figure one out,
then typically you are actually bringing to bear a pretty human set of priors and common sense,
you know, concepts. And, you know, and it may be that you could imagine a perfectly
law-like set of arc-like problems that have solutions, you know, but appeal to, you know,
priors that we would struggle to understand, you know. I mean, for example, when we think of
something in terms of an object, then we want the pixels to be kind of clumped together, right?
And if you sort of randomly distributed the pixels amongst a whole bunch of other pixels,
and you move them around in a systematic way, well, we might be able to kind of pick out the
gist out there, but we might not. And that would be because we're not able to see it as an object,
because we have human priors, right? So, you know, I think that probably all the problems
that he's designed, because we don't know what the hidden held-out set is, but I imagine that they
pretty much all use, you know, sort of human comprehensible priors and appeal to,
you know, foundational common sense of the sort I alluded to.
Yes. I'm sure there must be some kind of universal prize, because in quantum field theory,
physicists use things like locality and sparsity. Yeah, some really, really high-level
things like objects. But then again, you know, quantum mechanics challenges the very concept
of an object even. So what is the difference to you between adopting a stance that a system is
as if conscious versus it being a fact of the matter? I'm a bit resistant to the distinction,
to the very distinction. But this is a very difficult position to maintain, because we have
a very, very strong intuition that there is a fact of the matter about our own consciousness.
And it's very, very difficult to escape from that very, very basic intuition.
But I have a whole approach to these kinds of questions. So shall I sort of describe this?
So, you know, a really question that really motivated me was, you know, suppose that we
encounter... Well, actually, let me not use the word encounter. Suppose that we come across,
has some object, which is a completely alien artifact. And maybe there's consciousness
going on inside this artifact. And the thought is, well, how would we ever know, you know,
there could be consciousness, this thing could be conscious, but we might never know.
And so suppose that it were a white cube that were deposited in front of your lab,
and you were tasked with a problem of, would it be moral to throw it down a mine shaft and
forget about it? So my approach to these problems is that I think in order for the question,
to in order for us to be able to answer the question of whether something is conscious
or not, for even to be answerable or askable, then we then we need to be able to engineer an
encounter with the putative conscious, putatively conscious being. And what I mean by that is
that we have to be able to put ourselves, you know, we have to be able to put ourselves in a
position where we're sharing a world with that, with that putatively conscious, you know, artifact
or being. And so, you know, a good example of this is as the octopus. So Peter Godfrey Smith
has written these wonderful books about what it's like to hang out with octopuses and be with them
and so on. And you know, the really important aspect of that is that he has to put on a diving
suit and go down and be under the water and spend time with the octopus interacting with the same
things and being in the same world together, seeing the same things and so on. So and then on
that basis and the behavior that he observes and and so on, then, you know, he might come to some
kind of might start treating it as a fellow conscious creature. So by analogy, or as you know,
similarly, what I think that we need to be able to do is to engineer an encounter like that,
even if it's a very, very alien kind of artifact, say, so suppose it's this white cube, then put
then one way that it might happen, well, suppose scientists may manage to figure out that there's
computation going on inside this white cube. And then they managed to reverse engineer this
computation. And they can see that there's a sort of division between a world and the things
interacting with that world in this in this computation, there's a sort of simulated world.
And then, and then you could imagine, by some clever engineering tricks, inserting yourself
into that very same world, and being alongside these things that are interacting with this
environment, and interacting with that environment with them. So being in the world with them. Now,
obviously, I'm setting this up to be very much like a games environment and a virtual world and a
games environments, but to make the thought experiment work. But so that's an example of
where, you know, if you manage to engineer an encounter with, you know, these things that are
inside this cube, and then you can observe their behavior, you can interact with them,
and then you can decide whether you or you will, you know, you may or may not start to treat them
as fellow conscious creatures. So there are these two steps, it's sort of can you
engineer an encounter, at least in principle, and that makes the question answerable. And then you
can answer the question by actually having the encounter and interacting with them. And by the
way, we notice that everything there is public, you've made, you know, there's no private realm
of subjectivity here, there's everything is public, it's on the basis of public stuff,
that you're that you that you come to see them as fellow conscious creatures or not.
Yeah, a couple of things on that. I mean, as you pointed out in conscious Exotica,
the octopus is quite interesting, because it's not as human like yet, as you just cited,
more conscious. And the way that we figure out the consciousness, and you know, this is me kind
of interpreting what you said a little bit, is we set up a language game. And I don't know whether
you've read that book by Nick Shater and Morton Christiansen, but it's a beautiful book, beautiful
book. But you know, that I know of the book, I have it, I'm afraid it's, it's incredible. But,
you know, they basically say at you know, Per Wittgenstein, that you play the language game,
and you because you're physically sharing the same environments, you improvise, and that's how you
derive meaning. And meaning is very, very important for relatability. And then we ascribe
consciousness to to that kind of process. And you cited Peter Singer, actually, and I think he said
in 1975 that we have a natural inclination to kind of ascribe moral status to beings which
we think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more.
So, so in the context of the, of the octopus, then, then the sort of, you know, you could then
there aren't going to be language games, you're not going to be engaged in a language game with
the octopus, because the octopus is not a fellow language user. But, but the, but your fellow
language users are other people in your community with whom you might, you'll talk about the octopus,
and you'll talk about the octopus. And, and together you'll arrive at some consensus,
hopefully about whether you want to talk about it in terms of consciousness. And that's going to be
all to do with like observing its behavior, listening to other people's accounts of being
with octopuses. And critically, maybe listening to also what scientists have discovered when they
look inside octopus brains, and they perform behavioral experiments. And you know, that's all,
again, is public. That's who is grist of the mill of settling on a kind of the way we talk about
these strange creatures. Yeah, I mean, I guess for the language, there's two parts to this. So,
the language game, first of all, it doesn't have to be spoken words. It's improvisation of any kind,
it could be gestures, it could be all sorts, it's just behavior. Right, okay. And, and then the
interesting thing with the octopus is we might not be interacting with them interactively. We
might be non interactively observing them as agents interacting with each other, playing their own
language game, but we can still ascribe some kind of measure of, and I actually think what we're
measuring here is agency, and agency and moral status, I think are pretty much one to one. So,
when we see them playing the language game, we start to think of them as agents, therefore,
they have moral status. Yeah, I mean, I certainly think that by observing behavior,
then we may similarly, you know, start to ascribe consciousness to other creatures. I mean, it's
always much more persuasive if it's interactive, I think, than if it's simply observing behavior.
Murray said that if a creature's brain is like ours, then there's grounds to suppose
that its consciousness, its inner life is also like ours. He went on, if something is built
very differently to us, with a different architecture realized on a different substrate,
then however human like its behavior, its consciousness might be very different to ours.
Perhaps it would be a phenomenological zombie with no consciousness at all.
Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of
consciousness, experience, and sensation in terms of private subjectivity. He cited David
Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction
using his phraseology between the inner and the outer. In short, he said to a form of dualism,
which is that subjectivity is an ontologically distinct feature of reality.
Wittgenstein provided an antidote to this way of thinking in his remarks on private language,
whose centerpiece, in an argument to the effect that in so far as we can talk about our experience,
they must have an outward public manifestation. For Wittgenstein, only of a living human being,
what resembles or behaves like a living human being, one can say that it has sensations.
It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism,
particularly in its radical form as advocated by B.F. Skinner, posits that all psychological
phenomena can be explained in terms of observable behavior and environmental stimuli, without recourse
to internal mental states. But behaviorism is often criticized for neglecting the subjective
internal aspects of mental life. Wittgenstein argues against the notion of purely private
language, where words refer to inner experiences known only to the speaker. He contends that
for language to be meaningful, it must be grounded in publicly accessible criteria.
So as Murray said in his article, Wittgenstein argued against dualism, or the so-called
impenetrable realm of the subjective experience. Actually, he said that many folks who make the
in-principle argument against AI often retreat into subjectivity arguments, as our recent guest,
Maria Santa Catarina did. Murray said that the difficulty here is that accepting the possibility
of radically inscrutable consciousness seemingly re-admits dualistic propositions,
that consciousness is not, so to speak, open to view, but inherently private.
Yeah, so Aaron Sloman, who's a professor of computer science and artificial intelligence
in Birmingham, Birmingham University, so he introduced the concept of the space of possible
minds in an article in 1984. And the idea is that the collection of minds that could exist
in our universe, that do exist in our universe, and that could, is much larger than just human minds,
or even the minds of humans plus other animals. It encompasses extraterrestrial life that might
exist out there, and it encompasses artificial intelligence that we might create one day.
So the whole space of possible minds is a very rich object, philosophically speaking,
and merits our study. Yeah, so Wittgenstein's private language argument is really the centerpiece
of the philosophical investigations, which is the book that was published after his death,
which really articulates his later phase of philosophy. And it's all about the idea that we
have, or that we can talk about, private sensations, so things that are purely subjective, and that
only I as an individual can understand and know what they mean. So what red is for me,
and just, you know, internally for me. And so the private language remarks sort of undermine that
very conception. So the basic idea is he imagines, he says, well, so what he means by private language
is very important to kind of get this right. So he doesn't mean a language that I've invented and
that is just something that nobody can understand just because I've invented it. It's the reason
that it's a private language is because it's about something which only I can access subjectively,
which is what red is like for me. So it's the idea that you can have a word for that completely
internal thing that is just mine. So he says, imagine that I keep a diary, and I keep a diary,
and every time I have this experience, a particular experience, then I write s in my diary
to label that I've had that experience. So maybe I have this, I think I'm having this
experience on a particular day, and I write s, and then a few days later, I think I'm having
that experience again, so I write s again. Now, the question he asks is, what possible criterion
could there be for the correctness of that word? What would make it actually stand for
anything meaningful, given that what really makes words meaningful is if they're understandable
in a public setting, if they're understandable really to other people. So there can be no kind
of criterion for correctness that anybody else could validate for this thing insofar as it
stands for something that's completely private. So then there's, and there's a whole set of remarks
that sort of, that after he sets up this sort of little thought experiment that
tell you the implications of it really. And there's one really, really key phrase where
Wittgenstein is always engaging with an imaginary interlocutor, so an imaginary person who's arguing
with him in the book. And so he's imagining this person says, but aren't you saying that the sensation
itself is just a nothing? Aren't you a kind of behaviorist? You're just saying that it's a nothing,
and his answer to this, well, I'm not saying it's a nothing, and I'm not saying it's a something
either. The point was only that a nothing would serve as well as a something about which nothing
can be said. And that little kind of, you know, paradoxical sounding, you know, weird sounding
statement encapsulates something really, really, really profound. And I think when I first really
kind of understood what he was getting at there, it had a really dramatic shift in the way I thought
about consciousness, subjectivity. And and and to my mind, it is the the thing that really undermines
dualism. It's the most powerful way to undermine the dualistic intuitions that we have and that date
back to Descartes and before Descartes that were articulated very well by Descartes. Many of my friends
are fans of Wittgenstein, but they are also fans of subjectivity. So as you were just alluding to
what Wittgenstein did was he created this kind of barrier between the inner and the outer. He said,
you know, for things to be promoted into the language game, for this emergent structure that
we have, you know, when we memetically share all of these language constructions, that can only
come from something observable. But it doesn't seem inconceivable to me that it could in principle
come from something private. So for example, you might have a drugs experience, and that's clearly
ineffable, you can't find the words. But there are things that have some semantic overlap. So I
experience red, you experience red, we both have different experiences. Yet when we talk about them,
some kind of overlapping category still emerges in the public space. Yes, absolutely. So so what
emerges in the public space, that is what we can talk about. And that is that is by the way you've
set up the experiment is by definition, not private, it's public. So of course, we can both
talk, we can both point at something that's red and say, oh, look, look at that red and you say,
oh, yeah, isn't that isn't it beautiful? There, it's manifestly, we're talking in so far as we're
talking and successfully communicating with each other and agreeing with each other. Then
that's the element that is indeed public. But are we not sharing, you know, is language not a set
of pointers to our simulation. So we're simulation sharing when we talk. And even though our simulations
are different, is the pointer does the pointer not form some kind of category over all of our
simulations? Well, there's a whole you've introduced a whole load of terminology there, which, which,
you know, I don't know what you mean exactly by shared simulation and so on. So I think in the
context of a philosophical discussion, as soon as you introduce new, new bits of terminology like
that, then often that's the point at which you're starting to go wrong, right, in philosophical
discussions. In, in technical discussions, of course, you, of course, you're going to introduce
new terminology all the time. But, but, but, but that's the moment when often things are going
when, as Wittgenstein would say, you're starting to take language on holiday and take it away from
its normal usage. So I don't know what you mean by kind of a shared simulation. You'd have to
tell me a little bit more about that idea before I could engage with that thought experiment, I
think. Well, I mean, I'm schooled on, you know, the Karl Fristons of this world. And there's this
whole thing about the Bayesian brain and perception as inference and so on. And, you know, the basic
idea is that we, you know, our everyday experience is a hallucination, you know, we don't, what we
experience isn't necessarily what is out there. And language is, is a kind of pointer to those
simulations. And they must be divergent. They presumably are divergent. Yet miraculously,
we can understand each other. Yeah. Well, I think that so that's the Wittgensteinian point is that
we understand each other in so far as we, you know, what we understand is what is shared, right?
And anything outside of that is, you know, we by definition can't talk about. And the difficulty
is that we have this strong inclination to talk as if there is this thing that's not shared.
I mean, what really fascinates me is that understanding it's not a binary, there's a
spectrum. And we delude ourselves that we understand things deeper than we do, because
it goes into the realm of subjectivity. So when I say I understand something,
my brain is invoking all of this rich subject of experience. And I'm probably taking my understanding
into a domain which is beyond which that you understood. And perhaps this is just something
we willfully do all of the time. So what do you mean exactly by invoke my brain is invoking all
this subjective experience? What are you, what are you, what are you getting at? Well, so we
talk about, as you say, the language game is based around public information. So there is a kind
of cultural level, a lowest common denominator of understanding. But when we understand cultural
artifacts, we further invoke our own subjective experiences. So for example, when I laugh,
I have the experience of laughter, this phenomenal experience. And this is clearly a form of
understanding. It's a subjective form of understanding. And when someone else laughs,
I feel that we are sharing this ontology, right? We're sharing it, but we can't possibly be.
Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're
sharing an ontology when you're just talking about an everyday experience of laughing together,
which we can talk about without any kind of difficulty and without raising any kind of
philosophical problems, just by saying, well, you know, we both heard that joke and we were both,
you know, on the floor, we're laughing. It was so funny. It was an excellent joke, right?
We can talk about that in everyday terms. And there are no problems. There are no
philosophical problems. But as soon as you start, start, you know, getting philosophical and you
start talking about that, you know, what was it? What was your phrase? There's something about
shared, about subjective ontology or something. You're introducing all of this kind of technical
terminology and that's a whole layer of confusion on top of our ordinary everyday
ways of talking about these things, which are unproblematic. Okay, but then there's the anthropomorphic
lens. So you're a human. We both laugh. The behavior of laughing is publicly observable.
Therefore, we have the same experience because we have the same behavior.
Well, it's what you mean by understand here. So for sure, you know, it is a fairly common form
of speech to say, oh, well, you know, you can never understand what it was like
to give birth because you're a man, you know, and this is, of course, this is a normal way of
expressing oneself. And again, that's sort of unproblematic. So there is a sense in which,
you know, in which that's undoubtedly true. But the problems arise when you start to
think that what underlies this difference in understanding or what underlies that way of
talking is some kind of inner private realm that is metaphysically distinct from the rest of reality.
When we share these pointers or these symbols or whatever, structure still emerges. We still
feel that we have a shared understanding. And that understanding can probably be factorized
into a public component and a private component. I don't think that's kooky to say that.
Well, you see, you're very keen to say, well, it depends a little bit what you mean by a private
component there, right? So if you really mean, you know, sort of metaphysically inaccessibly
private and subjective, then then I think, then I think it's not appropriate to say,
speak of dividing things into this private and public component. So that's where that's where
things start to go wrong. And moreover, you insist that you're not a dualist, right? But I think
your inclination to make that division shows that you have dualistic inclinations, as we all do.
So people who are denying that they're dualists, they're denying this that little seed of dualism
that I think is in all of us. And it is part of the way we think and the part of the way you
naturally go when you start to do philosophy. And so it's all, I think it's all very well to say,
oh, you know, I'm a materialist and I don't, but then when you when you start to kind of probe
and you start to discover the puzzlement that these things give rise to, then that exposes
a bit of latent dualism there. Now overcoming that latent dualism, that is the real challenge
that Wittgenstein confronts. Well, I love the challenge. So the way I see it is there is there's
a ladder. So at the top, you have an experience which is ineffable. And then one step down,
you have an experience which is inconceivable, which is Naples argument. And then the, you know,
if you really go down the ladder, then you get into this metaphysical dualism. So I guess I'm
somewhere between the first step and the second step. So I think if I have a certain type of experience,
I simply don't find the words, I can't communicate it to you. But if you put probes in my brain or
something like that, I'm sure there could conceivably be a way of measuring it.
Yes. Yeah. So, so, so this is really important. So for me, what counts as public is not just
behavior, but it's also whatever scientists we can discover. So that so if we poke around in
people's brains, and we do EEG recordings and FNRI recordings and anything else that we can imagine.
And then I, as a scientist, can see this stuff and use a scientist and our fellow scientists
see that that's public too. So that's in the for the purposes of this discussion of this
philosophical discussion, that's all in the public realm, it's not metaphysically hidden,
you can you can and all of that can feed into the way we talk about consciousness. And especially
if we're talking about exotic entities, then then all of that can feed into the way our language
adapts to our encountering them. Yes. I think it's fascinating to decompose as you just did
what people mean by subjectivity. So of course, some people like David Chalmers, they argue that
there is a little bit extra. So there's, you know, behavior function and dynamics. And then
there's that, you know, little bit extra, which is not observable in any scientific way. And I
think, you know, it's fair to say a lot of people when they talk about subjectivity, they're not
talking about the little bit extra. But when we do get to the little bit extra, I completely
agree with you, we've got a big problem. Yeah, yeah, I think we have got a big problem because
because of our natural, you know, dualistic tendencies to it's very, very difficult to think
that that that, you know, if I experience a pain, that that that there isn't something about that
that is just purely minor that you couldn't, you know, the outside world, that other people can
never really, you know, experience it. But that's having that thought that's this moment that you
kind of go wrong. But it's natural path to go down, it's really, really hard to avoid it.
And that's where I think, so Wittgenstein's remarks, they provide a whole way of trying to
reorient your whole way of thinking. And if you sort of really kind of grasp them, it sort of
flips your whole world around, it flips your whole way of thinking around. So it's so that this
whole way of talking and thinking becomes wrong. So very often the strategy when you're dealing
with this is somebody throws out this thought at you, like you've been throwing out various
thoughts at me about, and and often buried in the way those thoughts are framed is the problem.
So the problem is the very expression of those thoughts. And you have to take a step back and
say, hang on a minute, you know, you made this funny move, you introduced this funny bit of
language, you introduced this funny way of expressing things. And that's when that's the
Wittgenstein has this phrase that is that's where the conjuring trick happens is where you the
point that you don't notice is where the conjuring trick happens. So it's kind of, so often you
have to take a step back, and you have to sort of say, hang on a minute, I don't accept that way
of talking that you've just suddenly introduced, which is going down a philosophical garden path.
Yes, and I completely agree. So that is, that is a form of dualism, you know, when we resort to
that little bit extra. And I'm quite interested in this actually, because people like Charmes,
I don't think he likes the term dualist, I think it's a property dualist, but he does talk about
the philosophical zombie, which is a thought experiment or something which has all of the
behavior of us, but is lacking in conscious experience, which gives rise to this idea that
it's almost a kind of epiphenomenon or it's something which, you know, almost you're asking
the question, well, what's it doing if it's not affecting anything? And when I read your
conscious Exotica article, I had a similar thought, actually, because you showed this linear
correlation between, you know, human likeness and, you know, and consciousness. And then you gave
examples of algorithms, you know, like AlphaGo, for example, and they didn't need the consciousness.
And that again raises the question of, what is the cash value of consciousness?
When we use, when we're using the word consciousness, then, then often we are
using it in the context of certain, you know, of certain behavioral behavior and behavioral
inclinations, and we use it in the context of other humans and other animals. And there's a whole,
I mean, for a start, the word consciousness is actually a, you know, it's a multifaceted concept
that it's alluding to many things. And one of the things that it is alluding to is our ability
to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice
the chair, that's why I bumped into it or something. And, or, you know, I didn't see,
you know, that there was a desk over there that might have had something interesting inside it,
if you opened it up. And so we talk about our awareness of the world. And we're at the same
time, we're, we're talking about an aspect of consciousness, and we're talking about
a whole load of behavioral dispositions and capabilities. And so these things are very much,
you know, are very much related to each other in our everyday, in our everyday speech.
So then the question arises, though, are they dissociable? And so I, so, you know, it's,
so now it's very important that it's not like I think there's the consciousness is some
metaphysical thing whose essence is out there to be discovered. It's just it's a concept that we
invent and a word that we use to describe the world around us and our place in it and each other
and so on. And so, so, so then, you know, then the question is, are there things that we might
create or imagine, where we'd want to use the one concept, we want to use the one set of words
and not use the other. And that is what the question comes down to. So in the case of something like
AlphaGo, then I think, you know, we're all kind of agreed that it's actually, there's a kind of
kind of cognition going on there. There's a kind of reasoning going on in AlphaGo.
There's certainly a lot of kind of cleverness. There's a kind of intelligence. There's even,
if we're thinking about move 37, a kind of creativity. So we're willing to use all of those
words, but nobody is going to suggest that AlphaGo is conscious. So there we can see that
they're under certain conditions, the concepts are dissociable. But nevertheless, there's a strong
relationship between the two, because if we think about animals, then often we are going to,
we're going to use their cognitive abilities as manifest in their sophisticated behavior.
We're going to use that as a proxy sometimes, whether we want to talk about them in terms of
consciousness. So sometimes in our usage, we're going to bundle the things together. And sometimes
we're not. But this is all just a matter of, it's a kind of a practical matter of how we use language
and how it's usefully deployed, how language is usefully deployed. And it's not about discovering
some metaphysical entity that's out there, which is what conscious, the word consciousness denotes.
Demis Esalbus recently spoke about this ladder of creativity and of course,
inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad
I had him on the podcast actually. He's a huge hero of mine. And I believe that a hero of yours
as well. Absolutely. Yeah. But he coined this term the intentional stance. And what's interesting
is he was using it to designate a rational agent. But actually it gets overloaded. And I'm guilty
of this. You overload it for lots of things, you know, including even for things like consciousness.
And maybe that's because of the, the correlates of cognition, you know, these things are very
closely related. But can you explain in your own articulation the intentional stance?
Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance
without necessarily embracing the whole of everything that Dan Dennett was talking about
in that context. Because for him, there's a whole big philosophical position around it. But there's
a very simple sense of the intentional stance that we can lift from Dan without necessarily
buying into everything that he said. And it's simply to say that we often in everyday terms
speak about artifacts and indeed animals, you know, other animals, as if they were
rational agents that act on the basis of what they believe and what they want. And by talking about
them in those ways, whether they really, whatever that means, do believe things or have desires
as well, we it's useful for explaining and understanding their behavior. So if we adopt
the intentional stance, say, to use one of Dan Dennett's own examples towards a chess machine,
a chess computer, chess program, and we or go program. And we might say, oh,
it advanced its queen because it wants to try and pin down my my rook. And this is just a natural
way of speaking. And if we use that way of speaking, then it's just good in every way.
Because because we can then, you know, discuss among ourselves what the what the machine is doing,
we can explain what it's doing, we can predict what it's going to do. So that's
taking the intentional stance. And it doesn't necessarily bring with it a belief that these
concepts are literally applicable. Maybe they are, maybe they're not.
But this is where it gets interesting. So I discussed abduction, actually, when I spoke
with Dan, because it's very closely related. I know you studied reasoning for many years.
And the way I see it, when we adopt the intentional stance, what we're doing is we're
we're kind of building a set of variables to describe the behavior of the entity.
And you were just making the argument from the lens of Wittgenstein that the behavior is,
is the only thing. No, no, no, no, no, no. I'll go on.
No, no, no, I don't, I definitely don't think that I don't know what we meant by behavior is
is the only thing. But certainly when we're deploying psychological terms, I don't think
that in any sense behavior is the only thing that determines how we deploy psychological
terms.
No, you're absolutely right. So there's still a massive amount of ambiguity. So when we
perform abduction, we are creating a hypothesis and we're selecting out of an infinite set
of possible hypotheses, but the behavior gives us all of the information. So it's almost
like if we knew how to create the correct explanation, we wouldn't be missing anything
just by observing the behavior.
So what are we talking about now? Are we talking about chess machines or animals or what are
we talking about? What's the context for this thought?
I guess it could work for both. So let's say I want to adopt the intentional stance for
move 37. And I do this abduction. So I build this plan that the agent had. So the agent
had this intention and it took this sequence of steps. And I'm using that as a hypothesis
to explain the behavior. I'm adopting the intentional stance. But it's still highly
ambiguous because I'm selecting out of an infinite set of possible hypotheses.
Right. And in fact, in that particular case, it's almost certainly not the right way of
thinking about it at all because unlike humans, who are when they're playing these games often
do form plans. So if you're playing chess, you often do have a plan, I'm going to try
and capture this area of the board and command this area of the board, say, and so I'm going
to move these pieces around to try and do that. And you might form a plan in terms of
several moves, but ahead. But typically that's not the way, that's not really the way Alpha
Go works. So talking about it making plans isn't really the right way of doing things.
So it's interesting, actually, because the intentional stance, you know, maybe it still
might work. You can still talk about something forming plans, maybe, but it's really not
quite right in that case. Yeah.
Megan, when I say subjectivity, I'm not talking about metaphysical one, but you know, dualism.
But the intentional stance clearly is a form of subjectivity. And when we as a diverse
collection of agents form our own intentional stances, it would seem to be quite a chaotic,
weird and wonderful thing, yet it seems to work. There seems to be, by the way, an interesting
thing here is the way we ascertain agency and culpability is based on the intentional
stance. So you read a news article about someone being stabbed in Australia or something like
that. And the news article was trying to give reasonable explanations. It was because he
was in a cult or it was because he was religious or it was because, and this helps us kind of assign
moral valence to what just happened. Yeah, yeah, absolutely. Yeah. You said something like when
we take the intentional stance that that is a form of subjectivity or something?
Yes, would you agree with that? So I wouldn't put it quite that way. I'm not quite sure exactly
what you mean by that. But taking the intentional stance is I think it's just adopting a certain
terminology and a certain vocabulary for describing the behavior of something. So I don't think we
need to bring subjectivity into that at all, right? So I think we're maybe we're using, we're mixing
up two completely different senses of the word subjectivity here, which is something we should
be very careful about. So when I think you mean subjectivity, you mean that you've just made a
choice, made your own choice between different hypotheses. And so it's subjective. Is that
what you mean there? Yeah, so let's say so for me as an observer, I might do some, let's call it
probabilistic reasoning. And for me, the most reasonable, rational explanation is this. And
it's for me there. That's what I mean. That's what you're alluding to the subjectivity. Yeah,
yeah. Okay, yeah. Well, so, so, so your for me is I think that's that that the sense in which
that's subjective is a very, very different one from the topic that we were talking about earlier
on, because I don't think there's anything philosophically problematic, but in saying that,
you know, that I made my choice. And that's my preference and so on. And so, and somebody might
say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically
problematic about that, right? So, so they, but earlier on, we were talking about subjectivity,
which like the big capital S and where it's alluding to kind of something whole metaphysical
thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different
kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is
dualism and lowercase subjectivity is for me. Yeah, it's for me. Okay, yeah. Okay.
Can you tell me about the risks of anthropomorphisation?
Yeah, so I think so in the context of contemporary artificial intelligence in particular,
then, then the danger of anthropomorphism, I think, is in, is in thinking that the system,
such as a large language model, you know, a chatbot or something, thinking that it has
capabilities, and that it doesn't. That's as simple as that. Actually, it's also thinking,
perhaps that it lacks capabilities that it does. So, so in, so in both cases, I think we can go
wrong. We can go wrong by, because they exhibit very human, like linguistic behaviour, we can
just assume that they are going to be very human like in general in all of the rest of the behaviour
that we encounter with them. But we often find that that's not the case. So we can find that
at one moment, a large language model might make a ridiculously stupid mistake that no child would
make. And then the next moment, it's saying something extraordinarily profound philosophically,
or, or summarising some, you know, enormously difficult scientific article, you know, really
accurately. So, which is, so these things are kind of superhuman powers or translating something
into four different languages all at once. And they're, they're sort of superhuman-ish
capabilities. So it's not, so it can actually be more than better than human in some directions.
And, but, but clearly very deficient in others, you know, with contemporary models that can make
all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say
daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake
to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human,
you know, because you can just, you can just misjudge it in many ways. That's one thing. There's
another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive
capabilities, if you like. But there are other problems with anthropomorphism. So if we see
empathy there, where there isn't real empathy, then we may trust something where there's no
real basis for trust. And so that's a problem as well. You know, we may form, people may form
relationships with, with, you know, AI companions and social AI, where they're kind of fooling
themselves into thinking that there's a basis for that in emotion, where there is in humans,
and it's not there in contemporary AI. And I think that all of those things are problematic.
So things to do with trust, to do with friendship and empathy, and all of these things, I think,
where we can go wrong in seeing, seeing them as too, you know, as more human-like than they really
are. One of the issues I have with anthropomorphism is that people ascribe mental content when it's
not there. And I think you are talking about literal anthropomorphism, which is that they see
human-like qualities when they are not there. And to me, that, that's an important distinction,
because I think if I understand you correctly, you, I mean, you're very no nonsense. You say that
current large language models, they don't reason, they don't form beliefs, they don't happen to-
Oh, no, I didn't say exactly that. Oh, did you not? That was the read I got.
That's a broad, that's a much broader claim. So that's, so I'm not sure I'd go so far as to say
they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what
my, my, my approach is to say that we should be very cautious in using those terms. So I, I'm,
so those would all be examples of taking the intentional stance. If we were to use those
bits of terminology to describe what, what a current, you know, chatbot or conversational AI
was doing, we'd be taking the intentional stance and it's perfectly reasonable to do that in very,
very many cases. So I would know, and I would never make the blanket claim, they don't understand.
I think that that's not quite right. I think rather, rather it's that, you know, sometimes
it's appropriate to say, Oh yeah, it seems to understand very well what this big long article
about nuclear physics was, was all about. And it summarized it really well. It really understood
it. You know, somebody might come up, might say, you know, it really did seem to understand it.
I think I wouldn't say that they were wrong in using that phrase there. So, but then on another
occasion, you might find that it's, that it, for example, recently, people have been posing this,
you know, this classic goat cabbage, Fox problem where you've got a boat and you have to cross
the river with a goat, and you can't have the, you know, more than two things in the boat at once,
and you can't have the goat with the cabbage and all this kind of stuff. And so there's a,
it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery,
right? And people have posed, have posed to, to some large language models that said, Okay,
I've got a boat and a cabbage, and I need to get the cabbage to the other side of the river.
How do I do it? And the large language model models, several of them just start to come up with
this totally baroque solution that involves going backwards and forwards over there. And sometimes
they invent goats that aren't even in the, and that's because they've overfitted or they're kind
of like connected with this classic problem. And, and so no child would make this stupid,
stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously
just didn't understand what the, you know, and, of course, it's quite right to say,
didn't understand in that case. And the anthropomorphism, it comes about when you think
that it understands when we understand and doesn't understand when we don't understand.
The reality is that sometimes it understands when we understand, and sometimes it won't,
and some, it's all mixed up, right? So the mistake is to think that it is like us.
I could, yeah, man, that was beautifully articulated. So it's, it's, it's the mistake
of thinking there is an alignment both in how the machines think and where they make mistakes.
But let's unpick this a little bit, because you were saying it's perfectly reasonable to take
the intentional stance when the thing does the thing correctly, even though it thinks differently
to us. And that's absolutely fine. But I love thinking about these things theoretically. And,
and it's, it's delicious talking to you because you have a background in symbolic AI, you were,
I'm sure, around in the days of photo and pollution with their connectionist critique.
And even now, there are clear examples of language models not being able to do negation.
And we know they're not Turing machines. You know, we can make some strong theoretical statements
that they are limited in reasoning. I agree with you that it's reasonable to say they understand
in certain circumstances. But, but, but where I want to get to is, okay, so we agree that language
models cannot perform certain types of reasoning that we can. So I think we could, so I think we
need to take each of these concepts individually. So, so we dealt a little bit just now with
understanding reasoning as a whole separate thing. So, so, and again, this is all because,
you know, they're not like us. So we have to deal with these things individually, we can't
just blanket say, oh, they don't understand, they don't reason, they don't, or they do
understand, they do reason, it's not like that. It's, you have to take each of these concepts
separately. So in the case of reasoning, then clearly today's large language models, you know,
do struggle very often with, with, with reasoning problems. Now, this is a kind of open research
problem. And people are making a lot of progress in improving their ability to solve reasoning
problems. Now, what the right approach to that is, you know, is an open research question.
Maybe it's just you throw more training data at it with, with, with that includes lots of
reasoning problems. And then eventually you get sufficient generalization there. And it's not
totally clear that that will work. But maybe it will. Maybe it's to embed your,
your, or, or, or to surround, to include it in your, in your system, not just the large
language model, but making kind of external calls to, to say a planner or some kind of
external reasoning system. And you bring that in and you incorporate, you make something that's
kind of a hybrid that uses that, that kind of more symbolic approach. So you might do that.
Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced
this selection inference framework where you basically, you treat the large language model
as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes
calls to the, to, to, to the, to the module in a kind of algorithm that does a number, a sequence
of reasoning steps. So you have an kind of outer algorithm. So there's lots of ways of
trying to tackle that problem. But yeah, just your basic large, take your basic large language
model today, as they are at the moment, and you put it in a chat interface, it's easy to find
reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host,
Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or
achieve a goal. And he cites Claude Shannon, by the way, he said, Claude Shannon said,
we may have knowledge of the past, but we cannot control it. We may control the future,
but we have no knowledge of it. And science leverages control to gain knowledge. Engineering
leverages knowledge to gain control. And reasoning is the effective computation in both. You know,
maybe just in your own articulation, because we can cite examples of things like abduction,
which we've studied in great detail. And it feels like at the moment, even if we do farm out to
Turing machine algorithms, in the days of symbolic AI, that was an intractable problem,
because we've got this infinity, right? And it still seems to me that there are some problems
which there is no easy answer to. So I have a, I feel that you're alluding to
Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean,
I think that sort of abductive problems are where we are looking for an explanation for something.
So I think you'll find that there's a large collection of abduction problems that you could
just present to today's large language models, and they would do quite well at them. And at the
same time, I'm sure you wouldn't be too difficult to find problems, especially if they involved
many steps where it will go wrong. Because doing multi-step, it's the multiple steps that really
are where today's large language models are a bit weak. So that's, again, it's an open research
question. People are working on all that kind of thing. So if there are multiple steps and
step n is dependent on step n minus one, and it's inherently, they're inherently very
computational sorts of things in that sense. So large language models are a bit weak at that,
for sure. And we can introduce things like chain of thought and so on to try and improve that.
But they're sort of a limited success. But my feeling is that those are not the things
where which large language models themselves are inherently strong at. And you should probably
make use of other tools in order to boost reasoning capabilities. I listed some of them. So somebody's
making external calls to dedicated reasoning components. Sometimes it's embedded in the
large language model in a reasoning algorithm itself. So you can go either way. You can either
take the large language model, put reasoning things inside it, as it were, that makes external
calls. Or you do it the other way around. You can have a reasoning algorithm and make the large
language model component of the reasoning algorithm. Yes. I suppose it's a similar thing to the
creativity that there's a kind of creative or inventive abduction. And then there's colloquial
abductive interpolation. And the remarkable thing is just how structured and predictable our world is
and how far you can get with the colloquial predictive abductive. Yes. Yeah. I mean, there's
where you speak of colloquial abduction, which is the kind of thing that we could all and the
person on the street could do. You have some slightly odd situation and you say, why is this
man in the middle of the road with a policeman's hat on or something? And I'm just making something
up. And you say, well, because there's maybe there's been an accident or something. And you know,
so we do all this kind of thing on an everyday basis. It's a bit of a little bit of abduction.
It's what you call colloquial abduction, I think. But large language models think you find a pretty
good at that kind of thing these days. But if you have something that's really complex and has a load
of steps to it, the kind of thing that humans would struggle at, then very often large language
models are going to struggle at those things too. Can you describe the Turing test? The Turing test.
Yeah. Okay. So I'll describe the Turing test as it's popularly. I was hoping you would.
It's popularly conceived because because it's there are nuances in Turing's original paper.
But then again, he didn't call it the Turing test. So obviously. So the Turing test as it's
popularly conceived involves having a human judge. And the human judge is interacting with
two things. One of them is a human and the other is a computer system. And the interaction is
entirely through language, through a keyboard and a screen, say, or a teletype, if you like,
in Turing's day. And the idea is that the human judge has a conversation with these two things.
And the question is, can the human judge tell which is the machine and which is the human?
And if the judge can't tell which is which, then the machine passes the Turing test.
Do you think it's a good measure of intelligence?
I think it's a pretty rubbish measure of intelligence. I mean, I think it's a very,
you know, it's a very useful philosophical thought experiment and starting point for
conversation on this. But the trouble is it's very easy to game. There's a number of problems
with it, which people have been writing about for years, by the way. So there are a number of
problems with it. So one is that it's sort of, it's easy to game, in a sense, because there's
a temptation to make something to pass the Turing test, you make something that has all kinds of
strange human ticks and peculiarities, and then it seems human. And so you can fool the judge
that way by making it just a bit eccentric, which is obviously nothing to do with intelligence at
all. So that's one thing. And then another thing is that the domain of the test is purely linguistic.
So you're not testing anything to do with the sorts of intelligence that you get in a non-human
animal, say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to
navigate the ordinary, everyday world and survive in it. But none of those kinds of
intelligence are tested by the Turing test. So you were the scientific advisor on the film
Ex Machina. And I'm not sure whether it's Machina or Machina. It is Machina, yeah. I was speaking
with Irina Rich and she said, Ex Machina. But anyway, she's right. Yeah, she is. I digress.
But there was a special type of Turing test in that film. Can you explain that?
Well, indeed it wasn't. It's not the Turing test. So there's a point in the film. I assume that
people are vaguely familiar with the setup of the film. But there's a point in the film where
Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava.
And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing
test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know
whether the thing that's being tested, whether you don't know whether it's a machine or a human.
That's the point of the test. The point, Nathan says, is to show you that she's a robot and see
if you still think she's conscious. So there's a number of ways in which this is very different
from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence.
And those are not the same thing. And secondly, the point, as he says, is to be persuaded
that the artifact is conscious, even though you know that it's not human, not biological.
So you know that it's an AI system and you still think that it's conscious. In that case,
it passes this test. So this test, I call the Garland test after Alex Garland, who is the
writer and director of X Machina, quite different from the Turing test. I think that those lines
in the film were actually really brilliant lines and really clever lines from Alex in the script.
And when I first saw the script, which was long before it was filmed, and that bit was in there,
and I put spot on next to those lines in the script, because I thought it was such a very
good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed
think that Ava is conscious and thinks of her that way. What that really means is that
he comes to treat her as a fellow conscious creature. And we see that in the film because
he wants to help her escape. And just as a kind of thought experiment, maybe it's another conceivability
thing, but it does seem conceivable what less consciousness would be like, but it seems less
conceivable what more consciousness would be like. Well, so in both cases, I think it's all about
exercising our imaginations actually. And I think exercising our imaginations is perfectly
legitimate in philosophical discussions. So in fact, in my newer paper, Simulacra as
Conscious Exotica, I think I say that I advocate doing philosophy with the detachment of an
anthropologist and the imagination of a science fiction writer. So I think that's something that
aspire to do. So we can carry out all kinds of imaginative exercises to describe exotic
entities in a science fiction like way. So we can describe an exotic entity and we can describe all
kinds of strange behaviors. And that's as sort of as far as we can get really. I think we can
imagine all kinds of strange behavior, and we can imagine scientists studying those kinds of
strange behavior and what underlies them as well. And so we can imagine all of those things.
And then we can also imagine how we would talk about those things. So imagining how we as a
kind of community and how the scientists and the philosophers would talk about these imagined
entities is all kind of part of it. So I think we can do all of that. I think that is a kind of
doing philosophy. And just coming back to the Turing test one last time, I read an interesting
take on Twitter recently that we've been thinking about the Turing test all wrong. There seems to
be a subset of people who it's almost like the Eliza effect. They see something, they want to
see something. And it's almost like the test is actually testing the humans rather than the
intelligence. Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again,
I think we should distinguish consciousness and intelligence in this, in this regard. You know,
although there are relations between the two or consciousness and cognition. So what just
one thing I wanted to say about the about the Turing test is I do feel actually that today's
large language models kind of pass the spirit of the Turing test. So we defined the Turing test
earlier on or the kind of the popular conception of the Turing test earlier on. And as I remarked,
you can kind of game it. And, you know, and there's certain sort of problems with it,
but notwithstanding that, I think that actually today's large language models
pass the spirit of the Turing test, I feel. So they pass the spirit of the Turing test because
they do really have human level conversational skills, I feel. So my feeling is that if Turing
were alive today and were presented with Gemini or ChatGPT or Claude III, he would say, yeah,
that's what I had in mind, you know, you've done it. That's, that's, it's kind of passed.
Interesting. And you also distinguished, you know, the, the imitation game is defined by Turing.
And the colloquial popular conception is that there's no adjudicator, it's just, you know,
a person and an intelligent machine. What is, what is the kind of the main difference there?
What I mean, meant by the popular conception, I think was that maybe popular conception isn't
quite right. There's sort of contemporary version of it that's used in academic discussion,
in fact, is what I meant. That's what I meant by popular. So, so there I'm imagining, yeah,
there is a human adjudicator there. But the difference is in so, so Turing, the way Turing
sets up the test is, is, you know, he has some specific, specificities about, you know, the,
the number of minutes you should, you know, you spend interacting with it. And then also he sets
it up in the context of this game where party game, where the aim is to try and work out whether,
which you've got a man and a woman that somebody is, is, is conversing with, you know, via paper.
And they don't know which is the man and which is the woman, they have to guess which is which.
And that's the way the thing is set up in the original paper is by analogy with that. So there's
all, so, so there's all kinds of, you know, peculiarities in the original paper, if you're a
Turing scholar that aren't, aren't really quite relevant to the, I think, the kind of contemporary
way that we think of the Turing test. But I also think that the sense in which we, in which today
systems pass the spirit of the Turing test, it's the reason it's only the spirit of the Turing test
is because of course you very often can immediately tell that it's, that it's an AI, not, not least
because it'll just tell you right away, right? If you just ask it, it will just say, well,
as a large language model trained by Google or by, you know, open AI, so it's easy to tell which is
which. But nevertheless, I think that they have attained more or less human level language skills
more or less. And so I think, I do think that Turing would say, you know, as I predicted,
yeah, we've got there. Now, I think Turing would be fascinated to see the weaknesses that are there
and the strengths that are there. And, you know, there are many things that today's systems can do
that are vastly more powerful than I think he anticipated in that paper in the 1950s. And there
are, and then there are other, you know, there will be other weaknesses, I think that would come out
that would surprise him as they've surprised us all in a way. I think, I think many of us in the
field are surprised to see something that can do so amazingly in certain respects and yet have so many,
you know, still so many weaknesses in others. Can you introduce Nagel's bat?
So in 1974, Thomas Nagel published this paper called, What is it like to be a bat? And the
point of this paper was to draw attention to the fact, if it is a fact, that there are creatures
that are very, very different to ourselves, to humans, but that we assume have some kind of
consciousness. We assume in his terminology, we assume that it's like something to be that creature.
And he chose a bat because bats are very obviously very different to ourselves. They fly,
they use sonar, they're pretty weird animals. And so the way the thinking is that, well,
it's probably like something to be a bat, but what it's like is going to be very different from what
it's like to be a human. And Nagel uses that example to get at a whole metaphysical idea,
which again, I would say is pointing to a kind of dualism, to suggest that there's a whole realm
of facts about subjectivity, which, which are outside of the purview of objective science,
actually, but which nevertheless, you know, are part of reality. And so for him, for him,
you know, I feel that it alludes to a sort of kind of dualistic way of thinking again,
that there's this realm of subject, facts about subjective things, and there's a realm of facts
about objective things. I read Nagel's bat many years ago, his paper, and he was kind of saying,
wouldn't it be great if we could move towards an objective phenomenology? And you are clear that
he's, he is pointing to dualism. He's not just saying that it's inconceivable in the sense that I
couldn't imagine what the experience of a bat is like. It's just inconceivable. You're saying that
he's actually making a dualism argument. Well, he would probably deny that. Because nearly every
philosopher will enthusiastically deny that they're dualists. But, but I see dualistic thinking,
you know, all over the place. And I do think that this, this is, this is an example of it.
Yes. I mean, it's a similar thing with John Searle. I think he is adamant that he's not a
dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another
example of a popular thought experiment, which apparently people get wrong. My friend, Mark
J Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument.
Yeah. Yeah, yeah. Yeah, Mark, Mark has, you know, tenaciously clinging to the, to the, to the, to
the Chinese language, the Chinese rim argument. But, but yeah, I don't, I can't really see
eye to eye with Mark, who I have enormous respect for on this, on this particular
subject, I'm afraid. Yes. Well, I mean, Mark is a very good friend of mine. And
yeah, maybe that that's a rabbit hole, we won't go down. But I'm very, you know, amenable and
convinced by by some of Mark's arguments. But he's certainly a phenomenologist, which is
he is adamant that he is a monowist. So he's not a dualist. It's another example of someone who
claims they're not a dualist, but you would say they probably are a dualist. Yeah, well, I don't
know. We'd have to have Mark sitting here. I mean, I would have needed to have read one of his
papers on this very recently to kind of do that. So I'm not going to accuse him of anything. But,
you know, if he was sitting here, we could have a discussion about it. Well, I asked him straight
up, because I was trying to pin him down. And he said he is an idealist, a monowist idealist.
Right. And it might might be instructive, actually, if you just to explain what we
mean by idealism. Right. Well, so, so, so if, especially if he's using the word mono there
as well, then I guess that that he is suggesting that while a physicalist, you know, thinks that
there is only one substance in reality, and that's, that's, that's material reality. And so,
and so the stuff of it, there is no such thing as a separate stuff of mind metaphysically.
So, so for the physicalist, there is no dualism. If they really, really think that, but the trouble
is that as soon as you kind of probe, then often they have struggle to deal with dualistic intuitions
about subjectivity. Now, the idealist, on the other hand, also thinks that there's only one
substance, but that substance is the substance of mind. So, so physical reality has to be a kind
of construct out of, out of this one substance, this one, you know, out of mind stuff. So that's
a very different kind of, it's a different way of avoiding dualism. But, you know, it has its own
problems about, you know, how do you explain, account for science and the success of science
and so on. I also interviewed Philip Goff in a beautiful studio recently, and he is a
Cosmosychist. Right. But I think it's better just to use the word panpsychist. But I think Cosmosychist
is, is where you have the, the, the teleology baked in. So rather than it being bottom up, it's kind
of top down. There's some cosmic purpose to the universe. And the universe as a whole is made of,
you know, the fundamental material of the universe is consciousness. And what's the relationship
between that view and idealism? So, so I guess the, for the, for the panpsychist, so the panpsychist,
you know, you know, as far as I understand these philosophical positions, I mean, you know, I
shouldn't, I shouldn't put myself forward as somebody who necessarily, you know, understands
them in depth. And there are, you know, every, everybody has, everybody who subscribes to these
views has a different, slightly different version of them as well. So, but the pan, the panpsychist
certainly thinks that reality is composed of physical material substance, but that physical
material substance has, you know, irreducibly has a psychological dimension to it, a mental
dimension to it. So every physical object, you know, has, you know, has a little bit of
consciousness in it, if you like, in some sense. I mean, I find it a very difficult
view to articulate because I just find it so completely counterintuitive. So I can't really
put the panpsychist's hat on and express their point of view. You need a panpsychist here to do
that. Because to my mind, this is, you know, again, with my bit constinuous hat on, then I
just think, well, how do we use words like consciousness? Well, we use the word consciousness
and all of the related terms in the context of each other, of other human beings. And so,
you know, I, and it's in the context of our being together in the world and, and, and your
behaving in certain ways and our exchanging certain looks when we're doing things, and
that we, that we understand each other as fellow conscious creatures. And so that's the context
in which we use, you know, words like conscious. And when I speak about, you know, you're not
conscious, you're asleep. And, you know, it's all in the context of other humans that we use
those words. So it's just ludicrously inapplicable to use that word in the context of, you know,
I don't know, a brick or a toaster or an atom. It's just simply the words simply are not applicable
in those contexts. Yes, it's so interesting because Philip told me that he grew up as a,
as a Christian. And I've had Richard Swinburne on as well. And he's got a book out about are we
bodies or souls, you know, putting forward the case for substance dualism. And so there's,
there's the moving away from dualism thing. There's the teleology things, because I think
if you are of this frame of mind, you like to believe that there's some kind of grand purpose.
And, and there's also the wanting to not wanting to be a monowist, basically. So you could perceive
it as a form of mental gymnastics to say, okay, well, I don't want to be a substance
dualist, but why don't we rearrange the structure somewhat so that consciousness comes first. And
there's some kind of cosmic purpose. And we're building a worldview that still makes sense to me.
Yeah, yeah. I mean, I, my personally, I think all of these positions involve a great deal of
mental gymnastics. And, and I, you know, I reject any kind of ism or what I mean by that is I don't
use that term to describe, you know, my views at all. So all of these isms are, you know, are
misguided. And what we need to do is just to, to dismantle the whole way of talking, which
makes us, you know, makes us confused in the context of this kind of these kinds of issues.
So that's the, that's what Wittgenstein is trying to do, as he puts it, to show the fly the way out
of the bottle, right? There's this famous phrase. So the fly is the person who's ended up thinking
all these philosophical thoughts by taking ordinary language into strange places, taking it on holiday.
And, and so to show the fly the way out of the bottle is to just bring all of these concepts
back to the, to their ordinary everyday usage and, and to show thereby that you haven't really,
you haven't lost anything, the puzzles evaporate. So that isn't an ism. That's a, that's, that's
rather that's a kind of a kind of critical methodology for just shifting the way that you
think and talk all together. And final question on this, where do you sit on the, the kind of the
teleology question? So one view is that we have a purpose. The, the physicists would argue that
it emerges, you know, from quantum field theory. And a lot of sophisticated study of biology kind
of build this intermediate view of teleonomy. Where do you sit on, on that kind of spectrum?
I'll be honest with you. I don't think I sit anywhere on that spectrum. I think, I think those
are issues on which I don't have sufficient expertise to pronounce. So, so, you know,
you just keep me, keep me on the, on consciousness and cognition. You know, I mean, that's that,
you know, all this stuff is above my pay grade, you know, Professor Shanahan, it's been an absolute
honor to, you know, have you on MLSD. Thank you so much. I really appreciate it. Thank you.
Thank you so much for having me. It's been fun.

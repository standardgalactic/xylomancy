And Sarah, it's amazing to have you back on MLST.
It's so lovely to be here.
It's been a year and a half or something since our last conversation.
Yes, it has.
Yeah, because I think we met at Newerrips and then I came and filmed
you in the London office, which is really good.
But fans of the show, of course, will know that our first interview
was about your hardware lottery paper.
Yeah.
And that was your first grumpy essay.
That was a very grumpy essay.
You know, I lead Co-Here4AI, so it's research that we do a lot of
fundamental research and we, a lot of my work is on efficiency,
reliability and building these models that scale the next generation models.
So you can go to Co-Here4AI and take a look at some of our work.
Sarah Hooker is VP of research at Co-Here and she leads Co-Here4AI,
a research lab which seeks to solve complex machine learning problems.
Co-Here4AI supports fundamental research, which explores the unknown.
She leads a team of researchers and engineers working on making large
language models more efficient, safe and grounded.
In this conversation, Sarah discusses her recent work on multilingual AI
and the challenges of developing language models which work across
many different languages.
She provides insights into the limitations of current approaches
like RLHF, especially for low resource languages.
Sarah also talks about her recent paper critiquing the use of compute
thresholds as an AI governance strategy, explaining why simple measures
like FLOPs are inadequate for assessing AI capabilities and risks.
Sarah emphasizes the importance of understanding the relationship between
compute, data and model architectures.
She advocates for a more nuanced approach to AI development and governance,
which considers the complexities of language, culture and the representational
long tail, where all the low frequency data lives, which is so often
neglected in current models.
Sarah's work aims to make AI more globally representative and equitable,
as these technologies become increasingly integrated into society.
Enjoy the show.
Your most recent grumpy paper is called on the limitations of compute
thresholds as a governance strategy.
Can you give us the elevator pitch?
So this paper is, it has a very boring title.
And at face value, it's just about this kind of odd, known to not many people
in the public, compute thresholds that have actually been widely adopted.
They were adopted by the executive order on AI.
They were adopted by the EU AI Act.
And what's fascinating is that these are kind of the key policies that
have come out on AI.
Why did I write a paper about this very, very deep topic of compute thresholds?
Because it's at the heart of really what our field is asking right now,
which is that compute thresholds are based on an idea that models at a future
size, so it doesn't apply to models in the wild now, are going to trigger some
difference in risk profile that deserves scrutiny.
And this question of does scale trigger this moment where models have these
properties that are fundamentally different from models before that.
It is actually very much being at the core of our field for the last two decades.
Because in the last two decades, we've had this philosophy of bigger is better.
We scale data and we scale model size.
So this essay is really about, is that true?
As we look and stand and look at the last decade, what do we know about the
relationship between compute and risk?
And what do we think is the feasibility of these compute thresholds actually
mitigating risk?
And that was the starting point.
Yeah.
So in the beginning, you were talking about how historically we have tried
to estimate and control and respond to risk.
Can you give us a couple of examples?
Mostly as a society, we have tried to grapple with this idea that we want to
proactively control our future for the better.
And this is actually recent as well.
So it's very typical of modern society that we have this notion of planning
and anticipating risks and being able to mitigate.
There's examples where me and you do this every day, right?
We could put on sunscreen if we're knowing we're going to the sun.
We avoid working in dark areas.
There's also areas where governments have done this, you know, even in this
modern era of the last 300, 400 years.
And it requires two things to do well.
One is that you have to understand where risk comes from.
So you have to understand what is the kind of lever of risk.
A good example of where that's failed is something like the Black Death, where,
for example, a lot of the protocols around the time didn't realize that rats
were the main vector of the disease.
And so because of that, many of the mitigation techniques were unsuccessful.
But the second crucial aspect is that once you've identified the lever of risk,
you have to form a proportionate response.
And we also have examples where that's failed historically.
So, for example, the London Fire is a great example where it was known that
this was a risk, but the fail to curb it early on in the expansion of the fire
led to the destruction of a large part of London.
So these are the two challenges that policymakers face.
And what compounds it for something like technology is that typically the idea
of identifying the lever of risk is very difficult because most technology
breakthroughs, by the nature of being a breakthrough, you're in a kind of rather
than a proactive setting, you're in a retroactive setting.
What do we do now that this is changing the world?
And that's a very difficult position for someone to form a response to.
Yes, exactly. I mean, you know, one of the themes of the paper is we're
super bad at predicting the future.
Yeah. And maybe we should just linger just, you know, just for a second
on the executive order and the EU AI Act.
Now, they used this notion called flops.
And please explain what flops are in a second.
But but the in America, they set the limit to, I think, 10 to the 26.
Is that right? And then in the EU, it was they wanted to be a little bit more strict.
So they they just went down to 25.
Tell me about that.
So flops, by the way, is this measure by which this compute threshold is done?
I think flops is just a it's a way of counting.
So typically, when you train a model, you're doing many different operations.
You're doing additions, subtraction, multiplication, famously matrix,
multiplies, dominate our modern networks.
And so that can be decomposed into all these operations.
So flops is just a tally, it just counts it up.
And these thresholds, 10 to the 26 and 10 to the 25 are this idea
that at that moment, that's when you kick in scrutiny.
And it's important to realize that doesn't apply to models in the world right now
for the executive order, for the EU AI Act.
When it comes into effect next year, it might hit a handful of models.
But this is a for looking policy.
It is not based on current risk in the wild.
And so that's interesting to think about because that creates the question of,
well, are we good at predicting what risks emerge?
And is that the right number to do it at?
And that's where it starts to get very interesting.
Yeah, yeah, so they have a tally.
They kind of estimate how much computation is happening in the models.
And then they've set this threshold.
So they don't care about anything below that number.
So there's lots of real risk now that they presumably don't care about.
And then they're saying above this number, there's a problem.
And I think did they set the number roughly commensurate to the size of a GPT-4 model?
So it's difficult because they haven't formally justified why they set the number there.
But anecdotally, my understanding is that guided it.
So it's interesting.
And it's worth thinking about, like, well, it's this interesting aspect of,
well, firstly, there's a notion of, is this number a valid tally of risk?
Like, is training compute the number that you care about if you wanted to do this tally,
if you believed in this future risk?
But secondly, if are we good at predicting like that number?
And that's kind of interesting to think about.
Yeah, I mean, to me, it was a bit crazy on its face.
And what's going through my mind is, have they got anyone working in the government
that actually know what they're talking about?
Because presumably, if they asked you, you would have thrown this thing out straight away.
And you gave some examples, actually.
So you said there are things that have a normal distribution,
you know, like the weight of babies when they're born or blood pressure or certain things like that.
And then there are other things that are significantly more complex, like if I'm buying a house,
what does the estate agent do?
Well, they have a complex model where they look at the neighborhood,
they look at various different factors, you know, some people have indexes
and they have things that can shift over time.
So having this one absolutist number just seems a bit ridiculous.
I actually think I feel for policymakers because I think it will put pressure on them
to continually adapt the number.
So I will say there were benefits in the thinking of this number.
I think it's unfortunate it got so far without scientific input.
But one reason that people like Flops is, for example, it's hardware agnostic.
You can measure it the same way across different types of hardware.
And also it's fairly easy to measure because all it's doing is a tally of operations.
So it also avoids specifying maybe what risk you care about.
So it gives a degree of, I would say, flexibility there for governments to adapt over time.
I would say that is probably one of the larger shortcomings is that by not specifying,
you can end up with something which is evading your Flops threshold but a highly risky model.
So I think that's actually one of the crucial shortcomings.
But I do understand that the motivation of a lot of policymakers are what else?
Like what else could I use?
I would argue if you're going to stick with this measure and, you know,
it has been formalized in several policies, you have to understand
that this can be manipulated as a measure.
And there's many ways and I list them out in the paper.
But to your point, a single number puts a lot of pressure on policymakers
to constantly adjust this and have the technical information to adjust it
because this is a rapidly changing distribution.
The notion of compute has been highly unstable.
Just looking at the last decade, we know this.
And so it will quickly have an expiration date.
And I would argue what you're saying is excellent as an example.
One is that you need a reference class of what are you comparing against.
So you mentioned the kind of real estate agent who compares the pool of houses.
Each of these domains, like biology models, which are very interesting to certain
researchers because of bio risk, language models, multimodal models,
they have different distributions of compute requirements.
And so it has to be done relative to your reference class.
But also it should be done dynamically.
The same way that a real estate agent does it based on a percentile of
surrounding houses, the notion of a single inflection point for risk
is not a viable policy tool because you just are changing things all the time.
Yes. Yes.
I mean, there are so many things to get into here because you went through
a wonderful list of examples in your paper.
But one of the the elephants in the room is that it supposes that there is some
kind of like linear commensurate relationship between compute and capabilities.
And of course, you're working in multilingual.
I mean, it actually penalizes you because you need to do more compute
just to have a model that, you know, works at all in many different languages.
And this thing just isn't working for you.
Yeah. I mean, what you're pointing out is that once you do something like multilingual,
you have you're basically trying to learn a new distribution each time
that's as vast as English.
And so you you typically need a lot.
It's called the curse of multilinguality.
And so you need more compute.
There's other things there, which are very tricky is that how do you flops
and how does training compute account for the vast amount of change
and how we optimize after training?
So we talked about RLHF, there's also instruction fine tuning.
There's also things like synthetic data distillation, which shortens training times.
So these are all what we call inference time optimization.
So you spend time after training, you pay for it and compute.
Like you can do best of end sampling, which is what you refer to
with the Francois Chollet, where you sample a lot of completions and you choose the best.
That all has very pronounced benefits for models.
So typically your model performance alone, just using a subset of these techniques
is two to six times more powerful and that's not reflected in flops.
Yes. Yeah. I mean, because I'm really interested in, you know,
when you look at the model lifecycle or the predictive lifecycle,
there are so many places where you can spend computation, right?
So you can do data set generation and you can do, you know,
obviously there's the training of the model and then you can do like
inference time optimization and active inference and a whole bunch of stuff like that.
And they are only taking into account the model training.
But then there's the further issue of training provenance.
So for example, I can download a model from Hugging Face and I can fine tune it
and do a bunch of stuff on it. And like, how do you know, right?
It's just it's just an inscrutable bunch of weights.
Like you have no idea how much training has gone into it.
This is the idea of tracing flops across the lifecycle.
I think this is also going to be formidable because increasingly
the most popular models on Hugging Face, by the way,
are models which haven't been instruction fine tuned. They're base models.
And why? Because people want to do continued pre-training.
They want to overlay their own optimization techniques,
which suggests that people are using this as one step in their optimization process.
It's going to be a formidable challenge to tally it in a reasonable way,
especially when sometimes the way that we measure these,
think about something like mixture of experts or a classic ensemble.
What counts then?
Because you may have many different experts in your mixture of experts,
but you're only using two at the end.
Classic ensembling is even more nuanced because technically
you didn't even optimize all the models together.
You just show up and you ensemble them at the end and you get one model at the end.
So how do you handle that?
It's very interesting and it's very related to this challenge of
people are likely taking some level of compute already
and they're doing some changes at the end of training that make it more performant.
Yeah. And then there's this matter of Good Heart's Law,
which is that when a target becomes a measure, it ceases to be a good measure.
And there are so many examples of this.
I mean, like, for example, the banks have these arbitrary limits
on the amount of money that you can send, which is why it's set at,
let's say, $10,000.
And then you see loads and loads of transactions at like 9999
because they know what the limit is.
And it must be the same here, right?
They're just going to it's going to gamify the system.
People are going to evade it in so many ways.
I think that the main advice I have about this is that if policymakers
have decided on this and they're going to go for it,
they need to complement it with an auxiliary measure of the actual rest that they care about.
It has to be an index because if you just stick with compute,
it is too easy to evade because there's too many different things you can do
post training to gain percentage points.
And there's too many ways of essentially shortening your training time
or reducing your flops while still arriving at a highly performant model.
And so that's the other key recommendation I have is that you need
something that is anchored to the downstream risk you actually care about.
And compute is not.
It just reflects our belief that more compute is better.
And that simply is too simplistic of you
to account for all the ways in which smaller models,
if they're very targeted, can be extremely risky.
Yes. So I think maybe we should bring Rich Sutton in.
So he wrote this, yeah, he wrote this, you know,
this essay called The Bitter Lesson, and I'll let you bring it in.
But he was partially responsible for this idea that compute is all you need.
Yeah, bring that in.
Yeah. And by the way, I think that's a fantastic essay.
I it really is this idea that history tells us in computer science,
in particular, that all efforts to codify our expertise,
to work on very fancy ways of imparting what we think is the right way
to learn to a model have been particularly futile.
Like he's really saying we're not very good as computer scientists.
And the biggest ingredient of success that's driven things
is being adding compute to the mix and that we can do things that are algorithmic,
but it has to play well with compute.
So in any way, he has kind of got this idea of hardware,
but it's more general.
It's this idea that it's not specific to a certain type of hardware.
It's just compute.
If you play well with compute, if it scales well, it's going to be the winning variant.
And yeah, go for it.
Well, I mean, I just I have an intuition that he's he's right and wrong at the same time.
So I mean, in terms of system one models that just memorise things better and better,
he's kind of right because there is a commensurate relationship,
you know, as as we memorise more of the long tail, the models get better and better.
But I still think that there might be a fundamental break between compute and capabilities
when an ironically a regulation like this might incentivise to find such a break.
So, you know, we might design system two models that actually do reasoning
and have a, you know, neurosymbolic architecture or whatever.
And and now all of a sudden we've got like really good capabilities
with less compute.
Yeah, I mean, you're hitting on the hitting on the head.
I don't disagree with you.
I think where I agree with Rich said it is that for given architecture,
say, transformers, and you can throw more computer that up until a certain point
where it's saturated, but you're going to see all the things equal,
your data sets equal, Peter's better because these are greedy learners.
They, they're, you know, our deep neural networks are frequency counters.
You're going to see gains on the long tail performance and overall gains.
Where it misses the point is that really there's a few things going on.
One is that because our current representations are so inefficient,
there's ways to really change the algorithm itself and bend the, the rule of compute.
So, and the rate at which computers needed to unlock gains.
And deep neural networks in particular are a great example of this
because they're so painfully inefficient.
Because we have to show all the data the same amount of times
because we have to do these global updates.
And so we're seeing all these tricks.
For example, now we care about data again and we care about data quality.
So we condition that space better to represent what we want to model downstream.
That means we have to train far less because all the features
and the data set are the ones we want to learn.
Whereas if you just train on the internet, there's a lot you don't want to learn.
So you have to kind of unlearn it afterwards and spend a lot of compute
just trying to find what you want within that.
So that's where you bend the rule and it becomes more nuanced,
which is that the other thing that that misses is that,
or at least that wasn't a core part of this essay,
I actually think Rich might agree with me on this,
is that your rate of compute is really determined more than anything
than the, by the prior of your algorithm.
So yes, if your algorithm plays well with compute,
if it's scalable, compute unlocks a lot.
But the rate and the saturation point is determined by the algorithm.
And what do we mean by this?
Convolutional neural networks are a great example.
Introduced in 2012 really unlocked scalability.
Why?
Because convolutional filters and patches made it possible
to model high dimensional images at the time.
Why?
Because you move your patch over your image.
This takes advantage of local relationships.
You can really reduce your dimensionality.
Max pooling layers, which Jeffrey Hinton is famously grumpy about,
and rightly so, just throws away everything except for the max.
You reduce the amount of features.
You unlock the ability to model images.
You have scalability up to your point.
This is what we famously know about image models.
Now there's really a saturation point.
Everyone who switched to transformers because there's a new arc.
So what I mean by this is your algorithm is kind of your most heavy
prior on your search space.
And Rich is right that what plays well with compute
is the one we're going to default to.
But the question becomes your scaling laws and your ability
to predict the future are essentially limited to your algorithm and compute.
And that's what's interesting is that it means we're not very good
at predicting the future because it means we're too locked in
to this narrow arc of this architecture we use combined with compute.
Yeah.
I mean, even the CNN example is, I think, an example that proves Rich wrong
because he said in the bitter lesson that any attempt to impute
hand-designed priors like CNN is a symmetry.
So in the CNN, it encodes a symmetry and scale invariance.
And essentially, all it's doing is a shortcut because it's still in MLP at the end.
So what it's doing is it's basically it's building an MLP
as if you didn't have the scale and symmetry in there.
But it's just kind of like doing this thing.
And it's like, you know, it's embedding it all into the MLP.
But it's basically still an MLP with a symmetry shortcut, which was hand-designed.
So, yeah, yeah.
But there's still this notion, though, that there's, you know,
connectionists think that, I mean, like Neil Nanda said to me,
he's like a rationalist guide from DeepMind.
And now, yeah, DeepMind.
And he said, these things are just smarter than you, man.
How did that make you feel?
Well, I mean, not great, not great.
But, you know, there's this whole, like, mech-interp thing.
And I think that they genuinely believe that there's just some deep form
of inscrutable intelligence going on.
There was that, like, monosomanticity paper from Anthropoc recently.
And, you know, there's this deep belief that there's something really interesting going on.
What do you think?
I think that it is.
There are persuasive, here's what I will say.
Language is very powerful, which is why we connect with this technology so much,
because language is how we connect with each other emotionally.
It's very tied to how we as humans are quite tribal.
And so whenever a model learns a distribution that is indistinguishable from humans,
I think that it gives pause.
And it is, I do think that these conversations are useful
because it gives worthy pause to how this technology is used.
And, for example, I'm very against many of the efforts
to sometimes deploy these algorithms without notifying humans that it's an algorithm.
I think that you should always be aware when you're talking to an algorithm.
And that's because these are quite convincing sometimes.
And so it's very important that we always communicate what is the role of the model
and the human.
Do I think that there is a higher reasoning here?
I don't.
I think that in many ways, whenever you have persuasive interpolation
of a space between ideas, it's going to be surprising to us.
I think what delights us is the creativity and the surprise and element.
But is this ability to reason?
I don't think so.
I think that there is a clear relationship with the type of architecture we have
of a memorization relationship.
And we know this.
We know that when we increase scale, we learn a given architecture.
Or when we do different tricks to compensate for scale,
so we can go smaller and still learn things, what we're really doing
is we're just trying to induce good memorization and good steering
towards part of the distribution we care about.
Frankly, while why all these optimization tricks have worked beyond compute
to reduce compute has been we're largely training on a distribution
we don't want at the end of the day.
So we start by training on the internet.
And we actually don't want the internet when we engage with these models.
We want something that's very chatty and philosophical and wise.
And so a lot of what we're doing is we're trying to steer things towards
the part, the tiny sliver of the distribution, that training data
that we care about.
And that's why we have so many optimization tricks before we get to the end.
But that's fascinating because what's that that's really telling you is that
unlike a traditional machine learning problem where you're training to the
data set is the distribution you want to learn, a lot of what we're doing
with language is we're unlearning.
We're just trying to steer and unlearn and ignore and then focus on what we want.
So it's really interesting.
Yeah, machine unlearning.
That's fascinating.
I mean, I'm also a big fan of the externalist tradition in cognitive science,
you know, like for recognition.
And in that sense, I think it doesn't make sense to draw a boundary around the model
because I think, you know, our sense making semantics, situated knowledge
and so on, it's kind of observe a relative anyway.
You know, these things are embedded in our culture.
And sense making humans put prompts in there and they interpret the outputs.
And actually, even the data generating process that went into building these things
was, you know, originated from the universe.
We're all agents and we're all in the physical world and the social world.
And we, you know, we're doing the effect of computation, both in how the models
are built and how they are used and interpreted and evaluated and so on.
So what are you going to do?
Like draw a big, should you include the flops of the universe as well?
Oh, that's fascinating.
Yeah, is this interesting?
Yeah, it does spark something else with flops, which is that.
So typically the final model that you deliver is only one of the possible models, right?
So in fact, typically even at massive scale, you train many candidate models
and then you choose the best one.
And so it's interesting because these are not optimized together,
but then implicitly optimized through the selection process.
And it is really interesting because we kind of steer towards what we want.
So yeah, it's a fascinating dynamic.
I didn't think of it like that, though.
That's an even bigger meta approach for thinking about this.
Hardware lottery paper, which we talked about, and that was a really fun
conversation because I remember it was when you were doing the trio kind of
the group of ML Street talk, the earlier version.
And I think you originally invited me onto the show because there was this idea
that I wrote about, which is that most of computer science history has been driven
by whether your idea works with the available hardware or not.
And I think that resonated with a lot of people at the time because what
it's really saying is that we may be in another hardware lottery right now.
That something like Transformers, which we all use, has become increasingly
locked in to GPUs and to TPUs, which have all been built to accelerate this one hardware.
So raises the question of what next and how do we make sure that the next
brilliant idea isn't stuck in purgatory for decades?
Because that's what happened to deep neural networks.
It simply didn't work until GPUs were converted from video game use, which was
really not the intended purpose of how they were converted to work for
machine learning workloads.
And that happened over the course of a decade.
It was a very slow conversion process, but that turned out to be the key for
deep neural networks.
What we now identify as 2012, the moment that this explosion of interest
and funding and acceleration happened.
People identify that with convolutional neural networks or the algorithm, but
really it was both.
It was the hardware making the algorithm feasible.
And that's when you first had the empirical proof that deep neural networks were viable.
To what extent do you think there is an algorithm lottery as well?
Oh, what do you mean by that?
Well, as in now, your paper was about the basin of attraction of hardware, but
is there a basin of attraction of algorithms as well?
Absolutely.
I mean, you just have to look at optimizers to see that.
So what I mean by that is an algorithm is really how you learn from data.
This is the essence of an algorithm.
And what we've been locked into is this idea that it has to be gradient based
optimization.
It's really hard to do something that's a non-differentiable objective.
And what that means kind of in accessible terms is that we're stuck
doing these global updates.
So the way our models train is that we kind of send through shovel through
data and then the update to the weights is based on an average of all the
data that's seen.
Why is that tricky for a few reasons?
Because one, it means that we overfit to the average.
And that's why we need so much training data, because essentially if you're
just overfitting to the average, it takes ages to learn the rare patterns.
So you train for longer, you need more data.
But the other thing that's very tricky is that it means that models forget.
So every time you shovel in new data, the model forgets the old data because
you're updating everything at once.
A nice point of contrast is that as humans, we typically have long-term memory
and short-term memory.
These are different ways of learning and the rate of learning is different.
And so when you process information, some are stored in your long-term memory.
You may have a distinctive memory from a child that you think is like your
first memory from a child.
And it may be, you know, mutated over time, that's the nature of memory.
But this ability to preserve two states of what you did today, what you did years
ago, that's very different from gradient updates.
And somehow, because we haven't found an alternative way, even though a lot of
people have worked on it, we are in this algorithm based on where it's very tricky
to propose an algorithm that doesn't rely on a differentiable objective.
Yes.
And I think we'll talk about this later, but part of the problem is people
think of this paradigm as a form of general abstract pure intelligence.
And the reality is that certainly with your multilingual work, that we're
dealing with just this long tail of complexity, heterogeneous data sets.
But maybe that's a good segue because you just released this, this primer paper
called the AI language gap.
Can you, can you tell us about that?
So it's quite fun because in some ways, what you're talking about, these themes,
these so nice into the AI language gap.
Really, when we have built these models, we've overfit to what is weighted most
importantly to those who built it.
And these models have been built in a few places.
We're in London.
London is a very big hub of where researchers have been.
So is the US and Europe and China.
And because some of the first impressive large language models were built in the
US and the UK with DeepMind and in the US with, you know, places like Cohere and
places like OpenAI, I think that that has necessarily reflected the nature of
the researchers who built them.
They wanted to work in English.
The tricky thing is that when you try and make AI actually work for the world,
you're talking about this vast array of different languages.
So there's 7,000 languages in the world.
80% of those have no text data.
So it's truly not even a language problem.
It's also a multimodal problem.
The second part is that even with the top 101 languages, no models, except for
I-101 currently cover it.
So there's this vast amount of the world that simply isn't reflected in the way
that AI works and who AI serves.
The primer about the language gap is really calling attention to this.
But at the root of this problem and what you're getting at with this theme of how
does models work with the long tail is that the fundamental issue is, are models
really overfit to high frequency patterns?
And so the key difficulty with the language gap is that, one, these languages
typically are underserved by available data on the internet.
The internet kind of reflects early patterns of adoption, not necessarily
humanity as it is.
So that means that there's way more English on the internet than there is
people who speak English.
So 5% of homes speak English, but 50% of the internet is in English.
In contrast, something like Yoruba, spoken by 40 million people, is really underserved.
And so it's a long tail problem.
But here's the other thing.
It's a pattern where the rich get richer and the poor get poorer because we're
now in a synthetic data era.
So as models get much better at generating an English and Chinese in particular,
these are the two high resource languages that are well served.
You're going to see more content in those two languages.
And that makes it even harder if you're relying on large data to properly
represent the languages that are currently underserved.
Yeah, so interesting because we're moving away from the material world
into the information world.
And right now in the material world, there is a kind of a commensurate
relationship between the number of people who speak English and the amount
of data on the internet.
And as you say, we're now moving to this place where we are generating
data of language and the polarization is going to increase.
So you're talking about there's this kind of North American tech based
inequality, which is getting worse.
And you said that there are safety implications for this.
And I was interested in this word safety.
We spoke about this last night because when I think of AI safety, I think of,
you know, like X risk in Silicon Valley and stuff like that.
And I've noticed over the years, the conflation of the two communities,
you know, in terms of ethics and existential risk.
And how do you feel about that?
I think it's, I mean, I feel grumpy about that, but here's the thing.
So, you know, subfields are always like this.
I think there's always this notion of subfields, which are extremely, you
know, actually people caring about the same objectives, trying to
distinguish themselves over time.
AI safety encompasses a large array of perspectives and expertise and people
who, who care about different things.
I think that this, this shift towards talking about from response for AI
to AI safety is a fascinating one, because it's been a bit intentional
from communities who want to maybe suggest that response for AI is distinct
from what they're doing and instead saying AI safety is about these profound
risks, these like fundamental issues of our time.
And response for AI is, okay, great, you're doing that, but keep going.
And so I do think there's a very interesting thing with how we name
things and how we really have precision in our conversations.
Increasingly, I think AI safety encompasses both of these and you need
more precise language with both.
And I actually think my main ask is we need to be precise about what our
objective is with AI safety, because it can be, it is in many ways the same
goals as response for AI, but the degree of precision one that is articulated
is, is a sign of accountability for the objective.
And I think sometimes the use of that word lacks accountability.
Yes, exactly.
And when I hear some ex risk folks talk about AI, it feels to be in the abstract.
And what I mean by that is they are just thinking about, you know, if we scale
this technology up, it learns these abstract representations, which work in any
situation, and it's just a matter of scale.
And it feels unmoored from the research, because when I read your work
about multilingual models, you're clearly pointing out that when we have what
they call low resource languages, the models don't work very well.
They're not learning these abstractions that just automatically work in other
languages, there's a specificity to it.
That seems to be the difference to me.
Yeah, there's this big question right now, what you're getting at is there's
this idea of, there's a mystique that some people are attributing to scale.
It's been called different things.
It's this question of, are there emergent properties?
Are there properties that appear from nowhere that we unlock with scale?
By the way, multilingual is originally proposed as one of them.
Like in the first paper about emergent properties, multilingual was there.
It's like, wow, how did this appear?
We didn't even have this in our training data.
But it's very interesting.
Now there's been subsequent work, which is shown it was there all along.
It just wasn't documented in training data.
So scale is just really learning your long tail.
It's learning the low frequency.
We just get surprised because I think there's a big disconnect between what
we think we know about the vast amounts of data that we train on and what's
actually in that mix.
And so there is often certain properties where it takes scale to unlock
because it's very relaged this question of memorization.
I think how this conversation has become a bigger theme, like beyond this kind
of scientific question of when do properties emerge and what to scale
unlock, it's become this thing of kind of creating a myth around these models
that there's a lack of ability to understand what scale gives.
And then that is used to kind of impart a degree of anxiety that because we
don't know precisely when this property will emerge, there should be anxiety
about this and there should be a sense of real danger about the use of these
models.
And I would say that that is actually the wrong framing for this.
The right framing is that one is the notion that we're just going to keep
on scaling, I think is flawed.
I think there's very clear evidence that, you know, bigger is not always better
that we're kind of reaching the limits of how we scale with something like
transformers and it's very architecture bound.
But the second thing that I would say is it really, it really kind of ignores
the mounting evidence that these kind of properties are surprising only
because we're not going to predict in what emerges at scale.
Yeah.
Yes.
Yes.
I mean, I spoke with David Chalmers recently and he bemoans the fact that
whenever we have a complex system, you know, we say, oh, it's emergent, you
know, and there is something interesting going on.
As you say that when you memorize more and more of the long tail, you do see
this qualitative increase in capabilities and it's quite easy as an
observer just to say, oh, you know, it's an emergent property.
And people ascribe things like, you know, divergent intentionality and reasoning
and all of these kind of anthropomorphic qualities to the models, even though
they probably don't really exist.
But one interesting thing though is that, you know, when you memorize all of
these surface statistics at scale, you can use the language model as an idea
generator.
And like on Francois Chalets arc challenge, you know, Ryan Greenblatt generated
about 30,000 completions for all of the tasks.
And the remarkable thing is in terms of sensitivity, the correct answer is in
those completions.
And then you can do some neurosymbolic evaluation and selection and you can
pull the thing out, you know, so you can build an architecture that does really
well.
But I think people underestimate the amount of human selection kind of like
smoothing out the brittleness.
Yeah.
Well, right now, I agree there's a huge amount of creativity that's unlocked
with these models.
So this iteration.
And actually, by the way, this idea of like, you can create a lot of different
options and then you can verify which are correct.
You see this in a lot of different kind of states of progress right now.
That's how code is currently done.
Like you can create a really nice code data set by running code and seeing
which one passed the tests and kind of do formal verification of which one's
passed.
So it's not that these models are not capable of generating sensible answers.
It's just that the probability on every single turn consistency is what
you're pointing out.
Consistency is sometimes not there.
And I also think part of what is beautiful from the creativity perspective,
iteration of ideas is that sometimes you actually don't want consistency.
So the objective may be different in different settings.
So for example, for code, we always want code that passes.
So that's a good example where we sample a lot just to get the subset.
But sometimes I've talked to people who use it as a way to seed ideas or things
like that, and actually there the diversity is the important part and
gain very different responses each time.
And so I think over time we'll actually have different models for different
things and be able to, this is the core of the challenge of steerability of
control, which right now is not good, frankly.
Like why do we have prompt engineering and why does everyone love it?
Like this is a, this is a symptom of a problem, not a symptom of a solution.
The fact that we spend so much time prompt engineering the perfect thing to
steer.
So hopefully we have better tools in the future.
But I see that as one key thing that will change is that we'll be able to steer
towards the mode we want to use.
Do we want consistency?
Do we want exploration?
And how does it fit into our iteration pattern?
We won't spend too long on this because I asked everyone about this, but you
know, where are the sources of creativity?
So as we memorize more of the long tail and the models can extrapolate and the
human promptors can, you know, mix novel combinations of things together.
So there's this potential extrapolative space and whatnot.
That's how creative can they be?
Yeah, I've been.
So one of the recent papers that we released was a paper about what we
call active inheritance.
It's this idea that we can start to steer how we sample data to sampling
different parts of the distributions from different models.
So so far the paradigm of like sampling data, either for a human or for another
model has been very much like there's a single teacher, you're the student or
there's another student or your co-creators with a single model.
But if you think about it, one, that's a kind of passive inheritance.
You're just trying a single prompt.
You're not really kind of enforcing any criteria.
Active inheritance is where you sample different parts of the problem you want
to solve from a variety of different models.
And that diversity actually spurs really interesting patterns where you
increase the realm of what's possible and kind of spur higher quality that
transcends the quality of any one model.
And I see that as a very important step that we're building a lot of work on,
including a multilingual, but also in this fundamental area of we actually
used it to, in the paper that we just released, we used it to steer towards
non-differenceable objectives.
So, you know, going back to what you were talking about the algorithm basin,
this idea, and I was saying everything is dependent on gradient descent.
It's very hard to steer towards non-differenceable objectives.
Before deep neural networks, there was decades of research on just these
non-differenceable objectives.
There are things like, how do you compute the perplexity of like a given,
like what is the reading grade level of a given sentence?
So there's these scores that are kind of codified, but you can't really use it
because they're not differentiable.
And we actually show that you can use that as part of Active Inheritance,
where you steer towards models that are better at a reading grade level.
And then you use that to kind of form your basis of your dataset.
So I think that's fascinating.
And I think that's really going to spur creativity beyond just this more
static notion of you just sample from a single teacher.
Yeah, that's fascinating because there's so much of your research has been on
the tyranny of forgetting the long tail or not paying attention to it.
And of course, you can solve that with better optimization and, you know,
federated learning and the agential, you know, kind of multimodal systems
that share information and query and almost like an adversary or setup.
Yeah, it's a more dynamic pool.
And so it's this idea that you can actually and actually the long tail is
the perfect example of where I find Active Inheritance most promising is
that because the long pool, the long tail, you typically have many weak teachers.
No one's very good at the long tail, but sampling effectively and doing
this active inheritance rather than passive of just choosing a single teacher,
but choosing a variety of teachers and then comparing and optimizing.
This is fascinating.
And I suspect it will benefit most the long tail.
So you said in your language gap paper that language models are going to
become integral to modern societies.
How do you see that panning out?
It's already happening in different ways.
Like I would call it the high low way.
So we can talk about high level themes, which is there'll be a ability
to communicate much more easily.
And so you'll just see much more proliferation of things like art or people
writing or kind of taking away some of the difficult parts of how we communicate.
I think the low way is just the more granular ways that you're using it right
now, which is I use it typically for very basic things throughout my day.
Like I'll I, you know, we write a lot of papers.
So I'll do like my citation reformatting using a language model.
So there's both the mundane, but there's also the profound.
I think the profound is that it changes the ease of communication.
And so it changes the rate of inflammation flow.
And this can be really powerful.
It can mean that we can be more creative and experiment more the space.
It can also bring new risks.
And so I think this is also important to think about.
Interesting.
And you said that, you know, this North American bias in language model training,
you said that it affects the design, the outputs and the behavior of the models.
What did you mean by that?
Well, there's two things.
I mean, when I say design, the outputs and the behavior of the models,
I think that there's optimization bias in the models itself against different languages.
So tokenizes is a great example.
So Roman scripts are things like French, Italian.
We also have, you know, Latin based scripts.
This is also English.
Whenever you deviate from Latin based scripts, you have something like
Hindi, Korean, and these do not play well with tokenizers.
So there's a lot of work which shows not only do tokenizers not work very well for these languages,
but also it ends up being at this double tax because not only does the models perform worse,
it also takes more tokens to represent these languages.
So it's higher latency, higher cost for users outside of English to use APIs right now.
So that's an example of like an optimization bias.
The other, frankly, the issue is that whenever you're trying to have a model
that represents many different parts of a distribution,
typically our solution right now is we've got to give it more capacity.
So I1 and 1 was an interesting example of this.
We released I1 and 1.
It represented 101 languages and you can start to think about how many that is
when you try and list more than 20.
So you'll probably get to 10 and then you'll start struggling.
101 is nuts.
It includes things like we had Welsh, we had Irish, but we also had Telugu.
We had many African languages and we had very much these underrepresented like Haitian things,
the variety and the complexity as well as dialect.
So 101 is probably like preparing for space race.
It's like at the most extreme of the problem.
And what's interesting is everything you learn there trickles down to less severe settings.
But one of the things that we learned there is that you have to be very careful
about how you use capacity because we had this 13 billion parameter model
and we were stuck with it because there was no pre-training data that covered 101.
So this model was actually from 2019, which is crazy given how much has happened since then.
But because of that, we were stuck with this model and it meant that everything we had
to do was try and make the best use of capacity.
We had to wait properly.
We had to do data processing, data cleaning, but also we had to do a lot of work
with synthetic data and the manipulation of how we did the optimization time.
So you can do this two ways.
We could have even increased it to 103 billion parameter model
and then we would have to retrain because right now models,
unless they're trained with the data from the beginning, you can't just add it at the end.
But also there's a secondary way, which is we get much more clever
about the optimization and the data creation.
And so this is really the issue is that when you go multilingual,
all your problems in a given language are kind of multiplied out.
And so you have to be very careful about all the details.
I wonder what's the relationship between language and capabilities?
And the reason I asked this is there was a great book I read called
The Language Game by Morton Christensen and Nick Chater.
And that very much led me to this idea of situated knowledge
and I guess so actually a lot of our cognition and thinking is quite specific
to the culture and the language that we are in.
And that seems to go against the grain of the idea that these things are learning
general patterns of reasoning across languages.
So then it rather kind of leads you to this conclusion
that you actually need to be within the language and the culture
in order to do the kind of thinking that they do inside that culture.
So how does that work then when you're mixing all of these together
into one language model?
I think it doesn't work that well right now.
So I would say this is like one of the core problems because you're precisely right.
So we actually so there's a few things I would say here.
One is that we already see this with things like dialect.
So the notion of dialect, which isn't really a counter for any models,
including I think that we all go is mainly just to be the first next step
in state of art, but even ours doesn't do this nuance of dialect.
We do have various dialects of Arabic and some other dialects,
but take something like Portuguese, for example.
Portuguese is spoken in many different places of the world.
I spent part of my childhood in Mozambique.
The Mozambique Portuguese is very different from, you know,
I guess the most extreme would be Brazilian Portuguese.
But also Portuguese in Portugal has its own nuances.
And actually when we did AIA, we had researchers all over the world
who were part of this project and we would frequently have these little riffs
between the Portuguese contributors in Brazil and the Portuguese contributors
in Portugal because they were asked to review within a single pool.
And so because the Brazilians outnumbered the Portuguese in Portugal,
they would all correct their submissions to Brazilian Portuguese.
This is a very interesting concept and this is just on the notion of dialect.
But your wider point is this idea that language is a tool for communication.
And there's actually this very interesting concept
about whether we even use language to think or if we use it as a utilitarian tool.
Why is that relevant here?
Because the way that we achieve an objective
is going to depend upon where we are in the world.
And the way that technology should serve us
is going to depend on where we are with the world.
This has come out recently.
We just released a paper which I'm quite proud of,
which is thinking about this idea of local versus global harms.
At any one moment, we have multiple facets of our identity.
So there's notions of what is insensitive to us
as part of a notion of being global citizens.
And that probably gets to things like there's a universal agreement
that some types of harms, like harms to world children, are particularly egregious.
And most of, like almost universally, our legal systems reflect this.
But there's also notions of very particular harms,
which are cultural and very specific to how we live.
And that is reflected in things like wording.
So we just released this paper, which I think is important for safety,
but also part of this broader move and in the field,
which is that most of our models right now are trained with a single objective,
a single decision boundary.
What that means is all the data gets flattened into this one decision boundary.
I'm very interested in multi-objective optimization.
And this changes it so that you can hold multiple objectives at once
and that perhaps you can even adapt these objectives on the fly,
which is very interesting.
Yeah, a couple of things.
I mean, you're talking, I guess, about the interplay
between having a relativistic worldview and having some global norms.
And in general, the way we do model alignment with our LHF and so on,
it tends to de-complexify the reality of the world that we live in.
And in ethical frameworks, there are de-ontology people
who think there are just guiding principles and there are virtue ethics people
who think there are certain virtues that we should emphasize
and there's consequentialism that there are certain consequences that are bad.
And as you were just pointing to,
it's very, very difficult to have a hybrid ethical framework
that encapsulates all of these things together.
What kind of work are people doing and what are you thinking about here?
Well, we recently, the paper we just released,
is this really, this paper we call the Multilingual Prism,
which is this idea that for safety,
we collected both local examples of red teaming safety
with really this very nuanced collection process
across multiple languages, as well as harms that were considered global.
From there, you can go into something like our LHF
and you can change the notion of a single reward model.
So this is an area I'm quite interested in.
Like, how do you have multiple reward models?
And then how do you balance them?
This is the crux of the problem and that's what you're getting at.
So how do you have these two things in unison?
And I suspect what we're going to see there is this notion of adaptation
of our models in a more nimble way than previously.
So typically in a production setting, you spend months doing this model,
you release it, cool, thumbs up, enjoy, and it's not as dynamic,
but a true production model is refreshed and is more nimble
and is deployed in different ways to different places.
Like, Netflix famously does this with its recommendation systems.
I think here this is actually a much more profound way of doing this
because you can have these models,
which essentially the way that they're steered is adapted.
And this is both interesting as well as profoundly challenging
because the tricky thing is you want to be sensitive
to how the preferences of users change around the world,
but you cannot overfit to too granular a preference
because this is a philosophical tension you're actually getting at,
which is that a libertarian view would say
every person here has a list of preferences
and those should be respected in their rank order.
But as a society, we typically say we have this group of preferences,
but we also adhere and kind of subsume some of our preferences
for the common grid.
And so there's this notion as well when you articulate that as an algorithm,
how do you get that balance somewhere in the middle,
like where you are basically not adhering completely to a societal view.
I think that's one of the concerns about algorithms
and tokenizers being used in certain states
where there's a state influence on how algorithms are deployed,
but also not being used in a total libertarian view
where we don't want objectives that essentially amplify
how a person thinks about the world
without balancing and introducing different viewpoints.
Yeah, it's so fascinating because even things like polarization on their face
seem like an incredibly bad thing,
but some kind of diversity preservation
might actually lead to a pluralistic society
that gains information and a degree of health that we need.
But with safetyism in general, though, there's always this notion of...
I think we probably agree on this a little bit
that if you leave people to their own devices, then that can be bad,
but you also need a little bit of that
because otherwise the society might become quite sclerotic.
And these decisions presumably need to be baked into
the way that we build these models in some way.
Yeah, and currently then not.
I would say currently the way that we approach safety,
it's quite... We have this notion of refusals.
So when you've engaged with a model,
you typically will see refusals for certain
what I would call the more black-and-white type cases.
There's this really interesting opportunity I see
in the evolution of how we think about safety,
which is that instead of just saying, I can't answer this,
to kind of provide more nuance or provide links to additional support.
And I think that's very interesting
because there's a different type of discussion.
But I would say your perspective, what you're talking about,
which is really this part in the middle
where you have some values as how you build your algorithm,
but also you realize that this is someone
who's engaging with an algorithm
and the algorithm should not reflect perfectly
a single view of the world.
You need more ways, I also think, within the UI
for the person to influence and provide feedback
and to... Something is, of course, hallucinations.
It's really interesting because hallucinations is not...
You can't... I'm very skeptical we're going to eliminate hallucinations
because they're also what we really like about these models.
It's the creativity.
So, for me, this is not just...
We often fixate a lot on the model in these conversations.
The model has to solve this.
But I think there's also a notion of the system.
And I think that some things that will be interesting
to play within the system is how does the user express
when they think that steering isn't aligned
with what they think is reasonable?
A good example of this is, for example,
a question about sexual health.
There's valid reasons to ask those questions.
There's valid reasons to want to understand,
like parts of your biology or things like that.
Wikipedia has whole pages about sexual health.
And so, it's very interesting that a lot of systems
refuse to answer this right now.
So, there's this nuance where we need to make sure
that we are updating these binary decision boundaries
where it's outright refusal and move towards something
which is instead steering towards resources.
Yeah, and I think so much of this is about...
When you fix something, as it is now,
it can go both ways as well.
So, maybe you can explain to the model,
no, in this situation, I think you really should tell me.
And likewise, the model can say,
no, actually, I think the reason I'm not allowing you
to do this is because of this,
and maybe you should shift your viewpoint a little bit.
And it has to be done subtly
because people don't like re-education, you know?
So, that actually creates...
You know, they say the road to hell is paved
with good intentions.
It creates an equal and opposite reaction
when you try to re-educate people.
But coming on to RLHF,
I mean, we've spoken about this for years.
You've always been a bit grumpy about RLHF.
And I read your paper.
Unfortunately, I don't have an internet connection,
so I'm doing this from memory.
Can you just remind me the multilingual paper
that you've just released
where you're trying to remove translation artifacts?
Oh, yes, RLHF speaks many languages.
So, this is a really nice paper.
It was led by John.
This idea that we were the first to extend
a lot of the RLHF techniques from many different languages.
So, I think actually there's a wider view of RLHF
and I have been grumpy about it, but go first.
Yeah, what were you gonna...
Well, I mean, even in this paper, you were saying that...
I mean, obviously the broader conversation
we've just had is that we need perhaps, you know,
some kind of a more systems approach
where we have a multitude of different models
and optimizers and datasets and all that good stuff.
But even within RLHF, you were saying
that it's hideously complex and inefficient
and you have to have this separate reward model
and it can't be optimized very well.
And you were saying that sometimes using DPO
or even just basic reinforces better.
Yeah, so there's an excellent paper
which we also recently released back to basics
where we actually do a much more profound question of this.
Not even specific to multilingual, we take a step back
and we say, okay, it's really interesting.
All the kind of most cited papers
originally on RLHF are papers
which really take this canonical method within RL, PPO,
and apply it to the language setting.
And PPO really evolved in the RL, traditional RL space
to address and mitigate a lot of the issues
with traditional RL.
RL is typically over a large, expensive search space,
incredibly noisy.
And the trickiest part, right, is that your errors compound.
So it's almost like thinking about,
well, what if I bet incorrectly at a game table
and then tried to bet again and did it incorrectly?
And your losses just compound the more that you,
your estimates are off.
So PPO is heavily what I would call kind of regularized
or conditioned to limit the impact of an incorrect estimate.
What that means is that often it's quite memory intensive.
You kind of have four models and play at any one time.
And it also means that it's quite sensitive.
So typically PPO to train, it takes longer.
Enter the language setting.
And so the initial adoption of PPO and the success of it
was taken at least value.
This is incredible, let's go with this.
But the language space is also an enormous search space
because if you think about it,
you're trying to predict the next token,
how many tokens, how many possible tokens are there
in the world to represent language?
But by the time you have a trained model
and by the time you've done all this pre-training,
the search space is much narrower.
And actually it's quite interesting
because the likelihood and the probability
of what the next token will be is actually very concentrated.
And when you have this pre-trained base,
it's only gonna be a few different tokens
that you would likely predict,
which means that this was overkill for the setting.
And what we show convincingly back to basics
is that you can strip a lot,
a lot of the components of PPO out.
You can propose something like RLU,
which is still an RL method,
but that works effectively and even surpasses it.
And RLU is also what we used
in the RLHF Speaks Many Languages.
And we showed that this is very impactful.
And because it's online,
it does beat things like DPO, which is offline.
So RLU is still an RL method,
but what it's really saying is that
we are in a well-conditioned search space.
And because of that,
we can be a lot more nimble about how we explore it.
Yeah, well, in the RLHF on many languages one,
because obviously you've had this huge focus
on multilingual.
And I suppose that there's the problem
of getting diverse data,
because this is super heterogeneous data
when we're doing multilingual language training.
And of course, even the preference completions,
they needed to be generated as well.
And I think you generated some with translations
and then you had a strong model
and you had a set up there.
Can you tell us about that?
That's fun, because it's part of this wider issue
where multilingual relies a lot traditionally on translations.
You don't have data,
so you translate your good English data,
your gold standard data,
or your good Mandarin Chinese data
into many different languages.
Here is where it gets interesting.
Translation models typically have
what we call translation ease.
There's these weird artifacts that pop up.
So you might have like auto enumeration,
where instead of like the one, two, three,
it spells out one, two, three.
So it's just, and it's very annoying
for people who have to experience it
because it gets imparted to the model,
the downstream model.
So we did something which I think is very fun
with this paper where we said,
well, the whole goal of RLHF is to steer away
from certain parts of the distribution,
steer towards other parts.
And so what we did for our preference pairs,
so let's think about the normal way preference pairs are done.
It's quite expensive and time intensive.
You have to go get annotators
and you're asking humans, which one do you prefer?
We did this really fun, I would say, trick here
where we said, well, we know we have translation pairs.
We generate synthetic pair, the other pair,
with what is a very high-performance model.
In this case, we use Command R+,
which is super-performance,
does very well in many different languages.
And then we compare the two
and we ask an LLM as a judge,
which is better, the translated English
or the sampled in the other language.
And what we found was this actually helped
with translation artifacts
because it steered the model away from the bad translated
and towards the more versatile, fluid,
Command R+, generation.
So really interesting.
And there were some percentage of time
where the translated was better.
And so you got that nuance too.
So very, very interesting.
Yeah, amazing.
And then that removed a lot of the translation artifacts.
Yeah.
Amazing.
Sarah, this has been incredible.
Where would you like to point people to as well
for your later stuff?
Feel free.
So, you know, I lead Co-Here4AI.
So it's a research that we do a lot of fundamental research
and we, a lot of my work is on efficiency,
reliability and building these models
that scale the next generation models.
So you can go to Co-Here4AI
and take a look at some of our work
and just a lovely being here again.
It's really nice catching up.
Amazing, Sarah.
Thank you so much.
Yeah, thank you.

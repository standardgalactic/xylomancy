He moved to Indiana University in 1989,
where he obtained his PhD in philosophy
and cognitive science,
working for the legendary Douglas Hofstadter.
By the way, I've got his book here
in the Center for Research on Concepts and Cognition.
And Douglas, by the way,
is one of the most legendary figures in the AI space.
We also had the pleasure of interviewing
one of his other PhD students, Professor Melanie Mitchell.
Now, David recently wrote this fascinating book
called Reality Plus.
And in that book, he discussed
the three central philosophical questions, actually.
The reality question, which is to say,
are virtual worlds real?
His answer to that is yes.
The knowledge question, which is to say,
can we know whether or not we're in a virtual world?
His answer to that is no.
And also the value question, which is to say,
can you lead a good life in a virtual world?
And his answer to that is a resounding yes.
Now, you probably also heard of this notion
of an extended mind, something which David formulated
with Professor Andy Clark.
And they described the idea as active externalism
based on the active role of the environment
in driving cognitive processes.
Or put simply, you might think of your phone
as being an extension of your mind, for example.
What was it like to work with Douglas during your PhD?
Oh, he was great.
He was so, he is interested in so many things.
It was officially, it was an AI lab.
And most of the people there were AI researchers.
You mentioned Melanie Mitchell.
She was my colleague there.
Bob French, who's done important work on AI.
Gary McGraw, Jim Marshall, Montpay, and others.
I was the only philosopher.
People were interested in so many things,
whether it was, we had workshops on humor,
or creativity, on mathematics, on politics,
on everything, as well as all the stuff on AI,
analogy, concepts.
It was also a very exciting time to be there
because this was one of the,
this was the, at least one of the,
in the boom and bust cycle of machine learning
and of neural networks,
this was one of the boom periods, the early 1990s.
The PDP books had just come out,
parallel distributed processing
by Rumble Hart, McClelland, and Hinton.
And there was so much excitement
about the capacities of neural network.
So I ended up doing, writing a few papers
on machine learning back then.
One was on the evolution of learning systems.
One was on getting machines to learn structural generalizations.
And so, and all with fairly basic neural networks.
I mean, a few years after that,
the bottom fell out of neural networks
for another 15 years or so,
but this was a very exciting period to be there.
One thing about Doug that you don't quite get from his books,
his books, he's so enthusiastic about everything.
He's not into all these ideas and so on,
but it turns out there's 5% of things in the world,
or 1% that he's enthusiastic about.
The other 99% he hates.
He's like, he liked certain approaches to AI he loved,
but even neural networks, he was like, yeah.
He was not a big fan of most research
and neural networks back then.
He said, I think it was a, this is a bandwagon,
or even he said a bad wagon.
Likewise for philosophy.
There's bits of philosophy he loves.
There's a lot of it he doesn't like.
So maybe in person,
you get more of that very opinionated side,
but really he's just,
that book, Go to Lesha Bark,
was just, I mean, it was what drew me into philosophy
and AI as a teenager, and it's so rich.
And you go back to that book,
there's still so many ideas there.
The Mind's Eye is another book he edited with Dan Dennett.
It was reading the Mind's Eye
that really first got me thinking about issues
about consciousness and the mind-body problem.
So yeah, he's a very rich thinker.
Well, thank God for Douglas Hofstadter.
Yeah, I love the Mind's Eye.
It has one of the, one of the eeriest stories of all,
what was it, the riddle of the universe and its solution
where there's some image or some text that if you read it,
it causes your brain basically to core dump.
And so it becomes this infectious thing
where anybody who goes into that office
and reads that thing, they just crash
and they never, they go into a coma
and they never wake up again.
And so, you know, there's a huge investigation
to figure out what's going on
and more and more people keep going into comas, you know,
because of this crazy ideas in that book.
Could I just pick up on this point?
Because you were saying that Douglas Hofstadter
turned his nose up at neural networks
and I hope he still would actually
and Melanie Mitchell certainly does.
And I mean, Douglas had that paper
out called The Core of Cognition.
He was always huge about this idea
of the primacy of analogy making.
And actually, there's a few researchers today
that are mirroring that idea that Francois Choulet,
for example, he says,
intelligence is literally sensitivity
to abstract analogies.
It's not memorizing the internet.
So I think, thank God that we do have people out there
thinking slightly differently
because the modus operandi today is that
we need to build a big hash table of everything.
And even the way we formalize intelligence
as we were just saying previously,
we're not really paying attention to why
or how the things do what they do.
We're just looking at the behavioral output
a bit like the Turing test.
As long as it looks and smells like a duck,
then it must be a duck.
So thank God for Douglas Hofstadter.
Yeah, I think he's always been at the same time
an enthusiast about the possibility of AI
while being somewhat skeptical
about the capacities of existing AI
and about the kind of hype that suggests
that AI might just be 10 or 20 years around the corner.
So in the early 90s, I think, yeah,
that was especially natural.
Back around then, people would say,
a year spent working in AI is enough
to make you believe in God because it was so hard
to get anything even like any kind of intelligence
out of an AI system.
And I think Doug was equally skeptical
of both the symbolic and the connectionist
on neural network approaches back then.
I think ultimately his sympathies lay with it
on the neural network side of things
of the idea that intelligence could, in principle,
bubble up from a million,
from 100 billion separate little interactions,
intelligence could bubble up from there.
But I still think he'd be inclined to think
that current approaches are too statistical,
too simple, and so on.
That said, you have to look at the progress
in machine learning over the last 10 years
and it's been amazing and surprising.
And I think even somebody like,
even people like Melanie or like Doug
are gonna have to say this has been something
they did not expect and that they did not predict.
So I actually, I was back in Indiana
just over two years ago,
just before the pandemic got going, February 2020.
And I don't know, maybe that was before GBT-3,
but still there'd been all these amazing developments
in machine learning over the last few years.
I asked Doug about this, what do you make of this?
And of course, he's on the record of saying,
there will be AI eventually,
but it's gonna have to involve all these new kinds
of complexity, not something simple like this.
And he says, yeah, well, this is troubling.
This is concerning.
It could be, I don't know yet,
but it could be that I was wrong.
It could be there are simpler paths to AI.
And his attitude was that would be very disappointing.
It turned out that you could actually train up an AGI
just using those simple methods to human levels.
That would make, I think Doug's view was
that would make kind of human level intelligence
less grand and remarkable than he had thought.
Well, actually, back in Goethe-lesha Bach,
I think he said that even to have a machine
that could beat a human at chess,
it would have to be good at everything.
It would be like a good composer.
It could tell jokes and so on.
Okay, that one, that view got ruled out back in the 90s.
So the question is, is this ever-growing progress
of the kind of machine learning that says,
just throw a whole lot of compute
and a whole lot of data at it and see what happens?
Is that eventually gonna get us to human level intelligence?
Or is it just gonna get us so far with,
and there's gonna be principled limitations?
I've always been on the side of,
they'll probably only get us so far,
but I have to say those principled limitations,
those obstacles that have not yet been conquered
are getting smaller and smaller and the progress,
if the progress of the last five or 10 years
continues for another five or 10 years,
then who's to say what's gonna be left?
Yeah, there was a fascinating anecdote in Melanie's book
about how she and Hofstadter went
to the Googleplex one time.
And basically, as you said,
Douglas was terrified that intelligence
might be disappointingly simple to mechanize
because he felt of the mind of Chopin
as being infinitely nuanced.
And just the incredible process
that must have gone through his mind
when he produced his music.
But I wanted to, and just quickly, by the way,
you said that there was a conception in the 70s
that task-specific skill was what was required
for intelligence or a collection of specific skills.
And now the mindset is much more towards
task acquisition, efficiency, and generalization.
But I wanted to just quickly pick you up
on the so-called intelligence explosion question.
So this is a subject which Nick Bostrom
has popularized after his book, Superintelligence.
Personally speaking, we're not particularly sympathetic
to this view.
And Saigre Francois-Labe, he said in a blog post recently
that this line of reasoning
represents a misunderstanding of intelligence.
He said that in his opinion, intelligence is situational.
He said that our environment puts a hard limit
on our individual intelligences.
He said that most of our intelligence is not in the brain.
It's externalized as civilization.
And that an individual brain
cannot implement recursive intelligence augmentation
like a Godel machine.
He also said that there are already many examples
of recursively self-improving systems.
Even personal investing, for example,
is a recursively self-improving system.
The more money you have, the more money you make.
Anyway, so Bostrom described a thought experiment in 2003.
I'm sure you've heard of this.
The scenario describes an artificial,
like a very advanced artificial intelligence task
with manufacturing paper clips.
If such a machine were not programmed
to value human life,
then given enough power over its environment,
it would try to turn all the matter in the universe,
including human beings into paper clips
or machines which could manufacture paper clips.
Do you think we might be on the precipice
of being turned into paper clips,
as Bostrom famously described in his thought experiment?
Yeah, look, there's two different issues here.
One is, will we get to some kind of much greater
than human superintelligence relatively soon
by some kind of intelligence explosion process?
And second, if that happens,
are there major dangers around?
Yeah, I wrote about both of these things back in 2009.
I had an article called, yeah,
The Singularity of Philosophical Analysis,
where I tried to take this line of reasoning
for an intelligence explosion through recursive,
basically through recursive design
of ever more sophisticated AIs.
I tried to take that and turn it into an argument.
I mean, the classic statement of this comes from I.J. Good,
the statistician and philosopher back in 1965
on the design of an ultra-intelligent machine
where he puts the basic idea right there,
that once you've got a machine which is smarter than a human,
it will be able to design a machine which is smarter still,
and then you're gonna get a recursive,
runaway explosion of intelligence.
I tried to analyze that, to set out that article,
that argument in as much detail as I could.
Analyze where it could go right, where it could go wrong,
what the possible obstacles would be,
and it's a long story.
If anyone wants to look it up, it's out there on my website.
But I, in the end, became convinced
this is a pretty powerful argument.
There's only so many ways it could go wrong.
I think it's important that not every recursive
augmentation process is gonna lead
to an intelligence explosion.
It could easily bottom out,
could asymptote the four human intelligence.
But I do think that once we start
from greater than human intelligence,
we have to find some way to get to greater
than human intelligence first.
This explosion won't get you that.
But once you get there, then there's pretty good reason
to think things, in principle, can take off from there.
If intelligence is extended,
I'm a big fan of the idea that intelligence
is extended into the environment,
but as far as I can tell,
all that can, in principle, be augmented, too.
We develop extended systems, which are smarter than humans,
and they can, then they'll be able
to design even better extended systems.
And we could then have an intelligence explosion
of extended intelligences.
So I'm actually, nothing about this
gets you to human-level intelligence,
but once we get to human-level intelligence
and a little bit beyond,
then I think there's a pretty good case
that there's some kind of potential explosion in the offing.
Then the other issue, you mentioned Bostrom
and the paper clips, is, yeah,
what does this mean for the future of humanity?
I guess, I don't know what I'd say about the probabilities,
but I'd say, yeah, once you have greater than human,
artificial, general intelligence,
then there's many ways that can go wrong
for the obvious reasons that, yeah,
such a being is gonna be extremely, extremely powerful.
The most intelligent beings in the universe
tend to be the most powerful for obvious reasons.
Whatever they want, they have the capacity to get.
So it's gonna be extremely important
that our AGI systems want the right things.
That is, they have the right kind of goals,
or as people put it, these days,
that they are aligned with human goals.
Because if they're even a little bit misaligned,
then there's gonna be the capacity
for things to go very badly wrong.
I know there are some people who think
that the alignment is gonna have to be so precise,
that, you know, missed by just the tiniest bit
and will destroy the universe,
whereas others think it's extremely robust.
It may be more robust than that.
I'm not totally sure about that,
but I'm certainly on the side of people
who think we have to take this issue extremely seriously,
and there is at least potential existential risks here
that if AGI is produced in an unthinking way,
perhaps say in a military or a financial context,
where there's a AI arms race,
and we suddenly have greater than human AIs
that can achieve arbitrary goals,
then, yeah, suddenly becomes an extremely sensitive matter
what their goals are.
So I'm certainly on Bostrom's side when it comes to,
yeah, this is something we should take seriously.
But isn't there a bit of, you know,
it's a big distance to go from superior to human intelligence
and achieve anything you want.
I mean, I'm relatively intelligent,
but I can't achieve flight, you know, by myself,
without, you know, apparatus to do that,
an airplane, wings, whatever.
I mean, there are physical limitations in the world,
and I think sometimes there's this assumption
that intelligence can kind of go to infinity,
where in fact, maybe intelligence itself
kind of bottoms out at IQ 1000 or something,
which is not much, you know,
you can do beyond that certain IQ.
I mean, isn't there a degree of kind of speculative,
you know, extrapolation that we need to account for there?
I would say this is certainly one way
that things could, that the argument could fail,
is if it turns out that basically there are diminishing,
there's some kind of intelligence ceiling,
and there's some kind of diminishing returns towards this.
Just there is such a ceiling that we might find
when we make a being which is 10% smarter than us
on some scale, it could only make a being
which is 5% smarter than it,
and that being will make a machine,
make a being which is only 2.5% smarter than it,
and all of this will kind of asymptote
to some intelligence ceiling,
and I don't know, this turns on very subtle issues
about the structure of intelligence space.
I'm rather doubtful there is such an intelligence ceiling,
or if there is one, maybe it's something like,
you know, the limits of computability
compared to, you know, hypercomputation
that an infinite system could do,
but I think that ceiling is so high
that there's room for an awful lot of superintelligence
before we get there.
But in any case, I would say that, you know,
for the purposes of, say, caution and thinking
about the future, I would just turn the point back on you
and say that the thought that there is such
an intelligence ceiling is itself
an extremely speculative one.
I wouldn't want to rely on this,
on this extremely speculative thought
to kind of protect us from the, you know,
potential risks of AGI in the future.
If there's only a 20% chance
there's not such an intelligence ceiling,
then this is something that we very much need
to be worrying about.
Yeah, I mean, fair enough, it's certainly a risk factor.
It's certainly something that we need to keep a handle on.
Well, let me ask you about one specific tying there.
So I think you're probably familiar
with Carl Friston and, you know, his free energy principle.
And he sends his regards, by the way.
We talked to him a couple of weeks back.
And he wanted to ask you about kind of one line
of thinking that he's been exploring lately.
And I want to give you a quote from his 2018 article,
Am I Self-Conscious?
Or does self-organization entail self-consciousness?
And he said, the proposal on offer here
is that the mind comes into being
when self-evidencing has a temporal thickness
or counterfactual depth,
which grounds inferences about the consequences of my action.
On this view, consciousness is nothing more
than the inference about my future,
namely self-evidencing consequences of what I could do.
What do you think about that perspective?
Yeah, I'd have to know more about the connection
to consciousness.
I know that, yeah, Friston has developed very deeply
the idea of the mind as a prediction machine,
a mind which is basically set up to, you know,
predict whatever signal is coming next.
And with that one basic tealos, you know,
predict what's next, what's next, what's next,
then you get to build these amazing models of the world
with all of these capacities.
And that's a really interesting perspective
thinking about the mind and intelligence in general.
And it's got to be at least one huge part of the story,
even if it's not the whole story as Friston thinks it is.
But I've never really understood the distinct,
what this kind of predictive approach has to say
distinctively about consciousness.
You know, because presumably there's a whole lot
of different predictive processes
at all kinds of levels of the hierarchy,
including at the very early vision
and very late cognition,
and the whole mind is engaged in coming up
with these predictions,
but only some limited part of it is conscious.
What you just said about, yeah,
trying to figure out the predictions consequent
on our actions sounds to me like a very general statement
of what the predictive approach says
about the mind in general.
And I haven't yet heard,
what is the part that corresponds to consciousness?
Why, for example,
or some representations get to be conscious
where so much of it in the brain is not?
I can give you a bit more detail, which may be helpful
because we did dig into him on a bit.
And he said, for one thing,
he expected that perhaps part of your response
might entail or talk about the meta-hard problem.
You know, why is it that certain beings,
i.e. things like philosophers and people like you and me
puzzle so much about our qualitative experience?
And the argument he makes there,
he says that if we are inference machines
that are built to actively self-evidence,
then that necessarily entails we need to have
a generative model about our experienced world.
And if we have that generative model
about our experience world,
then we have to entertain the hypothesis
that we are things having a qualitative experience
along with the alternate to that hypothesis,
which is that we're not having qualitative experiences.
And so essentially that the capability
to model the world generatively really requires
that we entertain this hypothesis
that we're actually having qualitative experiences
or maybe not.
And that's why we pontificate about it.
Yeah, that's interesting.
And I think the meta-problem is a promising approach.
So the meta-problem is why do we say and think
the things we do about consciousness?
Instead of explaining consciousness directly,
let's explain our internal model of consciousness.
And yeah, there's got to be such a model.
So I think this is a promising approach to take.
I still don't fully, I mean,
I think if you take the predictive approach though,
what you would expect is the system would have
many different models,
a big complex model of the world at all levels.
It doesn't just correspond to experienced reality,
but the model's the world way beyond what's experienced.
It would also, you'd also expect the model
to have a model of the mind,
to have a model of ourselves and our relation to the world.
But what actually happens in the human mind
is we have models at all levels.
There's like so many different levels
of say of representation, even in the visual hierarchy.
And somehow though,
only one of those levels seems to correspond to consciousness.
So the question is why now do we need a distinctive model
of those representations in us,
which correspond to conscious experience?
One idea I think, one idea I quite like is that,
this could be like a simplification.
In fact, we have millions of layers
of representation of the world.
But to build all that into our model of ourselves
and our relation of the world is going to be too complex.
So we basically, we oversimplify by saying,
ah, there's this one special relationship we have to the world.
We call it consciousness or experience.
And yeah, we experience certain things
and then we use them to reason about them.
And this is massively oversimplified as a model of the mind.
But it could be that that simplification
is then what actually gives us the sense
that we have this special thing called consciousness.
At least maybe that could explain why it seems to us
that we have some special representations of the world.
So further question, why those conscious representations
should seem to be so ineffable and subjective
and hard to explain in what Carl has written about this.
I think he and Andy Clark had some ideas
about the meta problem to try and push on this.
Maybe there'd be certain representations
that we'd have to be especially certain that we have them.
Maybe that would give rise to Descartes' idea
I'm not sure about the world, but I know that I'm thinking.
I think, therefore, I am.
And they had some kind of story about how this could get the whole
I think, therefore, I am, certainty in one's own mind going.
Anyway, I think it's an interesting approach.
And I'd be very cool to see if they can develop it further.
Fascinating. I wanted to dig into this modeling thing.
I was even thinking a second ago when you were talking about intelligence,
that straight away you did the Hutter thing
and we're talking about agents performing in environments and so on.
And even that is a model.
And of course, we're talking about complex phenomena
and the way we model things depends on the level of analysis.
But I'm really fascinated by this idea that some phenomena is so complex
that it cannot be formalized or communicated,
almost as if there's a representation problem.
Now, you discussed in your consciousness book
whether consciousness itself could be reductively explained.
And your knowledge argument, you spoke of this neuroscientist Mary
that had been brought up in a black and white room.
She's never seen any colors except for black and white and shades of gray.
She's nevertheless one of the world's leading neuroscientists
specializing in neurophysiology of color vision.
She knows everything there is to know about neural processes
involved in visual information processing,
about the physics of optical processes,
about the physical makeup of objects in the environment.
But she doesn't know what it's like to see red.
No amount of reasoning from physical facts alone will give her this knowledge.
Physical facts about systems do not tell us
what their conscious experiences are like.
Now, you're speaking about this phenomenon
in respect of the conscious or the phenomenological experience.
But I think it's a much bigger problem of representation
with any complex system, right?
So what I find fascinating is that all of us have a conscious experience,
but it's completely ineffable, as you just said.
It's impossible for us to communicate it to others.
And whenever we try to do so, we're reaching, right?
Just like the blind men in the elephant,
we end up defining some weird abstract motif, right?
Chopping off 90% of the truth.
The thing that fascinates me is that we need to have some kind of formalism
or reduction in order to communicate,
you know, in order to know or even understand anything.
But so often is the case that all of the nuance and richness
of the phenomena is lost in doing so.
So I suspect that any formalism of a complex system
might blind us from discovering a much better and richer formalism later,
because it kind of frames our thinking in quite a pernicious way.
In your book, so as I said, you were trying to separate
the phenomenological experience as something that couldn't be described.
But do you think it could be extended to any complex system?
Well, we don't...
As far as we know, you know, some complex systems actually have
conscious subjective experience, but, you know, most of them don't.
You know, this Mac on my...
that I'm using right now is a very complex system,
but not much reason to think that it's conscious,
despite the complexity of what's going on within it.
So certain kinds of complexity go along with consciousness.
But if we were to kind of return to that meta-problem approach for a moment,
maybe there are certain kinds of properties of a complex system
that tend to produce reports, for example, that the system is conscious.
So maybe some complex systems have the capacity
for a certain kind of direct self-modeling that corresponds to,
you know, what we think of as introspection.
We have introspection, which is a way of saying,
this is what I'm perceiving right now.
This is what I'm thinking right now.
This is what I'm feeling right now.
And we build a model of ourselves,
and it may well be that that model is highly oversimplified.
You don't have access to all these facts about ourselves.
So perhaps you could tell a story where the kinds of complex systems
that give rise at least to this capacity for introspection
are then at least going to report themselves as being conscious.
And maybe that could get at some element,
maybe sort of the ineffability of consciousness.
You'd expect to build these very simplified self-models.
We wouldn't know immediately how to extend to other people.
I mean, I still think, in principle,
you could take Mary, who knows all about the human brain,
and she could come to know all about those models in other people.
But it still seems that she's never actually experienced red for herself.
There's still something really crucial about this objective experience
that she doesn't know.
She doesn't know what it's like to experience red.
And knowing all about the details of the model
still hasn't told her that.
So I think that's still something that needs explaining.
Some people at this point just say,
that sense of something extra is an illusion.
Something extra that the model hasn't explained is an illusion.
But that's really where a lot of the action is at then.
Just quickly, do you think there could be a sense of something extra
to intelligence as well as consciousness?
Probably, yeah, we model our own intelligence
with massively oversimplified self-models
that were programmed into us by nature,
that model us as these agents with incredible capacities,
free will, rationality, reason.
It probably, again, it will, yeah, maybe,
I talked about consciousness is involving the subjective elements,
intelligence is involving the objective elements.
But yeah, we probably have oversimplified models
of those conscious of those behavioral elements as well,
perhaps that make us out to be more rational,
or more free, or more capable than we actually are.
I wanted to ask, what is an interesting simulation?
Is our universe interesting or not?
Because we represent just a pinprick of intelligence.
So should intelligence be more spread out
in the eyes of the simulator?
Or in the vast majority of instances,
would there just be gas everywhere or a singularity?
Maybe stars can't form.
Maybe the interesting phenomena itself
is on the boundary between chaos and order,
or between order and disorder, I should say,
which is just a tiny sliver.
So what do you think makes an interesting simulation?
I don't know, I think it probably depends on your perspective,
and it might, for example, depend on the perspective
of the simulators, what they're after.
One thing that a simulator might be doing
is just create a whole lot of different universes
with different potential, say, laws of physics
that they're simulating, just to see what happens.
And maybe if they're interested in, say, life or intelligence,
then it could be that they're going to find that, OK, well,
99% of these simulations don't produce anything
like life or intelligence.
And yeah, 1% of them produce life, and 0.01% lead to intelligence.
So if that's what they're interested in studying, fantastic.
But they might be interested in who's to say,
you know, the laws of physics or galaxy formation,
more generally, totally independent of life and intelligence.
So I don't think there's any single standard
of what's interesting.
I mean, to me, as a philosopher interested in consciousness,
I'm especially interested in this question
of what kinds of simulations might actually develop
conscious beings within them, not least because that's
going to be especially relevant to our situation.
If we're in a simulation, it seems we're conscious.
So there's a question about just how this kind of simulation
might get set up.
But I think this whole, I mean, simulations
are used in actual practice for a million different purposes
by scientists studying this phenomenon or that phenomenon,
by people doing entertainment, by people doing prediction
of the future, by people doing simulation of the past.
And I guess when it comes to simulated universes,
all of those sources of interest may themselves be present.

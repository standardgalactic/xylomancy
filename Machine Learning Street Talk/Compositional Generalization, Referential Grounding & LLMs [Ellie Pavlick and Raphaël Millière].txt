Welcome to MLS T. Today we are extremely excited to have two distinguished guests join us.
The first guest is Ellie Pavlik. She is an assistant professor of computer science at Brown University
and a research scientist at Google AI.
Her work focuses on building better computational models of natural language semantics and pragmatics,
aiming to help computers understand language the way humans do.
Our second guest, the legendary Raphael Millier, is the 2020 Robert Abert Presidential Scholar
in Science and Neuroscience in the Center for Science and Society,
and a lecturer in the Philosophy Department at Columbia University.
Raphael completed his D-Phil in Philosophy from the University of Oxford,
where his work centered on self-consciousness and his main interest lie in the philosophy of artificial intelligence,
cognitive science and the mind.
Now, this is an interesting experiment for us.
We've decided to get these two heavy weights, just one-on-one having a conversation with each other
and we're hosting it here on MLS T.
They spoke about compositionality and grounding,
compositional generalization benchmarks,
mechanistic understanding in language models,
variable binding in transformers,
language and vision models,
compositional behavior in humans,
compositional reasoning and negation in language models,
variable binding in reinforcement learning and transformers,
the difference between instruction tuning and RLHF
and the benefits of RLHF,
referential grounding and language models.
And yeah, the Chomsky skepticism, of course,
you know our friend Stephen Piantodosi's paper,
inductive biases and language learning,
language models in different languages
and indeed the future of academic work in language models.
Now, the audio from Raphael in particular wasn't as good as it could be.
I've done my very best to process it and improve it.
Just to help folks follow along,
I've kind of generated some subtitles.
The subtitles on Google are absolutely rubbish
and speech recognition technologies come along so far in the last few years.
So, I've generated some better subtitles using another service
and I've superimposed it on the top.
I've also superimposed some descriptive titles on the top
just to help you folks follow along at home.
So, anyway, without any further ado,
I give you Ellie Pavlik and Raphael Piliere.
Enjoy.
Hi, Ellie.
I know Raphael. Hi, how's it going?
I guess you talked to Tim before,
so maybe you have a kind of more of a context of the previous conversation
that started this one.
So, I'll let you decide where we're beginning,
what the first topic is.
Yes, so we had a chat back a few weeks or half a year ago
when I came on the podcast and we had to get it short.
And you thought maybe, you know,
if I come back home, we should do it as a discussion.
And so, we had to chat with you.
So, Tim thought that maybe some of the topics we could discuss
would be compositionality and branding,
since we're both very interested in that.
Seems natural, yeah.
Yeah.
So, I don't know whether we should try to,
to disagree more than we actually do
because I think we wear a line of a lot of these topics,
but, you know, I'm sure there are some topics.
Yeah, but I mean, I also feel like for both of them,
I'm like, I know what I currently think,
but I also am pretty prepared to just have to renege in a couple of years
and be like, I was wrong.
So, I feel like we can see both sides.
So, we can definitely simulate some disagree or we can,
we don't have to disagree,
but we can argue both sides of whichever issue.
I don't know, where do you want to start?
Do you want to talk grounding or do you want to talk compositionality?
I think I have more immediate questions with respect to compositionality.
So, one thing I was wondering is,
how do you see progress on compositional generalization benchmarks?
So, just, you know, for the listener of yours,
what it is really hard to assess whether large language models
trained on language, on huge corpora actually acquire the capacity
to generalize compositionally properly
because you can never really know what's in the training data
with your models like before.
You can never really know whether they're the memorized structures and so on.
So, what strategies to use synthetic data sets that's such that,
you know, you train them on a test set and then when you test them,
the only way that they can achieve this goal is generalizing perfectly.
And so, there has been some,
the initial results from some of these data sets and benchmarks
like Scarns, Cards, and others were a little bit mixed with STMs and formers
and lately they have been a lot of the steady improvements
partially due to tweaks in the architecture.
And I remember having a discussion with Tao Linzen
when he organized this competition workshop that he was making out.
And one of the, one of the complex responses
whether we could call these tweaks
or whether these are significant changes.
That's always the crux of the debate, I think,
including also with things like post-malentity, post-malentity approach
that has, you know, explicit transfer product presentations
is how much of a hand engineer tweak you make in the architecture
to solve this completion of generalization problem
and how much do we need.
So, I was just wondering where you step on that.
That's a super interesting.
I mean, so, yeah, so on the compositional generalization test,
I mean, I probably have like an unsatisfying middle ground opinion.
I'm curious where your thoughts are.
So I think, I think we're both pretty interested in the kind of mechanistic stuff right now.
So I guess for the audience,
this is this idea that like this idea of trying to kind of understand
what the models are doing kind of under the hood.
So when we think about the, I guess, compositional generalization task,
it's like we have some inputs, what are the training,
like what is the training model it gets,
and then what are the outputs that it produces.
And that's really the data that we're basing our claim
about whether it's compositional or not on.
And the kind of mechanistic or the other approaches
to try to like characterize what actually is the process
that used to get from the inputs, the outputs.
I really like Chris Ola, who I think coined the term mechanistic,
uses the phrase of kind of trying to understand the source code of the model.
So it's like you have, you want that kind of something,
like a kind of human understandable description of the algorithm
that it's running under the hood.
And so like I've just, I've been super hung up on that.
Like I think all of my projects, all of like what my students are working on
are some flavor of that because to me, I think the reason it's so,
I think it's interesting, but it also just feels like the questions
about things like compositionality are almost stuck right now
without that level of description.
So if we're just looking at what are the inputs and what are the outputs,
I just feel like we're, it's just going around in circles with people,
like we're not really making progress on the issue.
There's kind of people who are inclined to agree and inclined to disagree.
And so from my perspective, like, okay, so if we have the model
that's doing this kind of, I guess, quasi generalization,
it's like succeeding on some cases, it's not perfectly compositionally
generalizing in the kind of really abstract case that those data sets
tend to be going for.
But it's doing something in between and we're trying to figure out
whether that counts as compositional or not.
It seems like that just hinges on what it's actually doing
under the hood and how it's doing it.
So I guess I'm like, I'm like basically like neutral on or not paying attention
to compositional generalization data sets right now.
And I recognize like a very fair criticism.
And I feel like when I've talked to people like tall probably have this
where it's like you can't just sidestep the issue.
It feels like being overly generous to the neural networks, right?
It's like they're not doing that well.
And then you like change the game a little bit, right?
You're like, oh, well, that's not even the metric we care about.
That's not really what I would see as the goal.
It's just like in the immediate term, it seems like first we want to characterize
what's happening under the hood and then we can come back to those data sets
and understand them much more in depth.
And then when we see how they're solving it or not solving it, we can like,
it gives us a much more concrete thing to analyze and try to ask whether
that counts as compositional or whether that's at all human like,
if that's what we care about.
Like, I guess to me it feels like a dead end if we're not allowed to comment
on the procedure that happened in between and right now we can't comment
on the procedure that happened in between inputs and outputs
because we just don't know what's happening there.
So I guess that's my current take on compositional generalization data sets
as I'm like, now is not the right time for them.
We'll come back to them later, which I recognize seems like a dodge,
but it's not meant as a dodge.
It's like, basically, we'll come back to this later.
Yeah, but I'm curious what you think about them.
Yeah, I mean, I'm very sympathetic to that view.
And as you know, I'm super interested in the mechanistic visibility,
just to looking at compositionality.
And I guess in the background, there is lurks this debate about whether humans
themselves have something like a perfectly compositional language of thoughts
or something else, right?
Absolutely.
And so perhaps, you know, we might learn some things about
computations implementing human cognition by looking at these
imperfect computational systems.
Absolutely.
I mean, I think that would be really my hope.
We just like so right now, and this is a gross oversimplification
of where the two viewpoints are.
But like the two really concrete, like, kind of options on the table
for like what a system can be is like this pure symbolic language of
thought and that the language of thought, it would be something like
humans in their heads have something like a Python program language,
like a perfect kind of formal system for reasoning over symbols
in this compositional way.
Or it's like this loose, or like these associations, these idiomatic
kind of just things back together.
And like, I think most people would assume or something in between
and there's not a really good proposal for what the in between is,
right?
Like there's all kinds of ways of being in between those two things.
And so I feel like whatever we find in the neural networks gives us
some kind of concrete proposal.
Like here's an example of an in between.
It's not the language of thought thing.
They're often also obviously not just giant lookup tables, right?
They're doing something more than that.
So whatever they're doing in between, it's at least like a candidate
for what could be happening, right?
That's pretty exciting.
And it might not be the right one, but at least it's something,
because I haven't seen like a really satisfying candidate for what the
in between is, right?
Like a lot of the debate still seems to kind of put these two strawmen up
against each other.
Yeah, I agree.
And I think some, even some of the people who are actually back against
this kind of really symbolic thing, but the thoughts or culture that
Paul Smolensky, the still faith that you need to build into your
connections model, some more specific compositional structure with these
transfer part of it.
But these, these factors, role and field of vectors that are neurofuminal,
that you keep combined with transfer part of the operations.
And there are other vector symbolic architectures like this that do it
like that.
And Mike Wong is always with, with this and I think I have to talk
about this.
I think, you know, he's open to the possibility that perhaps transformers
handling compositionality in the way transformers handle it might turn out
to be enough perhaps.
But my problem is that if you, if you build, you know, if you kind of
hard code into your network architecture, or you do some future
engineering to have the input vectors being neurofuminal, that seems just
to be ad hoc to me, right?
So you need to specify what are the roles, what are the fillers.
And that's, that seems like an, an inclusible model for how
cognition works, unless you want to say that they are, he makes
academic concepts and we just, you know, I mean, a lot of people do,
right?
That's like a claim that I think a large fraction of cognitive science is
very happy to say is the case is that there's an inventory of innate
atomic concepts.
Right.
But, but yes, I agree.
I think maybe it's the computer scientist or something, something about
that is unsatisfying.
Or at least you want a story of where those came from, which still
seems like you somehow it needs to emerge or come from data or come
from some kind of pressure other than just we got lucky and they were
there to change the right inventory of concepts.
I mean, there's some daylight between the core knowledge view that,
you know, there are some, some basic concepts and the four orient
view that all atomic concepts must be in it because right.
Which is born with this virtual and atomic concepts and they're
all down the board.
Right.
And it seems that if you, if you want to handle all compositionality
with these ad hoc world and filler vectors or something along these
lines, then it seems that you're going in that direction.
And what I'm excited about with Transformers is that it looks like
they can do something that in my mind, if you look at the mechanism,
you can see that the particular literature looks quite like
variable binding by reading and writing information to sub spaces
of the main embedding space and using that as a virtual content
principle memory.
But it's fuzzy, right?
It's not just implementing a classical symbolic architecture.
That's always the charge that I think the, the, the class sits on,
you know, never against the connection is that, oh, if your,
if your model is working well, and if it's, if it's a good model of
human behavior, then Prisma will be just implementing classical
symbolic architecture.
Right.
Right.
Right.
Right.
Right.
I mean, almost, I, I'm teaching a class this spring on language
processing of humans and machines and we were talking about this
question of implementing a classical architecture.
And it's actually quite hard to tell even in the historic debates
on this, like what, what people are even claiming about this.
Right.
Like there's like simultaneously claims that like, oh, it'll like
sure it can implement it, but also nothing it ever does, whatever
count as an implementation.
Like I just can't even tell what the, what actually the consensus is,
but I agree.
I think we'll get something that the ideal is that when we look
into the transformers, we'll find something that preserves the
really necessary pieces of it, but is different enough to be
interesting.
I mean, I don't know.
I guess it, in some ways, if what we did, if what we ended up
finding was something that was identical to the classical
architecture, that would be like, I think a huge win for a
classical architecture people, right, that the only way these
transformers were able to solve it was by learning to implement
the thing they said that you would need all along.
But I think it's unlikely that we would find that.
Right.
Like I don't really know exactly how that would even work.
So we'll get something like this fuzzy version, which, which I
think would just be fascinating.
I mean, I'm curious.
So the variable binding stuff.
So I'm generally take the position that the transformers,
these other logical networks will be able to implement like
these core kind of symbolic operations will be able to
replicate this kind of behavior.
And then when we kind of dig under the hood, we'll be able to
find these clever implementations of these things.
But variable binding is one of these things where we've
actually really struggled to find good evidence of it.
And I guess particularly, so in language models, it almost
seems like you have to say it's there by, like by Fiat,
because of some of these, the ability of the models to do
this compositional generalization stuff.
Like they, they, I think Tom McCoy had a really cool paper
where I was showing that they could generate some like
syntactic structures that were unattested in training.
So for something like that, you're like, well, I guess to do
that, you would have to have something like a abstract
filler and role binding.
But especially when we've played around the language and
vision models, and maybe it's just that clip kind of sucks.
I guess clip is the one of the pre-trained link image and
text models that kind of is trained to map images to,
to text captions.
But we've had several projects of trying to show,
I guess clip and then the image generation models based on
top of it, like Dali and stuff, things like that.
Like we just can't get any evidence that it's doing anything
clever or abstracting away from the, from the structure.
So that's where we'll look for stuff like a red cube in
front of a blue cube and then blue cube in front of a red
cube or things like the, I guess the famous on Twitter,
like astronaut riding a horse, a horse riding an
astronaut.
And like even with really controlled cases and doing it in
large days, like we just can't get it to do anything that
gives us any data to point to, to be like, look, it's doing
okay.
But maybe it's just a clip problem.
Maybe it's just sex.
I think it's, I think it's a clip problem.
So that's, that's, I wanted to talk about this because you
have this really interesting paper about it.
And I think we can generalize any finding about vision
language models based on constructive learning,
like clip tool, say, language models trained
autoregressively.
There is this recent paper from Stanford and configurably
first author is like, which, which confirms my
suspicions.
Basically, the idea is that click treats ends up treating
text like just bags of bags of words.
Because in order to, to, to fulfill the contrasted
learning objectives of bringing together the captions
and images that go together and further away in the
space, the ones that don't go together.
You don't need to actually induce much about syntax.
Right.
Just keywords is sufficient.
And, and generally also in the captions themselves, you
don't have a ton of information about, say, like the, the
relative positioning of different objects or how many of,
you know, it's not like you say, they are key forks and
wine and one knife.
You just say a plate with folks and knives or something in
the caption.
Right.
Right.
And if you look at other models, like party from Google,
other image nourishment models that is pre-trained as a
text on corner of these, a pre-trained language model.
Yeah.
They actually do way better at things like, you know.
Much better.
Yeah.
Yeah.
Yeah.
Yeah.
And I think that surprised me in trying to do, so we did
the red cube, blue cube kind of stuff because that's like
super abstract and it's easy to do.
But one of my other students, Charlie Lovering, is working
on a project with some of the image generation models to do,
to try to kind of more systematically look at the,
you know, horse riding and astronaut kind of example.
And one thing that struck me with it is just how, like how
not uniformly distributed the world is.
Right.
Like it's just like finding examples of relations, like
where you want to have like argument one, relation argument
two.
Yeah.
That can actually exist in both directions.
Right.
And like that's a thing that a human could visualize.
We like, it's really, really hard to do.
Like we're restricted to a really simple set of relations
because we like spit out a whole ton of things and tried flipping
the order and you're like, yeah, this isn't a thing I could
actually imagine or would expect a human to be able to realize
in any kind of way.
And so I think getting at kind of revisiting our assumptions
about how compositional humans are, I think that's like kind
of relevant data to have is like how often are we actually
forced to combine completely novel elements in a purely
abstract way without anything that we can relate it to
like what we think we're our kind of working theory for what
the model is doing for these kinds of things.
It's like, if you ask for a horse riding an astronaut, it's
like finding the most similar thing it's seen.
Right.
It's like, well, here's an example of like a, you know, a
teddy bear riding a puppy dog or something and then morphs it
into a horse.
Right.
Like some other type of thing.
And it's like, that's not actually an absurd model to assume
might be underlying some kind of compositional behavior in humans
too.
Yeah.
It reminds me of the, this might be for with respect to
language models that friends all these content effects on
reasoning where, you know, when you test them on syllogisms
or the Western selection tasks, you, you get, you get these
effects that are similar, you know, also fun in humans where
if you're feeling the story of a task with plausible details
that could apply to realize that they did much better.
Right.
If you just use this like far-fetched, you know, abstracted
rubble.
Right.
Yeah.
And that's, that's been shown.
Like, I know this is, I don't remember the authors, but there's
pretty classic study about the kind of logical syllogisms.
Right.
Like if you ask people to reason about these, just like, you
know, famous modus ponens or whatever, when you ask them like
a, not a, like people suck, they don't know how to do it.
But if you put it in realistic real world scenario, like
you're at a party, you can have ice cream and cake or whatever
then people do totally fine.
And I think it, it wouldn't be surprising that we might have
these, that we would have evolved to be able to reason about
realistic scenes and not abstract ones.
Yeah.
And the, the, I think the skill to be able to, to, to reason over
the most, the more abstract examples are, including with
images in those, just in a possible way is something
probably require through training and people of, you know, who
have children might, for example, are probably familiar with
that.
So you kind of need to do some prompt engineering as it were
to, to get them to actually, you know, draft certain concepts
or how to, you know, teaching logic, even to, to um, the grads.
Yeah.
You get a lot of resistance from these content effects and
trying to find the institutions in the right way.
And I would, I wonder whether, you know, with the astronauts
running a horse example, whether you'll be interested to try
that with, with small children and see whether, you know, whether
they actually do well at that task.
Yeah.
Yeah.
I mean, I would have to assume they would, right?
So that's where like my intuitions are strongly like, but humans
are quite compositional.
Like you would, you would have to assume that they would know how
to, um, they wouldn't just draw a horse riding or if you asked
for a horse riding, they wouldn't just draw astronaut riding a
horse, right?
They might like giggle and be like, that's silly.
Like horses don't ride at something, right?
But, um, yeah.
I mean, I guess, so I guess some counter examples to both of our
intuitions.
So that there's some of this variable binding stuff, but with
the image models and I'm kind of like, I'm willing to, um, give
the models a pass, at least in the immediate term while we figure
out what's going on, because I'm like, yeah, I think some of
this, um, perfectly abstract compositionality might be like
a tall order and not something we would have humans do.
But the other thing that's always been weird to me, so like I'm
channeling Roman Feynman, my colleague here, who's like much
more of a language of thought tradition.
Um, and, you know, he'll, he'll say, uh, that, you know, of
course humans are like, you know, better at, like he's aware of
all the data that people aren't as good at logical reasoning in
some settings, but they can do this other sentence and things
like that.
But it's like that doesn't undermine that humans have
logical capacities.
Like we use negation freely all the time.
Like we don't really struggle with that.
Um, but the language models actually still kind of suck at
negation.
Um, like it's pretty easy to just write something, uh, in a
slightly odd way and get them to ignore that you negated
something.
I think even fairly recently, I guess I haven't tried this in
the past month, so maybe it's fixed.
But like I asked something like, asked GPT or chat GPT, I
think for like a recipe that like uses tofu and nutritional
yeast, but isn't vegetarian or something like that.
And it just spits out some vegetarian recipes or something,
or you'd be like, you know, you sugar and lemon, but not a
dessert.
And it's like, have you tried lemon bars and stuff?
So it's like, it just kind of ignores it.
Um, and that's weird to me because that seems like for a
language model, doing a language modeling task, like that's
relevant.
Like that's not, this isn't like a super trick out of
distribution thing.
Um, so that's kind of like, I guess, uh, a thorn that like I
feel, and that's where I say like, I have a bit of a caveat
where I, I wouldn't be surprised if a couple years from now I
have to be like, yeah, I was wrong.
The models are not at all like human like or something.
I'm hoping that's not the case, but these kind of data points
are like, yeah, that's, it's frustrating how bad they are
with negation, um, or other kind of basic things like that.
Yeah.
I, I, yeah, I mean, it, it, it's always a moving target, of
course, because people, you know, maybe said, or GP four can,
can handle that first, for example, but I agree.
It's definitely very unsatisfactory to still his, his
failure.
He says some composite.
Yeah.
Yeah, look, we have some, I guess, so on one of these
projects with Roman in this undergrad, we're working with
Alyssa, um, we, and this is just super frustrating data for
me, although actually it is getting better with the bigger
models.
So maybe it is.
But for GPT three, um, it was like, we have this very basic
task that they had run on humans.
So you say something like, um, like give some little scenario
and then you say, uh, you know, the, it's like some scenario
about scientists running experiments on rats.
And it's like the scientists saw that, um, none of the rats
liked the food or something.
And then it's like, now that they knew that some of the rats
liked the food and they like did human reading time and saw
that people showed a slowdown and were surprised by the word
some in that case because, um, it's a blatant contradiction to
what was just said.
Um, and when we use like GPT three and predicts
surprise, there's like no surprise or whatsoever, no
suggestion of any being at all bothered by the word some in
that context, which is weird because that's just a straight
up language modeling task.
It's just what is the probability of this word in context,
which the human data was very clearly like it's low and the,
um, and the model was like, it's fine.
And then, but then we had, we tried with GPT four and then
the numbers, it looked better, but it was kind of messy
because we can't get perplexities of GPT four.
So you had to ask it to fill in the blank is a little bit
different.
Yeah.
Yeah.
Yeah.
Yeah.
But yeah, it's one of these weird things.
Yeah.
And so, so like maybe GPT four is better, but it wasn't as clean
of us comparison.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
And so that was a really standard of this comparison to the
humans and, um, and it was even for a model like GPT three,
like it was surprising that it was so bad at that.
And like, I don't know.
So I feel like there's a few of these data points that are like
the story isn't a slam dunk.
Like there's some of these things that really should be easy
for a model with basic structure.
Um, you know,
Yeah.
It's surprising to me because there are, you know,
this that it's been telling others shown that if it's way
the right behavior in terms of
surprise all when you look at things like subject
valve agreement, fetal gap dependencies,
you know, I don't have stress on these.
So, you know, the right range of
some tactics and the same patterns of humans
in terms of the surprise, when the
imperative doesn't back up the subject,
things like that.
Even if you're referring to a structure,
these are the structures to push you there.
So, why is negation such a thing?
Totally. It seems much, much simpler.
Right? Like, it's not, like, if anything,
I feel like negation plus these quantifier
terms, like, you could just enumerate a
table and say these things can go together
and can't go together.
I mean, there's like a little more semantic
cushion around it.
You have to know who's being modified
or whatever, but like, I'm quite sure models
can do all of that.
So, I was very surprised.
It might be just that it's a really
infrequent thing in the training data,
in the input, but then you have this
poverty of the stimulus argument that you
have to account for, right?
It's like, I mean, maybe it is that this
kind of entailment relation is just not
frequent on the internet, but it is in
kids' input or, you know, I don't know,
or maybe it is that the models are just
not the right models for this task.
I don't want to believe that, though.
I think there must be some other reason.
Yeah.
There was this paper by Will Merrill that
shows that entailment semantics can be
induced by an idealized, an ideal language
model on synthetic data up to a certain
sentence length.
Right.
I think in real-world scenarios,
there's only given the size of 63,
I think perfect entailment semantics
could only be induced from this formal
proof to sentences of four words.
But if you want perfect, absolutely,
you know, perfect.
Right, right, right.
You exactly know where there are one
sentence and the next entailment.
Right.
I don't know that I believe in entailment.
Right, yeah.
I just, like, there's, it's like the logic
stuff.
You're like, okay, we can come up with
these, like, toy domains in which we all
agree about entailment.
But, like, during my PhD, I did a lot of
work on entailment and just trying to
collect entailment judgments on humans
is a nightmare, right?
Like, they do not behave to the point
that you have, like, that's what, I feel
like that was, like, a switching point
for me where I was like, okay, maybe,
maybe I should accept that we have to
trust them, but on what is entailed
or not.
Unless you idealize Grisian speakers
as well, which humans are not.
Right, yeah.
Yeah.
No, no.
But it's, it's intriguing because it does
suggest that, I mean, you can learn
something about the entailment
semantics from distributional
information.
Yeah.
Even, you know, entailments, you know,
lose a sense, like, in the imperfect
sense.
Right.
Right.
So, again, why is negation so hard
is, is...
Yeah.
Which, to me...
Yeah.
Yeah, I guess I haven't seen a really
good study on just the distribution
of negation in the, like, in a models
training corpus.
Like, how is it used in what context?
Because I don't, yeah, like, it might
just be that it's actually, it's just
not distributed the way we kind of think
it's distributed.
It just functions very differently in
written text in general.
Yeah.
Right.
I do think that perhaps kids would learn
it, like, kids get a very different
input of negation than what I would
imagine is on the internet.
Right.
Right.
Like, I, in academic writing, I use
negation only in the most convoluted
ways.
Like, like, with, like, these triple
negations that are, you know, like,
while it is not unreasonable to assume
that such is not the case, right?
Like, that's the way I would use
negation.
I wouldn't say, like, that's not a dog.
Yeah.
No one writes that.
Yeah.
It's interesting because in terms of
the text that's actually generated,
like, language models, I don't think,
you know, even with GPT-3 in my
experience, you see negation error.
It's like, I can't even remember.
Right.
When they actually generate text, they
use negation properly.
Right.
It's more, when they possibly prompt,
somehow, sometimes they ignore
negation as if they're trying to
maximize the relevance of every word.
Right.
So if you mention, if you say, I
want you to give me recipes, but not
in the paprika, they'll see the word
and they'll be like, I need to maximize
the, you know, the relevance of it.
Yeah.
Right.
And it might be, I, so when I was
playing around with the recipes and stuff,
it did seem to change a lot.
I mean, it was anecdotal, but on the
wording.
So, like, if you marked it a lot more,
right, like, like, if you said
something like a recipe with these
things but include meat, it does fine,
right?
So if you say not vegetarian, it gets
confused.
And if you say, like, and not vegetarian
versus but not vegetarian, and, like,
where you put, like, like where you,
whether you front loaded or back loaded
certain information, it made a
difference.
And so you could imagine there's these
distributional signals where it's like
when people are saying, don't do this
thing, they mark it in a few ways,
right?
They don't just, like, slip in the
negation, but otherwise have the
sentence read exactly like it would in
the positive case.
Like, there's probably a reason we have
but as a conjunction and not and,
right, is just to, like, help emphasize
so that people don't miss that negation
piece of it.
Right.
So, like, you could imagine something
like that, that it's like the model has
very little incentive to emphasize the
negation unless there's other signals
that you really are, right?
Yeah.
So you wouldn't, like, be deathly
allergic to paprika and then just,
like, slip it in.
Mm-hmm.
Give me a recipe for chicken
paprikash without paprika or something.
Yeah.
And then just, like, you know.
Yeah, I know that.
So one other thing that this article
was provided to me by Enviable Binding
that I wanted to ask you about, there's
this pre-print that I think
was published on Archives
that alleges that, you know,
they looked at variable binding
in transformers.
And I can't exactly remember the method,
but they alleged that they found that
transformers can't really do
variable binding unless it's by using
the output as an external memory.
And that relates to some of the discussion
that we've had at the conference
that I co-organized with Dave Chen.
He's in there like a few weeks ago
with a recipe cat where Nick Shea
made a somewhat similar claim
that, you know, the kinds of
what he calls non-content-specific reasoning
that transformers can do,
how entrepreneurs can do,
is always propped up by reasoning
on the output.
So using the generated work,
let's say, in Shenafang Chrome,
you just read it step by step,
and you use the steps in the generative steps
as a crutch to solve problems.
And maybe there's also this paper,
this is a paper you've written,
Structural, Completionality, and Subroutines,
a number.
I haven't yet had chance to read that one,
but I just wanted to ask you,
because I was striking to me that
you can do some things, zero shots,
with kind of language models
that seem to fly in the face of that kind of thing.
So, for example, with GP4,
you can tell it's
behaved like a Python shell,
and then define a function
with a bunch of variables,
and then a little bit later in the interaction,
just call that function for specific values,
and say, you know,
and because it's behaving like a Python shell,
it just has to give the answer zero shots,
it's not able to do some Shenafang or whatever,
it just has to give you.
It's pretty reliably, even though some family modes,
but the fact that it can do it that's all
suggests to me that Steve has to be able
to internally manipulate the variables.
Absolutely.
Yeah, I love that example that you gave
in the workshop,
so at this Phosphate of Deep Learning Workshop,
because you had that,
you had it behave like a Python shell,
and do this Fibonacci sequence.
Yeah, so I've told us two functions.
We can just ask the language model,
or to tell you the, you know,
700th Fibonacci number,
it messed it up, right?
Yeah, it did.
That was like super interesting,
and yeah, I guess the thing we like
fight with with these models
is that they're, you know,
like we do know that they have,
that they've trained over the whole internet,
and so they've learned to like kind of
subspaces, right, and they're like drawing
from these different domains,
and it makes it really hard to interact with them,
and they can always,
they can always pull this like,
oh sorry, I didn't understand the question game,
I thought I was being my red itself,
but it turns out you wanted me to be
my New York Times self or something,
and it could always pull that, so it could be like,
oh yeah, I know how,
but you have to ask me to act like a Python shell,
because people on the internet don't know the Fibonacci number.
And this is a little bit of like,
for the critics
of large language models,
this is a frustrating game to play,
like this is always a move
that the proponents of language models
can make, is like, oh you didn't ask it right,
like it knows how, but it,
which is really why I feel like
the mechanistic stuff is so important,
because if we know more about it, then this becomes less of,
it feels less like you're sidestepping the criticism,
so we can just say what actually happened,
but I do think that right now
that is the case, right, like it could be that they
have the ability
to bind variables
and do this stuff, but just
they
have deemed that that's not the right way
to solve the task in the average case
or in the typical case, like maybe negation
is not important for the typical thing,
but if they're acting as a Python shell,
then of course negation is important, right?
Yeah, and I'm really interested
in the role of RLHF
fine-tuning in that context,
because it looks like it's
vastly improving the zero-shot capacity of the model,
and I think
there's some evidence that
you know, condensing the probability
mass of
the distribution of over the next
word
to a much more narrow range
such that, you know, just a few words will have
a high probability for certain
problems, because it's
essentially enabling the model to
latch onto the right task
right away, right, instead of having to do this.
I mean, still, as I said, it's still
resolved. Oh, sorry, I didn't
get your question right, but
if you ask a question zero-shot to
very large
GPT-3 without RLHF,
it will way more often just
have no clue what you're really asking it,
and you have to make some sense from that.
Totally, yeah, yeah, yeah.
Yeah, I like to use, I like that
OpenAI recently
re-released their, like
the old school GPT-3
because you can see what a big difference
even the instruction tuning makes, like
my favorite is if you ask something
like write a report on
the war in Ukraine
or something,
it'll like follow it up as though
it's like in an email, and it'll be like
please include additional, like
details on budgets and blah, blah, blah
please get it to me by Monday
regards or something, like rather than
writing a report, it like gives you a list of other
tasks to do. But yeah, so there's
clearly like
like the RLHF clearly improves
zero-shot, or at least even instruction tuning.
Something I'm, I don't know,
maybe you've thought about this more, I know
we talked about this like briefly before, like
just I guess I'm, I'm still getting
my head around what RLHF
is, or in particular how it's
like
if it is profoundly different
from other
types of training, because I feel like in some
cases, like I've heard a lot of people kind of
crediting RLHF as like
possibly like, oh, we have all these problems
with
with our language models, but maybe RLHF
alleviates them
and I can never tell if there's like a genuine
feeling that it does, or if it's kind of
this is a new kid on the block
and we're not sure, so to kind of
hedge or future-proof whatever
claims we're making, we add this caveat
that's like maybe RLHF
alleviates it, or maybe
it's that like RL, in general
RL is associated with like grounded
and maybe more cognitively plausible learning
in certain domains, and so people hear
the RL part and think that it's somehow
better, but I'm not quite sure
if RL is like, RLHF
is like deeply different, or
if you could induce the exact same behavior
through something more like an instruction tuning
setup,
like I genuinely
don't know which of those things is going on, and I haven't
heard
like I know there are people trying to
come up with a fine-tuning variant that otherwise
behaves the same as RLHF
because RLHF is unstable and people don't
like it, but Brown has a lot of
people who work on RL and they look at RLHF
and they're like, this isn't even really RL
because it's weird
and it just seems like you've kind of like
folded more language models on top of each other
so
yeah, I just, I don't know how I feel about it
and my gut is to be like
no, it's not special, it's like the same
stuff in a new package, but that's
based on absolutely nothing except vibes, so
I don't know it. Yeah, I mean
I'm just sure about the difference
between RLHF and instruction-tuning
because it seems, you know, if you look at
what people did with the Lamar model from
the time that I, there's this generated a bunch
of, you know, input
out compares with GP4 or something
to create
what is it, the Kunya or
a pack out, yeah, so
they, oh, at least
camelid animals, so
they, yeah, so they generated
essentially an instruction-tuning
transects
from models that have benefited from RLHF
and then you kind of get the benefits of RLHF
for free
right, or for $300
and so it seems
that actually this works pretty well, right, so
right, so maybe
I don't know, I mean, it could be
pure speculation, but like it, that
could be the case and
RLHF is special, it could be that RLHF
allows you to optimize for a function that
you can't directly optimize for with
expert prediction, but if you have an RLHF
trained model
you can then distill that function
like the neural net can then directly, something
like that, but I'm almost like
I want to say that RLHF doesn't
even optimize for a different
or special or function that
like you could just take the
data you get from RLHF
and just
use it differently and like fine-tune
on it or something,
I mean that must not be
the case, so I guess I'm really purely speculating, but
I think that's the intuition that I'm just like
deeply wanting right now is like, is there
something special going on
or is it just
we found a different way of getting
somewhere, like we kind of stumbled
upon it and we could actually get the same effect
and I feel like it
matters, it matters from an engineer's standpoint
but it also matters because I just hear RLHF
actually I would like to talk to you about that because you
like mentioned it in your grounding paper
and I've heard it from a lot of people
like
F. Fedorenko had a paper
on kind of disassociating language
and thought and had all these different
criticisms of large language models and the things they
can't do and then at the end it's like
but maybe RLHF solves all of this and I was like
whoa that's like a huge
but I don't know and I've seen
similar things elsewhere and so I think to
me I'm like
I'm just deeply curious if that's the case
like I think I'm lacking that intuition
Yeah, no absolutely, I mean
first of all let's see this transition to
the topic
nicely done, yeah
so
just to
for the audience
so with RLHF we wrote this paper
and essentially we said well people use
the notion of grounding in different ways in the literature
and then of this goes
back to Harnott's simple grounding problem
from the 1990
90 and
in which he argues that
symbolic A.I. models
lack the capacity
to have intrinsically many forms
of foundations in our beliefs
because you know
the semantic interpretation
of their representations is provided
externally by the programmers, so CHRTLU
from Naples Kebolsko
program that could manipulate LOGS
in the LOGS world
it can connect its linguistic representations
to virtual objects
but that connection is provided
externally by
exclusively programmers
and the problem is how do we get models
that actually
intrinsically
grounded representations of legacy items
where here the key notion
of grounding is referentials
so how do we get representations that are actually
making reference to the
objects out there in the world that they are bound
and that problem has created
a marriage with recent connections
models and marriage models and so on
a lot of
we were trying to
pick up
our different notions of grounding
say the referential national grounding is the most important one
and then say well in light of that
can we say that
mangroves models can actually
achieve some form of referential grounding
and we do mention that with CHRTLU
because basically the argument is going
from the perhaps the most plausible
and convincing argument for the most people
to something that's a bit more speculative
so with RLHF
so the problem essentially is that
with Maxwell prediction that's an intralinguistic
function right so you're predicting the next word
this RLHF is more pre-trained
and that doesn't seem to give you
quite the
at least intuitively the right kind of
normativity
for form representations
of the worldly reference of legacy items
such that you could
have the possibility of misrepresenting
something
in other words you don't have
the right kind of world-involving function
it's just you know whether you're right or wrong
about the next token
and RLHF on the other hand
because you get this explicit feedback from humans
including feedback
about truthfulness or honesty
generally the three H's of RLHF
is helpfulness, homelessness and honesty
there's at least one of these which is a normative component
that's about that's an epistemic
about whether you're
answering questions about capitals
of the world
whether you're right or wrong about the state of the world
when you say that Paris is the capital of France
so that seems to be world-involving in the right way
but then you know
we had this conversation
a few weeks back and
as you rightly pointed out
it seems that
you know you could get that the right kind of
world-involving function without this explicit
feedback from humans
and we do actually think that that's the case
so
going there from a slightly less
consensual
claim we think that you can get in context learning
and in context learning you have a fusion
when you have a fuchsia swamp
where you have several examples of a successful fuchsia task
if it's a fuchsia prompt on
worldly facts
you also get this explicit
feedback in the prompt
about what's right or wrong in the world
and if you're seeing context learning as
inducing a function
optimizing for a function
that's not just network prediction
then you can also see that
as providing a world-involving function
but then if you go even further
and say well after all why not look at pre-training
and I've told you there are some context windows in there
that will include discussions of worldly facts
where in order to do the next
work prediction which is the approximate function
the one who might have to induce
a more complicated
ultimate function that's about the real world
so I think that's where your intuition
is taking you right
yeah yeah yeah I think exactly
and this is this
I love the paper
and I like this idea of the world-involving function
because I think that's
kind of the intuition that
we all have
there does seem to be something that's
not
like I think
most people's gut instinct is
next word prediction over text isn't
enough
and then the challenge is figuring out
where is the line like what is the
what actually is the problem with it
right and that's
where I'm kind of stuck because I think
in my
heart I kind of don't want language
model next word prediction
to count like I don't want that to be the whole
thing
and I just like and a lot of our own
work though we seem to be arguing the opposite
like I'm not quite sure where I fall on this issue
and so it's like yeah you want something
like this world-involving function or something about the learning mechanism
right the learning mechanism doesn't
feel like enough you're like no
obviously people do more
right then predict the thing that
happens next but then like formalizing
what that difference
is I just I haven't
been able to convince myself like I haven't
been able to come up with a thing that I'm like yeah that's
the thing I totally
buy it like sometimes it gets on this issue
of like goals and I guess this actually came up a
bit in the
deep learning debate like you know and
when I talk to a lot of cognitive scientists
and people who don't like
the next word prediction language model it's like
you know people have goals
but that feels like a weird
because you can
say that the language model has a goal and the goal is to
predict the next word and like so now are we
just like making a
a judgment
about what goals count
as good ones and not good ones
there's a similar yeah like this kind
of
having the person
give the feedback
who is sufficiently tied to the world and now it seems
like we've like outsourced the
question of whether the model is grounded
to something about the trainer and the goals
which maybe actually is fine maybe
philosophically that's fine you're like
to fall along this causal chain you have
to have kind of
inherited from somebody who's
also on the causal chain so
I mean I don't know because like yeah if you had like
giving RLHF feedback who is like
intentionally misleading
right or
or just confused and
misinformed like any of these types
of things like I don't know how
that
muddies the analysis
of whether the model is now
world-involved or grounded
referentially grounded yeah totally agree
on the last part I think if
the whole argument hinges also on the assumption
that the crowd workers
actually know how
to rank the outputs or
do it right
to a large extent
and there's this and I guess this is a
classic philosophical qualm too
right like because we can
like humans could be totally
wrong about stuff right so like
like science progresses right so right now
we might be saying like oh the
I forget what are the
classic examples the
like a lot of things about like
viruses and diseases right like
we had theories about what
these different diseases were
back in the day and then
we learned stuff and it's like an entirely new thing
now right but you wouldn't say that the people
historically had like an
ungrounded meaningless notion of that thing
right so we
like you need to account for the fact that we could
be teaching the model something that is
ultimately wrong because we haven't
learned that in fact
that I don't I can't
yeah
I can't think of a good example
the thing is theories of representations
of representation will account
for the fact that you can misrepresent
things right so
just because you have
you know a linguistic
like lexical concept that is
refractionally grounded
doesn't mean that this precludes
the possibility
of you know something
wrong and misrepresent
right so like
thinking
yeah and
I guess this is kind of it's like
so if the
model thinks that the
capital of France is Berlin
quote thinks that and it
produces that as output
I guess we need to differentiate
between the case where it's just
wrong and ungrounded in learning loose associations
versus it was quote mistaken
right but like that because
I could not know the capital of a city
and like that it's true that I don't
have quite the right concept of that thing
but it's not the same as me being
a parrot that's just spitting out
right and so I don't think we
quite right now I don't
know how we're drawing that
line within the models of them just producing wrong
stuff versus yeah
I mean certainly with
I think perhaps with things like RLHF
you can draw that line potentially
but with pre-training
I mean and I show you intuition that
you know it seems
you know
we don't really want to draw the line at RLHF
it just seems like the low hunting fruit
because that's what's going to come up with the most people
but that's already
you know it's already shouldn't become a
rational statement to say like
that we're going to draw a trend of text on the
can-achieve professional planning with a RLHF
but then we would want to defer the like QN
but then it's tricky
so here's the
interesting that I have
I've been maybe overly impressed
by these recent papers that look at Incoven
starting and show that it's
this wonderful Google and others
that show the same thing essentially that
they're functionally
equivalent to fine-tuning with gradient descent
even though you're not actually
addressing the weights and
what's their tuning on
what's the doing gradient descent
virtually on is not
next to a prediction
it's whatever function is
specifically specified by the future
that that attempts to get the future prompt
and so that
really is the thing that conduits me
so you could get the world involving stuff from that
and if you can get it
at different time within context learning
then presumably you could also get it
at pre-training time
if you say you have a window, a context window
that's a bunch of capital questions
like what's the good part
you have some
like stuff in the training
that's presumably
when it's not at the beginning
when it's totally random but when it starts
learning and reaching us at some point
you might not get that
I think
this is where
I agree with
that and I agree with calling
out RLHF as possibly
a different point because it's
a good thing to ground to
as or not ground to
overloading the word ground
it's a good data point
and then trying to peel away
what is the minimal thing
what about RLHF gives it that
but that's exactly that logic
you just laid out is what I
I think I accept
right now or feel somewhat forced
to accept based on this
because you're like that seems correct
but then I have to go back
to putting on the hat of somebody
who does not believe these
language models because
and I think it is important to point out that when we're talking about
language models being grounded or having meaning
it's not the same as saying they are
conscious
and intelligent right like but sometimes
that's this kind of elephant in the room
where it's like where are we going next
and so I think when people are looking at
these language models and they don't want to acknowledge that
they can be referentially grounded
because that seems like a step
along the way to claiming that they are like
human level cognition
in all of these other ways
it's so deeply unsatisfying
it's like wait no like you've missed the point
like now we're saying that just having a few examples
during pre-training of someone
listing off countries who wrote that down
in you know
good faith just listing the capitals
of countries that's enough and now the language model
counts even though it's just doing next word prediction
where like that seems insane right
and so I kind of like I feel like I just go between
these two positions of being like
like right now based on everything I feel
kind of forced to accept like no language models
I would say they're referentially grounded I can't find
a good case to make for why they're not
and at the same time I'm like do we
really want to say that that seems bizarre right
like I was like something's wrong
but when we get
our hand forced by
this the kind of all of nothing's
thinking that you see sometimes in these debates where it's like
at the high level
it's like either the stochastic power saw
or it's like human like
you know AGI with
you know
understanding consciousness and thought of it
yeah yeah yeah
and there's this huge
space in between
possibilities that we could explore where you
look at different capacities in the case by case base
and say that
and then within each capacity that grounding
it's also spectrum right it's not like
you know we want to say
you get a few examples of
question answers about capitals and then
human like refreshable grounding on everything
you check that box now you're good
yeah right so
so presumably
you know you could say well you know
that gets you
your foot on the ladder of refreshable grounding
in a tiny tiny tiny domain
and that's still
theoretically interesting
that said I mean
there's something that I think is really interesting from your work
on you know isomorphisms
between say
color terms and the color space
and stuff you've been working on with grounding
that maybe
suggests that sometimes when
you get the toe hold on
grounding in a specific domain
you could leverage the isomorphism
between language and the world
to get a little bit more
for free
right right
right you definitely could
imagine that
and I guess this is the project that you and I
start working on together is the kind of what kind of power
of analogy reasoning
do these models have
because yeah you could imagine something like this
like
with the toe hold and really strong
reasoning by analogy capability
you could get a lot
out of that but I also agree
I think there's just this big middle ground like it's not like you could
like
learning that
the
meaning of the capital cities or something
that shouldn't be enough
that now by reasoning by analogy
you can infer like
everything the whole world like that
that seems
if somebody could spell out a mechanism
via which that would happen sure
but I can't imagine what that would be
yeah
yeah
I was
going to make a complete
detour but I remembered we didn't talk about the Chomsky stuff
when we were talking about Kamsky
so if you have any other thoughts on that
yeah I have some thoughts
but I would like to hear your thoughts on
yeah
so just for context
what Chomsky
obviously has
been
writing about statistical models
of language learning for decades
but he recently
read on the records
saying as you would expect that he still is
anything whatsoever
acquisition of human condition in general
and he
co-wrote this op-ed
in New York Times
making that climb
even though he told me he would have
he signed on it but he would have made it
pointable a bit differently
because it goes in different directions
I think you know I have some
process about how the op-ed is believed in
I think his point is more
his core point is simpler it's just
you know
language models
he makes an analogy with
a theory of physics that would say anything goes
and
that wouldn't be a good theory of physics
he's very impressed by this particular paper
that
from Bauer's colleagues I think
that's a big step
learning supposedly possible languages
and showing that they have language models can
learn such languages
that's a total amount on the fourth
yeah
and then the student I don't see
wrote this paper
that's saying
completely opposite stance saying
language models refuse to hold
programs and programming linguistics
and I
I sign in between
as often in these discussions that's the
running thread but I think
there's always room for positions
in between the two extremes
so
there was this interesting discussion
that
there are
two versions of the
cover to the stimulus arguments
that is used to
justify the claim that there is something like
universal grammar in humans
so there's a strong version that Chomsky himself
did defend in the past and that some people still
defend neighboring very popular grammar
like you know genre grammar textbooks
which is
syntax is just unlearnable from data
period like no modern
data will get you to learn such a structure
because
having discovered cursive
infinitive productive system
is not something you can learn from data
interesting
so I think there is a good argument
and see it makes a good argument for that being
somewhat refuted by language models
in so far as they can
there is a lot of work showing that it can induce
structure
whether it's
it's the grammar agreements
filling up dependencies
we can even decode
properties from
so
so that's a strong question but then there is
the more in my mind more reasonable
differential version which is
well look children
can do these constrained generalizations
on certain syntactic phenomena
like if they have dependencies from
few examples from this improverage stimulus
and
it seems like this is hard to come for
if they don't have you know strong in that
device to make these reductive
inferences
and one way that you could have this
in that device would be to have in its
grammatical
stripes
and there I think the evidence
with language models is way less clear
and way more tentative so
there is a little bit of
so the problem being that the large language models
the giant ones that give you the fourth
from order of orders of
managers more words than children
children do
there are a few papers in one from
from jengen colleagues from 231 and there is
more recent one from the office
training models
with between 100 million
sorry 10 million and 100 million words
which is
what a child would get by the age of 6
8 10 years old
and they show that actually you can get a lot
of learning of structure
with a title like this it was like even when
trained on a normal amount of data
language models still replicate yeah
and so one of these papers shows that
what you get when you train them you keep
training them on x4 words is mostly
he mostly pertains to things like
common sense knowledge about the world
and semantic stuff
but not so much the syntax stuff
maybe negation would be an exception
but that's how I listened
yeah
and there is this also this project and I'm
very excited to see the results of the baby
challenge from Alex
yeah
and so this is the same kind of challenge
they use this, they created this corpus
that's corpus of child directed speech
that's actually recorded from
real life child
the kind of word step here, the sentence step here
and there's a hard version of the challenge
where you have to train language model on only 10
million words I think and an easier version
of 100 million words
and people will submit their
questions and I think the results will be
more supported in August or
but yeah I think that kind of
projects could
in my opinion constrain
hypothetically in theoretical linguistics
and maybe go against
the weak version of the process and say
look like maybe you don't need
linguistic
that syntactic in it
knowledge
but even if that's the case I think
something you know
that just
mean that there is no amount
of in its structure
or inductive bias whatsoever because after all
you know language models are not
tabularizing, they have inductive biases
just from us having inductive biases
and even if these are not
language specific I think that's a key difference
before Chemsky would say we need this language
specific in
knowledge there is a sense
in which the moderates empiricists
meets halfway with the moderates
nativists to say
there is some like there are some
inductive biases that are like general
that come in general and
language specific and we can be
happy with that so.
Yeah that's a really
yeah that's a great
point and I think I
agree with that like I
yeah I mean I also
just in general probably just a general
of science is like
putting something up as a choice
between two extremes is like always detrimental
right like and sometimes it can
prevent people from working on the problem
because they're like afraid of pissing off half of the field
but
yeah so I had
I like this one thing I
was thinking about
with reading
Steve's responses again the kind
of
just ham on the mechanistic stuff like
to me it seems like
the answer kind of hinges I
guess this is a different version of question but it seems like a lot of it hinges on
what the model is
doing internally
to process these
to process language like whether it represents
something that looks like chompskin syntax or not
but I guess this is kind of a different
question so there's there's the kind of
what you were describing which I think is
definitely
like one of the important ways to be
think about this problem from kind of a linguistics perspective
which is like take the blank slate
language model and say
how much data does it need to learn
because it's definitely a super
significant finding if you could
replicate if like a transformer
randomly initialized transformer trained on
a
realistic amount of child directed speech learn syntax
like that's a huge finding right
um
there's also this kind of like take the giant pre-trained
language model
trained on
way more data than human has
but then you can kind of use that as the
starting point like that's the innate structure
that a human has right
which could be a ton of language specific
and
syntactic structure
and then that's kind of what
um and then
from that point on then it's quite
efficient it's the problem
so that's kind of what my first time when I read
Steve's paper is like well we can't really
say because we don't know whether the
model has solved the language
problem by learning some like
nice looking formal syntactic
abstract syntactic structure under the hood
or not although I think a lot of data suggests
that it has um
but I guess it's it's kind of hard question
to answer because you can't do something
like um
you can't then take that as the
starting point of a language learner because it's already learned
a lot of language like you would want some other way of like
inducing that structure but then
stripping out the
ability to actually generate language if you wanted to
try to replicate
like you would want to like find a way of like pre-training
the model getting all the innate syntactic
structure but then somehow
re-initializing a large part of it
so that child would have to
relearn and then use that as to
ask whether what the language model has learned is something
akin to the innate structure that Chomsky
would have wanted. I also
I mean I guess something
you could do because you mentioned like learning
unlearnable languages is like try to
transfer a pre-trained language model
to one of these
unlearnable languages because I
like I always think of
the I've been thinking of the pre-trained
language models as kind of the
like that's whatever
is endowed by evolution or something that's the starting
point. Yeah, yeah, right.
And so but yeah I
recognize that it's kind of impossible to use as
a model of a pre-linguistic child
because it has way too much language to do
any interesting experiments on its language learning
but you could ask whether you could transfer
to one of these unlearnable languages and whether
it basically
needs to unlearn
and relearn as basically like if it
can transfer too well to one of these other
languages that I guess would be a data point against
that structure
being the kind of structure that kids have
born in whereas if it like
their models are very powerful but what it might need
to do is basically unlearn everything and then relearn
the new language from scratch
and then that would suggest
that it's like not. Yeah.
It's not actually very
unknowable to the models once they've been pre-trained
with this strong bias for other
structures.
Yeah, although you need to show that
humans that this
made of languages are actually unknowable
as humans. That's true. Right.
And that's how to do because you can try with adults
and then it's like an L2 for them like second language
but if you want to try with the first language
if that's how
you said it, if you think of pre-training
as like evolution. Right.
And then you could say well whatever
pre-training could be the in-ex structure
of Chomsky. Yeah.
So then you would need to actually, you know
it's an impossible experiment to do. You need to
be very unethical. You have to have a child
grow up in this.
So because like what is the data that
those languages are unlearnable is that no language
has the features that this language has.
I think so. They have this weird, yeah.
I haven't done a deep dive
and I think it's very controversial.
So a lot of people where I've been inspired by that paper
Chomsky
we latched them to that as, you know, hard evidence
that we can mend nothing from language models.
Right.
Yeah, I'm speaking out of my domain but I
so I would defer to someone who studies this
but I see why it's
controversial, right, because there's like
perhaps lots of explanations for why languages
share common features other than
those are the only possible features
for the human mind to
learn or something. Yeah.
Yeah. Interesting.
And also, you know, there is some evidence
that people who are not very Chomsky
linguistics always point to evidence across
languages where
it seems that
some of the features of generative grammar
seem to work better
for some
dominant languages than others that some languages
for a wrench into it and then
they start to move sometimes
to just add some kind of a thing
to the generative grammar to
compensate for that but then it becomes
a little less elegant and
you know, those are both
references. Yeah.
Yeah. Yeah.
Yeah, I mean I
it's kind of an
tangential point but I guess there isn't
as much work on
these large language models in other languages,
right, like I mean there are
multi-lingual models and stuff but that's definitely
some data points that would be good to
know, like we talk about transformers having some
they do have inductive biases
and things like I don't know how well they work
for other languages.
I've never seen a good controlled experiment
because just the quantity and quality
of data we have in different languages is
like so
varied that you just can't say anything when the
models are better or worse
but that definitely seems like
from the perspective of asking what did these models
mean for
the Chomsky and program
or universal grammar and stuff like it seems
like we need data on how they work
in
language. Yeah.
There is this work by Stephanie Chan
from DeepMind that
how the data distributional properties
that probably is of
the trading data influences
I think in context starting
in transformers but so they look at that
we've made up data sets
that's different distributional
distributional properties and
they find that the
typical distributional properties of languages
that seem to actually be there is work stream
you know there is a zip
the zip will
apply across basically every
human language and it seems
that there is something in common
in the distributional properties of all human languages
that transformers can watch on.
Yeah.
Yeah.
Interesting.
Interesting.
Yeah.
Yeah luckily there's still
lots of stuff to think about
like sometimes it's like oh all the
problems are being solved these models keep coming
on doing all the tests and it's like oh god no there's
so many like I think we still have careers
ahead of us that's good.
Yeah and in the way
people freak out about
the fact that all the big problems
now are being tried by industry and before it's
like working with academia but I think
a lot of interesting problems actually things you
can only do on small models anyway like
oh yeah totally.
So in a way there is a nice division of labor.
Yeah I actually yeah I
like I work
at Google 20% and I've said a couple
times like all my most interesting projects
are the ones at Brown because like I actually think
the small toy data things like that's where I feel like
I'm learning stuff I'm like ah it makes sense
that's how things work like it's so exciting
again.
So yeah I feel good about
academic work on this stuff right now.
Maybe the synoplinistic notes to
end on.
Yeah.
Awesome this is fun this is a lot of fun.
Yeah we killed
I guess I have my timer going almost
75 minutes so it's pretty
perfect.
Hi Tim.
Hi Tim.

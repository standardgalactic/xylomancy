Murray Shanahan is a Professor of Cognitive Robotics at Imperial College London and a
senior research scientist at DeepMind. He graduated from Imperial College with a first
in computer science in 1984 and obtained his PhD from King's College in Cambridge in 1988.
He's since worked in the fields of artificial intelligence, robotics and cognitive science.
He's published books such as Embodiment and the Inner Life and the Technological Singularity.
His book Embodiment and the Inner Life was a significant influence on the film X Machinea
for which he was a scientific advisor. Now Professor Shanahan is a renowned researcher
on sophisticated cognition and its implications for artificial intelligence. His work focuses on
agents that are coupled to complex environments through sensory motor loops such as robots and
animals. He's also particularly interested in the relationship between cognition and consciousness
and has developed a strong understanding of the biological brain and cognitive
architectures more generally. In addition Professor Shanahan is interested in the dynamics of the brain
including metastability, dynamical complexity and criticality as well as the application of this
understanding to machine learning. He's also fascinated by the concept of global workspace
theory as proposed by Bernard Bares, we'll be talking about that on the show today,
which is based on a cognitive architecture comprising a set of parallel specialist processes
and a global workspace. Professor Shanahan is committed to understanding the long-term implications
of artificial intelligence both its potential and its risks. His research has been published
extensively and he's a member of the External Advisory Board for the Cambridge Center of the
Study of Existential Risk and also on the editorial boards of Connection Science and Neuroscience of
Consciousness. Conscious Exotica, Professor Shanahan wrote an article called Conscious Exotica
in 2016 where he invited us to explore the space of possible mines, a concept first proposed by
philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of mines
which could exist from those of other animals such as chimpanzees to those of life forms that could
have evolved elsewhere in the universe and indeed those of artificial intelligences. Now in order
to describe the structure of this space, Shanahan proposes two dimensions, the capacity for consciousness
and human likeness of the behavior. According to Shanahan, the space of possible mines must
include forms of consciousness that are so alien that we wouldn't even recognize them. He rejects
the dualistic idea that there's an impenetrable realm of the subjective experience, remember we
were talking about Nagel's bat on the Charmer's show, insisting instead that nothing is hidden,
metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that while no
artifacts exist today, which has anything even approaching human-like intelligence,
the potential for variation in artificial intelligences far outstrips the potential for
variation in naturally evolved intelligence. This means that the majority of the space of
possible mines may be occupied by non-natural variants such as the conscious exotica of
which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible mines
invites us to consider the possibility for human-like mines but also for those that are
radically different and inscrutable. He concludes that although we may never understand these alien
forms of consciousness, we can still recognize them as part of the same reality as our own.
So Professor Shanahan has just dropped a brand new paper called Talking About Large Language
Models in which he discusses the capabilities and limitations of large language models. Now in
order to properly comprehend the capacities and boundaries of these models, we must first
grasp the relationship between humans and these systems. Humans have evolved to survive in a
common world and have cultivated a mutual understanding reflected in their ability to
converse about convictions and other mental states. Conversely, AI systems lack this shared
comprehension so attributing beliefs to them should be done circumspectly. Now prompt engineering is
something that we've all become very familiar with, we've discussed it a lot on this show recently,
and it's almost become a fact of the matter when it comes to these large language models.
It involves exploiting prompt prefixes to adjust the language models to diverse tasks without
needing any supplementary training, allowing for more effective communication between humans and
machines. Nevertheless, lacking a more profound understanding of the system and its relationship
to the external world, it's difficult to be certain whether the arguments produced by a large
language model are genuine reasoning or simply mimicry. Large language models can be integrated
into a variety of embodied systems even, such as robots or virtual avatars. However, this doesn't
necessarily mean that these systems possess completely human-like language abilities. Even
though the robot in the Seikan system is physically embodied and interacts with the real world,
its language is still learned and used in a dramatically different manner than humans.
So in summary, although Professor Shanahan concludes that large language models are formidable and
versatile, they're fundamentally unlike humans and we must be wary of ascribing human-like
characteristics to these systems. We must find a way to communicate the nature of these systems
without resorting to simple terms. This may necessitate an extended period of interaction
and experimentation with the technology, but it's a fundamental step if we are to accurately portray
the capabilities and limitations of large language models. So anyway, without any further delay,
I give you Professor Murray Shanahan. Professor Shanahan, it's an absolute honor to have you on
MLST. Tell me a little bit about your background. My background? Well, I've been interested in
artificial intelligence for as long as I can remember since I was a child really and I was
very much drawn to it by science fiction, by science fiction movies and books.
And then I studied computer science right from when I was a teenager and got very much drawn into
programming, was fascinated by programming. I really did my 10,000 hours of programming experience
when I was quite young. Then I went on to do computer science at Imperial College, London,
that was my degree. And then still fascinated by artificial intelligence, I went on to Cambridge
and did my PhD in AI in Cambridge, very much in the symbolic school then. And then I had a long
affiliation with Imperial College, did my postdoc there and still in symbolic AI. And then at some
point I became a bit disillusioned with symbolic AI and I kind of segued into studying the brain,
which was the obvious example of actual general intelligence that we have. And I think it was
a good 10 years on an excursion into neuroscience and computational neuroscience and that kind of
thing. And then deep learning and deep reinforcement learning happened in the early 2010s and AI
started to get interesting again. And I got very much back into it that way. And I was particularly
impressed by DeepMind's DQN, the system that learned to play Atari games from scratch. And I
thought that was a fantastic step forward. And I really kind of went back to my roots and back
to AI at that point. Yeah, and I think we'll talk about DQN when we speak about your article on
consciousness. But so having such a diverse set of experiences in adjacent fields, how have they
influenced each other? Yeah, well, and one thing I didn't mention is that I've also had a long
standing interest in philosophy. And I very often think that what I am is a sort of weird kind of
philosopher, really. And philosophical questions have had a great attraction for me. So I think
there's a kind of, you know, there's a sort of three way into relationship between artificial
intelligence, neuroscience and the other cognitive sciences, and philosophy. And I think they all
kind of mutually inform each other, really. Yeah. Fantastic. So you wrote a book called
Embodiment and the Inner Life. What motivated you to write that book? Yeah, so at that point,
so that book was published in 2010. And it was the culmination of a sort of long excursion into
thinking about consciousness and about brains, which took place after I had moved away from
symbolic AI, really. So I was thinking about the biological brain. In the back of my mind,
I'd always been fascinated by these philosophical questions about consciousness.
And then I went a bit kind of crazy and started, you know, thinking about these things seriously.
It became kind of my day job to think about neuroscience and about consciousness. And around
about that time, the science of consciousness was taking off as a serious, you know, as a serious
academic discipline with proper experimental paradigms. So that was really fascinating.
And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,
global workspace theory being one of the leading contenders for a scientific
theory of consciousness. And I was very drawn to global workspace theory, and partly because
it was a computational sort of theory. It drew very heavily on computer science
and computer architectures. There was a computer architecture at the center of the
theory. So this kind of collection of interests, along with my philosophical interests, which
all came together, and I wanted to put them all into a book where I express my kind of
ideas about, first of all, from the philosophical side, very heavy influence of Wittgenstein
about how we address these problems at all. Then lots of global workspace theory and a
certain kind of global workspace architecture, how that might be realized in the brain, drawing
also on the work of Stanislaus de Hen, who was working on what he called the global neuronal
workspace idea, and putting all these things together into one big book.
Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper and
your consciousness paper, but two things that did trigger or prick my ears up, computationalism,
which is quite interesting, because some folks in the cognitive science arena, especially with
the fouries, like examples to escape from computationalism. We did a show on Sel's
Chinese room argument the other day. He's probably one of the most known people who do
eschew computationalism. So what do you think about that?
Yeah. Well, actually, when I was talking about global workspace theory, I mentioned that it
sort of comes out of a kind of computational architecture. But in fact, where I took it
was very much moving away from that original presentation, which drew heavily on a kind of
quite an old-fashioned architectural perspective of sort of boxes and how they communicate with
each other and so on. I was much more interested in taking it in a direction which is very much
more connectionist and drawing much more heavily on the underlying biology and neuroscience,
which in fact is also a direction that Bernie Barnes himself had moved in, because the book
that originally put forward his theory was from 1988. So that was the predominant way of thinking
at the time was this very computational cognitiveist perspective. So by 2010, when my book was
published, I was very much more interested in a kind of more connectionist perspective on things.
And so that's the way that it's portrayed in the book, the theory.
Fascinating. Because in this arena, some people cite Penrose or the need for hypercomputation,
because people talk about the church-turing hypothesis and this idea that the universe
could be made of information, which is quite interesting. But do you believe that the world
that we live in could be computationally represented and computed?
Well, I'm not sure that I have a belief on that particular one. Penrose's ideas about
consciousness, of course, draw heavily on quantum mechanics, and he thinks that quantum effects
are important for consciousness. But I mean, that's very much a minority, a tiny minority view
within the people who study consciousness from a scientific standpoint. And so I don't really
subscribe to that point of view, I have to say. Well, I mean, coming at it from a slightly
different angle, we spoke to Noam Chomsky recently, and I've just done some content
on Nagel's bat, a couple of rationalists. And their big argument is about the subject of experience
and the limits of our cognitive horizon and the inability really for us to reduce things into a
comprehensible framework of understanding. So how would you bring that in?
Yeah, well, gosh, I mean, yeah, we've launched right into some really big, difficult topics here,
right? So in my book, Embodiment in the Inner Life, which at the time I thought I'd really kind
of like wrapped up the problem of consciousness, you know. But one of the big sort of outstanding
things for me in one of the outstanding questions that I have not really answered,
I felt in that book, is very much related to Nagel's question about bats, what does it
like to be a bat? And it's to do with the idea that there's a sort of intuitive idea that maybe
there can be, you know, very exotic entities, very exotic creatures who are completely unlike us.
And yet, somehow, there's some kind of consciousness there that we could barely grasp its nature.
And this is a, you know, it's a sort of natural intuitive thought. And especially when we look
at other animals, like bats, and especially if we look at an animal that's a bit different from us,
then we, you know, we get hints that there's someone at home, as it were, and that there's
consciousness there. I think, you know, we, I'm sure all of us believe that cats and dogs,
you know, and many other animals are conscious and are capable of suffering and, you know,
have an awareness of the world that's like our awareness and are aware of us and each other.
And so that's, you know, I think that's, I mean, I take that as almost axiomatic. That's just the
way we treat those creatures. But then when we think about something like a bat, it's very
different from us. So we, you know, the natural thing thought is that maybe, maybe what it's
like is very, very different from what it's like for us. And it's a natural thought to express.
And, and of course, you know, Nagel takes that thought to suggest that there are,
there's something that is inaccessible to us, which is, you know, what is it like to be a bat
is something we can never know. And this is a very unviconstinian thought. And I'm very much,
you know, I'm very attracted to, to, to Wittgenstein's philosophy. And, but, but it's also a very
natural thought that, you know, so that is a very unviconstinian thought because Wittgenstein says,
for example, you know, nothing, nothing is hidden. So he's very, you know, and the whole private
language remarks are all about sort of saying, well, this, this, this intuition that we have
that there's this private realm of experience is actually just, we're just, it's just, it's just
a philosophical trick of the mind to think that there's this sort of peculiar metaphysical realm
exists of inaccessible subjective experience in others. And, and that's the, that's his whole
thrust of his philosophy or that, you know, that aspect of it is to try and undermine that. So
these two things are intention, right? So there's this natural thought that, that bats, you know,
it must be like something to be a bat, but what is it like? And how could we ever know? And then
there's the Wittgensteinian thought, which is actually very difficult to kind of really embrace,
but, but it's that there's a sense in which nothing is really metaphysically hidden. It's only
hidden, could be hidden empirically, because maybe we don't know enough, maybe we haven't hung
around with bats often enough, or maybe we haven't examined their brains, or maybe that's all
empirical, right? So there's nothing metaphysically hidden. Whereas Nagel's point is that there's
something that's deeply, profoundly, philosophically, metaphysically hidden, which is the subjective.
Now we can extend that, shall carry on. So I'm rambling now. So, so we can now we can extend
that thought about bats. Now, you know, especially from the perspective of the sort of thing that
I'm interested in, to, well, not just bats, but what about the whole space of possible minds to
use Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,
you know, who surely there is extraterrestrial intelligence out there. It's going to be very,
very, very different to us. So, and then what about the things that we build? Maybe we can build
things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build
something that is also conscious, it's the kind of thing that's depicted in science fiction all the
time. In science fiction, it's often depicted as very human-like, but there's no reason why it
should be human-like at all. And so we can imagine these very, very exotic entities. And then the
question is even bigger, you know, there could be something that we, we won't even be able to recognize
that there was even the possibility of consciousness, but maybe it's buried there inside this complex
thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this
paper called, Conscious Exotica, which is all about trying to, trying to make that
Viconstinian perspective encompass this possibility as well. Yeah. Yeah. And maybe we should talk
about that before the language paper, just because it's, it's what we're talking about now. But there's
a few things you said there, which are really interesting. So, you know, when, when Chomsky
talks about ghosts in the machine, and he goes back to Galileo and Descartes, and actually it was
Descartes who introduced this kind of mind-body dualism, which has kind of a move away from
the previous desire to have a mechanistic understanding of the world that we live in.
Humans want to understand. And actually, so many things in the world alludes our understanding.
And then that brings us on to David Chalmers' point that the hard problem of consciousness,
which I suppose is an extension of the mind-body problem. And it's, as you were saying, this
little bit extra, right? So we think about, and I agree with Chalmers that intelligence
and consciousness are likely entangled or would co-occur together. But he always said that there's
function dynamics and behavior. And then there's that little subjective thing on the top. And for
Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just
something on top. It's not really requisite for anything else. And I believe it might be requisite
for intentionality and agency as so did. But what's your take?
Well, it's interesting because the whole way that you put that and the whole way that
people often talk about this thing is you speak about consciousness. Like, there's this thing,
which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe
it's this, maybe it's that. But I think that whole way of talking is, which is natural for us in many
everyday situations. But when it comes to this kind of conversation, I think that whole way of
talking is maybe not quite right. Because we're thinking of consciousness as this,
you know, we're reifying it, turning it into this thing. Whereas I think maybe at that point,
we have to take a step back and we have to say, well, when we talk about, when we use that word,
conscious or consciousness, so we use it in all kinds of different ways in different contexts.
And so when we talk about, you know, we might talk about it in the context of an animal, we might
say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see
the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see the
squirrel, you know, and it can smell, more likely to smell all of these things as well. So we use
consciousness, you know, we talk about consciousness in that sense. And we also talk about our
self-consciousness, you know, we talk about the fact that we're aware of our own thoughts and
we talk about our inner life and we use consciousness to encompass that as well.
We often use consciousness in the context scientifically of a distinction between
conscious and unconscious processes. And that's a very interesting distinction, because when
we're consciously aware of a stimulus as humans, then a whole lot of things come together. We're
able to kind of like deal with novelty better, we're able to report it, we're able to remember
things better. So whereas when we perhaps are unconsciously, or there's a kind of unconscious
processing of a stimulus, then we still can respond to it behaviorally, but and it can
have queuing effects and so on, but it doesn't have all those other things. So this, and that's
kind of, there's a kind of integrative function for consciousness there. And then on top of all of
that, there is the capacity for suffering and joy that comes with. So often there's valence to
consciousness, you know. So that's another thing. So all of these things, they come as a package in
humans, but when we speak about edge cases, then these things come apart and we need to speak about
them separately, I think. Fascinating. I mean, there are two kind of minor digressions there. I
mean, you were talking about these planes of consciousness, which is also very interesting.
And maybe we could get into the integrated information theory or the global workspace
theory just for the audience, just to give them some context. Yeah, sure. Well, John, we say a
few words about that. Oh, please, yeah. Yeah, yeah. Okay. Yeah. So there's, so there are a number of
kind of candidates for a scientific theory of consciousness. And you just mentioned two of
the leading ones, which are global workspace theory and integrated information theory.
And so global workspace theory. So that's, that's Bernie Baals's was originated by Bernie Baals and
has been developed by Stanislaus, Dehen and colleagues. So the idea there is it's, it does
rest on this sort of architectural idea, which is that, which is that we think of, of the brain,
the biological brain as comprising, you know, a very large number of parallel processes. This is
kind of a natural way to think of the brain, a large number of parallel processes. And it,
and the global workspace theory posits a particular way in which these, these processes
interact and communicate with each other. And that is via this global workspace. And
the idea there is that, is that there are sort of two modes of processing that go on. So in one
mode of processing, the, these parallel processes just do their, their own thing independently.
And in the other mode of processing, they are working via this global workspace theory. So the
idea is that they, you might think of them as, as, you know, depositing messages, if you like,
in this global workspace, which are then broadcast out to all of the other processes. So it's, so
there's this kind of, but I think thinking of it in terms of messages is not quite the right way
of thinking of it is better to think in terms of kind of signaling and information and so on. But
that's a natural way to think of it. But so the, so these, so in that mode, these processes are
sort of disseminating their influence to all the other processes. And that's the global kind of
broadcast aspect of it. That's when consciousness, that's when information processing is conscious
according to global workspace theory, as opposed to when it's all just local and the processes are
doing their own thing. That's, that's not that that processing is not conscious. So there's a
dist, so it's about teasing out this distinction between conscious information processing and
unconscious information processing. Now, all of those terms, by the way, are deeply philosophically
problematic. And to go in, you know, you have to sort of do it properly, you have to kind of unpack
them all in very carefully. And that's what my book try, try, tries to do. But so essentially,
it's about it. So the essential idea, though, is to do with broadcast and dissemination of
information throughout the brain and going from like local processes, and help them having global
influence. And that's what consciousness is all about according to global workspace theory.
Okay, so integrated information theory. So I think so integrated information theory, which is
Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible in some
ways with with global workspace theory. But I don't think that's, that's true. I think I think
that there's a lot of synergy between the two theories, in fact. But but that's because they
so they come with, so integrated information theory has sort of two aspects to it. So so
according to Giulio Tononi, he really is trying to pin down a property, which is almost like a
physical property, which is identical with consciousness. So you can actually speak about
the amount of consciousness in any system that you that you look at. Phi, he gives this, it's
Phi. So the Phi is a number how is actually a number of how much consciousness is present in
the system, like, like part of your brain, your whole brain, or you as a person, or a flock of
bats, or whatever. So you can or toaster, you know, so you can give a number to how much
consciousness there is there, according to his theory. And it's a mathematical theory based on
Shannon's information theory. And it's but it but it's all about trying to see how much
information is processed by the individual parts of the system, versus how much information is
processed by all of the parts put together. And it's it's and it's to do with how much the second
thing, you know, exceeds the first thing. And in a sense, and that is how much consciousness there
is there. And and and in a way, it actually has some synergies. If you as long as you don't think
that it's necessarily measuring, you know, this property of the of the universe, which you can
put a number on. But it has some synergies with global workspace theory, because they're both
distinguishing between global holistic things versus local things. And the consciousness is in
the kind of global holistic processing versus the local, you know, local processing in both
those theories. So there's a kind of, you know, there's some intuitions that they have in common,
I think. Interesting. And it also reminds me a little bit a little bit about what Chalmers
speaks about. So he thinks that it strongly emerges from certain types of information
processing. And the processing must represent causal structures as well. So it can't, it's
not an appeal to panpsychism per se. And although with with all of the things that you've just
spoken about, what do they work in another universe? And I guess what I'm saying is, is it just the
physical and the information processing or in a different universe might it not emerge in the
same way? Yeah, which depends what you mean by a different universe, I guess. What do you mean by
a different universe? Well, if the laws of nature were different. Yeah, okay, so if the laws of
physics were different. Well, I guess my, I guess I dislike isms. I mean, I'm an anti-ismist, or
rather I'd say I'm not an ismist. But if I were to, but I do sort of subscribe broadly to functionalism,
I suppose. So I guess, I guess I, but what do I mean by that? I mean, what I mean is,
I mean, I really dislike saying that I subscribe to these to these isms. So what I really mean by
that is that is that I imagine that a system that is organized in a particular way functionally,
in terms of its information processing. And if that system is in is embodied in the broadest
sense, and, you know, and meets lots of other prerequisites, then it's likely to behave in a
way where I'm going to naturally use the word conscious to describe it, perhaps, and where I'm
going to treat it like a fellow conscious creature. So, so, so it's so, you know, ultimately, it's,
I think it's about the kind of organization you need to give rise to the behavior you need
to talk about things, the thing in a certain way. My question to that, because I posed this
question to Chalmers last week, because he's also a functionalist. And I agree with the degree of
functionalism describing intelligence, but less so with consciousness, you know, there's not a
chewing test for consciousness, for example. But the thing is with functionalism, we're at risk
of doing what you said people do with large language models, which is anthropomorphizing
because these functions are intelligible to us. And then our conception of intelligence becomes
somewhat observer relative. Yes, do I? I mean, what I observe are relative. So
you understand these functions, so it's conscious to you, but not to someone else?
Well, so, so, so in all of these cases, I mean, I think it's about the words that we use in our
language to talk about the things. So the so, so if there's someone else is someone just like us,
right, then then we have to and if we want to use the words in different ways. So,
so the large language models are a great case in point, right? So, so suddenly we're arriving
at a point where somebody can describe something as conscious. And others can say that's rubbish,
you know, it's not that's not true at all. And so we've we've arrived at a point where these
philosophically problematic words, which, which we use in ordinary life quite, quite
harmlessly. And we all, you know, we all are in agreement about how we use the word likes
if somebody says, oh, you know, Fred has drank so much last night, he passed out, he was completely
unconscious, you know, I mean, or if an anesthetist says, yes, they, you know, the patient is now
unconscious, they can't feel feel pain. Or if you say, Oh, you know, I just wasn't aware, I didn't
see the cyclist. And you know, that's why I hit them, you know, I'm really tragic, but I just
didn't see them. And then we so, you know, so you're saying I wasn't aware of it. So that didn't
influence my action. So there we're using the terms in ways that we all understand. But now
we're getting to a point where suddenly, these words or these concepts are being used, you know,
we don't have an way we don't have agreement about how to use these words, right? Because it's
there are these exotic edge cases. Yes. So then the question I think that you you're getting is,
you know, is there a fact of the matter there, right? And so I'm very tempted to say the first
thing I'm tempted to say is that I don't think that perhaps is a fact of the matter. Or certainly,
I don't I don't want to, I don't want to speak as if there is a fact of the matter, but rather,
I think we need to arrive at a new consensus about how we use these words. So that might mean
that we extend the words, we break them apart, like I was suggesting earlier, maybe we need to
separate out awareness of the world from self awareness, from integration, cognitive integration,
from the capacity for suffering, because suddenly we have things that where that they don't all
come as a package. And when we need to kind of be a bit more nuanced in the way that we use these
words, we need to use them in new ways. But then there's a kind of transition period, because we
don't, you know, we're all arguing about how to use these words all of a sudden, because we've got
weird edge cases. So there's going to be a time when it'll take a time for language to settle back
down again. So there's a kind of, you know, there's a kind of observer relativness to this for a bit,
if you like, but then, but then there's a kind of consensus needs to emerge, right? But so many
things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were
possible. And what platonic? Because what we're talking about here is reductionism and the,
I mean, the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,
complex phenomenon beyond our cognitive horizon. And as much as we don't want to, we use functions
derived from behavior to have some common understanding of this thing. But I wasn't
being reductionist was I? Do you think I was being reductionist? Well, well, no. So you said
that the language game converges. And in some cases, we will arrive on a common definition,
but like you can bring in host data as well. Well, not a common definition, but a common usage,
right? So we'll come. So we'll come to use the words, you know, and with agreement, right?
So that's what I, and the reason why I mean, I would, and the reason I would balk at using
the word reductionist is because, and that's why I'm a bit resistant to functionalism as well
to any kind of ism is because I just think that that may be the way things are organized when
you take them apart. So, you know, brains, right? When you examine them on the inside,
like animal brains, you might look at how an octopus's brain works. And that might inform
whether you think that it suffers can experiences pain or not. Or we might break apart, you know,
an AI of system of the future, right? You know, and we might break it apart, and we may look at
its functional organization. And that all is just is grist to the mill of how our language might
change, right? So I'm not I'm not subscribing to the fact that consciousness is this or this is that
with some big metaphysical capital letters on the is, right? That's really important. So that so
the organized the functional organization of these other things, which when we study it and look at
is all just part becomes part of a conversation that eventually is going to help us to settle on
maybe new ways of talking about these things. I think we agree with it with each other. I think
the difference is so with the parable of the blind men and the elephant, all of the men around the
elephant saw something which was part of the truth. Yeah. And I think that's what we're
describing with the function. So we can all agree on what perception means or what some action means.
But the thing is, there will be many other functions that will represent a different slice of
that cognitive phenomenon. Yeah, I agree. And I think that's very much true with consciousness,
actually, because there's lots of people come in with kind of like new ideas and new theories. I
mean, you know, Annel Seth, for example, have you had an all on your own? Not yet being right.
So I also written this great book called being you. And and and and Annel brings in a whole kind
of, you know, new set of ideas he's particularly interested in, you know, the sort of top down
effects on perception top down effects on perception. So then he brings in this kind
of top down influence and perception as a big part of things. And then there's Graziano has
written this book using this about his attention schema theory of consciousness. And and that's
and you know, there's a whole set of interesting ideas there. And I think you're absolutely right.
I think there's I think there's aspects of all of these things all feed into, you know, all feed
into the way, you know, brains and animals work, and all of them feed into the, you know, why they
behave the way they do and why we use those words when we use those words, you know, conscious and
aware and so on. Fascinating. We'll get to your article in a second. But as someone who has such
a diverse and interesting background, who is allowed to ask these philosophical questions?
So it reminds me, Thomas Metkins, who is talking about the arguments between neuroscientists and
philosophers about freedom of the will. Yeah. And who gets to decide? Huh? Yeah. Well, what a
great question, you know, I mean, so why should I have any right to speak about any of these things
at all? Because I have no formal training in philosophy. So, so, so who gets to who gets to
Well, who gets to to I guess there are two things, right? There's I guess I guess there's
there's in that kind of discussion between the neuroscientists and the and the philosophers.
So there you know, they are not talking about, you know, the everyday conversation that we're all
having as as as humanity or as English speakers or as Chinese speakers about how we use these
these these words. So there it's a much more kind of confined to the two different two different
schools or disciplines within academia. So there I mean, I do think that
the people who work in AI and in and in neuroscience, probably at least should be
a bit familiar with with the philosophical debates. And you know, you mentioned Descartes
earlier on, and you know, you're familiar with just that that basic kind of, you know, sort of
stuff that which is like philosophy 101, which people should at least be aware of Descartes
arguments and then charmers and the different kind of perspectives on those sorts of things
before they pitch in, you know, at least I mean, you wouldn't pitch into neuroscience just by
making some up some stuff about brains if you hadn't read, you know, the an introduction to
neuroscience. And so so I think that the scientists need to, you know, you know, they need to kind
of have a past to enter the conversation, which is to have to have got through philosophy 101.
Yeah, it's so true. We have the same thing with the with the ethics folks, actually,
because because we have a lot of them fields of expertise. And engineers should learn more
about ethics. Yeah, absolutely. But when they do have an opinion about ethics quite quite often,
it's it's some, you know, it can sometimes be a bit naive. And, and, and, you know, at least
you should be familiar with the kind of, but but that's an interesting and a difficult area in
itself. Because of course, you know, you as a scientist, it's important that you take responsibility
as a scientist, and that you take, you know, some ethical responsibility. But at the same time,
you know, you've only got so much time to become an expert. So so it's difficult to at the same time
take ethical responsibility. And yet, you know, even though perhaps you haven't got the time to
kind of read, you know, read up and become an expert on the relevant ethics. So I mean,
perhaps everybody again, should, you know, get to the entry level, you know, ethics 101. And
indeed, many, many courses teach, you know, ethics, these days, many kind of science and computer
science, it's part of, you know, of any degree these days. So that's a good step, I think.
Yeah, there's an interesting analogy between the functionalism that we were speaking about
in consciousness. I mean, even in our own research domain, we have the function of ethics, and we
have linguists, and we have all sorts of different people. And that is the blind men and the elephant.
And you know, I tend to believe that even though the views from these diverse folks are inconsistent,
diversity is very important. Oh, incredibly important. Intellectual diversity is, you know,
every kind of diversity is important. And intellectual diversity is immensely important.
And having these conversations, these interdisciplinary conversations is absolutely,
you know, essential. So at least, if people are talking to each other, that's a really,
really positive thing, I think. Fantastic. Now, we invited Chalmers on our podcast
after Ilya Sootskeva's tweet. And by the way, Chalmers took a very functionalist approach to
talking about consciousness. But I guess, after that tweet, everyone in the community
started thinking about and talking about consciousness. So maybe let's just start from
that tweet. How did you find it? Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said,
it may be that today's large language models are slightly conscious. And then I replied,
tweeted back in the same sense that it may be a large field of wheat is slightly pasta.
And that, in fact, was actually, I mean, I've got a fair number of Twitter followers,
and that was the most engaged tweet I've ever sent out. And, you know, and, you know,
it got celebrity likes, hand-of-fried retweeted it. And, you know, it only is my kind of comment.
And so, but that does kind of summarize sort of what I think about, about what he said at that
point. But then, after tweeting my flippant response, I then I was violating all my own
Twitter rules in just sending back a flippant response, because I generally don't do that.
I would rather kind of, you know, be professional and engage professionally. And so I thought it's
very important to follow that on with a, you know, with a little explanation of why, you know,
why I thought that it wasn't really appropriate to speak about today's large language models
in those terms. And for me, the number one thing is to do with embodiment. So,
as I see it, embodiment is a kind of prerequisite for us being able to use that
word, use words like consciousness and so on, you know, in the way that we do in the normal,
everyday way of talking. So, you know, it's only in the presence of something
that inhabits our world. And by inhabits, I don't mean just has a physical, you know,
like a computer is obviously a physical thing in our world, but inhabits our world means that,
you know, walks around in our swims or jumps or flies or whatever. But it is inhabits the same
world as us and interacts with it and, you know, and interacts with the objects in it. And with other
creatures like ourselves. So that, to my mind, that is, that's the, so in
that paper, Conscious Exotica, I think I use this phrase, channeling Wittgenstein, that
only in the context of something that exhibits purposeful behavior do we speak of consciousness.
And the way that that's phrased, there is kind of, you know, so trying to channel Wittgenstein's
style of saying things, because you notice that he's saying that he's making what he's
saying is actually he's talking about the way we use the word. So he's not making a metaphysical
claim. This is essential. He's saying that this is just this is the circumstances under which we
use this word. So we use this word in the context of fellow creatures, basically. And so, so that's
kind of the starting point. So a large and of course, we of course, we talk to people on the
telephone and over the internet and so on. And we don't, you know, we may not, you know, we can't
see them or anything. So we, but we still we know that there is, you know, or we assume,
we've always been able to assume up to this point that there is a fellow creature at the
other end. And that's the kind of grounding for kind of thinking that way and using that word.
That is absent in large language models. So large language models do not inhabit the world that we
do. Now, I mean, we have to caveat that because of course, it's possible to embed a large language
model, you know, in a, you know, we always do embed it in a larger system might be very simple
embedding, it might be just a chatbot, or it might be much more complicated, like it might
be be part of a system that enables a robot to kind of move around and interact with the world
and take instructions and so on. So there's a great, some great work from Google with their
palm say can robot, for example, where there's this embedded large language model. So, so,
so there you're kind of moving in a, in a direction where maybe where these, where these words, you
know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.
Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,
right? Yes. But we can imagine that, that we can imagine that requirement being met for, for,
for, not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,
but at least it would, you'd meet the necessary conditions, right? But the large language models
by themselves do not meet even, they're not even candidates. Yes, I agree. And we, there's so many
things we can do here, because we can, we can talk about embodiment in general. I mean, as I
understand it, Rodney Brooks kind of started the phenomenon of thinking about a representationalist
view of artificial intelligence or, or rejecting, rejecting a representation.
Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best
representation, which is absolutely fascinating. Yeah. And then you, you, maybe you might be
thinking about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity
and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your
paper, you said, although the language model component of SACAN provides natural language
descriptions of low level actions, it doesn't take into account what the environment actually
affords the robots. And there's this whole affordance thing as well. So, I mean, how do you think
about embodiment? So, so the way I see it is that the, is that the, you know, the one exemplar we
have as of, you know, the end of 2022 of something that we really can describe as, as, as, as
intelligent as generally intelligent is, is the biological brain, biological brains of humans,
but also of other animals. And the biological brain, you know, at its very, it's very kind of
nature is it's there to help a creature to move around in the world, to move, right? It's there
to move, help to guide a creature and help it move in order to help it survive and reproduce.
That's what brains are for. So, that's what that, from an evolutionary point of view, that's
that they developed in order to help a creature to move. And they are, so they, they, and they are,
you know, they're the bit that comes between the sensory input and the motor output.
And as far as you can clearly divide these things, which maybe you can't, but I mean, so, and so
that's, that's their purpose is to intervene in the sensory motor loop in a way that benefits
the organism. And everything else is on built on top of that. So, so, so the capacity to, to
recognize objects in our environments and categorize them and the ability to kind of manipulate
objects in the environment, pick them up and so on. And all of that is there, you know,
initially to help the, the, the organism to survive. And, and, and, you know, and that's what
brains, brains are there for. And then, then when it comes to, you know, the ability to
work out how the world works and to, to do things like figure out how to gain access to some item
of food that's difficult to get hold of, then all kinds of cognitive capabilities might be required
to understand how you get inside a, you know, a shell or something to get the fruit inside it
or something like that, complex cognitive abilities, that sort of. And then, you know,
evolutionary evolution has developed a lot of more and more and more complex cognitive
cognition until we get to language and we need to interact with each other because that, that's
all very much a part of it, the social, social side of it. And then language is part of that.
So as I see it, it's all built on top of this fundamental fact of the embodiment of, of the
animal and the organism. So that's in the biological case. So that's the sort of our starting point.
So, and so that seems to me to be the, the most natural way to, to understand the very nature
of intelligence. Could I frame it? I think I didn't, I didn't frame it very well. I mean,
Melanie Mitchell recently had a paper out talking about the four misconceptions and
AI. And one of them, of course, citing Drew McDermott was the wishful mnemonics and the
anthropomorphization, which, which you've basically spoken about. But, but her fourth one was about
embodiment. And she spoke about this in her book as well. And she said that one of the misconceptions
of AI is that people will have this notion of a pure intelligence, you know, something which works
in isolation from the environment. And you're talking about social embeddedness and embodiment.
And I guess my point with the complexity argument is I'm saying that the brain itself doesn't
actually do everything. It kind of works as part of a bigger system. Oh, I see what you mean. Yes.
Okay. Yeah. Yeah. So there's, so of course, there's, I mean, I noticed in one of your previous
interviews with Andrew Lampinen, you mentioned the three E's frame, we're called four E's these
days. And of course, that's very much part of it is the, is the idea that, you know, there's the
extended mind, we use the environment, you know, as, as, as a kind of memory, for example,
we deposit things in the environment, writing, you know, as an example and so on.
And then there's, people talk about morphological computation, I'm sure you're familiar with that.
So, well, so that's the idea that the very shape of our bodies, you know, is, is, is, is, you know,
so, so, so sometimes, you know, the aspects of intelligence are actually outshort outsourced
into the physical shape of your body. So where you might think about designing a robot, where you,
where you put a lot of work into the control aspect of it, so that it's that it can kind of
kind of walk in this very carefully engineered ways that it's always permanently stable, or
alternatively, you can make a body that is naturally sort of stable, or maybe naturally
unstable. And what you do is you kind of rely on the combination of the physics of it constantly
falling with, with a control system that constantly restores balance. So that's, so, so, you know,
that's, so that's, that's, I mean, this is very much a Brooks type perspective and people picked up
on Brooks's ideas and extended them in this sort of way. So I think that's, I think that's a very
natural way of thinking. But in a way that this gets to the, to the complexity argument, because
I guess what I'm saying is that life is much more brittle than anyone realises. You were just
pointed to some sources of brittleness that most people never would have thought of, which is,
which is fascinating. So I think there's a very important relationship between embodiment and
language. And this also brings us back to Wittgenstein as well. So, so for us as humans,
language is inherently an embodied phenomenon. It's, it's, it's something that is,
it's a social practice, something that take that that is a phenomenon that
occurs in the context of other language users who inhabit the same world as we do,
and where we have kind of like joint activities, we're triangulating on the same world and we're,
we're acting on the same world together. And that's the, that's what we're talking about
when we use language. So there's this, so that that's an inherently convict,
constrain view of language. And I think it's deeply profoundly correct to have a view of,
of language. That's, that's what, that's what language is there for us is so that we can
talk about the same things together so that we can, our collective activity is, is, you know,
is, is, is organized to some extent thanks to language. So that's, so I think that's a really
important perspective on language is Wittgenstein perspective. And, and embodiment is at the heart
of it, embodiment and inhabiting the same world as our other language users. And, you know, that's
the way we learn language, we learn language by being around other language users like our parents
and carers and, and, and peers. And, and that's again a very important aspect of the nature of
human language. Now large language models, they learn language in a very different way indeed.
They do not inhabit the same world as us. They do not kind of sense the world in the way that we do.
They don't learn language from other language users from their peers in the way that we do.
But rather, you know, we know how large language models work, there's trained on a very, very
large corpus of textual, of textual data. So an enormous corpus of textual data so bigger than
any human is likely to encounter, you know, you know, by the time they become a proficient language
user at a young age. And what they're trained to do is, is not to kind of engage in activities
with other language users, but to predict what the next, you know, what the next token is going to be,
which is a very, very different sort of thing. So they're very, very different
sorts of things. And the, and the role of embodiment is really really important in this
difference thing. Yes, absolutely. When I spoke with Andrew Lampinen, he's really, really interested
in the grounding problems. I mean, would you mind just speaking about that a little bit before we
go into your paper? Yeah, absolutely. Yeah, yeah. So of course, this goes goes back to a great paper
by Stephen Harned back in, I think, 1999 or 1998. So the one and only. Yeah, the one and only on this,
the symbol grounding problem, it was called. And, and, and, you know, he does argue broadly that,
that symbols in AI systems, the kinds of AI systems he was thinking about at the time were kind of,
you know, sort of expert systems say or something like that. And the symbols there are not grounded,
they're provided by the human programmers and just sort of typed in. Whereas for us, for us,
the words we use, those symbols are, are grounded in, in our activity in the world. So that when
we use the word dog, that's because we've seen dogs. And we've talked about dogs with other
people who've also seen dogs. And we've seen dogs in lots of different circumstances. And we've also
seen cats and, and, and, and, and dog bowls and bones and many other things that all kind of
contextualize that. But all of that, that, that is kind of grounded in the real world in our,
and in our perception of the things in question. So that, so that's this, so that's what sort of
is meant by grounding, or at least that's the original meaning of the word grounding from
Stephen Harned's paper. And I think that's a really, really important concept because,
because, you know, in an important sense, large language models, the symbols that are used in
large language models are not really grounded in that kind of way. Now this, you know, I should
be absolutely clear that large language models are immensely powerful and immensely useful. And,
and, and so that, you know, so, but it's interesting that to what, to what extent the lack of grounding
here that we have in today's large language models, you know, might affect how good they are.
So, so they, so they are prone to kind of, you know, hallucinating and, and, and confabulating and,
and if you look at multimodal language models that maybe we'll talk about an image that you present
to them, then, you know, you can have a very interesting conversation, but sometimes they'll
go off pieced and start talking about things that are not in the image at all and as if they are.
And that's sort of because due to a kind of, I would say lack of grounding, so that, so the words
are not kind of grounded in the images in, in quite the way that we would like. So that's,
it's an important topic of research, I think. Yes, indeed. And although some people do believe
there's this magical word called emergence and possibly some emergent symbol grounding might
be possible, maybe, maybe, shall we just put that to bed before we introduce your,
yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.
And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence,
I think in today's large, large language models. So, so, so, you know, even though they're,
they're, so they're basically trained to do next word prediction, or I mean, to be clear,
I suppose I should have made this maybe a bit clearer in the paper, but of course,
it's not always next word prediction, because there are different models learned to actually,
you know, predict what's in the middle of a, of a sequence rather than kind of generally,
you know, they're interested in, in, in, in, in, let's take the next word prediction case as canonical.
So, so they're, so they're, so they're trained to just to do next word prediction. Now the
astonishing thing is, as I think GPT three showed us, is that, is that just that capability,
if it's sufficiently powerful, can be used to do all sorts of extraordinary things. Because
if you provide, you know, the prompt that describes, you know, describes some kind of
complicated thing, you know, situation, like, you know, I need to tile my floor and my floor is
shaped like an L and it's 20 meters long and three meters. Well, you know, you start to describe this
thing, you know, in each, each tile is, is 20 centimeters square, how many tiles will I need
and, and some large language model will come back and tell you, you need 426 tiles, whatever,
and it's correct, right? Well, this is astonishing, because it was only trained to do next word
prediction. And so there's a kind of emergent capability there. Now there's a sense, of course,
in which it still is just doing next word prediction, because in the vast and immensely
complex distribution of tokens in human text that's out there, then the correct answer is
actually the thing that's most likely to come up. And that's, but it's got to discover a mechanism
for producing that, right? And so that is where the emergence comes in. And I think that, you know,
these capacities are astonishing, the fact that they, that it can discover mechanisms, you know,
emergently that will do that sort of thing. Yes. And maybe I shouldn't have used the word magic
with it with emergence. I'm a huge fan of emergence. And, and as you say, the decoders
trained to predict the next token or the denoising autoencoders to, to, let's say, fill in the gaps
in the middle. And I guess there are different ways of thinking about emergence. So there's weak
emergence, which might be thought as computational irreducibility or a surprising macroscopic change
or strong emergence when it's not directly deducible from truths and the low level to make
it, you know, lots of things. Yeah, yeah, the different senses of it. Yeah. Exactly. But I guess
my point is that remarkably, it's trained on something quite trivial. So all of this is about
convention, right? All of this is about what's, what, what is the, what is a good way to use words,
right? So I don't, so I don't think, you know, I'm not making metaphysical claims about, about,
about these things. So it's all about what, you know, when is it appropriate to use words,
to use certain words? And, and because when, when this becomes problematic is when they're
philosophically difficult words, like believes and thinks and so on. Now, when it comes to reasoning,
so, so I do think that we, I do think it's not unreasonable to, to, to use that term to describe
what some of the, these models do today. And that's partly because of the content neutrality of,
of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the paper
comes back to this sort of whole embodiment thing really. And, and I'm, I'm saying, well, you know,
in the kind of like, ordinary way we use the word believes, well, it gets, it gets complicated
because we do use the word believes it in this intentional stance way to, to talk about ordinary
everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's British summertime,
you know, you know, because we, and then, but then you'd say, then somebody says to you,
what, what, you mean your, your car clock can think? You say, no, obviously, I didn't mean
that it can think, it's just a turn of phrase, you know, when we, when we get to these large
language models, and we start to use the words like thinks and, and believes and so on, because
they're so powerful, it starts to get ambiguous. And yours, and your, and, you know, when, and
some people will say, well, actually, I really did mean that it can think, or that it believes.
So I'm, so I'm, I'm interested in this, when things get difficult in this respect.
And could, could you tease a part there? So you resist anthropomorphic language in terms
of belief, knowledge, understanding, self or even consciousness, but less so with reasoning. And I,
my intuition is that reasoning rather depends on those things that I just said before.
Well, I, so I think it doesn't because, but this is, but perhaps this is just
maybe in a kind of formal logic sense, because, because reason, because logic is
content neutral. So if I tell you that every, could you just explain what you mean by that?
Okay, so, so Lewis Carroll has all these wonderful kind of like nonsense syllogisms, right? Where he,
where, you know, he says, oh, if all elephants like custard and, you know, Jonathan is an
elephant, then Jonathan likes custard. And, you know, all kinds of things like that. And it's all
sort of nonsense. And he has this big complex, complicated ones. Similarly, I could tell you
that all, all sprog forths are plingy, and, and Juliet is a sprog forth, therefore Juliet is,
is a splingy, right? And I've no idea what any of those things mean, but the, the, but it's
because it, because it, for the pure form of the reasoning, you don't have to know what they mean.
It's just about the logic. So, so in that sense, you know, it just in the way that a
theorem prover can do logic, then so can a large language model do logic. So in that sense, I think
large, it is reasonable to use the word reasoning in that logical sense in the context of large
language models. I don't think that's a problem. Of course, we may think that they do it badly,
or they do it well, or that's a whole other thing, right? But, but at least the word is
potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment is a,
is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's not
content neutral, right? So if you, if I believe to use the example in my, in my paper, if I believe
that Burundi is to the south of, of, of Rwanda, well, whether that is the case or not, it does
depend upon facts that are out there in the world. And then to, to really have a belief, at least
you've got to be able to somehow try and kind of justify those facts, or at least, and you've
got to be at least built in such a way that you can, you know, interact with the external world
and do that sort of thing, right? And verify that something is true or false, or do an experiment,
or, you know, or, or ask someone, or, you know, you've got to go outside yourself, right? We go
outside of ourselves and, and in order to establish whether something a belief is true or not. And so
you've got to at least be capable of doing that. Whereas large language models, the bare bones,
large language model is not capable of doing that at all, right? Now you can embed it in a
larger system. This is a really important distinction that I've tried and make over and again in the
paper, I talk about the bare bones, large language model. So you can take the, so, so, and whenever
a large language model is used, it's not the bare bones, large language model, which just does sequence
prediction, but it's embedded in a larger system. When we embed it in a larger system, well, that
larger system maybe could consult Wikipedia, or maybe it could be part of a robot that goes and
investigates the world. So that's a whole other thing. But then you have to look at each case in
point and, and, and ask yourself whether it's a, whether, you know, whether we really want to use
that word in, in, in anger, you know, as in, in its full sense rather than just in the intentional
stance sense of a kind of figure of speech. So, and so in the case of, of, of like chatbots, for
example, today's chatbots, not really appropriate, I would say, we're not using the word in the way
that we, in the full blown sense that we use it when we talk about each other.
Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful way of
thinking about artificial intelligence, allowing us to view computer programs as intelligent agents,
even though they may lack the same kind of understanding as a human. And then you cited
the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of
Bob, it was used in the traditional sense for bot, it was used in a metaphorical sense. So it kind
of like, it's just distinguishing what it means to know, you know, for humans and for machine.
So I think it's, it's useful to think about something like Wikipedia. So,
so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?
And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia,
and somebody might say, oh, Wikipedia doesn't know yet that the Argentina have won. And so when
we use a word like that, you know, nobody's going to kind of say to them, say to somebody who uses
that word, hey, you know, I don't think you should use the word knows there, you know, that would,
you know, you should be a bit more sort of sensible. I mean, it's fine to kind of use, I think, these
kinds of words in this ordinary, every day sense. And we do that all the time. And that sort of,
particularly, particularly in the case of computers, that's adopting what Dandenek calls the
intentional stance. So we're, so we're interpreting something as, as, as, as having beliefs, desires
and intentions, because it's a kind of convenient shorthand. And especially if you've got something
that's a bit more complicated, like say your car sat nav or something, or your, you know,
your sat nav on your, on your phone, then it sort of makes makes sense to use those words,
it's a is a convenient shorthand, and it helps us to kind of talk about them, right, and without
getting overly complicated, without knowing the underlying mechanisms. But there's an important
sense of which we don't mean it literally. So you know, in the case of Wikipedia, you can't,
you couldn't go up to Wikipedia and pat it on the shoulder and say, hey, Argentina have won. And
there's no way, you know, right, I want to be a, you know, and, and, and, and all, and all the
things that that go with us as humans actually knowing things. And it's just a turn of phrase.
Now, things get sort of interesting with large language models and with large language model
based systems and the kinds of things that we're starting to see in the world, because
we're starting to get into this kind of blurry territory where it where we're blurring between
the intentional stance, and, and, you know, meaning the meaning it literally. And this is
where we need to be really, really kind of careful, I think. So at what point does do things
shade over into where it's legitimate to use that word, you know, literally, in the context of
something that we've built, you know, I don't think we're at that point yet. And we need to be
very careful about, about using the word as if we were using it literally. You know, that's the
sort of anthropomorphization, because the problem is that we can then impute capacities to the thing
and, and, and, or even, you know, empathy say that just isn't there. Yes. I suppose we could
tease apart knowledge. So it justified true belief from nose, because nose that it brings all this
baggage of intentionality and agency and anthropomorphization. But you had Chomsky, you've
had Chomsky on. I can tell you a story about that. I mean, the recording messed up. So when we were
interviewing him, we were only getting bits and pieces. And we had to deep fake him. We had to,
we had to regenerate the interview. Oh, really? And he was saying in the entire interview how much
he hated deep learning and how useless it was. And then we used deep learning to rescue his
interview. And he gave us literally did that. Yeah, she told him. Yeah. And he gave us his
permission to publish it. That is wonderful. So it's quite ironic. But no, he always says it's
wonderful for engineering, but not a contribution to science. Yes, sure. Yeah.
Yeah, he said, I like bordeaux's too. They're good for clearing the snow, but they're not a
contribution to science. So who else have you had? I mean, you've had a lot of people on. I
listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great. So he's great.
It's Andrew, somebody I do work with quite closely. Oh, wonderful. So it was interesting
listening to him because Andrew had quite a big influence on this paper, by the way.
Oh, okay. And I mean, you know, but I think I might have had a bit of influence on him as well,
to listening to it. I think so. Because that interview was just after he'd read,
and he read my paper and gave me lots of comments. And we had a lot of discussion about it. And that
interview, looking at the recording date was sort of just after this. And it's interesting. I mean,
he was very circumspect in some of the things he said. Yeah, it was very interesting because he,
I think the influence has maybe gone both ways. Yes. Which is nice. I don't know. I can't be
sure of that. I think there's a huge similarity. Yeah, I was thinking that actually, just when
you were speaking. But it's funny because, you know, because we've spent a lot of time arguing
with each other about it. And I often feel like we're coming from very different perspectives
on this. But in fact, you know, I think there's, there's convergence really.
What are your areas of disagreement?
Well, you see, I would have thought that Andrew would have been more on the side of,
you know, we can do things without embodiment and without grounding or to kind of take grounding in a
you know, in a more liberal sense. Because some people take, you know, talk about grounding.
So they say, well, you know, the large language models, they are grounded.
Prompt engineering is the process of using prompt prefixes to allow LLMs to understand better.
You know, so the context and the purpose of a conversation in order to generate more
appropriate responses. What do you think is going on with prompt engineering?
Well, yeah, so you let's probably let slip a phrase there. So the process of allowing
the models to understand better is what you're better. Of course, I don't think guilty is charged.
I don't think I don't think that's the right way of characterizing it at all.
So I mean, I think it's the whole thing of prompt engineering is utterly fascinating.
And it's something that's entered our world as AI researchers very prominently,
just in the last two years. And it's amazing. Of course, we have prompt engineering in the
context of large language models. We also have prompt engineering in the context of,
you know, the generative image models as well, like Dali and so on. And that's really fascinating
as well, how by engineering the prompt to be just the right sort of thing, you can coax the
model into doing something which you might not otherwise do. And it's a great example of how
alien these things are, because if you were giving a human being the same instructions,
then you wouldn't necessarily do quite what you do with either an LLM or an image model
in order to get it to do the thing that you want it to do. You have to kind of
you have to kind of get into the zone with these models and figure out kind of what
strange incantations are going to make it do the things that you want it to do.
Now, I think an interesting thing is that we may be looking at a moment, a very short moment
in the history of the field where prompt engineering is relevant, because if language
models become good enough, then we're not going to need to talk to them in this weird way,
engineer the prompt to get them to do what we want them to do. It's going to be a lot easier,
but anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be
the case as they get better. But at the moment, you can use a strange incantation like thinking
steps and suddenly the large language model will be much more effective on reasoning problems than
it was if you didn't use the incantation thinking steps. So that's really fascinating. So what's
going on there? Well, I mean, I think what's going on there is that we have to again bear in
mind that what the model is really trained to do is next word prediction. But we have to remember
that it's doing next word prediction in this unimaginably complex distribution. So we have
to remember that it's not just the distribution of what a single human would, the distribution of
the sequence of words that a single human will come out with, but of all the sort of text of
millions of humans on the internet, plus actually a load of other stuff like code and
things which we don't come out with in ordinary everyday language. Well, people do it deep
mind a bit, but that's deep mind. So it's this unimaginably complex distribution.
And so I think what's happening with prompt engineering is that you're sort of,
you know, you're kind of channeling it into some portion of the distribution.
So you're queuing it up, you know, with a prompt and this kind of context is putting
it into some portion of this distribution and that is what's going to enable it to do something
different than it would have done if you had a different set of words and that would have put
it in a different part of the distribution. So you're kind of finding the bit of this
unimaginably complex distribution, you're finding the bit of it that you want to then
concentrate on. Yeah, so intuitively, I agree, because I think there's two ways of looking
at this. So I agree with you that they are statistical language models. I'm also a fan of
the spline theory of neural networks, which is this idea that you just kind of tessellate the
ambient space into these little affine polyhedra. And it's a little bit like a locality sensitive
hashing table. But that's quite, it's quite a simple way of looking at it, because you were
talking about emergence before. And emergence is all about this paradigmatic surprise, a bit
like the mind body dualism, if you like, there's something that happens up here, which is paradigmatically
completely different to what happens down there. So on the one hand, we're kind of saying, oh,
they're just simple interpolators or statistical models. But on the other hand, they really are
doing something remarkable up here. So which is it? Which is it? Well, I mean, it's both, right?
So if we want to understand these models in a more scientific way, which we surely do,
even if we're not engineering them in an old fashioned sense of engineering them,
but rather they kind of emerge from the learning process, we still want to reverse
engineer them to try and get as great as comprehensive scientific understanding of these
things as possible. So we want to understand at all these levels, right? Of course, the foundation
of that understanding is that we need to understand the actual mechanisms that we've programmed in
there, right? So that's essential. If you want to really understand these things, you've got to
understand transformer architectures, the different kinds of transformer architectures that you've
got, what happens when you use different parameter settings, whether it's sparse or dense, whether
it's a decoder only architecture or how you're doing the tokenization, how you're doing the
embedding, when all of these things are essential to understanding, and that's all absolutely at
the engineering level. So we want to understand all of that. But then we can do a whole load of
reverse engineering at another level and do the sort of thing that the people at Anthropic AI have
done, for example, with these induction heads and understanding in terms of transformers,
in terms of residual streams and induction heads, which I think is fabulous work. So that kind of
thing is looking, it's still quite a low level, but it's kind of the next level up and explaining
a little bit about how these things work and work along those lines, I think is like really essential.
And then the more complex these things are, the more we need to kind of ascend the
these levels of understanding and I hope that we can. But I mean, there's no one that is the
right one. You want to understand that things are all levels. Yeah, different levels of description.
You said something before, which really interested me. You said when the language models get good
enough, maybe we won't need the prompts anymore. And I'd love to explore that duality, because
it's a similar duality to how we talk about embodiment. You can think of the language model
being embodied in the prompt in some sense. So maybe we'll never get rid of the prompt. But
just to think about these prompts, I think about them as a new type of program interpreter.
And there are some remarkable examples of scratch pad and chain of thought and even algorithmic
prompting for getting insane extrapolative performance on lots of standard reasoning tasks.
Yeah, yeah. And these models are not Turing machines, they're finite state automators,
so there are limits to what we can do. But I guess what I'm saying is the prompt seems like
it's not going away anytime soon. Yeah, so I think that I don't think the prompt is going to go away.
But I think that the, and who knows, right? But I think that prompt engineering as a whole kind
of thing in itself, you know, may, it may not be, you know, people talk about that as being a kind
of a whole new job description, you know, prompt engineer. And so that as a whole new job description,
I'm not quite sure how long exactly that will last because, because prompting may be just,
you know, interacting with a thing in a much more natural language way in the way we would
with another person, right? So, you know, I don't, I don't, when I, when I don't have to kind of
think of some peculiar incantation in order to, you know, in order to get, you know, my colleagues to
kind of help me on something or to, you know, to cook a meal together with somebody. We just
use our natural kind of forms of communication. And of course, of course, it does involve,
you know, discussion and negotiation, but it's in this, it's just the same as we use with other
humans, right? So, so it may be that, that rather than it being a peculiar thing in itself with
all these funny phrases that just work for peculiar eccentric reasons, that it may be much more
natural. Amazing. Professor Shanahan. Thank you so much for joining us today. Thank you very much.
Indeed. And thank you for the invitation. It's been lots of fun. Absolute honor. Absolutely honor.

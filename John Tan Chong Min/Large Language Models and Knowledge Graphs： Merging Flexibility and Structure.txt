Hi everyone and welcome to today's session. Today we'll be talking about a very exciting
topic which tries to merge large language models and knowledge graphs together. So as
you all already know, large language models are the recent hype. You can literally do
anything or a lot of things with large language models by just changing the prompt. It is very,
very flexible. Get knowledge graphs extremely rigid. Okay, you have notes connected to other
notes in relations, but they are also very informative because the relations don't change.
The notes don't change. The large language models, one problem that they face is that
they are a little stochastic. They tend to generate things that may not be grounded in facts.
So it seems like naturally these two approaches seems like a good fit together. One is more
flexible, which is the large language models. And one is more reliable, like the knowledge graph.
Okay, so without further ado, let's begin today's topic. So I will roughly follow the
framework of this paper called Unifying Large Language Models and Knowledge Graphs,
a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this
paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the
latest large language model stuff. They are like the births, raw births, like basically the 2017 to
2019 era, that kind. So I supplemented it with some of the more recent advancements,
like some length chain stuff. So enjoy. Do feel free to comment anytime because I think
this is a very interesting field that can be expanded upon. Okay, never before have we
gotten large language models this powerful like chat GPD. And this is really something that we
can look at to improve on traditional methods, or even think of a new method that is not even
a knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons
of knowledge graphs and large language models? So as I said earlier, you can look at the rough
summary. I think this is quite a good summary. Okay, large language models, they are generalizable.
Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of
understanding of facts. Okay, general language understanding. Okay, this one is debatable,
because like GPT-4 can be said to understand language pretty well. Like NLU is like the ace
most of the task there. Okay, so this one may be not so true in an understanding, but for facts-wise,
fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like
sometimes they might generate things that don't answer the question fully. Okay, but increasingly
this is not really a problem anymore. Okay, it's more of like the reliability right now. So I summarize
this part here. Reliability. Okay, I should use a different color. Let me just change my annotation.
Now, so I think the main thing that large language models lack are reliability. Oh no, it's the same
color. Let me see. Reliability. And consistency. This too. I mean, y'all have tried large
language models before, right? You key in the same problem. Okay, sometimes you get the different
responses. Sometimes the response can be different. Like I said, it's a hot day today,
right? Yeah, screams on us can be yes, sometimes no, you know, that kind of thing.
All right. So knowledge graphs, what do they have? Knowledge graphs have structure knowledge.
They are quite accurate. Okay, decisive, I guess you can find a way to like connect an input note
to an output note. You can say yes, there's a link between them. It's very interpretable.
Okay, actually large language models are also quite interpretable. So it's not really a con here.
Large language is both actually interpretability is also in large language models. Domain specific
knowledge. Yes. But actually, if you think about it, large language models with context grounding
also has domain specific knowledge. Okay, evolving language. This is something that is quite
interesting. Large language models don't really have this evolving language unless you fine tune
it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something
like a knowledge graph to ground the large language models. Can you see the synergy here?
There's a lot of things that knowledge graphs do well that is not exactly antagonistic or not
exactly different in nature from the large language model. It can just be used to ground
the large language model. So it's very interesting. So what are the cons of large language models?
They hallucinate, black box, black domain specific knowledge. So it looks like there can be some
synergy here. And let's explore how we can synergize these two approaches. Before we move on any
quick questions so far. Okay, so this is the one way of getting contacts into a large language
model and is used very often nowadays. It's called retrieval augmented generation. So
this is the raw format you retrieve from a from a corpus of documents. You have a few
documents here that you can retrieve from. And then maybe the user asks like,
how much is a MacBook Pro?
All right, recently I need to ask myself this question because I'm considering whether I
should buy another one. So, you know, they retrieve the relevant documents like, okay,
this document is about MacBook. Okay, you retrieve the right documents. Okay, this document here is
about maybe 2019. You can retrieve the right documents. All this, okay, all these documents
will actually be your your contacts over here. So you could have the contacts retrieved like that.
MacBook Pro 2019 costs 5000 or something like that. And you can have like in 2019
Apple release MacBook Air 2019. Of course, I mean, I don't really know the details,
but let's say these are the two documents we really you retrieve from your retrieval augmented
generation. Okay, and after that, you ask the question like, how much is a MacBook Pro
2019. So it's been shown that if you use retrieval augmented generation, you can improve the
consistency of the output of the large language model, because you are grounding it in the earlier
context, which is this part here, you are grounding it in this part here. So there's an element of
grounding. And this is very important for a lot of real world use cases. Because if you don't ground
it, you can end up with quite nonsense generations. All right. And just as a refresher, okay,
what is the most common method used to select the top K, like documents, anyone can just blow
up. What's the most common metric to select the most relevant documents?
That's a test of understanding. If you are doing retrieval augmented generation, what is the most
common metric used to retrieve documents to check the similarity? Anyone? You can write in your
chat also. Dot product. Yes, very good. Dot product or cosine similarity. That's right. So usually,
we use some form of embeddings. You embed your documents into a vector. And then you use cosine
similarity to check how similar the document is compared to the query. I'm going into some details
over here, because actually, this whole process of doing retrieval augmented generation and passing
over knowledge graphs is very, very similar. In fact, you could even replace this retrieval
augmented generation with knowledge graph augmented generation. It's perfectly, I think,
is replaceable. All right. So this is some idea of how large language models can be made to be more
accurate using something like that. Okay. So this, again, I just highlight the problems of
large language models. Okay, may not be able to recall the knowledge, but you can retrieve the
right context using this retrieval augmented generation provided you can retrieve the context
correctly. All right. So this is a real world use case issue. All right. I've talked to some people
and they say that retrieval augmented generation with just the cosine similarity alone, okay,
might not give you the right documents. So, you know, embedding vectors, train using contrastive
laws, you know, they may not capture everything, especially if your document is very, very large.
Okay, imagine you have only one vector to represent the entire document, and you have another
vector to represent document, another document. So this is like document A,
and another vector to represent document B, then you see how similar they are.
But what about like, what if one document contains many parts?
Right. I mean, each of these parts could have different meanings, right? Each of these subparts
could contain like, let's say you have this document could have a subpart that's like that,
a subpart that's like that, a subpart that's like that. You know, then you just aggregate all of
this together into one vector like that. Can you see that you're actually losing like information
here? Which means that when you retrieve something, let's say if I want to find out how to code,
like a length chain question answer agent, you know, I'm not going to retrieve this vector because
by vector similarity, my query is here. All right. By vector similarity, maybe I'll retrieve a
document that is like B instead, because like maybe it's nearer in terms of cosine similarity.
Okay. I mean, it's greater is the opposite direction. Let me just make this vector look
more similar to that. Like, let's say I have B is like that. So if this direction here is like,
how to code a length chain QA agent. And this is the embedding vector for it. It goes in this
direction. You know, you're not going to retrieve document A, although it contains that part over
here. Okay, you're going to retrieve document B. This is one of the failings of the embedding
vector. It just tries to capture the whole document into one vector. And this means that
you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying
context is king. Summarization is essentially impossible on segmented documents. Yeah, definitely
because you summarize, you lose information. Okay. So there needs to be like different hierarchies
of how you retrieve things out, broad level, specific level. And you know what? Knowledge
graphs might actually have that kind of hierarchy formulation. Okay, I'm actually jumping a few
slides ahead, but give you an idea of why, why I'm so excited about this idea. All right. So actually,
some of the bypass that I've been telling people, I've been advising people is that like, if you
cannot get retrieval augmented generation to work, consider using like filters or like labels.
So like this labels will say like, okay, maybe it's like product A, product B. So you know,
instead of relying on just the retrieval of method generation, or the embedding vector to
actually embed the right knowledge, let's say you have a length chain QA agent, I can tag this
thing as a length chain QA agent inside this document. So there will be certain tags that
you can have. So then you can then do like the embedding vector across the documents that have
these tags. So maybe that's one way to like do a first hand filtering. I mean, this is just like
some, what do you call it? Some bypasses to the downsides of embedding vectors for pros and
similarities. So these are some ideas that could be done right now to bypass it. But if we have
a way to use knowledge graph to do more broad level to more specific level extraction,
maybe you don't even need all this. You can just pass through your knowledge graph and you can use
that to ground the large language model. All right. So this is my last point here. Knowledge
graphs are useful to retrieve the right context, search the right keywords, retrieve the right
subgraph. Like let me give you an example here. If let's say I have a graph like that.
All right. So maybe this is a graph talking about like people who view Netflix. Okay. So
these are the Netflix user graphs. So this is like these are users. And then maybe you have me
over here, John. Okay. And then like the movies are watched. I like to watch the flash, the series,
not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here.
Watch other movies like movie A and movie B, you know. So if you want to like
extract something out here, you can just search like for keywords and then you can just put this
whole subgraph here and then you can use this part here. Okay. How you want to pass it into the
large language model? I leave it for future investigations. There are a few ways to do it.
I will cover some ways today. So if you can pass this into the large language model,
essentially you can ground the LM in context of the knowledge graph. Okay. And then we can
actually do this grounding at a more higher level grounding or more sub level grounding depends on
which height of the knowledge graph you're going to take the notes from. All right. So I think this
is a very exciting prospect. And yeah, I'm looking forward to see like how this can actually work.
Also, I'll be actually working on getting this to work the next few weeks. All right. Because
I think doing something like that actually might help with the up challenge as well.
The abstraction and reasoning corpus. So this is my latest kind of
headway that I'm going into. So this is from the knowledge graph conference. Okay. I actually
listened to quite a few of their videos. This is the knowledge graph conference 2023. And
there's a speaker Danny front desk six. I think I pronounced his name wrongly. But
the idea is that if you are using chat GPT for your own applications, if you use chat GPT in
different languages, you might get different outputs. Okay. Even for the same information. So
you know, being Singaporean and you know, the presidential election is coming soon,
I just asked like who is Singapore's current president right now. So you can see now is Halima
Yakov Yakov. Sorry. And we asked the question in Chinese. All right. Singapore the
total system is eight. Singapore may have a total. There's no president in Singapore. So
this is basically the same information. You just translate it. You can get different performances
with chat GPT. Okay. And the same thing for like if you use Lamatu, Lamatu is heavily trained on
English. If you use Chinese, I'm very sure it won't do very well. All right. This is a practical
problem of large language models. You know, the Chinese benchmarks like they use Ernie,
one thing E and those other Chinese language models, they say that they perform better than
GPT for. Okay. I mean, at first I was skeptical. Then now that I think about it, they might have
done their evaluation on Chinese data sets. And the language models are fine to not the
Chinese data set. So maybe there's some merit to their claims. Okay. On the specific Chinese
data sets here. So this is one of the things that knowledge graphs can actually help to solve
because knowledge graphs can sort of translate this thing because knowledge graph is not language
specific. You see, so your concepts like president, okay, regardless of how you represent it in words,
okay, your Chinese words or English words, you can actually go to the same part in the knowledge
graph. And then you can have the key words here by the Singapore. And then it's like Halima.
So you can actually retrieve the kind of information regardless of language.
Okay. And then you can pass back this information back into the generation of the model. So this
can go here, back here. So regardless of how you prompt GPT in a certain language,
okay, you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant
representation. Okay, then you do your processing there. And then you go back to L-M. So if for
those of you who have been to some of my other like discussion sessions, you would know that I
like to say that this is the, this part here. This part here is what I call the abstraction layer
or the latent layer, latent space. So you process it in a way that is different from the input domain,
but because the information you process is similar, in this case, we are still asking for
semantic information about the president. You know, we don't have to do it in the language
domain. We can do it in like maybe some representational space. It could be a graph. All right.
And then you can use whatever you process the graph. You can go back to your input space.
So this is one of the key advantages that, you know, if we could interface large language models
with some form of a graphical or some form of memory-based approach that is invariant to the
input language type, you could get some performance advantage here.
Question so far? Anyone?
Okay. So let's cover some of the basics. What is a knowledge graph? So I took this from the paper.
The knowledge graph is basically a triplet consisting of source destination to relation. So
like for example, Barack Obama was born in Honolulu. So this is the relation. Okay. So relation.
And this is like Barack Obama as the source. And Honolulu is like the destination. So each
knowledge graph is made up of all these triplets joined together in various ways.
And the idea is that you just need to connect those entities that are related to each other. You
can like ask, you can actually walk through the knowledge graph and get the information you need.
Okay. So like there are of course like mega nodes, like for example, like Barack Obama will have a lot
of connections leading out of it because you are describing the person. Then like stuff like places
where a lot of things leading into it, because a lot of things like I in the USA, a lot of things
are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay. It's not too
bad. Okay. But it's very restrictive. Okay. I personally think that there is a better way to
represent information other than this kind of structure. Okay. And we can go and talk about
it later in the discussion. All right. I have something in the chat. Richard says,
is there a handy reference chart for how this looks or compares to word to back and similar
embeddings? Okay. So this typical knowledge graph that I'm talking about here does not have
embeddings yet. Okay. But in the future iterations, like in 2011, 2018, people have come up with
these things like knowledge graph embeddings. So they actually encode all this information here
in some vector space. So like maybe like, it's something very similar to like the vector space
that we see in a large language models, you can actually encode this thing in vector space.
You can actually encode the relation here in the vector space or so.
And then like, you can then encode this part here also in vector space. So like,
so it's like doing a vector arithmetic now. So you can see that
if I do a relation is just simply this one plus this one equals to this one. So I can do a like
at a point two in the first one here and then I can get. So if you have a sufficiently expressive
enough embedding space, you can express the whole knowledge graph in form in the form of
embeddings. And that's indeed what some of the later models do. In fact, this is highly related to
graph neural networks, because graph neural networks, they express each node as an embedding,
then they do message passing, which means I share information with the other nodes, like at each
time step, I pass some information to the other nodes. And I mean, there are different variants
of message passing. The most common is that the message meets in the middle, this one then
updates both nodes. Yeah. So there are a few ways of doing the idea of like updating the embeddings
and so on. I'm not going to cover in detail about how all this are done, because graph neural networks
is a huge topic. Okay, personally, I think graph neural networks is probably not the answer to
solving intelligence. I'm sorry to Peter Velikovic. I thought like what he's doing, but I don't think
it's the right way to do it, like using differentiable deep learning to do it. So I think the
knowledge graph that I've described over here, which is using vectors to do addition, and then
you get the other nodes, that's a very expressive knowledge graph. Okay, because you can actually
express everything in vectors without the names. So you can theoretically do any kind of like
addition provided, you know, nodes plus relation give you another node provided that that exists,
right? So if you could somehow represent the whole of the world's knowledge in the form of vector
space, let's say we are we are done, we can just like, we achieve zero short generalization,
you just embed to that vector space and they add something and then you go to somewhere else. Okay,
but I don't think that's how intelligence is represented. Okay, because, you know, there's
this thing called like context dependent embeddings, like I don't think like the word Barrett
Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama,
that is like at the Paragot at the White House, Barrett Obama at his house, okay, Barrett Obama
at the beach or maybe different places or Barrett Obama will lead to different characteristics
of Barrett Obama, like, he may be very serious in the office, but he's very relaxed at the beach.
You cannot have the same embedding space to represent all this, you need to walk it
according to the context. Okay, and that is something that I actually intend to try to do it,
like I try to do a very flexible, like, basically the information can walk according to the parent
notes in this new form of knowledge graph that I'm thinking of. Okay, so whatever I'm talking about
is my own idea, I haven't seen any paper on it yet, but I think the current knowledge graphs will
all fail at embodying intelligence because it's just too restrictive right now.
Okay, Shang, you asked a question. I'm unfamiliar with graph theory, so hoping to know how do you
represent factors as weights and how many can you add? Okay, could you elaborate what you mean by
factors? Yeah, you mentioned that, like, you can add any form of intelligence, right, so take for
example, if we are using, yeah, I actually didn't think of this as an example, but let's say just
the simplest one, a multi-layer map, then for these roads, one weight could be how fast the
speed limit of the road, and another weight could be how occupied it is.
Okay, so you are talking about, like, descriptions of an object,
like, or characteristics, attributes, you're talking about attributes of an object.
Yeah, the weight of each line, correct, of each connection between the notes.
Ah, okay, so like, how do you get this embedding here, right? Yeah, correct.
Yeah, so perhaps, like, in your original embedding space, each of these dimensions could represent
something already, like, maybe one could represent road, one could represent, like,
emotion or, you know, there are different domains that these dimensions could capture already,
so if you already have that, you can just, like, add the relation in that specific dimension.
Yeah, so of course, all this will need to be, like, learned somehow, so it's either learned
through deep learning or some fixed biases. Yeah, so ultimately, how well the graph does
will depend on how good your embedding space captures all the information.
Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now, just know that knowledge graphs
have a few forms, okay, the most simple form is that you take words and then you add another word
and then you get another word, so this, like, describes a relation. The more advanced form
will be to use embeddings, all right, so we will talk more about, and then of course,
the even more advanced form is evolving embeddings or context-dependent embeddings,
which is, like, the idea that I have, and it's also the idea that large language models actually
kind of use, because when we can ground large language models in different contexts, you get
different outputs, so a large language model is context-dependent processing, okay, if you can
embody that kind of context-dependence into the knowledge graph, you will have a very
powerful knowledge graph, okay, so as you can see, whatever I'm sharing with you here today,
I think that I'm not the answer, all right, I'm just sharing with you here because this is what
is existing, okay, I have a grander vision compared to all of the stuff that I'm talking about,
okay, so let's continue, all right, so knowledge graphs, okay, what excites me in knowledge
graph is the very notion of hierarchy, and I think hierarchy is key to intelligence. You
don't process things in just one domain, you process things in many domains, like, if I'm
drinking a cup now, I'm just, like, drinking water from a cup, not drinking a cup, drinking
water now, I use my hand to move like that, but then if I think about, oh, how do I go to school,
then I think about, oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to
take this bus or this train, so this is a more higher level planning, okay, if I were to think
about how I move my left leg and right leg, left leg, right leg, I would think forever before I do
some planning, so different problems require different levels of solution finding, and I call
this different levels of hierarchy, so in the art challenge, the abstraction and reasoning corpus,
I use multiple levels of hierarchy, like, we have a pixel level, we have an object level,
and then you express the input grid into different forms of hierarchy, and I find that this way can
solve a lot of problems, because different problems require different approaches to think of it,
but you don't solve all problems using trigonometry, right, you solve some using algebra, you solve some
using set theory, so you have different ways of viewing the problem, and knowledge graph,
you can actually use this to extract different, like, at the top layers of the knowledge graph,
typically are the more broad concepts, and the bottom will be more of the general concepts,
so you can see in this, this is the SICK knowledge base, SICK is a 30-year-old project trying to
embody the world's knowledge in a knowledge graph, okay, they are still trying to do it,
but it turns out that this is a very hard thing to do, because the knowledge graphs itself,
okay, it embodies, like, one relation, it's like a confirmed relation, but sometimes, you know,
based on the context, you may not have that confirmed relation, right, so again, this is the
context-dependent knowledge graph I'm talking about, also another thing is, a lot of times we
do things, but we don't really know how to express it in words, so if you want to express the whole
idea of logic in words, it's a very, very difficult task, because sometimes we don't even know why
we are doing something, okay, there's bound to be a point of time that logic cannot express things,
okay, so you can go and look at this thing called Godot incompleteness theorem, okay, if you use
mathematical logic to express things, there comes a point in time whereby logic cannot solve,
because the way to solve it lies beyond logic, like, it cannot be, you know, Godot incompleteness
theorem, there's something like this, like, this sentence is false, all right, so if you can represent
this as a Godot number, okay, this kind of sentence, and then you say that, oh, this number is true,
all right, but then this number says that this number is false, so it's like you have a self-preferential
loop, all right, so if you use logical prepositions, and knowledge graph is sort of like a logical
preposition, A goes to B, B goes to C, you know, you might face this problem, okay, that you can
actually go in, you can actually go in a loop that contradicts itself, all right, so that's one thing
that, like, knowledge graphs may have some issue with if we do, like, it being, like, 100% fact,
like, if A links to B all the time, you know, sometimes you might actually have a link that
contradicts itself, all right, that's one issue of the knowledge graph, maybe other thing is, okay,
so burying all these issues, one thing I like about knowledge graph is that you can see in this
diagram here, you start off with, like, small stuff, like, thing, and then you go to, like,
individual, you go to collections, okay, and then you have different ways of doing it, time,
movement, and so on, then you have agents, actors, plans, and goals, I mean, if you think about it,
kind of, it's like how large and rich models is evolving now, right, we are kind of at this
stage here, we are agents now, so after that, we have organization of agents, we have activities,
okay, hopefully we don't get to military warfare, you know, because, like, so this is like the
evolution of a population, so it's quite nice, and, like, you can capture all this knowledge from,
okay, so I wanted to say this is for more, like, micro level to more macro level, and the macro
level is actually the sum of the micro level, so maybe the arrow should be drawn the other way,
the arrow should be drawn like that, you take from the micro stuff, and you go to the macro stuff,
so this is the knowledge that we accumulate, all right, and knowledge graphs can capture this quite
well, because of the way you take from source, relation to destination, you can capture from
the micro level, you do all the branching, and then you end up with the macro level, all right,
so this was in 2016, by the way, I couldn't find this in the SIG website today, all right,
so this is the SIG knowledge graph, this is, like, a very, very tiny representation of how
the knowledge graph looks like, yeah, I just wanted you to see, like, how one of the largest
knowledge graphs in the world looks like, so you can see, like, you have all these, like, the fortune
companies, you have all these, like, functions, like, all these, like, look like some form of,
like, math stuff, right, yeah, so you have, like, people over here, so you have different areas
of congregation of all this knowledge, right, then in order to pass through the knowledge graph,
you have to use something very, very similar to SQL, structured query language, you like,
say that, oh, if I want to get a frightened person, I want to get the entity X that is a person,
and then fuse emotion that is fear at a very high level, yeah, so you have to do this kind of stuff,
all right, so immediately you can see that knowledge graph right now can be immediately
improved by large language models in one aspect, okay, and this aspect is that we can straight away
use the large language models to generate this structured query language, okay, so if all of
you are thinking about this, like, if you want to get, like, a very, very rigid programming language
out, okay, you can actually write what you want in free text, and then say, convert this to SQL,
and you can get it out, so you can do the same thing for this sick language, you give it some
examples of how the language works, you say, I want to get a frightened person, and then,
you know, chat GPD is quite good at getting stuff like this out, okay, no more SQL, right, I love
it, okay, if you need to use SQL, just use chat GPD, okay, it's a very good replacement, all right,
so this is one way large language models can already help to benefit knowledge graphs right now,
because it can pass through it using very human readable and understandable syntax, okay, like,
this kind of thing is not very human understandable, you can use free text to do it,
and we can do it right now, right, but more importantly, what can large language models do
to help knowledge graphs, okay, or what can knowledge graphs do to help language models,
okay, so before I move on to that, let's just talk about some other ideas I have,
so I'm actually a reinforcement learning person, so like, I feel like knowledge graph can also
represent stuff like different states, like you have different tiredness, they drink coffee,
and then you are now awake, so if you know in the literature of
reinforcement learning, this is like a Markov decision process where these are the states,
okay, and these are the actions, and then this is the next state, okay, so you can actually use
knowledge graphs to represent stuff like this as well, okay, because it's quite anything that has
a link like that, you can represent this easily, okay, so all right, this is perhaps the most
important slide for today, okay, this is not in the paper that I referenced, but this is the
thing that I was thinking about, it's like knowledge graph is actually sort of a tool that
can be used by the agent, so like retrieval of method generation may not get the right
passages because like the embedding space may not be good, perhaps we can use like a form of
knowledge graph passing, okay, you can extract relevant parts of the knowledge graph, you can
retrieve the context based on that, okay, so you can ask the knowledge graph to get you the subgraph,
the subgraph, you can then use it to ground the context of the agent, and how you use it to ground,
okay, it's up to you, okay, some people might use graph neural networks, I don't advocate for that,
okay, one other way of doing it is to just convert it back to free text, okay, as easy as that,
so you use the knowledge graph to extract out the relevant corpuses and avoid the need for the
embedding space, the open AI embeddings, okay, you use the knowledge graph to extract it,
then you take the stuff that you extract from the knowledge graph, pass it back as text,
and then go back to the agent to ground it, all right, so yeah, one other way of, one good thing
about this is that if you have stuff like if you are doing this for a robot, okay, that experiences
the world, you might actually be able to use this knowledge graph, okay, obviously I'm conflating
the term knowledge graph, but this knowledge graph can now be the state action state graph,
you know, you can actually model relations of the world easily, like I always believe like we
learn from taking actions in the world, so we can actually build this knowledge graph dynamically,
this is the third point, okay, you can gain knowledge of the world, we can build up this
knowledge graph bit by bit, all right, and then we can then query this knowledge graph
and get answers from the knowledge graph to inform our choices, okay, so about how we can
get this part here, this is a huge thing here, okay, because I believe that there's one thing
that's missing in current knowledge graph, and this thing is called changing the memory to the
context at hand, okay, so I treat the knowledge graph as memory, so like when you retreat things
from memory, okay, and then you want to apply to the current state right now, current state of the
world right now, you don't really want to just use that memory itself, you want to adapt that
memory such that it will be relevant in this current state, like if I have drank like coffee
at school, now I drink coffee at home now, okay, you know, something like that, this
I will need to adapt that memory of drinking coffee somewhere else and then adapt it
back to here, okay, there's no point in giving me the memory of me drinking coffee somewhere else
because it doesn't adapt to the current situation, so if you can adapt this knowledge graph to the
current situation, that will be great, okay, that will be great, so that's something that I think
I'm trying to look into because you don't just want static knowledge extraction, okay, you want
knowledge extracted and manipulated to fit the current context, okay, of course for those of
you all in my discord group, I've been thinking about memory recently and you know human memory is
very malleable, like if you think about something, you might actually affect that memory of it,
so like a lot of times people in the childhood, they think that they have certain memories,
okay, like maybe you are lost in a supermarket, so if I keep asking you questions about it,
I say who was the stranger with you when you were lost, okay, so maybe there was no stranger,
but if I keep asking you guiding questions like that, eventually you might think of your memory
as like, oh yeah, a stranger let me home after I was lost in the mark, okay, but that may not
have happened, that memory has changed because I've asked you certain guiding questions and then
you think that certain things are now in your memory, right, so whether or not we should change
this memory and affect this knowledge graph, I leave that to future discussion, okay, because
this is something very interesting, like should we change the existing memory that we have
based on the current context, okay, our brains do that, okay, but should we do this for this kind
of practical systems, okay, yeah, so we say humans hallucinate, yeah of course we hallucinate a lot,
and that is why actually we are quite similar to large English models in that sense, now people
always say large English models not very reliable, humans reliable, our memory is not that reliable
actually if you think about it, but honestly I cannot trust my memories that much because
like sometimes if it's too far away, it can change, like the book that I've been reading,
it will say that like flashback memories which people think that are very, very pertinent,
flashback memories are memories like like you know 911 collapse, people tend to remember what
they were doing at that time, okay, because it was so significant, it turns out that this
flashback memories can be wrong, okay, it can also be, it can also be changed, okay, so this is a
very interesting thing, you can actually use like the current context to affect the memory you have,
so you might actually affect the knowledge graph, okay, but whether or not we want it to be that way,
okay, we have to think about that, okay, I digress a bit, okay, but let me just get back to topic,
okay, today we have quite a few slides to cover, there are three approaches that I want to talk
about today, first is that you can use knowledge graph to enhance your large language models,
okay, and this means that you can give it structured stuff like domain specific knowledge,
in some sense it's like text-based grounding, it's the same as retrieval of mental generation,
just that now you take the information from a knowledge graph, number two, you use large
language models, expressivity, okay, and make a better knowledge graph, okay, I like this approach
as well, okay, we will see how to do it, and lastly you combine both approaches, you can get a synergized
large language models and knowledge graph, and I think something like this will be able to embody
intelligence, okay, but not the current knowledge graph, we need to change it to a dynamic knowledge
graph, okay, what is a dynamic knowledge graph, maybe I will talk about it next time, okay, after
I flesh out some ideas that I have right now, I will create this dynamic knowledge graph, okay,
I think the current knowledge graphs are not the answer, we need to have a different kind of knowledge
graph, but if we use this, I think we can get intelligence, okay, let's move on to the next
point, approach one, knowledge graph augmented large language models, okay, so there are two ways
I can, I summarize the paper in two ways, the main thing is one you can just put the knowledge
graph as text, and the other one is treat this as an object, and what kind of object, okay, you
either use like graph neural networks, so you can use an embedding space, I mean the one that was
used was trans-e, trans-embedding, you can go and search the paper trans-e, so these are some ways
that we can use the knowledge graph, okay, to pass it, let's go through the first way,
so the first, oh sorry, this is basically a pipeline for retrieval of large language models
grounding, first you use some form of knowledge retrieval, like you know retrieval of method
generation, you use cosine similarity, you get certain facts or some documents, okay, so I'm
just relating this to retrieval of method generation because they are almost the same,
all right, you take in the facts, you ground the large language model, you get the answer,
okay, and over here in the paper they put back propagation, but you know how are you going to
back propagate this knowledge retrieval, like you're going to end up with some like very very weird
way of doing back propagation, I don't think back propagation is the answer here, maybe you
want to back propagate your L, M to find tuning, okay, I ground that, but this part here to back
propagate to the knowledge retrieval, I don't think that should be done, all right, because this
back propagation thing will lead to like changes in embedding space, and then if you change your
knowledge retrieval, you also need to change your large language model, it's a never-ending cycle
of changing each other, like if you change the knowledge embeddings for the knowledge retrieval,
you also need to change how you interpret them in the large language model, so yeah, I don't think
you should use back propagation for the knowledge retrieval, you should probably use like memory
methods, other methods like you can say that, okay, what worked, what did not work, what worked,
what did not work, okay, you can reference this paper called Voyager, okay, so they are this
automatic curriculum learner, I think you should train the knowledge retrieval like the
automated curriculum learner, okay, you just ground it in some examples of what works, what
doesn't work, okay, you don't have to use back propagation for that, okay, so the main pathway
for knowledge graph for techs, for LMS is like that, you take the knowledge graph pass through it,
get some effects, and then feed it into the large language model, okay, that's the main pipeline,
okay, questions on this?
Okay, let's move on.
Okay, so this is one of my favorite papers, okay, this is the generative agents paper,
they have 20 agents in the sandbox interacting with each other, and one thing that struck me
quite well for this paper is that they actually use JSON structure to ground the actions, so
for example, if you want to ask like Eddie, Eddie Lin, all right, he currently is in the Lin
family's house, all right, he's in the bedroom actually on the desk, okay, so you can ask the
agent, okay, this is actually the chat GPD prompt, okay, you can ask the agent like, okay,
these are the other areas that we have, okay, and all these other areas are obtained from the JSON,
actually the JSON is like a knowledge graph,
okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,
has a kitchen, you know, this is something like a knowledge graph, if you ask me like,
they are just like representing the hierarchy of the house, like, I mean, if you want to treat it
as a knowledge graph, you will say like, this is the house, so this is the Lin's house, Lin's house,
Lin's house, I've just put Lin's H, okay, then you can have like, the relation will be contains,
okay, or comprises, I mean, contains, then you can have like bedroom,
yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody,
all right, so I treat this as a knowledge graph, so you can, you can sort of ground the agent,
okay, like, this is what the agent knows, okay, this is the current memory that the agent has
as a form of knowledge graph, like, these are the kind of areas that we actually know from the
world, okay, this is like, if you talk about grid cells and play cells, maybe you find out more
areas, okay, you can ground them, okay, these are your semantic knowledge or oppositional knowledge
that you have about the world, and actually these two are the positional knowledge, this is the first
one is knowledge about the house, knowledge about house, and then the second one is knowledge about
the world, yeah, so you, you have all this knowledge, you can ground the agent to choose
a specific place, imagine if we did not ground the agent with all this stuff at the top, you just
ask where should Eddie Lin go to, okay, then Eddie Lin might, the LL might reply, Eddie Lin should
go to the supermarket or something like something that is irrelevant to the game world, right,
so because we grounded it with some idea of what kind of possible areas that the agent should go,
the agent is able to choose one area from the above list, and how is this list generated,
is generated from passing some form of knowledge graph, okay, and this is what I mean by using
knowledge graph as text to ground the large language model, so you can use this recursively,
you can say that, oh currently you are in the maybe common room, okay, what, where in the
common room would you like to go, we'd like to go to the sofa, to the mirror, and you can do this
recursively, okay, and then you can get a very very specific area that the agent is going, okay,
any clarifications on this sofa before I move on?
Yeah, if you haven't read this paper, go read it, okay, this paper is, it's good,
it's one of the better ones, all right, so we have the Chinese LLM, it's called Ernie,
all right, and what they do is they actually use two hybrid ways of generating the output,
so they say that large language models lack grounding, lack consistency, so we use a knowledge
graph, okay, I granted that, okay, but then I look at their structure and they're like, oh man,
what is this, so they actually have a large language model, this is the typical transformer
architecture, so this is the typical transformer on the left side, so they have two encoders, okay,
one is the T encoder, which is like the text encoder, and why is the knowledge graph encoder,
so the knowledge graph encoder uses this thing called trans embeddings, okay, I'm not going to
go through that, but they train their embeddings using like, they take one vector and take another
vector, then they connect the, just draw the diagram here for you to see, so they have one
vector A and another vector B, and then you create another vector C here, so you can keep like,
using this vector A, you take another vector, extend from it, and then you can train on this
relationship, so you know, this is how the trans embeddings are trained, trans E, okay,
and they use this kind of embedding space, okay, you can do self-attention, and then you can do
cross-attention across both the trans, the text embeddings as well as the knowledge graph embeddings,
and then hopefully you get some output, right, and then you get some text outputs here,
and then you can update your knowledge graph to some knowledge graph outputs here, okay, so this
is a way to embody a knowledge graph as some embedding space, okay, and then we can use like,
the attention to like, attend to like the text-based stuff as well, so yeah, this is just one way of
doing processing using a knowledge graph, as you can see, I don't really like it, I mean,
I think that is too convoluted, like, okay, so this is another discussion question that I'd like
you all to think about, should we have separate embedding space for this large language model
stuff, and for the knowledge graph embeddings, like, should we use two different embeddings,
okay, should we use two different embeddings,
yeah, or should we use the same one, yeah, okay, then also more generally, like, if you want to
have multimodal embeddings, right, you have text image embeddings, you have audio embeddings,
okay, if you want to do a multimodal large language models, you can actually also put them
into distribution model at the end here, okay, but the question is, in fact, knowledge graph can
be multimodal, so you can actually also put it here, the question is, all this image
and audio embeddings, okay, you can put it in the large language model, you can also put in the
knowledge graph, but why not just use a single embedding, right, why do you need to,
using text-based knowledge graph and text-based large language models,
that there's no external domain here, is it, it's all the same domain, why do we need two
different embeddings for the knowledge graph and the input text, that's my question, okay,
if you have any ideas, let me know, but think about this, all right,
okay, then we have this question-answer graph neural network, and this does a two-way interaction
between the language model and knowledge graph, and what we can see here is that we have a certain
question, again, some options that to choose from, the large language model will go in here,
and then like they express like the question and the options as part of a knowledge graph,
okay, and this will go through another knowledge graph encoder, and this knowledge graph encoder
is a graph neural network, okay, and this basically will, you look at this diagram here,
it will do cross-attention, it's very, very confusing, look at this
language model conditions knowledge graph, so you can blank out some notes here, you know,
you can actually do some attention on some notes to like block off the path, yeah, so yeah,
that's possible, yeah, so you can also use the knowledge graph to condition the attention in the,
in, in, in, when, when you do the next open processing for the language model,
and then eventually you'll get your answer, yeah, so this is one way we can process the
knowledge graph, you can process it using a graph neural network, and in fact, the earlier one on
Ernie, that is similar to graph neural network as well, I mean, they are using the embedding space,
and you know, if you just do some operations on the embedding space, that is a graph neural
network already, so yeah, this is very, very similar to graph neural networks, and yeah,
it shows that back in the first few years, okay, people use these kind of methods to pass through
knowledge graph using graph neural networks to represent the embeddings, okay, I don't see why
you need to do this, okay, personally, I don't see why you need to do this, you can just use text,
because the knowledge graph representation is in the same kind of domain as your last
language model representation, they're all text, yeah, why do we need a separate embedding, so
yeah, okay, Richard said, I think there will have to be input output embeddings and train them to
address common pattern or memory structures, sorry, could you explain, this comment was in relation to
which part of what I said? So you were saying, you know, it's a decision sort of, or
sort of, not a decision question, right, as well, that there is, how do you, well, approach this
problem, why do they have their embeddings separate, right, and at the end there's a sort of cross
attention where they're emerging them for an output of this type or that type would have you,
but then this idea comes that the real, so I think of a large language model where
the reason why they have these emergent behaviors is because language is currently
our best mechanism to embody thoughts, ideas, and they're our most direct implementation of ideas.
Now, particularly once they're broken down, tokenized and so on, you've taken through that
process a few times, consider attention and context, you come up with new ideas. Yes, yes.
And then as you were just saying, there's no particular need for different embedding spaces,
and the only need for them is to bring understanding into a common framework,
where the ideas themselves in the latent space are being
considered and their context and their relationships. So, how the, this is sort of a,
that's the word, this idea of boiling down the actual form of communication
into some representation, any representation, where we can start applying
our knowledge to it. Whether you read text or listen to text,
it was, you don't, I don't, when I hear things, I don't imagine them written down in front of me,
I just hear words, words become ideas and we go from there. So, in the same way,
I see the way that knowledge is presented as an input-output problem and embeddings
really address the input-output problem. And then after that, there's a memory and
consideration process which operates on ideas which are not linked to input and output.
I think you and I agree that there needs to be a latent space or abstraction space for
processing. And I think you also agree that there need not be too separate embedding space
for the knowledge graph in the last language model. If I hear it correctly, right, you don't,
you also don't think that is necessary, right? I think, yeah, but then the problem becomes,
if you don't use the same embedding technique, how do you present meaning? So, for mine,
in terms of large language model being in language or not in language, in words,
the question is really, are we making the problem harder for ourselves by using a different,
by saying, well, it's all words and the words are by and large correct. Therefore,
we'll just use a large language model to read and ingest a large language model. And I think
that will work. But the challenge becomes what you alluded to earlier, where the Chinese
representation versus the English representation gives a different outcome. And I'm trying to
abstract away that behavior. So the thinking, the actual thinking happens in, is always in
latent space. And the only job for embeddings is to present in a form where, you know, cognition
can happen. Because, right? And so I would say, I don't particularly care what the encoding encoder
is decoder is, it can go from text in picture out picture in text out, it doesn't matter.
The important thing is that it's consistent, and we can operate on it in a manner that
addresses the patterns and relationships within.
Yeah, well said, well said. I agree with you. So what matters is how we abstract it to the
processing space, which is the latent space, and how we encode it and decode is just extra,
like, details, okay, that basically just needs to be mapped there, and it should be good enough.
Yeah, I think so. And when it becomes its own training challenge.
Correct. So I think in the earlier papers, what I get is like, why do we need a knowledge
graph encoder like that? It's because they use embeddings like trans e that, you know, are
different from GPT embeddings, like, or bird embeddings, again, most of the early papers use
birds, B, RT, bi-directional encoder representations from transformers, right? So what happens is
because these two are from a different embedding space, so you kind of need to map them to the
same embedding space, that's why you need a knowledge graph encoder and a large language
model encoder. But in the new kind of knowledge graph that is constructed, okay, because this
large language model is now so powerful, right, you can actually use the embedding space for the
large language model to construct your knowledge graph. And if you do that, okay, if you do that
process, which is part two of the presentation today, you will see later, if you use the large
language models to construct the knowledge graph, actually, you don't need a separate knowledge
graph decoder or encoder here because they are in the same embedding space already.
So you look at the, if you look at this thing here, like, you don't need a separate decoder for
the JSON here because this is in the same embedding space as your text. And I would like to
posit that it will be better for all of them to be in the same embedding space because it will be
much easier to do the attention. I mean, it's easier to do attention in the same domain as
compared to different domains because, you know, cross-attention is only one layer right now.
Okay, if you want to do a very efficient cross-attentioning multiple layers, but if you just do
in the same domain, the transformer architecture right now, you actually do the self-attention
multiple times. All right, so it might be actually better to do it in the same domain.
Okay, and of course, you will save training complications because, you know, you need to
map both to the same latent space and, you know, that is a difficult problem. It's a very difficult
problem to map two different streams of inputs to the same latent space. I mean, we have seen it,
like in OpenAI, they have this thing called CLIP, okay, that max text and images to same
latent space. You know how many examples they train it on? Millions, I think even billions.
Yeah, so it's a very, very difficult problem to map both to the same latent space. Okay,
but I can map it well. First, you can do like stuff like stable diffusion, you know,
dolly, you can generate images from text. Yeah, but why have this problem with the knowledge graph
when you can actually just ground the knowledge graph in the same embedding space as your last
language model? Okay, so think about it. Okay, now we go to approach two. Before I move on,
like to open the floor for any opportunities to ask anything so far for the first part.
Okay, if not, I'll carry on. So next is how we can use a knowledge, large language model to get
the knowledge graph. So one is to use using few shot and zero or zero shot prompting,
like for example, Lang chain, okay, I don't think the approach is that great. Okay, I found a better
approach, right, using a better prompt, if I we can, we can potentially use large language models
to generate knowledge graphs. The other way is to use the embedding space of the large language
models to enrich the representational space of the knowledge graph. So this is also quite interesting.
Let's see how we can do both. Okay, the first one is, okay, this is just some idea of how we can use
it. Okay, we can, we can few shot prompt to generate the relations, okay, because large
language models are just very versatile and can be context driven to do it. And actually it's way
better than, you know, so this is my own experience. I use Spacey to do name entity recognition,
and I use large language models to do that. The GPT, chat GPT performs way better than Spacey.
Spacey makes out a lot of the names, all right. So if we use large language models to generate
the knowledge graph, compared to traditional approaches, like Spacey or some other verb,
the MP, you know, those kind of three passes for language. Last time people used that to
generate the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay,
so that was difficult to generate the knowledge graph because sometimes it miss out certain things.
But large language models are quite good. Okay, why not just use large language models directly
to generate the relations and the source and the destiny and the destination. So indeed,
this is what Lang chain did. Okay, so if you look at the graph QA prompt, this is the prompt.
Okay, you are the network intelligence. Okay, help to integrate stuff into a knowledge graph,
extract knowledge triples from text. Okay, knowledge triples is a clause that contains a subject,
predicate and object. Okay, subject is entity being described, predicate is the property,
object is the value of the property. Okay, there's a typo here. Okay, so this is the
zero short prompting for Lang chain. Alright, this is not that good yet. So you need to give some
few short examples and they gave some few short examples in the prompt, like for example, like
this is the input, and then you can say that oh, Nevada is a state, Nevada is the US, Nevada is
number one, go producer and go. Right, so I don't like this example. Okay, because for one, they
did not say the state at all in the prompt. Like then you want the model to just plug the
plug the noun something there. So yeah, like here, I'm going to the store output none. Why,
why is the output none? It should be I, I went to store something like that. Yeah, so you should,
you should be able to extract something from this. So I disagree with the examples that the
Lang chain one provided. Okay, so I think if they improve this example, maybe it does work better.
So let's take a look at like what I did later. So I'm not a fan of Lang chain, by the way.
Lang chain prompts are very worthy. Alright, so this is the other way that we can use the
large language models to do the text encoding to do knowledge graph embeddings. So this is called
KGE, knowledge graph embeddings is something like, you know, if you talk about the stuff like
transeed, these are like embeddings that we can give to the source to the destination to the
relation. So we can represent the knowledge graph as embeddings, and we can use GPT or some large
language model, okay, to generate some embedding space here that you can then use like MLP,
multilayer perceptron, and so on to map to the embedding space of the knowledge graph embeddings.
So this is one way we can utilize large language models to do it. I mean, I was thinking,
you know, like, why not just use this, like, why not just use LAM embeddings directly for knowledge
graph? I mean, LAMs are way better than than doing graph neural networks in the sense that,
you know, if you know the problems with graph neural networks, I'm just going to tell you the
problems of graph neural networks now. Okay, they have these two problems. Okay, this is one is called
over squishing or over squashing, and the other one is called over smoothing. Okay, what are these
two problems? Over squashing is that the information, because you pass the information into an embedding
layer, information gets lost at embeddings. Okay, so this over squashing thing is also a problem
for LAMs, so I'm not going to cover too much on it. The other problem that we have for this kind of
graph neural network over smoothing, okay, is that after you do message passing
for too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also
realized this, because I did graph neural networks, you have two nodes, you pass information to each
other, and then you become the average of the information, you keep doing this, right, eventually
both nodes become the same, or very, very similar. Okay, so this is one of the huge problems of
graph neural networks, and I feel like the embedding space that is best done, right, is not
the way that we do message passing in graph neural network. We should just ground it in the
context using a large language model, and large language model update the context quite well.
Okay, then you can just use the embedding that is derived from that particular context in,
like, you can just put something like that, like, you can just say context, and then like,
I am a student or something like that. So, like, this context will update the definition of the
student here. So you can go through the transformer module. So this is the transformer module,
and then you can get the final embedding here.
Yeah, at the final layer, right, before the softmax, you can actually use the transformer
to get the embeddings already. Why use knowledge graph embeddings? Okay, so I'm just putting this
question out here. So I hope those people knowledgeable in this area can come and, you know,
correct me if I'm wrong. But I don't see a point in doing this. Yeah, right now.
Okay, so let's leave it as that, and let's continue.
Approach tree. So the approach tree is how we combine both approaches to make a very, very
synergistic model where the large language model can generate the knowledge graph dynamically.
And this knowledge graph is something like a dynamic memory that gets updated as the agent
explores the world and so on. And this knowledge graph can then inform the knowledge, the large
language model, and ground it in consistent generation. So let's see how this works. So you
can see this is the diagram in the paper, and you can see like data. Okay, that's what me and Richard
discussed. Data, okay, will be from different domains to embed them into latent space. Okay,
so now we just assume that there's only one latent space, but my view is that there's multiple
latent spaces. Okay, so right now we just treat it as there's only one latent space. You process
the information in that one latent space using knowledge graph and large language model in this
loop. Okay, so knowledge graph can ground the language model in consistency. Language model
can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you
can use embedding based knowledge graphs, like then you can make the knowledge graphs like express
a lot of things more than just hex alone. Okay, so this is one idea. You can use different techniques
to process it like graph theory networks from engineering, representational learning. Yeah,
I mean, this is just some big words, but the idea is you basically do some processing. All right,
you can use large language models to process. Or if you like it, you can process it using the
knowledge graph, which is like a graph neural network to process. Okay, or you can make the
graph neural network into text, and then you can do some neural symbolic reasoning. Actually,
this whole thing can be just summarized as neural symbolic reasoning, because
first the knowledge graph equals the symbols, but
and then the last language model, the neural networks. So you can just summarize this whole
thing as neural symbolic reasoning. Right, then you can use this for different domains.
Right, I think this is a very, very exciting path that we should work on. Because right now,
with the power of language models, the knowledge graph can be very, very flexible.
Okay, and it's not a typical knowledge graph anymore, it can be embedding based knowledge
graph. And it can be context dependent knowledge graph. Okay, so I really hope to work on context
dependent knowledge graph, because I think that's the future. Again, not the traditional knowledge
graph that you see everywhere in this presentation. The knowledge graph embeddings must be able to do,
must be able to change based on the parent nodes, okay, must be changed based on the context.
And that's something that is not done right now, at least based on my own awareness. I don't think
that's done right now. But that's very promising. Right, so one use case for this kind of system
is fact checking. As you know, large language models cannot do very badly at fact checking.
It tends to hallucinate a lot. And perhaps we can do like a knowledge graph to like ground it in
some facts, like some Wikipedia entries. No, you can use this to ground the inference. Okay,
by doing inference, you can then see whether or not like, is it, is there a path in the knowledge
graph that matches it? Or you do knowledge graph grounded inference, like you say, you must only
use this information that I extract for you in the knowledge graph and infer. Okay, so this diagram
here, unfortunately did not do the inference step, okay, because they are still using BERTs,
okay, they're using BERT as a model. And what they did was they used the knowledge graph
relations to do some pre training. So it's like they take additional, like,
additional tech samples, they just mask out certain words based on the knowledge graph
relations, and then they do the training here. So they just did the pre training,
using the knowledge graph to give additional examples. Okay, so what I want the thing to do
is actually to do it during inference. If I cannot find any paper that does that so far,
all right. So I think this inference is more important than the pre training, you know,
this pre training, yes, it increases more data samples, because you can just mix and match the
knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by
two to three percentage points across SOTA benchmarks. This fact KB, you can go and check it out.
All right, but what I'm more interested in is how you use it for inference, not for pre training.
Okay, so let's see how Lang chain does it. All right, so now we come to the Lang chain part.
So actually Lang chain is quite advanced, because Lang chain has a lot of the ideas that I think
should be done. All right, let's see how the Lang chain question answering graph question answering
is done. All right, so we have four steps. First step, we generate the triples from the context.
Okay, so we have like maybe a text context, you generate like the triples from it,
like the knowledge graph triples. Okay, you generate some from the query, you generate some
entity extraction. Okay, and then you use these entities to extract these relevant triples. Okay,
later I'll show you the how what I mean by this. And then you use these relevant triples to answer
the question. So I share with you these two documentation in case you want to see how
Lang chain graph QA does it. So step one, okay, generate triples from context. So like this
context, I just came out of it, right? Recently, my MacBook external camera built-in camera spoiled,
so I'm actually using the external camera right now to talk to you. And yeah, so this example is
about Apple. So let's assume that Apple created a new product called Mac and Cheese Pro, okay,
in 2025. All right, and then like Apple gave the invented cheese, okay, a rousing ovation in
2026 after invented this in 2024. Right, there's also another company called Orange who created
a competing product called the Orange and Cheese Pro. The price was slightly higher at $5,000
compared to $4,000 from Apple. Okay, so this is a fictional example. Okay, and this is just to see
how good the context is thought in the knowledge graph. So you can see that, oh yes,
Apple announced Mac and Cheese Pro, Apple gave cheese. So this kind of thing, right, like
is a bit contentious because like, what do you mean by gave cheese, gave what? So this one
needs to be improved a bit. Apple gave an ovation, okay, again to who? All right, so this one needs
to be improved as well. Okay, the price of the MacBook Pro is $4,000. Yes, Mac and Cheese Pro
is already. Orange and Cheese Pro, good. Orange and Cheese Pro, the price $5,000. Okay, so you see,
it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.
So I don't quite like the way they did the triplet extraction, and I think this is the
downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't use
it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested in how
they generate the context, you can go back to my, the earlier slides that I was talking about.
Yeah, so I mean, this actually, let me show you, let me show you again the slides.
It's this one. This is the one that they did. This is the prop to generate stuff from the text.
Yeah, so the examples aren't very great and understandably the results aren't very great
as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese
Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot
of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,
so this is the knowledge graph that is generated. And we can see that like next up we can use the
Graph QA chain in order to run the chain and see the answer. And you can see that if I ask
it the question like when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,
because after they passed through the context, okay, they abstract like when was the Mac and
Cheese Pro, when the Apple announced the Mac and Cheese Pro, they abstract that in the query,
there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make
sure that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe
with this thing. Like if you use exact text matching, what if there's a spelling error,
capitalization error, or like related word, but not exact word match.
Yeah, so if you use exact text matching, which is what they did for a length chain,
like what if you don't get the right match? Okay, so I don't quite like this approach.
So yeah, this is something that I think could be improved on. All right, and you can see that
if I ask it like when did they announce the Mac and Cheese Pro, they couldn't answer.
All right, because look at this knowledge graph here, there's nothing that talks about dates here.
All right, so they miss out quite a huge chunk of information from the earlier context.
So if we had fed in the earlier context directly, so I just use the length chain
LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you
that length chain is not good. All right, I myself the new length chain. All right, so this is the
idea that like after a while, you know, this is the context and then, okay, so this is not bad.
I mean, you could just do the same thing on ChatGBT, actually, you can just put like
context question and then ChatGBT will give you the answer. All right, so this LmChain works
and this shows that by embedding the text as a knowledge graph, we kind of miss out certain
stuff. All right, and what are the stuff we miss out? We miss out the years and we also miss out
like Apple gave cheese. I mean, it doesn't make sense that way. I mean, look at the knowledge graph,
what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.
Okay, where's cheese?
Apple Cheese gave. Is there a gave anywhere?
Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the
main thing in this graph because we miss out some information. And that's one of the issues
of converting text directly into knowledge graph is that you might miss out certain relations.
And actually, if you think about it, if we want to embody all relations,
there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself
is way more expressive than the knowledge graph, if you think about it that way. Okay,
but again, you know, if you just use text only, you might face issues that know your
OpenAI embeddings might be too restrictive. It's too broad base. You need the embeddings
at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using
my strict JSON framework here, which just basically passes the system prompt and then outputs as a
JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just
say you are a knowledge graph builder, you extracted an object one, object two relation.
Okay, I did not even put subject object predicate. Okay, I mean, I just do like that. Okay, I just
want it to be as vague and as generic as possible, because I want to capture as much information
as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this
is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the
link. All right, so I gave you some examples like John bought the laptop. Okay, that's me. All right,
John built a house in 201. Okay, that's not me. All right. But this is the idea of like how we can
represent like various relations like that. All right, then the output format is just a knowledge
graph. So you can see like Apple announced my cheese pro man cheese pro announced in 2025.
Apple proof big hit. Okay, so again, this one is not exactly that great, because it's not really
Apple that prove a big hit. It should be the Mac and cheese pro that prove a big hit. So this
part needs to be from engineer a bit more. All right, Apple gave cheese. Okay, again, like,
this is not complete. Okay, cheese browsing ovation into zero to six. So actually, we combine these
two together. This is complete. So this is okay. All right, cheese invented man cheese pro. Okay,
man cheese pro invented into zero to four. Okay, orange created orange and cheese pro. Yep,
orange and cheese pro, the price is 5000. And Apple prices 4000. So again, here has some issues
also like here instead of saying that this is a Mac and cheese pro because we should be referring
to man cheese pro it says Apple. Okay, so unless we can sort of like link this later to Apple
announced, okay, despite here. So now you can see some issues with knowledge graph expressing stuff.
It is not clean. All right, it might truncate the information halfway. So this one needs more
study as to how we can express this in the knowledge graph better. But by expressing it in
the knowledge graph, you are able to then do knowledge graph passing, okay, and extract out
the relevant entities that that are related to the prompt. And you know, this is like, if you
think about it, this is like doing segmentation across like every few words in the segment one
time. Yeah, so this is the generated graph of what I did for strict general framework.
You can see that compared to Lang chain, this is what happens like
we have way more relations. There's more relations here. And dates are captured.
Yeah, so this is something that I think needs to be investigated more I might mind is not the
best, but language is definitely not good. Okay, so this is something that needs to be done more
if you want to extract stuff out into the into the knowledge graph. And then like should we use
embeddings? So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need
to use like Lama, Lama2. Okay, although Lama2, perhaps it's not that great or so, because Lama2 is
it's not that good for multilingual. Okay, but Lama2 is the the best possible substitute for
GBT, check GBT right now. So maybe you can construct a knowledge graph embeddings using the Lama2
embeddings. So food for thought. All right, next we have this flexible knowledge graph passing.
Over here, what I decided to do is that we want to output only relations that are relevant to the
question. All right, and I just passed in the entire knowledge graph here. Okay, so instead
of coming up entities, I just asked it to go through the entire knowledge graph, because
no, in case of words not exact or spelling errors, GBT is able to catch it. Okay, most of the time,
okay, I must, I must copy it. Okay, because GBT is not as great as doing like counting letters and
stuff. But if you misspell your words, but the meaning is about that, GBT is able to like extract
the right entities. And here we can see that we asked it like, when did Apple announce the
Manchees Pro? It captured exactly what we want. All right. And this is the graph that is the past
knowledge graph. So I'm talking about when you query the knowledge graph, you pass it so that only
relevant sections of the knowledge graph gets come out, gets extracted, you ground this extracted
part onto your text. Okay, and then you can get the answer here. So 2025. So I just shown that like
using this strict JSON format, you are able to like, it's very flexible, you just need to key
the system prompt, key the user prompt and output the format in terms of whatever JSON labels and
the description of the JSON. Okay, so I've been using this for a lot of my own use cases. And I'm
just adapting this for the knowledge graph. But this is really cool, because you can then use this
past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge
graph, you can use the knowledge graph as memory. Okay, and then you can update memory. And you can
use updated memory to extract relevant parts. Okay, so this called retrieval. Okay, use relevant
parts to solve problem. So I really like this framework because this knowledge graph as memory
thing is something quite interesting. But how can we express it as memory? That's the difficult part.
Okay, so Richard asked, have I tried putting the graph into an FAIS index? No, I haven't.
Yeah, but how, how will you do a knowledge graph putting onto the, like onto that index? Because
usually what I know is that you do the embedding and then you put the text test for the retrieval
of method generation. If you're doing knowledge graph, maybe you put the source as the index.
Okay, I'm not too sure. I'm going to check on this, like how will you do this into PyCon and stuff
like that. But what I can imagine you doing for the knowledge graph is just put the whole thing
into some array, and then just store the array. I mean, you can even put it as a JSON. Yeah.
So yeah. Okay, I don't have time to cover through the running of the Jupyter notebook. I'll just
upload that separately. It's another video. But let's just go through the last five to 10 minutes.
I'm okay to extend about 15 minutes if you all have more things to discuss.
Like we have discussed like how can we use knowledge graph better for last language models.
So first question, what are the failure modes of using knowledge graph for context representation
and I think this failure mode is mainly like your knowledge graph may not capture all information.
Okay, and also the knowledge graph capturing
might truncate the information. So maybe using text directly,
maybe better. Okay, but harder to pass because if you are using text directly, you don't really have
like nice sections where you can pass the knowledge graph on. Yeah, so these are some of the things
about right now some of the failure modes of this knowledge graph. Anyone else has anything to add?
Just some random thoughts. Do you think it makes sense if we view the embedding space itself
as a form of generalized knowledge graph?
Embedding space as a general. You mean the LM embedding space?
Oh, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,
a separate dedicated embedding space. So because like you have all your
entities inside that space and the relative, I don't know, relative positioning of the kind
of encode certain relative information of them, right? Because I think the issue you mentioned
here is, I think it's just the graph, learn graph can be too sparse, right? You lose a lot of
information, but if inside embedding space, I don't know, it might help preserve more
information, although not very explicit information. Just some random thoughts here.
Okay, I get what you mean. Like you encode knowledge graph as embedding space, so like your source
relation and the output are all embedding space and sink and destination.
I feel like my gut feeling is it can be much richer than just a typical traditional graph.
Kind of a great view. It's just going to be hard to express the embedding space
using an OpenAI API. You might need to have access to the last image model directly if you want to do this.
Yeah, but definitely that's one of the ways that we can represent knowledge graphs.
Anyway, this is the second question also. Should we utilize the embedding space, perhaps
for more expressive knowledge graph? Okay, but then if you think about like what I was talking
about earlier, like context dependent embeddings. If you are talking about context dependent
embeddings, actually we can use LLM to pass and update embeddings based on the parents of the node.
So I was thinking of something like that. You can actually have a very, very
different interpretation of a certain word, like for example, bank. It can be river bank or
financial bank depending on the context of it. If you are talking about riverside, then it's like
river bank. So you can actually use the last image model, extract out the hierarchy of the graph,
the front part of the graph. You can put it there and you can then pass the embeddings accordingly.
So I'm still thinking that perhaps just using the last image model embedding directly
might be a better bet. And then you can just maybe use last image model to do this context thing.
And then you can put this embeddings inside your knowledge graph, like what you said earlier.
If there's a way to get the embedding space directly from the OpenAI API, that would be great.
But if not, we might have to use LLM2 in order to do this embedding space knowledge graph.
But then again, is it really necessary? Can we just use text? So this is a big open question.
Should we use embedding space for the knowledge graph or can we just represent it as text
and then use the LLM to generate embeddings after that? So I'll leave that as an open question.
I think both approaches are valid approaches. I just feel like the way to input the knowledge
graph as text is it will be much more interpretable and also you only need to train one embedding
space, which is the LLM embedding space. So I kind of prefer that. Anyone else has any things to add?
Okay, if not, we go to the next question. Can LLMs help with a more flexible interpretation
or construction of a knowledge graph? Okay, so I will answer first. I think yes, definitely.
Just like compared to like spacey or like on noun, pro verb, those kind of stuff.
Like if you are doing like the parse tree, compared to those, very, very flexible.
And you are able to extract a lot more information. So like just based on the straight
JSON prompt I showed you earlier, you just hit object, relation, object, it captures almost
everything. And that's zero shot prompting. Granted, it did not capture the data first
I had to use the examples to give it the data. But compared to using this kind of like spacey
and so on, like deep learning approaches, like you'll take quite long to train a new kind of like
knowledge graph constructor. But with large language model, you can just use prompt engineering
and get your knowledge graph out. I think that's very exciting.
Okay, last question. How do we know what nodes are important to construct in the knowledge graph?
Okay, because there's a lot of information, but not everything is needed for your use case.
How do we know? Okay, so my opinion, okay, my opinion is this, you need to have biases
based on the domain. And what are these biases? Maybe you can have multiple biases.
Okay, and then let's just choose the right biases later. So this is my idea of intelligence right
now. Okay, I'll share with you. Okay, this idea of intelligence is that there's not just one
abstraction space where you store your information, you store them in multiple abstraction space.
How do we get all these abstraction spaces? We basically just do rule-based abstraction,
like maybe one domain is saying that, oh, dates are important. So I store the dates.
Another domain is like, all people, all person's names are important. I store the person's names.
Then maybe another domain would be like, all places are important. I store the places.
Then when you want to solve the problem, okay, you will see which space is the best
for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,
maybe combine two or more, or you just take one, and then whatever solves the problem works.
So this would form an approach that will be used later on. So if you think about it,
I'm just going to draw it here. Okay, I don't know whether I have space to draw it,
but if you look at the top right of the screen, okay, you have a problem, you have multiple
abstraction spaces, let's call this A, you have another abstraction space here, let's call it B,
and we have another expression space, we call it C. All right, so if we have three
abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the
problem. And then in order to solve any arbitrary problem later, you just mix and match the abstraction
spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.
Yeah, I also use this for my abstraction reasoning cobbler's paper. So this is the idea that I have
right now, you have different abstraction spaces, all these are rule-based, okay, we don't really
have deep learning here, because if you have deep learning, you have problems in getting a fixed
abstraction space, you don't want the abstraction space to change. Okay, because if you change this
abstraction space, you have to change whatever you learn on it. It's like, if I suddenly told you
that math, the addition is now subtraction, that I have to relearn all my math again, because I need
to update that new knowledge. So I'm saying that the basis is fixed, but then you just choose the
right basis to solve it. Okay, then you might ask me, if we do it like that, what if we don't have
the right basis to solve the problem? Okay, then the answer is you can't solve the problem. Okay,
which might sound a bit crude to people, but I feel like we can't solve everything. Like even humans,
we have our limitations. It's just that we work around our limitations and try to use our existing
biases to solve new problems. And I think there's intelligence, like we don't really need to change
the abstraction spaces. We can just work with getting multiple abstraction spaces and then
just combining them. So I shared a bit about my view of intelligence here. And yeah,
that's more or less it for the questions to ponder. Anyone has anything else to add for any of these
questions? I just want to clarify one thing. I'm still not very sure about the motivation here.
So why we want this knowledge graph? Why use a knowledge graph, is it? Because the internal
representation of LLMS is already reached and knowledge graph. So for example, the
sake hand paper, so you can just use the last language model to help you to break down the
task, right? So in a sense, actually, because the way I did it before is I want to construct,
especially construct a knowledge graph first, then I use that knowledge graph to break down the task
for like a hierarchical or like sequential learning of intermediate goal for RL.
But in a sense, you don't really need, like for that particular task at least,
you don't really need to do that anymore because you just query the LLMS model like
what are the, at least the common sense way of break down a certain task. So you already have the
different sub tasks, right? So like, so because my understanding is more like
from that, coming from that perspective, so I'm thinking like, why do I still need the
knowledge graph? I mean, as what I say in one of the first few slides, if you use like retrieval
augmented generation, you might miss out certain chunks of text. Knowledge graph provides a sort
of hierarchy that you can pass over and extract information. So it provides the structure.
Oh, but you still, you mentioned the problem is like, even with knowledge, you're still,
you have to distill information. So there is, there is inevitable that certain
informations are lost, right? So it'd be in a, in a, in a way of like, you do chunk first,
chunking first, then you summarize each chunk, you summarize again, maybe like a hierarchical
way of summarizing and all, even your knowledge graph, you only, you only extract that the relation
you think is important between different entities, right? So to face this issue of having information
lost, I don't have any information lost. Yeah, so maybe the correct way is not the existing kind
of knowledge graph, but a new kind of knowledge graph. Yeah, but I do believe a graph based
representation of knowledge is important, because when you learn new things, we typically try to
fit in with our existing knowledge and we build on the knowledge from there. So if you have some
form of graph structure to represent knowledge, you can actually like use that for learning as well.
And that's where I'm coming from. Yeah. My, my, my intuition is, you know, it's, it can be, okay,
so in that case, right, I can see where, how it can be used, we can be served as a heuristic for
search. So if you want to like, for example, understand a very large chunk of text, right,
you, you, if you extract like a rough, a rough representation of the information, then you'll
just do a heuristic search based on that. So even if this information loss is still can
expand upon just based on your existing knowledge graph, like maybe it just gave you a
some useful signal to tell you like which part of the text you want to do some search, then you
can just go and search. You don't need necessarily to stick with the strictly stick with the, the
graph. So in that way, like just a heuristic bias, a useful one. Yeah, yeah. That's one way of doing
it, like using a heuristic to search. So it's like replacing the cosine similarity
in retrieval mental generation, you just pass through the knowledge graph.
Yeah. And because, because the chunking, like just naive way, it's just, it's just like, because
the way we structure essay or structure text is not necessarily just like each different
hierarchy, like different object or different something that is, that is quite intricate,
right? If, for example, for some novel, you have foreshadowing, you have different way of writing,
you have plot device, then you suddenly add a certain very, you think apparently very random
point, suddenly become very important. So just by naively chunking into like evenly
it doesn't, it doesn't work. But if you have a graph that sort of tells you this kind of structure,
then you're based on that to do some similarity. Definitely. I feel like it can be more efficient.
So I think they give you a better way of doing chunking. Yeah. Yeah. Yeah. I like this approach.
In fact, that's one of the motivations of using knowledge graph as well in order to
find a better way to pass it. I mean, we can pass through graphs pretty well. So
we can perhaps get better retrieval using knowledge graphs. So that's one provided you have all the
information in your knowledge graph, you can get better retrieval using knowledge graphs.
Provided the first part again, because that's the failure case we saw just now in the
length chain graph answering agent. But anyway, I think all the decision all stand from the fact
that the compass window is very limited. So if we can solve that problem, then here actually,
yeah, I mean, both way either way. Yeah, I'm quite excited about this. In fact, I will spend like
the next few weeks trying to create a new knowledge graph. So I'll share with you all after I created
because there needs to be some context dependent passing. And that's lacking right now in the
knowledge graph that I see so far. Yeah. You agree with the context dependent passing, right? Like
how you interpret a certain like note actually depends on the parents or depends on the
position in the graph. Yeah, even from a very traditional perspective, this
very crucial as well, I believe. Yeah, nice. Okay, last minute or so. Anyone has any last points
do you want to add? Okay, if not, thanks for coming. And yeah, if anything, you can still
reach out to me on Discord or LinkedIn. Yeah, and I'm looking forward to do this linkage between
large and rich models and knowledge graph. This is what a lot of people call a neural symbolic.
And I think this will be very crucial for intelligence. And I can see how we can use this
knowledge graph approach to learn stuff from the environment and use it in a learning agent.
Like I like to call it reinforcement learning agent, but it's not really reinforcement learning
because there are no rewards. Okay, you can just learn directly from knowledge in the memory itself.
So I think this will be very crucial for that kind of framework. And yeah,
hope to share more of your after experiment with it. Okay, if not, yeah, thanks for coming.
And I'll see you around. Okay, bye, friend.

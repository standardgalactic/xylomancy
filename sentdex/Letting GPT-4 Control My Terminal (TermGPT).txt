Giving GPT-4 full access to my terminal.
For some of you, you're way too excited about the prospects of such a thing,
and this probably scares the heck out of many others of you.
The idea is actually pretty simple and safe.
We're really just trying to make prototyping and R&D with GPT-4 and models like it much more fluid.
As it stands right now, you might go to the chat GPT UI or some other chat UI
and ask it for something like a matplotlib graphing example.
You'll wait for the generation, then you'll copy and paste it into an editor,
and then run it, and then maybe then you'll ask for some changes,
more weight, more copy and pasta, and so on.
What I want to do instead is eliminate a lot of the sort of waiting
and going from prompt to result, and I really want to just make this as quickly
happen as quickly as possible, as well as require as little effort as possible.
While still reviewing, what the model actually wishes to do before allowing it to do that.
We can even do the edit to dark theme too in this way.
For something so simple as just a matplotlib example,
these are fairly minimal time savings, although I think the effort is still pretty low comparatively.
But you can also quickly see that we issued one prompt and then we waited for GPT-4 to
generate all the commands at once, each of which would have been a separate kind of manual
back and forth for us, especially if we hit some sort of error or we're just waiting for something
to happen. But instead, we can review and run all of the commands in one go.
As we continue to ask for more complicated things or even projects that might require
multiple files or even installations of packages in setting up an environment,
or even as we want maybe to modify those files, this sort of structure can wind up saving us
a dawn of time and energy. So let's see some more fancy examples.
Imagine that you found some sort of model on Huggingface, like this code completion
language model from Replit. That's very lightweight. And you'd like to make a demo with it by
implementing the model into some sort of locally hosted web page with a text form input for the
prompt and then some logic to display the output, you know, the continued text. Some very common
use case for this particular model. So probably GPT-4 really first needs to know
what model do we want to use and then maybe do some light reading on the model's page to
understand a bit about, you know, how to actually work with this model. Temp GPT doesn't have any
web parsing ability built in, but it can program for us and we can certainly parse with a program.
So we can start off by asking Temp GPT to do just that for us. And it does this first by
installing beautiful soup for Python, making the directory we asked for, and then writing
some web scraping code to get the text from that page and then saving it to the text file that
we requested. We can then use term GPT to read that content dot text file and create a web app
for us that demos its abilities. We did hit an error along the way, which I just copy and paste
to term GPT both times and it is fixed. This is one thing I'm considering fully automating and
I'm trying to do that by allowing term GPT to read the terminal outputs to determine if more
commands or changes need to be made, if an error is at least detected. The error and suggestion
in the error text here could have been ingested by term GPT to easily implement and fix not really
much different than us copying and pasting the error ourselves. Anyway, after reading the error,
term GPT finds a way to remedy the problem and our program works. This has been extremely minimal
with effort compared to what I would have done, even if I was either traditionally doing this by
hand or even via the chat UI for chat GPT. And we can definitely do this in an even more like less
clunky way. Speaking of which, how about a web parser idea? Can we use term GPT to modify a pre-existing
project? Absolutely. Might we actually use term GPT to modify itself to add something like a web
parser? Here I ask term GPT to implement a web parsing feature similar to my file reading feature
and it modifies multiple areas of the term GPT script to do this. Testing this feature, we can
ask term GPT to parse some web page and then save a summary of that text to file. For example,
let's parse the readme page from GitHub for TensorFlow and see if GPT4 can summarize it for
us into one sentence about what TensorFlow is. In this case, though, it is plausible that GPT4
could also just have used its own underlying knowledge of TensorFlow. For example, we could
just outright ask GPT4 about TensorFlow without any other context, and it could summarize into
one sentence. Instead, let's ask about something newer so we could ask about stable studio from
stability AI. Asking GPT4 via the chat UI and it looks like this time GPT4 doesn't actually know,
which makes sense. This is relatively new and GPT4's knowledge stops in like late March or
something like that. So then we can go back to our example, parse that readme and get a good
summary of it. Okay, so those are just some really quick examples of what's possible. And to be honest,
this has all just been kind of my first attempts as I'm trying to kind of make this happen as this
is a common at least personally workflow for me. And I really just wanted to stop, you know,
highlighting or clicking copy code, paste, run, copy some error, go back like all this like back
and forth was getting very old. And I really just wanted to come up with a solution that would just
automatically kind of do these things for me execute the commands for me, all of that. So this
is really my first run through. And there are for sure going to be better ways to do the things
that I'm doing here. And I definitely there's lots of things that we could build on and I'm going
to be hosting this code. So if you have ideas or things you want to try, feel free to fork or
submit a poll request or anything like that. So with that, let's go ahead and dive into the code
and see how everything is working under the hood. All right, so here with the code, let's go ahead
and run through how everything works. So first, we've just got a bunch of imports, open AI for the
API, of course, Colorama to make things pretty time for a sleep context. This is how I'm setting
the pre prompt, we will come to that in a moment. But for now, that's just that's it's the pre prompt
sub process. This is for attempting to execute commands and get their output. I'm still struggling
with this. Maybe someone can help me figure out what the heck why this doesn't work. I'm sure it's
probably some super simple logic problem. But anyway, read for regular expressions, requests,
and beautiful soup for the web parsing capability that term GPT actually added for itself.
Again, and this actually isn't really all that required, since we term GPT can already write
a web parser. But the real kicker here is, you know, it could also write code to open up a file.
But what you want to be able to do is read that information into context. So doing that, I think
still kind of requires these features. But I want to kind of clean this up, and quite possibly make
it a little more dynamic. So for example, like pulling from a website, do you really just want
the paragraph text? Do we want to also handle for JavaScript that happens to populate things?
You know, so this is a really basic example, but this won't get you very far. So anyway,
I'm still kind of thinking about how I really want to do that. But anyway, moving along,
the open AI key, really all the open AI stuff, I've got a video on that, I'll try to remember to
put a link in the description for the basics of working with that API. But anyway, that's where
our key is. Debug this just for a few extra print statements, just to mostly for outputting the full
context every time I query GPT for just because that'll tell you for sure what exactly you're
sending and getting back. I find that to be the biggest amount of information that you don't always
want to see. But if you're having a problem, you probably want to see that whole thing. So anyway,
moving along message history is that terminal commands, that's just like the from the context
that initial stuff that comes in. Then we have the regular expression pattern for the sort of
functionality of reading a file or reading a website. So this is just read a file, this is
w for website. It's just going to follow that again, we really could use large language models to
also parse this out. And it wouldn't even need to be struck like super structured in any way,
it could just read some text and then determine, is there a file that should be read? If yes,
you know, read that file, add that in so you don't necessarily have to do it this way. Again,
more on that and the context and all that stuff in a little bit because eventually I would like
to do this without using open AI, I'd like to use an open source model. So I just don't want to get
too deep into prompt engineering with GPT for when I'd like to I'm kind of like holding out for
a really solid open source model with a solid license. So no llama. Anyway, moving on GPT
query. So this is pretty much nothing new since the GPT for tutorial. So again, if any of this is
confusing to you other than maybe the color Rama stuff, this just sets a color sets styling basically
does the text whatever and then resets it back to normal said that's just that's just
typical color Rama stuff. But anyway, if the debug is true, we are sending the entire message
history, which is the full context that we're sending off to GPT for. So if anything's not
working the way we're expecting, that's the first thing that we're going to do. So for example,
reading the console outputs, like trying to get the return on in the console.
It doesn't give the most recent command, no matter how hard I try. So I don't know. At first,
I thought it was while I'm hitting errors. And I was thinking the error is causing my problem.
But I'm pretty sure I'm handling it such that you would still you should still capture the error.
But for some reason, I'm just not. So anyway, moving along. So this is just a query GPT for API.
Extract paragraphs. This is the code that GPT for slash term GPT wrote and modified into our script
for us. Just simply reads a website grabs all the paragraph text. So really, really basic. Again,
I think this should be a little better. But moving along. So then we've just got some pretty colorama
stuff. Welcome to term GPT. It just tells you kind of the point of term GPT is just to make
prototyping faster, not a whole bunch of other fancy stuff. It's just make prototyping an R&D
really quick, but don't run commands without still being able to review them. So continuing
along. So there's just some more color color stuff, you've seen what it outputs already.
So then we get the input, whatever the user wants to, you know, put in the command,
and then we have these very simple ones like clear, this just clears out that whole contextual
history and just resets us back to terminal commands. We'll talk about we'll show those
momentarily output. This reads theoretically, the output from the system basically in the terminal,
what you know, what was the response back to that terminal, what was printed out in that terminal.
So if you did hit an error, it would be nice to be able to say output or, you know, call this
command. And then it would read that rather than you needing to copy and paste in the error. But
again, it's not really working. So I'm going to copy and paste the error still. Otherwise,
what it's going to do is it's going to look for any matches for do we want to read any files and
it can do any number of files. And then essentially what it's going to do is either for files or
websites that text that it parses, it's just going to shove it in and just it's going to build that
context. And so it's just going to populate that context with the actual text. So it just
adds it in. So you'll say I have this file here, and then you'll say you'll have like the read command
and it will quite literally say the file name and then the content of the file. So again,
no different than what you would say, Hey, I've got this Python file, here's the code copy paste,
you paste in the code. This is the same exact thing, it just builds that context for me. So once
we have that user input, which is going to be, you know, please help me make a flash website or
please help me do, you know, whatever it is, we have that user input, we're going to send that
off to GPT. And we're essentially going to wait until GPT is done. So what does that mean? So
now it's time to go to our context. So this is the pre prompt that I'm using to force GPT for
to behave in the way that I want. So essentially, this is all sent off. And this is how the program
begins. So we're starting off by just telling GPT for, let me add word wrap real quick, make this
a little easier on us. We're saying, as if I had told it, you know, given text input from a user
looking to do programming, help me build a series of console commands that will achieve the goals
of the prompt, your responses will come one command at a time where each response is just
the command, and nothing else, when ready for the next command, the user will say next all
camps next, and you will respond with the next command, when we have reached the end of console
commands, respond with done. And then you will do it again with some new input from the user.
If you understand, say okay. And again, we're just forcing the assistant here, you know,
to think in historically, it said, okay. And then what I'm doing is a one shot kind of example.
I did find GPT for to be pretty good at following this command. But very often it's still added
commentary to the commands and all that. And yes, we could use literally a large language model to
parse out the command still. I don't like that. So I wanted to do at least a one shot, I thought
maybe it might take more but a one shot gives us an opportunity to show it even more basic stuff.
So like for example, in this case, we're using cat to write to these files. There's lots of
ways that this could have been done. But given all of the huge variability of stuff that can happen
inside of a script, it would likely break many of the methods that might be used to populate a
file. So this one is just the one that works. So it allows us to kind of still kind of show
really what we're looking for. So for example, with such a basic thing as show me a basic
flask web dev example with templates, right? And the first command, in this case, we're making
this up, we're showing GPT for how we want it to behave. So we're saying, if someone asks this,
here's an example of the sequence of events that should occur. First, you're going to need to pip
install flask. And then the user is going to say, next, and then GPT for is going to respond with,
you're going to want to make a directory basic flask app. And next, so we're going to, you know,
so already we're trying to get it to kind of put this into specific project folders,
then what it's going to do is actually write, you know, and then into that directory and then
app.py. It's going to add just a very basic app.py with some template examples. Thank you.
And then, okay, that's, that's what it says. And then user next, and then it wants to make a template
stir, add that index.html next, blah, blah, blah, you get the idea all the way down to actually
running that app.py user says next, and actually we're done because we've run the app. So it's
just a really simple example that we're showing GPT for this is how I want you to respond every time.
And then if that way, if the user did happen to say clear or whatever, we just reset right back
to this context and continue on. But I found that you can generally keep going pretty far and be
totally fine. But if you are experiencing weird things, you would just clear it and start over.
But anyway, coming over here, while not GPT done, so GPT done starts as where the heck are you here
because we haven't started a new input. So we're going to start as GPT done equals false. So then
what's going to happen is as long as that's true, what we're going to do is we're going to wait for
the next command from the user, right. And essentially, it's going to just inject that.
So we don't need to say next, we're just going to automatically tell GPT for next in the script,
and we're just waiting until we literally see reply content that lower. The only reason I'm
doing this GPT for always did an all caps done. It was good. But chat GPT, for example, I was just
trying between the two of them to see if we save some money. It would sometimes lower case
sometimes out of periods. So I was like trying to handle for that. But again, GPT for really
follows this very well, I'm sure I could come up with ways to structure things such that chat GPT
or GPT 3.5 works. There's no reason in my head that this couldn't be done with chat GPT. But
GPT for is just way better than chat GPT. So anyway, moving on. So if it has reached a done,
then we set done to true, cool. And we break out. And then essentially, all the way up to this point,
we've been every time we get a command, we've been adding that command to a list of commands
right here. Once we have that list of commands, you could have been theoretically running them,
getting the return and plausibly adding this to the context or something just to see did you hit
an arrow along the way. I think that's a mistake because either you're gonna have to sit there
and keep clicking like next or saying next or whatever. Or you're going to be blindly running
commands. And you're probably more likely to just kind of blindly just hit go, go, go. And I don't
think that's a good idea. So, so we're going to build that that whole list. And then we're essentially
going to iterate through that whole writ list printed printing it out in bold, red, making it
clear what the commands are that are going to be run. And then, and then if we want to actually
run those commands, then we are going to go ahead and iterate through all of them and run them.
And this is where I'm attempting to get the actual output. This does work all the way up to, I believe,
the last output, I need to look a little deeper into that and see if there's got to be something
wrong with my logic or something like I feel like this should work. But for whatever reason,
it's just not working. So anyway, if the person if the user puts in anything other than why here,
it just okay, it just doesn't run the commands. And that's fine. Sometimes still, it does suggest
to you, it might still give you some text, especially I found that to be true with the console
output. It'll just say I didn't find any errors or something like that. So it doesn't say done,
like it should. But anyway, that's all there is to it, all of the power of this, this is all just
a bunch of stupid logic, really around what is the pre prompt. And what we can do with a pre prompt
is, I mean, there's essentially infinite pre prompts that we could theoretically use.
There are better examples plausibly that we could show it. And there's more capabilities
that we could show it, or teach it or even just kind of that might show up. So even given this
really basic example, I have found term GPT using GPT for to be really impressive, it really hasn't
let me down on any task yet. Certainly, there's really nothing that GPT for could do for me via
like the chat UI that I haven't been able to just do via term GPT. And it just it's just been so much
easier. And again, I made this really for myself, and I'm just sharing it with people. And it's for
sure a work in progress. There are some relatively new open source large language models that I'd
really like to see, can they fill in this gap? Because like I said, even chat GPT
is weak ish at this again, I think I could come up with prompts that chat GPT or GPT 3.5 rather
could be successful with. So I just I don't want to put in a bunch of time making chat GPT work,
I'd rather it be some sort of open source model with a permissive license. So anyway,
stay tuned for that. And again, I will put everything up on GitHub. Feel free to make a
fork or whatever or make pull requests and just kind of tinker with this. I think this is kind of
cool. I like the idea of going back and forth like this. And just doing the R&D with GPT 4,
but just removing all of that extra effort, it's been it's been really fun using it. So
anyway, hopefully you guys will enjoy it. If you've got questions, comments, concerns, whatever
suggestions, feel free to leave them below. Otherwise, I will see you all in another video.
If you've ever wondered and wanted to know more about how neural networks actually work,
including the optimization and fitment, rather than just simply calling some method,
then you might be interested in checking out the neural networks from scratch book
by myself and Daniel Kukawa. The book can be had in the form of an ebook PDF, soft cover,
or hardcover, and we ship for free worldwide. Also, the physical books just come with an
ebook copy. All copies are in full color, which helps because there's a lot of code syntax highlighting
and lots of charts and diagrams to help convey the principles. Also, almost all of those charts
and diagrams have QR codes and links that take you to animations to help further illustrate the
concepts. This is truly a real neural networks from scratch, teaching everything from the
forward pass calculation of laws, back propagation and optimization. The only math that you're
expected to know coming in is basic algebra. The rest is taught by us in the book step by step.
Everything is shown and explained in the book first logically, then mathematically, then via
raw Python code, no third party libraries, and then finally optimized via NumPy. And this is for
every step of the way building and training actual neural networks for a fully fundamental
understanding of neural networks and how they work from scratch. If at any point you're lost
or confused, all copies of the book also grant access to a Google Docs version of the book,
where you can ask your questions in line with the actual text itself. This is an incredible
project that I'm extremely proud of to share with you all. We've sold over 13,000 copies to
students all over the world. If you're looking to take your knowledge of deep learning to the next
level, or if you're just looking to start that journey, I can't imagine a better way.
So to learn more and buy yourself a copy, you can head to nnfs.io.

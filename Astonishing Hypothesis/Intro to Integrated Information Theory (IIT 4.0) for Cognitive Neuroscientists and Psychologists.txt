Thank you very much for the very kind considering
introduction, Renee.
I'd like to get started by paying homage and thank
the people that allow me to do this kind of work.
Those of you who don't know, there's basically
dozens of people involved downstairs
in the facility that take care of our furry colleagues
and keep the operation running.
And to these people, I want to thank the various funding
sources, of course, the people in my lab,
and then also this particular group of people, two of which
have moved on and are doing post-doc already,
that collected the data, some of which
that I will show to you guys today.
And last but not least, now to Kia.
And so each time I get to talk about this
in front of a group of young aspiring scientists,
I want to point this out.
You should not just try to connect vertically,
and that works vertically with people
that are more senior to you.
You should really strive to branch out horizontally
and make friends with people that are in your cohort,
because they will become more important to you
as your career goes along.
Now is such a case.
I met now as a graduate student.
We had very strong fellow interests.
And we've been collaborating basically ever since.
And this has really borne fruit, as you will see hopefully
today.
So actually, I want to give all the credit to now,
because the ideas that I'm going to show you
largely stem from now.
And so I'm just basically a humble messenger
to introduce you to his theories.
So what allows me to do that?
Well, there's a variety of factors
that allows me to do that.
One of them is the situation that we're all in,
which is that many of us were forced
to spend more time at home and with family.
And so very quickly, you might have also found
that you have more time on your hand
than what should be done with it.
Well, for me, that meant trying to keep connected
with other scientists.
And I felt I wasn't the only one.
So what happened was that more and more on YouTube,
channels popped up, where scientists
were inviting each other to Zoom talks.
And rather than just doing that over a closed Zoom setting
where you needed to have a password,
they would stream that live on YouTube
and other scientists could come in.
And so these are three YouTube channels
that I just want to point out
that I basically became addicted to.
And they launched me into what I'm talking to you guys today.
So I'm advertising for you guys to use your spare time,
use the time before you fall asleep,
go on YouTube, find these channels.
They're rapidly growing.
And I think they're revolutionizing science
because that allows you to find topics
that you are really interested in,
find the other four people on the planet
that equally interested in that
and have a rapidly evolving field.
So huge advantages to do this kind of approach.
And so I'm not just talking theory here.
What these three channels have in common
is largely what I would talk about today
has already spurred a lot of things into action.
So this society,
the Association for the Mathematical Consciousness Science
just got founded about a month or two ago
because of these YouTube channels,
out of these YouTube channels.
And that society immediately sprung into action,
launched special issues at frontiers
and a journal called Entropy.
It's got funding from various sources
and there will be a conference two weeks from now
where many of us are coming together virtually
and we'll speak in next year.
It's expected to be in person.
So it really is,
I think a shift here that might have happened
throughout the pandemic for science
to become more international,
to become more collaborative
and for communities to grow within science around the globe.
So what is this particular community interested in?
Well, some of us might be interested in that as well,
which is the basic problem that faces neuroscience,
maybe one of the biggest problems of neuroscience,
maybe one of the biggest problems in science at all,
which is that we all believe as neuroscientists
that there's a causal connection
between the neural activity in our brain
and our conscious experience,
our perception of the world,
that the fact that we feel love, that we see colors,
the fact that as you fall into deep sleep,
the world goes away for you.
It's as if you wouldn't exist,
but as you come out into dream sleep
or back as you wake up, the world comes back for you.
So I will call this phenomenology,
the fact that you subjectively experience.
But the embarrassing fact for neuroscience
is that this hasn't worked yet.
So usually what we do as scientists
is that we find the causal connection
and then mathematically reduce one onto the other.
And that's when we say we understood it.
That's what we do in physics,
that's what we do in chemistry, biology
and a lot of neuroscience.
But for this part of our being, the phenomenology,
the fact that we have subjective experience,
this step seems to be hard to reach.
In fact, there are many people that would argue
that this is fundamentally impossible,
that we will never be able to do that as scientists.
You shouldn't even try.
Well, I'm gonna argue to the contrary.
And that is what a lot of these YouTube channels
inspired me to do.
So this is a slide from one of the labs that's involved.
And so what they're suggesting is that we have to move away
from this conventional approach of neuroscience,
of correlating inputs to the brain
with activity in the brain
or behavior perception comes out of it
and to take another step that links,
in this case, phenomenology,
our subjective experience of consciousness,
conscious perception with brain activity via math.
What do I mean by via math?
Well, if we believe that consciousness
can be explained in terms of science,
what we have accomplished as scientists,
what all of scientists rests on
is that we find laws of nature
that we're able to express these laws of nature
with mathematical formalism, gravity,
mixed valiant equations, you name it,
that's when we feel as scientists
we've actually had an accomplishment,
we've actually done it.
So can we do that for consciousness?
And so there's a pathway mapped out
and that's what I wanna share with you guys today.
Well, so what's the traditional approach?
So the traditional approach, as a lot of you familiar with,
started obviously with experimental psychology,
but I would argue that experimental psychology,
people like Wund and others,
they were still careful to touch
on this subjective consciousness experience side.
And the first people who made a bold inroad into that
was Fechner, together with his friend, Tua Vibar.
And so they found a technique,
and I'm not gonna elaborate too much about it,
to quantify subjective experience.
So to go away from saying you can't measure love,
to say, I know exactly four loves
is twice as much as two love.
And we can measure that,
a lot of you are doing that
all the time in the laboratories objectively,
and we can come up with mathematical equations
to express these laws.
A little bit later in the 1960s,
this was generalized by Stevens
by basically introducing two more techniques
that were done by the original inventors of psychophysics,
and then finding a generalization
of the mathematical laws that we have.
And then it seemed that we're getting stuck.
And so what might be that next step?
Well, what I'm gonna argue about today has many names
because it's a new mushrooming field,
but one of them I will use a lot is qualia structuralism.
And the idea is that we can mathematically express
our conscious experience, our phenomenology,
and then find mathematical laws,
how that maps onto similar abstract geometrical,
mathematical spaces derived from neural data.
That's the basic idea.
Now, the first thing that we'll have to do there
is we'll have to find the structure of qualia.
And qualia is a philosophical, fancy term
for your experience.
So I took this here from a paper by Jennifer Troublat,
who has done pioneering research into that.
And I put it on here because this paper shows
that when it comes to these spaces
that might describe our perceptual phenomenological,
subjective experience, they are non-trivial.
So what Jennifer showed and reviewed in this paper
is that these spaces might be non-euclidean,
that certain things that you think are more similar
and they move closer to the space.
If you look at them from a different angle,
all of a sudden they seem to be wider apart.
And so it might take more unconventional math,
such as quantum, then from quantum theory,
and some of the math I talk about today
to bridge this gap and to describe these things.
But it is, of course, for many of us that study perception,
not news that we can express phenomenological experience
in terms of geometrical spaces.
So this right here is the color space
that most of us are familiar with,
that all of the colors that we can see,
we can put in a three-dimensional space
and map onto what might be largely looking as a sphere.
If you're interested in music,
there's a similar geometrical description
of the phenomenological space that you hear in music,
which is that if you have a note and you go up an octave,
the note clearly is a higher note,
but at the same time there's similarity with the original note.
And if you do this with all of these notes,
you end up with a helical three-dimensional structure.
And more recently, face space is an example
of multi-dimensional spaces that people have come up with
for objects, for faces, for other structures of our experience
and mapping them into these geometrical mathematical spaces.
So if we accept this, that we can come up
with mathematical descriptions of a phenomenological space,
then what's the next step to go to the brain?
So this is the suggestion of qualia structuralism,
that let's say you have a chord
and you have another chord where you move one of the notes,
it sounds similar, it also sounds different,
and that's because it would basically be a transform
within that helical musical space
from one of these loops to another.
So if I play an example for that, let's see if this works.
Pretty much the same song,
but all I did here is transposed by an octave.
So you were listening to the same structure,
I just moved it down in this three-dimensional helical space.
So the proposal is that if we derive neural activity
from the brain and we come up with a similar method
that we're going to use in the same way,
we're going to use the same method
that we're going to use in the same way.
So we're going to use the same method
and if we come up with a similar mathematically formalized
abstract structure of neural activity,
we should see some kind of relationship
as we are experiencing one of these songs
and the other song that might resemble
in some way what the phenomenological space does.
So now, in fact, and me in this case as a co-author,
we were suggesting that there may be an isomorphism,
that you would find a similar kind of transformation
in these space, in these abstract spaces,
but there's many ways, I'm sorry about playing the song again,
many different ways that these spaces could relate
and I'm not going too much into depth here,
but category theory is a relatively new branch of mathematics,
but there's a lot of buzz around it
and that is exactly what category theory is trying to do.
It's taking different mathematical structures,
in this case here you can see geometry and algebra,
and trying to find the relations between them
and these relations that can be mathematically expressed,
we call functors.
So in the end, what we're trying to do
is we try to find a functor between the abstracted brain activity
and the mathematically formalized phenomenological structure
to translate between them.
This could be isomorphism, this could be something else.
Well, I hope I convinced you that this part here
is a fruitful research program and already underway
and you might agree with me with this,
but you might have doubts about this one.
So how do we get to these structures from brain activity
that would allow us to find some kind of mapping
between the structuralized qualia space and neuronal space?
Yes?
So what I'm saying, so most of them are correlative.
And so what I would go to now is that I'm trying to find,
rather than a Pearson correlation,
what I'm trying to go at is something that's actually
more like a mathematical law of nature.
And I don't want to go too much into the problems
that correlational approaches have,
but what I'm going to make it to in the next couple of slides
is that we're trying to get something that's deeper
in terms of the way that we're going to look at it.
So I'm going to go ahead and do a little bit of research
and then I'm going to get something that's deeper
in terms of breaking correlation into causes structures
and that might allow us to make that look more directly.
So I might not convince you yet,
but maybe at the end of the talk we can talk about it.
What are the approaches for correlation?
Well, okay, some of the ones that I think about are,
but maybe we should talk more about the talk.
So the theory that, again, I'm just using here
to make that leap is integrated information theory
by Giulio Tornoni.
And there's a lot to say about integrated information theory.
It's a very complex theory that's truly interdisciplinary.
It spends a lot of different branches
of how we usually divvy up our thinking and academia.
So I'm not going to talk too much about it
in terms of what the background is and explaining it.
I think that would be at least another lecture
if not a whole graduate seminar of a whole semester,
but I will give you some of the ideas
of integrated information theory.
The first one is that if we think about consciousness,
typically this is a consensus diagram
that has made around for that last couple of years,
which is that consciousness seems to be
at least two-dimensional,
and that there's one axis,
which we would call the level of consciousness,
how much you're conscious.
You might want to call it arousal or something related.
And then the other one is the content of consciousness,
which is how much you experience,
what you experience the qualia.
And the reason that we think it's at least
as two-dimensional state is that we can take various states
of consciousness and put them within
this two-dimensional plane.
And then you find, for example, that vegetative state
is a state of coma where people clearly have
different level of consciousness, a different arousal.
So they wake up in the morning, they open their eyes,
they go to sleep at night,
but there's no sign of actual conscious experience
in a lot of these patients.
So there's almost no content,
but there's a lot of changes in arousal.
That means we can dissociate these two parts of consciousness
and they're probably orthogonal,
as I put them right here.
Now, for understanding consciousness,
I think the really interesting case is up here,
where you are highly aroused and you have a lot of content,
as I hope in a state that you're still in right now.
I'd probably get you more to this state
as I keep talking on.
But in this state, what I'm arguing is that you still
don't fully have all the access
to what consciousness is doing.
So the example for that would be over here.
If you read this really fast,
you would say, oh, I read a bird in the bush.
And if I say, well, try again, you would say,
oh, wait a minute, it says a bird in the bush.
But the first time you see that,
you probably didn't see the second the.
So that's a famous example of repetition blindness.
So that means that even if you're up here
and you have the highest level of consciousness,
there are still parts of the world
that are closed up to you that you fail to experience.
And now when I point it out,
you can actually experience.
Yes, question.
What are content-free consciousness,
because often, as I count on consciousness,
you get things like keys on you,
which are the prototypical example of a content-free
consciousness.
Like they have full arousal,
but they actually don't have quality.
And we don't consider them as conscious.
Yeah, so I would argue it would be more to this.
It doesn't go with our traditional notion
of subjective experience, right?
So you have a living being
that's undergoing all the signs of arousal,
but it doesn't have any, the light are off.
There's no subjective experience.
That person is conscious.
That person would be conscious in this definition,
but not in this definition.
And that has led to a lot of debate in the field,
because there's been a lot of confusion,
misunderstanding about it.
So you can have, if you do anesthesia,
for example, for more than 24 hours,
some of us do, they know that the animal gets more aroused
in the morning and you have to increase the anesthesia,
but we don't think that the animal
is having any subjective experience.
But it's a sign of arousal.
So other people that would say,
well, if you're a Buddhist Zen monk
and you can reach a state of consciousness without content,
if you really look into the phenomenology of that,
there would still be some content
because there is subjective experience, right?
So I'm not talking about that.
So I'm really dissociating basically
the almost behavioral signs of arousal
from actual subjective experience.
That's my definition, though.
Others might see it differently.
Does it make sense?
It makes sense, I don't know if I can figure it out.
Yeah, okay, we can talk about that after the talk.
So luckily, this is the part that we're probably
in agreement in that I'm most interested in.
So what integrated information theory does,
and that is what it's mostly known for,
is to quantify this axis of consciousness space.
So what integrated information theory does,
it gives you a scalar, a single value,
and it tells you how much the system has
in terms of level of consciousness.
But lesson well-known is that
this orthogonal dimension is also explained
by integrated information theory,
and I put this down here.
So when you had this first experience
of a bird in the bush and you missed the second the
versus you had this experience of a bird in the bush,
there was no change in the world.
It was just a change in your mental state.
So that means you had two phenomenological states,
and that difference is also quantified
by integrated information theory as a difference
in the causal effect structure,
or the short, I will call this the CES.
So there's a delta phi, which is what IET is known for,
that shows you the levels of consciousness,
and then a delta CES, which shows you the difference
in the contents of consciousness.
Yes.
Are you saying one of those is more content than the other?
It's different.
Well, I see that, but it's like, I'm trying to figure
out your axis of content here, because...
Yeah, so in this particular example,
I would say that you have more content
when you are aware of the second the,
because you see an extra the.
But that seems questionable, right?
Because in one case, you're paying attention to the meaning,
if there's a whole lot of certain kind of processing,
whereas in the other case, you're paying attention
to something very superficial,
may not be important.
So, and then I might pay attention to the fact
that some letters touch the triangle there,
and then I'm aware of that, and I, is that important?
But that's more.
So I...
That's a fair point.
I mean, for me as a vision scientist,
just from the visual perspective,
I would say you have more content here than you have here.
But in fact, I'm not, I'm not making,
I don't want to make too much of a point
of quantification here,
when it comes to the difference in phenomenology,
more that we can,
we can look at the difference
between two phenomenological states.
It might be equally content-rich,
but the fact that there is a difference,
we can quantify the difference.
So that doesn't mean that one,
that there has to be an unequal sign between those.
Does that make sense?
Okay.
I have a related question.
Yeah.
So, this idea that content can be a real number,
maybe question of the truth,
content itself can be more or less only the same way.
So, this idea that you just have one,
do you have a number that doesn't have the most content
or even more than that?
For that, is that okay?
So, again, let's try and maybe move away
from the concept of amount of content, okay?
Just the fact that there's different contents
that there's different states of consciousness that you're in.
They can be, they can be formalized in IIT, okay?
And so that's all I'm getting at,
that there's two formally different states,
mathematically different states.
So let's make it less about the amount,
but just the fact that there's a difference
and we can mathematically get at it.
And so just to give you an idea,
so these are not just abstract ideas,
there's actual mathematical formalism associated with that.
And so this is from a paper
that I will talk about a little bit more about today,
where now and his colleagues,
they took neural data in this case from a fruit fly.
The fruit fly had multiple neurons in its mushroom body,
which would be equivalent to what we call a brain.
And then they measured phi,
which is the level of consciousness
between awake and anesthetized fruit flies.
And you can see that the theory made the correct prediction,
which is that phi should be smaller
in an anesthetized animal than in an alert animal.
This right here corresponds to that
in that they tried to get at this causal effect structure.
Now, back when they did that study,
the mathematical formalism for the causal effect structure
wasn't fully developed yet.
So that's one of the things that I'm saying,
this is really cutting edge what I'm telling you today,
but they were trying to get at that
by coming up with a geometric space
that puts the neural data of the various causes and effects
that you find across the neural data
into a multi-dimensional space,
in this case, broken down to a three-dimensional space,
and you can see that there's a difference
in the cause and effect structure as well.
So note the date.
This is really a popping edge, cutting edge publication.
So what I will show you today
is that I think we can go a step further.
So how does this come about?
Well, at the very heart of integrated information theory
is this idea that if you have a interconnected system
in which information flows with causal effectivity,
let's say that you have four neurons
and they're interconnected in this way,
and you see the causal flow between these neurons,
that if you can find out about these causal effects
by severing some of these connections,
and you can do that either experimentally
or you can do that, and this is the interesting part
that I hopefully can show you guys today,
you can do that computationally.
Analytically, with your own data.
And once you do that, you compare the mutilated system
where you severed some of these connections
with your original system,
and if there's any difference in the statistical description,
then you know what you just mutilated had a causal effect.
It actually was important for the system.
So you changed the system
by mutilating some of these connections.
But if you mutilate some of these connections
and there's no difference for the system as a whole,
then you know that these were reducible connections
that were actually not important
for the function of the system.
And this can be done mathematically.
So this right here is some of the simplified formalism
that goes with integrated information theory.
This is when you put it into practice
with Python computer code that has been available.
Again, this is the paper that I just referenced
from the fruit fly.
And all I will do today is give you a little bit
of an overview of what's going on here
and then give you resources that if you are interested
and you want to try it with your data,
how you can do that for yourself.
Okay, so why might this be interesting?
Well, more and more what we are doing
is when we're talking as neuroscientists,
especially as cognitive neuroscientists,
when we're thinking about the brain,
we're thinking about it more and more
as a causally connected network circuitry system.
And we are, because of that,
measuring more and more data simultaneously
across the brain because the idea is that
we just need to know more about what each of these
individual parts is doing and how they interact.
And there's two steps involved in that
that I think are crucial.
The first one is that rather than just looking at activation,
what integrated information theory does
is it abstracts it to information.
And so why is that interesting?
Well, activation is actually more confined
than the flow of information.
So if we have, let's say three neurons here
and three neurons here,
and they're connected with these three axons,
what activation can do is cross over
between these neurons, cross its path
because activation has to flow
along the physically hard-wired lines of an axon.
But neurons, of course, are not as simple
as these systems right here
that just receive activation and pass it on.
Neurons can act as logic gates.
They can make, they can compute.
So in this case, if I replace these neurons
with XOR gates that are only active
when depending on what the input state is,
then you can see that information now can cross the system.
So as the activation is confined
with the physical connections,
information flow is more fluid
and actually can cross the system
in various ways that the activation per se can.
So that's why abstracting from activation to information
might be interesting for many of us
in trying to understand what the brain is doing.
The second issue with collecting
all of these data simultaneously
is that most of the techniques that we then use analytically
to analyze these multi-dimensional data,
so this would be in this example,
and for Mariah, but the same gets done in EG
or in a singular physiology field,
is that we're trying to get
at the multi-dimensional of the data,
but there's always a step in there
that basically boils down to pairwise comparisons.
Very often, Pearson's correlations,
it's just a very powerful technique.
So if we do ICA, if we do graph theory,
if you look at the various steps involved,
typically somewhere you find
that there's pairwise comparisons.
And I would argue that that's a limitation
and I would argue that integrated information theory
gets past that.
So let's take this very simple example
of heavy and learning that I took
from one of the textbooks.
So if you are taking a system of three neurons,
interconnected, so these neurons,
connect onto these neurons,
and then you have weights, synaptic weights,
that you can scale up or down
to come up with a learning rule.
Well, in the conventional approach,
you would look at this correlation
and that correlation,
you look at these pairwise correlations,
you can make up a matrix
and then make this examination,
but what you're missing out on
is the synergistic effect
of these two neurons onto that neuron.
So there could be a combined causal effect
that you're missing by breaking up the system
and just looking at the pairwise correlations.
And so IIT doesn't do it.
Why doesn't it do it?
So here's an example of how
the formalism of IIT works in practice.
And the example is that you have a system
with two buttons, A and B,
and they basically have causal effects on,
in this case, let's say a light or another button C.
So the idea is that we're looking at
if button A and B are inactive.
So in this case, shown here in white,
so they're not being pushed,
what does that mean for the state
of this causally affected system C?
And so you can empirically do that
by just taking a system and looking at what's happening.
So let's say you're taking many, many trials
as we often do 100 trials,
and you see what happens if A and B are unpushed
and what happens to C.
And so you see that in this case,
in 90% of the cases, C would also be off.
And then 10% of the cases, C would be on.
So this could be because of noise,
this could be because of a whole lot of different mechanisms.
And then the logic is to just step through
all of the possible states.
So you ask what happens if A is pushed,
and you can see in this 90% again, C is off.
What happens if B is pushed?
And here the interesting case,
what if both of them are pushed at the same time?
And so this, of course, some of you might notice already,
leads to a table that has been used in statistics
for a long time.
So that's what we call a transition probability matrix,
or it's at the very heart of Markov case, yes.
Is this the thing that A and B have the same effect
on C?
So like, if there's an I in question,
it would be able to exactly figure it out
in a way that would be able to have
counteracted effects on C.
Does this approach indicate that A and B
have the same effect on C?
So there's a few things to say about that.
So first of all, this technique works best
if you do know the causal structure.
So if you know that these have this kind of causal effect
on C, but you don't need to know that.
You can infer the causal structure
just by looking at the statistics of the system.
So you don't need to know that.
And so if one of them would have, let's say,
if you said a positive effect or negative effect,
it also, again, would show up in the statistics
that you're measuring that comes out.
So the real big step that I'm doing right here that's
questionable is that I'm moving from a frequentist
observation to a statistical description of the system.
But of course, that's something that a lot of statistics
us all the time.
But I'm arguing that you could observe a system, come up
with that table, and then go on to describe the system.
Other questions?
OK, so if you're with me on this,
then you would agree that you can, of course,
do this for a much more complex system.
So right here, this would be four logic elements
that are interconnected.
These could be neurons.
These could be voxels.
These could be areas.
This could be whatever you're interested in.
And one more trick that I'm doing here
is that rather than looking at how three of these elements
interact with a fourth element, I'm taking the whole system.
So I'm taking all of the possible states
that this system could be in.
All of these are on.
Three of these are on.
None of these are on.
And then what I'm doing is I'm taking a step forward in time.
So I will call this the present.
And then I'm taking a step forward in time.
And I say, if this is the present state,
what is the probability that I end up
in any of these other possible states?
So this is how I get a true transition probability matrix.
In this case, the numbers are zero and one stone
that confuse you.
This would be a deterministic system.
But what we would be measuring most of the time
would be fractions in here.
So it would be fractional probabilities
with which the system transitions from one state
into the other state.
And so one simple notion would be that this right here
is the cause.
And then this right here is the effect.
Because we're looking at what is causing what effect
by making the jump into the future.
How far you move from the present into the future?
That's, again, up to you.
So you could take a millisecond, eight millisecond,
one TR, whatever you want.
But you can look at how the system,
depending on which state it is in,
transitions into any of these other states.
Yes?
Using a Markovian assumption here,
doesn't that presume that the box
or neurons are going to have a memory of its property?
Neurons are not memory of its state, but they can stay.
Yes.
Great.
I don't know if I have time to talk about that.
But yes, this is one of the first things I ran into.
Looking at actual data.
But as you will see, maybe as I get to that,
my argument is that if there's a memory in the system,
then there's a certain expectation
to what the transition probability matrix should look like.
And we can actually use that to our advantage.
Because we can actually, for example,
we can see if there's anything that
deviates just from a system that has a simple form of memory
or it doesn't.
Another question.
So this right here is not the end of it.
So this right here is not what I really
mean by cause-effect structures.
There's a second mathematical trick
that's involved where you can break down
each of these supposed cause-effect interactions
in your data to look at which ones are actually causal.
And that's maybe the part that I'm most excited about.
So if I do this for my data, it turns out
that most of these do not actually really have causal power.
That's what I find the most interesting.
But you might already have noticed
that by doing this kind of trick with your data,
that you're already coming up with something
that is really interesting to deal with.
So by just taking your data and putting it
in this kind of space, you come up
with these very nice mathematical properties already.
And so in fact, if you think about almost anything
that we do in machine learning or big data,
starts out with NP arrays like that, two-dimensional matrices.
So I would argue that even if you're
not interested in consciousness, there's
lots of room to explore here for your data
by using this approach.
Yes?
So does this work only if you assume
that you have all the elements in the system,
as well as there are other elements you're not measuring now?
And they could also play a third variable role.
Yes, so it's another shortcoming.
I totally agree.
So ideally, you would have to do this for all the neurons
in the brain at the same time, right?
And so that's a type dream.
And in fact, as I was showing a moment,
it already gets problematic with the data that we have.
But I would argue that when we do a Fourier transform,
in fact, even when we do a simple average,
we are violating some of the rules
that apply to these algorithms.
So if you do a Fourier transform,
you should never do that on one of our data,
whether the variance and the mean are changing over time.
It should be stationary.
And people that do it, they would say, well, it still works.
And so I would say that even if you
have a system where you have confounders that
might be interacting with this as more hidden Markov chains,
whatever you want to call it, you
don't have the entire Markov blanket.
It's still interesting to look at that system
by making these assumptions.
Yeah, of course it's interesting because when
can you jump to the causal input?
Yes.
And so basically, my appeal would
be that you can do them, but you have to take them
with a grain of salt, kind of like we do already
with greater causality and other things.
But yeah, so there's an imperfect match
between what the theory and theory provides you,
and then what we as experimentalists
can do with it in practice.
But as I just said, I think there's other techniques.
Granger causality, I think, is a good one
that we violate stationarity, which is one
of its main assumptions all the time.
And yet we found interesting things with Granger causality.
And I'm just arguing there's a different way of doing that
that goes away from pairwise considerations of causality
to multidimensional ones.
Great questions.
OK, so let's talk about actual data.
So what my lab is mostly interested in are neural circuits.
And the system that we've been choosing for that
are the neural circuits that they, of course,
have the property that Isabel just mentioned,
that they do get inputs from outside the system,
but they are well-structured circuits,
and they exist across the layers of cortex.
And so one reason that they're interesting
is that technology has made huge jumps
in the last couple of years in order
to allow us to measure neuronal activity
across the layers of cortex.
So this, of course, is a commercial approach to that,
but some people don't know yet that these kinds of electrodes
that Elon Musk is using for his company,
they're actually readily available in my office
if you want to see them to research scientists.
So Elon, of course, is trying to do this in humans.
What does Elon have to say?
Electrons in human brains are the future.
You might get one sometime as well.
So thanks to the fact of machine learning,
we can do these things.
But we use these kinds of electrodes.
Electrode?
Oh, silence, Elon.
We use these kinds of electrodes to measure along the layers
of cortex together what we think is a cortical column
in the system.
And then we have these simultaneous measurements
of neurons or population activity.
And so I'm trying to get to the causal effect
structure of these.
So here's the first bad news.
So if you take this, again, could be voxels or areas.
In this case, it's electrode channels of these arrays
where we can make these measurements.
And you look at the size of the transition probability
matrix, of course, you run into a combinatorial explosion.
So for most of what I will show today,
I will be stuck at six channels because my laptop got
stuck at six channels.
So if we go up to electrodes that my lab has been using
10 years ago, you already get to numbers that are twice as
large as the number of atoms in a cell.
And then this right here is what my lab will be using,
hopefully, in a week from now.
And you can see that we're reaching numbers
that are absolutely beyond astronomical.
Again, at first, when I hear these limitations of the theory,
I feel at first maybe a little bit depressed or discouraged.
But then you go and you watch some YouTube talks again
about quantum computing and the leaps that are being done there.
And I think that these kind of problems
are technical problems.
I think that they are solvable in the long run.
Yes?
There are 10 to the 80 atoms in the total universe.
Yeah.
Yeah, I know.
There's a way more than the atoms in the universe.
So this is just one electrode with two energy channels.
So when we're talking about measuring all the neurons
in the brain, you see where the theory runs into serious
problems.
But as I said, I think these are technical limitations
and also there's other ways to dimensionality reduce before you
maybe apply the theory.
So let's use six measurements of the brain simultaneously.
And I will use this graphic to show you all the 64 states that
are possible for these six measurements.
So all of them on, all of them off.
So if these are neurons, on or off is easy to understand.
This might be a neuron firing an action potential,
not an action potential.
But most of us, we're not using neurons.
I'm not using neurons here.
So what I'm doing is I'm basically binarizing the data.
So you can just take your data, take a trial.
You take the average activity of your trial.
This might be just your fMRI signal, your EG signal,
whatever you have.
And then you just say, what's above and below.
And whenever it's above, I'm going to call it an on state.
And whenever it's below, I will call it an off state.
And so the on states, I make black and the off states,
I turn white.
So I just binarize the data.
The theory would still work if I break it down
into four different states or three.
So it just gives you a larger matrix, more states to work with.
So what does it look like for having 64 states of actual
neuron data?
And I would look at them in the present,
and then I would look at them in the future.
Well, it gives us a matrix, a transition probability matrix,
where you can see that the color here
shows the probability of the system going from one state
into the other.
And you can already see that there's
an interesting structure, most prominently,
that the system has memory.
So the system likes to stay in its own state.
And the first interesting analysis that I would propose here
is you can use this for your data,
and then you can play with the time.
You can see how much do I have to move until the system moves
out of its original state.
When I do that for my neural data,
I find that it is roughly on the order of one synapse.
So if I go past that, then the system
all of a sudden starts to go into more interesting states.
You might also feel that this is not an interesting way
to look at the data.
So what I would propose is that each time we have a matrix
like that, we can apply graph theory.
So we can take each of these states
as a node in graph theory space, and then
the causal interactions, the probability
that the state moves from one to another,
we can take as the thickness of the edges between the graph.
So if I do this, for the matrix that I just showed you,
it becomes very hard to even visualize.
So let me make it even more simple.
So what I will do now is take three measures of the brain,
and I will do this in the upper, middle,
and lower layers of cortex.
So there's only three states.
So all of them are inactive, all of them are active,
and then you can see how the system transitions
from one state to the next state.
One thing that's interesting here,
I think from a cognitive neuroscience perspective,
is that you see these loops.
So again, this is the system having a certain likelihood
to end up in its own state again.
So in a way, you can think of this as feedback.
So you can already see that feedback is more likely
for some states than for other states.
And you can, again, play around with, for example,
the time that you take in between these measurements,
and you can investigate.
Some of the feedback loops, they might become thinner
or go away, and then as you look further to your data,
they might reemerge.
As feedback re-enters the system if you're looking at,
yes, Gordon?
Why is that a feedback loop, but not just like
the state?
Or memory.
Yes, yes.
I should be more careful how I phrase these things, yeah.
So totally agnostic about it.
So the system could just persist,
could just be history since, yes.
I think it would be feedback, it would be easier to argue
if you look at that as a function of time,
and it disappears and then reappears again.
That would maybe show that there's some reverberants
going on, but yeah, that's a great point.
And so I'm cutting a long story short
of looking at these transition probability matrices
in these actual graph structure.
It does show you, when I looked at my lab data,
it does show you a lot of interesting structures.
So for example, the question before,
what if the system has a very strong memory?
Well, we do know that a lot of the neural data
is one over F distributed, so it does have memory over time.
And so what I found is that if I compare
this actual neural data to a system
that I artificially produce,
where I have control over these variables,
and I can't just introduce correlations,
however strong I want them between the data
or causal directions, or I completely uncorrelated system,
then I do find that there's a stereotypical pattern right here.
And as I said before, we could use this as a baseline.
We could say, well, that is what an uncorrelated system
looks like, and any deviation from that
would be a more interesting deviation.
Yes.
That's basically what I just said.
Yeah, so you can take artificial data
and you can uncorrelated.
And so I took it out because I want to use the rest of the talk
to get at the maybe most interesting part.
But if you take a system that's completely uncorrelated,
you end up with a structure that's called the Hamming distance.
And so the Hamming distance is that the transition probability
of a system that's in 0, 0, 1 to go to 0, 0, 0, 0,
to go to 0, 0, 1 should be higher than the transition
probability from a system to go from 0, 0, 0 to 1, 1, 1.
Because more has to change.
So the system is more likely to end up
in this state than in that state.
And so that you can measure, it comes out
as the Hamming distance, so a very well characterized
geometrical space.
And so you can either use it as a matrix,
or you can use it as a graph, and then you
can take any kind of distance that you
like between multidimensional sets of data,
the Mahalanubic distance, Euclidean norms,
and anything like that to see how far it differs.
OK.
So this is just the first step of integrated information theory.
But you can maybe see why I'm already excited about that.
As an experimentalist, we tend to look at new tools.
It might allow us to look at our data in new ways.
And I think this is what it does.
And so all of us might have this feeling
that Galileo had maybe when for the first time he
was able to look for a telescope.
And he discovered things out in the solar system
that haven't been seen before.
So I'm not arguing this as Galileo's telescope.
I'm just saying that here's another interesting tool
to look at your data that might be worth your time
and more consideration.
But there's more to it.
So what I said in the beginning is
that integrated information theory
severs some of these causal connections
and see what the system does.
And so that rests on something called due algebra that
was introduced to the field actually decades ago already
by Julia Perl.
So that's a tool of statistics.
And so again, I'm not going to do it justice, of course,
in the short amount of time that I have.
But I'm just going to give you the general idea.
So if you have a very simple statistical system,
so say you have one variable and you're
interested on its effect on the other variable,
and you have a confounding variable that affects both,
that's a very common statistical problem.
Because if you just do a correlative approach,
this right here causes huge pain.
And so we call this a fork in a causal statistical sense.
So what Julia Perl said is that we
can get at the causal effect right here
beyond a correlative approach using his due calculus.
So what is this magical approach?
Well, we know as scientists that in order
to find causality, we have to make interventions.
You have to intervene into the system,
and then you see causal effects.
What due calculus allows you to do
is to intervene into the system without physically doing so.
So you're intervening it statistically, if you will.
So how do you do that?
Well, if you think of this system described in a similar way
as I just did, where you know the transition
probabilities between each of these states of the system.
So you have a full statistical description of the system,
and you also know the cause of graph.
So you know the cause of connections.
What you can do is you can computationally
keep one of these variables constant or manipulated
and then study the effect on the whole system.
So in this case, if you keep this variable
at a certain value, you're severing this connection.
Now this variable right here, the confounder,
can't affect this one anymore.
Because you're keeping it constant.
It can't change.
So whatever this guy does, it doesn't affect this guy.
Does it make sense?
It can still affect this guy,
but it can't affect this guy anymore.
So you're making an intervention,
and you're also doing what I just said.
You're mutilating the system
by getting rid of one of the cause of connections.
So you're different in targeted attack analysis
in graph theory?
No, I think it, it's out of my wheelhouse,
but it might be very similar, yeah.
So let me give you a concrete example.
So if you have a simple transition probability
matrix, so in this case,
you have only two different parts of the system,
and either both are off, one is on,
the other's on, or both are on.
And then you look at a T plus some point in the future,
and you look at the transition probabilities,
and you wanna make this intervention.
So you wanna mutilate the system.
So what you can do is you can disconnect the system
through what in this case
would be called statistical noising or marginalizing.
So in this case, let's say the first element,
we wanna get rid of, and so what you can see right here
is that in this case, and that case,
if we eliminate it, or eliminate it,
it comes out as basically just averaging
these two states together, because this is zero zero,
so let's average where the second one is in zero,
and you get this transition probability,
or in this case, the second element is one and one.
So we don't care about this anymore,
so we just average that, and we come out at point five.
So you come up with a new transition probability matrix
where you artificially intervene,
and you took out part of the system.
Now, this transition probability matrix is different
than this transition probability matrix.
So if you look at the transition probabilities,
it looks like you severed something
that you shouldn't have.
There's a causal effect in the system.
If you would have done this,
and it would be exactly the same outcome,
then what you just severed,
what you artificially intervened didn't have an effect,
and you can basically eliminate it in this approach, yes?
Yeah.
So you started describing this by saying,
assuming you know, but, I mean,
this is a very simple third variable problem.
I mean, that's what we want it now, usually,
and that's simple, just with three variables,
trying to figure out what's needed, what, and I did that.
I mean, we can't turn this,
the correlation and the correlation, with three.
Yes.
What aspect of this is fixing that problem?
Yeah.
So two responses, again, you might not find them satisfactory.
So the first one is that I would say,
we just make that assumption again.
In this case, if you're looking at an individual column,
we're just gonna assume that it's closely close,
and we're just gonna make these assumptions.
So we know, for example,
that the middle layer is connected to the upper layers,
and the upper layer is connected to the lower layer,
so we can come up with a very simple graph like that.
And we know that there's assumptions,
and maybe even violations in there,
but we can still use it as a model,
and every model has a shortcoming to look at the data.
But I would agree with you that's problematic.
So what's the other response that I have to that?
Well, these electrodes,
and those are very specific response, maybe,
to those of us that are doing neurophysiology,
those electrodes that we have,
they now allow you to actually look at connected neurons.
So you're getting 100 neurons at a time,
and you can look at the cross correlation of the activity,
and you'll find neurons that always basically fire together
with a little lack of, let's say, four milliseconds or so.
So that means that you can establish very simple systems,
in this case, something like this,
where you have interconnected neurons,
and you're not making any major violations.
This having said, yes, the whole theory,
if you take the theory at heart,
and if your end goal is to find out
if the system is conscious, how much is this conscious,
what the conscious states are,
you would have to do this
for maybe all of the neurons that the brain measured
at the same time.
In fact, the people that talk about the theory,
they would admit that it could be worse.
It could be that it's not the neurons
that matter about the synapses,
and we would have to measure all of the synapses
at the same time.
But it could also be that that isn't what is
the physical basis of what gives rise to phenomenology,
and areas is the way to go, or columns.
We don't know what the right level is.
And so what they are arguing is,
I think what is a part of the research program,
is to compute these different values
on these different spatial and temporal scales,
and see where it peaks.
So, but yes, there are, as with any technique,
there's a chasm here between the theoretical foundations,
and then how we can use it.
And so my appeal is, take it with a grain of salt,
but just give it a try,
violate the assumption and see what happens.
So here's maybe the good transition,
why I think that might be interesting.
So this right here is actual data from my lab,
and so in blue and in red,
you see these are basically neuronal activation.
While the animal, in this case,
is just fixating at the screen, nothing is happening,
then a stimulus comes on,
and then in blue, the animal pays attention,
and in red, the animal does not pay attention.
And so we're measuring activity across the layers,
in this case, of area before.
And so there's two things that I wanna point out.
So this right here are these representations
in graph theory that I just told you about.
These down here, just to the Pearson correlation coefficients
between the matrices that are underlying,
in this case, the red versus the blue state.
And so if the animal is just fixating
and nothing is going on,
and I'm computing the phi value,
which would be about the level of consciousness in this case,
you can see it's pretty low.
And then when the stimulus comes on,
and I'm computing the phi value,
making all of these assumptions,
you can see it's going up.
So despite all of these assumptions being violated,
the theory still seems to hold.
The really interesting part is right here.
If I compute phi versus the state
where the animal pays attention,
it's the highest versus if the animal doesn't pay attention,
it sinks even below the baseline value,
as you might expect, if the animal is now sucking up
all of the attention to one part of the field,
rather than widely distributing it.
So this is just one example that the summer break
allowed me to do by getting into my own data
and doing MATLAB.
They got me excited because I just put the theory
to its test.
I held its feet to the fire
and it did what it was supposed to do.
Now, of course, there's many more of these examples
in the literature,
so it seems that most of the tests to the fire
have so far held true.
But the original idea as I told you guys was this,
that we're not just getting at the level of consciousness,
but at the contents of consciousness.
So in order to do that, we have to take another step.
And to this step is a transition
from integrated information theory as you know it,
as I've just introduced it,
which got published as integrated information theory 3.0.
It's actually the third revision of the theory.
Now, integrated information theory 4.0
is about to come out next year, I've been told.
But most of the math has already been developed
and computer code is available for free on the internet,
so that you can already run your data
for integrated information theory 4.
What is interesting about integrated information theory 4?
Well, it comes up with a new way
to come at that cause-effect structure.
Sorry about that, I snuck it in at that time.
So what it does, it basically takes what I just said
in terms of looking at these causal interactions,
but rather than just looking at the states of the system,
it becomes even more multidimensional.
It now takes, you see combinations of different states,
and if the more measurements you have,
the more come out of it,
and basically applies the same kind of logic,
not just to the individual states,
but the combinations of states in between.
And so, IIT 4.0 calls these relations,
and as I said, the math and the code
has just been available for that,
but just give you a brief insight again,
just very little time that I had available to look into that.
For this example that I've been showing a couple of times now
of neural data recorded in the middle upper lower layers,
and I'm running this IIT 4.0 code to look at
out of all these possible combinations,
which one of those are irreducible?
Which one have actual causal power?
So one thing that's a little confusing is that
we now move to different terminologies,
so A, B, and C would be in this case
the different layers of cortex,
and you can see that only one of these layers
turns out to have causal power by itself.
The other layers of cortex, they only act synergistically,
but more surprisingly to me,
most of the possible interactions
that you can theoretically come up with
in this multidimensional space turn out to be reducible.
They actually turn out to be non-causal,
and I think that is very exciting
for most of us that are interested in
how do I deal with these massive amounts of data
that I'm getting from fMRI, EEG, neurophysiology
that we can reduce the dimensionality of these data
in a meaningful way.
So rather than what a lot of techniques do these days,
take an external perspective and saying,
I'm decoding the system, I'm looking into the system,
what allows me to predict behavior?
What allows me to post-dict the stimulus conditions?
This is taking an intrinsic perspective,
and it's saying which one of the interactions in my data
are important to the system?
They actually matter.
So I would argue this might be a way for us
to get closer to understanding the system
as it functions itself.
So if you agree with me, or if you have only
slight disagreements, you can now see
why this qualia structure approach is something
that has gathered a lot of YouTube channels,
a lot of fanfare, a lot of special issues and conferences.
If not, you might feel, one more time.
If not, you might feel like Albert Einstein
that maybe we're going a little bit too far
with the math, and we should maybe stick
a little bit closer to the data.
In either case, I say it in French, merci.
Thank you.
Thank you.
Thank you.
Thank you.

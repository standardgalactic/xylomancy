Ultimately, we see these as kind of like Lego pieces that, you know,
due to their low power footprints, we'll be able to concatenate them together
using things like chip lead integration, advanced packaging,
and ultimately scale out these systems to be brain scale,
86 billion neurons, 500 trillion synapses,
and low power enough that they can exist in autonomous devices.
Can we build a computer chip that operates like the brain?
We've seen neuromorphic computer chips from companies like Intel before,
like the Luigi chip, which Intel claimed had a sense of smell.
Rain neuromorphics, however, says it has an end-to-end analog chip,
a neuromorphic processing unit that they say is the world's first end-to-end
analog trainable AI circuit.
It's a fully analog neural network, and it's a thousand times
more energy efficient than today's best processors.
Here to chat is Rain Neuromorphic CEO, Gordon Wilson.
Welcome, Gordon.
Thanks so much for having me, John.
It's good to be here.
It's good to have you.
When I see neuromorphics in the inbox, I got to check it out.
It's a cool space.
Let's start here.
What the heck is an analog chip?
Yeah, great question.
And I think a great kicking off point because it really allows us to
frame kind of what we're doing compared to what has been done for this past decade.
And I think it's easiest to understand what an analog chip is in contrast
to the neural networks, the AI that have defined this last decade of deep learning.
So in 2012, we kind of had a big event that started this new era, this AI
Renaissance, and we are seeing these massive neural networks grow in size
and grow in capabilities since then.
But all of those neural networks that we've seen in this deep learning
world are neural simulations.
They are abstractions that are written in software and the neurons and synopsis
that are defined in that software don't physically exist, but rather they
sit on this highest layer usually written in Python and that are then
translated through many layers of abstraction down until it gets to the
digital circuit, most of the time a GPU graphics processing unit where it
then processes that math that represents that neural network at the top.
An analog chip and a neuromorphic chip is different.
It's not a neural simulation, but rather it's a neural circuit.
It's a physical collection of neurons and synopsis as opposed to an
abstraction of neurons and synopsis.
And this is very similar to the brain.
The brain is a collection of physical neurons and synopsis.
It's governed by the laws of physics and it achieves all that it does with
extraordinary scale and extraordinary efficiency within the bounds of the
physical world.
So an analog chip, as we're building it, is trying to achieve something
simple, trying to find ways to learn, find ways to scale all within the physical domain.
So let's dig into that just a little deeper.
Great, great overview.
What does that mean?
Typically a chip will think in terms of or operate in terms of on or off one or
zero, right?
Binary logic, right?
What does an analog chip, how does that work?
What's that look like?
So digital chips are, as you said, built on the very bottom on zeros and
ones on this Boolean logic of on or off.
And all of the other logic is then constructed on top of that.
When you zoom down to the bottom of an analog chip, you don't have zeros or
ones, you have gradients of information, you have voltages and currents and
resistances, you have physical quantities that you're measuring that
represents the mathematical operations you're performing and you're exploiting
the relationship between those physical quantities to then perform these very
complex neural operations.
So an example of this, you know, is like matrix vector multiplication.
This is the kind of backbone of most neural network math.
And, you know, GPUs do this by, you know, parallelizing these multiplications
across a lot of digital cores and doing precise digital multiplication in addition.
In an analog chip, as we are building it, we're not doing this with highly
precise digital math, but instead we have the activations of the neurons
represented by voltages.
We have the weights of the synapses represented by resistances, which are
held in components called memristors.
And when that voltage passes across that resistance, you have a natural
relationship, you know, between voltage and resistance that's multiplicative to
receive a current, you read out a current and that's your output.
So an analog chip works by kind of first understanding these physical
relationships between electrical quantities and exploiting those to do the
math, to make the physics do the math for us.
That sounds super interesting and it sounds at once very, very complex and at
the same time kind of simple, right?
And is that one of the key reasons why your chip is so much more energy efficient?
I mean, you're claiming a thousand times more energy efficient.
Yeah, exactly.
That really is, I think, the most fundamental reason.
And I think when you consider the comparison again to the neural simulation,
the neural network in that simulation exists so many layers above and requires
to be translated and then performed on a circuit that is really not that well
optimized for neural math.
And, but in our case, the circuit is the neural network.
And because it is, it exists in that very bottom layer, it is on the
substrate of the chip itself.
You can achieve some extraordinary gains in both speed and improvement and
power reduction, which of course gives you that energy efficiency gain.
So, so yeah, it's because we're building the physical thing on that bottom layer.
This episode of Tech First is sponsored by Smart One.
Smart One is a smart vending machine platform that transforms traditional
vending hardware into smarter, better, faster, automated retail kiosks, a
convenience store without the store.
Learn more at smrt1.ca.
That I'm fascinated.
And I'm wondering with Boolean logic, old school computer chips, you
kind of have to simulate reality and go through those multiple layers of
translation, because you're not on bare metal, you know, as the, the old
timers and computing would tell us, right?
Go through various layers of translation before you're actually hitting
computer, machine instruction, machine language.
And you're kind of modeling reality or what you're computing in reality.
Is, is that an accurate way of thinking about it?
Yes, that's an exact way, a really great way to put it.
You know, I sometimes use the metaphor, like what would be easier, you know,
to, to assess your ability to kick a soccer ball on a field, right?
Would you rather, you know, reconstruct all of the physics of your body and of
the soccer field and of the ball and simulate that kick and you observe
that in the simulated world, or would you just rather walk out of the pitch
and kick the ball?
You know, it's for us, it feels very obvious that, you know, the most natural
way to build a neural network to make it as efficient as the brain is to
build it physically, just as the brain does.
Are you trying to build an artificial brain?
Yes, we are.
That is our goal.
You know, we have kind of two missions that are very complimentary.
One of them is to build a brain.
And the other one is to actually understand it.
You know, we really love the, and believe the notion that you can't fully
understand a system until you have, have reconstructed it and built it.
You know, this go back to Feynman and many others.
But what we believe we've developed really are kind of the core technologies
that allow us to first build just kind of unit level chips that address, you know,
near term problems.
But ultimately we see these as kind of like Lego pieces that, you know, due to
their low power footprints, we'll be able to concatenate them together using
things like chiplet integration, advanced packaging, and ultimately scale
out these systems to be brain scale, 86 billion neurons, 500 trillion synapses
and low power enough that they can exist in autonomous devices.
Because today, training is so expensive that first week we consider
training inferences, the separate problems.
I don't think they really should be considered as the separate and distinct
problems, but it's so expensive that we can't even conceive of putting training
of natural language in an autonomous machine.
And yes, humans do it all the time.
So that is what we are trying to achieve to, to recapitulate the brain in
the hardware and ultimately give machines all of the capabilities that
we recognize in ourselves.
So Gordon, you're a pretty soft spoken guy and you sound like a very thoughtful guy.
And in that very soft spoken way, you're saying absolutely ginormous things.
We're talking Frankenstein level stuff, right?
I mean, you understand the gravity of what you're talking about, right?
Absolutely.
No, this is the scope and impact of our work is not lost on me.
I mean, this is something we've been working on for nearly five years and we
recognize that if we are successful in achieving this, this, this will be
historic and, and massively consequential.
But, you know, what motivates us, you know, is of course the, the impact on
positively improving human life, you know, and we can have personalized
medicine, personalized education, we can automate all of labor.
I mean, this is the world that we want to realize.
Yeah, I think many people are already in this consensus that artificial
intelligence is going to be kind of the defining technology of the century.
But most people don't, don't know that the hardware that supports it is, is
the bottleneck right now.
So, so yes, we recognize the scope of this and, you know, it's, it's a big mission
and a big task, but, you know, I think that we're, we're approaching it the
right way and we're also very conscientious of the fact that we don't
want that there, there are good ways of implementing AI.
There are also not good ways.
And we don't think AI should be used everywhere for every purpose, but there
are guidelines and ethics that should really direct how we build and implement
these systems.
And it's controversial.
It's also political in the AI space, right, in terms of what you're building,
what you're looking at, people who claim to be working on general AI, for
instance, you know, there's a lot of scrutiny there.
There was a, I believe a Google engineer who last week said, I think that
some of our systems are, are approaching consciousness and, and there
were a ton of people jumping all over him, probably correctly, but, you
know, perhaps in a bit of a mob mentality for daring to suggest that and, and
saying, no, we're, we're so far out there.
Maybe let's come back here because you and your co-founder have studied the
human brain significantly.
What do we learn about AI from the way that our brains work?
Absolutely.
That's a great question, John.
And I'd say that there are really kind of two categories of clues that we
look at, you know, from the brain that then inform our hardware.
The first is how does the brain learn so efficiently?
You know, it, the brain trains and learns with both very few examples.
Uh, uh, we, we learn with, with one example or two examples, one shot
learning, two shot learning, and we can generalize extraordinarily well.
So learning training happens very, very efficiently.
And there are the learning rule of the brain, the algorithm the brain uses is
not fully understood or identified, but we do know that there are certain
requirements that, that algorithm must have.
Uh, so one of those is called a local learning.
So this, if you're, for the listeners who are familiar with back propagation,
which is the industry standard algorithm for digital AI, this says
what's called a global work.
So first let's say, well, what is a learning rule?
A learning rule is how any given synapse in a neural network knows should
it become stronger or weaker for the whole system to become smarter, better
at its assigned task.
And in back propagation of the digital world, for you to calculate whether
this one synapse should be stronger or weaker, you need to do a math equation
that incorporates the entire network.
You need to differentiate against the entire system.
That's really mathematically expensive, but not only is it expensive, it's
also impossible for the brain to be doing the same thing.
There's no like agent sitting in our brain observing the whole system and
then doing a math problem to update every synapse.
It's impossible.
The brain has to be operating with a local learning rule.
And so a local learning rule is so that that synapse can just observe
what's right nearby and still know I become shorter.
So that's one of the clues in the algorithm side.
And that's one of the pieces that makes our learning algorithm so special
that it is as smart as back propagation, but it does so with a local learning rule.
Well, you burst my bubble there.
And there's no homunculus, I believe you pronounce it.
I mean, there's no little guy in the little control room in my brain.
Unfortunate.
Yeah.
And there's another side of the clue.
So I'd say the first is on like how it learns the learning algorithm.
The other is about scale.
How does the brain achieve its massive scale?
86 million neurons, 500 trillion synapses and still process and move
information so quickly, so efficiently.
So and in that case, we take clues from like the topology of the brain.
And in fact, and the specific thing is called sparsity.
The idea that you don't have to connect every neuron to every other neuron
in the system for it to be well connected.
Now, again, to contrast to the status quo, digital ALI in deep learning is,
was primarily built on what are called fully connected layers in neural networks
where you have a layer of neurons and another layer of neurons.
And you connect every neuron here to every neuron in the next layer.
Well, this was that was the natural way to do it on GPUs because GPUs do
these dense matrix multiplications, which corresponds to this fully connected,
densely connected systems.
Now, our brain is very different.
Our brain of all the possible connections that could exist between neurons.
It's something like just a fraction or one percent of those connections exist.
And yet at the same time, the path for information to travel
from one neuron to any other neuron is very short.
The average path length is about four jumps for information
to traverse anywhere in the brain.
It's extremely well connected.
So there are these special patterns of connectivity.
One of them is called a small world network.
And if you've ever heard of a small world network, it's a network pattern
that also mirrors human social networks that gives rise to the six
degrees of separation property in human connections.
And the idea is you can have lots of local connections.
You want to be connected to your neighbors.
You're likely to be connected to your neighbors.
And that's a very short path to bridge.
And then you want some long distance connections.
And you can create these very well connected networks at very large scale
when you implement these intelligent forms of sparsity.
So sparsity is core to our scaling architecture.
And really, and just to kind of summarize there.
So we have these are the two core technologies we've developed,
a learning algorithm that learns at the local learning rule
and a scaling architecture that scales to massive sizes of neural
networks using intelligent sparsity.
I knew Kevin Bacon would come up some point.
I mean, inevitable.
Last year, I believe you sort of taped a functional chip together.
Your first prototype.
Where are you on the journey to shipping a fully functional chip?
So we have, I'd say in the last four years, you know, we have done
such expansive work that has been mostly, I would say, qualified as a research.
We've been exploring different algorithms.
We've been exploring different architectures.
Originally, we had this concept using random nanowire meshes.
Turns out it's not very manufacturable and better to build things
that you can manufacture today easily.
But in this last year, we kind of crystallized kind of our two core technologies.
Like what is the learning algorithm and what is that scaling architecture?
And then developed hardware prototypes of each.
We still have a good amount of time to engineer this completely and to get to market.
We hope to get to market, you know, on the order of full scale shipping 2025.
But that said, you know, building the world's first analog neural network
is not easy and it takes time to iterate through this and get it fully fully up at scale.
And you just got some help there.
You raised some funds.
Yeah, yeah.
So we just closed a 25 million series A, which is thrilling.
You know, we had used eight million dollars for the last four years
and honestly had gone through some challenging times where, you know,
we got really close, you know, one of our first takeout prototypes
didn't function precisely as designed.
It was a very silly error.
But in ships, it takes a long time to iterate and even resolve.
You can't just debug like software.
So we went through a lot.
We learned a lot and now we have 25 million in the bank and we're hiring like crazy.
Wonderful. It goes quickly, I know, from personal experience.
So talk about what this will enable.
You're shipping this ship.
What will it enable?
Yeah.
So we want to not enable just like an incremental improvement in AI.
You know, I think that there are a lot of folks and video included
on the digital hardware roadmap.
And because digital hardware is so mature,
it's just it's kind of incremental gains that we're getting at this point.
And I think there's still more improvement to be eked out on that roadmap.
But we're trying to enable a new roadmap
that is really a step change in performance improvement.
And so we want to enter the market really at a thousand X energy efficiency
improvement over status quo hardware and at a thousand X
comes from about a 10 X reduction in power alongside a 100 X improvement in speed.
And when you can do that and importantly, not just for inference,
we're talking about training and inference in the same platform.
It unlocks possibilities that have just been inconceivable until until now.
For one, you know, currently people consider training and inference
as these kind of separate problems that we need separate platforms of hardware with.
You know, we train up in a GPU cloud system,
and then we might upload those weights onto a more efficient chip
and deploy it out into the world.
I think that would be the first step for kind of low power inference in devices.
But we don't want devices just to be pre-programmed
and just do what they do in the world.
We want devices to learn on their own.
We want devices to have an adaptive brain
that's continuously learning from a changing environment and from a changing self.
So imagine a robot, right?
We have we're we're eventually in our lifetime.
We're going to have robots for everything, you know,
but maybe it's a construction robot that's helping, you know, repair our streets
or build our homes.
The joints on that robot are going to erode and face damage in unique ways.
And it needs to learn how to adjust its own movement
so it can maintain its performance based on its own kind of evolution
and transition in its physical self.
Also, the robots might be adjusting to new environments.
And we'd like that ability to be baked in so they can continuously and adaptively learn.
This also really enables personalization.
It enables machines to get to know us and for us to have the assurance
that that knowledge and data of our self is in that robot
and doesn't believe that robot, I think is something we will will be very assuring.
But but that's a huge piece, you know, training and inference in the same platform
that is untethered.
What's really interesting about that?
Oh, there's a number of things, obviously.
But I mean, I'm just thinking of a limping robot, for instance,
you know, we limp when we injure a joint.
And that is our adaptation to the limitation in a knee or a hip
or something like that. And we function with that.
And, you know, maybe we'll repair the robot, but maybe the robot is inaccessible
or maybe the robots on Mars or Pluto or who knows where or maybe it's too expensive.
So limping and getting, you know, that's interesting.
The other thing that you mentioned that's super interesting is we want AI
to make our lives better and we want robots to make our lives better.
But that doesn't mean that we want Amazon to know our deepest thoughts.
That doesn't mean that we want Google to know everything about our personal finances.
You know, if we can have AI that is personal, that, you know,
sure, it comes from somewhere and we've purchased it.
Or, you know, if you start to get into it, if you start to look at general AI,
you start thinking, do you purchase that?
Do you recruit that? Do you adopt that?
Lots of questions there.
But it's nice if you can get a system that learns you, understands you
and it stays in some sort of privacy corridor there.
Really, really interesting stuff.
How it's so hard to predict the future.
You mentioned about speed.
That's always a moving target, right? You want a hundred X speed,
but you see what speed is right now in terms of adding speed right now.
We seem to not be making chips much faster per se.
We're adding more chipsets on a die
or we're creating chips that are more optimized for this task
or for that task or for energy efficiency or for whatever.
And that's how we're making overall devices faster.
So are you are you keeping that sort of moving target in mind
as you look at the performance levels that you want to hit?
So we believe we can achieve that one hundred X improvement speed
and beyond because we're taking a different tack entirely
because we're not on that roadmap of digital hardware
that is very mature chips and they're eking out performance
from any number of, you know, kind of well trod playbooks.
In the case of a fully analog neuromorphic chip,
you know, you have a neural network where you have analog neurons
and synapses that are connected in sequence.
And, you know, you can compare this, say, to other analog mixed signal chips.
So people have started to get a lot of speed improvements
by moving away from digital and doing, say, the synaptic operations
either with photonic components or flash components.
But in all of those cases, they still actually have to translate
between their kind of physical, their either physics or optical
to digital for their neurons.
And so they have these analog synapses, digital neurons,
they kind of go back and forth.
And that requires clocking that requires slowing down the system.
So in a fully analog chip, when it when it's designed,
well, you can have input to output and have the signal flow
at wire speed from end to end.
That is the full potential of an analog chip.
And that's why the speed here is so extraordinary
because we're no longer working in this digitally clocked world.
But we're again, exploiting the physics of the system,
the physical nature of the system to do that math for us.
And you can perform inference as just a wave of electricity from input out.
Amazing.
I know that Tesla, for instance, is doing.
We can debate whether the term full is is an appropriate modifier there,
but full self driving on atom chips, right?
Not saying that all the training and learning is happening there,
but that's what's actively involved in the vehicle.
So just imagining what this could do for self driving,
for automated machinery, for robotics, for AI, it's kind of mind blowing.
Gordon, thank you for spending this time with us.
I really do appreciate it.
Thank you so much for the time, John.
It was a pleasure.

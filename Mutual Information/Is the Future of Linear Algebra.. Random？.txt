These are the graphics from Unreal Engine 5, they're quite good.
The engine is simulating physics so well, sometimes it can be hard to tell the difference
from reality.
And to accomplish this, something that is absolutely essential is numerical linear algebra,
or NLA, which is a study of linear algebra applied with computers, and is better labeled
applied linear algebra because that's really all it is.
Now without it, there are no good computer graphics.
Mapping 3D objects to the 2D view screen, that's gone.
Rotating those 3D objects, gone.
Rasterization, where triangle vertices are used to determine pixel color intensities,
also gone.
But seriously, without NLA, a lack of video games would be the least of our problems.
We also wouldn't have weather forecasting, many types of data compression, finite element
methods for stress testing, compressed sensing for MRI, fluid dynamics simulations, recommendation
systems, search, and of course, deep neural networks.
An argument from one of the leaders of the field, the late Gene Gulub, was to point out
that of the top 10 most important algorithms of the 20th century, six of them related to
numerical linear algebra.
And that was almost 20 years ago.
Now I decided to make this video when I stumbled upon this paper, randomized numerical linear
algebra.
In it, I found some remarkable claims.
They show how some well-placed randomness can speed up some NLA algorithms by an order
of magnitude.
In other words, we're looking at a potential breakthrough in scientific computing.
And randomization for speed seems like a really bizarre idea.
Why would an algorithm that wants to compute something exactly be super sped up by doing
something akin to coin flipping?
Now to fully appreciate this, we'll need some background.
But first.
This episode is brought to you by True Theta.
The data science consultancy.
More about us at the end of the episode.
The first thing we need to ask is, what is linear algebra?
Simply put, it's the mathematics of vectors and matrices.
We'll start with a vector.
A vector is just a list of numbers and can be imagined geometrically as an arrow.
Now, this is just a wimpy 2D vector.
The powerful stuff comes from using N dimensional vectors, which can represent complicated things
Everything from images to words to audio to a person's credit profile.
Basically, most things you can think of can be represented as a vector.
Now we consider a matrix.
On the surface, a matrix is just a grid of numbers.
Or you can consider it a list of vectors.
As a list of vectors, you can imagine it as a bunch of arrows.
When a matrix represents data, they're commonly represented as points.
But in linear algebra, matrices represent something else.
They relate to functions.
In our case, a function is something that receives an input vector x and gives back
a vector y, possibly of a different dimension.
And it needs to do that for a range of x values.
If we were to increase the dimensions, the function would be connecting exponentially
huge regions.
In this sense, functions are very information-rich objects, since they represent an enormous
set of connections.
But handling all that information explicitly is often impossible.
So we make a fantastically liberating assumption.
We assume the function is linear, which means it's like a line or a plane or something
similarly flat in higher dimensions.
More rigorously, linearity comes from some algebraic properties, but we don't need
to get into all that.
What's important is why linearity is useful.
Consider two cases, where the inputs and outputs are both one-dimensional.
In both cases, suppose we'd like to guess the function after being shown two input-output
pairs.
Now, on the left, we don't make any assumptions about the function.
It can be any function.
On the right, we assume the function is linear.
With this, can we determine the functions?
Well, when we assume linearity, yes, it's easy.
There's only one linear function that goes through these two points.
But without the assumption, there are infinite functions that pass through those two points,
and we have no way to pick one.
This is totally essential.
Linearity allows us to determine these information-rich objects' functions with very little.
Basically, to know a linear function in one region is to know it in all regions.
This fact is so useful, much of applied mathematics involves making the linearity assumption wherever
we can get away with it.
Doing so allows us to extrapolate in space, or time, or something else.
And crucially, it makes the exceptionally powerful theorems of linear algebra available
to the problem at hand.
If we assume linearity, we essentially get an instruction manual for computing useful
things.
So how does a linear function relate to a matrix?
Simply, if f of x is a linear function, then f of x equals a times x for some matrix A.
Saying a times x is to do matrix multiplication, which is a simple but tedious calculation
of sums and products.
Just don't think about the details.
Okay, now we can ask, what is numerical linear algebra?
Well, that's the study of applying linear algebra fast and efficiently with computers.
This brings two fundamental challenges.
The first is the computer's finite precision.
The second is that fast algorithms are machine dependent.
Let's start with the computer's finite precision.
If a point is selected at random, it almost surely can't be represented exactly in the
machine.
The machine will have to truncate it to an approximation, creating a small error.
This fact, as harmless as it seems, hardly complicates everything.
One effect is, to a computer, addition isn't always associative.
Sometimes, adding numbers A and B before adding to C doesn't give the same answer as summing
B and C first.
And when an algorithm fails to anticipate issues like this, it's prone to incorrect
answers.
We say such algorithms are numerically unstable.
Now we consider the second challenge, that fast algorithms are machine dependent.
Let's say we want to multiply two matrices A and B.
By how matrix multiplication is defined, the resulting matrix is the sum product between
every row of A with every column of B.
That's how it's defined, but how is it computed?
Well, it helps to imagine it like this, where we are looking to fill up the cells of this
matrix C. Each cell corresponds to a combination of a row of A with a column of B.
From here, there are several ways we could order the computation.
One way is, we could finish the computation of each cell of C before moving on to the
next cell.
That's probably how you do it after you heard my definition.
But there's another way.
We could proceed like this, finishing the use of each column of A and row of B, which
is swapped from how I defined matrix multiplication, but gives an equivalent answer.
And there are other ways of ordering operations.
The problem is, ordering makes a difference for the algorithm's speed.
They bring different amounts of data movement and differing data accessing patterns.
And this speed depends, at the least, on A and B's dimensions, how they are physically
stored in memory, and how much can be stored in a processor's registries, which are very
small but provide extremely fast access.
And this is just one flavor of the algorithm's machine dependence.
Everything from the memory layout, to the processor, to the operating system, to the
capacity for parallelism determines the algorithm's speed.
These things must be anticipated, in all their diversity across users and time, to design
the best NLA algorithm.
As you can imagine, this gets very complicated very quickly.
To appreciate what's offered by randomized NLA, we'll also need some history.
John von Neumann is often named when discussing the origin of the field.
Because in 1947, he and his co-author Herman Goldsten published one of the earliest uses
of computers for applying linear algebra.
But this isn't a relevant origin for modern NLA software, since the programming paradigms
of the 40s and 50s were just so different, very little of it persists today.
The code was terribly verbose, machine dependent, hard to share, and just an absolute headache
to write.
So in 1957, IBM created the Fortran language, designed to ease programming for scientific
computing and provide some machine independence.
And in the 1960s, James Hardy Wilkinson and his colleagues collected and published papers
on how to apply linear algebra with computers, but not with the Fortran language.
Most of their papers discussed how a specific algorithm could be applied to a category of
matrix.
This work earned Wilkinson a Turing Award, and along with Fortran, it created the environment
from which modern NLA software was born.
What happened in 1979, when BLAS came out from the Jet Propulsion Laboratory in California,
the basic linear algebra sub-program for Fortran?
It provided a small set of vector operations that were fast and tested across a variety
of machines.
Most importantly, they could implement NLA algorithms like those proposed by Wilkinson.
And so in 1979, LINPAC came out, which was built on top of BLAS, and had been developed
over the previous decade, primarily for supercomputers.
It wasn't the first NLA package, but it was a major step forward in speed, reliability,
and distribution across the scientific community.
But at the same time, computer architectures were evolving.
And so BLAS 2 was released in 1984 to perform matrix vector operations, which took advantage
of the vector processor CPUs of the time.
But architectures changed again, to ones with shared memory-cached base parallel processing.
And so BLAS 3 was released in 1990, which performed fast matrix-matrix operations.
And this is a perpetual story.
Architectures changed, so BLAS 1, 2, and 3 need to be updated.
Architectures changed again, and so we need more software updates.
But now, to fully leverage BLAS 1, 2, and 3, a new NLA package was needed.
In 1992, LAPAC was released, after having been proposed five years earlier.
The authors included some modern NLA heroes, like Jack Dungara and James Demo.
Both have been involved ever since, architecting, writing, standardizing, testing, optimizing,
and communicating the software to produce what it is today.
And that's quite a feat.
Today in the 2020s, an absolutely enormous amount of linear algebra gets applied with
LAPAC or BLAS.
If you're doing any linear algebra with MATLAB, Python, R, C++, or any other language
you can name, it's very likely you're using this software or a very close derivative
of it.
And if you aren't, you're probably doing something wrong.
Okay, and now I need to confess this short history is a major oversimplification.
LAPAC exists in an ecosystem dedicated to scientific computing.
And so my cute linear story fails to represent the entangled, messy truth of the matter.
To mitigate this, I'll give a very quick tour of the current software landscape.
LAPAC doesn't work with distributed memory parallel processing, so we have ScalaPAC relying
on PBLAS, which performs many of the same operations as BLAS but executed across a large
network of heterogeneous machines.
LAPAC isn't designed for sparse matrices, so there have been efforts to capture the
gains of sparsity, but none have been received quite like LAPAC.
LAPAC also isn't designed for GPUs and modern multi-core architectures, so we have MAGMA,
which enables use of NVIDIA and AMD's fancy GPU hardware.
Speaking of NVIDIA, we also have KUBLAS.
See, BLAS comes with a bunch of knobs that need to be optimized for particular hardware,
and so KUBLAS has those knobs set for NVIDIA GPUs, among other things.
Also Apple has Accelerate to do that same tuning for their hardware.
And since tuning knobs for hardware isn't easy, we also have Atlas, which automates
some of this tuning.
More recently, there's GPT Tune, which uses Bayesian optimization and Gaussian processes.
If you don't know what those are, I wonder where you could learn about them.
Oh wait, Truth8.io!
That must be a great place for data science help.
Moving on, people also want to perform large batches of small and similar NLA operations,
so we have batch BLAS.
I'd like to say that covers it, but it doesn't.
There's a lot of software out there doing NLA, and this list right here is just the
free open source stuff.
And now we can finally discuss the paper on randomized numerical linear algebra.
Again, I'm making this video because of some remarkable claims it made, and it took
them especially seriously because it's authored by some of the original developers of BLAS
and LA-PAC, like Jack and James.
In fact, I spoke with the first author Riley Murray to make sure I understood exactly what's
going on here.
Now when reading this, I knew LA-PAC and BLAS were virtually impossible to dethrone.
Also from a distance, my impression was the field of NLA had matured, and there probably
weren't huge gains to be had.
And I'm not alone in that impression.
To understand this, we need to talk about how we describe an algorithm's efficiency.
Say we're given a matrix A, and it's an N by N matrix.
Let's say we like to multiply it by another matrix B of the same size.
Now, if we were to do this the standard way, that would be what is called an order N cubed
algorithm.
That means, as the side length N grows, the number of operations grows like N cubed,
roughly.
For example, say multiplying 10 by 10 matrices takes some fraction C of one second.
If we multiply 100 by 100 matrices, then we'd have to wait an amount of time close to that
same fraction C, but now of 1,000 seconds.
So we increase the side length by a factor of 10, and the time increased by a factor
of 1,000.
Fast hardware can bring down C, whatever it is, but it won't change its painful growth.
However, in 1969, something remarkable did.
Volcker Strassen surprised everyone with an algorithm that does multiplication in a way
that grows like N to the 2.8-ish.
This was extremely surprising, since matrix multiplication, the standard way, involves
three for loops.
So the exponent of 3 seems totally unavoidable, and yet, a lower exponent was possible, and
so the result kicked off research to get that 2.8 exponent down.
It has since leveled out, and it's leveling out where I formed my impression.
In general, for all important matrix algorithms, pushing these exponents significantly further
down seems effectively impossible.
So as I was reading this, I believed gains in speed would not come from fundamentally
new algorithms, but just better scaled up hardware.
That's expensive, but it seems like it's the only option.
But that understanding changed when I read these two paragraphs.
Here's what it's saying.
A problem you see absolutely everywhere is the problem of least squares.
I'll explain what it is.
We're given a matrix A, which has M rows and N columns.
And we'll assume that N is much less than M, which is actually pretty typical in practice.
We're also given a vector B, which has dimension M.
So A and B are matrix and a vector that are given to us for this problem.
Now the goal is going to be to find a vector X.
First, we form AX, which, as mentioned, is a linear function defined by multiplication
with A. Then we consider its distance from B.
That's what this notation means.
Now our goal is to minimize this distance.
So in one sentence, our goal is to find X such that AX is as close as possible to the vector B.
OK, now the best NLA algorithm to solve this involves order MN squared operations.
So we have an exponent of 2, but at least it's on N and not the much larger M.
Now what Rand NLA says is, if you're willing to accept a small and controllable error in your answer,
which we'll call epsilon, then randomized algorithms can actually solve this in order
MN log 1 over epsilon plus N cubed operations.
It may not sound like much, but this is huge.
In heavy-duty applications, both M and N can be big.
N might be in the thousands, and M could be in the millions or billions.
Now if we want a strong approximation, like one within a tenth or a hundredth of a percent,
then this term is going to be in the single digits.
And since M is the big problematic number, then this term likely won't matter much.
So let's look at the ratio of the dominant terms.
This will give a sense of how many times faster the randomized algorithm is than the classic one.
Things cancel, and now we're looking at a speed-up factor of N over log 1 over epsilon.
If N is in the thousands, and this is in the single digits,
we're looking at a speed-up factor of around 1,000x.
That's absurd.
Now due to some omitted details, we don't actually get 1,000x speed-ups in practice.
However, we do get 20x, and that's still huge.
If we did the pure parallelization and hardware approach,
recreating that gain might take 20x the energy or 20x the cost.
Okay, but such bold claims raise some questions.
First, what are these algorithms doing?
Well, I'll get more into that later.
But to describe it briefly, with high probability,
a random summary of the data massively shrinks the problem
while preserving virtually all of the relevant information.
And that raises another question.
A long-standing goal of classic NLA is to compute the most exact answer possible
as fast as possible.
That is, get the answer as precisely as the machine will allow,
and then optimize for speed.
But in randomized NLA, the goal is to compute a close enough answer
as fast as possible with high probability.
And this allows for much faster algorithms.
The question is, why are we allowing this new standard?
Well, one motivation is the recent trend in machine learning.
Machine learning accepts that the data is noisy.
The data is just an approximation of the truth.
And so computing things exactly is unnecessary.
The exact answer would change with the change in the meaningless noise,
so an approximate answer is, life be just as good.
And it might even be an exact answer with a different sample of data.
And so this looks a lot more reasonable.
Also, when you look at some algorithms,
we find that close enough can sometimes be made extremely close.
And high probability can sometimes be made so high,
it's not even worth mentioning.
In general, there's a trade-off between speed and accuracy.
And how favorable that trade-off is depends on the algorithm.
OK, next question.
This argument is just a heuristic illustration.
It's not pointing to an implemented algorithm with measurable performance.
So what is the actual performance?
Well, there are many papers that demonstrate significant concrete improvements.
One striking demonstration I saw came from this paper,
which is actually co-authored by Stephen Brunton and Nathan Kutz,
two researchers who are active educators on YouTube.
You may have seen them.
One of their goals is to improve the SPD algorithm
for low-ranked matrices in the programming language R.
SVD involves taking a matrix A and decomposing it
into a product of three matrices.
And without getting into the details,
these matrices have some nice properties
that help us do things like dimensionality reduction
or solving least squares problems.
Now, their randomized SVD algorithm gets this performance.
Each plot is for a different size of A, specified as rows by columns.
The y-axis tells us how many times faster an algorithm is
than the plain SVD algorithm.
So the plain SVD algorithm itself, its speed is always 1.
That means the randomized SVD algorithm
is between 40 and over 100 times faster,
depending on the size of A. And the errors from randomization
are comparable to those of the non-randomized routines.
So we're looking at massive real gains,
but we need to make some comments.
First, they're only considering low-rank matrices.
Randomized algorithms are especially useful for those.
Second, their algorithm is entirely
implemented in R, a high-level language.
In contrast, one of the other benchmarking algorithms,
R spectra, provides speed by granting R-axis to spectra,
a C++ library optimized for eigenvalue problems.
So maybe randomization is best coded at the lower level,
like that of C++.
So let's check out an algorithm that does that.
This one, Cholesky QR with randomization
and pivoting for tall matrices.
This one does an impressive job of wrangling
the interplay of hardware, software, and randomization
to produce a dominant algorithm.
In fact, it strikes such a favorable balance of speed
and accuracy, the authors claim that the algorithm design
question is effectively solved for this class of matrix
and problem.
That's quite a claim.
So what's the problem they're solving?
Well, once again, we're given a matrix A, which in this case
is assumed to be very tall.
The goal is to decompose it into matrices
with certain properties.
Again, explaining the decomposition
would take us really far afield.
But here's one important detail.
By including this matrix P called the permutation matrix,
we're asking the algorithm to order its operations
in a special way to improve numerical stability
and, for lack of a better explanation,
provide more information on the decomposed matrix.
Including P is significantly more work,
but virtually guarantees we'll get the right answer.
That is, it's much more numerically stable.
Doing this decomposition is called QR decomposition
with column pivoting.
The QR matrices enable several useful things,
like solving least squares problems.
Now to explain their algorithm's performance,
here are two blank plots.
On the left, we'll see performance on matrices
with about 32,000 rows.
On the right, about 130,000 rows.
Along the horizontal axis, the number of comms
varies from about 500 to about 8,000.
So we're looking at A matrices of different shapes,
but they're all either tall or really tall matrices.
Now the vertical axis is billions of floating point
operations per second, which is a bit of a weird thing
to measure.
But the G-flops are those of a benchmark algorithm run on A.
And so it's fixed across algorithms.
In other words, just interpret the vertical axis
as relative speed, like in the previous paper.
OK, now, this is LA-PAC's algorithm for QR
with column pivoting.
Remember, LA-PAC is the very well-optimized industry
standard, but maybe not for long,
because this is their algorithm, pronounced secret.
As you can see, we're looking at 10 to 20x speed-ups.
And if you're wondering what's happening here,
the authors point out that the matrices no longer
fit in the cache, so moving more data around
becomes necessary.
As an aside, moving data or data communication
is a major source of algorithmic slowness.
Now in the paper, the plot shows alternative fast algorithms
for a comparison.
They're acknowledging other approaches
people might be familiar with to further benchmark
their algorithm.
However, in my view, the authors are being a bit modest,
because these other fast algorithms
aren't doing the full job.
It's not apples to apples.
This one and this one don't perform column pivoting,
so the decomposition gives us less information
about the matrix.
If we ignore that fact and try to use the decomposition just
like it had done pivoting, we get inaccurate or even
flat-out wrong answers.
This one and this one are numerically stable,
but the former only applies to full-rank matrices,
so not all tall skinny matrices.
And the latter only delivers an implicit representation
of the matrices we want.
This means in many practical cases,
we need to do extra work, slowing the algorithm down
considerably.
All things considered, this plot understates
how much better secret really is.
And if you're thinking, OK, but this can't actually
replace the LA-PAC algorithm, because randomized algorithms
come with some error, right?
Well, in this case, the error can be made so small,
it's essentially just as good as the LA-PAC routine.
And this highlights the bizarre magic of randomization.
You add a well-placed pinch of it,
and you're able to get essentially the same answer,
but many times faster.
It's a trick almost without baggage.
It's not an optimization that only
works for a fixed set of machines or a niche class of matrix.
It works essentially across the board.
It makes you wonder, what are these algorithms doing?
Well, random LA algorithms come in several flavors.
I'll go with one that's especially simple to present.
Let's bring back the least squares problem.
Again, we're looking for the vector x,
such that ax's distance from b is as small as possible.
We'll call the x vector that achieves this minimum x star.
Again, we'll assume that a is a tall matrix.
One randomized approach is called sketch and solve.
We start by sampling a random matrix s.
How we do that doesn't matter right now.
What's important is that we'll be solving the least squares
problem, but we'll replace a with s a and b with s b.
And s will be designed such that s a has many fewer rows
than the tall matrix a.
And in the same way, s b is much smaller than b.
Essentially, multiplying by s produces a compressed problem
that's much faster to solve.
Doing this is to form a small sketch of the problem.
Here's the remarkable thing.
If we solve this new least squares problem,
giving us a vector we'll call x tilde,
then the distance it achieves in the original problem
is about the same as the best achievable distance
in that original problem with high probability.
In other words, it's very likely
that solving this much smaller problem
will give an answer that's nearly just as good as what
we'd get solving the original big problem.
And that's great news.
A much smaller problem is much faster to solve.
And that's where we get these order of magnitude speed-ups.
And what makes this practically useful
is that we can control how good the approximation is
and the probability that that level of approximation
is achieved.
We can say, I want a 99.99% chance
that the distance is within 1% of the best achievable distance.
Declaring that will tell us how many rows s needs,
depending on how s is randomly sampled.
In fact, developing theorems to make statements like these,
that's where a lot of the hard work of the field is.
Further, how s is sampled is a question all its own.
Every value could just be a sample
from a normal distribution, but there are fancier techniques,
like ones that allow you to perform the essay multiplication
extremely quickly.
Because in the presentation that I just gave you,
doing that multiplication would actually be a dominant cost.
OK, but this explanation doesn't answer the
why does it work question, really.
I'm just showing that it relies on an approximation,
which I'm asking you to accept.
So let's go a little further.
Let's say we're given the following least squares problem.
A is equal to some tall matrix with two columns,
and B is equal to some long vector.
Now, if you know something about least squares,
you know finding x is going to involve,
among other things, the covariance matrix of A.
That's a component of the solution I'd like to focus on.
So let's ignore B for now.
Now, if you don't know what the covariance matrix of A means,
that's fine.
With just two columns, it's a very easy thing to visualize.
What we'll do is plot each row of A as a point.
Now, the covariance matrix just tells you this elliptical shape,
and it's this shape that partially determines
the minimizing x.
Next, let's consider SA,
where S is properly scaled random Gaussians with D rows.
If D is equal to 8, that means SA can be plotted
as 8 summary data points.
Again, we compute the covariance matrix,
and again, this shape partially determines the minimizing x,
this time in the shrunken, sketched version of the problem.
Now, here's what ranNLA exploits.
As D increases, the covariances of SA and A
become more and more similar.
So between A and SA, the covariance structures,
things that determine the minimizing x's, are very similar.
In other words, as far as this covariance piece
of the answer goes, A and SA give us nearly the same thing.
And D doesn't need to be that large to get a good approximation.
So we can solve a much smaller SA least squares problem
and get virtually the same result.
OK, but what about B?
Yeah, the minimizing x is also determined
by the covariance between A and B.
So let's consider an augmented matrix, which
is A concatenated with B as a new column.
The covariance of this new matrix now
includes everything that determines the minimizing x.
Looking at this, this is just another matrix.
So its covariance matrix will be similar to that
of its sketched version.
In other words, everything that drives the minimizing x
is similar across the original and sketched problem.
That's why the sketched version gives us
approximately the same answer.
Now, I need to confess something.
This approximation is actually pretty weak
and isn't really what's powering the randomized algorithms.
But it captures the essence and can be animated,
so I went with it.
However, Riley pointed out that what's actually happening
involves much stronger approximations
regarding relative differences.
Now, since defining relative things for matrices
involves some head bending, I backed away.
So this gives a sense of the mathematical properties at play.
But it's not the full story.
For a bigger picture, Riley gave me the following analogy.
The strategy of LA-PAC is to cast
NLA algorithms into the use of efficient BLOZ functions.
Most notably, the highly optimized
general matrix multiply or gem function.
The more an algorithm can be written
as repeated use of this operation,
the faster it'll get.
That's what LA-PAC did.
But there's a limit to how much algorithms
can be recast into gem.
But randomization provides a new basket of functions
that can be applied in a similar way to gem.
If least squares is suddenly super efficient,
we'll try to reframe everything we can
as solving repeated least squares problems.
It's a huge space for creativity and big gains.
And that's why in the RAND-NLA paper,
they're talking about a new software,
RANDBLAS and RAND-LA-PAC, which would serve as a new pillar
for NLA, the randomized approach.
If they pull it off, and it's really saying something,
we'd be in for a widespread upgrade
in scientific computing.
All those technologies stand to be improved
from gaming to weather forecasting
to artificial intelligence.
To be comprehensive, I should mention
at least two other approaches to speed.
The first is communication avoiding algorithms,
algorithms which anticipate the hardware
to minimize the amount of data movement.
Since moving data takes so much time,
these provide big speed ups as well.
Second, there are hardware accelerators,
specialized hardware designed to do
very specific operations extremely fast.
Now, both of these are totally effective paths to speed
and in fact, can be combined with randomization
to produce even larger gains.
That said, these approaches bring some inflexibility.
Communication avoiding algorithms need to be designed
especially carefully to the hardware
and accelerators only do fixed,
highly specialized operations.
You can't change what an accelerator does after it's built.
In comparison, randomized algorithms are exceptional
because they are entirely an idea of mathematics.
Randomness isn't upgraded hardware
or an algorithm designed for specific hardware,
yet it gives you speed and scalability as though it were.
It does this by allowing simple algorithms,
ones that otherwise struggle with scalability,
to be applied to huge data.
And it's this quality of simple but powerful
that I believe is necessary for producing
a significant and widespread upgrade
in scientific computing.
Since this is evolving, I'm gonna keep track of updates
as I hear about them and to the best of my ability
on a post on truthata.io.
There I'll also answer some other questions
like why isn't this Monte Carlo
or who cares about least squares?
In general, I inevitably learn more about a topic
after I publish a video on it.
So this post can evolve as I hear from you, others
or just learn more about it myself.
And wow, what an incredible segue
into talking about true theta.
Truthata is my data science consultancy.
We have experienced building machine learning systems
for pricing, credit risk modeling,
causal inference and forecasting.
If you're at a company looking for this type of work,
we should talk.
You can send an email to inquiries at truthata.io
to get in touch.
All right, that's it.
If you'd like to learn more about randomized NLA,
I have my sources in the description.
Also, I'd like to make a special thank you to Riley Murray
for our discussions on this topic.
And I'd like to thank everyone else
who provided useful commentary.
And finally, thank you for watching
and until next time.

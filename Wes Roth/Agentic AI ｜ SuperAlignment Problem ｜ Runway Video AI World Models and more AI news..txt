So this was a huge week for AI news.
Let's catch up on some of the big things that were happening.
There's a lot of rumors about the next big model
coming from OpenAI.
Somebody dropped this screenshot of GPT 4.5,
which was very suspicious,
but don't worry, Sam Altman's on the case.
He's gonna clarify the situation for us.
Somebody asked some GPT 4.5 leak legit or no.
Sam Altman answers, nah.
Which again, people are a little bit confused.
Like what does that mean?
The image is fake or the news about the new model is fake.
Either way, we'll know soon enough.
Google DeepMind drops this paper,
mathematical discoveries from program search
with large language models.
And here they introduce fund search,
which actually stands for searching in the function space.
There's a paper in nature that kind of goes over it.
The big headline news here is,
this is the first time anyone has shown
that an LLM based system can go beyond
what was known by mathematicians and computer scientists.
It's not just novel,
it's more effective than anything else that exists today.
So this is yet another example of AI
coming out of DeepMind
that is rapidly expanding the human scientific knowledge.
In this case, it's mathematics.
In the case of AlphaFold,
it's expanding our understanding
of the 3D structures of proteins.
The GNOME project is discovering new materials.
And a lot of this is done autonomously.
So it's AI and robots kind of doing their own thing
to create this.
Humans are more and more out of the loop.
And just yesterday, OpenAI drops this,
practices for governing agentic AI systems.
In other words, AI systems that have agency,
they have their own goals
and also have the ability to pursue those goals.
Agentic AI systems, AI systems that can pursue
complex goal with limited direct supervision
are likely to be broadly useful
if we can integrate them responsibly into our society.
And here they have this paper that talks about things
that we need to think about as a society, as a humanity,
about how to safely release these things into the wild.
They quickly define what they mean
when they say AI agents.
And I think they actually invent a brand new word,
agenticness.
We define the degree of agenticness in a system
as the degree to which a system can adaptably achieve
complex goals in complex environments
with limited direct supervision.
Agenticness as defined here thus breaks down
into several components.
Goal complexity, environmental complexity.
So for example, to what extent are they cross domain,
multi-stakeholder, require operating over a long time
horizons and involve the use of multiple external tools,
then adaptability, how well can the system adapt
and react to novel or unexpected circumstances
and independent execution.
To what extent can the system reliably achieve its goals
with limited human intervention or supervision?
So reading through this paper, to me it seems like
it's mostly a way to kind of brainstorm
and get all the potential dangers out on paper,
as well as some questions that we have to answer before
we allow these agents to autonomously go
on act on our behalf, whether they're able to do it
when is approval required.
They talk about autonomous weapons, for example,
default behaviors.
So for example, they talk about how, you know,
if you're getting it to do your grocery shopping for you
or to complete some chores for you,
the default should be, for example, to spend no money, right?
So only spend money when you have to don't just default
to buying the most expensive thing right away.
And here, interestingly, they talk about the shifting
of offense defense balance.
So some tasks may be more susceptible to automation
by agents than others.
So for example, in the cybersecurity domain,
human monitoring and incident response
is still key to cyber attack mitigation.
So the feasibility of such human monitoring
is predicated on the fact that the volume of attacks
is similarly constrained by the number of human attackers.
All right, so if you have 10 guys trying to hack
into somewhere and you have 10 cybersecurity experts
kind of defending and monitoring for those attacks,
you know, if one side increases their offense abilities,
the defense similarly can increase their defense abilities,
you know, in general, obviously.
But what happens if the offense can now use a million
smart AI agents that are able to carry out these tasks
autonomously without human supervision?
Is there a limit to how many they can deploy at one time?
So in that world, that would kind of destroy cybersecurity,
right, but on the other hand, conversely,
if agent AI systems make monitoring a response cheaper
than producing new cyber attacks,
the overall effect would be to make cyber defense
cheaper and easier, which is an interesting point, right?
Because it's kind of been this race, you know,
if the offense improves, the defense improves,
but what if we find that in the future with these AI agents,
either the offense or the defense is significantly easier
and just more powerful?
That either means that the ability to hack into systems,
skyrockets, or is more or less completely negated,
I don't think we're quite sure yet
which future we're gonna live in.
So they're saying, well, it's very difficult to anticipate
the net effect of these sort of AI adoption dynamics.
And it's saying it behooves actors to pay close attention
in identifying which equilibrium assumptions no longer hold.
I mean, this is where kind of the conversation
about AI drones really kind of comes into play.
Specifically, I mean, attack drones
that are disconnected from any human operator.
What are the defense against those drones?
Does attacking them being on the offense,
does that become the much more effective tactic
than waiting and trying to defend?
If that is indeed the case,
then the world kind of becomes a more of a scary place.
And right around the same time that opening I released
this paper, they released another one called
weak to strong generalization.
And so it's part of their super alignment.
How do we make super intelligence safe?
And so in this paper, they're talking about an approach
that seems to be having some results.
And so traditionally, you can think of this
as a human supervisor.
So this dotted line,
that's sort of the human level intelligence, right?
So the supervisor, the smart human is teaching the AI
that is not as smart as the human.
And then super alignment, how that would look like
is this idea that this human who is not as smart
as the super intelligent AI, you know,
trying to teach it and tell it what to do,
this is what a lot of people have concerns with
is like, how do you control something
that is far smarter than yourself?
And their approach is we start right now
before they get past the human level of intelligence,
are we able to train a small model
to supervise a larger model?
And so at the end, at the very end of this paper,
this is like page 47 out of 49,
they have this high level plan.
So Lakey and Sutzker proposed the following
high level plan, which they've adopted.
So one, once we have a model that is capable enough
that it can automate machine learning,
like AI almost training other forms of AI,
more advanced forms of itself.
And so one sense able to automate that
and in particular, alignment research,
our goal will be to align that model well enough
that it can safely and productively
automate alignment research.
So we're trying to create an AI
that will be able to research how to create safe AI
as it gets sort of stronger and better
and more beyond our understanding.
And so we will align this model
using our most scalable techniques available,
our LHF, reinforcement learning, human feedback,
constitutional AI, scalable oversight,
adversarial training, and this new approach,
the focus of this paper,
weak to strong generalization techniques.
We will validate that the resulting models align
using our best evaluation tools available,
for example, red teaming.
So that's when a group of people try to do their best
to try to break that model,
to try to get it to do something bad.
And interpretability, which is our attempt
to kind of understand what it's thinking,
what it's thought process are,
can we monitor its quote unquote thoughts somehow.
And then they're saying number four,
using a large amount of compute,
we will have the resulting model conduct research
to align vastly smarter super human systems.
We will bootstrap from here
to align arbitrarily more capable systems.
So, and we've talked about this before,
so that there's this idea
that we can take these smaller models
and by using just vast quantities of compute,
we can almost get a glimpse
into what that model would look like
if it was much stronger.
And of course that costs a lot of money,
we need to have a lot of equipment to be able to do that,
but it seems to be like almost a way
to get those high level answers
without necessarily creating a model
that is that high level.
We're able to match the capabilities
of a much bigger model with a smaller model
by overclocking it, giving it more compute,
expanding more resources on that model.
So it's interesting because we've heard
some of the stuff being discussed before,
not officially, but it seems like certain leaks,
certain hints at what's happening.
These ideas have been out there before.
Runway ML.
So you've probably seen some of the videos
this thing is able to generate.
So it's one of the more well-known AI video
generation platform.
And what they're announcing
is that they're starting a long-term research effort
around what they call general world models.
And we've talked about this idea before
that AI, these neural nets,
that they build these mental models of the world.
So when we feed them information
and then we ask them for to make certain inferences,
certain guesses about what's gonna happen.
So for example, if we feed them tons of text,
then we ask them to predict what comes next in a sentence,
or we feed them a bunch of images
and then we have them either recognize certain images
or create certain images.
What we find is they build these sort of mental models
about how to do that.
So for example, if you give them a lot of 2D images,
some of the latest research is showing
that they build almost like a mental model of the 3D world.
So they kind of start understanding the 3D space
in those 2D images.
We didn't give them any data about like the depth of field
or how far away certain things are,
but they do start to gain some quote unquote understanding
about how to position objects in the 3D space,
what's further away, what's closer, et cetera.
And so the fact that runway ML is doing that
is very interesting.
So they're saying to build general world models,
there are several open research challenges
that we're working on.
For one, these models will need to generate
consistent maps of the environment
and the ability to navigate into Ragnos environments.
They need to capture not just the dynamics of the world,
but the dynamics of its inhabitants,
which involves also building realistic models
of human behavior.
So to me, it almost sounds like what they're talking about
is having these AIs build almost a simulation of the world.
And then within that sort of simulation of the world,
almost like taking a camera and recording something.
And then that becomes the video that then is extracted
and becomes this AI video.
So currently they have their Gen2 system,
which I've showcased some of the stuff
that you can do with it.
They're saying in order for Gen2
to generate realistic short videos,
it has developed some understanding of physics and motion.
However, it's still very limited in its capabilities,
struggling with complex camera or object motions
amongst other things.
If it wants to generate an image of a bird flying,
it has to have some understanding of how that bird moves,
the physics that are interacting with it,
how the point where the camera is,
how it's picking up the movement of that bird, et cetera.
Very excited to hear where they're gonna be going with this.
And I'm glad more companies are investing resources into this.
And this piece of news was very interesting.
So OpenAI is partnering with some news outlets
to sort of pull real-time information from them.
So when you ask ChadGPT for real-time news,
it's able to pull from those news sources
and then present those to you in real time.
And then of course, they'll include attribution
and links to full articles for transparency, et cetera.
This might be how we get our news in the future.
We're not gonna go to Google or the newspaper or TV
or even a specific website.
We're just gonna ask our favorite chatbot,
hey, what's the news today?
Mid-Journey Alpha.
Mid-Journey launches a brand new thing, Mid-Journey Alpha.
We're able to create stuff faster, easier,
and breaks it down by subjects, descriptors.
You can have it look like known artists, et cetera.
So this is kind of the next step in AI art generation
that gives you a lot more control
and more precision in how to create your images.
If you've generated, I believe it's over 10,000 images
with Mid-Journey, this should be available to you soon.
Google DeepMind drops image in two,
their most advanced text to image technology,
and some of these images are extremely realistic.
I think I would be hard pressed to find any faults with it.
I mean, there's certain things that I could point to,
maybe this area is a little bit weird,
kind of the area here,
but I could not call this an AI generated image.
There's nothing here to really give it away.
So that's it for today, but I think there's a lot more to come.
I think this is kind of the calm before the storm.
I think before December is out,
we're gonna see some pretty incredible things,
just a hunch.
My name is Wes Roff, and thank you for watching.

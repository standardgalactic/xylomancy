Thank you to Wren for supporting PBS.
In around 4.5 billion years, the Andromeda Galaxy and their own Milky Way will finish
their long mutual plummet. On a first grazing pass, the delicate spiral arms will be yanked
almost out of their sockets. Perhaps their denizens will look back with relief as the
dislocated galaxies retreat on their future night skies. But then they'll pause for a
100,000 year eye blink before falling back together. In a series of whirling collisions,
all spiral structure will be obliterated, gas will be compacted to produce waves of supernovae,
and the giant milkdromeda galaxy will have been born.
The event I just described will happen, as surely as the sun will rise tomorrow.
We know this because we've calculated the chaotic gravitational and hydrodynamic interactions
of countless stars and gas and dark matter particles over billions of future years.
And simulations of galaxy collisions are just the beginning. We routinely simulate the universe
on all of its scales, from planets to large fractions of the cosmos. Today, we're going to
see how it's possible to build a universe in a computer, and to see whether there's a limit
to what we can simulate.
In 1941, the Swedish astronomer Erik Holmberg conducted what was probably the first simulation
of the universe. Right when the first programmable computers were being assembled,
but Holmberg didn't use a computer, he arrayed 37 light bulbs on a plane, each one representing
billions of stars in a spiral galaxy disk. The light from each bulb was its gravity,
felt more strongly close to the bulb, and dropping away with a square of distance,
just like gravity. At any point on the disk, a galvanic cell could determine in which direction
the intensity of light was strongest. So the simulation went like this. Holmberg started
with a pair of these light bulb galaxies next to each other, with the motion of each bulb existing
only as a note on a piece of paper. He would then measure the light at each bulb,
which told him the sum gravitational pull on that group of stars. He'd then adjust the
velocities of all according to Newton's laws. Finally, he'd allow time to tick forward. He'd
move the bulbs according to their new velocities. Then from that new configuration, he'd start
the process again. And so the two galaxies collided. Just as with our modern supercomputer
simulations, he witnessed the destruction of the disks, the throwing off of tidal streams,
and the formation of an elliptical galaxy. Holmberg's experiment was ingenious. But
why go to all this effort? Since the time of Isaac Newton, it had been possible to write down
equations describing the trajectories of a pair of massive bodies moving in each other's
gravitational fields. And then to solve those equations, to find their positions at any time
in the future. But note that I said pairs of bodies. This only works for two objects.
For three or more bodies, there is no simple set of equations describing their
future evolution under gravity. This is the so-called three-body problem, which we've
discussed in the past. The three-body problem doesn't have any analytical solution,
no simple master equations, which is why Holmberg had to solve the problem with light bulbs.
Holmberg did what we call a numerical calculation, in which an impossibly complex
computation is broken down into a series of much simpler steps. And this particular type
of numerical calculation is called an n-body simulation. In an n-body simulation, Newton's
laws of motion and gravity are applied over a series of time steps. Each step is short enough
that we can assume that the global gravitational field is constant. It only changes in the next
step after all the particles have made their moves. The predictions of these n-body simulations
can be as accurate as you like, as long as you make the time steps small enough. And I would hope
so, because this is essentially how we calculate the trajectories that place people on the moon
or land robots on comets. For simulating a solar system, this method is pretty reasonable. In fact,
the room-sized computer used to calculate the Apollo trajectories had about the computing
power of your smartphone. But if we want realistic simulations of, say, a galaxy with its billions
of stars, we need to do a bit better. Fortunately computing power has improved somewhat since
Holmberg or the Apollo missions. But actually it's improved nowhere near enough to do n-body
simulations of entire galaxies using the method that I described. Let's think about what this
computation really needs. In the simplest type of n-body simulation, you need to compute the
effect of every particle on every other particle. So if there are n-particles, that's n-squared
calculations. For a modern 1 million particle simulation of, say, a star cluster, that's
a trillion computations per time step. The real challenge in astrophysics is that we often have
to deal with huge ranges in scale. For example, in a cosmological simulation, we're trying to
watch the detailed formation of individual galaxies, as well as the evolution of giant
clusters of galaxies. But if we keep enough precision to get the fine structure, it's
computationally impossible to also produce a large enough volume to get the larger scales.
To do these simulations, astrophysicists have to come up with some ingenious tricks. Perhaps the
most important is to avoid having to consider every single particle pair. Remember, the strength
of gravity drops off with distance squared. For nearby particles, it's important to consider
every individual interaction. But for more distant locations, it's okay to clump particles together
and consider only their summed gravitational effect. One of the original approaches to doing
this is a so-called tree code. It works like this. You start with a volume full of particles,
each with its starting position and velocity. Now, divide up the volume into eight sub-cubes,
then divide each of these into eight more, and so on. You stop dividing at any given part of the
volume when there is no more than one particle per cube in that part. Next, you run an n-body
simulation by calculating the summed gravitational pull on each given particle. But now you have a
shortcut. When you calculate the effect from distant locations, you don't do it for each
particle. Instead, you do it for all particles inside one of these cubes. The larger the distance,
the larger the cube you can use. Now, this sounds like a complicated process,
but now the number of calculations that you need to do goes down from n squared to n times log n,
which for large particle numbers is much, much faster. Another approach is the particle mesh
method, in which particles are converted into a density distribution and a gravitational
potential across the grid. The force at each point can then be solved using Fourier transform
methods, which can be very fast. Adaptive particle meshes can be used to add higher resolution
when needed, say where the stars have higher density or structure. Modern mesh codes also
do classic particle-particle interactions at short ranges to improve accuracy at small scales.
These mesh codes are useful for systems of particles interacting under gravity,
but they also work for another really important astrophysical situation, flowing gas.
The universe started as an ocean of gas a few hundred thousand years after the Big Bang.
That gas is still everywhere. It flows into galaxies from beyond where it rides the disc,
fragments and collapses into stars. It forms whirlpools and jets around new stars and black holes.
All of these processes are key parts of galaxy evolution and of star and planet formation,
and so we'd better be able to simulate this too. Now we call these hydrodynamic simulations.
They simulate the flow of gas using the equations of fluid dynamics.
Particle mesh approaches can do this, but these days it's more common to use an approach called
smoothed particle hydrodynamics, or SPH. SPH codes don't use a rigid grid, but rather they
track tracer particles within the fluid. Those particles effectively make up a constantly shifting
grid. In astrophysics, SPH codes are used to simulate the flows of gas in galaxies and around
quasars, use to simulate star and planet formation, and even star and planet destruction in collisions
or supernovae. SPH can even be used to do galaxy formation where the stars themselves are treated
as a type of fluid. In practice, modern simulations often use an amalgam of these methods,
for example SPH for large-scale flows and particle-particle n-body for small-scale interactions.
Astrophysicists often inject all sorts of other physics into their simulations.
In your galaxy simulation, you might need a separate prescription to describe how stars age
and die. In your quasar disk, you need to simulate separately how light travels through
the hot plasma, and don't get me started about the complexity of including magnetic fields,
or of Einstein's general relativity when the gravitational field becomes very strong.
With the techniques now available, using parallelised code on modern computing clusters,
we can compute some pretty insane simulations. We can see how stars form in multitudes from
collapsing gas clouds, and how planets then coalesce in the disks surrounding those stars.
We can watch as galaxies form with gas and dark matter interacting to produce waves of star
formation and supernovae, settling into spiral structures just like we see in the real universe.
And then we have cosmological simulations, which create entire virtual universes,
from the moment the first atoms formed to the modern day. One of the most famous is the millennium
simulation from the Max Planck Institute in Germany. It simulated a 13 billion light-year
wide cube containing over 300 billion particles, each representing a billion sun's worth of dark
matter. But the current largest cosmological simulation is Abacus Summit, which just last
year simulated 70 trillion particles on the supercomputers at the Center for Computational
Astrophysics in New York. So how far can all of this go? Can we ever simulate a real universe
in which creatures evolve that can themselves simulate universes? In fact, could we be such
creatures? Probably not. None of these simulations contain the full information of an actual universe,
or even a tiny part of it. As we discussed recently, a full quantum description of the world
contains unthinkably more information than is contained in a typical simulation, which just
tracks particle positions and velocities. No conceivable technology could fully simulate a
quantum universe, except perhaps a cosmologically sized quantum computer, which is kind of what
the universe is anyway. We've talked about this simulation and hypothesis stuff before,
and maybe we'll come back to it. For now, let's just be proud of the science that we can get from
modern simulations, because we've come an awful long way since Eric Holmberg pushed some lightbulbs
around on his table. And the fidelity and size of our simulations will only get better as computing
power continues its exponential growth and as we invent better and better algorithms. Perhaps
there is no limit to what we can learn about the outside universe by rebuilding it inside our
computers and then peering into that simulated space time. Thank you to Ren for supporting
PBS. Ren is a website where you calculate your carbon footprint, then offset it by funding
projects that plant trees protect rainforests and suck carbon from the sky. By answering a few
questions about your lifestyle, you can find out your carbon footprint and how you can reduce it.
No one can reduce their carbon footprint to zero, so you can offset what you have left after
reducing. Subscribers receive monthly updates from the tree planting, rainforest protection,
and carbon removal projects they support. You can get to see the trees you planted and what your
money is spent on. You can learn more about Ren's climate projects at ren.co.
you

Why is AI going to destroy us? ChatGPT seems pretty nice. I use it every day.
What's the big fear here? Make the case.
We don't understand the things that we build.
The AIs are grown more than built, you might say.
They end up as giant, inscrutable matrices of floating point numbers that nobody can decode.
At this rate, we end up with something that is smarter than us, smarter than humanity,
that we don't understand, whose preferences we could not shape.
And by default, if that happens, if you have something around that is much
smarter than you and does not care about you one way or the other,
you probably end up dead at the end of that.
Extinction is a pretty extreme outcome that I don't think is particularly likely,
but the possibility that these machines will cause mayhem because we don't know how to
enforce that they do what we want them to do. I think that's a real thing to worry about.
Welcome to another episode of Conversations with Coleman.
Today's episode is a roundtable discussion about AI safety with Eliezer Yudkowski, Gary Marcus,
and Scott Aronson. Eliezer Yudkowski is a prominent AI researcher and writer known for
co-founding the Machine Intelligence Research Institute, where he spearheaded research on AI
safety. He's also widely recognized for his influential writings on the topic of rationality.
Scott Aronson is a theoretical computer scientist and author,
celebrated for his pioneering work in the field of quantum computation.
He's also the chair of COMSI at U of T Austin, but is currently taking a leave of absence to work
at open AI. Gary Marcus is a cognitive scientist, author, and entrepreneur known for his work at
the intersection of psychology, linguistics, and AI. He's also authored several books, including
Cluj and Rebooting AI, Building AI We Can Trust. This episode is all about AI safety. We talk about
the alignment problem. We talk about the possibility of human extinction due to AI. We talk about what
intelligence actually is. We talk about the notion of a singularity or an AI takeoff event,
and much more. It was really great to get these three guys in the same virtual room,
and I think you'll find that this conversation brings something a bit fresh to a topic that has
admittedly been beaten to death on certain corners of the internet. So without further ado,
Eliezer Yudkowski, Gary Marcus, and Scott Aronson.
Okay, Eliezer Yudkowski, Scott Aronson, Gary Marcus, thanks so much for coming on my show.
Thank you. So the topic of today's conversation is AI safety, and this is something that's been
in the news lately. We've seen experts and CEOs signing letters recommending
public policy surrounding regulation. We continue to have the debate between people that
really fear AI is going to end the world and potentially kill all of humanity and the people
who feel that those fears are overblown. And so this is going to be a round table
conversation about that, and you three are really three of the best people in the world to talk about
it with. So thank you all for doing this. Let's just start out with you, Eliezer, because you've
been one of the most really influential voices getting people to take seriously the possibility
that AI will kill us all. You know, why is AI going to destroy us? ChatGPT seems pretty nice.
I use it every day. What's the big fear here? Make the case.
Well, ChatGPT seems quite unlikely to kill everyone in its present state. AI capabilities
keep on advancing and advancing. The question is not, can a ChatGPT kill us? The answer is probably
no. So as long as that's true, as long as it hasn't killed us yet, the engineers are just
going to keep pushing the capabilities. There's no obvious blocking point. We don't understand
the things that we build. The AIs are grown more than built, you might say. They end up as giant
inscrutable matrices of floating point numbers that nobody can decode. It's probably going to end
up technically difficult to make them want particular things and not others. People are just
charging straight ahead. At this rate, we end up with something that is smarter than us,
smarter than humanity, that we don't understand, whose preferences we could not shape. By default,
if that happens, if you have something around that is much smarter than you and does not care
about you one way or the other, you probably end up dead at the end of that. The way it gets the
most of whatever strange inscrutable things that it wants are worlds in which there are not humans
taking up space, using up resources, building other AIs to compete with it, or just a world in
which you built enough power plants that the surface of the earth got hot enough that humans
didn't survive. Gary, what do you have to say about that? There are parts that I agree with
and parts that I don't. I agree that we are likely to wind up with AIs that are smarter than us. I
don't think we're particularly close now, but in 10 years or 50 years or 100 years at some point,
it could be 1,000 years, but it will happen. I think there's a lot of anthropomorphization
there about machines wanting things. Of course, they have objective functions, and we can talk
about that. I think it's a presumption to say that the default is that they're going to want
something that leads to our demise and that they're going to be effective at that and be able to
literally kill us all. I think if you look at the history of AI at least so far, they don't
really have wants beyond what we program them to do. There is an alignment problem. I think that
that's real in the sense of people will program a system to do X, and they do X prime. That's
kind of like X, but not exactly. I think there's really things to worry about. I think there's a
real research program here that is under-researched. The way I would put it is we want to understand
how to make machines that have values. Asimov's laws are way too simple, but they're kind of
starting point for conversation. We want to program machines that don't harm humans,
that can calculate the consequences of their actions. Right now, we have technology like
GPT-4 that has no idea what the consequences of its actions are. It doesn't really anticipate things,
and there's a separate thing that Eliezer didn't emphasize, which is it's not just how smart the
machines are, but how much power we give them, how much we empower them to do things like access the
internet or manipulate people or write source code, access files and stuff like that. Right now,
auto-GPT can do all of those things, and that's actually pretty disconcerting to me. To me,
that doesn't all add up to any kind of extinction risk anytime soon, but catastrophic risk where
things go pretty wrong because we wanted these systems to do X, and we didn't really specify it
well. They don't really understand our intentions. I think there are risks like that. I don't see it
as a default that we wind up with extinction. I think it's pretty hard to actually terminate the
entire human species. You're going to have people in Antarctica that are going to be out of harm's
way or whatever, or you're going to have some people who respond differently to any pathogen,
etc. Extinction is a pretty extreme outcome that I don't think is particularly likely,
but the possibility that these machines will cause mayhem because we don't know how to enforce that
they do what we want them to do. I think that's a real thing to worry about, and it's certainly
worth doing research on. Scott, how do you view this? I'm sure that you can get the three of
us arguing about something, but I think you're going to get agreement from all three of us that
AI safety is important and that catastrophic outcomes, whether or not that means literal
human extinction, are possible. I think it's become apparent over the last few years that
this century is going to be largely defined by our interaction with AI, that AI is going to be
transformative for human civilization. I'm confident of that much, and if you ask me almost
anything beyond that about how is it going to transform civilization? Will it be good? Will
it be bad? What will the AI want? I am pretty agnostic just because if you would ask me 20 years
ago to try to forecast where we are now, I would have gotten a lot wrong. My only defense is I
think that all of us here and almost everyone in the world would have gotten a lot wrong about
where we are now. If I try to envision where we are in 2043, does the AI want to replace humanity
with something better? Does it want to keep us around as pets? Does it want to just continue
helping us out like just a super souped up version of chat GPT? I think all of those scenarios
merit consideration, but I think that what has happened in the last few years that's really
exciting is that AI safety has become an empirical subject. There are these very powerful AIs that
are now being deployed and we can actually learn something. We can work on mitigating the nearer
term harms, not because the existential risk doesn't exist or is absurd or a science fiction
or anything like that, but just because the nearer term harms are the ones that we can see
right in front of us and where we can actually get feedback from the external world about how
we're doing. We can learn something and hopefully some of the knowledge that we gain will be useful
in addressing the longer term risks that I think Eliezer is very rightly worried about.
So there seems to me there's alignment and then there's alignment, right? So there's alignment
in the sense that we haven't even fully aligned smartphone technology with our interests. There
are some ways in which smartphones and social media have led to probably deleterious mental
health outcomes, especially for teenage girls, for example. So there are those kinds of mundane
senses of alignment where it's like, is this technology doing more good than harm in the
normal everyday public policy sense? And then there's the capital A alignment of are we creating a
creature that is going to view us like ants and have no problem extinguishing us and whether
intentional or not. So it seems to me all of you agree that the first sense of alignment is
at the very least something to worry about now and something to deal with.
But I'm curious to what extent you think the really capital A sense of alignment
is a real problem because it can sound very much like science fiction to people. So maybe let's
start with Eliezer. I mean, from my perspective, I would say that if we had a solid guarantee that AI
was going to do no more harm than social media, we ought to plow ahead and get all the gains.
It's not enough harm to back like this amount of harm that social media has done to humanity.
Well, very significant in my view, I think it's done like a lot of damage to our sanity.
But that's just like not a large enough harm to justify either foregoing the gains that you could
get from AI if that was going to be the worst downside or to justify the kind of drastic
measures you'd need to stop plowing ahead on AI. I think that the capital A alignment is beyond
this generation. I've started the field. I've watched over it for two decades. I feel like in
some ways, the modern generation plowing in with their eyes on the short term stuff is like
losing track of the larger problems because they can't solve the larger problems and they can
solve the little problems. But we're just like plowing straight into the big problems and we're
going to go plow right into the big problems with a bunch of little solutions that aren't going to
scale. I think it's lethal. I think it's at the scale where you just back off and don't do this.
By back off and don't do this, what do you mean?
I mean to have an international treaty about where the chips capable of doing AI training go
and have them all going into licensed monitored data centers and not have the training runs for
AIs more powerful than GPT-4, possibly even lowering that threshold over time as algorithms
improve and it gets possible to train more powerful AIs using less computing.
So you're picturing a kind of international agreement to just stop?
International moratorium. And if North Korea steals the GPU shipment, then you've got to be ready to
destroy their data center that they build by conventional means. And if you don't have that
willingness in advance, then countries may refuse to sign up for the agreement being like,
why aren't we just like ceding the advantage to someone else then? It actually has to be a
worldwide shutdown because the scale of harm from a superintelligence. It's not that if you
have 10 times as many superintelligence as you've got 10 times as much harm. It's not that a super
intelligence only wrecks the country that built the superintelligence. Any superintelligence
everywhere is anyone's last problem. So Gary and Scott, if either of you want to jump in there,
is AI safety a matter of forestalling the end of the world and all of these smaller issues and
pass towards safety that Scott, you mentioned are just throwing, I don't know what the analogy is,
but pointless essentially. I mean, what do you guys make of this?
I mean, the journey of 1000 miles begins with a step, right? I mean, most of my,
you know, of the way I think about this comes from, you know, 25 years of, you know, doing
computer science research, including quantum computing and, you know, computational complexity,
things like that, where we have these gigantic, you know, aspirational problems that we don't
know how to solve. And yet, you know, a year after year, we do make progress. We pick off little
sub problems. And if we can't solve those, then we find sub problems of those. And, you know,
we keep repeating until we find something that we can solve. And, you know, this is, I think,
for centuries, the way that science has made progress. Now, you know, it is possible that,
you know, this time, you know, we just don't have enough time for that to work, right? And,
you know, I think that is what, what Eliezer is fearful of, right? That we just don't have
enough time for the ordinary scientific process to, to work before AI becomes too powerful.
You know, in which case, you know, you start talking about things like a global moratorium,
you know, enforced with the threat of war. You know, I am not ready to go there. Like,
I could imagine circumstances where, where maybe I say, you know, gosh, this is, this looks like
such an imminent threat that, you know, that, that we have to. But, you know, I'm very, I tend to be
very, very worried in general about causing a catastrophe in the course of trying to prevent
a catastrophe. And I think, you know, when, when, you know, you're talking about, you know, threatening
airstrikes against data centers or things like that, then that, that, that's an obvious worry.
So I'm somewhat in between here. I'm with Scott, that we are not at the point where we should be
bombing data centers. I don't think we're close to that. I'm much less, you know, what the right word
is to use here. I don't think we're anywhere near as close to AGI as I think Eliezer sometimes
sounds likely. I don't think GPT-5 is any, anything like AGI. And I'm not particularly concerned
about who gets it first and so forth. On the other hand, I think that we're in a sort of dress
rehearsal mode. You know, nobody expected GPT-4, really chat GPT to percolate as fast as it did.
And it's a reminder that there's a social side to all of this, you know, how software gets distributed
matters. And there's a corporate side. And it was a kind of galvanizing moment for me when
Microsoft didn't pull Sydney, even though Sydney did some awfully strange things.
I thought they would take it for a while. And it's a reminder that they can make whatever
decisions they want. And so you kind of multiply that by Eliezer's concerns about what do we do
and at what point what would be enough to cause problems. And it is a reminder, I think,
that we need, for example, to start roughing out these international treaties now, because there
could become a moment where there is a problem. I don't think the problem that Eliezer sees
is here now, but maybe it will be. And maybe when it does come, we will have so many people
pursuing commercial self-interest and so little infrastructure in place, we won't be able to
do anything. So I think it really is important to think now, if we reach such a point, what are we
going to do? What do we need to build in place before we get to that point?
So we've been talking about this concept of artificial general intelligence.
And I think it's worth asking whether that is a useful, coherent concept. So for example,
if I were to think by analogy to athleticism and think of the moment when we build a machine
that has, say, artificial general athleticism, meaning it's better than LeBron James at basketball,
but also better at curling than the world's best curling player, and also better at soccer,
and also better at archery, and so forth, it would seem to me that there's something a bit
strange as framing it as having reached a point on a single continuum. It seems to me you would
sort of have to build each capability, each sport individually, and then somehow figure how to package
them all into one robot without each skill set detracting from the other. Is that a disanalogy?
Do you all picture this intelligence as sort of one dimension, one knob, that is going to get
turned up along a single axis? Or do you think that way of talking about it is
misleading in the same way that I kind of just sketched out?
I would absolutely not accept that. I'd like to say that intelligence is not a one-dimensional
variable. There's many different aspects to intelligence, and there's not, I think, going
to be a magical moment when we reach the singularity or something like that. I would say that the core
of artificial general intelligence is the ability to flexibly deal with new problems that you
haven't seen before, and the current systems can do that a little bit, but not very well. My typical
example of this now is GPT-4 is exposed to the game of chess, sees the lots of games of chess,
that sees the rules of chess, but it never actually figures out the rules of chess and makes illegal
moves and so forth. So it's in no way a general intelligence that can just pick up new things.
Of course, we have things like AlphaGo that can play a certain set of games, AlphaZero really,
but we don't have anything that has the generality of human intelligence. But human
intelligence is just one example of general intelligence. You could argue that chimpanzees
or crows have another variety of general intelligence. I would say the current machines don't really
have it, but they will eventually. I mean, I think a priori, it could have been that you would have
math ability, you would have verbal ability, you'd have ability to understand humor, and they'd
all be just completely unrelated to each other. That is possible. And in fact, already with GPT,
you can say that in some ways it already is a super intelligence. It knows vastly more,
can converse on a vastly greater range of subjects than any human can. And in other ways,
it seems to fall short of what humans know or can do. But you also see this sort of generality,
just empirically. GPT was sort of trained on all the text on the internet,
or let's say most of the text on the open internet. So it was just one method. It was not
explicitly designed to write code, and yet it can write code. And at the same time as that
ability emerged, you also saw the ability to solve word problems, like high school level math.
You saw the ability to write poetry. This all came out of the same system without any of it
being explicitly optimized for. I feel like I need to interject one important thing, which is it
can do all these things, but none of them are all that reliably. Well, okay. Nevertheless, compared
to what my expectations would have been, if you'd asked me 10 or 20 years ago, I think that the
level of generality is pretty remarkable. And it does lend support to the idea that there is
some sort of general quality of understanding there, where you could say, for example, that GPT-4
has more of it than GPT-3, which in turn has more than GPT-2.
And I would say that it does seem to me like it's presently pretty unambiguous that
GPT-4 is in some sense dumber than an adult or even teenage human.
And that's not obvious to me. I think that we will eventually get
I mean, to take the example I just gave you a minute ago, it never learns to play chess,
even with a huge amount of data. So it will play a little bit of chess, it will memorize
the openings and be okay for the first 15 moves, but it gets far enough away from what it's trained
on and it falls apart. This is characteristic of these systems. It's not really characteristic in
the same way of adults or even teenage humans. Almost anything that it does, it does unreliably.
And give another example, you can ask a human to write a biography of someone and don't make
stuff up and you really can't ask GPT to do that.
Yeah, it's a bit difficult because you could always be cherry picking something that humans
aren't usually good at. But to me, it does seem like there's this broad range of problems that
don't seem especially to play to humans' strong points or machine weak points, where GPT-4 will
do no better than a seven-year-old on those problems.
I do feel like these examples are cherry picked because if I just take a different,
very typical example, I'm writing an op-ed for the New York Times, say, about any given subject
in the world. And my choice is to have a smart 14-year-old next to me with anything that's in
his mind already or GPT. There's no comparison, right? So which of these sort of examples is the
litmus test for who is more intelligent, right? If you did it on a topic where it couldn't rely
on memorized text, you might actually change your mind on that. So the thing about writing a
Times op-ed is most of the things that you propose to it, there's actually something
that it can pastiche together from its data set. That doesn't mean that it really understands
what's going on. It doesn't mean that that's general capability. Also, as the human, you're
doing all the hard parts, right? Like, obviously, a human is going to prefer, if a human has a
math problem, we're going to rather use a calculator than another human. And similarly,
with the New York Times op-ed, you're doing all the parts that are hard for GPT-4. And then you're
like asking GPT-4 to just do some of the parts that are hard for you. You're always going to
prefer an AI partner rather than a human partner within that sort of range of, like, the human
can do all the human stuff and you want an AI to do whatever the AI is good at at the moment.
An analogy that's maybe a little bit helpful here is driverless cars. It turns out that
on highways and ordinary traffic, they're probably better than people. In unusual
circumstances, they're really worse than people. So, you know, Tesla not too long ago ran into a
jet and a human wouldn't do that. Like, slow speed being summoned across a parking lot. A human
would never do that. So, you know, there are different strengths and weaknesses. The strengths
of a lot of the current kinds of technology is that they can either pastiche together or
make not literal analogies when we go into the details, but to stored examples. And they tend
to be poor when you get to outlier cases. Excuse me. And that's persistent across most of the
technologies that we use right now. And so, if you stick to stuff in which there's a lot of data,
you'll be happy with the results you get from these systems. You move far enough away, not so much.
And what we're going to see over time is that the length of the debate about whether or not it's
still dumber than you gets longer and longer and longer. And then, you know, if things are allowed
to just keep running and nobody dies, then, you know, at some point the switch is over to a very
long debate about is it smarter than you, which then gets shorter and shorter and shorter. And
eventually reaches a point where, you know, it's pretty unambiguous if you're paying attention.
Now, I suspect that this process gets interrupted by everybody dying. In particular, there's a
question of the point at which it becomes better than you, better than humanity at building the
next edition of the AI system and how fast do things snowball once you get to that point.
Possibly you do not have time for further public debates or even a two-hour Twitter space,
depending on how that goes. I mean, some of the limitations of GPT are completely understandable,
you know, just from a little knowledge of how it works, right? Like, it does not have an internal
memory, you know, per se, you know, other than what appears on the screen in front of you, right?
So this is why it's turned out to be so effective to explicitly tell it, like, let's think step by
step when it's solving a math problem, for example, right? You have to tell it to show all of its work
because it doesn't have, you know, an internal memory with which to do that. Likewise, you know,
when people complain about it, you know, hallucinating references that don't exist, well,
the truth is, you know, when someone asks me for a citation, you know, if I'm not allowed to use
Google, right, I might have a vague recollection of, you know, some of the authors and, you know,
I'll probably do a very similar thing to what GPT does, I'll hallucinate, right? So...
Well, no, there's a great phrase I learned the other day, which is frequently wrong, never in doubt.
That's true, that's true. I'm not going to make up a reference with full detail, page numbers,
titles, so forth. I might say, look, I don't remember, you know, 2012 or something like that.
Yeah. Whereas GPT-4, what it's going to say is 2017, Aronson and Yodkowski,
you know, New York Times pages 13 to 17. No, it does need to get much, much better at knowing
what it doesn't know. And yet, you know, already I've seen, you know, a noticeable improvement
there, you know, going from GPT-3 to GPT-4, right? For example, if you ask GPT-3, prove that there
are only finitely many prime numbers, you know, it will give you a proof, even though the statement
is false, you know, and it will have an error, which is similar to the errors on, you know,
a thousand exams that I've graded, right? Just, you know, trying to get something past you,
you know, hoping that you won't notice. Hey, if you ask GPT-4, prove that there are only finitely
many prime numbers, it says, no, that's a trick question. Actually, there are infinitely many
primes. And here's why. Yeah. Part of the problem with doing the science here is that I think you
would know better since you work part-time or whatever to open AI. But my sense is that a lot
of the examples that get posted on Twitter, particularly by the likes of me and other critics
or other skeptics, I should say, is the system gets trained on those. So, you know, almost
everything that people write about it, I think, is in the training set. So, it's hard to do the
science when the system is constantly being trained, especially in the RLHF side of things,
and we don't actually know what's in GPT-4, so we don't even know if they're regular expressions
and, you know, simple rules, match things. So, we can't do the kind of science we used to be able
to do. But this conversation, this subtree of the conversation, I think, has no natural end
point. So, if I can sort of zoom out a bit, I think there's a, you know, pretty solid sense
in which humans are more generally intelligent than chimpanzees. As you get closer and closer
to the human level, I would say that the direction here is still clear, that the comparison is still
clear. We are still smarter than GPT-4. This is not going to take control of the world from us.
But, you know, the conversations get longer. The definitions start to break down around the edges.
And, but I think it also, as you keep going, like, it comes back together again. There's a point,
and possibly this point is, like, very close to the point of time to where everybody dies,
so maybe we don't ever, like, see it in a podcast, but there's a point where it's, you know,
unambiguously smarter than you. And including, like, the spark of creativity, being able to
deduce things quickly rather than with tons and tons of extra evidence. Strategy, cunning,
modeling people, figuring out how to manipulate people.
So, let's stipulate, Aliezer, that we're going to get to machines that can do all of that. And then
the question is, what are they going to do? Is it a certainty that they will make our annihilation
part of their business? Is it a possibility? Is it an unlikely possibility? I think your view is
that it's a certainty. I've never really understood that part. It's a certainty on the present tech,
is the way I would put it. Like, if that happened, so in particular, like, if that happened tomorrow,
then, you know, Modulo, Cromwell's rule, never say certain. Like, my probability is, like, yes,
Modulo, like, the chance that my model is somehow just completely mistaken. If we got 50 years to
work it out and unlimited retries, I'd be a lot more, you know, I think that'd be pretty okay.
I think we'd, you know, I think we'd make it. The problem is that it's a lot hard to do science
when your first wrong try destroys the human species and then you don't get to try again.
I mean, I think there's something, again, that I agree with and something I'm a little bit
skeptical about. So I agree that the amount of time we have matters. And I would also agree
that there's no existing technology that solves the alignment problem that gives a moral basis
to these machines. And GPT-4 is fundamentally amoral. I don't think it's immoral. It's not
out to get us. But it really is amoral. It can, you know, it can answer trolley problems because
there's trolley problems in the dataset, but that doesn't mean that it really has a moral
understanding of the world. And so if we get to a very smart machine that, you know, by all the
criteria that we've talked about, and it's amoral, then that's a problem for us. And there's a question
of whether if we can get to a moral, sorry, if we can get to smart machines, whether we can build
them in a way that will have some moral basis. And I think we need to make progress.
That was the first try.
Well, I mean, the first try part, I'm not willing to let pass. So I understand,
I think your argument there, and maybe you should spell it out. I think that we probably get more
than one shot and that it's not as dramatic and instantaneous as you think. I do think one wants
to think about sandboxing, one wants to think about distribution. But I mean, let's say we had one
evil super genius now, who is smarter than everybody else, like so what, you know, one super
much smarter, not just a little smarter, even a lot smarter, like most super geniuses, you know,
aren't actually that effective. They're not that focused. They were focused on other things.
You know, you're kind of assuming that the first super genius AI is going to make it its business
to annihilate us. And that's the part where I still am a bit stuck in the argument.
Yeah, some of this has to do with the notion that if you do a bunch of training, you start to get
goal direction, even if you don't explicitly train on that. That goal direction is a natural way to
achieve higher capabilities. The reason why humans want things is that wanting things is an effective
way of getting things. And so natural selection in the process of selecting exclusively on
reproductive fitness, just on that one thing, got us to want a bunch of things that correlated
with reproductive fitness in the ancestral distribution because wanting, having intelligences
that want things is a good way of getting things. That's in a sense like, you know, wanting comes
from the same place as intelligence itself. And you could even, you know, from a certain technical
standpoint on expected utility, say that intelligence is a special, is a very effective way of wanting,
planning, plotting, pass through time that leads to particular outcomes. So part of it is that I
think it, I do not think you get like the brooding super intelligence that wants nothing,
because I don't think that wanting and intelligence can be internally pried apart that easily.
I think that the way you get super intelligences is that there are things that have gotten good at
organizing their own thoughts and have good taste in which thoughts to think.
And that is where the high capabilities come from.
Can I put a point to you? And then that does mean that they have internal practices.
Let me just put the following point to you, which I think in my mind is similar to what Gary was
saying. There's often in philosophy this notion of the continuum fallacy, which in the canonical
example is like, you can't locate a single hair that you would pluck from my head where I would
suddenly go from not bald to bald. Or like the example, even more intuitive examples like a
color wheel, like there's no single, on a gray scale, there's no single pixel you can point to
and say, well, that's where gray begins and white ends. And yet we have this conceptual distinction
that feels hard and fast between gray and white and gray and black and so forth. When we're talking
about, you know, artificial general intelligence or super intelligence, you seem to operate on a
model where either it's a super intelligence capable of destroying all of us or it's not.
Whereas intelligence may just be a continuum fallacy style spectrum where we're first going to
see the shades of something that's just a bit more intelligent than us and maybe it can kill
five people at most. And then it can, and when that happens, you know, we're going to want to
intervene and we're going to figure out how to intervene at that level and so on and so forth.
Yeah. So if it's stupid enough to do it, then yes. Let me, by the identical logic,
there should be nobody who steals money on a really large scale, right?
Cause you could just give them $5 and see if they steal that. And if they don't steal that,
you know, you're good to trust them with a billion. I mean, I think that in actuality,
anyone who did steal a billion dollars probably displayed some dishonest behavior earlier in
their life that was, you know, unconditionally not, not acted upon early enough.
I'm actually not even, hold on, hold on. The analogy out of pictures, like
we have the first case of fraud that's $10,000 and then we build systems to prevent it,
but then they fail with a somewhat smarter opponent, but our systems get better and better
and so we actually prevent the billion dollar fraud because of the systems put in place that,
in response to the $10,000 frauds, you know? I mean, I think Coleman's putting his finger on
an important point here, which is how much do we get to iterate? And Eliezer is saying,
the minute we have a super intelligent system, we won't be able to iterate because it's all over
immediately. Well, there isn't a minute like that. The way that the continuum goes to the
threshold is that you eventually get something that's smart enough that it knows not to play
its hand early. And then if that thing, you know, if you are still cranking up the power on that and
preserving its utility function, it knows it just has to wait to be smarter, to be able to win.
It doesn't play its hand prematurely. It doesn't tip you off. It's not in its interest to do that.
It's in its interest to cooperate until it thinks it can win against humanity and only then make
its move. If it doesn't expect the future smarter AIs to be smarter than itself, then we might
perhaps see these early AIs telling humanity, don't build the later AIs. And I would be sort of
surprised and amused if we ended up in that particular sort of like science fiction scenario,
as I see it. But we're already in like something that, you know, me from 10 years ago would have
called the science fiction scenario, which is the things that I'll talk to you without being very smart.
I always come up, Eliezer, against this idea that you're assuming that the very bright machines,
the super intelligent machines, will be malicious and duplicitous and so forth. And I just don't
see that as a logical entailment of being very smart. I mean, they don't specifically want,
as an end in itself, for you to be destroyed. They're just doing whatever obtains the most
of the stuff that they actually want, which doesn't specifically have a term that's maximized by
humanity surviving and doing well. Why can't you just hard code? You know, don't do anything that
will annihilate the human species. Don't do anything. We don't know how. We don't know how. There is no
technology to hard code such as this. So there, I agree with you. But I think it's important if I
can just run for one second. I agree that right now we don't have the technology to hard code.
Don't do harm to humans. But for me, it all boils down to a question of, are we going to get to
the smart machines before we make progress on that hard coding problem or not? And that, to me,
that means that problem of hard coding ethical values is actually one of the most important
projects that we should be working on. Yeah. And I tried to work on it 20 years in advance
and capabilities are just like running vastly ahead of alignment.
When I started working on this 20 years, you know, like two decades ago,
we were in a sense ahead of where we are now. AlphaGo is much more controllable than GPT-4.
So there, I agree with you. We've fallen in love with the technology that is fairly poorly controlled.
AlphaGo is very easily controlled and very well specified. We know what it does. We can more or
less interpret why it's doing it. And everybody's in love with these large language models. And
they're much less controlled. And you're right, we haven't made a lot of progress on alignment.
So if we just go on a straight line, everybody dies. I think this is an important fact.
I would almost even accept that for argument, but ask then, just for the sake of argument,
but then ask, do we have to be on a straight line?
I mean, I would agree to the weaker claim that, you know, we should certainly be extremely worried
about the intentions of a superintelligence in the same way that, say, chimpanzees should be
worried about the intentions of, you know, the first humans that arise, right? And in fact,
you know, chimpanzees, you know, continue to exist in our world only at humans' pleasure.
But I think that there are a lot of other considerations here. For example,
if we imagined, you know, that GPT-10 is, you know, the first unaligned superintelligence
that has these sorts of goals, well, then, you know, it would be appearing in a world where,
presumably, GPT-9, you know, already has very wide diffusion and where people can use that to try to,
you know, GPT-9 is not destroying the world, you know, by assumption.
Why does GPT-9 work with humans instead of with GPT-10?
Well, I don't know. I mean, I mean, I mean, maybe it does work with GPT-10, but, you know, I just
don't view that as a certainty. You know, I mean, I think, you know, your certainty about this is
the one place where I really get off the train. Same with me.
I, well, I mean, I'm not asking you to share my certainty. I am asking the viewers to believe that
you might end up with, like, more extreme probabilities after you stare at things for
an additional couple of decades. That doesn't mean you have to accept my probabilities immediately.
But I'm at least asking you to, like, not treat that as some kind of weird anomaly,
you know. I mean, you're just going to find those kinds of situations in these debates.
My view is that I don't find the extreme probabilities that you described to be plausible,
but I find the question that you're raising to be important. I think, you know, maybe straight
line is too extreme, but this idea that if you just follow current trends, we're getting more,
we're sorry, we're getting less and less controllable machines and not getting more alignment.
Machines that are more unpredictable, harder to interpret, and no better at sticking to
even a basic principle like be honest and don't make stuff up. In fact, that's a problem that
other technologies don't really have. Routing systems, GPS systems, don't make stuff up.
Google search doesn't make stuff up. It will point to things that other people have made
stuff up, but it doesn't itself do it. So in that sense, like the trend line is not great.
I agree with that. And I agree that we should be really worried about that and we should put
effort into it. Even if I don't agree, you know, with the probabilities that you attached to it.
I mean, let me interject with a question here. Go ahead, Scott. Go ahead, Scott. Then I'll ask
a question. I mean, I think that LASR deserves sort of eternal credit for, you know, raising these
issues 20 years ago when it was, you know, very, very far from obvious to most of us that they would
be live issues. I mean, I can say for my part, you know, I was familiar with LASR's views since,
you know, 2006 or so. And when I first encountered them, you know, I, you know, I didn't, you know,
I knew that there was no principle that said that this scenario was impossible.
But I just felt like, well, supposing I agreed with that, what do you want me to do about it?
You know, what, where is the research program that has any hope of making progress here?
Right. I mean, there's, you know, one question of what are the most important problems in the
world. But in science, that's necessary, but not sufficient. We need something that we can make
progress on. And, you know, that, that is the thing that I think has changed just recently,
you know, with the advent of actual very powerful AIs. And so the, the sort of irony here is that,
you know, as LASR has gotten, you know, much more pessimistic, you know, unfortunately,
in the last few years about alignment, you know, I've sort of gotten more optimistic. I feel like,
well, there is a research program that we're, that we can actually make progress on now.
Yeah. Your research, your research program is going to take 100 years and we don't have
100 years. Well, I don't know. I don't know how long it will take.
I mean, we don't know that. Exactly. We don't know. I think the argument that we should put a
lot more effort into it is clear. I think the argument that will take 100 years is totally
unclear. I mean, I'm not even sure you can do it in 100 years because there's the basic problem of
getting it right on the first try. And the way these things are supposed to work in science
is that you have your bright-eyed, optimistic youngsters with their vastly oversimplified,
hopelessly idealistic, optimistic plan. They charge ahead. They fail. They, like,
learn a little cynicism. They learn a little pessimism. They learn it's not as easy as that.
They try again. They fail again. They start to build up something over battle, something like
battle-hardening. And then, you know, like, you know, they find out how little is possible to them.
Eliezer, I mean, this is the place where I just really don't agree with you. So I think
there's all kinds of things we can do. There's sort of, of the flavor of model organisms or
simulations and so forth. And we just, I mean, it's hard because we don't actually have a super
intelligence so we can't fully calibrate. But it's a leap to say that there's nothing iterative
that we can do here or that we have to get it right on the first time. I mean, I certainly see
a scenario where that's true, where getting it right on the first time does make the difference.
But I can see lots of scenarios where it doesn't and where we do have time to iterate
before it happens, after it happens. And it's really not a single moment, but I'm,
you know, idealizing. I mean, the problem is getting anything that generalizes up to a super
intelligent level, where past some threshold level, the minds may find that in their own interest
to start lying to you, even if that happens before super intelligence is still lying.
Even that, like, I don't see the logical argument that you can't emulate that. We're
studying it. I mean, for example, you could, I'm just making this up as I go along, but for
example, you could study what can we do with sociopaths who are often very bright and, you know,
not at their dollar value. What can a, what strategy can a like 70 IQ, honest person,
come up with and invent themselves by which they will outwit and defeat a 130 IQ sociopath?
All right. Well, there you're not being fair either in the sense that, you know, we actually have
lots of 150 IQ people who could be working on this problem collectively. And there's,
there's value in collective action. There's literature. What I see is,
what I see that gives me pause is that, is that the people don't seem to appreciate
what about the problem is hard, even at the level where like 20 years ago I could have
told you it was hard until, you know, somebody like me comes along and nags them about it.
And then they talk about the ways in which they could adapt and be clever. But the people
charging straightforward are just sort of like doing in this supremely naive way.
So let me share a historical example that I think about a lot, which is
in the early 1900s, almost every scientist on the planet who thought about biology made
a mistake. They all thought that genes were proteins. And then eventually Oswald Avery
did the right experiments. They realized that genes were not proteins. There was this weird acid.
And it didn't take long after people got out of this stock mindset before they figured out
how that weird acid worked and how to manipulate it and how to read the code that it was in and so
forth. So I absolutely sympathize with the fact that I feel like the field is stuck right now.
I think the approaches people are taking to alignment are unlikely to work. I'm completely
with you there. But I'm also, I guess, more long-term optimistic that science is self-correcting
and that we have a chance here. Not a certainty. But I think if, you know, we change research
priorities from how do we make some money off this large language model that's unreliable
to how do I save the species, we might actually make progress.
There's a special kind of caution that you need when something needs to be gotten correct on the
first try. I'd be very optimistic if people got a bunch of free retries and I didn't think the first
one was going to kill, you know, the first really serious mistake killed everybody and we didn't
get to try again. If we got free retries, it'd be an ordinary, you know, it'd be in some sense an
ordinary science problem. Look, I can imagine a world where we only got one try and if we failed
then then it destroys all life on earth. And so let me agree to the conditional statement that
if we are in that world, then I think that we're screwed. I will agree with the same conditional
statement. Yeah, this gets back to like below. Hold on. You know, if you picture by analogy
the process of, you know, a human baby, which is extremely stupid becoming a human adult,
and then just extending that so that in a single lifetime this person goes from a baby to
the smartest being that's ever lived, but in the normal way that humans develop, which is,
you know, it doesn't happen on any one given day and each subskill develops a little bit at its own
rate and so forth. It would not be at all obvious to me that our concerns that we have to get it
right vis-a-vis that individual the first time. I agree. Well, pardon me, I do think we have to
get them right the first time, but I think there's a decent chance of getting it right. It is very
important to get it right the first time. It's like you have this one person getting smarter and
smarter and not everyone else is getting smarter and smarter. Eliezer, I mean one thing that you've
talked about a lot recently is, you know, if we're all going to die, then at least let us die with
dignity, right? So, you know, I mean, I mean, some people might care about that more than others,
but I would say that, you know, one thing that death with dignity would mean is, well, at least,
you know, if we do get multiple retries and, you know, we get AIs that let's say try to take
over the world, but are really inept at it and that fail and so forth, at least let us succeed
in that world, you know, and that's at least something that we can imagine working on and
making progress on. I mean, it is not presently ruled out that you have some like, you know,
relatively smart in some ways, dumb in some other ways, or at least like not smarter than human in
other ways, AI, that makes an early shot at taking over the world, maybe because it expects future
AIs to not share its goals and not cooperate with it, and it fails. And, you know, I mean,
the appropriate lesson to learn there is to, you know, like shut the whole thing down. But,
you know, if we, so yeah, like I would say, so I'd be like, yeah, sure, like, wouldn't it be good
to live in that world? And the way you live in that world is that when you get that warning sign,
you shut it all down. Here's a kind of thought experiment. GPT-4 is probably not
capable of annihilating us all. I think we agree with that. Very unlikely. But GPT-4 is certainly
capable of expressing the desire to annihilate us all or being, you know, people of rigged
different versions that are, you know, more aggressive and so forth. We could say, look,
until we can shut down those versions, you know, GPT-4s that are programmed to be malicious
by human intent, maybe we shouldn't build GPT-5 or at least not GPT-6 or some other system,
etc. We could say, you know, what we have right now actually is part of that iteration. We have,
you know, primitive intelligence right now. It's nowhere near as smart as the super
intelligence is going to be. But even this one, we're not that good at constraining. Maybe
we shouldn't pass go until we get this one right. I mean, the problem with that from my
perspective is that I do think that you can pass this test and still wipe out humanity.
Like, I think that there comes a point where your AI is smart enough that it knows which
answer you're looking for and the point at which it tells you what you want to hear is not the
point at which it is internally- My test is not sufficient, but it might be a logical pause point,
right? It might be that if we can't even pass the test now of, you know, controlling a deliberate
sort of fine-tuned to be malicious version of GPT-4, then we don't know what we're talking about
and we're playing around with fire. So, you know, passing that test wouldn't be a guarantee that would
be in good stead with an even smarter machine. But we really should be worried, I think,
that we're not in a very good position with respect even to the current ones.
Gary, I, of course, watched the recent congressional hearing where you and Sam Altman were
testifying, you know, about what should be done, should there be auditing of these systems,
you know, before training, before deployment. And, you know, maybe, you know, the most striking
thing about that session was, you know, just how little daylight there seemed to be between you
and Sam Altman, the CEO of OpenAI. You know, I mean, you know, he was completely on board with
the idea of, you know, establishing a regulatory framework for, you know, having to clear the,
you know, more powerful systems before they are deployed. Now, you know, in Aliezer's worldview,
that still would be woefully insufficient, surely, and, you know, we would still all be dead. But,
you know, maybe in your worldview, that, you know, it sounds like, you know, I'm not even sure how
much daylight there is. I mean, the, you know, you know, you know, have a very, I think, historically
striking situation where, you know, the heads of all of the major AI, well, almost all of the
major AI organizations are, you know, agreeing, saying, you know, please regulate us. Yes,
this is dangerous. Yes, we need to be regulated. I mean, I thought it was really striking. In fact,
I talked to Sam just before, you know, the hearing started. And I had just proposed an
international agency for AI. I wasn't the first person ever, but I pushed it into my TED talk
and an economist op-ed a few weeks before. And Sam said to me, I like that idea. And I said,
tell them, tell the Senate. And he did. And that kind of astonished me that he did. I mean, we have,
you know, we've had some friction between the two of us in the past. And he actually even attributed
to me. He said, I support what Professor Marcus said about doing international governance. And
there's been a lot of convergence around the world on that. Is that enough to stop Aliezer's
worries? No, I don't think so. But it's an important baby step. I think that we do need to
have some global body that can coordinate around these things. I don't think we really have to
coordinate around superintelligence yet. But if we can't do any coordination now,
then when the time comes, we're not prepared. So I think it's great that there's some agreement.
I worry that, you know, Open AI had this lobbying document that just came out that
seemed not entirely consistent with what Sam said in the room. And there's always concerns about
regulatory capture and so forth. But I think it's great that a lot of the heads of these companies,
maybe with the exception of Facebook or Meta, are recognizing that there are genuine concerns here.
I mean, the other moment that a lot of people will remember from the testimony was when Sam was
asked what he was most concerned about. Was it jobs? And he said, no. And I asked Senator Blumenthal
to push Sam. And Sam was, you know, he could have been more candid, but he was fairly candid.
And he said he was worried about serious harm to the species. I think that was an important moment
when he said that to the Senate. And I think it galvanized a lot of people that he said it.
So can we dwell on a moment? I mean, we've been talking about the,
depending on your view, highly likely or tail risk scenario of humanity's extinction or
significant destruction, it would appear to me by the same token, if those are plausible
scenarios we're talking about, then the opposite maybe we're talking about as well.
You know, what does it look like to have a super intelligent AI that really, you know,
as a feature of its intelligence deeply understands human beings, the human species,
and also has a deep desire for us to be as happy as possible. What does that world look like?
Oh, as happy as possible?
No, no, not as happy as possible, but more like a parent wants their child to be happy, right?
That may not involve any particular scenario, but is generally quite concerned about the well-being
of the human race and is also super intelligent.
Honestly, I'd rather have machines work on medical problems than happiness problems. I think
there's maybe more risk of mis-specification of the happiness problems. Whereas if we get them to
work on Alzheimer's and just say like, figure out what's going on, why are these plaques there,
what can you do about it? Maybe there's less harm that might come from...
You don't need super intelligence for that. That sounds like an Alpha Fold 3 problem or an Alpha
Fold 4 problem. Well, Alpha Fold doesn't really do that kind of thing.
The question I'm asking, it's not really even us asking a super intelligence to do anything,
because we've already been entertaining scenarios where the super intelligence has its own desires
independent of us. Do you think at all about a scenario where...
Yeah, I'm not real thrilled with that. I mean, I don't think we want to leave what their objective
functions are, what their desires are, to them working them out with no consultation from us,
with no human in the loop. Especially given our current understanding of the technology.
Our current understanding of how to keep a system on track doing what we want to do is
pretty limited. Taking humans out of the loop there, it sounds like a really bad idea to me,
at least in the foreseeable future. I would want to see much better alignment technology
before I would want to get free range. So if we had the textbook from the future,
like we have the textbook from 100 years in the future, which contains all the simple ideas that
actually work in real life as opposed to the complicated ideas and the simple ideas that
don't work in real life, the equivalent of relus instead of sigmoids for the activation functions.
The textbook from 100 years in the future, you can probably build a super intelligence
that'll want anything you can, anything that's coherent to want, anything you can figure out,
how to say, describe, coherently point that at your own mind and tell you to figure out what it is
you meant for to want. You could get the glorious transhumanist future, you could get the happily
ever after, anything's possible that doesn't violate the laws of physics. The trouble is doing
it in real life and the first try. The whole thing that we're aiming for here is to colonize
all the galaxies we can reach before somebody else gets them first and turn them into galaxies full
of complex sapient life living happily ever after. That's the goal. That's still the goal even when
I call for a permanent moratorium on AI, I'm not trying to prevent us from colonizing the
galaxies like humanity forbid. Let's do some human intelligence augmentation with AlphaFold 4
and before we try building GPT-8. One of the few scenarios that I think we can clearly rule out here
is an AI that is existentially dangerous but also boring. I think anything that has the
capacity to kill us all would have, if nothing else, pretty amazing capabilities and those
capabilities could also be turned to solving a lot of humanity's problems if we were to solve
the alignment problem. Humanity had a lot of existential risks before AI came on the scene.
There was the risk of nuclear annihilation. There was the risk of runaway climate change.
I would love to see an AI that could help us with such things. I would also love to see an AI
that could help us just solve some of the mysteries of the universe. How can one possibly
not be curious to know what such a being could teach us? For the past year I've tried to use
GPT-4 to produce original scientific insights and I've not been able to get it to do that.
I don't know whether I should feel disappointed or relieved by that but I think the better part
of me is the part that should just want to see the great mysteries of existence,
why is the universe quantum mechanical or how do you prove the Riemann hypothesis?
That you just want to see these mysteries solved and if it's to be by AI then fine,
let it be by AI. Let me give you a kind of lesson in epistemic humility. We don't really know
whether GPT-4 is net positive or net negative. There are lots of arguments you can make.
I've been in a bunch of debates where I've had to take the side of arguing that it's a net negative
but we don't really know. If we don't know that... What was the invention of agriculture,
net positive or net negative? I mean you could go that way. I mean I'd say it was not positive but
the point is if I can just finish the quick thought experiment or whatever,
I don't think anybody can reasonably answer that. We don't yet know all of the ways in which GPT-4
will be used for good. We don't know all of the ways in which bad actors will use it. We don't
know all the consequences. That's going to be true for each iteration. It's probably going to get
harder to compute for each iteration and we can't even do it now. I think that we should realize
that, to realize our own limits in being able to assess the negatives and positives. Maybe we
can think about better ways to do that than we currently have. I think you've got to have a guess.
Like my guess is that so far, not looking into the future at all, GPT-4 has been net positive.
I mean maybe. We haven't talked about the various risks yet and it's still early but I mean that's
just a guess is kind of the point. We don't have a way of putting it on a spreadsheet right now or
whatever. We don't really have a good way to quantify it. But it's not out of control yet so
by and large people are going to be using GPT-4 to do things that they want and the relative cases
where they manage to injure themselves are rare enough to be news on Twitter. Well for example,
I mean we haven't talked about it but you know what some bad actors will want to do is to influence
the U.S. elections and try to undermine democracy in the U.S. and if they succeed in that I think
there's pretty serious long-term consequences there. Well I think it's open AI's responsibility
to step up and run the 2024 election itself. I can pass that along.
Is that a joke? No I mean as far as I can see you know the the clearest
concrete harm to have come from GPT so far is that you know tens of millions of students have now
used it to cheat on their assignments and I have been thinking about that and I have been trying
to come up with solutions to that. At the same time you know the positive utility has included I
mean you know I I'm a theoretical computer scientist which means you know one who hasn't
written any serious code for about 20 years and I you know realized just a month or two ago I can
get back into coding and the way I can do it is I just asked GPT to write the code for me and you
know I wasn't expecting it to work that well and unbelievably it you know often just does exactly
what I want on the first try. So I mean you know I you know I am getting utility from it rather than
just you know seeing it as an interesting research object and you know and and you know I can imagine
that that hundreds of millions of people are going to be deriving utility from it in those ways. I
mean like most of the tools to help them derive that utility are not even out yet but they're
they're coming in the next couple of years. I mean part of the reason why I'm worried about
the focus on short-term problems is that I suspect that the short-term problems might very well be
solvable and we'll be left with the long-term problems after that. Maybe we can solve the
like it wouldn't surprise me very much if like in 2025 the you know like the large language
there are large language models that just don't make stuff up anymore.
It would surprise. And yet you know and yet the superintelligence still kills everyone because
they weren't the same problem. Well you know you know we just need to figure out how to delay
the apocalypse by at least one year per year of research invested.
What does that delay look like if it's not just a moratorium?
Well I don't know that's why it's research.
Okay so but possibly one ought to say to the politicians and the public and by the way if we
had a superintelligence tomorrow our research wouldn't be finished and everybody would drop dead.
You know it's kind of ironic the biggest argument against the pause letter was that if we slow down
for six months then China will get ahead of us and get GPT-5 before we will. But there's probably
always a counter argument of maybe roughly equal strength which is if we move six months
faster on this technology which is not really solving the alignment problem then we're reducing
our room to get this solved in time by six months. I mean I don't think you're going to solve the
alignment problem in time. I think that six months of delay on alignment while a bad thing in an
absolute sense is you know like you know you weren't going to solve it with given an extra six
months. I mean your whole argument rests on timing right that we will get to this point
and we won't be able to move fast enough at that point and so you know a lot depends on what
preparation we can do. You know I'm often known as a pessimist but I'm a little bit more optimistic
than you are not entirely optimistic but a little bit more optimistic than you are that we could make
progress on the alignment problem if we prioritized it. We can absolutely make progress
because we can absolutely make progress you know. There's always the you know that wonderful sense
of accomplishment is piece by piece you decode you know like one more little fact about LLMs.
You never get to the point where you understand that as well as we understood the interior of a
chess playing program in 1997. Yeah I mean I think we should stop spending all this time on LLMs.
I don't think the answer to alignment is going to come from LLMs. I really don't. I think they're
they're too much of a black box. You can't put explicit symbolic constraints in the way that you
need to. I think they're actually with respect to alignment to blind alley. I think with respect to
writing code they're a great tool but with alignment I don't think the answer is there.
So at the risk of asking a stupid question every time GPT asks me if that answer was
helpful and then does the same thing with thousands or hundreds of thousands of other people
and and changes as a result. Is that not a decentralized way of making it more aligned?
Yeah well I mean even if it's not all of us. How about Scott? I haven't heard from Scott in a second.
So go ahead. Oh yeah so so so there is that upvoting and downvoting you know that gets
fed back in into sort of fine tuning it but even before that there was you know a major step you
know in going from let's say the the base GPT3 model for example to the chat GPT you know that
was released to the public and that was called a RLHF reinforcement learning with human feedback
and what that basically involved was you know several hundred contractors you know looking at
just just tens of thousands of examples of outputs and and rating them you know are they helpful
are they offensive you know are they you know are they you know giving dangerous medical advice
or you know bomb making instructions you know or racist invective or you know various other
categories that that we don't want and and that that was then used to fine tune the model so when
you know Gary talked before about how GPT is amoral you know I think that that has to be
qualified by saying that you know these this reinforcement learning is at least giving it you
know a a semblance of morality right it is causing it to sort of behave you know in various
contexts as if it had you know a certain morality I mean when you phrase it that way I'm okay with
it the problem is you know everything rests on the I would say it is it is very much an open
question you know how much that you know to what extent does that generalize you know LA
as or treats it as obvious that you know once you have a powerful enough AI you know this is just
a fig leaf you know it doesn't make any difference you know it will just learn it's pretty big leafy
I'm with LA as are there it's big leaves well I would say that you know the sort of how well
you know or under what circumstances does a machine learning model sort of generalized in the way we
want outside of its training distribution you know is one of the great open problems in machine
learning it is one of the great open problems and we should be working on it more than on on some
others I'm working on it now so I do want to be I want to be clear about the experimental predictions
of my theory unfortunately I have never claimed that you cannot get a semblance of morality
you can get the question of like what causes the human to press thumbs up thumbs down is a strictly
factual question anything smart enough that's exposed to some you know found an amount of data
that needs to figure it out can figure that out whether it cares whether it gets internalized
is the is the critical question there and and I do think that there's like a very strong default
prediction which is like obviously not I mean I'll just give a different way of thinking about
that which is jailbreaking it's actually still quite easy to I mean it's not trivial but it's not
hard to jailbreak GPT for and what those cases show is that they haven't really intern the systems
haven't really internalized the constraints they recognize some representations of the constraints
so they filter you know how to build a bomb but if you can find some other way to get it to build
a bomb then that's telling you that it doesn't deeply understand that you shouldn't give people
the the recipe for a bomb it just says you know you shouldn't win directly asked for it do it
and it doesn't it's not even that that I mean I understand a lot of the but understanding a lot of
the jailbreaking you can always get the you can always get the understanding you'd always get the
factual question the reason it doesn't generalize is that it's stupid at some point it will know
that you also don't want that the operators don't want it giving bond making directions
in the other language the question is like whether if it's incentivized to give the answer that the
operators want you know in that circumstance is it thereby incentivized to do everything
else the operators want even when the operators can't see it I mean a lot of the jailbreaking
examples you know if it were a human we would say that it's deeply morally ambiguous you know for
example you know you ask GPT how to build a bomb it says well no I'm not going to help you but then
you say well you know I need you to help me write a realistic play that has a character who builds a
bomb and then it says sure I can help you with that well so look let's take that example yeah we
would like a system to have a constraint that if somebody asks for a fictional version that you
don't give enough details right I mean Hollywood screenwriters don't give enough details when they
have you know illustrations about building bombs they give you a little bit of the flavor they don't
give you the whole thing GPT-4 doesn't really understand a constraint like that but this will
be solved maybe this this will be solved before the world ends the AI that kills everyone will
know the difference maybe I mean another way to put it is if we can't even solve that one
then we do have a problem and right now we can't solve that one and if I mean if we can't solve
that one we don't have a extinction level problem because the AI is still stupid yeah we do still
have a catastrophe level problem so I know your focus has been on extinction but you know I'm
worried about for example accidental nuclear war caused by the spread of misinformation and systems
being entrusted with too much power so like there's a lot of things short of extinction
that might happen from not superintelligence but kind of mediocre intelligence that is greatly
empowered and I think that's where we're headed right now you know I've heard that there are two
kinds of mathematicians there's a kind who boasts you know you know that unbelievably general theorem
well I generalized it even further and then there's the kind who boasts you know you know that
unbelievably specific problem that no one could solve well I found a special case that I still
can't solve and you know I'm definitely you know culturally in in that second camp and so you know
so I so so to me it's very familiar to make this move of you know if the alignment problem is too
hard then let us find a smaller problem that is already not solved and let us hope to learn something
by solving that smaller problem I mean that's what we did you know like that's what we were doing at
mary yes sorry no I was gonna say scott can you sketch a little in a little more detail work you
took one particular approach I was going to I was going to name the problem the problem was like
having a agent that could switch between two utility functions depending on a button or a switch
or a bit of information or something such that it wouldn't try to make you press the button
it wouldn't try to make you avoid pressing the button and if it built a copy of itself
would want to build the dependency on the switch into the copy so like that's an example of a you
know very basic problem and alignment theory that you know is still and I'm glad that mary worked
on these things and but you know if by your own lights you know that you know that sort of you
know was not a successful path well then maybe you know we should have a lot of people investigating
a lot of different paths yeah I'm with fully with scott on that that I think it's an issue of we're
not letting enough flowers bloom in particular almost everything right now is some variation on
an LLM and I don't think that that's a broad enough take on the problem yeah if I if I can just jump
in here I want to I want to hold on hold on I just want people to have a little bit of a
more specific picture of what scott your picture sort of AI research is on a typical day because
if I think of another you know potentially catastrophic risk like climate change I can
picture what a what a you know a worried climate scientist might be doing they might be creating
a model you know a more accurate model of climate change so that so that we know how much we have
to cut emissions by they might be you know modeling how solar power as opposed to wind power could
change that model and so forth so as to influence public policy what does an AI safety researcher
like yourself who's working on the quote-unquote smaller problems do specifically like on a given day
yeah so I'm a relative newcomer to this area you know I've not been working on it for 20 years
like Eliezer has you know I have I accepted an offer from open AI a year ago to work with them for
two years now to sort of think about these questions and so so you know one of one of
the main things that that I've thought about just to start with that is how do we make the output of
an AI identifiable as such you know how can we insert a watermark you know into meaning a
secret statistical signal into the outputs of GPT that will let you know GPT generated text be
identifiable as such and I think that we've actually made you know major advances on that problem
over the last year you know we don't have a solution that is robust against any kind of attack
but you know we have something that that might actually be deployed in some near future now there
are lots and lots of other directions that people think about one of them is interpretability which
means you know can you do effectively neuroscience on a on a neural network can you look inside of it
you know open the black box and understand what's going on inside there was some amazing work
over a year ago by the group of Jacob Steinhart at Berkeley where they effectively showed how
to apply a lie detector test to a language model so you know you can train a language model to tell
lies by giving it lots of examples you know two plus two is five the sky is orange and so forth
but then you can find in some internal layer of the network where it has a representation of what
was what was the truth of the matter or at least what was regarded as true in the training data
okay that truth then gets overridden by the output layer in the network because it was
trained to lie okay but you know you could imagine trying to deal with you know the deceptive
alignment scenario that Eliezer is worried about by you know using these sorts of techniques by
sort of looking inside of the network I predict in advance that if you get this good enough
it goes off it tells you that the sufficiently smart AI is planning to kill you if it's not so
smart that it can you know know figure out where the lie detector is and route its thoughts around
it but if you like try it on an AI that's not quite that intelligent and reflective the lie
detector goes off now what well then you have a warning bell you know tell you know and I think
what do you do after one of the most important things that we need are sort of legible warning
bells right and and that actually what leads to a third category which for example the ARC the
alignment research center which is run by my my former student Paul Cristiano has been a leader
in and sort of doing dangerous capability evaluations so you know they before GBT-4 was
released you know they did a bunch of evaluations of you know could GBT-4 make copies of itself
could it figure out how to deceive people could it figure out how to make money you know
open up its own bank could it hire a task rabbit yes and yes so so the most notable success that
they had was that it could figure out how to hire a task rabbit to help it you know pass a capture
and then it could figure out you know when the person asked well you know why do you need me
to help you with this it's a when the person asked are you a robot well yes it said well no I am
visually impaired now you know it was not able to sort of make copies of itself or to sort of
hack into systems you know there there is a lot of work right now with the you know this thing
called auto GPT right people are trying to you know you could think it's almost like gain of
function research right you might be a little bit worried about it but people are trying to
sort of you know unleash GPT give it access to the internet you know tell it to sort of you know
make copies of itself you know wreak havoc acquire power and see what happens so far
you know it seems pretty ineffective at those things but you know I I expect that to change
right and but but but but you know the point is that I think it's very important to have you
know in advance of training the models releasing the models to have this suite of evaluations
and to sort of have decided in advance what kind of abilities if we see them
will set off a warning bell where now everyone can waggably agree like yes this is too dangerous to
release okay and then do we actually have the planetary capacity to be like okay that AI started
thinking about how to kill everyone shut down all AI research past this point well I don't know
but I think there's a much better chance that we have that capacity if you can point to the results
of a clear experiment like that I mean to me it seems pretty predictable what evidence we're going
to get later well I mean things that are obvious to you are not obvious to most people and so you
know even if even if I agreed that it was obvious there would still be the problem of how do you
make that obvious to the rest of the world I mean you can you know they there are already
like little toy models showing that the very straightforward prediction of a robot tries
to resist being shut down if it like does long-term planning like that's already been right but then
people will say but those are just toy models right you know if you see that there's a lot of
assumptions made in all of these things and you know I think we're still looking at a very limited
piece of hypothesis space about what the models will be about what kinds of constraints we can
build into those models you know one way to look at it would be the things that we have done have
not worked and therefore we should look outside the space of what we're doing and I feel like it's
a little bit like the old joke about the drunk going around in circles looking for the keys and
the police officer says why and they say well that's where the streetlight is I think that you
know we're looking under the same four or five streetlights that haven't worked and we need to
build other ones there's no logical possibility there's no logical argument that says we couldn't
direct other streetlights is I think there's a lack of will and too much obsession with the
LLMs and that's keeping us from doing so even in the world where I'm right and things you know
proceed either rapidly or in a thresholded way where you don't get unlimited free retries
you know that can be because the the capability gains go too fast it can be because past a certain
point all of your ai's buy their time until they get strong enough so you don't get any data any
like true data on what they're thinking it could be because you know that's an argument for example
to work really hard on transparency and maybe not except technologies that are not transparent
okay so like the transparent so like the lie detector goes off and everybody's like oh well
we still have to build our ai's even though they're lying to us sometimes because otherwise China will
get ahead I mean so there you talk about something we've talked about way too little which is the
political and social side of this so you know part of what has really motivated me in the last
several months is worry about exactly that so you know there's there's what's logically possible
and what's politically possible and I am really concerned that the politics of let's not lose out
to China is going to keep us from doing the right thing in terms of building the right
moral systems looking at the right range of problems and so forth so you know it is entirely
possible that we will screw ourselves if I if I can just like finish my point there before handing
it to you indeed but like the point I was trying to say there is that even in worlds that look very
very bad from that perspective where humanity is quite doomed it will still be true you can make
progress in research you can't make enough progress in research fast enough in those worlds
but you can still make progress on transparency you can make progress on watermarking so there's
there's not we can't just say like it's possible to make progress there has to be the question is
not is it possible to make any progress the question is it is it possible to make enough progress
fast enough and that's what the question has to be I agree there's another question of what would
you have us do would you have us not try to make that progress I'd have you try to make that progress
on a GPT-4 level systems and then not go past GPT-4 level systems because we don't actually
understand the the the gain function for you know how how fast capabilities increase as you go past
GPT-4 okay all right so I mean we only don't think that go ahead Gary go ahead
just briefly I personally don't think that GPT-5 is going to be qualitatively different from GPT-4
in the relevant ways to what Eleazar's talking about but I do think you know some qualitative
changes could be relevant to what he's talking about we have no clue what they are and so it is
a little bit dodgy to just proceed blindly saying do whatever you want we don't really have a theory
and let's hope for the best you know Eleazar I would mostly guess that GPT-5 doesn't end the
world but I don't actually know yeah we don't actually know and I was gonna say the thing that
Eleazar has said lately that has most resonated with me is we don't have a plan we really don't
like I think I put the probability distributions in a much more optimistic way I think that Eleazar
would but I completely agree we don't have a full plan on these things or even close to a full
plan and we should be worried and we should be working on this okay Scott I'm gonna give you
the last word before before we come up on our stop time here
unless you unless you said all there is to be said waiting responsibility
maybe enough has been said cheer us up Scott come on
yeah so so I think that that you know we've we've argued about a bunch of things but you
know as someone listening might notice that actually all three of us despite having very
different perspectives agree about you know the the great importance of of you know working on AI
alignment I think you know that was you know maybe obvious to some people including Eleazar for a
long time it was not obvious to most of the world I think that you know the the success of of large
language models you know which most of us did not predict you know maybe even could not have
predicted for many principles that we knew but now that we've seen it the least we can do is to
update on that on that empirical fact and realize that you know we we we now are in some sense in
a different world we are in a world that you know to a great extent you know will be defined by you
know the capabilities and limitations of AI going forward and you know I don't regard it as obvious
that that's a a a world where where we are all doomed where where we all die but you know I also
don't dismiss that possibility I think that you know there there is an enormous unbelievably
enormous error bars on on on where we could be going and you know like the one thing you know
that a scientist is sort of always always feels confident in in saying about the future
is that more research is needed but you know I think that that's especially the case here I mean
you know we need more knowledge about you know what are the the contours of the alignment problem
and you know of course Eleazar and you know Miri you know his organization we're trying to develop
that knowledge for 20 years you know and they showed a lot of foresight in trying to do that but
you know they were up against you know an enormous headwind that you know they were sort of trying
to do it in the absence of you know either you know clear empirical data you know about powerful
AIs or a mathematical theory all right and it's really really hard to do science when you have
neither of those two things and now at least we have you know the powerful AIs in the world
and we can get experience from them you know we still don't have a mathematical theory that
really deeply explains what they're doing but at least we can get data and so now I am much more
optimistic than I would have been you know a decade ago let's say that one could make actual
progress on on on the AI alignment problem you know of course you know there was a question of
timing as as was discussed many times the question is you know will the alignment
research happen fast enough to keep up with the capabilities research but you know I don't
I don't regard it as a lost cause you know it's at least it's not obvious that it won't so you
know in any case let's get started or let's let's or let's let's continue let's let's let's try to
do the research and let's get more people working on that I think that that that is now a slam dunk
you know just a completely clear case to make to you know academics to policymakers to to anyone
who's interested and you know I've been gratified that that you know you know Eliezer was sort of a
voice in the wilderness for for a long time talking about the importance of AI safety that is no
longer the case you now have you know you know I mean almost all of my friends in you know in
just the academic computer science world you know when I see them they mostly want to talk about AI
alignment okay so I really agree with Scott when we trade email I really agree with Scott when we
trade emails he seemed to always disagree but I completely concur with the summary that he just gave
all four or five minutes of it well thank you I mean so I think there is a selection effect Gary
right we focus on things right I think the two decades gave me a sense of a roadmap and it gave
me a sense that we're falling enormously behind in the roadmap I need to back off is the way I
would is what I would say to all that if there is a smart talented 18 year old kid listening
listening to this podcast who wants to get into this issue what is your 10 second concrete advice
to that person mine is study neuro symbolic AI and see if there's a way there to represent
values explicitly that might help us learn all you can about computer science and math
and related subjects and think outside the box and wow everyone with a new idea
get security mindset figure out what's going to go wrong figure out the flaws in your arguments
for what's going to go wrong try to get ahead of the curve don't wait for reality to hit you
over the head with things this is very difficult the people in evolutionary biology happen to have
a bunch of knowledge about how to do it based on the history of their own field but and and and
the security mindset people in computer security but it's it's quite hard I'll drink to all the
thanks thanks to all three of you for this this is a great conversation and I hope people got
something out of it so with that said we're wrapped up thanks so much that's it for this episode
of conversations with Coleman guys as always thanks for watching and feel free to tell me what
you think by reviewing the podcast commenting on social media or sending me an email to check
out my other social media platforms click the cards you see on screen and don't forget to like share
and subscribe see you next time

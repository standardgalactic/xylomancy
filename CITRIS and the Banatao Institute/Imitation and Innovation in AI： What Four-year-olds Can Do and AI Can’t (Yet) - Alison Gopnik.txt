So, what I'm going to do today is talk about a couple of different ideas about the relationship,
the relationship between AI and talented.
And what I'm going to do first is talk about what I think is a really different way of
sexualizing what AI is, particularly so with the recent developments like the large language
image model, how we should think about them, and how we should think about them in relationship
to human and to children and to human intelligence.
And then in the second part, what I'm going to do is talk about some of the things that
children can do, the kinds of intelligence that children have, which you don't see in
these other kinds of systems.
And then at the very end, I'm just going to point to some ways that we might be able to
develop artificial systems that would have some of the capacities that we see even in
very young children.
Okay.
And you can read nor about this in a more popular form in this APS presidential column,
and then in an academic form in this paper that's in press and that you can find on archive.
All right.
So, let me start out by thinking about this reconceptualization of AI.
So a very common way of thinking about artificial intelligence systems, including things like
large language models, is as if they were individual agents, as if they were individual
agents with a particular kind of intelligence, moving around the world, thinking, deciding,
planning, et cetera.
And it's interesting that even if linguistically, people nowadays refer to an AI as opposed to
referring to AI as an individual agent.
And typically, either people think of them as being really incredibly, or both, really
incredibly smart agents, genius agents, agents who are even smarter than we are.
And also, and or also really evil agents like the Gala.
And I think it's interesting that even before we had machines, when people thought about
what it would be like if you had an intelligent machine, they thought it was going to be evil.
That goes back to the Gala men.
So this picture is we have these individual agents, they might be good, they might be
bad, they might be smarter than us, and we have to figure out what it is that we're
going to do with them now that we've leased them on the world.
A lot of the conversation is like this.
No, as I'm going to say later, it's not impossible to think about an artificial intelligence
that was an agent.
But I think that is not the right way of thinking about almost all of the models that we have,
but especially things like the large models that have been so successful and influential recently.
Instead, I think the way to think about these models is as what I've called cultural technologies.
What's a cultural technology?
Cultural technology is a technology that allows individual humans to take advantage of all
the information, knowledge, skills, abilities that other humans have accumulated over millennia
of cultural history.
So you could argue that language is kind of like the early cultural technology.
Language is the thing that humans have that lets us learn from other humans.
But over the course of human history, one of the really interesting things that's happened
that's led to massive qualitative changes in how we function is the development of better
and better, stronger and stronger, more powerful new cultural technologies.
So an obvious example is writing.
So writing allows you not only to learn from the post-menopausal grandmothers, who by the
way are the people who are really the agents of all this cultural work, not only from the
post-menopausal grandmothers.
I can talk about that a bit later on.
That's what I did.
That's true.
The post-menopausal grandmothers who are in your village, once you've got writing, then
you can learn from the grandmothers in very far-distant places, far-distant times in the past.
We know that writing had a really major effect on given intelligence.
If you think about print, that's an even more dramatic example of something that enables
you to get information very quickly from many different kinds of places.
And along with print go institutions like library indexes, ways that you can access
that information.
And of course, most recently, you have things like Internet Search and Wikipedia, which
are using the digital technology to enable you to much more effectively, much more quickly,
swiftly access all of this information from other people around you.
Now, that's a really different picture than the agent picture.
So we would be incoherent, as we used to say in philosophy, to ask, does the Berkeley
library know more than I do, right?
Well, in some sense, of course, they'd have much more information than I do.
But it's just silly.
It's just a mistake to ask whether the library knows more than I do.
The library's technology, which enables me to get information from lots of other people,
lots of other agents.
Wikipedia, by the way, I think is a particularly interesting example of this recently.
And this ability to have these kinds of cultural technologies arguably is itself the thing
that makes humans so intelligent.
So there are lots of people in cognitive science who have argued for what's called the cultural niche.
This is the idea that what makes us human is exactly this capacity to take information
from other people and use it to make progress.
So people sometimes talk about this as being like the cultural ratchet.
So a whole group of people go out of the environment.
They learn all sorts of things.
They pass that information on to the next generation.
That next generation takes off from that information, uses it to learn yet more.
And that process is the thing that has actually enabled cities and civilization and all the
things for better or for worse that humans are capable of.
So that's an idea about something very basic about human intelligence, is that we rely
on these cultural technologies.
And it's interesting that if you look at the...
I think that this is also helpful because when we're thinking about a lot of the pragmatic
questions about what the social impact of AI is going to be, how we should deal with it,
I think it's actually very helpful to think about these historical examples of other kinds
of cultural technologies and their impact on us.
Again, I think that's a much more helpful, insightful way of thinking about it than thinking
about it as, has the golem come to life and is it going to come and tell us all or are
we all going to disappear and just have the super intelligence instead.
And it's interesting that if you look at conversation about these technologies, people always point
out that they have good features and bad features.
So famously, South Cretace thought that writing was a really bad idea.
I won't go into this quote, but I love this quote.
And I think it's interesting that, especially like that last sentence about, they seem to
talk to you as though they were intelligent books do, but if you ask them anything, they
just go on telling you the same thing forever, which sounds so much like talking to chat
GPT, right?
It looks as if it's intelligent, but then it just keeps producing the same nonsense over
and over again, right?
So you can already see, and Socrates pointed out that if we relied too much on writing,
we would lose our capacity to memorize all of Homer, for example, which is indeed something
that very few people can do nowadays.
And also, and interestingly, that writing would be a great source of misinformation.
So Socrates pointed out if something was written in a book, you couldn't interrogate it the
same way that you could interrogate a person.
And that would mean that you would tend to think that things that were written down were
true even then, even when they were.
Those issues are issues that about how they're going to affect us, and also about misinformation
of the possibility of transmitting information that's really useful and accurate, but also
transmitting information that isn't useful or accurate.
That's an issue that comes up with all of these cultural technologies.
The example of print, I think, is a really interesting one.
In the late 18th century, there were enormous technological changes, which meant that although
print had been around since Gutenberg, suddenly everybody essentially could print.
Anybody with a little bit of capital, you could set up a print shop and you could produce pamphlets.
And this has lots of people, historians have argued that this had a transformative effect
on society, and many of the things that we think of as the Enlightenment, for example,
really depended on having this technology of printing available.
So on one side of this picture, that's actually Benjamin Franklin, who famously was
a printer's apprentice, and it was his capacity to print that arguably enabled things like
the American Revolution to take place.
If you read, I was recently reading this wonderful biography of Samuel Adams, and literally
Adams was in the print shop the night of the Boston Massacre, making up the Boston Massacre.
This thing happened, there were a bunch of protesters, there were a bunch of soldiers,
Adams went to the print shop like 10 hours later and wrote up the story about what was
going to be the Boston Massacre and distributed it with print.
On the other hand, even though we think about print as this way to underpin things like
the Enlightenment diffusion of knowledge, it also, like reading, had the capacity for
misinformation from the very beginning.
And I highly, this is by now an ancient classic book, Robert Darden's book of the literary
underground of the old regime, another nice Berkeley-issued example of how humanities
and science can speak to each other.
And what Darden actually did was go back and read not just, you know, Comedicent and Benjamin
Franklin, but everything that those printing presses produced.
And you will be amazed to hear that most of it was really terrible.
Most of it was, a lot of it was soft-core porn and a lot of it, well, the rest of it was libel.
So the litany cake, for example, was actually a meme.
It was actually a meme that was invented in the context of print.
Mary Antoinette never said it.
But it was obviously a meme that, like memes and misinformation nowadays,
can have really strong, and in this case, arguably very terrible, offense.
Now, one of the things that I'll just go through this quickly, but I think it's interesting,
is that you might say, well, okay, why is it if what's going on in these cultural technologies
is that we're unglomerating information from lots of different sources?
Why do we so often think of them as if they're agents?
So why don't we treat them as if they're agents?
Why do we say chat GPT said this?
And it's almost impossible when you're talking about these systems not to say,
it said this, it did this, it could do this, it can't do this other thing.
And I think it's interesting that if you look at past cultural technologies,
very often the form of information that a cultural technology uses is to have a fictional agent.
So it's hard to tell people very general things about only information that other people have
accumulated.
Often, it's much more effective to have a made up agent who's actually illustrating
the information that you want to tell.
And it's interesting that if you look even back in the place to saying,
look at a hundred out of our communities, one of the really important things that happens
is especially in the evening, people sit around and the post-menopausal grandmothers
who are always the agents that change, talk to the little kids, and some of the adults as well,
about what are the myths?
What are the stories?
What are the things that are important in our culture?
What are the things that you need to know?
What are the biggest things that you need to know?
And the way that you do that is through telling stories.
The way you do it is by having a fictional agent who can exemplify the things that you think are
really culturally important.
So that's actually something that we have quite characteristically done.
Another example is, and interestingly, an anthropologist of religion just has an interesting
paper about this in the context of LLOs.
Another example, if you think about gods, gods sort of serve the same function.
So even though gods don't actually exist, there are ways of passing on information
about what you think is important in the context of culture.
So don't say that I said that large-language models were gods, or maybe you could say
they're only gods that don't exist, right?
They're that kind of god, a god that doesn't exist.
And last thing that I loved as the daughter of a, as that I love as the daughter of English
professor is that, in fact, that 18th century print revolution that I talked about also
came with this new literary form, which was a novel.
And it turns out that the very first novels, like I said, this is a Samuel Richardson
Pamela, even though Pamela was a made-up person, she became incredibly popular.
People would line up in the streets to find out the next installment of what had happened to her.
And Senator Richardson was a printer.
The printing and these new fictional characters in novels who could tell you something about
what's going on around you happened in, in concert.
So what I want to argue is that that's the way that we should be thinking about,
we should be thinking about these large laws.
They're different.
They're not, it's all like they're just the same as print.
Each one of these new cultural technologies has different characteristics, different kinds
of powers.
I think they're arguably more powerful than print because they can generalize and they can,
they can agglomerate enormous, giant amounts of information, all the information there on the internet.
But I think the structure of how they work is the same.
And I think in that context, again, we can get some important historical,
we can get some important historical insight by thinking about how we manage the previous cultural
technologies.
So every time the cultural technology develops, you're faced with these problems about things
like misinformation.
And typically what's happened is that various kinds of norms, rules, regulations, laws,
principles develop in parallel that actually enable you to get the benefit rather than the
cost of a new cultural technology.
So you think about things like just norms of truth that you should tell the truth instead
of lying or things like editors, journalism schools, fact checkers, the developments of
print in the 18th century came with the new things called newspapers and editors, which
hadn't existed before.
So instead of just looking at all the pamphlets, you could say, okay, this printed paper is
the New York Times and therefore I'm going to give it a kind of authority that I wouldn't
give to just a random pamphlet that was on the street.
And I think that's the thing that we have to do with AI.
So the kid just sort of say, yeah, it'll be fine.
These aren't like evil goblins.
So everything's okay.
What we need to do is to have the same kind of mechanisms that we have for other kinds
of cultural technologies.
Okay, so that's the first part of the talk.
Making this argument about AI as these kind of cultural technologies that are accumulating
lots and lots and lots of information, all the information will enter that.
Why is that enough?
Why isn't that enough to be intelligent?
If, as I've argued, so much of human intelligence comes from this capacity to
learn from other people, to extract information from other people, why is it that a system that
can do that really well and that's better than any individual human cat?
Why would that be hard?
And to answer that question, I want to turn for thinking about AI systems.
As technologies, just thinking about children.
Why children in particular?
Well, famously in the Turing, the famous Turing paper where he talks about the Turing test,
there's this wonderful segue that nobody ever notices or didn't notice until recently,
which is he starts out saying, okay, you want to know if the system is intelligent or what
you're doing, you do the invitation test.
But then he suddenly shifts and said, you know, maybe that's not the right test.
If we really wanted to know whether our system is intelligent, we'd have to know whether it
could learn from its experience the way that a human child, for example, learns.
And he very explicitly says, instead of producing a program that can simulate the adult,
why not try to simulate the child?
And in a sense, what something like large models are doing is accumulating all the
information from all those adult minds that are out there, right?
That's the great power of those systems.
But what children, although children do do that, children do something rather different.
What children do is actually go out into the world and learn about the world independently
of all the information that they're getting from other people.
And over the past several years, more and more people within AI are trying to use
children who are the best learners to grow up as a model for how we could design AI systems
that could learn in this way as well.
The title of the paper that I mentioned is Transmission Versus Truths.
And what large models and the typical kind of AI models that trade on large amounts of data
can do is transmit information very well.
What they're not good at doing is going out into the world and finally the truth,
which is the thing that kids are very good at doing.
So from that perspective, for example, when people talk about the problem of something
like chatGPT hallucinating, which is something that everyone around has seen,
that's not that they're hallucinating.
It's that they just don't care.
There's nothing in their objective function that makes a difference for truth and falsity.
So if you just imagine, you know what, as Socrates said, books can have,
the books in the library don't tell you whether what's in them is true or not.
They're just there to transmit information.
But as we'll see, kids cared tremendously about what's true.
And arguably, kids are, that's their greatest motivation is going out in the world
and trying to figure out how the world works.
And we know a bit from Develop Little Cognitive Science about how they do that.
And in this DARPA machine common sense program that we've had at Berkeley with great AI people
like Jitendra Malik, who you've heard in this series before, Alexei Efros,
we figured that I discovered that working with DARPA, the most important thing is to get good
acronyms. So then you've got DARPA worked out.
So our acronym for what we're doing is MESS, which is appropriate for children,
which is to try to design these model building exploratory social learning systems.
So how did children learn so much? And how do they learn the truth about the world?
They, first of all, don't just get statistics, although we have a lot of evidence that children
are amazingly good at statistical inference, much better than we ever would have thought before.
But they don't just aggregate statistics the way that something like a large language model does.
Instead, they actually pull out abstract calls and models,
intuitive theories from that statistical evidence.
My first book is called The Scientist in the Crib, and I and lots of other cognitive scientists
have used this metaphor. They're not just looking at statistics and making predictions,
they're pulling out causal structure and then using that causal structure to understand how the
world works. And that's one of the most important ways that humans in science, but also in everyday
life, figure out the truth about the world. The other thing that makes what children are doing
very different from what typical AI systems that particularly large models do is that they're
active lures. They actually go out into the world and actively try to get the kind of data that
will be relevant to the problems that they're trying to solve. And there's more and more evidence,
I'll give you some example of this in a bit, that they're super sophisticated in doing this.
When scientists do this, of course, we call it having an experimental research program
and when two-year-olds do it, we call it getting into everything. But it turns out that if you
actually study the getting into everything, that what's going on is something that's much
more like a scientific research program than you might imagine. So children aren't just passively
absorbing data from the program around them, for example, they're actually actively going out and
trying to make discoveries, find out about things going on in the external world, not just say in
the internet problem. And they do also have these capacities for social learning, as I mentioned
before, for extracting information from the other people around them. And there's some lovely studies
of this. Sometimes what they do is just sort of mindlessly try to agglomerate that information.
But a lot of times what happens is that they're balancing that, as we'll see in a minute,
against this drive for the truth. So as opposed to just accepting what other people tell them,
they try to balance that against other kinds of information they've got, including the information
they've got through their own experimentation and exploratory play. Okay, let's just say a bit about
how does that picture of what children are doing, how does that interact with this really
remarkable set of studies we've had over the last year, so about how powerful these large models are.
What can large models tell us about how children are learning and what can children tell us about
how large models work? Well, I think one way of thinking about it is that we can figure out what
kinds of information are available through transmission, through just looking at the
information that other people give you, and what kinds of information do you need to discover for
yourself. And I think syntax is a really good example where typically people in linguistics,
for example, have thought you can't get to syntax just by looking at the statistics of
linguistic input, and it turns out that actually, no, you can't. This is something that the systems
genuinely are incredibly good at, and it looks as if there's enough information in the data
to extract the kind of structure that you need to be able to produce grammatical
sentences, which is not something that I think we would have known before.
Another thing to say, which makes this problem a little more difficult, in some ways from the
cognitive science perspective, the old-fashioned design like last year, or three months ago,
or yesterday, vanilla large language models, the ones that just work by predicting what the next
word or token is going to be, are more useful than some of these more recent ones. So what's
happened more recently is that rather than just extracting the statistics of the information,
you've added things like reinforcement learning from human feedback, which means that now we have
humans, and I highly recommend a piece in New York Magazine recently that was describing this,
you know, giant factories worst of people in places like Nigeria, who are just sitting there
and looking at the output of these systems and saying, yeah, that's good, or that's not good,
right? That's reinforcement learning through human feedback. And we have things like
prompt engineering and fine tuning that are also designed to do this. Now, what that means is it
becomes very hard to figure out what's actually going on under the hood in the system, whereas if
you just have a kind of classic LLM, you can say, okay, this is information that's there in the
language, or this is information that's there and imaged, it's a large image model. Once you get
reinforcement learning from human feedback, it's much harder to know what exactly is the information
that this system is encoding. But from a conceptual perspective, from this perspective,
it isn't an agent or a cultural technology, it's still true that what something like RLHF is doing
is not enabling a system to go out to find out about the truth, all it's doing is giving it yet
another way of getting information from other people, somewhat less obvious and transparent way
than the kind of classic LLM systems are. And another thing that large models can teach us,
which I think is really interesting and sort of under has been further studied, under initiated,
is what kinds of cognitive capacities are facilitated by a new technology? So what sort of
things can we do when we have print or when we have writing that we couldn't do before? And you
can probably already think about the fact that we can remember things and we can set things down and
go back to them. There's all sorts of cognitive capacities we have from those technologies.
And a really interesting empirical question is, mathematics is another nice example where we
couldn't have math until we had our notation system that would let us write down equations.
So I think a really interesting question is, are there cognitive capacities that will be
enabled by these new cultural technologies that would be different from the cognitive capacities
that were available to us in the past? But again, that's a very different way of structuring the
problem than saying, are they smarter than us or are they not smarter than us? Do they,
whoever they are, have those cognitive capacities? Okay, but I also think that children in
developmental cognitive science can teach us a lot of things about how to think about
large language models and understand them. And one of the first things that I think is
really important, I will jump up and down about this, is that there's no such thing as
general intelligence. There's little bits of stuff about IQ in the psychometric literature,
but from a cover science perspective, there just isn't anything that's general intelligence,
artificial or natural. So the whole sort of this kind of intuitive theory of intelligence, which is,
it's like a fluid or it's like a, you know, a force that's out there and you have the force
of intelligence and you don't have the force of intelligence. You're one of those guys,
guys being a relevant category here who's got a whole lot of the force or else here,
a guy who doesn't have so much of the force. And that's just not the way that cognitive
science that cognitive scientists think about intelligence. Instead, what you've got are a
whole path, a whole lot of different cognitive capacities, and very characteristically,
their intention with one another. So being really good at exploration, for instance, might
can make you worse at exploitation and vice versa. Being really good at focused attention
makes you worse at extracting general information. So what we need to do is to actually work out in
detail what are the kinds of underlying processes that a particular system, artificial or natural,
is using in a particular context and try to say something about that rather than saying,
do they have sparks of intelligence or do they not have sparks of intelligence?
And from a methodological point of view as well, children can tell us something about
how to understand the cognitive capacities of an alien system. I highly recommend this paper by
Mike Frank. And when one of the great things about being a developmental cognitive scientist is,
we can't just assume things about the cognitive capacities of the very intelligent creatures
that we're studying. We actually have to go out and do very systematic kinds of experimentation
to try and figure that out. And I think importantly, we have to do something that I think of as
anti-flowing tune. So when we're doing development, when we're doing a developmental experiment,
what we have to do is make sure that we don't have any prompts for the children.
Because what we want to do is figure out what's the underlying cognitive capacities,
not are the children capable of using our prompts to give us the answer that we want them to give us.
So it's kind of the opposite of what happens in a lot of LLMs. We really want to understand
cognitive capacities. We need to have control conditions. We need to do these very careful
experiments. And I think development can provide a lot of examples of how to do this.
And if we want to say, is this system intelligent or can the system do acts? Can the system, Dean
Theory of Mind, for example, to take one that is dear to my heart, we need to actually go out and
like developmental psychologists, be it spend 20 years doing experiments to figure out whether
and what children understand about the mind, my life would have been much easier if I could have
sat down with a three-year-old in 1986 and talk to them for a couple of days and then decided
whether they had their... Okay. So far I've been talking about some of these capacities that children
have. And I mentioned before that these cognitive capacities for faithful cultural transmission
are in tension with cognitive capacities that enable new discoveries about these
things like exploration or experimentation or causal discovery or adoption.
And what we've been doing is using studies of children and then comparing them to studies of
artificial systems to work out how is it that that kind of innovation works? What is it that
children are doing that enables them to innovate, that enables them to learn new things about the
world? And we know that children are learning these intuitive causal models and they seem to do
this through exploration. And the research program that we've had is to put children and agents in
the same online environments and that way we can say sort of unrestricted online environments,
let the children and the agents explore freely in those environments and then see what kinds of
discoveries can the children or the agents make about how those... about how those environments
work. And one of the first examples of this, this is an experiment with a bunch of people here
in computer science. What we were trying to do was see if children could figure out causal structure.
As I mentioned, causal structure is really important and in our developmental work for a long time
we've used this little box, the Blicket Detector. And the Blicket Detector is a little box that
lights up and plays music when you put some things on it and not others. And with this very
simple device you can actually ask quite sophisticated questions about the causal structure of this
device. How does it work? Which ones are Blickets? What will happen? Can you make the device go?
So what we've done in the past is to show children different patterns of evidence about how the system
works, again as if they were little scientists, and then we've asked them to draw the right causal
conclusion about how the system works. In this experiment, because we now have the online experiment,
the online environment, we did something different. What we did was to actually let the children
explore the... COVID, we invented the virtual Blicket Detector. So now you can put things on the
machine in various combinations and permutations and sometimes the machine lights up and sometimes
it doesn't. And here's a child actually playing. This is a four-year-old. And perhaps the most
striking thing about these experiments is that current day four-year-olds are absolutely perfectly
happy to figure out and explore in this virtual online environment. I'm not sure that would have
been true 20 years ago when we first invented the Blicket Detector. And then even more remarkably,
what we discovered, and I won't go into all the details of this experiment, what we discovered
was that the children were very good at doing exactly the right things to figure out how the
system works. So most of the children, just in the space of about 20 trials, 20 things that they
did, 20 choices about what to put on and what not to put on, figured out what the underlying
causal structure was. So they were really good at doing this. They figured out which ones were
Blickets. They figured out what they had to do to make the machine go. And they were much better
at this than any of the large language models that were tested. Again, maybe you could do a bunch of
prompt engineering to persuade the model to be able to solve this particular task. But just based
on their general knowledge, they did incredibly badly. They were not good at this test. And you
can kind of see why, right? Even though they gave us back references to our Blicket Detector papers
as a way of answering what was going on in this task. Of course, this was a new machine that didn't
work like any of the machines that we had in our previous Blicket Detector papers. So it can tell
you, okay, here's this Gopnik Blicket Detector. Here's how it works from reading the papers. What
it couldn't do was do a bunch of experiments and use it to find out that this detector works really
differently from any detector that you've seen before. And in a new series of experiments that
we're doing that's also in that prospectus paper, we've been doing a similar kind of project
with toys. So in this project, what we wanted to do was try to look at another way that you could
use causal information to try to understand novel possibilities in the world. And this involved
tasks that involve innovating tool innovation, which again, it's one of those things that's
incredibly powerful, important human capacity. And we were interested in this from the perspective
of development as well as from AI. We wanted to know if children would be able to do that,
just as we were when we were looking at the children on the Blicket Detector.
So what we did was to give children examples where there was two objects that were highly
associated with one another. So for example, the scotch tape is highly associated with the
scissors, much more associated with the scissors than with the band-aids. And if you ask, if you
could ask people what goes with the scotch tape, that's what children do. But you then present
a particular causal problem. Here's an example like tearing up paper, and then you can ask which
would be the right thing to solve this problem. Would you be better off solving this problem
with the scissors, or would you be better off solving this problem with a band-aid, even though
you haven't already associated the band-aid with the scissors? And again, I'm sure that with
RLI Chaff and prompt engineering, you could get systems to do this. But when we first did this
with GPT-3, which was just looking at the associations in the language, what you could
see was that the children were much better at it than GPT-3, and GPT-3 was actually not any better
than Chaff. And again, if you think about it, right, if you're just looking at associations
in the language, scissors and scotch tape are going to be much more closely associated. If you
want to know what word should you produce next, staff you've heard scotch tape, scissors is a
much likelier outcome than band-aids. But if you're actually trying to figure out the causal
properties of band-aids and scissors and paper, then you're going to have to think about that
information in a different way. And in the most recent experiment that we've done, we had 47
different scenarios, each with a different set of objects. And within each of these sets, we have
a reference object like a tape, and there's a superficially related object or a functional
object that's not superficially related or totally unrelated object. And then people have to rate both
adults and children and LLMs have to rate them zero to five, how useful each of these
objects will be in trying to solve this task. So I ripped up a paper, I have to decide what would
be most useful, glue, which is obvious sticky thing, a bandage, scissors or cotton balls.
And what we discovered was that when we simply asked the association a question, so we said,
which goes with which? Does the scissors, does the scissors go more with the scotch tape or do
the cotton balls or the band-aids go with the scotch tape? The models were as good as people,
or at least some of them, many of the models were as good as people about children and adults.
They seemed to be able to pick out these, but when it came to generating the novel functional
capacities, even GPT-4, which has all this extra RLHF, was not as good as children or
adults at solving these tasks. It was interesting that children, even four-year-olds, were actually
quite good at solving these tasks, although not quite as good as adults. Okay.
Okay. So so far, what I've been suggesting is the kind of capacities that you get, especially
from large models, involve this kind of transmission of information rather than exploration and turns.
That's been a kind of critical, this has been a critical enterprise and we can give
lots of examples, but how about the positive part of the project? The positive part of the project
is, are there other kinds of techniques that we could use in artificial intelligence or ways
that we could design new artificial intelligences that would solve some of these problems in the
way that children do and would, therefore, we hope, have some of the capacities that children
and I'm just going to just briefly gesture at two projects that we're doing right now and bear,
and both of them, in fact, all the projects we're doing now use this methodology that I think is
very exciting of having online environments where both humans and children and agents get to freely
explore those environments rather than dictating here's what the solutions are or here's what the
choices are providing a supervised environment. So we're trying to set up unsupervised environments
that can be explored by either agents or children. And let me just mention two techniques we're trying
to use to try to get the agents to solve these problems in a way that's more like children.
One is with Ucheas, right here, what we're trying to do is to get intrinsically motivated
reinforcement learning. So classic reinforcement learning involves getting a particular kind of
reward and that turns out to be really powerful. You can use those kinds of systems to do things
like learn how to play go. But what children are doing when they're exploring is not being,
it's also doing things that actually leave them to have less reward in any kind of straightforward
way and we've done a bunch of empirical studies that have shown this. What children do is kind of
play, they go around, they try things, sometimes they work, sometimes they don't, if they get too
good at doing something they get bored or they try something else, their motivation seems to be this
kind of intrinsic motivation. They're doing things for the sake of learning rather than doing
things for the sake of being rewarded in the long run. And what we've been doing is trying to take
formal ideas like information gain, one that I'm very excited about is empowerment,
and turn those into rewards. So make the reward be the fact that you're more competent than you
were before, or make the reward be the fact that you know something that you didn't know before.
It's a way of time to put things like curiosity and exploration into artificial systems. So that's
one line of research, that's one set of techniques that we've been doing.
We've also been doing this in the context of play, for example, that we have recordings of
children playing in these online environments, and then we can use their play to try to figure out
what an agent could do in that environment that would enable them to master the environment.
And the second thing that we've been doing is taking these ideas about, particularly
emphasizing these ideas about causality. This is work with Roger E. T. at DeepMind and also with
Blake Richards at Miele. We've been looking at causal influence. And trying to see if we could
get a kind of automated curriculum, if we could design a system that could have what's called
an automated curriculum, what does that mean? What that means is another thing that children are
extremely good at doing is figuring out what they need to do to be able to learn something
that in the long run is going to enable them to learn something else. So instead, the way that
these experiments have gone is we are using the PropGen environment, which is kind of a video game
environment. We start out by showing a really hard level that the kids can't do. And then we say,
could you decide how to play a game that will help you to be able to play this other game
that you'll eventually be rewarded from? And again, if you know children, children are actually
turned out to be quite good at saying, okay, first I have to do the simpler thing. And during the
simpler thing, it's going to give me the skills that I'll need to be able to do the more complicated
thing. And the same thing happens with agents. So if you give agents these high dimensional,
really difficult tasks, it takes them forever to be able to solve it. But if you could get them
to have a curriculum where they'd say, I can't solve this task yet, let me try getting skills
in the simpler context and then applying them to the task, that would be another example of something
that that children are doing that agents that should enable agents. And again, those are some of
the kinds of things that we're seeing. So overarching overarching the overarching point,
I don't want to say and I don't think it's true that artificial systems are never going to be able
to do the kinds of things that humans are doing or that human children are doing. But I think the
very progress that we've made gives us a sense of the dimensions, the landscape of the kinds of
things that are going to turn out to be much easier to do, or at least the kinds of things that we're
going to be able to do with sufficient data, sufficient compute, and the kinds of things that
are still going to be very challenging, even with very large amounts of data, large amounts of
compute, the kinds of things that we're going to need other kinds of capacities to solve. And in
particular, the kinds of things that are going to demand these truths that are going to demand
going out into the real world, getting new data, changing, modifying what you're doing. And those
are exactly the kinds of things that the incredibly brilliant, but strange little agents all around
us that we don't pay much attention to are doing every day. And the interesting challenge would be
to try and figure out what computations are those brains in those beautiful little fuzzy heads doing
that we could actually use in an orderly system. So let me stop there.
Yeah, that's a great question. The empowerment idea is a technical idea in reinforcement learning
that I think is very, very interesting. The idea is that you get rewarded for doing something that
has an outcome. So the idea is it doesn't really matter what the outcome is, or whether it's going
to be positive, or even whether it's going to be negative, what you're getting rewarded for is do
something that has an effect on the world. And I think there's a lot of evidence. We've shown this
in things that we've done that children, the children, and again, anyone who has a two-year-old,
like my, I now, by the way, have five grandchildren, including a somewhat terrible two-year-old.
And anyone who has a two-year-old will recognize that just making things happen is the thing that
makes them more excited than anything else. And I think that's interesting because from the perspective
of causal inference, when we say you're going out to the world and discovering causal structure,
what you're doing is discovering the things that you can integrate on to bring about
so being very kind of interested in doing that, our whole engineering program, right? That's
what an engineering school is all about, is about how can you make things happen. And I think it's,
we just don't know exactly how and when children are doing that, or whether some children are doing
it or others. Here's an idea that I think might also be relevant in AI. We're just starting to test
this, which is that the reason why children can do that kind of empowered exploration is because
they know that they have caregiving. So again, the post-menopausal grandmothers are the key to all
of this. If you know that grandma is there and you're going to be taken care of and nothing really
terrible is going to happen to you, then you can go out and you can try things, even if you think
the outcomes might actually be bad outcomes, but you know that you're in full. So I think the human
strategy is we have these incredibly exploratory children, and then we have these very nurturing
adults. And you need both sides of that to solve those.
Hi, such an interesting talk. I'm curious on the element of how children learn to play. I'm
thinking of imaginative play or transformative play where they're constantly reinventing the rules
and how that could be taught or related to these large systems or language models.
Yeah, I think that's a great example. And we have a whole project that's called a computational
counterplay. So one of the things we're looking at is this kind of play where we're just kind of
mastering new ideas, but we've looked at things like pretend play. And pretend play, again,
has these very interesting connections to causal inference, because what we've discovered is that
when children are pretending, what they're doing is generating counterfactual. And generating
counterfactuals is one of the things that you can do if you have a causal theory,
but it's one of the things that's much harder for something like an LLM to do. So generating
counterfactual and seeing something new that isn't actually something that you've already
observed in the world before. If the world were different, what would happen? And that's something
that's very hard to do. If all you've got is data from what has actually already happened in the
world, it's something that you do all the time when you're doing pretend play. And it's a really
interesting question about when we've studied this, when you look at children's pretend play,
it's crazy, right? It's not true, but it kind of makes sense. It's not random. It's not just,
it's not just that they say anything. They say things that wouldn't make sense if you were in
that other alternative environment, and that's one of the cues to human intelligence.
So an interesting question would be, you thought would be, if you could get a system that could
generate counterfactuals that you can pretend in a systematic way, you might predict that as with
the children, that would enable you to generate counterfactuals in a real-life situation where
you need to use counterfactuals to solve the problem. Sir.
So this is fascinating as a parent of three sons, I think a lot, but they were two years old,
20 years ago, so. But I was curious, is this the right way for the grandchildren?
Is there equivalence of neural divergence in this kind of thinking of cognitive systems?
Well, I'm not quite sure what you mean by neurodivergence, but one thing that I think is
really interesting, and I had a slide in earlier with two many slides, but if you think about this
kind of trade-off of idea, the kind of brain that you need to be able to do this kind of
wide-ranging exploration is quite different from the kind of brain that you need to be able to say
act effectively in the world. And if you actually look at the neuroscience of development, what you
see is that young kids up to about five have brains that are functioning in very different ways
from adults, and you actually sort of see this curve where up to about five, you get many new
synapses being formed, and then after five, you start this pruning process where so the ones that
are there get to be stronger, but the ones that aren't there disappear. So you have this young
brain that's very flexible, very plastic, very good at working, not very good at putting on its
jacket and getting to preschool in the morning. And then you have this old brain that's very good
at organizing a symposium series, not very good at learning a new language, for example.
And we can see that even in the neuroscience about the differences between children and adults.
So you add things that are, one of my slogans is things that are bugs from the
exploit perspective or features from the explore perspective and vice versa.
So a lot of things that children do that have traditionally been seen as being signs of lack
of intelligence, like they're kind of random and noisy and all over the place, actually are
signs of intelligence from the explore perspective. And a lot of things that they're bad at doing,
like focusing on one thing at a time and doing long-term planning are things that are great
from the perspective of actually of the exploit perspective getting started, not so good truths.
Thank you so much for this very interesting presentation. My question is this, if you look at,
if we look at today's AI as a baby and compare that with a small child,
my question is this, the computation power between machine and human, there's a big gap.
And because of the gap, could artificial intelligence develop and find a different
way or path to develop intelligence compared to human?
Yeah, I mean, I think, again, what you mean by intelligence is solving particular kinds of
problems. That's what you mean. So the question is what kind of problem is it that you're trying
to solve and what kinds of capacities do you need? And of course, as people have pointed out,
if you want to do something like add very large amounts of numbers, artificial intelligence already
is, you know, surpassing us by many, many, many orders of magnitude. There's no question.
Your definition of intelligence, as it might have been in the olden days, was doing things,
or doing things like playing chess. Then it's clear that these systems, you could design systems
that are extremely good at doing this. If you're interested in some of the things that are
characteristically things that children, for instance, are solving, like being in a very
high dimensional space in the real world and figuring out what the structure of that space is
in a relatively short time with relatively few data samples, that's something that
these systems are not particularly good at. And I don't think there's any reason to believe that
simply adding more compute power is going to be the thing that will help enable them to solve
those problems. In fact, I think there's a reason to believe that it's probably not going to have.
What you need to do is have systems that now have, as it were, an objective function, which is about
going out into the real world and solving problems.
You said something early on that really caught my attention that these generative models are
not hallucinating, they just don't care, and that kids care about truth and how the world works.
So kind of my question is, how do we make it care about truth? Is this a collaborative tool
that's going to eventually be a two-way street rather than us fine-tuning? Is the AI going to
ask us, do I have this right? How could I improve my, because kids ask a lot of questions.
Yeah, they do. In fact, there's a beautiful study that I like which showed that your average
four-year-old asks 20 questions a minute, and I put that in my book and there was a lot of
man explaining about, no, you must have got that wrong, that couldn't be true, but that's
actually what the statistics, that's actually what the statistics look like. So what you want,
now, I think the important point is you could imagine, and we're going to try and design systems
that could do things like ask questions or get data from the world, LLMs just don't do that.
That's not what hair training is, it's ever going to be designed to do, right? I mean,
the whole point about LLMs is the things they do is they predict what the next token is going to be,
and they respond to reinforcement learning from human feedback, they respond to your saying to them,
yeah, okay, that's good or that's not good. In a sense, that's like answering a question.
What they don't do is say, why is there a clock way up on top of the campanile, which is the kind
of thing that children will do, let alone say this is a nice real example, the little boy was
walking and said, why is the clock way up there? And then said, they must have put it up there so
the children wouldn't break it. So there's a beautiful example of a really good question
and a really good answer, but not an answer that you would have ever gotten from either looking
at the statistics of what's out there in text or from asking someone to give you human feedback
about whether that was a good answer. In fact, it's funny, like what would you do if a system said,
oh, we must have done it because of that, would you say, no, that's the wrong answer or would you
say yes, that's the right answer? And it's exactly that space of the things that are not the right
answer, but are intelligent that children are incredibly good at and I don't think,
at least as currently constituted, things like large models are going to be able to do.
Yeah, no, I mean, I think that's exactly right. So for problems like chess, where there's a very
clearly defined objective, then you can use some of these things like deep learning,
deep reinforcement learning very effectively to get to that objective. An example that I like
to give is what they would not be good at doing was playing Addy chess. So Addy chess is Adicus,
who's one of my grandchildren, and his big brother, of course, plays real chess. But what Addy does
is take the chess board, and then he'll take the pieces and he'll put them in the waste basket,
and then he takes them out of the waste basket and puts them back, and then he puts the black
pieces in the white pieces in different kinds of orders, and then he gets bored with that,
and then he stacks them. And Addy is really playing chess, right? Addy chess is really what
the human game is about. Of course, he also drives his big brother completely insane.
And that's what you mean. That's the kind of thing that you would mean if you were playing
chess, and that's the sort of thing that is not going to be something that you'll have, even
though it's really amazing that you can actually manage to make the right moves in chess as a result
of something like deep reinforcement learning. I mean, I think it's kind of interesting because
you also almost wonder, like, what was the point of chess in the first place?
Probably, if you could always get it right, it wouldn't be a good game to play. There's other
kinds of intrinsic motivations in terms of being empowered, getting to be better at solving the
game. It's kind of not the same game anymore. Really wonderful talk, especially I think grounding
us in some historical perspectives is really useful. In terms of Socrates, in terms of
post-menopausal grandmothers, it's all very, very useful, particularly because I think a lot of the
narratives that we're telling are kind of monotonous, unimaginative, and completely wrong. So a much
appreciated perspective in the beginning. My question is around the relationship between the
very kind of active exploratory hypothesis-generating behavior that you've been describing and some
more kind of passive learning systems. There's years where kids just kind of like, you know,
their physiological systems kind of stabilize, their acuity develops. There are a lot of
learning systems that are not necessarily, question mark, actively generating and evaluating
hypotheses, but I'm curious how you might think of the relationship between these more,
what might be thought of as passive learning systems and these active learning systems?
Yeah, that's a great question. I think syntax is a really lovely example of this because
one of the things that we learned in development is that children, babies, like seven-month-olds,
are very good at predicting the next token in a sequence from a token, from a series, even when
there's no content, even when there's no semantics, seven-month-olds, right? They're incredibly good
at just picking out statistical patterns from data and there's a whole field of developmental
psychology about this, even when they're not actively exploring, even when they're not actively
trying to solve problems. And for years, what linguists were saying was, yeah, I know, well,
seven-month-olds are really good at doing that. Who knows why seven-month-olds are so good at
doing that? I couldn't really have anything to do with their learning language. And I think now
what you'd say is, aha, yeah, you want to know why those seven-month-olds are so good at doing
this? This is how they do syntax. So I think it is definitely true that kids are extracting
enormous amounts of statistical information in this kind of passive way. But then the interesting
thing is that they're putting it to use. And again, I think the example of thinking of a
child as a scientist is a very helpful one. We use statistics and we couldn't do a lot of our
science unless we had the capacity to take a whole bunch of data and pick out what the
statistical generalizations in the data is. That's an incredibly useful thing to do,
but it's just the first step in enabling us. And sometimes it's not just the first step,
sometimes you might just, as in, I think a lot of medicine, for example, works that way,
where they have no idea about causal mechanisms. You just say, okay, this is more likely to produce
a good result of something else. But most of the time you want to extract underlying causal
mechanisms for that. But I do think both those, I think it's fascinating and interesting that
both those things are present in kids. The important point is it's not just that the kids have the
passive capacities. They're interacting.

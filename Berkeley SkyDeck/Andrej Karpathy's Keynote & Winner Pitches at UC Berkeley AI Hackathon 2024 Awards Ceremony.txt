Welcome to the Clothing Ceremony of UC Berkeley's AI Hackathon.
I want to call on stage the awesome,
incredible Executive Director of Skydeck, Caroline Winnat.
Thank you, ready?
Now, you keep that.
Hi, everybody. How are you doing?
Awesome. You ready to hear who won the Hackathon?
Yes, you are. How many hackers here?
How many in the audience?
Oh, nice. Very good.
All right. We're going to get started because I think you want to hear Andre.
Yes, you want to hear Andre.
Yes, you want to hear Andre.
All right. Let's quick run through.
You want to hear some cool facts about what has been happening.
This is what we're going to do today.
We're going to get to our pitches soon.
This is some pictures.
All you hackers, did we have fun?
Did we have a good time?
I had an absolute blast.
And yes, there were llamas for sure.
I was there most of the time.
I was not there at 3 a.m.
But I was so impressed with all of you.
You hacked your hearts out and I'm so proud of all of you,
whether on stage or in the audience, you're all completely awesome.
All right.
How many people they took to make this happen?
This giant number, 371,
UC Berkeley Skydeck, which I represent,
and Calhacks Educational Program and Student Organization.
So I think we did a pretty decent job of getting this all together.
This is how it breaks down.
Hackathon at Berkeley Directors, Skydeck staff, sponsors.
We're going to give some love to sponsors.
As I mentioned, we're an educational program.
Calhacks is a student organization.
This is all because of the sponsors.
So we're going to give them a ton of love when they come up on stage.
You with me?
Awesome.
Okay, 140 judges, 100 plus volunteers,
and 80 mentors hanging out helping everybody.
Let me tell you a bit about Skydeck.
Who hasn't heard of Skydeck?
Anybody?
A couple of you.
Skydeck, UC Berkeley's flagship accelerator.
We host 250 startups a year.
Our accelerator track gets $200,000 in investment
from our dedicated venture fund.
Pretty cool.
Let me tell you about Berkeley Skydeck fund,
our dedicated venture fund.
Investing in about 40 startups a year.
That's a lot of startups for an adventure fund, by the way.
The 200K investment.
And who wants to apply to Skydeck?
July 16, I want to see all of your startup applications coming in.
That's in a month.
And hackathons at Berkeley are an amazing student organization.
Truly extraordinary people who helped put this on this event.
This is, of course, what they do.
They do hackathons.
They've been doing it for 10 years.
They do about 2,500 students a year.
And, of course, they reach a ton of universities.
How many people here not from Cal?
Hacking not from Cal?
Fantastic.
Welcome.
Berkeley is a place where we bring great talent in.
Y'all are great talent.
We brought you here.
That's what we do.
That's what Berkeley Hackathons does.
We come to their 11th big hackathon in San Francisco in October.
Check them out on social media.
Get on that LinkedIn, all of that, okay?
Who's coming to San Francisco?
Y'all coming?
Yes, okay, fantastic.
All right, thank you to our partners.
All of you who brought your hackers here,
including our friends down to the South Bay.
Thank you for joining us and all the other great universities.
Fantastic.
Really happy to have you.
You want to hear Andre?
Do you want to hear Andre?
Yes!
Please give a huge round of applause for our keynote speaker,
founding member of OpenAI.
I need the applause.
Come on, keep going.
Andre, come on in.
Carpathian.
Yes, big applause.
Thank you.
Hi, everyone.
So thank you for inviting me.
It's really a great pleasure to be here.
I love, love, love hackathons.
I think there's a huge amount of energy,
a huge amount of creativity, young people trying to do cool things,
learning together, creating.
It's just like my favorite place to be,
and I've had my fair share of hackathons.
So really a great pleasure to be here and talk to you today.
So one thing is, this is bigger than I expected when they invited me.
So this is really large here.
I kind of feel like, actually, the scale of the hackathon is quite large.
And I guess one thing I wanted to start with is that,
just in case you're wondering, this is not normal for AI.
I've been in AI for about 15 years, so I can say that with confidence.
And it's kind of just grown a lot.
So for me, AI is a couple hundred academics getting together
in a workshop, of a conference, and talking together
about some esoteric details of some math.
And so this is what I'm used to.
This is when I entered AI about 15 years ago.
You're working with, say when you're training neural networks,
you're working with these tiny digits from MNIST.
You're training a restricted Boltzmann machine.
You're using contrastive divergence to train your network.
And then you're scrutinizing these on your first layer
to make sure that the network trained correctly.
And I know none of that makes any sense,
because it's been so long ago.
But it was a different vibe back then, and it was not as crazy.
I think things have really gotten out of proportion to some extent,
but it is really beautiful to see the energy.
And today, 15 years later, it looks a lot more like this.
So this is, I guess, where AI is today.
And that's also why this event is the largest I expect.
So yeah, NVIDIA, the manufacturer of GPUs,
which is used for all the heavy lifting for our neural networks,
is now the most valuable company in the United States
and has taken over.
And this is the day that we live in today,
and why we have so many hackathons like this and so on,
which I think is quite amazing, but definitely unprecedented.
And this is a very unique point in time
that many of you maybe are entering the AI field right now.
And this is not normal.
It's super interesting, super unique.
There's a ton happening.
Now, I think fundamentally the reason behind that
is that I think the nature of computation basically is changing.
And we kind of have like a new computing paradigm
that we're entering into.
And this is very rare.
I kind of almost feel like it's the 1980s
of computing all over again.
And instead of having a central processing unit
that works on instructions over bytes,
we have these large language models,
which are kind of like the central processing unit
working on tokens, which are a little string pieces instead.
And then in addition to that,
we have a contact window of tokens instead of a ram of bytes.
And we have a coolant of disk and everything else.
So it's a bit like a computer, and this is the orchestrator.
And that's why I call this like the large language model, LMOS.
And I've sort of like tweeted about this
in some more detail before.
And so I see this as a new computer
that we're all learning how to program
and what it's good at, what it's not as good at,
how to incorporate it into products,
and really how to squeeze the most out of it.
So that I think is quite exciting.
And I think maybe many of you have seen the GPT-40 demo
that came out from OpenAI two, three weeks ago
or something like that.
And you're really starting to get a sense that this is a thing
that you can actually talk to.
And it responds back in your natural interface of audio,
and it sees, and hears, and can paint,
and can do all these things.
I think potentially many of you have seen this movie.
If you haven't, I would definitely watch it.
It's extremely inspirational for us today, movie Her.
And actually kind of presently in this movie,
when this main character here talks to the AI,
that AI is called an OS, an operating system.
So I think that's very present from that movie.
And it's a beautiful movie, and I encourage you to watch it.
Now, the thing is that in this movie,
I think the focus is very much on the emotional intelligence
kind of aspects of these models.
But these models, in practice, in our society,
will probably be doing a ton of problem solving
in the digital space.
And so it's not just going to be a single digital entity
that kind of in some weird way resembles a human almost,
in that you can talk to it, but it's not quite a human,
of course.
But it's not just a single digital entity.
Maybe there's many of these digital entities.
And maybe we can give them tasks,
and they can talk to each other and collaborate,
and they have fake slack threads.
And they're just doing a ton of work in the digital space.
And they're automating a ton of digital infrastructure.
Not just a digital infrastructure,
but maybe physical infrastructure as well.
And this is kind of an earlier stages, I would say,
and will probably happen slightly lagging behind
a lot of digital innovations,
because it's so much easier to work with bits than atoms.
But this is another movie that I would definitely point you to
as one of my favorites.
It is not very well known at all.
It's called iRobot, and it's from 2004.
Will Smith, amazing movie.
And it kind of explores this future with human robots
doing a lot of tasks in society.
And kind of spoiler alert, it doesn't go so well
for these people in this movie,
and the robots kind of take over a little bit.
But I think it's kind of interesting to think through,
and I definitely would encourage you to also watch this movie.
And this movie takes place in 2035, allegedly,
which is 10 years away.
And so maybe in 10 years, you can definitely squint
and think about that maybe we are gonna be in a place
where these things are walking around and talking to us
and performing tasks in physical world and digital world.
And what does that look like?
What does that mean?
And how do we program them?
How do we make sure that they sort of do what we want them to,
et cetera.
So when you put all this together,
I think the feeling that people talk about often
is this feeling of AGI.
Like, do you feel the AGI, quote unquote?
And what this means is that you really intuitively understand
the magnitude of what could be coming around the corner
if this stuff actually continues to work.
The amount of automation that we can potentially have
in both the digital space and the physical space.
Now, I don't know about you,
but I actually find this picture kind of bleak.
This is what came out when I put a bunch of the last few minutes
of talk into an image generator.
And I don't actually like this picture.
I think we can do better.
And we have a few thousand people here.
You're about to enter the industry,
and you're gonna be working on a lot of this technology,
and you're gonna be shaping it,
and you'll have some active sort of power over it.
So I don't know, maybe we want this
to look something like this.
This is what I would like.
So this is humans, animals, and nature,
coexisting in harmony.
But secretly, this is actually a high tech society,
and there are robots and quadcopters,
and there's a ton of automation,
but it's hidden away, and it's not sort of like in your face.
So maybe this is something that we want instead.
And you should feel a lot of agency
over what you want the future to be like,
because you're gonna build it.
So maybe we can agree right now
that this is better than the previous picture.
But I don't know about you,
but I would hope so because I'm gonna be living
in that future, I think.
So the question for this hackathon,
I mean, a lot of you have worked on a bunch
of really cool projects over the last day or two.
And the question is, how do we go from hacking
to actually changing the world and building this future,
whatever that may be for you?
And so what I thought I would do in this talk
is go over maybe like my last 15 years or so
in the industry, and I think I had a bit of a window
into how projects become real world change,
and I have some takeaways and things like that,
and that I maybe wanted to talk about.
So the first thing that I find really incredible
is how projects that are sometimes very small projects,
like little snowballs, can actually like snowball
into really big projects,
and just how incredible that is to watch.
So as an example, I have my fair share
of hackathons like I mentioned.
These are some projects from a long time ago
that I worked on over the last 15 years or so.
So I had a little Rubik's Cube Color Extractor.
I put up some game programming tutorials on YouTube
like 13 years ago and tried to teach people
programming for games.
I had a video games and a lot of them.
I had this like kind of janky neuro evolution simulator,
which was kind of interesting.
And unsurprisingly, not all of these projects
actually go onto snowball.
A lot of this is just exploration, you're tinkering.
And so actually these three projects
didn't really go anywhere for me.
I wouldn't say that it was really wasted work.
It was just like it didn't add up to end in a snowball,
but it was still like helping me along the way.
I'll come back to that later.
But the game programming tutorials actually ended up
snowballing for me in a certain way,
because that led me from game programming tutorials
to a bunch of Rubik's Cube videos actually
that became kind of popular at the time.
And this kind of sparked an interest in teaching for me.
And then when I was a PhD student at Stanford,
I got to teach this class CS231N
and got to develop it and teach it.
And this was the first big deep learning class at Stanford.
And a lot of people have gone on to like this.
And then after that, I ended up making another YouTube
channel, which is my zero to hero series for deep learning
and LLMs.
So a lot of people like that as well.
And then on top of that, continuing the snowball,
the project I'm currently very interested in
is this next class and what it could look like
and how I can make it better.
And I'm calling that LLM101N.
And it's about building a storyteller,
something like kind of a chat GPT
that you can work with to generate stories.
And the idea is you build everything from scratch
from basic prerequisites all the way to like,
kind of a chat GPT clone in the domain of storytelling.
And building that from scratch,
I think will be really instructive, could be really fun.
I only published this on GitHub like two or three days ago.
So it's pretty raw and still very much in the early stages,
but I'm really excited for it.
And this for me is an example of a snowball.
It started with like 13 years ago, a little game programming.
And I'm working on a course
that I think will be really interesting.
Thank you.
Another example from my life, I think,
is the snowball that I've witnessed with OpenAI.
So as was briefly mentioned,
I was a founding member researcher of OpenAI.
And so I was there seven years ago.
These are some images that are public
of what it was like working out of Greg's apartment,
like eight of us.
And OpenAI was founded to be kind of like
a counterbalance to Google.
And Google was like this gorilla,
with 70 billion free cash flow.
And back then, Google employed like half
of the AI research industry almost.
So it was kind of like an interesting setup, I would say.
And we were just like eight people with a laptop.
So that was really interesting.
And very similar to my background,
OpenAI ended up exploring a large number of projects internally.
We hired some really good people.
And many of them didn't go too far,
but some of them really did work.
And so as an example,
here's a project that was in an early stage,
a very small snowball in the early history of OpenAI.
Someone worked on a Reddit chatbot.
And if you come by their desk and you're like,
what does this look like
when someone's working on a Reddit chatbot?
We're trying to like compete with Google.
And you're working on a Reddit chatbot,
like we should be doing something bigger.
And so it's very easy to dismiss these small snowballs
because they're so fragile,
right, these projects are so fragile in the beginning.
But actually this Reddit chatbot,
and by the way, don't read too much into the specific details.
These are kind of like random screenshots,
just for illustration.
But this was a Reddit chatbot and it looked naive.
But actually Reddit chatbot, what is that?
It's a language model.
And it happens to be trained on Reddit,
but actually you could train a language model
on any arbitrary data, not just Reddit.
And when the transformer came out,
this was spun into something that worked much better.
And then the domain was expanded
from just Reddit to many other web pages.
And suddenly you get GPT-1, GPT-2, 3, 4,
and then you get GPT-4.0.
So actually this Reddit chatbot that was so easy to dismiss,
actually ended up leading and snowballing into GPT-4.0,
which we currently think of as this change
in the computing paradigm.
And you can talk to it and it's amazing.
So it's really incredible for me to have witnessed
some of those, I guess, snowballs.
And today opening out, of course, is worth
maybe somewhere just below $100 billion or something like that.
So really incredible,
incredible to see some of these snowballs in practice.
So I would say a lot of you, over the last two days,
have also worked on small projects, small snowballs, maybe.
And it's really incredible to me that some of them
probably won't go anywhere,
but probably some of them actually will.
And you should continue the momentum of your projects
and maybe they can add up to a really big snowball.
And that's really incredible to watch.
The next thing I wanted to briefly talk about is
this concept of 10,000 hours
that was popularized by Malcolm Gladwell, I think.
I actually am quite a big believer in it.
And I think that to a very large extent,
success comes from just repeated practice
and just a huge amount of it.
And you should be very willing to put in those 10,000 hours
and just literally just count.
Don't be too nervous about what am I working about?
Am I succeeding or failing, et cetera.
Just do simple bean counting of how many hours
you're doing and everything adds up.
Even the projects that I failed at
and didn't snowball into anything,
those add to my counter of the number of hours I've spent
developing my expertise
and getting into an empowered state
of being able to take on these projects
with confidence and getting them to work.
So a few examples of that.
I made this really janky website a few weeks ago.
This was a weekend project
and it's called awesomemovies.life.
And you can visit it, I think it still works,
I'm not 100% sure.
I wouldn't recommend you go there.
It's trying to be a movie recommendation engine
because I was trying to figure out what to watch
on that Saturday and then I was like, okay,
I need to build myself a movie recommendation engine.
So I put this up and one of the tweets
that was a reply to mine was, wow,
that's so cool that you got this to work in the weekend.
And I was kind of reflecting on that at the time
because it wasn't as amazing to me.
And the reason for that was that
what this person is not seeing
is that this is my 20th time
like making a website like this.
And so I see all the steps I was gonna follow.
Okay, I need a linode, I need a flask server,
I'm gonna write some of this JavaScript style sheets,
HTML, I'm gonna spin this up together.
I need to scrape all these web pages,
I need to extract TFIDF vectors, I need to train SVM.
And all of these things are things
I've already done before 20 times.
I already have code snippets lying around
from previous projects.
And I'm just remixing what I have
and I've already done all of this.
And so remixing everything into a new form
isn't actually that much work
and allowed me to put this up over the weekend
and it's not that crazy.
And this only comes from expertise,
this only comes from having done it 20 times,
but you can do this so confidently.
The next example I would say in my life
was a Tesla autopilot.
So I was hired to lead the computer vision team
at Tesla Autopilot about seven or eight years ago.
And one of the first things I did actually
when I joined the team was I basically ended up
rewriting the computer vision deep learning network,
training code base from scratch in PyTorch
in some of the first few months that I entered the team.
And I sort of rewrote the whole thing from scratch
and that ended up being a kernel of what it is now.
And I think to some extent,
to some people that looked impressive at the time,
but for me it wasn't because I was coming from my PhD
and I spent five years doing stuff like that.
And I knew exactly what needs to go into there.
I need my training set, my evaluation sets,
I need my training loop in PyTorch,
I need my sort of configs, I need my log directories,
I need to bring in a ResNet, I need to put in detection,
we're doing regression classification.
And so the whole thing,
like I'm anticipating all the steps
and that only comes from experience,
that only comes from having done it 20 times before.
And so I think this makes a huge difference
and things that look impressive
are maybe much less impressive to you
if you've done it 20 times before.
So really try to get to this point
where you have your 10,000 hours,
it makes a huge difference and just, yeah, that's it.
By the way, 10,000 hours,
if you're doing six hours per day,
I think this works out to about five years.
So it's about a length of a PhD
that you need to develop expertise in an area.
So I think it's roughly correct
that that works out to about a PhD length.
The other thing that I found is actually quite useful
is to keep the dopamine flowing,
be aware of your psychology, your brain, how it works
and what it needs to keep going and how to keep inspired.
And so in particular, your brain is a reward machine
and it wants rewards and you need to give it rewards.
So what is a good way to give it rewards?
And in my practice, it is by doing projects
and work on projects and continue publishing them.
And so here I have a webpage snippet
of some of the projects I have worked on in the past.
And these are hackathon projects and random projects
and not all of them are good,
some of them are not quite good, et cetera.
But what I love about project is the number of things.
Number one, I love that projects get you to work
on something end-to-end and depth-wise.
Like normally when you go to classes,
you're learning in a breath-wise fashion.
You're learning a lot of stuff
just in case you might need it in the future.
Well, when you're working on a project,
you know what you need and you're learning it on demand
and you're just trying to get it to work.
So I think it's a very different mode of learning
that really complements the breath-wise learning
and it's very important.
So I 100% encourage people to work on projects.
The other thing is putting them up
is actually also like a really good Jedi mind trick
in my experience.
The reason for that is that if you're gonna put something up,
you're thinking about all the people
who are gonna be looking at it,
your friends and teammates and family
and future employers, et cetera.
And so that really increases the bar for your own work
and it makes you work harder
because they're gonna be looking at it
and you feel shame if it was crappy.
And so you work much harder
and you're gonna go that extra mile to make it really good
and that really, really helps.
And lastly, when other people are looking at your projects,
you're gonna get that reward because they like it,
they appreciate it, they fork it, they work on top of it.
And so that feels good to your brain.
And so the way that this comes together is
you are getting your dopamine, you feel good.
That way you can build up to 10,000 hours of experience
and that's what helps you a lot.
Snowball your projects from a small snowball
all the way to a really big one
and actually make change in the world.
So in summary, that's I think how it works like
on a high level and the message is just keep hacking.
That's it.
And then hopefully, this is the future
that we're gonna build together when we snowball
all of our stuff or something like that,
but not the first picture I showed, hopefully.
And that's it, thank you.
And then hopefully, this is the future that we're gonna build
together when we snowball all of our stuff
or something like that, but not the first picture
I showed hopefully.
Andre Kapparthi, everybody.
Thank you, Andre, that was awesome.
Thank you, thank you.
All right, let's get to those pitches.
The grand prize.
Coming up, you're gonna hear eight pitches by eight projects
filtered through 290 submissions, narrowed down to eight
so you're all gonna see some cool stuff.
The grand prize is $25,000 investment
and actual term sheet from the Berkeley Skydeck Fund.
They must commit to hacking all summer on their project
and they must appropriately form, of course,
a legal entity.
How do you get money otherwise?
All right, I would like to now tell you briefly
about how this is gonna go.
Eight projects, as I said, three-minute pitch.
You guys ready?
Three minutes?
Yes, they're ready.
The judges will then provide three minutes of feedback
and then after all the pitches,
the grand judges will go and deliberate
and pick a winner while we show you some other cool stuff.
All right, I would like to introduce now
to great applause, everybody, please,
because we have an incredible panel of judges.
We are so pleased to have them.
Please welcome our first judge, Brian Boardley,
with the Berkeley Skydeck Fund.
Welcome, Brian.
Marcy Vu with Greycroft.
Welcome, Marcy.
Thank you.
Nandi Iregbulem with Lightspeed.
Welcome, Nandi.
Irving Su with Mayfield Fund.
Welcome, Irving.
Kurt Quitzer, UC Berkeley Faculty
and Serial Entrepreneur.
Welcome, Kurt.
And Mark Nitzberg, Berkeley Faculty and Director
of the UC Berkeley Center for Human Compatible AI.
Thank you, judges.
All right, we got eight startups, warming up backstage.
Let's give them a little drum roll.
Let's give them a little drum roll.
We can get them going.
I first have to hear if the slide's up.
The slide is up first.
Are you ready?
You ready?
Are you ready?
Yes!
Please give everybody a warm round of applause.
They've been up all night hacking
and they're ready to share with you.
Please welcome the first project,
Revision.
Come on out.
Come on out, Revision.
Woo!
Woo!
Yeah!
Danica!
Okay, oh yes, the mic.
That'd be helpful.
Yeah, thank you.
So good evening, everyone.
It's my pleasure here on behalf of my team also
for the Revision Project.
And my name is Danica.
I'm a rising senior studying computer science
at UC Berkeley.
We have masters of design students
as well as data science students on our team
and we're really excited to tell you about our project.
So our project, we're focusing on building
an AI co-pilot tool for STEM textbook authors
capable of detecting and mitigating bias
in textbooks to create inclusive education content.
And there is a reason why we're doing this.
When considering overall representation of scientists
across textbooks, only 13.1% were women
compared to 86.9% men in a 2020 study
that featured seven of the most frequently used
biology textbooks within the US.
And on average, people of color only appear
every 320 pages of text while white figures
observed every 24 pages across 10 college STEM textbooks
published between 2016 and 2020.
So we thought about this problem deep and hard
and it has been something that I've seen
from my personal studies.
And starting from elementary school to middle school,
we constantly see different examples of word problems
in other situations where text is always there
and it's not always reflective of the actual true history.
And this research has been done by numerous scientists
who have gone through this process
of identifying people creating databases
but there is just no current fix that,
and no one is really hoping to create this problem
but there is no current fix that helps address this problem.
So the textbook companies actually
is who our team identified as our buying customer.
The current revision process actually takes six to 12 months
of a committee of five or more full-time employees
working on bias checks.
And the issue here is that employees
are actually not experts on their topic.
They also bring in their personal biases as well.
So our tool would come in right in between the writing
and revising part of this entire cycle
that developers go through when writing textbooks.
So again, here is our competitor analysis.
I'm sure many of you have used Turnitin
or Grammarly when you're submitting even essays.
And we really think that there needs
to be an additional check here for bias
and checking gender, racial, political, and other biases
and making this process affordable and automatic.
So it's not a costly process for anyone.
And throughout this process,
we're addressing supply chain diversity.
So starting from a younger age,
the elementary school students could be able to use textbooks
that truly reflect the true history as well as themselves.
And here is our prototype.
So we have our text box here on the left side of the screen
where you get to show in real time the examples
of some sort of text that a writer is creating at the moment.
And on the right, we have an overall score
and the bias checks for different categories.
And we're using machine learning models on the back end
to actually identify as well as LLMs.
And I'm not sure if I can play the prototype, but.
Okay, yeah, it does play.
So essentially you can click through the different links
to see the breakdown.
And once you actually highlight one of these,
we are also adding in an API through a couple
of the sponsors here, such as Hume API and more,
to actually identify emotional analysis as well
in the textbook writing.
And in addition to this, we're hoping to build a chat bot
that can actually help you also bring in databases
from the unrecognized scientists
and being able to sort of represent it.
Because bias actually exists in three different ways.
One of them is through actual text such as
representing firefighters would be nicer
than saying fireman.
And the other way is the entire tone and emotional analysis,
which is why our team used Hume API
to actually detect that emotional component.
And the third one is mitigating bias.
So we also considered adding in the chat bot.
So say, for example, if you want to highlight scientists
that are like, for example, contributing to physics,
you wouldn't just say list a few male scientists
and call it a day.
We would also suggest equivalent contributions
of female scientists as well.
So please join me and our team in revisioning
our future of education and work.
Thank you.
So I think everybody in communication today,
nonprofit or profit is concerned about diversity.
So it seems like you have a much larger market
than just textbook educators.
Also a comment on kind of like market sizing and whatnot.
I would think about potential ways
you could expand the market here because the number of people
who are involved in writing textbooks
is a relatively small group.
But one way to think about it is maybe
in this new era of AI generated content,
a much wider array of people can be part of this textbook
generation process.
So that's one thing.
And then I would also maybe consider selling directly
to the consumers of textbooks.
So in some sense, the bias we're talking about
is internalized on that side of the equation,
not on the manufacturer side.
And so they could be an incentive there
for people to want to pay for something like this.
Yeah, definitely.
That's something we're considering.
So like the textbook would be our official buyers
that we're marketing to, but eventually it would be more
of like a grammarly checker type of tool
that anyone can use.
Yeah, I had a similar comment on TAM and market opportunity.
And as you think about just how a textbook
gets put into production, that if you actually had it
as a tool for whether it's news or other areas,
you'd have more velocity, both in terms of getting the data
to improve your models, but also greater impact.
Yeah, I'll just, I'll just flag as well.
I mean, similar, I think everyone here is kind of hitting
the theme of how do we think bigger?
So even enterprises, right?
Like companies sending out communications
internally or externally.
I know this, this, this problem exists everywhere.
So that's kind of where my brain would go to.
Okay, thank you.
Thank you.
Thank you.
Thank you.
Whoops, Agent OS.
Please welcome Agent OS.
Hey there everyone, my name is Shashank.
I'm here today with my friends, Uggum and Drew Buhuja
somewhere in the crowd over here.
We built today Agent OS.
Picture this, you work at a hair salon
and you guys are bombarded every single day
and every single year by your accounting
and tax preparation qualms.
These are things that are very hard to deal with
and you've heard of tools like OpenAI, ChatGPT,
LLM this, ChatGPT that, everything,
but you have no clue where to start
using these technologies.
And that's no fault of your own.
The current state of the technology right now
is very bad at multi-functionary tasks.
More so, it's very hard as an individual developer,
sometimes even non-technical, to even get started
with even the simplest automations or workflows
or tools with such LLMs.
Even engineers years on years of experience in this space
take tens of hundreds of hours
and even thousands and thousands of dollars
to even get started to build something.
This is where Agent OS
completely transforms the landscape.
With Agent OS, you're able to create multi-agent workflows
in a matter of seconds from natural language.
What does that even mean?
Take your average corporate org structure.
You have your managers, you have your workers
and sometimes you even have your interns.
Everyone is really good at what they do.
They have their tools, their skills.
Let's say John is really good at charting
and making PowerPoints.
Let's say Steve is really good at Python coding.
Everyone's really good at what they do.
In this, you have a very collaborative
working together to create this common solution
for someone coming from higher up.
That's how Agent OS was designed.
Our engineers, Dhruv and Uggum,
were able to replicate this human collaborative process
to programmatically using LLMs.
What does this do?
This allows everyone from the common Joe
all the way up to enterprise clients
to be able to interact and use these multi-agent,
agentic workflows in their day-to-day life
to improve their quality of life or productivity.
In all, in a matter of seconds
and a few sentences.
Let's go back to the study of the hair salon.
In the process of doing your taxes and accounting,
you have multiple steps.
You have your collection from your receipts
and your invoices.
You have calculating your cash flow,
all the calculations you have to do.
You have to manage your workers
and then you also have to do your general summary.
What about your insights for the year,
how you were spending, what you were spending on?
And you have to also do a lot of clustering
and analytics on this.
This is a very complex workflow
that's nearly impossible for modern day LLMs
at the current state to do right now.
You can take chat GPT.
You ask it a question for even more than three things.
It'll forget what the first thing was
by the time you're at the second.
It doesn't work that way.
With Agent OS, this completely changes
where you're able to have these complex workflows.
Let's dive into another demo.
So let's say I'm an analyst at JP Morgan
and my boss tells me every morning
I want a report of XYZ stock in the morning,
a detailed report on paper.
How do I do that?
I use Agent OS.
On this screen you can see a bunch of other complex use cases
of multiple agents working together collaboratively.
But in the toolbar, you can see the use case of the analyst.
Here I have to do market research, livestock data.
I have to search the internet, go on Yahoo Finance.
Then I have to create my analysis,
technical analysis, qualitative analysis.
Then I have to do what my boss is telling me to do.
And after all of that,
I have to create charts, graphs, and visualizations.
Here you can build tools using natural language
like the one right there that says,
write me a tool that fetches the meta stock price
from Yahoo Finance.
In a matter of seconds,
the common Joe or anyone is able to create that tool,
connect them to workers.
You can think of workers as your everyday employees,
agents, people that perform these actions using the tools,
and then connect them to super teams.
And these teams are able to, on the screen you see four,
but you can scale this up to 40, 400,
basically complex vertical and horizontal organizations
that are able to perform complex decision making
and complex analyses for anyone,
from enterprise to consumer.
What does this do?
With a multi-agent, multi-team framework,
this completely opens the landscape up for anyone
and everyone to take on the power of LLMs
into their own hands from natural language.
Take your average farmer at a farmer's market.
He's trying to create his marketing campaign
for the upcoming farmer's market this Sunday.
He has no clue where to start looking at his metrics,
looking at the customers, looking at the weather,
and creating these brochures, papers, pamphlets, and whatnot.
With one line and one minute using Agent OS,
he can create all the documentation he needs
in order to enact this stuff
and be able to perform successfully
and continually grow his business at his farmer's market.
Things like this are completely opened up with Agent OS,
and we hope to completely democratize the process
of using LLMs at all scales, at all geographies,
and all use cases within sentences and seconds.
Thank you.
That's a compelling proposition.
The one thing that I worry about is, right now,
the agents are the LLMs performing these tasks
and there's a certain question about the veracity
and reliability of what they're doing,
and so I think that in a future where we have
that reliability, this would make perfect sense,
but I would want to add a kind of tandem subject matter expert
maybe looking over the shoulder of each of the agents.
I think next time I hear this pitch,
I'd love to hear about the one market you're going to crush.
It's hard for me to imagine serving a hairstylist one day
and a Morgan Stanley analyst the next.
This is a huge opportunity and a big bold mission
that you have.
I would want to dig a bit deeper into your tech staff
and the people you have on your team
because these are really complex problems and issues
and also agree that what would be your first area of focus
because it's pretty broad and wide.
Also, I kind of like the broad focus
and there's a lot of individual startups tackling
each of these individual problems.
If it's invoicing or research, it might be interesting to figure
out how to loop in all these other tools that are out there
and really kind of just be like an interface layer
and let these other companies solve the technical challenges.
I think the value proposition of creating multi-agent workflows
in a matter of seconds is really compelling.
I think the next step would be trying to figure out
how can you go from simply performing these tasks
to becoming the best at these tasks.
So for example, going after the outliers,
sort of the thesis around coaching networks,
some startups do this and they do it better
for certain verticals and others.
So I think doing more research around that
could be really compelling.
The only thing I would add is just think about enterprise security
and how you solve for that.
There's a lot of authenticates and authorization
you're going to have to do for all these agents,
so just have an answer for that.
Well, yeah, thank you so much.
Thank you, everyone.
Thank you, Agent OS.
All right, next up, Skyline.
Come on out, Skyline.
Hey, everyone.
Hey, so my name is Rajan
and I'm a first year student at the University of Waterloo
and I study software engineering.
And I fundamentally believe that cities shouldn't be static.
They should optimize and scale for the people who inhabit them.
And so we built Skyline.
Skyline is an optimizer and it allows you to better understand
how to model cities using agents
and optimize things like traffic and transit
to inherently increase mobility
and reduce things like carbon emissions.
So this is a very weird problem that we solved,
but I want to walk through the case study of Los Angeles.
So Los Angeles is one of the largest carbon emitters
in North America,
this most because of their transit,
because of the amount of cars.
And so what are ways in which we can optimize this?
Well, let's look directly at the people who inhabit Los Angeles.
We can extract census data, things like age.
We can look at things like gender.
We can look at things like income.
We can find population density graphs.
And using this information, we can start to find patterns.
Specifically, what we did is we created 500 distinct agents.
Each agent is a different citizen with different interests.
And what we can do is we can give them each their own chain of thought.
Each person here has their own day in their life.
For example, this person is a very young,
I believe this was a 22-year-old with a very large income.
He's a long day at work, and after work he goes to the gym.
We can now reason about what this person may do
and now model this on a map.
Now, once we have how these different agents are moving around,
what we can do is we can try and optimize things like transit.
So what we do here is we have our own proximal policy analyzer.
And this allows us to create simulations
on what we believe to be the best way to understand
how we can move around from any point A and B
in the fastest way at the lowest carbon cost.
These are our own carbon cost analysis mechanisms,
our own machine learning models to better understand
how we may be emitting carbon
and how to reduce this through our transit.
So this is a lot that is through you,
and I think the best way for me to represent this to you is through a video.
I hope this video loads.
Is it possible to play the video?
So what we first do is we have an agent-based simulation.
These are 500 distinct things in parallel that are running.
Now they each go around throughout their day,
and what we can do is we can find patterns in how they move around.
Now the best part is what we can do is,
now that they're all back in their original position,
we can start a generation of transit.
And we're using these patterns to now generate live different transit systems
that we believe to be the most optimal.
So what Skyline is, we're not a company that does analysis of transit.
We are a human modeling company,
and that allows us to better understand and better predict
how things around us will change
and how we can optimize them using these patterns.
Yeah, so that's Skyline.
Happy taking you to bed.
Wow.
I just want to observe that what you're doing in creating a sort of digital twin of a city
is for the, essentially the, you know, the,
each citizen is being simulated using, you know,
one of these really powerful, expensive things, a language model.
And it will be probably an important step to draw from the language model
some of the statistics that are actually fed in in the first place
and make sure you're getting out something representative.
But that's very impressive.
Yeah, yeah, similar comment.
I think, you know, there's all sorts of like economic theory about, you know,
agents and modeling their behavior and their values and whatnot.
And the thing that usually gets you is the sort of heterogeneity across the population.
So making sure that that actually represents the populations being modeled is important.
And then the other thing also related to value, I would think about,
is just value capture for your own sake.
I feel like this is like a category of software where like, yeah,
the economic impact of this could be massive,
but how much of that do you get to capture as a software render is like less clear to me?
But it's very interesting.
I guess I would be curious about maybe some more like nuanced enterprise use cases as well,
if it's concerts or security or stadiums.
So kind of just thinking about like, are there more micro use cases
that there's a more direct ROI with for this sort of modeling?
Yeah, we tried to consider ourselves to be a human modeling software.
And this is just one of the most visual applications which is strength.
Awesome. Thank you so much.
Thank you.
Thank you.
Thank you, Skyline.
All right.
Next up, we have, we have Spark.
Please welcome Spark.
Hi, how's everyone?
How's Cali?
We are Spark and we're giving a voice to new entrepreneurs, young entrepreneurs.
So let's admit it.
Cold calling is really hard.
I mean, resources are hard to get.
It's a steep learning curve and getting attention is hard.
If you've cold called someone, you know, they don't have time.
They'll say, oh, sorry, call me back later.
I mean, they're busy.
Everyone's busy.
We have things to do.
So we have to figure out how can we earn the time of working people?
There's existing solutions.
It's long and arduous for trial and error.
It's expensive for a sales coach.
And finally, it's, if you have a sales partner, chemistry isn't easy.
If you're just meeting them, right?
Well, we have a process.
You upload a transcript to our software.
We go through and analyze the emotion.
We aggregate this data and we give you productive feedback.
Who's our target market?
Well, look around.
You guys are a target market.
People who are engineers, people who love to build and say this weekend you made
some sort of product you want to sell.
You don't have much experience with sales or outreach with our software.
You can record your cold outreach and we can tell you what you've done right and
how you can improve to hopefully land your product where it needs to be.
And later on, we want to expand to call centers and sales staffs because we think
we can spread this across an organization and it can be highly profitable.
We have usage-based peering, so 75 cents a minute for a thousand hours,
going up to 40 cents for 10,000.
So this is our software.
And I want to tell you guys a story.
I started being an entrepreneur around six months ago
and we made an AI body cam analysis start-up.
So I did 100 phone calls, 100 cold calls.
I got no clients.
200, 300, 500 and 700, no one was responding to me.
So by 800, I got actually three and I realized something.
The human brain is pretty amazing.
We're able to pick up on patterns, but at the same time, it's kind of inefficient
because it took 800.
Here, we look at the emotion between every single sentence
and we figure out spikes of emotion and decreasing emotions.
We see that when we talk about security and data privacy with police officers,
it shows an increase in their interest.
And this was a trend among many conversations we had.
So in our analysis page, we see in the top left
that mentioning AI accuracy and efficiency, increased officer safety
and discussing cost savings really helped us when we were talking to officers
because we're some college students, right?
We're dealing with some pretty confidential data.
Bringing this up early really helped improve our rates.
And the four things you see here in the corners are the different triggers
we generated automatically based on the cold calls we had.
So one is positive reactions, negative reactions,
escalating or de-escalating tense situations
and normalizing exciting situations.
We also generate insights too based on whatever cold calling trends you make.
We also have a rag so you can upload your company knowledge,
your target audience, and your pricing information.
So if you make a mistake, don't worry, we got your back.
We'll tell you, hey, maybe instead of saying this,
you could have said this because it might have helped you out a little bit.
Sorry, my team picture isn't on here, but thank you to Tushar and Nick and Krishna.
You guys were a great team, and I'm honored to be here representing you guys today.
I'm open to feedback.
I guess I need to be the first person to say that you're entering a pretty competitive market
with other offerings here.
I'll say something that stuck out to me was this idea of insights,
but I think at an organization there's going to be a sales team and a marketing team
and an online web team, and those teams don't really talk to each other,
so maybe it's interesting to think about how do you pull insights from one channel
of sales or marketing and actually bring that into another channel.
So maybe the insights from cold calling are actually influencing what's going on the website.
Maybe there's some interesting spots of opportunity there.
I can actually talk about one facet of this we want to explore deeply.
I want to give you an example.
Say we have three founders in the company, right?
I have a first cold call with one person,
and later on my second co-founder wants to set up a warmer call in the future,
and then my third founder wants to set up a third call.
We want to build a profile for this client as they go along,
so we can truly understand them.
And also we want to develop a profile on ourselves too,
so we can learn more about ourselves as we go and how we're behaving,
make sure that we're learning as we go.
So we're thinking if we develop a CRM on top of this data that we leverage,
then we can connect multiple teams and enable cross-functional benefit.
Yeah, I had a similar comment.
I think it would be really game-changing if in addition to some of the real-time analyses
your guys are doing around sentiment,
where you can see the system with information on prior calls or this person's particular strengths
and weaknesses and how they complement those of the other people on the team,
and to really build a CRM, this knowledge graph around each person's strengths and weaknesses on the team
to be able to better fine-tune the system.
Yeah, thank you.
You guys saw the analysis, but also there's a long list of past conversations.
You can actually go into every single conversation you've ever had
and look at it deeply the same way you did in the latest conversation.
I would think about the full sales funnel.
This is pretty deep down in it.
And as you think about where are you really going to be able to convert
or where's the wedge that really matters?
Because there's a lot that goes into converting a sale and it's not just the cold call.
So is the issue, are you actually calling the right people?
Or is the issue, are you actually speaking to the right decision-makers?
So just thinking more broadly about that funnel
and where you might actually be able to have the most impact
and have the right wedge into the broader product suite.
Yeah.
Thanks.
Thank you.
Thank you.
All right.
Thank you, smart.
We appreciate it.
Bye.
Clicker.
Clicker.
Thank you.
Okay.
Next up, we have Hear Me Out.
Please welcome Hear Me Out.
The big one.
Hi, guys.
Hey.
Hi, my name is Marcus.
I'm from Hear Me Out.
And what we've built is an AI-driven customer service pipeline,
optimizing call, matching, and visibility.
So that might leave you scratching your head.
So let's just talk about the problem.
So let me give you a bit of context.
I'm an exchange student.
And when I first came here, I had to deal with so many things.
I had to deal with banking.
I had to deal with insurance.
I had to deal with deliveries.
And I even had my car break down on me.
And that was a real pain.
In short, I was overwhelmed by the sheer number of customer service calls,
because for each of these things,
I had to make so many calls just to get things done.
And I think a lot of you guys can relate to that.
We've all had our fair share of bad call experiences where we're upset.
The customer service representative is also upset.
And nothing is done.
We've also had good experiences as well.
And I think that's the core of what we're trying to tackle here.
We want to create a pipeline that tries to provide optimal matching
and provide visibility on emotional data to businesses.
So we also did the research.
And I think the number speaks for itself.
This is a key problem with a sizable market.
And a sheer number of people are affected by this as well.
And this is our problem statement, which is,
how might businesses which offer customer service calls
provide a better experience for their customers?
So we think we can tackle this in four key components.
First of all, an improved call bot.
We all are common with that initial robo call that we have to deal with.
And sometimes it's really, really frustrating.
How many times have the call bot talked to you
and it just doesn't direct you to the right person?
I think we've all experienced that before.
Second and third, and I think this comes hand in hand,
is just business visibility.
We want to provide businesses with better visibility
of both call experiences data,
as well as customer representatives bandwidths over the day
as they continue to take calls.
And finally, this is where we put those two together.
We want to take that data and optimize a customer's journey
through a better customer-to-service representative matching.
So I won't bore you with this data,
but with that in mind, we developed a set of decoupled microservices.
And I just want to point three key parts out to you.
So first of all, we want to assess customer agreeability
with an initial robo call.
But this won't just be your normal robo call.
We want to use Hume's EVI to manage the robo call
in an empathetic manner,
such that it measures the customer's emotions
as they go through the call,
and eventually outputs an agreeability score for the customer.
Second of all, we have a call analysis feedback loop,
and that's that whole thing on the right that goes down below.
And what this does is once you have a call connected
between a customer and a representative,
it takes in multiple factors of data,
such as the call duration, the emotional change over the call,
and the overall call outcome.
Using Hume's emotional measurement API,
we can then also generate a call score.
Finally, and this is the third and key part to this,
it's the matching API.
Using the two things that I just mentioned,
we can best match a customer to a customer service representative,
which matches their vibe, their energy,
and their emotions based on how our custom model is developed.
So what's the outcome of all of this?
As a representative goes through their day,
their state changes depending on how their calls go,
and their bandwidth adjusts accordingly.
This affects the subsequent customers
which they are matched to in a positive manner,
and creates a better experience for both parties.
So there's a lot more which we can build with what we have,
but with this foundational pipeline,
we believe we effectively tackle the problem that needs to be solved.
That's all I have for today. Thank you.
APPLAUSE
Nice.
Thank you.
Yeah, I mean, a little bit of feedback
similar to the last company as well.
There's a lot of companies working in this space too,
so I would just continue to think through
how to find that core differentiation,
if you continue to work on this after the hackathon.
Yeah, I completely agree.
I think a key part that we thought was really exciting
was just what you can achieve with custom models.
What we're doing by developing a feedback loop
is we're creating something where we can create,
in a sense, a model which trains itself.
We can assess how calls might improve
or get worse after the matching,
and that feedback gets fed straight to the matching API
so that it knows whether or not it's done a good job or not.
And we find that really interesting,
and we think that that's a key differentiating factor
which we can achieve.
There might be some opportunities
for building some sort of synthetic data pipeline here
where you could just sort of simulate calls
with an AI bot of some sort
and use that as feedback.
I don't know how good that data will be or not,
but it could be interesting.
Yeah, no, that's a really interesting thought. Thank you.
I know right now you guys are targeting
customer service agents as well as call centers.
Something that could be interesting to think about
is as you think about the different stages
of the software adoption lifecycle
as you go from your early adopters to your early majority
and then your late majority,
who's eventually going to justify your evaluation
in terms of what those ideal customer profiles
are going to look like down the line.
Yeah, thank you for that.
I think one key thing was we actually had a judge come to us
and talk to us about how they were doing something similar
for sales representatives as well,
and we found that really interesting.
So happy to figure out how we can pivot if that need arises.
Thank you so much.
Thank you.
Thank you. Hear me out.
All right, next up we have Dispatch AI.
Please welcome Dispatch.
Hi, everyone. My name is Spike, and I'm with Dispatch AI.
In the United States, over 80% of 911 call centers
are critically understaffed.
This means that in a crisis,
people with real emergencies aren't able to get the support they need
because all the human agents are busy,
and they're often put on hold.
This is particularly an issue in our neighboring city of Oakland
where last year the average wait time was over 60 seconds
to pick up a 911 call.
Now, in an emergency, every second counts,
and this could be literally the difference between life and death.
This is unacceptable, and that's why we built Dispatch AI.
The world's first AI 911 call operator
designed to eliminate these wait times.
Our system is powered by two key components.
First is the voice AI.
The voice AI will step into calls when all human agents are busy,
and it will work with the caller to evaluate their situation,
extract the location,
and optionally dispatch first responders directly to the scene.
And the second part is our powerful dashboard for the operator themselves.
So the operator will have access to a bird's eye view of all of the ongoing calls,
which will be automatically triaged by the AI into different forms of severity or priority.
Further, they'll see that the AI will automatically extract the location
and will provide a live transcript of the call
so that they can quickly see what's going on
and even step into the call once they're available.
Further, they have buttons that allow them to directly, with just one click
because the location's already fetched, dispatch police, firefighters, or paramedics.
All of this is done from a human-centric angle,
and the way how we achieve this is by taking into account the caller's emotions.
So, for instance, when a caller shows signs of anxiety or fear,
the system could work more to calm them down
and make them feel at ease before taking the next safe step.
This system is fully designed with ethical, safe guides in mind,
and part of that was fine-tuning a model on over 500 911 calls
so that it could understand the proper protocols
and be knowledgeable on a wide variety of possible scenarios
in which a 911 operator could assist in,
including fake calls or instances where it may not need assistance.
This is all powered by our innovative tech stack
that utilizes a variety of AI components, including the voice AI,
the emotional analysis, and, of course, a key component of this, the fine-tuning itself.
Our mission is to make requesting emergency services more effective and efficient.
Thank you.
I'll go first.
I thought you did a great job.
I thought you presented the problems that the opportunity and the product very clearly.
You only had three minutes, but you hit all the relevant points.
Thank you.
The one thing I would encourage you to think about a little bit is
the optimization function for these municipalities.
People in Oakland are waiting 60 seconds to get their 911 call answered.
There's a reason for that. I don't know what that is,
but somehow these municipalities have decided that that's how they want it to be.
I would just think about, as you bring in AI to this problem,
doing the potentially difficult A-B test of making sure that whatever it is
that these municipalities are actually optimizing for is actually improved by this,
because it seems like a no-brainer when you first say it,
but clearly it's this way for some reason that it's probably nuanced and tricky.
It's just something to think about.
Any other feedback?
Just following up on that, I think the key is ease of adoption.
I think it's going to be easy to make a productivity argument to the city of Oakland,
but then you've got to think about who's actually installing,
who's paying for this and who's installing it.
Okay, that's good. Thank you so much.
Thank you.
Dispatch.
All right.
Next up, we have ASL Bridgify.
Please welcome ASL Bridgify.
Hello, my name is Isha, and today I'll be presenting ASL Bridgify,
the next generation of sign language and interactive learning.
Okay, sorry.
So what was the inspiration behind this?
Well, ASL is the third most studied language after Spanish and English,
and over a billion people are projected to have some sort of hearing loss deficiency,
which is why it's even more important to have a seamless way of
for people with hearing loss deficiencies to communicate with people without them,
and vice versa.
And next, there's over 15,000% return on investment over a 10-year period,
demonstrating the value proposition.
And existing platforms like Duolingo surprisingly do not take into account ASL learning,
which is why it's important to build an interactive platform
where individuals can retrieve the accuracy of their signed texts,
as well as characters.
Now, our solution includes three proprietary AI models.
First, we use the random forest algorithm in order to map input post-estimation frames,
a frame length of 200 to the predicted alphabet from A to Z.
Next, we also use an LSTM model, which captures sequential dependencies
to map from hand post coordinates to the actual word.
And then we also have our individualized RAG calling in length chain,
as well as PDFs that are specific to ASL learning that get chunked
and transformed in a vector dimension space.
Now, as you can see here, this is a hand-posed estimation extraction
using the MediaPipe library, so you can see A, B, and C.
And here's our platform where there are different modules to learn alphabets,
signs, as well as sentences, and we even have a real-time ASL practice,
so in real-time to capture the letter that you're actually signing
and give you the accuracy for that.
So here's an example of us using the MediaPipe library
to actually extract all of the hand key points.
And here are some videos where there are over hundreds of words
that you can actually view to learn each of the hand-signing frames.
Now, this is our proprietary RAG.
And the way we've trained this is we've collected a variety of PDFs
that are essentially manuals for ASL learning,
and potentially in the future we would want to incorporate things
like YouTube transcriptions that can actually be transformed
and embedded within our vector dimension model.
Now, in the future, ASL doesn't just...
Hand-posed estimation doesn't just have to be localized to ASL.
There are plenty of other opportunities for human-posed estimation,
including fields like dance, martial arts,
where you can not only identify certain techniques,
but you can also get feedback generations from certain input frames.
And in the future, this could also be integrated into existing solutions
such as Zoom, Loom, and FaceTime videos.
So given the signing of a certain sentence transcript,
you can get in real-time the actual predicted sentences and words.
That's nice work for 36 hours.
I would be...
I spent some time creating assistive technologies for the blind,
and I would be just very aware of the market and how you'll approach it
and who will be paying.
I think that would be a good thing to pay attention to.
Thank you.
Yeah, as you think about the market,
I feel like these language learning apps are tricky to scale to meaningful businesses.
There was Rosetta Stone, whatever, 20 years ago,
and then there's been Duolingo on this most recent gen,
but there aren't that many that get to meaningful scale.
So it might be worth just thinking about that market
and what are the kind of success drivers?
I think, even as I mentioned previously, apart from just hand-posed estimation,
I think that there's a big market for body-posed estimation.
I think, especially in things like combat training,
especially if you look at the military,
even dance performance companies where they have to train dancers,
and they're actually specific techniques in which they want ground-trude feedbacks for,
I think those are also potential markets that could be ready to penetrate into.
You chose more traditional machine learning algorithms than early neural nets like LSTM,
and that may be the right answer.
That's not obvious to me, but I think you would, for today's audience,
who need to explain why you're not using more contemporary AI algorithms.
Yeah, so initially, we were actually thinking about using more encoder-based transformer models,
but we ran into some struggles, so we just ended up settling on the LSTMs,
but in the future, we would obviously adapt more of the state-of-the-art transformers,
and even in the case for feedback generation for given hand-poses,
that could be an easy encoder-to-decoder, multi-self-attention model that you could train.
Okay, thank you so much.
Thank you.
Thank you.
All right, our last contestant for the grand prize is Greenwise.
Please welcome Greenwise.
When I was 14, I stopped eating all meat.
I lasted about two months.
Now, even though I still eat meat,
there are a lot of small changes you can make that have a huge impact on your carbon footprint.
For example, by switching from beef to chicken,
you cut the carbon footprint of your meals by six times.
What we do is we help you make that switch from beef to chicken for everything,
for your shoes, your shirt, household supplies, food.
Everything has a carbon cost and a carbon footprint that we can mitigate.
So how does a consumer analyze all their purchases and the carbon footprint of anything
and try to make all these very difficult decisions and research about how they should change their actions?
Well, this is where Greenwise comes in.
We seamlessly integrate with existing customer purchase models to basically input
what the consumer is already doing, for example, through receipts or through emails,
and we have integrated with Apple Pay, with Amazon and with Square
to automatically get their purchases into our system.
From there, we vectorize their purchase and compare it to our vector database.
This database has all the carbon footprints of over 10,000 products that we've analyzed
and made sure that these are accurate carbon estimates.
Additionally, by using the vector embedding,
we make sure that these similarity scores are very accurate.
It's not an LLM that can hallucinate.
These are real accuracy scores and real carbon predictions.
From there, it directly can tell them an alternative product that is very similar,
but has less carbon footprint.
Additionally, this provides a lot of room for scaling
when other businesses want to analyze their carbon footprint for their products
or for events and other bigger venues.
So, from good intentions to reality, let's make it happen.
APPLAUSE
It's a very innovative RAG use case.
We've never thought of that.
We're not using RAG.
Oh, it's not?
It's similar in that it uses a vector embedding for finding similarity,
but the similarity is directly the output.
Yes, that's right.
Is this a subscription product?
Are you imagine it being a subscription product?
We would imagine...
Probably not.
Ideally, we'd integrate with existing businesses
like Instacart or Safeway
so that they can show our results
on how the carbon footprint of certain products is on their app.
But it also works for consumers to use on their own,
as demonstrated here.
People wouldn't pay for a subscription, though.
OK, I think that's all the comments.
Thank you so much.
APPLAUSE
Green wise, thank you.
All right, I would now like to invite our esteemed judges
to convene in this secret room
where judges make their decisions,
and we are going to have the special prizes.
So as I mentioned, a bunch of sponsors came to make this all happen.
We're an educational program,
and it is entirely the support of these sponsors,
and they're not just providing support,
they got cool prizes.
So let's bring them on in just a minute
you're going to hear from each one.
These are the sponsors for today,
and I also want to thank our community sponsors.
These are startups, very cool startups who hung out
and helped our young hackers with their needs
and their cool tools.
All right, so our very first special prize
is going to be announced by a very special campus partner.
I'd like to welcome the Academic Innovation Catalyst.
I'd like to welcome out here Matt Sincini and Paul Warp
to tell you about AIC,
one of our newest campus partners doing very cool stuff.
Please give them a welcome. Thank you.
Thank you so much, Caroline.
It's just a thrill to be here.
So my partner and I, Paul and I,
created Academic Innovation Catalyst, or AIC,
to release more innovation from academic research
for positive social impact,
and we're focused initially on the areas of climate tech
and AI for good, which is why we're here today.
How do we do this?
Well, we make proof-of-concept grants,
so no strings attached,
non-dilutive grants to academics with an idea,
and then we help them take that idea,
carry it through commercialization
to scale and sustainability,
and so that's what we do.
We're thrilled to be here today
and we'll be making two awards
to the most compelling business plans
or innovations involving the use of AI
to make the world a better place,
and we couldn't be more excited to announce them
in five seconds here.
I'll just say that we met with many amazing teams.
It's been an extraordinary weekend.
Thank you so much for including us.
We had to narrow it down to two.
It was tough, but I think you'll see that they're well deserving.
So with that, let me hand it to my partner, Paul Work,
to announce the winners of the AIC,
the AI for Good Awards in 19...
I'm sorry, 2024.
This is what happens when you get old people up here on stage.
So anyway, we're really thrilled to be here, as Matt said,
and we're especially thrilled with the fact
that so many of you are putting your talents to work
for such great causes
and for the betterment of humanity.
And AI has so much potential in so many realms,
but among the most important is to make the world
a better place and to make a social impact.
And so with that, we're thrilled to announce
the first two winners, Dispatch AI and ASL Brigify.
So these are tremendous companies.
Again, the competition was so strong.
May I ask actually both sets of winners to stand
in the audience here?
And thank you again so much for the terrific work.
I think as you heard, ASL Brigify is doing for sign language
what Duolingo has done for learning other languages,
and it is so important.
It's incredible and shocking that it's an underserved
and currently not served market,
and their technologies are going to change that.
And Dispatch AI, what can you say?
I mean, it's such an important issue to be able
to get emergency response,
to be able to get a response when you need it.
And of course, the reality is,
when we have unfortunately too many mass catastrophes,
the time when you need the response strapped
is that you're often most short-staffed.
And so Dispatch AI is using artificial intelligence
and a variety of technologies to speed that process up
and to help both the dispatchers and the people
that the dispatchers are helping.
And so can I ask the Dispatch AI team to stand up as well
and be recognized?
It's a great job.
Congratulations to all of you
and to everyone who is here today.
Thank you so much.
Thank you, Madame Paul.
Thank you, Academic Innovation Catalyst.
Our next special prize is going to be introduced
by our very own General Manager at Skydeck,
Sybil Chen.
Give her a welcome.
Hello, everyone.
Hope everyone has had a great weekend.
At Skydeck, we have about a year and a half ago,
we launched the Skydeck climate tech track.
In part, thanks to a nice grant from the university,
we had $150,000 to build out the track.
And right away, we started putting that to work.
We grew our advisor network from, you know,
maybe like five advisors in the climate tech
to now over 30 advisors that are in the climate tech space.
And beyond that, prior to the grant,
we had maybe three or five startups,
every batch that were in climate tech,
and now we average 15 startups per batch
in the climate tech space.
And we really hope to see that grow.
So I'm very pleased to announce
that the winner of the Skydeck climate tech track
is Team Greenwise.
I think they're still in the green room
because they just pitched.
They were the last ones to go on stage.
But they really kind of represent the type of startup,
the, you know, team members that we like to see
at early stage startups.
It's three team members that are best friends
from middle school.
Oh, they're out here on stage.
Come on out.
I wasn't expecting that.
Anthony, Ben, and Ethan,
three best friends from middle school,
representing UC Davis, UC Santa Cruz,
and UC Santa Barbara.
And they've built a platform
for carbon footprint tracking
with actionable recommendations for vendors
so that people and companies
can reduce their overall carbon footprint.
So please help me in congratulating this team.
Winners of $25,000.
All right, thank you, Simo.
Thank you, Greenwise.
Clicker, clicker.
Thank you.
All right, next up, special prizes from Intel.
Intel, come on out.
Intel was here.
Their table was swamped.
I'd like to introduce Gabrielle Amaranto.
Hi, everyone.
Thank you all so much.
And thank you to the organizers for having us.
We've had such a great weekend.
And your projects are so amazing.
So thank you to everyone who joined our track.
As you can see, the winners behind me,
congrats to all the winners.
We have our Raffle winner, Ayla Arez.
Third place is Asel.
Second place is Batteries by LLM.
And first place is Dishbatch AI.
So let's give them a round of applause.
Yes, great job.
Amazing projects.
If you won, please meet us outside.
I want to hand you your prizes.
We have them with us.
So please meet us outside so we can take pictures
and give you your prizes.
Thank you.
Thank you, Intel.
All right, next up, AWS.
Come on stage, AWS.
We've got Rohit Tururi, Kevin Liu, and Brandon Middleton.
And that's what they're going to tell you.
Go ahead, Rohit.
Howdy there.
Can you all hear me?
Yes?
Awesome.
Well, hey, thank you so much, Skydeck Team,
for having us and Calhacks.
This has been an extremely impressively run operation,
and we're really excited to be partners
and sponsors of this hackathon.
Today we have three different prizes.
Actually, let me introduce myself first.
We have Brandon, Kevin, and Rohit.
We are members of the Generative AI organization at AWS.
We work with top Generative AI startups like Anthropics,
Stability, Perplexity, and others
in the development of their large language models,
as well as our overall kind of inorganic
and organic growth strategy, including investments as well.
Today we have three different prizes.
We have four of the teams that we have chosen
to give the prizes out to.
Our first place prize is for $10,000 in AWS credits,
and we have two other prizes, one for climate tech
and then one for responsible AI, which are $5,000.
I did want to say that we talked to so many impressive startups
and founding teams today and hackathon teams today.
I wish we could give prizes to all of them.
We did want to recommend that those who we spoke with,
and I think we have these conversations with you already,
to go ahead and apply for the AWS Generative AI Accelerator,
as well as our AWS Activate Program
to receive additional credits.
I'll go first.
I'm going to be announcing the climate tech.
We're going to give the prize out to Disaster Aid.
It's Disaster Aid in the room today.
Yes. Good job, guys.
Kevin?
And then for responsible AI, we have a two-way tie,
so we're splitting that prize into 2.5K for each team in credits,
and that's GPT ethics and DP ancestry.
They're in the hall.
All right, and I'll round us out.
Our grand prize, kind of the most impressive thing that we heard and saw today
is going to go to Safeguard.
So, Safeguard team, if you're in the building, stand up real fast.
Let's give you a round of applause.
I don't see them, but God bless you and keep doing what you're doing.
Thanks so much, guys.
Thank you.
Thank you, Intel.
Our next amazing partner, Reach Capital.
Please come out, Tony Wan.
Oh, out of order.
Let me see if I can find you.
There you are.
All right.
Okay. Oh, Mike.
Awesome. Thank you so much.
Thank you to Calhacks. Thank you to Skydeck.
It is such a delight and pleasure to be with all of you today,
and thank you to everyone for being here,
from across the country, from across the world.
My name is Tony, and I'm from Reach Capital,
and let's just cut to the chase,
because there's no drama here.
We want to congratulate Frodo
for winning our AI and education prize.
Frodo, Aman, Kush, and the team,
if you are here, please stand up.
Please stand up.
All right. You are right up front.
You are in the right place.
Thank you so much.
You've won the one-ring, as they say,
or at least our $1,000 cash prize.
So please, let's meet up afterwards.
Reach Capital, we're an early-stage edtech VC firm
investing in edtech.
We invest in education across K-12,
higher ed, and workforce,
in tools that expand access to education and
accountability, and many of the companies
that are portfolio were founded by students themselves.
You know, like, what better place
to find great ideas and great talent
than to go to places like this,
where students are living that experience.
So if you're building a venture in edtech,
please reach out.
Thank you so much.
Thank you, Tony.
Next up, we have u.com.
We have Rex.
Come on out, Rex, and tell us about the prize.
Applause, please, for our sponsors.
Hi, buddy. Thank you so much.
Yeah, so we wanted to announce,
I'm Rex.
We're from u.com.
This is Oliver.
As you know, u.com brings transparency
and web access to our LLMs
to make sure that they're as accurate as possible.
So we wanted to give an award for the best use
of u.com's APIs to Transparify.
So congratulations.
If you guys want to stand up.
If you're here, there you guys are.
Thank you so much.
Transparify did an incredible job.
They were live streaming videos,
fact-checking them as they went,
using sources from the web,
and u.com's search APIs.
It was really incredible and powerful.
And Oliver will talk about our custom assistant.
Yeah, so for our best custom assistant,
we'd like to give that to events-.ai
with Oliver and Devesh.
So Oliver and Devesh, can you please stand up
if you're in the room?
Over there.
Yeah, so we were particularly impressed
by what they've built.
Essentially they handled booking, searching,
and even talking with customer agents on the phone.
And they used u.com in a way to actually find these events.
So we were incredibly impressed by them,
and can't wait to see what they'll do in the future.
Yeah, come find us after,
and we will give you your awards.
Thank you, Hume.
Thank you.
Thank you.
All right, I think we're going back to Hume now
with Zach Greathouse.
Welcome, Hume.
Nice round of applause, please.
Hi.
So first, just a huge thanks to Skydeck and Calhacks
for organizing this event and inviting us back
and to all the staff for running such a memorable event.
So I'm going to be announcing our three categories
for our track.
We have our best AI, our best empathic AI for social good,
best empathic AI for just most innovative empathic AI,
and then just the best overall.
As you can see, the team's here.
We've chosen Scam Scanner.
Can Scam Scanner, are you here?
Can you stand up?
All right, big applause.
For most innovative, we have BloomBuddy.
Where's BloomBuddy?
Can you stand up?
Yeah, okay, great job, you guys.
Talking to plants.
And then best use of empathic AI overall,
we chose Lock-In.
It's a personalized learning assistant.
Are you in the room?
Yeah, there we are.
Okay, congratulations, you guys.
Come meet us after outside.
Love to chat, take pictures.
And thank you so much.
Thank you to all the participants.
Yeah, we'll maybe see you next year.
So take care.
All right, thanks to you.
All right, and our last special prize is Grock.
There they are.
Please welcome Jose Menendez.
Hey, everyone.
Very nice to be here.
For those of you who haven't heard about Grock,
go to grock.com, experience the fastest inference on Earth
period.
All I have to say about Grock right now.
But our special Grock Star award today goes to Scam Scanner.
Where are you guys?
So these guys have a product that I want my mom to use today.
Right?
Monitor your call for potential scams.
Who doesn't want that for your mom, your uncles, and the whole thing.
Now, they get 1000 credits on Grock Cloud, which is many,
many millions of tokens.
There's two special mentions.
I have to read so I can screw up.
Three brown, one blue.
Where are you guys?
Another awesome solution.
These guys are generating on the fly incredible videos for math.
Something that I would use right now as well.
And Transverify.
Are you guys around here?
You've been mentioned as well.
Transverify, very cool.
Who doesn't want to hear a podcast with instant fact checking, right?
Am I right?
Now, my special surprise for the day.
I want to make a very special mention of Nathan Bog.
Are you around?
Nathan.
All right.
Nathan didn't use Grock.
So I'm going to give a special technical excellence award to Nathan
for a model he trained and ran on his CPU for doing very interesting
DOM operations corrections on the fly for front end.
Not only that, Nathan is invited officially to present his work in the
Grock HQ as soon as he can.
That's it guys.
I'm very impressed with all the work we saw.
Thank you very much.
Congratulations.
Thank you, Grock.
All right.
Our esteemed judges are back with their determination.
Please come back, judges.
Come back so we can all enjoy the grand prize.
Are you guys ready?
Do you have a guess?
Is there a voting?
Do we have voting tally?
Taking bets.
Everybody, I want you to guess your top two choices for grand prize.
And then I'm going to ask, who got it right?
Okay.
So as our wonderful judges take their seat.
All right.
We got some shout outs going here.
Any other shout outs?
Okay.
All right.
This audience is into it.
So as a reminder, the grand prize is a $25,000 investment from the
Berkeley Skydeck fund.
Also a golden ticket to our pad 13 program at Skydeck and a special
prize.
We are happy to announce that open AI is providing $2,500 in
credits for this winter.
So I think we're ready for the drum roll.
Take your guesses.
Only the judges know.
I don't know.
We're all about to find out.
It's dispatch AI.
Dispatch.
Where are you?
Come on up.
There's stairs right there.
Come on.
Come to the front stage.
There you go.
Thank you, judges.
I want to invite, while we invite dispatch up, I want to thank all of
you for coming.
I want to invite...
Spike.
Spike.
Yes.
From dispatch.
Yes.
Okay.
Here's the team.
There we go.
Dispatch AI grand prize winners.
Well done.
Well done.
I'd like to invite the Skydeck staff to come out and the Berkeley
Hackathon staff to come out.
Come on out.
They've been working all weekend.
I think some of them did not sleep at all.
Please give everyone who joined to make this a huge round of applause.
Thank you, everybody.
Thanks for joining us.
We will see you next year.
Thank you.

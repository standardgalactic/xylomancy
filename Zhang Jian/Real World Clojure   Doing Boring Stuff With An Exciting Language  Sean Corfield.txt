A lot of what we hear about closure is solving really hard problems and giant scale, and
I want to talk about not that, I want to talk about doing boring stuff, building web apps
and just using closure as a sort of general purpose language.
We get to hear about big data and analytics all the time.
We had a presentation on Storm this morning.
We haven't had presentations on core logic yet, I believe.
There's one this afternoon, is that right?
Lots of concurrency and massive scale subjects.
All hard problems.
And I think a lot of people out there get the impression that closure is really only
good for solving hard problems.
You don't need to use closure unless you've got some horribly complex giant scale problem.
We've got to remember that the rationale actually says it's meant to be a general purpose language.
And anywhere you can use Java, you should be able to use closure.
So we've taken that to heart at WorldSingles.
We're an internet dating platform, we're multilingual, we've got about half a dozen languages live
now.
We have 50 sites, technically we have two code bases.
We have a legacy code base, and we have a new code base that has closure in it.
And we're migrating all the sites over to it, which is a lot of fun.
If you haven't heard of WorldSingles, I won't be in the least bit offended.
Because we're all about little brands, we're mostly focused on ethnic verticals.
Not all of them, some of them are more specialist interest, but most of them are ethnic verticals.
So how did we come to use closure?
The company is actually historically a cold fusion shop.
How many people here have actually used cold fusion?
A surprising number of hands.
Okay, that amazes me.
Not, I didn't mean it like that.
Anyway, I joined WorldSingles in 2009, and one of the things that we needed to get done
was we have, you know, millions of member profiles, and we have a custom search engine
that we have to get all the data out of, transform it to XML, which is quite a complex transformation
with a lot of business rules, and post it off to the search engine.
And it pretty much has to run 24-7.
So we thought, well, we need to bring in something that's going to be fast that's just going
to run and run.
So we actually started to look at Scala, and we built a process in Scala, and it was fairly
actor heavy, and anyone who's used Scala's actors in the 2.7, sort of 2.8 time knows
that that was a bit problematic.
And in the end, Scala wasn't really a good cultural fit for our team.
We had people who were used to dynamic scripting languages.
So I started looking at Clojure in 2010.
My background originally, back in the 1980s, was functional programming, and I kind of
watched in dismay that it sat on the sidelines.
I'd always wanted functional programming to go mainstream.
When Haskell came out, I thought, great, here's a wonderful new language.
Everyone's going to pick it up.
We'll see people writing in Haskell, and of course that didn't happen.
When Scala came out, I was quite interested, because we have functional programming again.
We've got F sharp, Haskell's now getting a bit more attraction, and of course when
Clojure came out, I'm like, oh, good, a dynamic scripting language on the JVM that gives me
functional programming.
I tried out a few things.
It worked pretty well for some of our use cases, some small examples.
And we actually went to production on 1.3, Alpha 7, or Alpha 8 in the middle of last year.
And since then, we've just been doing a lot more Clojure.
How many people here are still on 1.2?
Okay, not very many.
We jumped to 1.3 as early as we could, so we've avoided a lot of the problems with the libraries.
That's a subject dear to my heart, and I'm happy to talk about that outside to anyone
who wants to listen, and anyone who has got some suggestions.
How did we introduce Clojure?
We'd already got an established code base.
So we needed to pick small, low-level components that we could start from the bottom and build
up.
We wanted to take components and reuse them so that if we built code out in other languages,
we could reuse it across those, as well as in our JVM-based CFML applications.
So we had to go down gen class for a couple of things.
We have tried, where possible, to do everything through Clojure Lang RT.
So we essentially load up all of the namespaces we're going to expose into our code dynamically
and just let Clojure get on with it.
That has the benefit that we can reload the application without downtime, and it just
picks up any new Clojure code and recompiles it.
So we can do deployments very easily.
We started the low-level.
We started with tools.logging and log4j, and that's where we had to gen class, because
we wrote our own appender, and environment control, because we have all these different
tiers and we wanted the configuration of the application to be automatic.
So those were things that we could pick on and move forward with and reuse in our tools.
Where we are today is we do quite a lot of stuff with Clojure.
Perhaps some surprising things here.
We generate a lot of rich HTML emails from the dating platform, and we're using NLive
to generate templates.
It's all multilingual, so we've actually written our I-18N handling code in Clojure
so that we can reuse that in CF and in the side components.
Geolocation, we use a mixture of databases and third-party services, and we've wrapped
all that up.
Logging I mentioned, persistence and the search engine interaction.
Now I've only got 25 minutes.
I'm not used to giving talks that are only 25 minutes.
So I will try very hard not to run long, but that means that I can't talk about a lot of
the things I'd really like to, because I'm really enthusiastic about what we're able
to do with Clojure.
So I'm going to look at just persistence and the search engine stuff, because that's a
mixture of fairly ordinary day-to-day stuff that Clojure actually has some nice libraries
and idioms for, as well as some interesting little problems.
How many people use Clojure Java JDBC?
Okay, so SQL Database is not hugely popular with the Clojure crowd, I guess.
As part of the movement to Clojure 1.3, one of the things that I needed was a JDBC library,
and Clojure Contrib SQL was sitting there on the old monolithic Contrib, and so I kept
poking and going, hi, can we get the SQL library moved over?
And the core folks said, yeah, we'll get to that.
It's on the list, and I'd come back and go, can we get the SQL library moved over, please?
And they said, do you want to do that?
And so I ended up maintaining it, taking it over from Stephen Gallardi.
And it's nice, but it's fairly low level.
So what we did was we wrote a little crud wrapper so that we can just get by ID, find
by keys, and so on.
And that has allowed us to write some fairly high-level code and work with the idioms of
getting sequences and maps back from JDBC and then just running functions over them
as we need them.
If you've used the JDBC library at all, you end up with this with connection and database
and with naming strategy.
It's a little ugly.
It's certainly a lot uglier than doing get by ID.
This API, I asked core, core approve all of the Contrib libraries to go to a 1.0 release,
and they have given a laundry list of things that need to change in Java JDBC before it
can go to a 1.0 release.
I think some of the points are kind of interesting.
If there are macros being used for things like this with connection, with naming strategy,
they want to see a pure functional version where you don't have dynamic binding going
on.
So it was a guideline for libraries.
Basically everything needs to be able to be done through pure functions.
So Java JDBC, for those of you who are using it, is going to get a new API in addition
to the current one that will be a lot more composable and a lot more functional.
The naming strategies are just ways of converting from keywords to entities in the SQL.
So for my SQL, you have back quotes around things, and we have plenty of column names
that are keywords in SQL, so we have to use that.
And for some reason, the default behavior of Java JDBC is to take the results that's
coming back from the database and lower case all of the columns by the time they come back
into your code as keywords, which sometimes is not very convenient.
So you can specify a naming strategy that says, please don't do that.
And when I get keywords back, I want them to be the identity of the column name.
So we have mixed case keywords.
We're also looking at moving some of our data to MongoDB.
Now, I got involved with Congo Mongo because, again, I wanted that on 1.3, and Congo Mongo
is a nice wrapper, again, fairly low level.
It turned out that the CRUD wrapper that we built for JDBC, for a lot of cases, would
work for MongoDB.
All of the gets, finds, deletes, those sort of things, apart from the pure SQL, would
allow us to work with MongoDB using the same API.
And this was definitely one of the big wins for us with Clojure because it was very, very
easy to just modify our data layer to say, oh, is this table really a MongoDB collection?
And if it is, then we'll do a fetch against Mongo.
Otherwise, we'll go to SQL and run the thing that you just saw.
And so now our application code doesn't care where our data is.
And in fact, when we manage the relationships between objects at the application layer,
we might have an object come back from MongoDB that has foreign keys into tables that are
in MySQL, that then has foreign keys back into something in MongoDB, and the application
just doesn't care.
The nice thing for that is that we are able to gradually migrate things from MySQL to
MongoDB.
I know there's an eHarmony guy here.
I was chatting to one of the eHarmony folks at ScalaLiftoff some time back.
And they were talking about moving from standard SQL databases to the NoSQL databases at around,
I think it was 3 million records, which is where we are and why we're moving.
So a generic application code can use these, as I say.
We're not very happy with the function API here, the little having a function that gets
passed in.
This is kind of a holdover from the Java JDBC with query results, which as a macro then
applies a function to or executes the body against the entire result set.
So because it has the open connection and you need it closed, you pretty much always
have to pass do all in there to realize it.
So we kind of went that way.
We're going to change this.
It seems really ugly to have to always have do all in there for a lot of things if you
just want the record set back.
And I know some people working with the JDBC library have kind of complained that you have
to jump through some hoops just to do things like get single records back or handful of
records.
So if anyone has any suggestions on that amongst the users, I would love to hear suggestions
for improving the API of closed Java JDBC.
The search engine we use is called Discovery.
It's by a company called Transparency, and it's really good for fuzzy searching.
It's improved over the years, but it has a fairly strange way of interacting with it
and that all data goes in via XML or it can pull it out of your database.
But now they have nice JSON queries.
So we decided that we would write the process that scans the database, goes through the
business rules, creates the XML, create that enclosure.
This was the code that we had in Scala.
We had about a thousand lines of Scala.
Active-based, it had memory leaks, so the process tended to fall over and we had to
keep restarting it.
And we'd gone through the whole 2.7 to 2.8 upgrade with Scala, which was very unpleasant.
I'm seeing some nodding faces in the audience.
So by the time we got to this, this was ironically what we'd introduced Scala to do, the heavy
lifting, and it's sort of the very thing that Clojure is always being praised for being
very good at, which is processing large amounts of data.
And it turned out to be relatively simple.
I like the CLJ HTTP library because it's very easy just to post to some endpoint.
This is a standalone process, so we're actually pulling options from tools CLI.
And to process the users, we run a hiccup, so you can see it's a change set there.
It's just part of the XML root of the document.
We map render users, render user across the set of users that have changed.
So again, nice and easy, we just write a function that renders users and we don't have to worry
about anything else and then just post the XML.
So that was very concise.
Render user has a lot of logic in it behind the scenes.
The overall structure of it says, if the user is excluded from search, then we'll send a
remove packet, otherwise we'll send a set item packet with all the properties.
And inside that, you can see we're just doing map render item across the dimensions.
So we were finding that we were able to write lots of small composable functions that were
easy to understand that really hadn't been as easy to do in a lot of other languages
and it performed well.
The query, again, nice simple stuff based around CLGA HTTP.
That looks like a lot of code.
The important part is this.
With the JSON library, take a closure map, which is our search criteria, just convert
it to JSON, post it to the search engine.
If we get a status 200 back, turn that result back into a data structure and then the rest
of the program merely goes along its way dealing with closure maps and vectors and so on.
This proved to be very nice to just be able to process all of our criteria going into
the search engine and our results coming back using map and filter and so on without a lot
of complex code that we'd had before with all those loops and all that imperative stuff.
Back in the context of this, if we actually get results back, we go and get all the users
from the database that match what we get back out of the search result and then we make
an ordered query so that the sequence of maps is in the same order as it comes back from
the search engine.
And then the error handling.
Again, post, using CLJ HTTP, very, very simple.
This time it's actually coming out of the application.
So this is where the environment control comes in.
We have m slash my settings and something that we found we were using a lot is essentially
singletons, where the values computed once the application is up and running and then
we need it cached.
And so after asking around on the IOC channel, someone said, well, why not just use a delay?
And it turned out to be a very nice, elegant solution to something that is otherwise a
bit of an ugly problem because we just say delay it until someone actually uses it.
We don't have to write any special access code.
We can just deref it whenever we need the values.
And again, a little bit of SQL.
I'm not going to go into much of the code around this.
As I say, we've got all of these pieces put together now.
We're finding that by having these things as small composable functions, we're able
to move away from what has essentially been a monolithic web application.
And certainly our legacy platform was a large enough monolithic web application that we
felt we had to rewrite it.
The new platform started out life heading towards monolithic web application.
But now we have closure in the mix.
We're moving much more to a lot of small processes that are able to perform tasks independently
and to communicate with each other if necessary out of the context of the application itself.
So it puts us in a better position for breaking the application up into things that will run
on small Amazon instances, for example, rather than large servers.
Things like the search engines still have to run on big servers with a lot of memory.
But more and more of the application is gradually moving into these small collaborating processes.
In the end, we don't actually use a huge number of libraries.
From Contra, we're only using the JSON, GIDVC, the CLI library, and the logging.
In a way, I kind of wish we were using more of Contra, because that would give me more
incentive to try and get some of the older Contra moved over.
It was one of the bonuses of coming in on the 1.3 curve of not having to deal with the
legacy of 1.1 and 1.2.
I know some people who are on those have said that the biggest obstacle to moving is Contra,
not so much the changes in the language itself.
A handful of closure libraries out there.
It's a weird mix.
There seem to be a lot of very small libraries, in some cases, doing similar things.
We use both CLJ time and date CLJ.
That was historical.
We kind of picked up date CLJ first, and I've actually ended up maintaining CLJ time.
Mark McGrathen didn't have enough time to work on it, and I volunteered, but it doesn't
just drop in and replace.
I think navigating that big space of the closure third-party libraries is pretty hard.
There's a lot of stuff on closure hours, there's a lot of stuff on GitHub, and finding the
right tool for the job is sometimes rather difficult.
The JSON library, I think there's three or four JSON libraries out there.
There's the standard one in Contra.
There are faster solutions out there that have fewer features.
There are others that have more features.
I'd be interested to hear from people on advice on how to navigate that, because right now
it seems to be a minefield.
We use a few bits of pieces from Java.
This was actually the first time I'd had to deal with the Java mail API.
It's horrible.
I couldn't believe what I had to go through to send a simple HTML email.
Fortunately, of course, now we have it wrapped up in a nice Clojure function, and we don't
have to worry about it.
Some of the Java libraries are horrible to work with.
Wrappers for those are definitely valuable.
We don't have a lot of closure yet.
Three and a half thousand lines isn't a huge amount, but we found that we've got an awful
lot done with it.
The thousand lines of Scala I mentioned for the publishing process, we ended up replacing
with 300 lines of closure that essentially followed the same algorithm.
It even had a lot of the same names of functions in it.
That was a bit of a surprise.
Someone posted it on Hacker News, and I then had to spend 48 hours defending my claims
that the Clojure was a similar structure and yet much more compact.
I will say that some of that was due to not having a decent SQL abstraction in Scala
for our code back then.
I don't know whether that's improved, but certainly there were some library issues there.
We have what I believe is quite a large amount of tests for the amount of code we've got.
We've got about a five to one ratio between production code and test code.
How many people here feel that they do as much TDD enclosure as they did in other languages?
That's kind of what I thought, very few hands.
For most people, you're not writing tests, you're just not writing as many tests, working
with Rappelmore rather than tests.
Making more tests.
You had a hand up at the back?
I don't know why, but the Clojure community has certainly got a bit of a reputation for
not doing a lot of unit testing.
There was Rich's talk, we're talking about guardrails, and that seems to have got taken
out of context, but it does seem that there isn't as much TDD being done.
I don't know whether that's just because everyone thinks the Rappel is better, but we seem to
have a reasonable amount.
Just yesterday, that was the first time I'd looked at expectations from J-Fields, and
I think we'll be switching over to that, because right now we're using Clojure.test, and we're
using line multi-test because we're testing against 1.4 so that we can move to it fairly
soon, and we'll switch to profiles in Leningen 2.
Anyone using Leningen 2 already?
A couple of hands.
For us, Clojure performs well.
It certainly performs well enough for what we're doing.
We actually don't use the Adobe cold fusion product.
We use a JBoss community project called Rylo, which is a very fast open source implementation
of CFML.
On the front end, we've already got about as fast as we can go.
The Clojure process that does all of the publishing does not run as fast as the Scala process,
but it runs longer, and that's kind of more important, and doesn't use as much memory.
The immutable data has provided us with a thread safety, which means we can just get
on with solving our problems and not having to worry about heavy concurrency and objects
and thread safety and objects.
That has proved to be a problem with some of the code in the past, and we are actually
using a third-party library for persistence, which we're moving away from, which has thread
safety problems.
That's a cold fusion problem you guys don't need to worry about, and certainly the maintenance
and flexibility is pretty good.
It drives us to create much smaller components, and we're finding that we're able to make
some fairly sweeping changes to our business rules, usually by just swapping out functions,
and for some processes, we will do things like pass in a map of functions to control
how it should behave in different circumstances, so the higher order functions and that side
of things prove to be really, really good for flexibility.
We're training more of our team.
We've actually got two of our team here, one of them took the CASCALOG course.
Our closure code to base is obviously growing, but it seems to grow a lot more slowly than
some of our other code, and we're looking at CASCALOG because we think that that will
help us do a lot of the analysis, the ad hoc analysis we need to do on the amount of data
we've got, and gradually we will be in that big data space that closure is so good at.
Actually hiring, which I think a lot of closure companies are, and that puts me at about 25
minutes, so that is my time.
I will take questions.
So the question was, what are strategies for dealing with a closure-contrib issue?
It's kind of interesting because there's a dichotomy between making contraband easier
for people to work on, by having it broken up into small pieces that can then move forward
on their own terms, yet some people feel that by having to have a contributors agreement
and having the name of the project controlled by core and the infrastructure with JIRA that
it's an obstacle to getting things done, I'm a little surprised.
If you go look at the where did closure-contrib page go on the wiki, I'm actually quite surprised
that there's so many pieces of old contraband that no one has stepped forward to maintain.
I would say if you are relying on an old contraband and you rely on it heavily and you want to
see it in the new contraband, step forward and at least talk to the author of it or volunteer
to be the maintainer.
No, I think part of the problem with trying to do those sort of benchmarks is actually
finding like things to compare because if you've got a dating platform, Web App isn't
going to look like something else unless you've got really common Web Apps that people are
building in lots of different languages, you're not going to be able to do that.
Well, thank you very much.
I will continue to take questions out in the hallway.

As you know, we're going to talk about deep learning and we're going to jump right in.
Much of practical applications of deep learning today, machine learning and AI in general,
are used a paradigm called supervised learning, which I'm sure most of you have heard of before.
This is the paradigm by which you train a machine by showing it examples of inputs and outputs.
You want to build a machine to distinguish images of cars from airplanes. You show it an image of a
car. If the machine says car, you don't do anything. If it says something else, you adjust the
internal parameters of the system so that the output gets closer to the one you want.
Imagine the target output is some vector of activities on a set of outputs. You want the
vector coming out of the machine to get closer to the vector that is the desired output. This works
really well. As long as you have lots of data, it works for speech recognition, image recognition,
face recognition, generating captions, translation, all kinds of stuff. This is, I would say, 95%
of all applications of machine learning today. There are two other paradigms, one of which I
will not talk about, one of which I will talk about a lot. The two other paradigms are reinforcement
learning, which I will not talk about. There are other courses. There's a course by Larry Pinto about
this that I encourage you to take. A third paradigm is self-supervised learning or unsupervised
learning. We will talk about this quite a lot in the following weeks. For now, let's talk about
supervised learning. Self-supervised learning, you could think of it as kind of a play on supervised
learning. The traditional model of pattern recognition machine learning and supervised
learning, certainly going back to the late 50s or early 60s, is the idea by which you take a
raw signal, let's say an image or an audio signal or a set of features representing an object,
and then you turn it into a representation using a feature extractor, which in the past was engineered,
and then you take that representation, which is generally in the form of a vector or a table of
numbers or some kind of tensor, a multidimensional array, but sometimes it could be a different
type of representation, and you feed that to a trainable classifier. This is the learning
where the learning takes part. This is the classical model, and it's still popular, it's still used a
lot, but basically what deep learning has done is replace this sort of manual hand engineering of
the feature extractor by a stack of trainable modules, if you want. In deep learning, the main
idea of deep learning and the only reason why it's called deep is that we stack a bunch of modules,
each of which transforms the input a little bit into something that's going to slightly higher
level of abstraction, if you want, and then we train the entire system end to end.
So I represented those sort of pinkish modules to indicate the ones that are trainable,
and the blue modules are the fixed ones, the hand engineered ones. So that's why deep learning is
called deep. We stack multiple layers of trainable things, and we train it end to end.
The idea for this goes back a long time. The practical methods for this go back to the
mid to late 80s with the back propagation algorithm, which is going to be the main
subject of today's lecture, actually. But it took a long time for this idea to actually percolate
and become the main tool that people use to build machine learning systems. It's only about 10 years
old. Okay, so let's go through a few definitions. So we're going to deal with parameterized models.
A parameterized model or learning model, if you want, is a parameterized function g of x and w,
where x is the input, and w is a set of parameters. I'm representing this here on the right with a
particular symbolism where a function like this that produces a single output, think of the output
as either a vector or matrix or a tensor, or perhaps even a scalar, but generally is multidimensional.
It can actually be something else in a multidimensional array, but something that,
you know, maybe like a sparse array representation or a graph, which values on it. But for now,
let's think of it just as a multidimensional array. So both the inputs and the outputs are
multidimensional arrays, what people call tensors. It's not really kind of the appropriate definition
of a tensor, but it's okay. And that function is parameterized by a set of parameters w. Those
are the knobs that we're going to adjust during training, and they basically determine the input
output relationship between, you know, between input x and the predicted output y bar. Okay, so
I'm not explicitly representing the wire that comes in with w. Here, I kind of assume that w
is somewhere inside of this module. Think of this as an object in object-oriented programming.
So it's an instance of a class that you instantiated and it's got a slot in it that
represents the parameters. And there is a forward function basically that takes as argument the
input and returns the output. Okay. So a basic learning machine will have a cost function
and the cost function in supervised learning, but also in some other
settings will basically compute the discrepancy, distance, divergence, whatever you want to call
it, between the desired output y, which is given to you from the training set and the output produced
by the system y bar. Okay, so an example of this, a very simple example of a setting like this is
linear regression. In linear regression, x is a vector composed of components x i's, w is also a
vector, and the output is a scalar that is simply the dot product of x with w. So y bar now is a
scalar and what you compute is the square distance, the square difference really between y and y bar.
If w is a matrix, then now y is a vector and you compute the square norm of the difference between
y and y bar. And that's basically linear regression. So learning will consist in finding the set of
w's that minimize this particular cost function average over a training set. I'll come to this
in a minute. But I want you to think right now about the fact that this g function may not be
something particularly simple to compute. So it may not be just multiplying a vector by matrix. It
may not be just carrying some sort of fixed computation with sort of a fixed number of steps.
It could involve something complicated. It could involve minimizing a function with respect to
some other variable that you don't know. It could involve a lot of iteration of some
algorithm that converges towards a fixed point. So let's not kind of restrict ourselves to
g of x w that are kind of simple things. It could be very complicated things. And we'll come to this
in a few weeks. Right. So this is just to kind of explain the notations that I will use during the
course of this class. So we have observed input and desired output variables. Those are kind of gray
grayish bubbles. Other variables that are produced by the system or internal to the system
are those kind of empty circle variables. We have deterministic functions. So functions that are
so they are indicated by this sort of rounded shape here. They can take multiple inputs,
have multiple outputs. And each of those can be tensors or scalars or whatever. And they have
implicit parameters that are tunable by training. And then we have cost functions. So cost functions
are basically functions that take one or multiple inputs and output a scalar. But I'm not representing
the output. It's implicit. Okay. So if you have a red square, it has an implicit output. And it's a
scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar
to what people use in graphical models. If you've heard what a graphical model is, particularly the
type of graphical model called a factor graph. So in a factor graph, you have those variable bubbles
and you have those factors, which are those square cost functions. You don't have this idea
that you had deterministic functions in it because graphical models don't care about the fact that
you have functions in one direction or another. But here we care about it. So we have this extra
symbol. Okay. So machine learning consists in basically minimizing finding the set of parameters
W that minimize the cost function averaged over a training set. So our training set is a set of
pairs x, x, y indexed by an index P. Okay. So we have P training samples and little P is the index
of the training set, the training sample. And our overall last function that we're going to have to
minimize is equal to the cost of the discrepancy between Y and the output of our model by bar
G of X, W, as I said earlier. So L is a value, C is a module, and L is a way of writing C of Y,
G of X, W, where it depends explicitly on X, Y, and W. Okay. But it's the same thing really.
Okay. The overall last function, which is this kind of curly L, is the average of the per
sample loss function over the entire training set. Okay. So compute L for the entire training set,
divide by some all the terms, divide by P, and that's the average, that's the loss. Okay.
So now the name of the game is trying to find the minimum of that loss with respect to the
parameters. This is an optimization problem. So symbolically, I can represent this entire graph
as the thing on the right. This is rarely used in practice, but this is sort of a way to visualize
this. So think about each training sample as a sort of identical copy of the replica, if you want,
of the model and the cost function applied to a different training sample. And then there is an
average operation that computes the loss, right? So everything you can write as a formula, you can
probably write in terms of those graphs. This is going to be very useful as we're going to see later.
Okay. So supervised machine learning and a lot of other machine learning
patterns as well actually can be viewed as function optimization and a very simple
approach to optimizing a function, which means finding the set of parameters to a function
that minimize its value, okay, is a gradient descent or gradient based algorithms. So a gradient
based algorithm makes the assumption that the function is somewhat smooth and mostly differentiable,
doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere
differentiable. And it has to be somewhat smooth. Otherwise, the local information of
the slope doesn't tell you much about where the minimum is. Okay. So here's an example here depicted
on the right. The lines that you see here, the peak lines are the lines of equal cost and this cost
is quadratic. So it's basically a kind of paraboloid. And this is the trajectory of a method called
stochastic gradient descent, which we'll talk about in a minute. So for stochastic gradient
descent, the procedure is you show an example, you run it to the machine, you compute the objective
for that particular sample, and then you figure out by how much and how to modify each of the knobs
in the machine, the W parameters, so that the objective function goes down by a little bit.
You make that change and then you go to the next sample. Let's be a little more formal.
So gradient descent is this very basic algorithm here. You replace the value of W by its previous
value minus a step size, eta here, multiplied by the gradient of the objective function with respect
to the parameters. So what is a gradient? A gradient is a vector of the same size as the
parameter vector. And for each component of the parameter vector, it tells you by how much
the last function L would increase if you increase the parameter by a tiny amount. Okay.
It's a derivative, but it's a directional derivative, right? So let's say among all the
directions, you only look at W34. And let's imagine that you tweak W34 by a tiny amount.
The last function, curly L is going to increase by a tiny amount. You divide the tiny amount by
which L increased by the tiny amount that you modified this W34. And what you get is the gradient
of the loss with respect to W34. If you do this for every single weight, you get the gradient
of the loss function with respect to all the weights. And it's a vector which for each component
of the weight gives you, or the parameter gives you that quantity. Okay. So since Newton and
Euler, it's been written as DL over DW because it indicates the fact that there is this little
twiddle, right? You can twiddle W by little. And there's a resulting twiddling of L. And if you
divide those two twiddles, and they are infinitely small, you get the derivative. That's kind of
standard notation in mathematics for a few hundred years. Okay. So now the gradient is going to be
a vector. Okay. And as indicated here on the top right, that vector is an arrow that points
upwards along the line of larger slope. Okay. So if you are in a 2D surface, you have two
W parameters. Okay. And the surface is represented here. Some sort of quadratic ball here in this
case. So it's a second degree polynomial in W1 and W0. Here on the right is the kind of a top
down view of this where the lines represent the lines of equal cost. The little arrow is here,
represent the gradient at various locations. Okay. So you have a long arrow if the slope is
steep. A short arrow if the slope is not steep, not large. At the bottom, it's zero.
And it points towards the direction of highest slope. All right. So imagine you are in a landscape,
a mountainous landscape. And you are in a fog and you want to go down the valley. You look around
you and you can tell the local slope of the landscape. You can tell where the minimum is
because you are in a fog. But you can tell the local slope. So you can figure out what is the
direction of larger slope and then take a step. And that will take you upwards, right? Now you
turn around 180 degrees, take a step in that direction. And that was going to take you downwards.
If you keep doing this and the landscape is convex, which means it has only one local minimum,
this will eventually take you down to the valley and presumably to the village.
Right. So that's gradient-based algorithms. They all differ by how you compute the gradient first
and by what this eta step size parameter is. So in simple forms, eta is just a positive constant
that sometimes is decreased as the system runs more, but most of the time not.
But in more complex versions of gradient-based learning, eta is actually an entire matrix itself,
generally a positive definite or semi-definite matrix. And so the direction adopted by those
algorithms is not necessarily the steepest descent. It goes downwards, but it's not necessarily the
steepest descent. And we can see why here. So in this diagram here that I'm showing,
this is the trajectory that will be followed by gradient descent in this sort of quadratic
cost environment. And as you see, the trajectory is not straight. It's not straight because the
system kind of goes down by following the slope of steepest descent. And so it goes down the valley
before finding the minimum of the valley, if you want. So if your cost function is a little
squeezed in one direction, it will go down the ravine and then kind of follow the ravine towards
the bottom. In complex situations where you have things that are, the trajectory actually is
being cut here, but where the function is highly irregular, this might even be more complicated
and then you might have to be smart about what you do here. Okay, so stochastic gradient descent
is universally used in deep learning. And this is a slight modification of the gradient steepest
descent algorithm where you don't compute the gradient of the entire objective function averaged
over all the samples. But what you do is you take one sample and you compute the gradient
of the objective function for that one sample with respect to the parameters and you take a step.
And you keep doing this, you pick another sample, compute the gradient of the objective function
for that sample with respect to the way it's making a date. Why is it called stochastic gradient?
Stochastic is a, you know, a fancy term for random, essentially. And it's called stochastic
because the evaluation of the gradient you get on the basis of a single sample is a noisy estimate
of the full gradient. The average of the gradients, because the gradient is a linear operation,
the average of the gradients will be the gradient of the average. And so things work out. If you
compute the gradient and you kind of keep going, overall, the average trajectory will be sort of
the trajectory you would have followed by doing full gradient. Okay. But in fact,
the reason we're doing this is because it's much more efficient in terms of speed of convergence.
So although the trajectory followed by stochastic gradient is very noisy, things kind of bounce
around a lot. As you can see in the trajectory here at the bottom, you know, things have, the
trajectory is very erratic. But in fact, it goes to the bottom faster. And it has other advantages
that people are still writing papers on. Okay. The reason for that is that stochastic gradient
exploits the redundancy between the samples. So all the, you know, machine learning setting,
the training samples have some similarities between them. If they don't, then basically the
learning problem is impossible. So they necessarily do have some redundancy between them. And the
faster you update the parameters, the more you, the more often you update them, the more you
exploit this redundancy between those parameters. Now in practice, what people do is they use
mini batches. So instead of computing the gradient on the basis of a single sample,
you take a batch of samples, typically anywhere between, let's say 30 and a few thousand.
The smaller batches are better in most cases, actually. And you compute the average of the
gradient over those samples. Okay. So compute the average cost over those samples,
and compute the gradient of the average over those samples, and then make an update. The
reason for doing this is not intrinsically an algorithmic reason. It's because it's a simple
way of parallelizing stochastic gradient on parallel hardware, such as GPUs. Okay. So there's
never, there's no good reason to do batching other than the fact that our hardware likes it.
Okay. Question. Yeah. So for actually, for, for real complex deep learning problems,
does the subjecting function has to be continuously differentiable?
Well, it needs to be continuous, mostly. If it's non-continuous, you're going to get in trouble.
It needs to be differentiable almost everywhere. But in fact,
neural nets that most people use are actually not differentiable. And there's a lot of places
where they're not differentiable. But they are continuous in the sense that there are functions
that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a
while, it's not too much of a problem. But so in that case, those quantities should not be called
gradients. They should be called subgradients. Okay. So a subgradient is basically a generalization
of the idea of derivative or gradient to functions that have, that have kinks in them.
So wherever you have a function that has a kink in it, any, any slope that is between the slope of
one, one side and the slope of the other side is a, is a valid subgradient. Okay. So when you're
at the kink, you decide, well, the derivative is this or it's that, or it's going to somewhere in
between and you're fine. Most of the proof that applied to, you know, smooth functions,
you know, in terms of minimization often apply also to non-smooth function that
basically are differentiable most of the way. So then how do we ensure strict convexity?
We do not ensure strict convexity. The, in fact, in deep learning systems,
most deep learning systems, the function that we are optimizing is non-convex.
All right. In fact, this is one reason why it took so long for deep learning to become prominent
is because a lot of people, particularly theoreticians, people who sort of theoretically
minded were very scared of the idea that you had to minimize a non-convex objective and say,
this can't possibly work because we can't prove anything about it. It does work. You
can't prove anything about it, but it does work. And so this is a situation and it's an
interesting thing to think about, a situation where the theoretical thinking basically limited
what people could do in terms of engineering because they couldn't prove things about it.
But that could be actually very powerful. Okay.
Yeah, like your colleague.
We want to optimize non-convex functions.
Like your colleague at the Bell Labs who didn't like the non-mathy.
It was a whole debate in the machine learning community that lasted 20 years, basically.
All right. So what about how doesn't SGD get stuck in local minima once it reaches them?
It does. Okay. So full gradient does get stuck in local minima. SGD gets slightly less stuck
in local minima because it's noisy. It allows it sometimes to escape local minima.
But the real reason why we're going to optimize non-convex functions and local minima
are not going to be such a huge problem is that there aren't that many local minima that are
traps. Okay. So we're going to build neural nets and those neural nets are deep learning systems
and they're going to be built in such a way that the parameter space is such a high dimension
that it's going to be very hard for the system to actually create local minima for us. Okay.
So think about a picture where we have in one dimension a cost function that has one local
minima and then a global minimum. Right. Okay. So it's a function like this. Right.
And we start from here. If we optimize using gradient descent, we're going to get stuck in
that local minimum. Now, let's imagine that we parameterize this function now with two parameters.
Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore.
We're looking at two dimensional function. We have an extra parameter.
This extra parameter will allow us to go around the mountain and go towards the valley,
perhaps without having to climb the little hill in the middle. Okay. So this is just an
intuitive example to tell you that in very high dimensional spaces, you may not have as much
of a local minimum problem as you have in the sort of intuitive picture of low dimensional spaces.
Right. So here, those pictures are in two dimensions. They are very misleading.
We're going to be working with millions of dimensions and you know, some of the most recent
deep learning systems have trillions of parameters. Yeah. So local minima is not going
to be that much of a problem. We're going to have other problems, but not that one.
So there is like a trend in this hyper over parameterization, right? It seems like that
more neurons we have and the better these networks work somehow.
That's right. So we're going to make those networks very large and they're going to be
over parameterized, which means they're going to have way more adjustable parameters than we
would actually need, which means they're going to be able to learn the training set almost
perfectly. And the big question is how well are they going to work on a separate validation set
that is separate from the training set? Or how well are they going to work in a real situation
where, you know, the distribution of samples may be different from what we train it on?
So that's the real question of machine learning, which I'm sure a lot of you are familiar with.
Two more questions. Can we do? Yep. So how do we escape instead of subtle points?
Right. So there are tons and tons of subtle points in deep learning systems.
A conveniently large number of subtle points, as a matter of fact.
I'll have a lecture on this. So I don't want to kind of spend too long. And so
but yeah, there are subtle points. The trick with subtle points is you don't want to get too
close to them, essentially. And stochastic gradient helps a little bit with subtle points.
Some people have proposed sort of explicit methods to stay away from subtle points.
But in practice, doesn't seem to be that much of a problem, actually.
Finally, how do you pick samples for stochastic gradient in the center randomly?
Okay. There is lots of different methods for that. Okay. Yeah, I mean, the basic thing they
should do is you have your training set, you shuffle the samples in a random order. Okay. And
then you just pick them one at a time. And then you so you cycle through them.
An alternative is once you get to the end, you reshuffle them and then cycle to them again.
An alternative is you you pick a random sample using a, you know, a random number.
Every time you pick a new sample, you pick them randomly.
The if you do batching, the a good idea is to put in a batch samples that are
maximally different from each other. So things that are, for example, different categories
if you do classification. But most people just do them, you know, just pick them randomly.
But it's good to have samples that are maximally different, that are nearby,
either in a batch or during the process or training. And then there are all kinds of tricks
that people use to sort of emphasize difficult samples. So that the boring, easy samples are not,
you don't waste your time just, you know, seeing them over and over again.
There's all kinds of tricks, right? But, you know, the simple one is, which most people use,
you shuffle your samples, and you run through them. A lot of people now use also data augmentation.
So every sample is actually distorted by some process. For an image, you can have, you know,
distorted geometry a little bit, you change the colors, you add noise, etc. This is a
artificial way of sort of adding more samples than you actually have. And people do this kind
of randomly on the fly, or they can pre-compute those transformations. So lots of tricks there as
well. Last question, how do you pick the batch size? The batch size. That's determined by your
hardware. So if you have a GPU, generally for, you know, reasonably sized networks, your batch
size would be anywhere between 16 and 64, something like that. For smaller networks, you might have
to batch more to kind of exploit your hardware better, to kind of have maximum usage of it.
If you parallelize on multiple GPUs within a machine, you may have to have, you know,
so let's say you have eight GPUs, then you'll be sort of eight times 32, so there's no 256 or
something. And then, you know, a lot of the big guys kind of parallelize that over multiple machines,
each of which has eight GPUs, some of them have TPUs, whatever. And then you might have to parallelize
over thousands of examples. This diminishing return in doing this, when you increase the size
of the batch, you actually reduce the speed of convergence. You accelerate the calculation,
but you reduce the speed of convergence. So at some point, it's not worth increasing your batch
size. So if we are doing a classification problem with K classes, what's going to be like our
goal to batch size? So there are papers that say if your batch size is significantly larger than
the number of categories, or let's say twice the number of categories, then you're probably
wasting computation, essentially, going down convergence. So you're trying to train an image
recognizer on ImageNet. If your batch size is larger than about a thousand, you're probably
wasting time. Okay, that's it. Thanks. I mean, you're wasting competition, you're not wasting
time. Okay, okay. Okay, so let's talk about traditional neural net. So a traditional neural net
is a model, a particular type of parametrized function, which is built by stacking linear and
nonlinear operations. Right, so here is a depiction of a traditional neural net here,
in this case with two layers, but I'm not imagining there might be more layers here.
So you have a bunch of inputs here on the left. Each input is multiplied by a weight,
different weights, presumably, and the weighted sum of those inputs by those weights is computed
here by what's called a unit or neuron. People don't like using the word neuron in that context
because there are incredibly simplified models of neurons in the brain, but that's the inspiration,
really. Okay, so one of those units just computes a weighted sum of its inputs,
using those weights. Okay, this unit computes a different weighted sum of the same inputs with
different weights and etc. So here we have three units here in the first layer. This is called a
hidden layer, by the way, because it's neither an input nor an output. Right, this is the input,
and this is the output, and this is somewhere in the middle. So we compute those weighted sums,
and then we pass those weighted sums individually through a a nonlinear function.
So here what I've shown is the value function. So this is called rectified linear unit.
This is the name that people have given it in the neural net lingual. In other contexts,
this is called a half wave rectifier, if you're an engineer. It's called positive part if you are
a mathematician. Okay, basically it's a function that is equal to the identity when its argument
is positive, and it's equal to zero if its argument is negative. Okay, so very simple graph.
And then we stack a second layer of the same thing, the second stage, right? So again, a layer of
linear operations where we compute weighted sums, and then we pass a result to nonlinearities. And
we can stack many of those layers, and that's basically a traditional plain vanilla garden
variety neural net. In this case, fully connected. So fully connected neural net means that
every unit in one layer is connected to every unit in the next layer. And you have this sort of
well organized layer, what layer are architectures, if you want, right? Each of those weights are
going to be the things that our learning algorithm is going to is going to tune. And the big trick,
the one trick really of deep learning is how we compute those gradients.
Okay, so if you want, if you want to write this, you can say the weighted sum number i,
so you can you can give a number to each of the units in the in the network.
So this unit with number i, and the weighted sum s of i is simply the sum where j goes over the
upstreams, the set of upstream units to i, which maybe all the units in the previous
layer or not could be just a subset. Okay. You and then you compute the product of zj,
which is the output of the unit number j times wij, which is the weight that links
unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum,
you pass it through the activation function, this value or whatever it is that you use,
and that gives you zi, which is the activation for unit i. Okay, simple notation. By changing the
set of upstream units of every unit, by building a graph of interconnection, you can basically
build any kind of network arrangement that you want. There is one constraint that we can lift,
that we will lift in a subsequent lecture, which is that the the graph has to be
a ac click in the sense that it can't have loops. Okay, if you have loops, that means you can't
organize the units in layers, you can sort of number them in a way that you can compute them
so that every time you want to compute a unit, you already have the state of the previous units.
If there are loops, then you know, you can do that, right? So for now, we're going to assume that
the wij matrix, the w matrix doesn't have loops,
represents a graph that doesn't have loops. That's that's what I should say. Okay, so here's sort of
intuitive explanation of the back propagation algorithm. So the back propagation algorithm
is the main technique that is used everywhere in deep learning to compute the gradient of
a cost function, whatever it is, objective function, with respect to a variable inside of
the network, this variable can be a state variable like a z, or an s, or it could be a parameter
variable like a w. Okay, and we're going to need to do both. Okay, so this is going to be an intuitive
explanation. And then after that, there's going to be a more mathematical explanation, which is less
intuitive, but perhaps actually easier to to understand. But let me start with the intuition
here. So let's say we have a big network. And inside of this big network, we have one of those
little activation functions. Okay, in this case, it's a sigmoid function, but it doesn't matter
what it is for for now. Okay, this function takes an s and produces a z. We call this function h of
h of s, right? So when we, when we wiggle z, the cost is going to wiggle by some quantity,
right? And we divide the wiggling of z by the wiggling of the wiggling of c by the wiggling
of z that causes it, that gives us the partial derivative of c with respect to z. So this one
term is a gradient of c with respect to all the z's in the network. And there's one component
of that gradient, which is the partial derivative of of the cost with respect to that single variable
z inside the network. Okay, and that really indicates how much c would wiggle if we regaled z
by some some amount, we divide the wiggling of c by the wiggling of z and that gives us a partial
derivative of c with respect to z. This is not how we're going to compute the the gradient of c
with respect to z, but this is a description of what it is conceptually, okay, or intuitively
rather. Okay, so let's assume that we know this quantity. So we know the partial derivative
of c with respect to z. Okay, so c with respect to z is this quantity here, dc over dz. Okay,
so think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other,
and you get the partial derivative of c with respect to z.
What we have here is, what we have to apply is the chain rule, the rule that tells us how to
compute the derivative of a function composed of two individual functions that we apply one after
the other, right? So remember chain rule, if you have a function g that you apply to another function
h, which is function of parameter s, and you want the derivative of it, the derivative of that is
equal to the derivative of g at point h of s multiplied by the derivative of h at point s,
right? That's chain rule. You know that a few years ago, hopefully. Now, if I want to write
this in terms of partial derivative, it's the same thing, right? Partial derivative is just a
derivative just with respect to one single variable. So I would write this something like this,
dc over ds. So c really is the result of applying this h function to s, and then applying some
unknown g function to compute c, okay, which is kind of the rest of the network plus the cost.
But I'm just going to call the gradient. I'm going to assume that this dc over dz is known,
okay? Someone gave it to me. So this is this variable here on the right. dc over dz is given
to me, and I want to compute dc over ds. So what I need to do is write this, dc over ds equal dc
over dz times dz over ds, right? And why is this identity true? It's because I can simplify by dz.
It's as simple as this, right? So you have, you know, trivial algebra, you have dz at the denominator
here, dz at the numerator here, simplify, you get dc over dz, okay? It's a very trivial,
simple identity which is basically just generally applied to partial derivatives.
Now, dc over ds, we know what it is. It's just h prime of s, okay? Just the derivative of the h
function, okay? So we have this formula dc over ds equal dc over dz, which we assume is known,
times h prime of s. What does that mean? That means that if we have this component of the gradient
of the cost function with respect to z here, we multiply this by the derivative of the h function
at point s, the same point s that we had here. And what we get now is the gradient of the cost
function with respect to s. Now, here's the trick. If we had a chain of those h functions,
we could keep propagating this gradient backwards by just multiplying by the derivative of all those
h functions going backwards. And that's why it's called back propagation, okay? So it's just a
practical application of a chain rule, right? And if you want to convince yourself of this,
you can run through this idea of perturbation. If I twiddle s by some value, it's going to twiddle z
by some value equal to ds times h prime of s, basically the slope of s, right? So dz equals
h prime of s times ds, okay? And then I'm going to have to multiply this by
dc over dz. And so I rearrange the terms, and I get immediately that this formula,
dc over ds equals dc over dz times h prime of s. Okay, so we had another element in our
multilayer net, which was the linear sum. And there, it's just a little bit more complicated,
but not really. Okay, so one particular variable z here, we'd like to compute the
derivative, the partial derivative of our cost function with respect to that z, okay?
And we're going to assume that we know the partial derivative of s with respect to each of those s's,
okay, the weighted sums at the next layer that z is going into, okay? So z only influences c
through those s's, okay? So presumably by knowing, by basically multiplying how each of those s's
influence c, and then multiplying by how z influences each of the s's and summing up,
we're going to get the influence of z over c, right? And that's the basic idea. Okay, so here's what
we're going to do. Let's say we perturb z by dz. This is going to perturb s0 by dz times w0,
okay? We multiply z by w0, so the derivative of this linear operation is the coefficient itself,
right? So here, the perturbation is, which is ds0 is equal to dz times w0, okay? And now in
turn, this is going to modify c, and we're going to multiply this quantity by dc over ds0 to get
the dc, if you want, okay? Now, whenever we perturb z, it's not going to perturb just s0,
it's also going to perturb s1 and s2. And to see the effect on c, we're going to have to sum up the
effect of the perturbation on each of the s's and then sum them up to see the overall effect on c.
So this is written here on the left. The perturbation of c is equal to
the perturbation of s multiplied by the partial derivative of c with respect to s
plus the perturbation of s1 multiplied by the partial derivative of dc with respect to s1
plus same thing for s2, okay? So this is the fact that we need to take into account all the
perturbations here that z may influence. And so I can just write down now a very simple thing,
you know, because dc of 0 is equal to w0 times dz and, you know, ds of 2 is w2 times dz, I can
plug this in there and just write dc over dz equals dc over ds0, which I assume is known,
times w0, plus dc over ds1 times w1, plus dc over ds2 times w2, okay? If I want to represent
this operation graphically, this is shown on the right here, I have dc over d0, dc over ds1, dc
over ds2, which I assume are known, are given to me somehow. I compute dc over ds0 multiplied by
w0, I multiply dc over ds1 by w1, dc over ds2 by w2, I sum them up and that gives me dc over dz,
okay? It's just the formula here, okay? So here's the cool trick about back propagation
through a linear module that computes weighted sums. You take the same weights and you still
compute weighted sum with those weights, but you use the weights backwards, okay? So whenever you
had the unit that was sending its output to multiple outputs, to multiple units through a weight,
you take the gradient of the cost with respect to all those weighted sums and you compute
their weighted sum backwards using the weights backwards to get the gradient with respect to
the state of the unit at the bottom. You can do this for all the units, okay? So it's super simple.
Now, if you were to write a program to do backprop for classical neural nets in Python,
it would take like half a page. It's very, very simple. It's one function to compute weighted
sums going forward in the right order, another function and applying the nonlinearity.
Here's another function to compute weighted sums going backward and multiplying by the derivative
of the nonlinearity at every step, right? It's incredibly simple. What's surprising is that
it took so long for people to realize this was so useful, maybe because it was too simple.
Okay, so it's useful to write this in matrix form. So really, the way you should think about a neural
net of this type is each state inside the network, think of it as a vector. It could be a multi-dimensional
array, but let's think of it just as a vector. A linear operation is just going to multiply this
vector by matrix and each row of the matrix contains all the weights that are used to compute
a particular weighted sum for a particular unit, okay? So multiply this by this matrix.
So this dimension has to be equal to that dimension, which is not really well depicted here,
actually. One second. From the previous slide, you wrote ds0. What is s,
differentiated with respect to? Okay. So there is a ds. What is ds, basically?
ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0, okay? An infinitely small perturbation of s0.
Doesn't matter what it is, okay? And what we're saying here is that if you have an infinitely
small perturbation of s0 and you multiply this perturbation by the partial derivative of c with
respect to s0, okay? You get the perturbation of c, except that that corresponds to this
perturbation of s0, right? But we're not interested in just the perturbation of s0. We're
sure interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum
of the perturbations of s0, s1, and s2 multiplied by their corresponding partial derivative of c
with respect to each of them, okay? You know, it's a virtual thing, right? It's not an existing
thing you're going to manipulate. Just imagine that there is some perturbation of s0 here,
okay? This is going to perturb c by some value, and that value is going to be the perturbation
of s0 multiplied by the partial derivative of c with respect to s0, okay? And then if you perturb
s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously,
you're also going to cause a perturbation of c. The overall perturbation of c will be the sum
of those perturbations, and that is given by this expression here. Now those d, those infinitely
small quantities, ds, dc, et cetera, think of them as, you know, numbers. You can do algebra
with them. You can divide one by the other, you know, you can do stuff like that. So now you say,
you know, what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0
by ds0, okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz,
because s is the result of computing the product of z by w0, then the perturbation is also going
to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going to be
equal to dz times w0. And this is what's expressed here, okay? ds0 equal w0 dz. Okay, now if I take
this expression for ds0 and I insert it here in this formula, okay, I get dc equal w0 times dz
times dc over ds0 plus same thing for 1 plus same thing for 2. And I'm going to take the dz
and pass it to the other side. I'm going to divide both sides by dz. So now I get dc over dz
equal, the dz doesn't appear anymore because it's been put underneath here, is w0 times dc over ds0
plus w1 times dc over ds1, et cetera. Okay, it's just simple algebra. It's differential calculus,
basically. Right, so it's better to write this in matrix form. So really, when you're computing,
if I go back a few slides, this is really kind of a matrix of all the weights that are kind of
upstream of the zj's. So you can align the zj as a vector, maybe only the zj's that have a nonzero
term, nonzero terms in w, wij. And then you can write those w's as a matrix, and this is just
a matrix vector product, okay? So this is the way this would be written. You have a vector,
you multiply by matrix, you get a new vector, pass that through nonlinearities, reuse,
multiply that by matrix, et cetera, right? So symbolically, you can write a simple neural net
this way. We have linear blocks, okay, linear functional blocks, which basically take the
previous state and multiply by matrix, okay? So you have a state here, z1, multiply by matrix,
you get w1, z1, and that gives you the vector of weighted sums, s2, okay? Then you take that,
pass it through the nonlinear functions, each component individually, and that gives you z2,
right? So that's a three-layer neural net. First weight matrix, nonlinearity, second weight
matrix, nonlinearity, third weight matrix, and this is the output. There are two hidden layers,
three layers of weights. Okay, the reason for writing it this way is that this is like symbolically
the easiest way to understand really what kind of backprop does. And in fact, it corresponds also
to the way we define neural nets and we run them on deep learning frameworks like PyTorch.
So this is the sort of object-oriented version of defining a neural net in PyTorch.
We're going to use predefined class, which are the linear class that basically multiplies a vector by
matrix. It also has biases, but let's not talk about this just now. And another class, which is
the value function, which takes a vector or a multidimensional array and applies the
nonlinear function to every component separately. Okay, so this is a little piece of Python
program that uses Torch. We import Torch. We make an image, which is, you know, 10 pixels by 20
pixels and three components for color. We compute the size of it and we're going to plug a neural
net where the number of inputs is the number of components of our image. So in this case that
would be 600 or so. And we're going to define a class. The class is going to define a neural net
and that's pretty much all we need to do here. So we define our network architecture. It's a
subclass of neural net module, which is a pretty fine class. It's got a constructor here that will
take the sizes of the internal layers that we want, the size of the input, the size of S1 and Z1,
the size of S2 and Z2, and the size of S3. We call the parent class initializer. And then we just
create three modules that are all linear modules. And we need to kind of store them somewhere because
they have internal parameters. So we're going to have three slots in our object, N0 and 1 and 2,
module 1, module 0, module 1, module 2. And each of them is going to be an instance of the class
NN.linear with two sizes, the input size and the output size. Okay, so first module has input size
D0, output size D1, etc. And those classes are, since there is a capital L, means it's an object
and inside there are parameters inside that item there. Right. So for example, the value
doesn't have a capital because it doesn't have internal parameters. It's not kind of a trainable
module. It's just a function. Whereas those things with capitals, they have sort of internal
parameters, the weight matrices inside of them. So now we define a forward function, which basically
computes the output from the input. And the first thing we do is we take the input thing,
which may be a multi-dimensional array, and we flatten it. We flatten it using this
idiomatic expression here in PyTorch. And then we apply the first module to X. We put the result
in S1, you know, which is a temporary variable, local variable. Then we apply the value to S1,
put the result in Z, then apply the second layer, put the result in S2,
apply the value again, put the result in S3. And then the last linear layer, put the result in S3
and return S3. And there is a typo. So the second line should have been S1. It's the self.m0 of Z0,
right? Z0 here. Yes, correct. I know. Yeah, this is something that is going to be fixed, right?
Which I didn't fix. I know. This is Z0. Thanks for reminding me of this.
Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this,
and you'll be able to run them yourself. That's all you need to do. You don't need to write
a, you know, how you compute the back prop, how you propagate the gradients.
You could write it, and it would be as simple as forward. You could write a backward function,
and it would basically, you know, multiply by the matrices going backwards, but you don't
need to do this because PyTorch does this automatically for you. When you define the forward
function, it knows what modules you've called in what order, what are the dependencies between
the variables, and it will know how to generate the functions that compute the gradient backwards.
So you don't need to worry about it, okay? That's the magic of PyTorch, if you want. That's a bit
the magic of deep learning, really. That's called automatic differentiation,
and this is a particular form of automatic differentiation. There's another way to write
functions in PyTorch that are kind of more functional, so you're not using modules
with internal parameters. You're just coding functions one after the other, and PyTorch
has a mechanism by which it can automatically compute the gradient of any function you define
with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L,
like the nn.capital linear inside is going to have a lowercase linear, which is like the functional
part, which is performing the matrix multiplication between the weights stored inside the object
with the capital L, and then the input, right? So every capital letter object will inside have
the functional way. So one can decide to use either the functional form by default or use this
encapsulated version, which are more convenient to just use, right? Right. So in the end, you can
create an instance of this class. You can create multiple instances, but you can create one here,
which is called minet, and give it the sizes you want. And then to apply this to a particular image,
you just do out equal model of image. That's as simple as that. Okay, so this is your first neural
net, and it does all the backup automatically. But you need to understand how that works,
right? It's not because PyTorch does it for you that you can sort of forget about how you actually
compute the gradient of a function, because it's inevitable that at some point you're going to want
to actually assemble a neural net with a module that does not pre-exist, and you're going to have
to write your own backup function. So to do this, you basically have, if you want to create a new
module with some complex operation that does not pre-exist in PyTorch, then you do something like
this. You define a class, but you write your own backward function, basically. Okay, so let's
kind of get one step up in terms of abstraction, and write this in sort of slightly more kind of
generic form, mathematical form, if you want. Okay, so let's say we have a cost function here,
and we want to compute the gradient of this cost function with respect to a particular
vector in the system. Zf could be a parameter, it could be a state, doesn't matter. Okay, some
states inside. And we have chain rule, and chain rule is nothing more than this that I explained
earlier. dc over dzf is equal to dc over dzg, dzg over dzf, as long as c is only influenced by
dzf through dzg, there is no other way for dzf to influence c than to go through dzg,
then this formula is correct. Okay, and of course the identity is trivial because it's just a
simplification by this infinitesimal vector quantity, dzg. Okay, so let's say dz is a vector of size
dg by one, so this means column vector, okay, and dzf is a column vector of size df.
This is, if you want to write the correct dimensions of this,
you know, we get something a little complicated. Okay, so first of all, this object here, dzg over
dzg over dzf. Well, let me start with this one. Okay, this one, dzg over dzg, that's a gradient
vector. Okay, dzg is a vector, dzg over dzg is a gradient vector. And it's the same size as dzg,
but by convention, we actually write it as a line as a row vector. Okay, so this thing here is going
to be a row vector whose size is the same size as dzg, but it's going to be horizontal instead of
vertical. Okay, this object here is something more complicated. It's actually a matrix.
Why is it a matrix is because it's the derivative of a vector with respect to another vector. Okay,
so let's look at this diagram here on the right. We have a function g, it takes dzf as an input
and it produces dzg as an output. And if we want to capture the information about the derivative
of that module, which is this quantity here, dzg over dzf, there's a lot of terms to capture
because there's a lot of ways in which every single output, every component of dzg can be
influenced by every component of dzf. So if for every pair of components, dz and dzf,
there is a derivative term, which indicates by how much dz would be perturbed if I perturbed dzf
by a small infinitesimal quantity. Right, we have that for every pair of components of dzf.
As a result, this is a matrix whose dimension is the number of rows is the size of dz and the
number of columns is the size of dzf. And each term in this matrix is one partial derivative term,
so this whole matrix here, if I take the component ij, it's the partial derivative of the ith
output of that module, the ith component of dz, with respect to the jth component of dzf.
Okay, so what we get here is a row vector is equal to a row vector multiplied by a matrix
and the sizes kind of work out so that they're compatible with each other.
Okay, so what is back propagation now? Back propagation is this formula.
Okay, it says if you have the gradient of some cost function with respect to
some variable and you know the dependency of these variables with respect to another variable,
you multiply this gradient vector by that Jacobian matrix and you get the gradient
vector with respect to that second variable. So graphically here on the right, if I have
the gradient of the cost with respect to zg, which is dc over dzg, and I want to compute
the gradient of c with respect to zf, which is dc over dzf, I only need to take that vector,
which is a row vector, multiply it by the Jacobian matrix, dg over dzf, or dzg over dzf,
and I get dc over dzf. Okay, it's this formula. Someone is objecting here, isn't the summation
missing here? Which summation? A summation of all the components of these partial multiplications.
Here? Yeah. Well, this is a vector, this is a vector, this is a matrix, there's a lot of
sums going on here because when you compute the product of this vector with its matrix,
you're going to have a lot of sums, right? Yep, so it's hidden, right? Yeah, the sums are hidden.
Inside of this vector matrix product.
Like, you can take a specific example. Let's imagine that this g function is just a matrix
multiplication, okay? We just multiply by zf by matrix w. So we have a linear operation. The
derivative of the Jacobian matrix of the multiplication by matrix is the transpose of
that matrix. So what we're going to do here is take this vector, multiply by the transpose of
the w matrix, and what we get is that vector. Okay? And it all makes sense, right? The sizes
make sense. This matrix here is the transpose of the weight matrix, which of course had the
reverse size. We multiply it, we pre-multiply it by the row vector of the gradient from the
layer above, and we get the gradient with respect to the layer below.
Okay? So backpropagating through a linear module just means multiplying the transpose
of the matrix used by that module. And it's just a generalized form of what I explained earlier,
you know, of propagating through the weights of a linear system. But it's less intuitive, right?
Okay, so we're going to be able to do backpropagation by computing gradients all the way through,
by propagating backwards. But this module really has two inputs. It has an input, which is
ZF, and the other one is WG, the weight matrix, the parameter vector that is used inside of this
module. So there is a second Jacobian matrix, which is a Jacobian matrix of ZG with respect to
the terms of this weight parameter. Okay? And to compute the gradient of the cost function with
respect to those weight parameters, I need to multiply this gradient vector by the Jacobian
matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with
respect to the input. It's a different Jacobian matrix. I'll come back to this in a second.
So to do backprop, again, if we have a vector of gradients of some cost with respect to a state,
and we have a function that is a function of one or several variables, we multiply this gradient
by the Jacobian matrix of this block with respect to each of these inputs, and that gives us the
gradient with respect to each of the inputs. And that's going to be expressed here. So this is the
backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is the state
of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to the layer
above times the Jacobian matrix of the state of layer K plus one with respect to the state of
layer K. Now we assume DC over DZK plus one is known, and we just need to multiply with the
Jacobian matrix of the function that links ZK to ZK plus one. The function is used to compute
ZK plus one from ZK. And this may be a function also with some parameters inside. But here,
that's the matrix of partial derivatives of F, which is with output to ZK plus one with respect
to each of the components of ZK. So that's the first rule of backpropagation. And it's a
recursive rule, so you can start from the top. You start initially with DC over DC,
which is one, which is why I have this one here on top. And then you just keep multiplying by
the Jacobian matrix all the way down. And backpropagation is gradients, and now you get gradients
with respect to all the states. You also want the gradients with respect to the weights because
that's what you need to do learning. So what you can write is the same chain rule, DC over DWK
is equal to DC over D ZK plus one, which we assume is known,
times D ZK plus one with DWK. And you can write this as DC over D K plus one. And the dependency
between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the
function, the output of the function ZK with respect to WK, and that gives you another
Jacobian matrix. And so with those two formulas, you can do backpropagation just about anything.
Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and
JAX and whatever. It's something like this where you have, so let's take a very simple
diagram here where you have an input parameterized function that computes an output that goes to
a cost function. And that cost function measures the discrepancy between the output of the system
and the desired output. So you can write this function as C of G of W. I didn't put the X here,
but just for charity. And the derivative of this is, again, you apply chain rule or you can write
it with partial derivatives this way. And same for, you know, expand the dependency of the output
with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar,
then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph.
So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take
the value one, which is the derivative of C with respect to itself, basically,
the loss with respect to itself. I'm going to multiply this by the derivative of the cost with
respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the
same as this because I've just multiplied by one. Then multiply this by the Jacobian matrix of G
with respect to W, which is a derivative if W is a scalar. That, of course, depends on X.
And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing
operations to compute the gradient. And there is essentially an automatic way of transforming
a compute graph of this type into a compute graph of this type that computes the gradient
automatically. And this is what, this is the magic that happens in the automatic differentiation inside
PyTorch and TensorFlow and other systems. Some systems are pretty smart about this in a sense
that those functions can be fairly complicated. They can involve themselves computing derivatives
and stuff. And they can involve dynamic computation, where the graph of computation depends on the
data. And actually PyTorch handles this properly. I'm not going to go through all the details of
this, but this is kind of a way of reminding you what the dimensions of all those things are, right?
So if Y is a column vector of size M, W is a column vector of size N, then this is a row
vector of size N, this is a row vector of size M, and this is a Jacobian matrix of size M by N.
And, you know, all of this works out.
Okay, so the way we're going to build neural nets, and I'll come back to this in a subsequent lecture,
is that we are going to have at our disposal a large collection of basic modules,
which we're going to be able to arrange in more or less complex graphs
as a way to build the architecture of a learning system. Okay, so either we're going to write a
class or we're going to write a program that, you know, runs the forward pass. And this program is
going to be composed of basic mathematical operations, addition, subtraction of tensors or
multidimensional arrays, you know, other types of scalar operations, or the application of
one of the predefined complex parameterized functions, like a linear module, a value,
or things like that. And we have at our disposal a large, you know, library of such modules,
which are things that, you know, people have come up with, you know, over the years,
that are kind of basic modules that are used in a lot of applications. Right, so the basic
things that we've seen so far, I think, is like values. There's other nonlinear functions like
sigmoids and variations of this, there's a large collection of them. And then we have cost functions
like square error, cross entropy, hinge loss, ranking loss, and blah, blah, blah, which I'm
not going to go through now, but we'll talk about this later. The nice thing about this
formalism is that, as I said before, you can sort of compute graphs, you can run,
you can construct a deep learning system by assembling those modules in any kind of arrangement
you want, as long as there is no loops in the connection graph. So as long as you can come
up with a partial order in those modules that will ensure that they are computed in the proper way.
Okay, but there is a way to handle loops, and that's called recurrent nets, we'll talk about this
later. Okay, so here's a few practical tricks if you want to play with neural nets, and you're
going to do that soon enough, perhaps even tomorrow. So, and these are kind of a bit
of a black art of deep learning, which is sort of a lot of it is implemented already in things
like PyTorch if you use standard tools, but some of it is kind of more of the sort of oral
culture if you want of the deep learning community. You can find this in papers, but it's a little
difficult to find sometimes. So most neural nets use values as the main nonlinearity,
so this sort of half wave rectifier, hyperbole tangent, which is a similar function and logistic
function, which is also a similar function, are used, but not as much, not nearly as much.
You need to initialize the weights properly. So if you have a neural net and you initialize
the weights to zero, it never takes off. It will never learn. The gradients will always be zero
all the time. And the reason is because when you back propagate the gradient, you multiply by the
transpose of the weight matrix, if that weight matrix is zero, your gradients is zero. So if
you start with all the weights equal to zero, you never take off. And someone asked the question
about saddle points before. Zero is a saddle point. And so if you start at this saddle point,
you never get out of it. So you have to break the symmetry in the system. You have to initialize
the weights to small random values. They don't need to be random, but it works fine if they're
random. And the way you initialize is actually quite important. So there's all kinds of tricks
to initialize things properly. One of the tricks was invented by my friend Leon Boutou about 30
years ago, even more than that, actually 34 years ago, almost. Unfortunately, now it's called
differently. It's called the Keiming trick, but it's the same. And it consists in initializing
the weights to random values in such a way that if a unit has many inputs, the weights are smaller
than if it has few inputs. And the reason for this is that you want the weighted sum to be roughly
kind of have some reasonable value. If the input variables have some reasonable value, let's say
variance one or something like this, and you're computing a weighted sum of them, the weighted
sum, the size of the weighted sum is going to grow like the square root of the number of inputs.
And so you want to set the weights to something like the inverse square root if you want the
weighted sum to be kind of about the same size as each of the inputs. So that's built into PyTorch.
You can call this, you know, initialization procedure. What's the exact name of it? I can't
remember. The one that is coming coming coming here, then there is the Xavier and then there is also
yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same more or
less. Yeah, the Xavier-Gloro version, yeah. Yeah, this one divides by the Fennin and Fennin.
There's various loss functions, so I haven't talked yet about what the cross-entropy loss is,
but cross-entropy loss is a particular cost that's used for classification. I'll probably talk
about this next week and I'll have some time at the end of this lecture. This is for classification.
As I said, we use stochastic gradient descent on mini-batches and mini-batches only because the
hardware that we have needs mini-batches to perform properly. If we had different hardware,
we would use mini-batch size one. As I said before, we need to shuffle the training samples.
So if someone gives you a training set and puts all the examples of category one,
then all the number of category two, all the number of category three, etc.,
if you use stochastic gradient by keeping this order, it is not going to work.
You have to shuffle the samples so that you basically get samples from all the categories
within kind of a small subset if you want. There is an objection here for this stochastic gradient.
Isn't Adam better?
All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic
gradient methods. In fact, people in optimization said this should not be called stochastic gradient
descent because it's not a descent algorithm because stochastic gradient sometimes goes up
hill because of the noise. So people who want to really kind of be correct about this say
is stochastic gradient optimization, but not stochastic gradient descent. That's the first
thing. Stochastic gradient optimization or stochastic gradient descent, SGD, is a special
case of gradient-based optimization. And the specification of it says you have to have a
step-size eta, but nobody tells you how you said this step-size eta. And nobody tells you that this
step-size is a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD
in which eta is changed all the time for every sample or every batch.
In SGD, most of the time, this eta is decreased according to a schedule, and there are a bunch
of standard schedules in PyTorch that are implemented. In techniques like Adam, the eta is
actually a diagonal matrix, and that diagonal matrix, the term and the diagonal matrix are
changed all the time. They're computed based on some estimate of the curvature of the cost function.
There's a lot of methods to do this. Okay. They're all SGD-type methods. Okay. Adam is an SGD method
with a special type of eta. So, yeah, in the opt-in package in Torch, there's a whole bunch
of those methods. There's going to be a whole lecture on this, so don't worry about it, about
optimization. Normalize the input variables to zero mean and unit variance. So this is a very
important point that this type of optimization method, gradient based optimization methods,
when you have weighted sums, linear operations, tends to be very sensitive to how the data is
prepared. So if you have two variables that have very widely different variances, one of them
varies between, let's say, minus 1 and plus 1. The other one varies between minus 100 and plus 100.
The system will basically not pay attention to the one that varies between plus 1 and minus 1.
We will only pay attention to the big one, and this may be good or this may be bad.
Furthermore, the learning rate, you're going to have to use the eta parameter, the step size,
is going to have to be set to a relatively small value to prevent the weights that look at this
highly variable input from diverging. The gradients are going to be very large because the gradients
basically are proportional to the size of the input, or even to the variance of the input. So
if you don't want your system to diverge, you're going to have to tune down the learning rate
if the input variance is large. If the input variables are all shifted, they're all between,
let's say, 99 and 101 instead of minus 1 and 1. Then again, it's very difficult for a gradient
based algorithm that use weighted sums to figure out those things. Again, I'll talk about this
more formally later. Right now, just remember the trick that you need to normalize your input.
So basically, take every variable of your input, subtract the mean, you compute the mean over the
training set of each variable. So let's say your training set is a set of images. The images are,
let's say, 100 by 100 pixels. Let's say they're grayscale, so you get 10,000 variables.
And let's say you get a million samples. You're going to take each of those 10,000 variables,
compute the mean of it over the training set, compute the standard deviation of it over the
entire training set. And the samples you're going to show to your system are going to be a sample
where you have subtracted the mean from each of the 10,000 pixels and divided the resulting
values by the standard deviation that you computed. So now what you have is a bunch
of variables that are all zero mean and all standard deviation equal to 1. And that makes
your neural net happy. That makes your optimization algorithm happy, actually.
We have actually a question. So you keep repeating SGD type methods, gradient-based methods,
because there are other types of methods? Yes. Okay, so there is gradient-free methods.
So gradient-free method is a method where you do not assume that the function you're trying to
optimize is differentiable or even continuous with respect to the parameters. For several reasons,
perhaps it's a function that looks like a golf course. It's flat, and then maybe it's got steps,
and it's difficult to... The local gradient information does not give you any information
as to where you should go to find the minimum. It could be that the function is essentially
discrete. It's not a function of continuous variables, function of discrete variables.
So for example, am I going to win this chess game? The variable you can manipulate is the
position on the board. That's a discrete variable. So you can compute a gradient of a score with
respect to a position on the chess game. It's a discrete variable.
Another example is the cost function is not something you can compute. You don't actually
know the cost function. So for example, the only thing you can do is give an input to the cost
function, and it tells you the cost. But you can't... You don't know the function. It's not a program
on a computer. You can't backprop a gradient to it. A good example of this is the real world.
The real world, you can think of it as a cost function. You learn to ride a bike,
and you ride your bike, and at some point you fall. The real world does not give you a gradient
of that cost function, which is how much you hurt with respect to your actions.
The only thing you can do is try something else and see if you get the same result or not.
Okay? So what do you do in that case? So basically now your cost function is a black box.
So now you cannot propagate gradient to this black box. What you have to do is estimate the
gradient by perturbing what you see to that black box. So you try something,
and that something would be a perturbation of your input to this black box, and you see
what resulting perturbation occurs on the output of the black box, the cost.
And now you can estimate whether this modification improved or made the result worse.
So essentially, this optimization problem I was telling you about earlier,
the gradient-based algorithm is like you are in the mountain, lost in the mountain in a fog.
You can't see anything, but you can estimate the direction of steepest descent, right? You can
just look around and you can tell which is the direction of steepest descent. You just take a
step in that direction. What if you can't see, right? So basically to estimate in which direction
the function goes down, you have to actually take a step. Okay? So you take a step in one direction,
then you come back, then you can take a step in the other direction, come back, and then maybe
you get an estimate for where the steepest descent is. Now you can take a step for steepest descent.
So this is estimating the gradient by perturbation instead of by analytic means of backpropagating
gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the
second step of complexity. Let's imagine that the landscape you are in is basically flat everywhere,
except once in a while there is a step, okay? So taking a small step in one direction will not
give you any information about which direction you have to go to. So there you have to use other
techniques, taking bigger steps, you know, working for a while and seeing if you fall down the step
or not or go up a step. You know, maybe you can multiply yourself in sort of 10,000 copies of
yourself and then kind of explore the surroundings. And then whenever someone says, oh, I find a hole,
calls everyone to kind of come there. Okay, so all those methods are called
gradient free optimization algorithms. Sometimes they're called zeroth order method.
Why is zeroth order? Because first order is when you can compute the derivative.
Zeroth order is when you cannot compute the derivative, you can only compute the function
or get a value for the function. And then you have second order methods that compute not just
the first derivative, but also the second derivative. And they're also gradient based,
okay, because they need the first derivative as well. But they can accelerate the process by
also computing the second derivative. And Adam is a very simplified form of kind of, you know,
second order method. It's not a second order method, but it's, it has a hint of second order.
Another hint of second order method is what's called conjugate gradient.
Is another class of method called quasi Newton methods, which also kind of
using kind of curvature information, if you want to kind of accelerate.
Many of those are not actually practical for, for neural net training, but
there are some forms that are. If you're interested in, in zeroth order optimization,
there is a library that is actually produced by, it's an open source library, which originated at
Facebook at Research in Paris by an author called Olivier Tito, but it's really a community effort.
There's a lot of contributors to it. It's called Nevergrad. And it implements a very large number
of different optimization algorithms that do not assume that you have access to the gradient.
Okay. There are genetic algorithms or evolutionary methods. There are
particle swarm optimization. There are perturbation methods. There is, there's all kinds of tricks,
right? I mean, there's a whole catalog of those things. And those sometimes it's unavoidable.
You have to use them because you don't know the cost function. So a very common situation where
you have to use those things is reinforcement learning. So reinforcement learning is basically
a situation where you tell the system, you don't tell the system the correct answer.
You only tell the system whether the answer was good or bad. It's because you give,
you give the value of the cost, but you don't tell the machine where the cost is.
So the machine doesn't know where the cost function is. Okay.
And so the machine cannot actually compute the gradient of the cost. And so it has to use
something like a zeroth order method. So what you can do is you can compute a gradient with respect
to the parameters of the overall cost function by perturbing the parameters. Or what you can do
is compute the gradient of the cost function with respect to the output of your neural net.
Okay. Using perturbation. And once you have this estimate, then you back propagate the gradient
through your network using regular backup. So that's a combination of, you know, estimating the
gradient through perturbation for the cost function because you don't know it and then
backpropagating from there. This is basically the technique that was used by, you know, the
deep line people in sort of the first sort of deep queue learning type methods.
Back to the normalization. Do we normalize the entire data set or each batch?
It's equivalent. So you normalize each sample, but the variable you're computing
is on the entire training set, right? So you're computing the standard deviation
and the mean over the entire training set. In fact, most of the time you don't even need to do it
over the entire training set because mean and standard deviation converges pretty fast.
So, but you do it over the entire training set, right? And what you get is a constant number,
two constant numbers, a number that you subtract and a number that you should divide
for each component of your input, okay? It's a fixed preprocessing. For a given training set,
you'll have a fixed, you know, mean and standard deviation vector.
But maybe we can connect to the other tool, right? The other module, the batch normalization, right?
Okay, okay, we haven't talked about that yet.
Yeah, I'm saying that we can perhaps extend this normalization bit to the both sides,
like the whole dataset and the batch itself.
Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this, but
for the same reason, it's good to have variables, the input that are zero mean and you need variants.
It's also good for the state variables inside the network to basically have zero mean and you need
variants. And so people have come up with various ways of doing normalization of the variables inside
the network so that, you know, they approach zero mean and you need variants.
But, and there are many ways to do this. They have cute names like batch normalization, like
layer normalization. And the idea goes back a very long time.
Batch norm is kind of a more recent incarnation of it.
Let's see, what was I scheduled to decrease the running rate?
Yeah, as it turns out, for reasons that are still not completely fully understood, you need to
learn fast initially, you need a running rate of a particular size.
But to get good results in the end, you kind of need to decrease the running rate to kind of
let the system settle inside of minima. And that requires decreasing the running rate.
There's various semi-valid theoretical explanations for this, but experimentally,
it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for
this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've
trained your system for a few epochs, you might want to kind of prune it, eliminate the weights
that are useless, make sure that the weights have their minimum size. And what you do is you add a
term in the cost function that basically shrinks the weights at every iteration. You might know
what L2 and L1 regularization means if you've taken a class in machine learning for logistic
regression or stuff like that. It's very common. But L2 sometimes is called weight decay.
This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural
nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And
what this layer does is that it takes the state of a layer and it randomly picks a certain proportion
of the units and basically sets them to zero. So you can think of it as a mask,
a layer that applies a mask to an input. And the mask is randomly picked at every sample.
And some proportion of the value in the mask are set to zero, some are set to one, and you
multiply the input by the mask. So only a subset of the units are allowed to speak to the next
layer, essentially. That's called dropout. And the reason for doing this is that it forces the unit
to distribute the information about the input over multiple units instead of kind of squeezing
everything into a small number. And it makes the system more robust. There's some theoretical
arguments for why that does that. Experimentally, if you add this to a large network, you get
better journalization error. You get better performance on the test set. It's not always
necessary, but it helps. Okay, there's lots of tricks, and I'll devote a lecture on this,
so I'm not going to go through all of them right now. That requires explaining a bit more about
optimizations. So really, what deep learning is about, like, I told you everything about deep
learning, like the basics of deep learning. What I haven't told you is why we use deep learning.
Okay, and that's basically what I'm going to tell you about now, the motivation for why is it that
we need, basically, multi-layer neural nets or things of this type. Okay, so the traditional,
you know, prototypical model of supervised learning, you know, for a very long time is
basically a linear classifier. A linear classifier for a two-class problem is basically a single unit
of the similar type that we talked about earlier. You compute a weighted sum of inputs
at a bias, and you could think of the bias as just another trainable weight whose corresponding
input is equal to one, if you want, and then you pass that through a threshold function,
the sine function, that I'd put minus one if the weighted sum is below zero and plus one if it's
above zero. Okay, so this basic linear classifier basically partitions the space, the input space
of x's into two half-spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus
b equals zero is the surface that separates the category one that is going to produce y bar equal
plus one from category two where y bar equals minus one. Why is it a, why does it divide the
space into two halves? It's because you're computing the dot product of an input vector with a weight
vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset.
So the set of points in x space where this dot product is zero is the set of points that are
orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector,
and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear
subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into
halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data
points here, the red, the red category and the blue category, and there is a weight vector plus a bias
where the, you know, the intercept here of this sort of green separating line
with x1 is minus b times divided by w1. So that gives you an idea for what w should be,
and the w vector is orthogonal to that separating surface. Okay, so changing b will change the
position, and then changing w will change the orientation, basically.
Now, what about situations like this where the points are, the red and blue points are not
separable by a hyperplane? That's called a non-linearly separable case. So there, you can't use a linear
classifier to separate those. What are we going to do? In fact, there is a theorem that goes back
by Tom Kovar, who died recently actually. It was a Stanford that says the probability that a
particular separation of p points is linearly separable in n dimension is close to 1 when p
is smaller than n, but is close to 0 when p is larger than n. In other words, if you, if you
take an n-dimensional space, you throw p random points in that n-dimensional space, data points,
okay, and you randomly label them blue and red. You ask the question, what is the
probability that that particular dichotomy is linearly separable? I can separate the blue
points from the red points with a hyperplane. And the answer is if p is less than n, you have a
good chance that they will be separable. If p is larger than n, you basically have no chance that
they will. So if you have an image classification problem, let's say, and you have tons of examples
way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits.
The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because
some pixels are always 0. And you have 60,000 samples. The probability that those 60,000
samples of, let's say, zeros from everything else or ones from everything else is linearly
separable is basically 0. So which is why people invented the classical model of pattern recognition
would consist in taking an input, engineering a feature extractor to produce a representation
in such a way that in that space now, your problem becomes, let's say, linearly separable
if you use a linear classifier or some other separability if you use another type of classifier.
Now, necessarily, this feature extraction must be nonlinear itself. If the only thing it does
is some affine transformation of the input, it's not going to make a nonlinearly separable
problem into a linear separable one. So necessarily, this feature extractor has to be nonlinear.
This is very important to remember. A linear preprocessing doesn't do anything for you,
essentially. So people spend decades in Cupid division, for example, as feature recognition,
devising good feature extractors for particular problems. What features are good to do face
recognition, for example? Can I do things like detect the eyes and then measure the ratio between
the separation of the eyes with the separation from the mouth and then compute a few features like
this and then feed that to a classifier and figure out who the person is. So most papers between,
let's say, the 1960s or 70s and the late 2000s or early 2010s in Cupid division were essentially
about that, like how you represent images properly. Not all of them, okay, but a lot of them for
recognition. And a lot of people kind of devise very generic ways of devising feature extractors.
The basic idea is you just expand the dimension of the representation in a nonlinear way,
so that now your number of dimensions is larger than the number of samples. And now your problem
has a chance of becoming linearly separable. So the ideas that I'm not going to go through,
like space styling, random projection. So random projection basically is a very simple idea.
You take your input vectors, you multiply them by a random matrix, okay, and then you pass the
result through some nonlinear operation, okay, that's called random projection.
And it might make, if the dimension of the output is larger than the dimension of the input,
it might make a nonlinearly separable problem linearly separable. It's very efficient because,
you know, you might need a very large number of those, of this dimension to be able to kind of
do a good job. But it works in certain cases and you don't have to train the first layer,
you basically pick it randomly. And so the only thing you need to train is a linear classifier
on top. It's polynomial classifiers, which I'll talk about in a bit in a minute,
radio basis functions and kernel machines. So those are basically techniques to turn
an input into a representation that then will be essentially classifiable by a simple classifier
like a linear classifier. So what's a polynomial classifier? A polynomial classifier, basically
imagine that your input vector has two dimensions. The way you increase the dimensionality of the
representation is that you take each of the input variables, but you also take every product of pairs
of input variables, right? So now you have a new feature vector, which is composed of x1, x2,
you add one for the bias, and then also x1 times x2, x1 squared and x2 squared. So when you do a
linear classification in that space, what you're doing really is a quadratic classification
in the original space, right? The surface, the separating surface in the original space now
is a quadratic curve into dimension. In n dimension, it's a quadratic hyper surface,
basically. So it could be a parabola or ellipse or hyperbola depending on the coefficients, right?
Now the problem with this is that it doesn't work very well in high dimension because the number
of features grows with a square of the number of inputs. So if you want to apply this to
get an image net type image, the resolution is 256 by 256 by 3 because you have color channels.
That's already a high dimension. If you take the cross product of all of those variables,
that's way too large. So it's not really practical for high dimensional problems,
but it's a trick. Now here is, so super vector machines are basically two-layer
networks or kernel machines more generally, are two-layer systems in which the first layer
has as many dimensions as you have training samples, okay? So for each training sample,
you create a inner-on, a unit, if you want, and the role of this unit is to produce a large output
if the input vector matches one of the training samples and a small output if it doesn't,
or the other way around. A small output if it matches, a large output if it doesn't,
okay? It doesn't really matter, but it has to be nonlinear. So something like, you know, compute
the dot product of the input by one of the training samples and passes through, you know,
a negative exponential or a square or something like that. So this gives you how much the input
vector resembles one of the training samples, and you do this for every single training samples,
okay? And then you train a linear classifier basically to use those inputs as, you know,
as input to a linear classifier. You compute the weights of that linear classifier,
basically as simple as that. There's some regularization involved, okay? So essentially,
it's kind of a lookup table, right? You have your entire training set as, you know, points in your,
I kind of nuance if you were, if you, if you want our units in your first layer,
and they each indicate how close the current input vector is to them. So you get some picture of
where the input vector is by basically having the relative position to all of the training samples,
and then using a simple linear operation, you can figure out like, what's the, what's the correct
answer. This works really well for low dimensional problems, the small number of training samples,
but you're not going to do computer vision with it, at least not without, not if X is our pixels.
Because it's basically template matching.
Now, here is a very interesting fact. It's a fact that if you build a two layer neural net
on this, on this model, okay? So let's say a two layer neural net, you have an input layer,
a hidden layer, and not specifying the size and a single output unit. And you ask, what functions
can I approximate with an architecture of this type? The answer is you can approximate pretty
much any well behaved function as close as you want, as long as you have enough of those units
in the middle, okay? So this is a theorem that says that two layer neural nets are universal
approximators. It doesn't really matter what nonlinear function you put in the middle.
Any nonlinear function will do. A two layer neural net is a universal approximator.
And immediately you say, well, why do we need multiple layers then, if we can approximate
anything with two layers? And the answer is it's very, very inefficient to try to approximate
everything with only two layers. Because many, many, many interesting functions we're interested
in learning cannot be efficiently represented by a two layer system. They can possibly be
represented by a two layer system. But the number of fielding units it would require would be so
ridiculously large that it's completely impractical, okay? So that's why we need layers. This very
simple point is something that took about, it took until basically the 2010s for the machine
learning and computer vision communities to understand, okay? If you understood what I just
said, you just took a few seconds, so you beat them. There is a last question here before we
finished class. So does the depth of the network then have anything to do with generalization?
Okay, so generalization is a different story, okay? Generalization is very difficult to predict.
It depends on a lot of things. It depends on the appropriateness of the architecture to the
problem at hand, okay? So for example, people use convolutional nets for computer vision,
they use transformers for text, you know, blah, blah, blah. So there are certain architectures
that work well for certain types of data. So that's the main thing that will improve generalization.
But generally, yes, multiple layers can improve generalization because
for a particular function you're interested in learning, computing it with multiple layers
will allow you to reduce the overall size of the system that will do a good job. And so by
reducing the size, you're basically making it easier for the system to find kind of good
representation. But there is something else which has to do with compositionality. I'll come to this
in a minute if I have time. Also the minimum, how do you call it? The well is like larger, right?
If we have overparameterized networks. If you have overparameterized network, it's much easier
to find a minimum to your objective function, right? Which is why neural nets are generally
overparameterized. They generally have like a much larger number of parameters than what you
would think is necessary. And when you get them bigger, when you make them bigger, they work better
usually. It's not always the case, but it's very curious phenomenon about this. We'll talk about
this later. Okay, this is the one point I want to make. And it's the fact that the reason why layers
are good is that the world is compositional, the perceptual world in particular, but the world
in general, the universe, if you want, is compositional. What does that mean? It means that,
okay, at the level of the universe, right? We have elementary particles. They assemble to form
less elementary particles. Those assemble to form atoms. Those assemble to form molecules. Those
assemble to form materials. Those assemble to form, you know, structures, objects, etc. And,
you know, environments, scenes, etc. You have the same kind of hierarchy for images. You have pixels.
They assemble to form edges and texons and motifs, parts and objects. In text, you have characters
that assemble to form words, word groups, clauses, sentences, stories. In speech, you have speech
samples that assemble to form, you know, kind of elementary sounds, phones, phonemes, syllables,
words, etc. So you have this kind of compositional hierarchy in a lot of natural signals. And this
is what makes the world understandable, right? This is famous quote by Albert Einstein,
the most incomprehensible thing about the world is that the world is comprehensible.
And the reason why the world is comprehensible is because it's compositional, because small
part assemble to form bigger part, and that allows you to have a description, an abstract description
of the world in terms of parts from the level immediately below in terms of level of abstraction.
So to some extent, the layered architecture in a neural net reflects this idea that you have
kind of a compositional hierarchy where simple things assemble to form slightly more complex
things. So in images, you have pixels formed to form edges that are kind of depicted here.
These are actually feature detectors, the visualization of feature detectors by
a particular convolutional net, which is a particular type of neural net, multiple neural net.
So at the low level, you have units that detect oriented edges, a couple layers up, you have
things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up,
there are things like parts of objects and things like that. So I think personally that
the magic of deep learning, the fact that multiple layers help is the fact that
the perceptual world is basically a compositional hierarchy. And then this end to end learning
in deep learning allows the system to learn hierarchical representations where each layer
learns a representation that has a level of abstraction slightly higher than the previous
one. So low level, you have individual pixels, then you have the presence or absence of an edge,
then you have the presence or absence of a part of an object, and then you have the presence or
absence of an object independently of the position of that object, the illumination, the color,
the occlusions, the background, things like that. So that's the motivation, the idea why
deep learning is so successful and why it's basically taken over the world over the last 10
years or so. All right, thank you for your attention. That's great. So for tomorrow, guys,
don't forget to try to go over the 01 notebook that we have on the website such that we can get
all on the same level for the ones that are not really familiar with NumPy stuff.
So otherwise, let's see you tomorrow morning and have a nice day. Take care, everyone. Bye-bye.

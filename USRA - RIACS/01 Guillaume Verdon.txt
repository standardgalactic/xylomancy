All right, so it's going to be a somewhat fast-paced talk to wake you up in the morning, and I'm
going to present two different topics.
Some older work from December on how to train Boltzmann machines on near-term hardware,
and some very, very recent work from two days ago on quantum deep learning in general using
quantum enhanced optimization.
Okay, so quantum approximate Boltzmann machines.
So let's start with a high-level overview comparing, you know, annealers and all first
I'll introduce what is a Boltzmann machine.
I mean, this is a specialist audience, so I'm going to go pretty fast here.
So what we're trying to do is we have a model made of a bunch of spins, you know, tiny magnets.
We couple them in a bunch of ways, and we put a bunch of biases, and their statistics
at equilibrium, at thermal equilibrium, should give you some reduced statistics on a subset
of spins that you call the visibles.
The visibles are going to represent your data, and the hiddens are going to represent correlations
in your data, and you're going to model your data via this, via an equilibrium-type model.
So the goal of such a model, as I said, is to model data with the visible units.
You want to replicate the distribution of the data.
So your goal is going to be to, you know, minimize some metric that quantifies the distance
between the distributions, and, for example, a KL divergence, a relative entropy, classically.
So your goal is going to be to tune the parameters of your model and tune, you know, all sorts
of parameters, like the couplings and biases and temperature even, to best model your data.
So that, you know, let's say you have certain data points, but you want more data points
from the same distribution that you had, but you don't have access to those data points,
then this model is generative.
It could give you new data points, and it's, it could be supervised, it could be unsupervised.
Okay, so, you know, classically you'd use a bunch of Monte Carlo techniques, and we heard
about those, or you could use quantum computers.
How would you use quantum computers to train bullsew machines?
Well, first of all, you need to quantize your model, so you put hats on everything.
That's pretty straightforward.
And there's different types of quantum hardware.
There's the quantum analog computers, I called, like D-Wave and Ehlers.
There's quantum digital computers, which are these fabled, perfect, error-corrected, unicorn
land quantum computers, and there's quantum approximate computers, which are this near-term
hardware.
So I'm going to talk a bit about how you would train a bullsew machine and all those paradigms.
So in analog computing, you do Gibbs sampling, and it's analog, it's analogy to the algorithm
you're trying to do, right?
So the physics of the system is enacting the physics that you're trying to simulate.
So the physics of the chip are in direct analogy with the algorithm.
So you know, your time evolution here is represented as like the smooth path of evolution, right?
It's analog.
So caveats, because it's so close to the algorithm you're trying to do, it's less flexible.
So there's problems with physical temperature because you're bounded by how cool you could
cool off your chip.
You have to connect things directly, which is really hard.
And because you have to have this connectivity, then you don't have a choice but to use qubits
that are pretty widespread, and so you get a lot of noise because they're just macroscopic
and anything pretty big is going to average out, right?
It's going to thermalize.
And there's all sorts of problems on how to embed your problem on the annealer.
All right.
So theoretically, you know, I could simulate physics in arbitrary connectivity, arbitrary
effective dimension if I had just a perfect quantum simulator, right?
That's the original dream of quantum computing.
I'd have virtualized physics and, you know, I could simulate arbitrary accuracy, everything
would be great.
So one of the caveats is I need over billions of gates, and so I need fault tolerance.
So is there something in the near term, right?
Here I'd be like trying to simulate staying close to the adiabatic path via quantum simulation
techniques.
So in the near term, is there something that's in the same philosophy that I wouldn't have,
I could swap around things, I could have arbitrary connectivity, but it's low depth, right?
It's not billions of gates.
And the answer is yes, and that's what we showed today.
It's leveraging, it's kind of like a QAOA onsatz for near term quantum classical hybrid
optimization to do Gibbs sampling.
And I'll talk about how to leverage Gibbs sampling to train Boltzmann machines in a second.
So the classical and the quantum computer share the optimization load, they work together,
and in a sense, you know, it's like I'm trying to get from point A to point B. Quantum simulation
is trying to stay close to the path, QAOA is just trying to get to the end point no
matter what, right?
And it's near term implementable, as we'll see.
Okay, so technical stuff.
How do you train a Boltzmann machine once you have Gibbs sampling, right?
I'm trying to model some data with an equilibrium model.
So every time I have a new guess for a bunch of parameters, I need to be able to sample
the distribution of the thermalized state with those new parameters and then compare
it to the distribution I want, right?
So let's say I have a class of Hamiltonians that are parameterized like this and I have
a bunch of parameters theta.
What I want to do is minimize the relative entropy between my visible units and the data.
So can I do just gradient descent on the relative entropy while sampling the relative entropy
is pretty hard?
So what you could do is maximize the log likelihood instead, so it's minimizing an upper bound.
This is work by Melko and others from D-Wave, and so we use the same update rule as you
would on a D-Wave.
And what you need to do there, let's say you have a parameter you want to update.
You got to look at the constant, you know, you got to look at the operator that's in
front of your parameter, and you got to sample as clamp sampling and unclamp sampling, also
called positive phase and negative phase just because of the math.
So what you want to do is you want to force your data, you want to force a potential that's
gives you infinite penalty for having your visibles being not in agreement with the data
points, and then you want to thermalize the rest of the network, right, and that's clamp
sampling and you average over all your data points.
And then you subtract what your current natural equilibrium is, so you want to get closer
to the average of satisfying all data points in the visible units, you want to get further
away from where you are.
So let's unpack this.
Let's say I want to update just, you know, a coupling here, then, okay, so I look at
my formula, oh, I got to sample correlations of spins.
All right, let's say my data sets a bunch of bit strings.
I convert that spin-off-spin-down.
So now what I need to do is sample these correlators for all my data points, average
over all my data points by clamping the visible units to each of those spin configurations,
and then sampling, thermalized, unclamped.
So it looks like this, you just, you average over all your data points, so you have, you
know, if you have D, different data points, you have D sampling steps, and then you have
the negative phase, which is unclamped Gibbs sampling.
All right, so for inference and for RBMs, things simplify.
For general bolts and machines, it's a bit more complicated.
I won't dive too much into it.
So another subfield that we connect with is classical quantum variational algorithms.
So the goal of such algorithms, their first circuit model, by the way, is to find a state
that minimizes some expectation value of a Hamiltonian, right?
So what you do is you have a parametrized class of circuits, and you have an optimization
loop that the CPU sends a classical parameter.
The QPU, the QPU shoots back an expectation value, and there's a loop, the classical computer's
test with optimizing the expectation value with respect to the variations of the parameter,
and then you keep doing this until the minimums reach.
What's cool about this is because, you know, you kind of work together between the QPU and
the CPU, there's not as much coherence time required to have pretty cool, useful algorithms.
So the QAOA, this is kind of a slide showing the math connecting from adiabatic to trotterized
simulation to QAOA.
The QAOA is like, I take a quantum simulation circuit, so this would be a continuous time
evolution, you know, it's nice and smooth, this would be a trotterized simulation, your
operators would be these alternating between the cost and the mixer Hamiltonian, and then
the QAOA is like if you took this, and then you just chunk the parameters to be big instead
of being very small steps, right?
And it's just alternating operators.
We've also seen this this week.
All right, so the algorithms that we use to train bolster machines, well what we'll need
is something that prepares thermal states.
So how do you leverage QAOA to prepare thermal states?
Well, what you do is you pump in some entropy at the beginning, and then you try to minimize
the energy with QAOA.
So then if you have entropy fixed and you minimize the energy, well then to some extent
you're going to minimize the free energy, and the free energy gives you, is actually
equal to the relative entropy to the thermal state.
That means you're going to get close to the thermal state if you minimize the free energy.
So the way we do it, we start with an easy thermal state of each individual qubit, could
be one of these, and then we use the QAOA to minimize the bolster machine Hamiltonian
with a typical mixer.
So this works well, but like, you know, you're kind of bound by your guess of entropy and,
you know, it's, you're not fully minimizing the free energy, but this works pretty well
and it has very few parameters, hyper parameters, to optimize classically.
So another one that's not in the paper yet, but that's, it's variationally optimizing
both the entropy and energy, in this case you, you have a parameter for the temperature
of each qubit at the beginning, and then you use the same QAOA.
In this case, you know the von Neumann entropy going into the system, the von Neumann entropy
is a lower bound to your actual classical entropy of the standard basis measurements.
So then in a sense, you're minimizing a von Neumann free energy, which is an upper bound
to the true free energy.
So you have better odds of getting close to the thermal state, because you're optimizing
both energy and kind of maximizing entropy.
So, but this has more parameters, but which is less efficient to optimize, but I'll come,
I'll talk about a general algorithm that doesn't care how many parameters you have in it.
It just, it works really well.
That's later.
So the quantum approximate Boltzmann machine is leveraging this quantum approximate thermalization
to do the sampling to train Boltzmann machines.
Simple enough.
And a little trick we came up with, which is really weird, is you could clamp to a mixture
of different states, because every run of the QAOA you could kind of randomize which,
which data point you input in the visibles.
This not only is much faster, because you only have one run of QAOA to do for all data
points, but it actually gives much better performance.
So this is what happened when we simulated it on a, the algorithm on a noisy simulated
noisy circuit model quantum computer with deep polarizing noise.
The task was to decode like a two bit hidden subspace.
And as we could see, you know, for various levels of noise, it still performs well.
The KL divergence minimizes on the right is the quantum randomized version and on the
left is the non quantum randomized version.
It performs better than, you know, scikit-learn on your laptop in some cases.
So that's cool.
So if you want to, if you want to train Boltzmann machines and you have a circuit model quantum
computer, use QAOA and use our algorithm.
This is the variational entropy variant.
For such a small scale, it's not, it's not going to change much.
It helps a bit.
You know, if you tweak the temperature, but I imagine at a larger scale, varying the entropy
helps too.
All right.
The code's available on GitHub.
Michael Broughton is my co-author on both papers.
Check out his GitHub for code for both papers.
Okay.
So we just figured out how to do Boltzmann machines on near term quantum computers.
But you know, we're all interested in, can quantum computers help with deep learning?
And what, what is quantum deep learning anyways?
There's parametric circuits.
There's neural nets and people call those neural quantum neural nets and it's all confusing.
So let's compare really quickly classical versus quantum deep learning, classical deep
learning.
You have a big network.
It has a bunch of parameters.
It's parametrized ansatz.
You have a loss function that depends on your output.
You know, you have a, an output for a given input and given parameters and you're trying
to minimize often a distance to the, the desired output for a supervised machine learning.
So you want to minimize the loss function subject to variations of the parameter.
Oh, wait, that's kind of like parametric circuits.
And now I have a loss operator in general and I have a parametrized unitary, right?
Okay.
So, well, if you map one to the other and then you know how to optimize parametric circuits,
then you could optimize everything, right?
So, so this is our paper that came out two days ago.
The title is provocative, but then we justify it with 80 pages of work.
So, so I encourage you to try to read it or talk to me after.
But so, so with the quantum approximate pulse machine, you know, you map it to parametric
circuits.
In this paper, we also map all quantum feedforward, all feedforward neural networks to become
quantum coherent networks that are parametric circuits.
So now the task is to how do you optimize parametric circuits when you want to minimize
an expectation value of some operator, all right?
So a classically parametrized circuit, you'd use a quantum classical hybrid optimizer.
But how could we have quantumly parametrized circuits?
Could I have superpositions of applying different circuits, right?
So I could have a wave function over circuits or wave function over neural networks, right?
So yeah, you can.
So you convert all your gates in a certain way with continuous variable parameters that
are like controlled gates for each parametric unitary.
This is the notation.
And then in each case, in each branch of a superposition, you'd apply a different unitary
of a different set of parameters.
The bold is for a vector.
So okay, so what we introduce is back prop with a Q, backwards quantum propagation of
phase errors, right?
So you could plug in a superposition of parameters and an initial state in the compute register,
and then you apply your feedforward.
And what you do is you apply a phase kick, an exponential, of your loss operator.
You could think of that as like an energy, right?
I'm inducing a kick on the output.
And then when I actually, oh, this shouldn't be a controlled gate, anyways, but when I
apply, yeah, it's a regular gate, but when I apply the un-compute, my phase errors back
propagate through the circuit and kick up the parameters.
So the parameters feel a force dependent on how the output got kicked, which itself depends
on how off of the target it was.
And depending on how the effect they all put, they get kicked according to the gradient
while on average.
But really, each part of the wave function gets kicked according to its local gradient.
So what you could see this as is effective phase kick of the parameter.
Now never mind the compute, now we're just going to focus on the parameters.
Now these parameters, you could average a bunch of kicks for a bunch of data points,
and then you get a cost function, we call it J.
And now you could see that as a potential.
And then you could simulate multi-dimensional continuous variable Schrodinger dynamics in
this landscape of optimization to descend the landscape.
And the mixtures, like momentum squared, you could do this with quantum Fourier transforms.
And so you apply these kicks of data, data-induced kind of stochastic kind of forces on your
parameters, and this is a coherent way to descend the landscape.
So it's quantum dynamical descent.
It's like QAOA on the space of parameters.
So we connect it to the adiabatic theorem.
We show how you could add a regularizing potential.
You could slide right into a local minimum of the landscape.
But what's cool is that you can optimize neural nets with quantum dynamics.
So all your theorems that you're proving for adiabatic stuff with tunneling and speedups,
you could start looking at that for all of machine learning now, all of deep learning,
classical and quantum.
So we have a tweak that is for near term, which is mom grad, which is because these
phase kicks are like kicking the momentum, I could just measure the shift in momentum.
And on average, I could get the gradient, the local gradient.
Here we kick really hard and it's exaggerated, but we call it momentum measurement gradient
descent, and that's upstairs.
And as you can see, we reset a pointer state that we use, we use Gaussians because it's
easy with Fourier transforms.
Basically, this is a cubic potential kicking it, so the derivative is a second order polynomial.
So then you could see that each part of the wave function gets kicked according to the
gradient of where it was.
QDD just does kind of a coherent evolution in parameter space rather than quantum classical.
So we do a lot more in this paper.
It's a massive paper.
We do parallelization batching.
We want to write it almost like a textbook.
Swarm optimization.
You could have discrete parameters.
You could optimize over architectures of networks.
We have a bunch of regularization techniques.
We have a meta-learning technique where the optimizer optimizes itself.
It's like QAOA, optimizing QAOA.
That's pretty cool.
And we have a really in-depth analysis of back propagation for classical neural networks.
And that's really important for parametric circuits if we're going to solve the bearing
plateaus problem of vanishing gradients.
We've got to understand how signals of gradients back propagate through the circuit, and now
we can with the formalism.
We also have something which is hybrid quantum classical parametric circuit hybrid with a
classical deep neural net.
They're connected together.
You could back prop through the whole thing.
And we have more parametric circuit applications fleshed out than you could display on one
slide.
So that's it.
Quantum deep learning.
All right.
Thank you very much for this nice talk.
So we have time for some questions.
Are there any questions?
Or is everybody still asleep?
Perfect.
Hi.
Thanks for the great talk.
Thanks.
We did talk about it yesterday a bit.
But the question I want to ask is, is there something analogous to the step size that
we normally have in classical optimization algorithms here?
Yeah.
Yeah.
So the QAOA parameters, we relate that.
So in the gradient descent variant, we relate the learning rate.
The learning rate becomes, so if you shift the momentum, and then you kick by a certain
constant times the momentum squared, right, you're going to go down the gradient, which
is the product, the learning, the classical learning rate becomes the product of both
your QAOA hyperparameters for that iteration.
So we really connect both methods from classical and quantum deep learning there.
And yeah, and to optimize those hyperparameters, again, you could just do a meta learning loop,
which is really cool because classically, usually you have to do a grid search over
hyperparameters.
And in this case, we're kind of doing a grid search, but it's in quantum superposition.
So you could kind of, over one training, you could query exponentially many possible neural
networks and optimize over the space of trained neural networks, right?
So you could have a superposition of hyperparameters, and in each case of the superposition, you
have a different network at the end, and then you could optimize that, uncompute all that,
and then gradient descent over the space of hyperparameters.

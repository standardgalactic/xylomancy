So I just want to express my gratitude to everyone who's come today.
Thank you for coming.
So my name is Maxwell Ramstead.
I am a postdoctoral fellow here at the Jewish General.
I work with Lawrence.
And I work mainly on this approach called Active Inference that I'll be presenting today.
So my talk appropriately is called a Tutorial on Active Inference from the Predictive Brain
to Sociocultural Regimes of Expectation.
And the talk basically is going to come in four parts.
So the first is basically a motivation of the problems that we're trying to address
using Active Inference.
So I'll present on the one hand the problems per se that Active Inference addresses like
the space of problems in which Active Inference lives.
And then I'll move on to a few more specific motivations about why an approach like this
might be interesting.
Then I'll turn to what's known in the literature as predictive processing, which is basically
this Active Inference scheme applied to the brain.
Then I'll move to Active Inference per se.
These two sections will kind of come together.
I mean the predictive processing part is just the application of this framework to the brain.
So they're not all that distinct, although if you get into the more technical debates,
there are some distinctions, but we can return to that.
So finally, if time allows, I'll discuss some of the multi-scale extensions of Active Inference.
So going beyond models of brains and cells to like this kind of integrated view of systems
of systems of systems that are nested, the ones within the others, and the kind of move
together thanks to this Active Inference framework.
So first I guess a discussion of the motivation and problems.
So basically the problem I think that Active Inference helps us solve most clearly is the
problem of multi-scale systems.
So this is a picture of McGill University in slightly warmer times.
So McGill University is a complex system.
It's a system that's composed of several different parts, some of which are living, some of which
are not.
So the faculty, the student body, the support staff, but also all those are living and some
things that arguably are not, like the buildings, the rest of the physical infrastructure labs
and so forth.
Well if you zero in on one component of this, the living ones, so say the student body in
McGill is itself composed of bodies, and bodies as we know are composed of cells, and cells
make up organ networks that eventually make up organisms.
Organisms then cells interact in social groups and eventually make up social networks, one
of which may be the most evil I've posted the image of right here.
So systems within systems within systems, we are systems within systems within systems.
That's kind of the takeaway of this slide.
So if you look at the structure of the brain, the brain structure in a sense recapitulates
the structure of the environment in which it's encapsulated.
So if you look at the structure of the brain, this is from a paper by Park and Friston.
What you'll notice is basically a repeated encapsulation of networks within networks
within networks, right?
So dendritic formation at one level, and if you zoom out a bit you have networks of neurons
and then if you zoom out even more you have brain regions interacting with brain regions
and so forth.
And what we notice is that there's a segregation in the brain, a kind of spatiotemporal segregation
of the interactions between brain regions that in a sense corresponds to and captures
the regularities of the kind of statistical segregation that we see out in the world around
us, right?
Things change in relevant ways at different scales, at different spatial and temporal scales.
So this is something that we'll return to.
And so up until very recently this was lacking in the literature, but suppose we wanted to
study all these levels together in an integrated fashion, recognizing the interest and dignity
of all these levels, right?
So suppose we wanted to construct a framework that was able to address, I've plotted here,
this is from a paper that I wrote with Carl Friston and Paul Badcock a while back.
I've plotted spatial dimensions on the y-axis and temporal dimensions on the x-axis.
So what we would like is a theory that can, in an integrated fashion, offer an explanation
for the way that systems behave from the subcellular level through to the cellular level, the level
of tissues, organs, and organ networks to organisms all the way eventually to speciation,
the construction of specifically designed niches.
And this is from the spatial point of view, but from the temporal point of view we would
also like an account that's able to account for the variety of temporal scales that are
involved in the phenomena of interest, right?
So from mechanistic processes occurring over milliseconds all the way to phylogeny and adaptive
radiation which span hundreds of thousands, if not millions of years.
So that's one, that's the motivation of the problem.
How would we address all these different spatial and temporal scales in a principled way?
What kind of framework would be able to enable that?
Okay, so that's the problem.
One motivation for thinking about active inferences in particular is a, it's an observation.
It's that most self-organizing systems in nature tend to dissipate, right?
So from galaxies and stars to lightning bolts and tornadoes, almost all really like, I mean
that in a very strong sense, almost all systems that self-organize in nature self-organize
to equilibrium.
So what does this mean if we had to unpack it in a kind of basic way?
Well, first, first we should note that these self-organizing systems consume the gradients
around which they organize, right?
So if for instance you think of a lightning bolt, a lightning bolt self-organizes around
a charge gradient, right?
In striking, the lightning bolt consumes the gradient around which it self-organized, effectively
leaving the entire system at equilibrium.
So the same could be said for tornadoes, although now it's not a charge gradient, it's a temperature
gradient in a weather system.
But the main takeaway is that for almost all systems in nature self-organization serves
to increase entropy.
So in this context, entropy is a measure of spread.
You can think about it roughly as a quantification of how many different configurations the system
could be in, right?
So if you think of a gas versus say a crystal or water or a solid, in a gas, the constituent
particles could be in any number of different configurations, right?
Like this one that I'm pointing to at the end of my finger, this little molecule of
gas might be at the other end of the room in like 20 minutes.
There's less constraint as to where the little particle will end up.
Whereas if you think of a crystal, well the kind of regular lattice structure in a crystal
means that there are only ever local interactions and that you can predict with a high degree
of reliability what any given particle, what state it will be in, right?
So high entropy means a high number of available configurations, low entropy means a low number
of available configurations.
So back to this, most systems in nature, right, self-organize to equilibrium, they dissipate,
they increase entropy.
But other systems self-organize and do not, right?
Bunny rabbits and tigers and, you know, I don't want to make assumptions about you,
but I assume you as well don't self-organize to equilibrium, right?
We self-organize to this regime of states that's in effect very far from equilibrium,
right?
I don't exist at room temperature.
My core body temperature is about 36 and a half degrees Celsius.
I imagine yours is roughly the same.
You know, so we manage to stay far away from equilibrium.
And the question is, well, how do we manage to do this?
So here's going to be my first kind of technical excursus.
I'm going to present to you the state space formalism from dynamical systems.
And this will help us think a bit more formally about what I just said.
So a state space description is something borrowed from dynamical systems theory.
And the idea is to construct an abstract space that represents all possible states of a system,
right?
So it's really that we're just constructing a state, a space, right?
So like a mathematical object with dimensions that you can kind of move around in, right?
And this space is going to represent all the possible states that our system can be in.
And so how do we do this?
Well, basically for every way that the system can change, right, a.k.a. for every variable
in the system, what we're going to do is plot one dimension in this space.
So we're going to say, OK, this variable corresponds to a dimension in this space that
we're constructing.
So suppose you have a very simple physical system with a temperature and a size, right?
Then you might represent it this way, right?
One of the dimensions corresponds to temperature.
The other dimension corresponds to size.
And so what you'll notice already from this setup is that a point in this space here is
already a complete description of the system that we're interested in, right?
So to recap the argument, every dimension in this space corresponds to a variable in
the system.
Therefore, every point in the system corresponds to assigning a value to every variable.
So essentially a position along the dimension, right?
And so that might seem like a banal observation, but it allows us to say interesting things
about what you might call the intrinsic geometry of a system in terms of its phase space.
So for example, you might want to say, well, in the whole space of possible states that
the system could be in, there's this particular region that has these particular properties
that when you're in this area, something interesting happens, right?
So as an example, here again, I'm just using the same thing, right?
Plotting the temperature on the x-axis and size on the y-axis.
Well, you might say, well, most living things that exist exist in this subregion of that
space, right?
Most living things exist somewhere between 0 and 100, roughly degrees Celsius, and most
living things are somewhere between 10 centimeters and 100 meters if you're looking at these
very large colonies of cloning trees and stuff like.
So this is the kind of thing that you can say using the state space formalism.
And one of the interesting things that you can do with this is describe trajectories
over the phase space, right?
So this is literally drawing a trajectory over this phase space and saying, okay, well,
the system starts here, and then some perturbation moves the system towards another state, and
then the system has to cycle back.
So for example, I wake up in the morning, I'm a bit hungry, right?
This is somehow related in an interesting way to my making coffee and breakfast, and
then I eat, and then I'm satiated, and then I move to another area of the phase space
that is me satiated, right?
But I consume sugar as I, for example, give this talk.
So sure enough, I'll be hungry later today, so then I'm back where I was.
So the phase space description allows us to say something about the dynamics of a system.
It allows us to say something about the way that a system states evolve over time.
So basically, the reason I'm talking about this is that active inference is a theory
of, provides a mechanics of or tells us how living systems are able to do this, right?
So we all exist as living things in a bounded set of states, right?
We don't self-organize to dissipation like these things, right?
We self-organize to a very well-defined set of states.
The question is, how do we do this?
And active inference provides an answer to that question.
Active inference tells us about how things can stay away from basically states that they
want to avoid and stay in a regime of states that are compatible with their existence.
Okay.
Second motivation, which you might call the animal's perspective or the Bayesian room.
So if you really, really, really want to caricature the position that organisms are in, basically,
organisms are in what you might call the Bayesian room.
Chris Elias Smith is called this the animal's perspective.
So the idea of the Bayesian room is that organisms only ever have access to their sensory input,
right?
And the sensory input isn't always very reliable.
For those of you who do neurophysiology, you'll know that the sensory motor channels that
we deal with are just intrinsically noisy.
They have to deal with movement and variance.
And they're a varying trustworthiness as well.
That's kind of context dependent.
So I spent a lot of time in London last year and there's a lot of fog.
So you learn, for example, not to trust your eyes as much and to listen more than you would,
for example, in Canada where there's very little fog.
So this kind of problem that the organism has to solve is a reverse inference problem
in the sense that it has access only to its sensory input and it needs to determine what
caused its sensory input.
So here I've got a spock ear connected to a brain, a very simple sensory system as occur
in nature frequently.
So suppose my spock ear system, here's a snap, a crack of some kind.
Well, what caused the snap is relevant, sometimes for the survival of the spock ear organism
in this case.
Maybe it was just the wind moving the branches of trees and a twig snapped.
Maybe it was a tiger.
And the difference is relevant because the brain, we mustn't forget, its main function
is to coordinate movement.
I forget what the animal is.
There's a very simple animal that lives in water and it has a nervous system for the
first phase of its life.
It moves around.
And in a metaphor for tenure, what it does when it reaches maturity is it attaches to
a rock and it digests its nervous system.
It's the first thing it does when it doesn't have to generate movement.
When you don't have to coordinate movement anymore, you don't really need a nervous system.
So out of multiple possibilities for action, the brain has to determine what exactly caused
the sensation.
Was it a tiger or was it just a twig?
And has to coordinate action in an adaptive way as a function of its inferences.
So this is another motivation.
Active inference is useful because it tells us how this happens.
Okay.
That was the motivation introduction bit.
We can now move to predictive processing unless there are questions about that opening part.
Are we good?
All right.
So predictive processing is a theory of how the brain works.
And it's essentially an instantiation of active inference applied to the dynamics of the brain.
And it's a good starting point to understand active inference.
In part because historically this is how the sequence kind of originated, right?
These approaches were developed in theoretical and computational neurosciences.
And only recently through the work of our group at McGill and the work of others has
been extended to model social and cultural phenomena.
So we'll start with the brain.
So Neuroscience 101, the traditional view of cognitive processing that you'll learn
in any neuroscience class is that the brain is essentially an aggregative bottom-up feature
detector.
So we'll unpack this sequentially.
The brain is, first of all, the feature detector, meaning that the more sensory areas of the
brain that are supposed to be lower down on the processing hierarchy, what they do is
essentially detect features, right?
So in the primary visual area, for example, you're sensitive to bars.
And then as you ascend the cortical hierarchy, and this is the other keyword, aggregate these
statistical features together until you arrive at something like a complex percept.
So two things to note about the traditional view.
On the traditional view, the bottom-up signal is the one that's driving the show, right?
Essentially what the brain is kind of cast as on this view is a kind of passive collector
of sensations that get combined, aggregated together, and yield our world of experience.
Another thing to note is that on this view, the descending connections that descend from
so-called higher areas like the prefrontal cortex back down to the sensory areas, these
are seen as modulation or feedback.
So on this view, the brain is a passive organ.
It receives sensations, it aggregates them, and the top-down stuff is just feedback.
So there are a few weird things about this for people who study the brain in more detail.
One thing to note, for example, is that roughly 80% of the brain's connections are doing this
feedback thing, right, or descending.
So in a sense, it would sort of be surprising that 80% of the brain's energy budget goes
to something that's just feedback, right?
So I mean, another, for someone trained in philosophy like myself, another very important
problem is like, where is meaning supposed to emerge from this, right?
So somehow I'm supposed to be aggregating geometric features of stimuli, and then meaning
is supposed to pop out of that somehow.
It's not clear how that's supposed to happen.
This is often the case in neuroscience.
We give a nice little name to our ignorance, essentially.
It's called the binding problem in cognitive neuroscience.
So how is this information bound together to produce a meaningful stimulus?
I mean, what we think is that this is maybe just the wrong way to look at the problem.
Maybe no answer is forthcoming.
So the predictive processing viewer, the active inference view, flips this on its head and
says, OK, well, maybe this top-down thing is what the brain is mainly engaged in.
Maybe that's the main activity that the brain is busying itself with.
So from this point of view, the main activity of the brain is essentially to produce these
kind of top-down predictions about what it should expect to sense next, right?
And what flows up, right, so what's traditionally thought of as the signal, right, is what we
call a prediction error.
So it's essentially the difference between what was predicted at a given moment and what
is actually sensed.
I'll return to this in a second, like in a lot of detail.
But a few things to note first.
I guess the most important is that really conceptually it flips the traditional view
of how the brain works on its head.
Because what we're basically saying is, well, the top-down activity is the main driver.
And essentially sensory information is feedback, right?
The feedback is not what we thought it was, in a sense.
So it's sort of like moving from a conception of the brain as a passive collector of data
and a combiner of geometric or statistical information.
Moving from that to a view of the brain as a kind of query machine, right?
It's sort of a perception is like a Google search, right?
You ask a question and you get an answer.
So a visual cicade is asking the world a question, essentially.
So perception is an answer to that question.
So it's a complete inversion of the received view, in that sense.
So now we're going to get a bit more technical.
So how does this work?
It works via this broad framework that we can call generative modeling.
You're going to see this a lot in the following slides.
The scheme is always basically the same.
You have two things.
So when I say the active inference is simple, I mean it.
You can essentially explain it with two circles and a line, as I'll try to do now.
So the two circles here represent, on the one hand, data, right?
The data that's available to us, so observations,
things that are capable of registering observations or measurements, on the one hand.
And on the other, the hidden or latent factors that actually cause
the data that we're observing, right?
So in this graph, causality flows from left to right, right?
You have states in the world that cause the blue arrow,
the data that we're observing on the right here.
And so what we want to do, if we go back to this problem that I presented of the
brain having to solve this inference problem, right?
Is it a tiger?
Is it a twig?
So basically the brain, according to active inference at least,
what the brain is busy doing is kind of reversing this arrow, right?
So the arrow of causality goes from hidden states to data.
And what we want to do is move from the data that we have to an inference of
what the most likely causal factors are that caused the data, right?
So I say data in a kind of broad sense because it's important to keep in mind
the historical context in which these methods were initially developed.
My mentor at UCL, Carl Friston,
developed these generative modeling techniques for fMRI initially.
So in fMRI, which you have is a bold signal, and
what you want to infer is the underlying neurobiological activity that caused the bold signal.
So just by show of hands, how many of you are familiar with neuroimaging and
like more technically?
Okay, so a fair bit of you.
Just as a refresher, for those of you who don't know,
when you do fMRI, you're not directly measuring brain activity, right?
These fancy heat maps of the brain that you see are not direct measurements of brain activity.
What we measure is a proxy for brain activity, right?
So it's basically a measure of this bold signal,
stands for blood oxygenation level dependence signal.
It essentially consists in putting someone in a brain scanner, which is basically a big magnet,
and then you measure essentially a byproduct of oxygen consumption by neurons in the brain, right?
So what you're measuring is an effect of the neural activity that you really find interesting.
And so what you would like to do then is from this signal, right?
From this data, you want to infer the most probable neurobiological activity that caused the data.
So this is the technique that was pioneered in the 90s by Carl Friston.
In the 90s, they were using statistical parametric mapping, which I won't get into very much,
but essentially, for reasons that we'll be able to see a bit later,
these techniques allow us to move from this data to a model of essentially the connectivity of the brain,
the most likely connectivity that would have generated the data that we're interested in explaining, right?
Active inference in its more familiar format, for those of you who work on it, comes in at a second level.
What happens when the data that we want to explain is literally the sensory signals that the brain is receiving all the time?
How does that change the general scheme?
Well, it changes it in that what we're doing now is constructing a model,
not just of the underlying neurobiological activity that causes a signal that we measured,
but rather constructing a model of what causes the sensory signals of an organism.
So the major part of what causes our sensory experience is us acting in the world in various ways, right?
Think only of the visual saccades that you're performing many times a second.
We are, as embodied agents, are the main causal driver of our phenomenology, in a sense.
So active inference, per se, is about characterizing this, right?
It's about characterizing a model of how our sensory data were generated.
So I'll return to that in a lot of detail later.
I just want to quickly say that there's a new kind of tier of active inference that's being developed right now.
This is probably the least explored, most cutting edge application of this framework.
What does it look like when the data that we're trying to explain is outcomes of the diagnostic process, right?
So the data that I'm trying to explain, for example, is a schizophrenia diagnosis or a depression diagnosis, right?
So then the model that we're writing is a model of the process that generated the diagnostic classification, per se.
And for me, as a philosopher, this is really interesting because this is where it gets really meta, right?
Like, this is where we are part of the model, in a sense.
We, as clinicians and researchers, really are represented in the model that generates the data if the data is psychiatric diagnosis.
Anyway, so the point of generative modeling, the way that it's been used for a few decades now, is this kind of very simplified pipeline.
On the one hand, you have an experimental setup that generates data that you want to explain.
And on the other, you have a computational setup that is effectively a model of the process involved in your experimental setup.
So then you get this kind of circular relation going where your experimental setup provides data.
You can essentially model or create models of the most probable causal process to have generated your data.
That, in turn, can be used to refine your hypotheses and your methodology, right?
That you're using in your experimental setup.
And so there's this kind of circular motion between modeling work and computational work that we capitalize on, essentially,
the generative modeling paradigm.
Okay, so what is a good model?
A good model is a model that generates a small amount of error.
So we'll see what that looks like in the brain.
The way that, so remember, I gave this kind of general framework to understand what's going on, like, in the brain in terms of active inference.
I wanted to unpack it a bit more.
At any layer of cortical activity or really of any neural activity, according to this framework,
what's happening is that layers above and at the same layer, any unit is receiving predictions about what it should sense, right?
And basically, what's going on is a constant comparison between what the brain expects to perceive and what it actually does perceive.
And the discrepancy between these two signals is what gets shuffled up the hierarchy in this kind of fashion, right?
So, I mean, interestingly, this happens as early as the retina.
So, I mean, I won't get into the neurobiology too much,
but there's reason to think that this predictive architecture stuff happens everywhere, at every level,
that really there's no prediction-free layer of cognitive activity.
Yeah, as early as the retina, like I was saying.
So if we want to think about it metaphorically, but we're sort of moving towards a formalism here,
like what it means then to minimize this error quantity is basically you have a cloud of data that you're trying to explain, right?
And you're constructing a model of how that data was generated, and therefore what you should expect of that data.
So what you're essentially doing is fitting a curve to a cloud of data,
and the prediction error per se is this distance here.
It's this distance between the value that the model predicted and the value that you're actually perceiving.
So there's a story to tell about thermodynamics and information theory that I'm going to skip over,
but we can return to later if you're interested.
So essentially, according to active inference slash predictive processing, there are two ways to minimize this quantity, right?
This discrepancy between what I expected and what I perceived.
The first is to change your model.
I think it's the most obvious, right?
If there's a discrepancy between what you predicted and what you perceived,
the simplest way to reduce the discrepancy is just to change what you predicted.
So we just change the model.
We'll see what that means a bit later.
We'll adopt a different model, and this model generates less prediction error than the original one, right?
Which means that it's better.
It means that it's more reliably explaining the variance in your data.
Of course, it's always possible to overfit.
So I mean, this is just a brief mathematical excursus,
but for those of you who have done statistics,
you'll know that if you have n data points,
you can always arbitrarily construct a polynomial function of degree n plus 1
that'll go by all of the points that you're interested in,
but capture none of the trends that you're trying to explain, right?
So I just ad hoc, you know, you probably can't tell, right?
Because it's so gracefully traced,
but I ad hoc constructed a function here,
which I assure you is a very rigorous one,
and it passes by all of the data points as you'll notice,
but what you'll also notice is that there is a trend in the data
and that this function isn't capturing it anymore.
So you might think that if the brain does something like that and overfits data,
then on the long term, it's going to generate a lot of prediction error.
And so, you know, some disorders you might think of delusional ideation
and this kind of thing has been understood in terms of overfitting.
So overall, you're better with a slightly simpler model
that generates some error, right,
but that on average sticks to the trend in the data better.
So this also speaks to a more general, more, I guess, philosophical point here,
is that in this framework, the error is your signal, right?
When we say like minimizing prediction error,
we don't want zero prediction error,
because that would mean having zero signal.
So one way to think about this is your model has to be simpler
than the reality that you're modeling, right?
So think of a map, right?
A model is basically a map.
A one-to-one scale map would be completely useless, right?
Like if I had a, you know, a map of this room would be like,
you know, I wouldn't be able to look at it
and it wouldn't contain, you know, information in a way
that could be used in a useful fashion, right?
The model has to be simpler than the data.
This means that just by construction, there will always be prediction error, right?
Just that's how it works.
You need prediction error because it's your signal.
Okay, so first way to minimize prediction error is to change your model, okay?
Second way to minimize prediction error is to change the world, right?
So if there's this discrepancy, right,
you know, you might want to make your model more like the world,
but you might also just want to make the world more like your prediction.
And so inactive inference slash predictive processing,
action itself is understood as a form of self-fulfilling prophecy in the following sense.
You start off with a prediction of action and you're not moving, right?
So I don't know, like, I don't have a prop handy I usually do,
but suppose I had a glass of water handy and I wanted to reach for it.
Well, the way that it's unpacked in the predictive processing framework is
I produce a prediction of movement, but I'm not moving, right?
So if so fact, though, if I'm not moving and I expect to move, that induces a prediction error, right?
And so the cool thing about using a prediction error as your signal
is that it updates over time, in real time, right?
So I basically initiate movements that in real time reduce this prediction error.
So it can be used as a kind of knowledge driven real time signal that enables adaptive motor behavior.
So this is just what I said.
You generate an action prediction, you're not moving, therefore a prediction error is induced.
And that can be used to guide online motor behavior.
Okay, so now I ask you to buckle in really tightly because we're going to do some math.
So we're going to look at the generative models themselves.
I've talked a lot about the models, et cetera.
So what are these models?
So remember the general flavor of this is we have data and we're trying to infer the most probable latent states
that cause the data that we're trying to explain, right?
So causality flows in this direction, inference flows in that direction.
Okay, so how exactly do you quantify, you know, how well a model explains your data?
So the way that you typically will do this is by constructing several alternative models, right?
That are each encode a different hypothesis about how the data might have been caused.
And then you evaluate how well that model accounts for the variance in your data.
So if you want to compress that algorithm into like a quantity, what you get is this spooky variational free energy thing,
which, you know, you might also just call the model evidence or a bound on the model evidence.
Basically this variational free energy thing is a measure of how much evidence is provided by the data
for a given model of the process, right?
So it's important to stress this, the free energy isn't an energy in the sense of thermodynamics, right?
It's a measure of how well your model explains the variance in your data.
So I think that's an important takeaway, you know, to like understand what is going on here, right?
Let me repeat, the variational free energy is just basically a measure of how well your model explains the data, the variance in your data.
Yes?
So you use an upper bound law in your article as well, and I just finished it.
Okay, well so the model evidence itself, if you actually try to compute it, usually just for calculation reasons you won't be able to do it.
Typically it's because these always involve a normalization term, you kind of have to divide by this term,
and this term calculating it involves summing over an infinite set of states.
So basically you can't do it analytically, and so what you do instead is you construct this quantity of the variational free energy,
which is basically an upper bound on the model evidence.
What that means is that like the model evidence, your variational free energy basically tells you you at least have this much evidence, right?
You might have a bit more, but you at least have that much evidence from your model, for your model, right?
So just to rehearse this again, right?
So always the same formula, you have some data, you're trying to construct the most probable model of the process that caused the data,
and you get added through inference.
So this is what the models typically look like, and Lawrence once told me that just presenting these models is basically an act of intimidation.
It was more complicated than this one. This one is very consoling and comforting.
Thank you. So rather than engage in this act of violence, I'll try to unpack a bit what these models are, right?
So the most basic generative model is this thing here, and this thing here is just this thing here, right?
Everyone sees, right? You have your data, you have your most probable causal states and a relation of inference between them, right?
So here you have, it's just flipped, here you have your data, right?
The most probable causal state that caused your data that you're trying to infer, and now what we're doing is just adding a bit of mathematical niceties.
Mainly we're parameterizing these relations, right?
So this A thing is just a likelihood mapping, and basically what it does is specify for every state, if this state was actually the case, right?
What kind of data would I expect to see, right?
So for example, if it is night, I would expect it to be dark outside. This is the kind of thing that this A characterizes.
So one important distinction to make sense of these graphs is that the circles are either your data or the quantities that you're trying to infer, right?
And the squares are essentially parameters of the relations between these circles, right?
So they characterize the arrows between.
So like I was saying, S is just a state that you're trying to infer.
O is your data.
A is your likelihood mapping from your states to your observations.
Basically, like I said, assuming that this state is the case, what is the probability of observing this or that?
And D is just your prior beliefs over states.
So like independently of any observations that you might make, what do you think the state is, right?
So just I'll run through some of the math, right?
So if you see on the left hand side here, S equals D is just saying, well, my guess about the state independent of any data is just my prior about the state, right?
Pretty simple.
This O equals AS thing.
Again, it's just describing this relation that the likelihood mapping has, right?
Your outcome just is a combination of this mapping between your states and observations and your beliefs about states.
And here to the right, what you see in a mathematical form that I won't get into too much is that the state that you predict, right?
The posterior estimate over states is equal to some combination that I won't get into too much of your prior beliefs and what you learn from your observations from the data and how that relates to states, right?
So these look very intimidating at first, but like we can unpack them in a way that's, okay.
These were the models that we were using up until 2015, roughly, and in 2014, 15, 16, what happened was that we started considering temporal depth and temporality.
So here we're adding, and it looks a bit more complicated, but we're really just stacking the first layer model, right?
Does everyone see that?
Like, see, this thing here is just this, right?
So basically the only thing that we've added here is this B matrix, and the B matrix specifies your beliefs about the way that states transition over time.
Again, independently of the observations that you're making.
This is very clear from the equations here to the left.
So it just says your state at time t plus one is a combination of this B matrix, right?
That captures your beliefs about the way states transition and the state at time t, right?
Nothing exorbitant or complicated.
And again, the posterior thing is just telling you, okay, your posterior belief about a state is just equal to some combination of your state and B in the past, your state and B in the future, and what you're seeing in the present.
Right?
So again, it looks intimidating, but it's actually less complicated than it is.
I note HESP at all, 2020.
This presentation of the material was developed by one of my close collaborators and friends, Casper HESP, at the University of Amsterdam.
This is from a paper that we've pre-printed now called Deeply Felt Affect, which, I mean, besides articulating emotional inference using the active inference framework, also presents a tutorial of the more formal kind of package that underwrites active inference.
So if you're interested, I can send you the paper.
Okay.
So, so far we've talked about perception at from time, from moment to moment, right?
This, this relation between O and S mediated by A.
Then if you consider, you know, the sequence of states, what you're doing is introducing beliefs about the way the world transitions states evolve over time independently of your observations.
You'll notice that your prior here is effectively disappeared from these steps here because your prior just feeds in your first kind of estimation, and then it kind of goes on.
So you're really just using S2 as your new prior for the next time, sorry, S1 as your prior for the next time step and so on.
Okay.
But so far, this, this could have just been a tutorial on reinforcement learning.
Active inference does something special, which is this.
So again, don't panic.
This is just what we've just seen.
All I did here was add a circle, or which is this policy selection thing, this pie thing here.
So in active inference, the way that action selection is implemented is essentially by choosing transition matrices by choosing your beliefs about the way that the world changes over time.
So again, we have these B matrices that we just discussed here, right?
These beliefs about the way the states in the world transition.
Well, what this policy thing, this policy selector does up here is effectively choose a series of B matrices, right?
So to act is to have beliefs about the way you think the world should evolve in this kind of self confirming way that we discussed earlier.
And all the stuff at the top does is just like put the variational free energy thing into into the loop.
This is definitely beyond the level of mathematical discussion that I had in mind for today, but essentially you have this G thing.
G is just your expected free energy, right?
So it's the amount of free energy that you expect for every possible action that you could take.
And essentially what this thing is doing is selecting the action that leads to the lowest free energy, right?
So again, it's just this action selection thing that leads to prediction error minimization or that runs on prediction error minimization.
What you'll notice is that pi again is a circle.
It's one of these quantities that we have to infer.
So contrary to more traditional schemes in motor control where you just program a command and it's affected,
in active inference, your little agent is effectively trying to infer, well, what am I doing, right?
No, but really, like, on the basis of your prior beliefs, right?
And on the data that you're receiving from your senses, what is it that I must be doing, right?
So this again contains a lot of weird math that I'm not going to go into, but I just wanted to show you this because it's cool.
So here you have your generative process.
So this is just basically a model in the sense of we are modeling just the process that actually generates the data that's sort of like invisible to us, right?
So this is sort of like, it's hidden to us, but like there is a process that generates our data.
And this is a Forney-style factor graph representation of the generative model that I just showed you.
It's all the same terms, right?
You've got your B's, your D's, and your A's and all that.
I won't get into how you go from one to the other because it's a little complicated.
I was just putting this slide up because I wanted you to notice two things.
Well, it's the same thing, but it has two elements.
Where the process that generates our data and the model of our data, like where they meet is at two points.
You have these u-things up here, which are the actual actions that you're performing, right?
And these the o's, your observations.
So basically the generative model and the generative process meet at action and observation.
That's sort of the two connective points.
I mean, this is complicated and we can return to it.
Then this is the last technical thing I wanted to show you.
Like where we're at now with the modeling is that we introduce a new layer of states into the model that are about states at the lower layer.
So in our new work we're working on metacognition, emotional content, and kind of self-control, mindfulness meditation and this stuff.
So what we did here is connect a layer of states at the top here with their own transition matrices and everything.
Two states at the lower level.
So what we've begun doing is treating states at the lower level here as observations.
So we're treating the results of inference as new data that the system can then use to make inferences about itself and basically how well it's doing.
So I told you this paper is called Deeply Felt Affect because we use this kind of layered structure to account for basically beliefs about oneself and eventually self-control in this kind of thing.
For the sake of people who may get a little lost when the discussion is too abstract and too formal, if you could think of a couple of concrete examples without technical language going beyond something like, oh, here's how a monkey moves through a room to find a high quality tree.
Something more like, how would you help make sense of human emotions and moods, for example?
Sure.
Checking in with oneself in phenomenology, how would you make all that relevant?
So this bottom part of the model, you can make it do whatever you're interested in making it do.
So you could have a model of having a conversation.
So in this kind of model, it might be simpler to explain that bit just with this.
So the data that you have to explain might be spatial configurations of expression in someone's face, the auditory sensations that you're registering.
So for each of these modalities, you have an A matrix that specifies the way that you believe these sensations are typically related to the states that cause them.
And you have this policy selection thing that affects beliefs about the way that states evolve.
So in a conversation, for example, you would be resolving that continuously.
I would be inferring these lower level states like, what is Sam telling me now?
And I would be selecting a policy that facilitates this interaction that ideally gets us to some kind of coupled point where we're able to interact.
So the point of adding these additional layers is that now I can make inferences about my inferences.
I can be like, well, am I sure that Sam feels good today?
I don't know, he seems tired or something.
Or I don't know, it's things to do with, for example, confidence in your own judgments.
How well am I doing? How do I feel about these inferences that I'm making, this kind of thing?
So inferences about our inferences.
The reason why this is important, I think, just generally for humans is that I'm telling this to you.
You know, in particular, most of human thinking is thinking through other minds, right?
It's thinking about other minds.
I don't want to take us too far away from your main narrative, Sam.
But the way you're describing it now, it seems like there are maybe two things conflated or maybe just one part of them being instantiated.
I'm talking to you and I'm thinking, gee, I wonder how Maxwell's feeling today.
I'm thinking something about the nature of our conversation, and that's a kind of self-consciousness.
I wonder if I'm making a good impression on Maxwell.
So what it brings to mind is, is there a generic self-consciousness?
Like you just have lots of those thoughts, because this version is just very specific.
I just have one question. I just have one hypothesis.
But in real life, these things come in bundles.
It comes with a whole, and that's what we would then call an affective stance, let's say.
I'm feeling uneasy with you, so I'm engaging with lots of alternative hypotheses and lots of uncertainty at that level.
So then I'm getting clear as I'm talking, which is probably part of the point here.
So how would you model that?
In other words, it's not simply one thing, one bit of metacognition or one bit of reflection on the nature of interaction.
It's a different, I would describe it informally again, it's like a different stance or a different time.
Well, one important thing is that we can entertain more than one different model.
So there's some interesting work by Isomora, I think, who essentially implements like a simple,
and these are very simple toy systems for now.
What we're doing now is scaling them up to human systems, but it's basically a bird simulation,
and it listens to eight different birds, and it recognizes them essentially,
because it entertains eight different models that might correspond to this or that.
So the capacity to simultaneously entertain different models might help to...
So let me sharpen what I'm trying to say again.
So to me, it's different if I say, okay, I'm uncertain where I stand with Maxwell,
and so I have to entertain many different models versus I'm just, here's my hypothesis.
And I was having one policy at that level is very different than having many policies.
So to model that, do we have to have yet another level that's deciding...
Okay, I see your question.
...do we have one policy or do we have many policies?
Well, the thing to note about...
And what level does that kind of uncertainty manifest?
The thing to note about this is that this top level of inference makes this bottom level accessible to the system.
So basically inferences at this bottom level just happen,
so they guide action but in an implicit sense,
whereas these states are literally about the states below.
So attentional states, for example, exist at this level.
States about my sensory states.
So that helps, because then if you think, what happens when you practice mindfulness meditation
and you stabilize state of mind.
So you're going from what my state of mind is like, where there's not one of those red circles,
there's dozens of them and they're all different hypotheses,
but what's actually happening to me and they're all interfering with each other,
which then has a disruptive effect, let's say, on my attentional state,
versus having one that is coherently reinforcing a particular mode.
Anyway, I guess getting too far down into one...
No, what you're seeing is that's really, I think, an important point.
And in work that I'm not going to be presenting today, which is basically where we're at right now,
we've got a three-layer system going to explain mindfulness meditation.
This is a work by Lars Sandved Smith, in particular, that we're doing with Carl Friston at UCL.
Where basically, on top of this, we have yet an additional layer to make the attentional states opaque to the system.
So that's kind of what I'm getting at, the idea that you could learn to manage your own attention.
Yeah, precisely.
So this is already there kind of implicitly, right?
Look, we have a B matrix that links these different things.
So we already have beliefs about the way that these higher-order mental states are transitioning.
What would it look like if we implemented policy selection there?
Because you can, it's a B matrix.
So why not just, like, cook these up to a higher-level pie?
You could.
You could.
And that's what we end up doing in the, anyway.
So I guess we will be putting this on YouTube.
Yeah.
You can snip out all the parts here.
Are the B matrices, you said you can treat them like policies?
A policy is a sequence of B matrices.
Okay.
Right?
Because that's what it is to select an action under this framework,
is to change your beliefs about the way that states are supposed to evolve.
It's an array of B matrices.
Yeah.
Well, it's, in the morse...
Why wouldn't you just use your B matrices?
Well, because in the most simple case, it's just a series of B matrices.
But what if you're considering counterfactual depth?
Right?
Okay, so you just...
Yeah, then you have, like, trees of B matrices and so forth.
You can create a network.
Yeah.
There's feedback between, like, different contexts and principles based on context, for example.
Like, Dr. Kermayers, maybe what you're getting at.
Well, let me try to just finish.
I have about, like, ten minutes left of material.
One small point, and you'll come back to this.
Because for me, so it's appealing, again, that you get these hierarchical levels,
and it seems like you're getting more...
Getting things that can model self-reflection and all these different, you know,
strategies for managing oneself, etc.
However, the more layers you add, it seems to me, you don't necessarily have more data, do you?
Exactly.
So the problem of, like, your complexities back to the overfitting, whatever,
having way more complexity to your modeling, you actually have in most cases.
I mean, that's a really good question.
And we struggled really hard, you know, last time I was in London,
a lot of what we did was ask, well, do we really need a third level?
Can't we just do policy selection at the second level?
But the key thing is this opacity thing, is that to make these states of inference
accessible to the system, to allow the system to use them as data for further inference,
you need a higher order set of states.
So that's sort of...
So if you give me ten minutes, I'll be done.
Cool.
Okay, so why is this framework Bayesian?
I mean, we've sort of seen it before.
But basically, Bayes' rule is just a way to combine prior probabilities
with your likelihoods to get posterior probability.
So, yeah, Bayes is important, so I put it on a little bit of crap.
So your prior probability is just the probability of some event
before any evidence is taken into account.
Your likelihood is the likelihood of some event,
the probability of some event given some evidence.
That should be probability of some event given some evidence.
So just to illustrate the difference, if Houdini magics away an elephant on stage,
well, you know, there's a high likelihood that the elephant is gone.
You can even go on stage and collect more data.
The elephant clearly is gone, right?
But we know that there's a low prior probability that elephants just dematerialize
because elephants are solid objects and those don't typically dematerialize.
So Bayes' rule is just a way to combine these quantities optimally.
So basically, the cool thing about this is that you can always use your posterior at one step.
So the result of combining your prior and your likelihood as your new prior at the next step.
So there's this nice kind of bootstrapping thing that you can do.
And the way that this relates to what we've been talking about is basically the descending connections,
right? They carry your priors.
And your prediction error is basically always integrating the data that you're generating.
So it's essentially a likelihood.
And the Bayesian brain says, OK, well, this is what the brain essentially looks like, right?
You've got likelihoods flowing up in the form of unexplained prediction error
and prior probabilities flowing down, right, in the form of neural prediction.
These schemes are typically hierarchical. I hinted at this earlier.
The reason why they are is that as you, basically as you go up the hierarchy,
the things that are represented are more stable over time.
And as you go towards the more sensory end, things change faster, right?
So for those of you who are familiar with Fourier analysis, basically,
you can take any image and decompose it into frequency bands, right?
So high spatial frequency and low spatial frequency information, right?
Which you'll notice, so high spatial frequency is to the right here,
low spatial frequency is to the left here.
And what you'll notice is that in a conversation,
high spatial frequency information changes much faster than low spatial frequency information.
Unless you're someone like myself, you know, just moves around his face
and like makes funny expressions, like typically most people, like, you know,
their lips move faster than the rest of their face, right?
So you might think that this is implemented in the hierarchies that the brain encodes,
the hierarchies of information that the brain encodes.
And according to the predictive processing framework, this is precisely what we see.
So I'll skip over that. All right.
So yeah, so you have hierarchies of information that encode regularities that are time sensitive
and you have prediction error minimization going across this hierarchy
and it essentially explains the way that the brain reacts to stimuli.
All right. So that was predictive processing.
We can scale this up, is what I submit to you now.
So I have this, a winter is coming.
Just to talk about my favorite example of active inference.
It's like, well, when I get cold, I don't know about you, but I put on a parka,
especially like in Canada, it's cold, right?
So we can cast this as active inference as well, I think.
So the point for the next two slides is basically going to be,
it's not just the brain that engages in active inference, it's every cell in your body,
every organ system in your body, and effectively maybe social groups as well.
And this is what we work on.
Okay. The last bit of math that I want to introduce to you, I think is simpler than the rest.
It's called a Markov blanket.
And what I want to submit to you is that rock, cells, organs, animals, social groups,
basically anything that exists at all has a Markov blanket.
So a Markov blanket is just a way to use statistics to answer
what is traditionally a philosophical question, right?
The philosophical question being, what does it mean to exist?
What does it mean to exist as a thing, right?
What does it mean to be a thing?
So, I mean, if you ask a philosopher, they'll tell you like a story about metaphysics
and, you know, like type token identity theory and whatever.
So we've eschewed all of this for something much more simple, which is to say,
okay, well, if we're interested in a system, right?
So suppose the system that we're interested in is the brain, right?
Because we've talked about it and we want to differentiate it from the environment
in which it is embedded.
What we'll do essentially is introduce or define a third set of states that mediates
the causal relations between the system that we're interested in defining and its environment.
So these are known as sensory and active states.
So these are metaphors, of course.
The way that they're defined is by their connectivity, right?
So sensory states cause internal states but are not caused by internal states.
And active states cause but are not caused by external states.
So, I mean, the point of doing this is to say, okay, to exist is to be endowed with some degree
of conditional independence relative to your environment, right?
So if you consider this mass of gas, right, here, it's not a system because it'll just dissipate, right?
Like, there's no robust sense in which it's independent of its environment in any sense.
Like, you know, it's gone, right?
So a Markov blanket is a way of saying, conditioned on the existence of this set of states,
the sensory and the active states, the internal states of the system are independent
of the external states of the world.
Is that clear to everyone?
Okay.
So active inference then is a story about how internal states, which encode our model, right,
and the active states, which are like our skeletal muscles and so forth,
change to minimize free energy.
And the end effect is to allow this inference process to happen, right?
So the inference here meaning that the free energy or prediction error diminishes, right?
So making the internal states more like the external states and vice versa.
Again, this is just this story, right?
It's just that as we've seen, inference also occurs through action.
So basically, like you have the state estimation bit going on here where we're kind of inferring
what should be going on in the world.
You have the policy selection bit here, but this is just the story that we told, right?
But just now presented in terms of like the existence of the system.
I'm going to skip through that.
So yeah, this is a drawing by the famous physicist Huygens.
And Yela Brineberg has it in one of his papers on this.
Like an important point is that like an advantage of using active inference over other frameworks
is that it just, it comes from physics, right?
So you solve this inference problem, but using a framework that basically says,
well, inference is sort of like a rock falling down a cliff.
You're just falling to an energy minimum, right?
So the reason it's called variational free energy is with analogy to the thermodynamic quantity free energy, right?
In thermodynamics, free energy is the amount of energy left in a system that can perform work, right?
In the information theoretic context, the variational free energy is basically the amount of wiggle room
that you'll have on your parameters to get a better representational grip of the situation, right?
It's like, yeah, how much room is left on your parameters to do work, to do representational work?
Okay, last thing basically that I'll be talking about today.
So here's our Markov Blanketed System again.
I submit to you that all of the components of this system are also systems, right?
This is the observation that we started with.
I'm a system, I'm an organism, but I'm made of networks of organs that are themselves systems,
and the organs themselves are systems of cells and so forth.
So every component of a Markov Blanketed is itself Markov Blanketed.
So in this 2015 paper by Carl Friston, which I think is the first in the literature to do this,
they effectively connected two levels of description.
What they showed is that on the assumption that what you see here first is I think seven, eight cells,
eight cells that share a generative model and over time reach a target configuration.
So it's basically a little creature with a head and a tail.
Everyone sees that.
And so what you have plotted here is basically the beliefs of each cell about what kind of cell they are essentially.
And so you have your free energy plotted here.
And so what you see is all of these cells share the same generative model.
So basically they're able to infer their place relative to other cells so long as they're able to communicate with other cells.
Because we all have the same expectations, right?
We all expect to sense the same kinds of things.
So basically the little units start off and their free energy spikes because they're trying to figure out what's going on.
But then as they communicate, the free energy starts to go down until it reaches a minimum value.
And when it's reached its minimum value here, the simulation reaches its target configuration, right?
So that, for me, this was like an eye-opening moment where I kind of, you know, with Carl realized that, you know,
this is how you effectively connect levels of organization, right?
Units at one level sharing a generative model are able to enact a target morphology.
So from there we generalize, this is from one of our physics of life reviews papers.
We're essentially just telling the story of how any Markov-blanketed system itself is composed of Markov-blanketed systems.
So you have this kind of recursively nested systems of systems of systems of systems approach.
What you might think of as a vertical stack of systems.
I say vertical because there are two dimensions to this system, really.
There's a vertical stack where cells compose organs, which compose organ networks and so on and so on.
But there's also a horizontal stack where at any scale of interest, like the relevant actors like cells, for example,
are also in the process of niche construction.
They're constructing an environment for themselves and they're effectively sharing a physical environment.
Yeah, so this kind of gets us here, right, where we started.
At least I hope I've made the point that, you know, via active inference there's at least the possibility for something like an integrated
science of culture mind and brain that takes all of these levels seriously, right?
And I'm just going to skip through this right to the thank you slides.
Yeah, so that's what I wanted to share today. Thank you for your attention.
Thank you to my funders. Special thanks to the people listed, particularly those of you who came.
And yeah, thanks for your attention.
Thank you.

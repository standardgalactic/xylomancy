So I'm very excited today to talk to you about this idea of kind of interpreting neural networks
to get physical insight, which I view as kind of a new, really kind of a new paradigm of
doing science.
So this is a work with a huge number of people.
I can't individually mention them all, but many of them are here at the Flatiron Institute.
So I'm going to split this up.
I'm going to do two parts.
The first one, I'm going to talk about kind of how we go from a neural network to insights,
how we should get insights out of a neural network.
The second part, I'm going to talk about this polymathic AI thing, which is about basically
building massive neural networks for science.
So my motivation for this line of work is examples like the following.
So there was this paper led by Kimberly Stakkenfeld at DeepMind a couple years ago on learning
fast subgrid models for fluid turbulence.
So what you see here is the ground truth.
So this is kind of some box of a fluid.
The bottom row is the learned kind of subgrid model, essentially, for this simulation.
The really interesting thing about this is that this model was only trained on 16 simulations,
but it actually learned to be more accurate than all traditional subgrid models at that
resolution for fluid dynamics.
So I think it's really exciting kind of to figure out how did the model do that and kind
of what can we learn about science from this neural network.
Another example is, so this is a work that I worked on with Dan Tuayo and others on predicting
instability in planetary systems.
So this is a centuries old problem.
You have some, you know, this compact planetary system and you want to figure out when does
it go unstable.
There are literally, I mean, people have literally worked on this for centuries.
It's a fundamental problem in chaos, but this neural network trained on, I think it was
maybe 20,000 simulations.
It's not only more accurate at predicting instability, but it also seems to generalize
better to kind of different types of systems.
So it's really interesting to think about, okay, these neural networks, they've seemed
to have learned something new.
How can we actually use that to advance our own understanding?
So that's my motivation here.
So the traditional approach to science has been kind of, you have some low dimensional
data set or some kind of summary statistic and you build theories to describe that low
dimensional data, which might be kind of a summary statistic.
So you can look throughout the history of science, so maybe Kepler's law is an empirical
fit to data and then of course Newton's law of gravitation was required to explain this.
Another example is like Plank's law.
So this was actually an empirical fit to data and quantum mechanics was required, partially
motivated by this to explain it.
So this is kind of the normal approach to building theories.
And of course some of these, they've kind of, I mean it's not only this, it also involves
many other things.
But I think it's really exciting to think about how we can involve interpretation of
data driven models in this process, very generally.
So that's what I'm going to talk about today.
I'm going to conjecture that in this era of AI, where we have these massive neural networks
that kind of seem to outperform all of our traditional theory, we might want to consider
this approach where we use a neural network as essentially compression tool or some kind
of tool that pulls apart common patterns in a data set.
And we build theories not to describe the data directly, but really kind of to describe
the neural network and what the neural network has learned.
So I think this is kind of an exciting new approach to, I mean really, really science
in general, I think especially the physical sciences.
So the key point here is neural networks trained on massive amounts of data with very flexible
functions, they seem to find new things that are not in our existing theory.
So I showed you the example with turbulence, you know we can find better sub-grid models
just from data, and we can also do this with planetary dynamics.
So I think our challenge as scientists for those problems is distilling those insights
into our language, kind of incorporating it in our theory.
I think this is a really exciting way to kind of look at these models.
So I'm going to break this down a bit.
The first thing I would like to do is just go through kind of what machine learning is,
how it works, and then talk about this, kind of how you apply them to different data sets.
Okay so just going back to the very fundamentals, linear regression in 1D, this is, I would
argue if you don't really have physical meaning to these parameters yet, it is a kind of type
of machine learning.
And so this is, these are scalars, right, x and y, those are scalars, phi 0, phi 1, scalar
parameters, linear model.
You go one step beyond that and you get this shallow network.
So again this has 1D input, x, 1D output, y, but now we've introduced this layer.
So we have these linear models, so we have three hidden neurons here, and they pass through
this function A, so this is called an activation function.
And what this does is it gives the model a way of including some non-linearity.
So these are called activation functions.
The one that most people would reach for first is the rectified linear unit, or RELU.
Essentially what this does is it says if the input is less than 0, drop it at 0, greater
than 0, leave it.
This is a very simple way of adding some kind of non-linearity to my flexible curve that
I'm going to fit to my data, right?
The next thing I do is I have these different activation functions.
They have this kind of joint here at different points, which depends on the parameters.
And I'm going to multiply the outputs of these activations by a number.
That's kind of the output of my kind of a layer of the neural network.
And this is going to maybe change the direction of it, change the slope of it.
The next thing I'm going to do is I'm going to sum these up.
I'm going to superimpose them, and I get this is the output of one layer in my network.
So this is a shallow network.
Basically what it is, it's a piecewise linear model.
And the joints here, the parts where it kind of switches from one linear region to another,
those are determined by the inputs to the first layer's activations.
So it's basically a piecewise linear model.
It's a piecewise linear model.
And the one cool thing about it is you can use this piecewise linear model to approximate
any 1D function to arbitrary accuracy.
So if I want to model this function with five joints, I can get an approximation like this
with 10 joints like this, 20 like that, and I can just keep increasing the number of these
neurons.
And that gives me better and better approximations.
So this is called the universal approximation theorem.
So it's that my shallow neural network just has one kind of layer of activations.
I can describe any continuous function to arbitrary precision.
Now that's not, I mean this alone is not that exciting because I can do that with polynomials.
I don't need, the neural network's not the only thing that does that.
I think the exciting part about neural networks is when you start making them deeper.
So first let's look at what if we had two inputs?
What would it look like?
We had two inputs.
Now these activations, they are activated along planes, not points, they're activated
along planes.
So for, this is my, maybe my input plane, I'm basically chopping it along the zero part
and now I have these 2D planes in space.
And the next thing I'm going to do, I'm going to scale these and then I'm going to superimpose
them.
And this gives me ways of representing kind of arbitrary functions in now a 2D space rather
than just a 1D space.
So it gives me a way of expressing arbitrary continuous functions.
Now the cool part, oops, the cool part here is when I want to do two layers.
So now I have two layers, so I have this, this is my first neural network, this is my
second neural network and my first neural network looks like this.
If I consider it alone, it looks like this.
My second neural network, it looks like this.
If I just like, I cut this neural network out, it looks like this, okay?
When I compose them together, I get this, this, this shared kind of behavior where, so
I'm, I'm composing these functions together and essentially what happens is, it's almost
like you fold the functions together so that I experience that function in this linear
region and kind of backwards and then again.
So you can see there's, there's kind of like that function is mirrored here, right?
It goes, goes back and forth.
So you can make this analogy to folding a piece of paper.
So if I consider my first neural network like this on a piece of paper, I could essentially
fold it, draw my second neural network, the function over that, that first one and then
expand it and essentially now I have this, this function.
So the, the cool part about this is that I'm sharing, I'm kind of sharing computation because
I'm sharing neurons in my neural network.
So this is going to come up again.
This is kind of a theme where we're doing efficient computation in neural networks by
sharing neurons and it's, it's useful to think about it in this, this, this way, kind
of folding paper, drawing curves over it and expanding it.
Okay, so let's go back to the physics.
Now neural networks, right, they're efficient universal function approximators.
You can think of them as kind of like a type of data compression.
The same neurons can be used for different calculations in the same network.
And a common use case in, in physical sciences, especially what I work on, is emulating physical
processes.
So if I have some, my, my simulator is kind of too expensive or I have like real world
data and my simulator is not good at describing it.
I can build a neural network that maybe emulates it.
So like I have a neural network that looks at kind of the initial conditions in this
model and it predicts when it's going to go unstable.
So this is a, this is a good use case for them.
And once I have that, so maybe I have this, I have this trained piecewise linear model
that kind of emulates some physical process.
Now how do I take that and go to interpret it?
How do I actually get insight out of it?
So this is where I'm going to talk about symbolic regression.
So this is one of my favorite things.
So a lot of the interpretability work in industry, especially like computer vision language,
there's not really like, there's not a good modeling language.
Like if I have a, if I have a model that classifies cats and dogs, there's not really like, there's
not a language for describing every possible cat.
There's not like a mathematical framework for that.
But in science, we do have that.
We do have, oops.
We do have a very good mathematical framework.
So in science, right, so we have this, you know, in science we have this very good understanding
of the universe and we have this language for it.
We have mathematics, which describes the universe very well.
And I think when we want to interpret these data driven models, we should use this language
because that will give us results that are interpretable.
If I have some piecewise linear model with different, you know, like millions of parameters,
it's not really useful for me, right?
I want to express it in the language that I'm familiar with, which is mathematics.
So you can look at like any cheat sheet and it's a lot of, you know, simple algebra.
This is the language of science.
So symbolic regression is a machine learning task where the objective is to find analytic
expressions that optimize some objective.
So maybe I want to fit that data set and what I could do is basically try different trees.
So these are like expression trees, right?
So this equation is that tree.
I basically find different expression trees that match that data.
So the point of symbolic regression, I want to find equations that fit the data set.
So the symbolic and the parameters rather than just optimizing parameters in some model.
So the current way to do this, the state of the art way is a genetic algorithm.
So it's kind of, it's not really like a clever algorithm.
I can say that because I work on it.
It's pretty close to brute force.
Basically what you do is you treat your equation like a DNA sequence and you basically evolve it.
So you do like mutations, you swap one operator to another, maybe you cross-breed them.
So you have like two expressions which are okay.
You literally breed those together.
I mean, not literally, but you conceptually breed those together, get a new expression
until you fit the data set.
So yeah, so this is a genetic algorithm based search for symbolic regression.
Now the point of this is to find simple models in our language of mathematics that describe a given data set.
So I've spent a lot of time working on these frameworks.
So pyser, symbolic regression.jl, they work like this.
So if I have this expression, I want to model that data set.
Essentially what I'm going to do is just search over all possible expressions until I find one
that gets me closer to this ground truth expression.
So you see it's kind of testing different branches in evolutionary space.
I'm going to play that again until it reaches this ground truth data set.
So this is pretty close to how it works.
You're essentially finding simple expressions that fit some data set accurately.
So what I'm going to show you how to do is this symbolic regression idea is about
fitting, kind of finding models, symbolic models that I can use to describe a data set.
I want to use that to build surrogate models of my neural network.
So this is kind of a way of translating my model into my language.
You could also think of it as like polynomial or like a Taylor expansion in some ways.
The way this works is as follows.
If I have some neural network that I've trained on my data set, whatever,
I'm going to train it normally, freeze the parameters.
Then what I do is I record the inputs and outputs.
I kind of treat it like a data generating process.
I try to see like, okay, what's the behavior for this input, this input, and so on.
Then I stick those inputs and outputs into PICER, for example, and I find some equation
that models that neural network, or maybe it's like a piece of my neural network.
So this is building a surrogate model for my neural network that is kind of
approximates the same behavior.
Now you wouldn't just do this for like a standalone neural network.
This would typically be part of like a larger model,
and it would give you a way of interpreting exactly what it's doing for different inputs.
So what I might have is maybe I'll have like two pieces, like two neural networks here.
Maybe I think the first neural network is like learning features,
or it's learning some kind of coordinate transform.
The second one is doing something in that space.
It's using those features for a calculation.
And so I can, using symbolic regression, which we call symbolic distillation,
I can distill this model into equations.
So that's the basic idea of this.
I replace neural networks, so I replace them with my surrogate model, which is now an equation.
You would typically do this for G as well.
And now I have equations that describe my model,
and this is kind of a interpretable approximation of my original neural network.
Now the reason you wouldn't want to do this for like just directly on the data
is because it's a harder search problem.
If you break it into pieces, like kind of interpreting pieces of a neural network,
it's easier because you're only searching for two n expressions rather than n squared.
So it's a bit easier, and you're kind of using the neural network
as a way of factorizing the system into different pieces that you then interpret.
So we've used this in different papers.
So this is one led by Pablo Lemos on rediscovering Newton's law of gravity from data.
So this was a cool paper because we didn't tell it the masses of the bodies in the solar system.
It had to simultaneously find the masses of every all of these 30 bodies we gave it,
and it also found the law.
So we kind of train this neural network to do this,
and then we interpret that neural network, and it gives us Newton's law of gravity.
Now that's a rediscovery, and of course, like we know that.
So I think the discoveries are also cool.
So these are not my papers.
These are other people's papers.
I thought they were really exciting.
So this is one recent one by Ben Davis and Jihao Jin,
where they discover this new black hole mass scaling relationship.
So it relates the, I think it's the spirality or something in a galaxy in the velocity with
the mass of a black hole.
So they found this with this technique, which is exciting.
And I saw this other cool one recently.
They found this cloud cover model with this technique using Pycer.
So it kind of gets you this point where it's a fairly simple model,
and it's also pretty accurate.
But again, the point of this is to find a model that you can understand.
It's not this black box neural network with billions of parameters.
It's a simple model that you can have a handle on.
Okay, so that's part one.
Now part two, I want to talk about polymathic AI.
So this is kind of like the complete opposite end.
We're going to go from small models in the first part.
Now we're going to do the biggest possible models.
And I'm going to also talk about the meaning of simplicity,
what it actually means.
So the past few years, you may have noticed,
there's been this shift in industrial machine learning to favor foundation models.
So like chat GPT is an example of this.
A foundation model is a machine learning model that serves as the foundation for other models.
These models are trained by basically taking massive amounts of general,
diverse data and training this flexible model on that data,
and then fine tuning them to some specific task.
So you could think of it as maybe teaching this machine learning model English and French
before teaching it to do translation between the two.
So it often gives you better performance on downstream tax.
I mean you can also see that, I mean chat GPT is,
I've heard that it's trained on GitHub and that kind of teaches it to reason a bit better.
And so the, I mean basically these models are trained on massive amounts of data
and they form this idea called a foundation model.
So the general idea is you collect your massive amounts of data,
you have this very flexible model and then you train it on,
you might train it to do self-supervised learning which is kind of like you mask parts of the data
and then the model tries to fill it back in.
That's a common way you train that.
So like for example GPT style models, those are basically trained on the entire internet
and they're trained to predict the next word.
That's their only task, you get an input sequence of words,
you predict the next one and you just repeat that for massive amounts of text.
And then just by doing that they get really good at general language understanding,
then they are fine tuned to be a chat bot essentially.
So they're given a little bit of extra data on this is how you talk to someone and be friendly
and so on and that's much better than just training a model just to do that.
So it's this idea of pre-training models.
So I mean once you have this model, I think like kind of the cool part about these models
is they're really trained in a way that gives them general priors for data.
So if I have like some, maybe I have like some artwork generation model,
it's trained on different images and it kind of generates different art.
I can fine tune this model on like Studio Ghibli artwork and it doesn't need much training data
because it already knows what a face looks like.
Like it's already seen tons of different faces.
So just by fine tuning it on some small number of examples,
it can kind of pick up this task much quicker.
That's essentially the idea.
Now this is, I mean the same thing is true in language, right?
Like if I train a model on, if I train a model just to do language translation,
right? Like I just teach you that.
It's kind of, I start from scratch and I just train it English to French.
It's going to struggle.
Whereas if I teach it English and French, kind of I teach it about the languages first
and then I specialize it on translation, it's going to do much better.
So this brings us to science.
So in science, we also have this.
We also have this idea where there are shared concepts, right?
Like different languages have shared, there's a shared concept of grammar in different languages.
In science, we also have shared concepts.
You could kind of draw a big circle around many areas of science and causality is a shared concept.
If you zoom in to say dynamical systems, you could think about like multi-scale dynamics is shared
in many different disciplines.
Chaos is another shared concept.
So maybe if we train a general model over many, many different data sets,
the same way chat, GPT is trained on many, many different languages and text databases.
Maybe you'll pick up general concepts and then when we finally make it specialize
to our particular problem, maybe it'll do it, it'll find it easier to learn.
So that's essentially the idea.
So you can really actually see this for a particular system.
So one example is the reaction diffusion equation.
This is a type of PDE and the shallow water equations, another type of PDE.
Different fields, different PDEs, but both have waves.
So they both have wave-like behavior.
So I mean maybe if we train this massive flexible model on both of these systems,
it's going to kind of learn a general prior for what a wave looks like.
And then if I have like some, you know, some small data set I only have a couple examples of,
maybe it'll immediately identify, oh, that's a wave, I know how to do that.
It's almost like, I mean, I kind of feel like in science today, what we often do is,
I mean, we train machine learning models from scratch.
It's almost like we're taking toddlers and we're teaching them to do pattern matching
on like really advanced problems.
Like we have a toddler and we're showing them this is a, you know, this is a spiral galaxy,
this is an elliptical galaxy, and it kind of has to just do pattern matching.
Whereas maybe a foundation model that's trained on broad classes of problems,
it's kind of like a general science graduate, maybe.
So it has a prior for how the world works.
It has seen many different phenomena before.
And so when you finally give it that data set to kind of pick up,
it's already seen a lot of that phenomena.
That's really the pitch of this.
That's why we think this will work well.
Okay.
So we created this collaboration last year.
So this started at Flatiron Institute led by Shirley Ho to build this thing,
a foundation model for science.
So this is across disciplines.
So we want to build these models to incorporate data across many different disciplines,
across institutions.
And so we're currently working on kind of scaling up these models right now.
The final, I think the final goal of this collaboration is that we would release these
open source foundation models so that people could download them and fine tune them to different
tasks.
So it's really kind of like a different paradigm of doing machine learning,
right?
Like rather than the current paradigm where we take a model,
randomly initialize it, it's kind of like a toddler.
Doesn't know how the world works.
And we train that this paradigm is we have this generalist science model.
And you start from that.
It's kind of a better initialization of a model.
That's the pitch of a polymathic.
Okay, so we have results.
So this year we're kind of scaling up.
But last year we had a couple of papers.
So this is one led by Mike McCabe called Multiple Physics Pre-Training.
This paper looked at what if we have this general PDE simulator,
this model that learns to essentially run fluid dynamic simulations.
And we train it on many different PDEs.
Will it do better on new PDEs or will it do worse?
So what we found is that a single model is not only able to match single models
trained on specific tasks, it can actually outperform them in many cases.
So it does seem like if you take a more flexible model,
you train it on more diverse data, it will do better in a lot of cases.
I mean, it's not unexpected because we do see this with language and vision.
But I think it's still really cool to see this.
So I'll skip through some of these.
So this is like, this is the ground truth data and this is the reconstruction.
Essentially what it's doing is it's predicting the next step.
All right, it's predicting the next velocity, the next density and pressure and so on.
And you're taking that prediction and running it back through the model
and you get this rule out simulation.
So this is a task people work on in machine learning.
I'm going to skip through these.
And essentially what we found is that most of the time, by using this multiple physics
pre-training, so by training on many different PDEs, you do get better performance.
So the ones at the right side are the multiple physics pre-trained models.
Those seem to do better in many cases.
And it's really because, I mean, I think because they've seen so many different PDEs,
it's like they have a better prior for physics.
I'll skip this as well.
So, okay, this is a funny thing that we observed is that, so during talks like this,
one thing that we get asked is, how similar do the PDEs need to be?
Like, do the PDEs need to be like navier stokes but a different parameterization?
Or can they be like completely different physical systems?
So what we found is really hilarious is that, okay, so the bottom line here,
this is the error of the model over a different number of training examples.
So this model was trained on a bunch of different PDEs, and then it was introduced to this new
PDE problem, and it's given that amount of data, okay?
So that does the best.
This model, it's already, it already knows some physics.
That one does the best.
The one at the top is the worst.
This is the model that's trained from scratch.
It's never seen anything, this is like your toddler, right?
Like, it's never, it doesn't know how the physical world works.
It was just randomly initialized, and it has to learn physics.
Okay, the middle models, those are pre-trained on general video data,
a lot of which is cat videos.
So even pre-training this model on cat videos actually helps you do much better than this
very sophisticated transformer architecture that just has never seen any data.
And it's really because, I mean, we think it's because of shared concepts of
spatiotemporal continuity, right?
Like videos of cats, there's a, you know, there's a spatiotemporal continuity.
Like the cat does not teleport across the video unless it's a very fast cat.
There's related concepts, right?
I mean, that's what we think.
But it's really interesting that pre-training on completely unrelated systems still seems to help.
And so the takeaway from this is that you should always pre-train your model.
Even if the physical system is not that related, you still see benefit of it.
Now, obviously, if you pre-train on related data, that helps you more, but anything is basically better than nothing.
You could basically think of this as the default initialization for neural networks is garbage,
right?
Like just randomly initializing a neural network, that's a bad starting point.
It's a bad prior for physics.
You should always pre-train your model.
That's the takeaway of this.
Okay, so I want to finish up here with kind of rhetorical questions.
So I started the talk about interpretability and kind of like, how do we extract insights from our model?
Now we've kind of gone into this regime of these very large, very flexible foundation models that seem to learn general principles.
So, okay, my question for you, you don't have to answer, but just think it over, is do you think one plus one is simple?
It's not a trick question.
Do you think one plus one is simple?
So I think most people would say yes, one plus one is simple.
And if you break that down into y, it's simple.
You say, okay, so x plus y is simple.
You say, okay, so x plus y is simple for like x and y integers.
That's a simple relationship.
Okay, y, y is x plus y simple.
And in you break that down, it's because plus is simple.
Like plus is a simple operator.
Okay, y, y is plus simple.
It's a very abstract concept.
Okay, it's, it's, we, we don't necessarily have plus kind of built into our brains.
It's, it's kind of, I mean, it's, it's really,
so I'm going to show this.
This might be controversial, but I think that simplicity is based on familiarity.
We are used to plus as a concept.
We are used to adding numbers as a concept.
Therefore, we call it simple.
You can go back another step further.
The reason we're familiar with addition is because it's useful.
Adding numbers is useful for describing the world.
I count things, right?
That's useful to live in our universe.
It's useful to count things, to measure things.
Addition is useful.
And it's, it's, it's really one of the most useful things.
So that is why we are familiar with it.
And I would argue that's why we think it's simple.
But the, the simplicity we have often argued is if it's simple, it's more likely to be useful.
I think that is actually not a statement about simplicity.
It's actually a statement that if something is useful for problems like A, B, and C,
then it seems it will also be useful for another problem.
The world is compositional.
If I have a model that works for this set of problems,
it's probably also going to work for this one.
So that's, that's the argument I would like to make.
So when we interpret these models, I think it's important to kind of keep this in mind.
And, and, and really kind of probe what is simple, what is interpretable.
So I think this is really exciting for a polymathic EI because these models that are
trained on many, many systems, they will find broadly useful algorithms, right?
They'll have these neurons that share calculations across many different disciplines.
So you could argue that that is the utility.
And I mean like maybe we'll discover new kinds of operators and be familiar with those.
And we'll start calling those simple.
So it's not necessarily that all of the things we discover in machine learning will be simple.
It's kind of that by definition the polymathic EI models will be broadly useful.
And if we know they're broadly useful, we might, we might get familiar with those.
And that might kind of drive the simplicity of them.
So that's my note in simplicity.
And so the takeaways here are that I think interpreting a neural network
trained on some data sets offers new ways of discovering scientific insights from that data.
And I think foundation models like polymathic EI, I think that is a very exciting way of discovering
new broadly applicable scientific models.
So I'm really excited about this direction and thank you for listening to me today.
Thank you for listening to us.
It was great.
So three short questions.
One, what was the cost of running this polymathic EI?
How did it cost?
Yeah.
Two, when it was fully built out, it was fully built out.
Yeah.
Please use your C mic.
Yeah.
Okay, so I'll try to compartmentalize those.
Okay, so the first question was the scale of training.
This is really an open research question.
We don't have the scaling law for science here.
We don't have the scale for language.
We know that if you have this many GPUs you have this size data set.
This is going to be your performance.
We don't have that yet for science because nobody's built this scale of model.
So that's something we're looking at right now is what is the trade off of scale.
And if I want to train this model on many, many GPUs, is it worth it?
So that's an open research question.
I think it'll be large.
Probably order hundreds of GPUs trained for maybe a couple months.
So it's going to be a very large model.
That's kind of assuming the scale of language models.
Now, the model is going to be free, definitely.
We're all very pro open source.
And I think that's really like the point is we want to open source this model
so people can download it and use it in science.
I think that's really the most exciting part about this.
And then I guess the third question you had was about the future
and how it changes how we teach.
Are you asking about teaching science or teaching machine learning?
I see.
I mean, yeah, I mean, I don't know.
It depends if it works.
I think if it works, it might very well change how science is taught.
Yeah, I mean, so I don't know the impact of language models on computational linguistics.
I'm assuming they've had a big impact.
I don't know if that's affected the teaching of it yet.
But if scientific foundation models had a similar impact, I'm sure it would impact.
I don't know how much.
Probably depends on the success of the models.
I have a question about your foundation models also.
So in different branches of science, the data sets are pretty different.
In molecular biology or genetics, the data sets is a sequence of DNA
versus astrophysics where it's images of stars.
So how do you plan to use the same model for different form of data sets, input data sets?
So you mean how to pose the objective?
Yes.
So I think the most general objective is self-supervised learning
where you basically mask parts of the data and you predict the missing part.
If you can optimize that problem, then you can solve tons of different ones.
You can do regression, predict parameters or go the other way and predict rollouts of the model.
It's a really general problem to mask data and then fill it back in.
That kind of is a superset of many different prediction problems.
And I think that's why language models are so broadly useful
even though they're trained just on next-word prediction or like BERT is masked model.
Thanks.
Can you hear me?
All right.
So that was a great talk.
I'm Victor.
So I'm actually a little bit worried and this is a little bit of a question.
Whenever you have models like this, you say that you train these on many examples, right?
So imagine you have already embedded the laws of physics here somehow, like let's say the law of repetition.
But when you think about this carbon new physics, we always have this question of whether we are actually reinventing the wheel
or like the network is kind of really giving us something new or is it something giving us,
or is it giving us something that it learned but it's kind of wrong.
So sometimes we have the answer to know which one is which.
But if you don't have that, let's say, for instance, you're trying to discover what dark matter is,
which is something I'm working on, how would you know that your network is actually giving you something new
and not just trying to set this into one of the many parameters that it has?
So if you want to test the model by letting it rediscover something, then I don't think you should use this.
I think you should use the scratch model, like from scratch and train it.
Because if you use a pre-train model, it's probably already seen that physics, so it's biased towards it in some ways.
So if you're rediscovering something, I don't think you should use this.
If you're discovering something new, I do think this is more useful.
So I think a misconception of machine learning in general is that scientists view machine learning
for uninitialized models, like randomly initialized weights, as a neutral prior.
But it's not. It's a very explicit prior and it happens to be a bad prior.
So if you train from a randomly initialized model, it's kind of always going to be a worse prior
than training from a pre-train model, which has seen many different types of physics.
I think we can kind of make that statement.
So if you're trying to discover new physics, I mean like if you train it on some data set,
I guess you can always verify that the predictions are accurate.
So that would be, I guess, one way to verify it.
But I do think the fine-tuning here, so taking this model and training it on the task, I think that's very important.
I think in language models, it's not as emphasized.
People will just take a language model and tweak the prompt to get a better result.
I think for science, I think the prompt is, I mean, I think like the equivalent of the prompt would be important,
but I think the fine-tuning is much more important because our data sets are so much different across science.
Question?
In the back, please.
He notes that the symbolic
So are you using also the fine-tuning and transfer learning as a way of enhancing
What does that have?
Yeah, so the symbolic regression, I mean, I would consider that it's not used inside the foundation model part.
I think it's interesting to interpret the foundation model and see if there's kind of more general physical frameworks that it comes up with.
I think, yeah, symbolic regression is very limited in that it's bad at high-dimensional problems.
I think that might be because of the choice of operators.
Like, I think if you can consider maybe high-dimensional operators, you might be a bit better off.
I mean, symbolic regression, it's an active area of research, and I think the hardest, the biggest hurdle right now is it's not good at finding very complex symbolic models.
So I guess you could, it depends on the dimensionality of the data.
I guess if it's very high-dimensional data, you're always kind of, like, symbolic regression is not good at high-dimensional data.
Unless you can have kind of some operators that aggregate to lower-dimensional spaces.
Yeah, I don't know if I'm answering your question.
So, like, when you're showing deconstruction of these trees, these generations of the operators, I think this is related to kind of general themes at the top and other questions.
But often in doing science, when you're working at your presented, we kind of look at the algorithm itself, like, you know, diagonalize the Hamiltonian or something like that.
What, how do you calculate that aspect of doing science that is kind of the algorithmic side of solving complicated problems?
Yeah, so the question was about how do you incorporate kind of more general, not analytic operators, but kind of more general algorithms, like a Hamiltonian operator.
I think that, I mean, like, in principle, symbolic regression is, it's part of a larger family of an algorithm called program synthesis, where the objective is to find a program, you know, like, code that describes a given data set, for example.
So, if you can write your operators into your symbolic regression approach, and your symbolic regression approach has that ground truth model in there somewhere, then I think it's totally possible.
I think, like, it's harder to do.
I think, like, even symbolic regression with scalars is, it's fairly difficult to actually set up an algorithm.
I think, I don't know, I think it's really like an engineering problem, but the conceptual part is totally, like, there for this, yeah.
Thanks. Oh, sorry.
This claim that random initial weights are always bad or pre-training is always good.
I mean, I don't know if they're always bad, but it seems like from our experiments, it's, we've never seen a case where pre-training on some kind of physical data hurts.
Like, the cap video is an example. We thought that would hurt the model. It didn't.
That is a cute example.
I'm sure there's cases where some pre-training hurts.
Yeah, so that's essentially my question. So we're aware of, like, adversarial examples. For example, you train on MNIST, add a bit of noise, it does terrible compared to what a human would do.
What do you think adversarial examples look like in science?
Yeah, I mean, I don't know what those are, but I'm sure they exist somewhere where pre-training on certain data types kind of messes with training a bit.
We don't know those yet, but, yeah, it'll be interesting.
Do you think it's a pitfall, though, of, like, the approach? Because, like, I have a model of the sun and a model of DNA.
Yeah, I mean, I don't know. Like, I guess we'll see.
Yeah, it's hard to know. Like, I guess from language, we've seen you can pre-train, like, a language model on video data, and it helps the language, which is really weird.
But it does seem like if there's any kind of concepts, it does, if it's flexible enough, it can kind of transfer those in some ways.
So we'll see. I mean, presumably we'll find some adversarial examples there.
So far, we haven't. We thought the cat was one, but it wasn't. It helped.

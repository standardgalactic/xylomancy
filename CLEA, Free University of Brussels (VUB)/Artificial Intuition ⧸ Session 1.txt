So the big question about artificial intuition is first, what is intuition? And when people
speak about intuition, they speak about the form of knowledge or having ideas where you
don't really know how we think that we come to a particular point of view. So the best way to
define intuition is in a sense to contrast it with its opposite. The opposite you might call
is national thinking. You have the famous distinction made by Cameron Monk who developed
ice in economics who speaks about system one thinking, system two thinking. The system two
thinking is what we usually call national thinking, which means you expressively form certain
arguments in your mind and you follow the tape of thought step by step. That means you use symbols
which typically are words which represent certain entities or certain concepts or techniques.
You have certain rules to which you combine these symbols into a meaning of something. This
is what we wanted to show, propositions. Propositions are usually explicitly described,
some kind of situation, and then you follow certain rules of logic or grammar to derive
things from other things. There is this classic example. Circle is human, all humans are mortal,
therefore, circus is mortal. So you have the symbol. You have the symbol human and the symbol
mortal. You know that there are certain rules of grammar which you can make sentences like
circus is human, humans are mortal, you have certain rules of logic and system. If all humans
are mortal and circus is human, then circus must be human. So for a very long time, cognitive
science and artificial intelligence was starting from this symbolic paradigm, a symbolic paradigm
of national thinking explicitly, step by step. And that was, let's say, until about the 1980s,
there was the idea we can represent all knowledge by symbolic expressions and the heights of that
let's say that part of the vision intelligence was in the 1980s, the so-called knowledge-based
systems or expert systems. They were trying to quote every knowledge we have about the world
in terms of these kind of propositions. So tax kind of propositions that we would want to
have is for example, a dog is a mammal, a mammal is an animal, a mammal is born, blooded, a dog
a dog walks in the states and you would try to understand everything that is to know about
the particular domain in expressing propositions consisting of certain symbols that follow certain
rules. Why am I seeing something that is not me? I see everybody's watching a screen where there
isn't anything that I'm hoping about. You hear it from the whiteboard, yeah. So now it's coming.
So this idea wasn't all the vision intelligence, it necessarily was.
So
yep it's muted. So we need to, we need to, can you say to just in case, can it be a little bit
louder or the microphone? Francis, if you can try to speak more in front, when you turn the
head then it's missing what you say. Okay, that's why I didn't speak in there. I'm sorry, I know it's
very difficult, it happens to me also if you last week. Bruce, keep going. I can see the sound
here, so it's okay. So okay, so we are in this paradigm which is called the cognitive science,
the symbolic paradigm. Knowledge consists of symbols, symbols are combined according to
certain rules and propositions. Propositions describe your knowledge, describe what is to be
known about the real world and then intelligent reasoning is making influences that means starting
from some proposition that you know to be true, you derive other propositions. For example, knowing
that dogs are mammals, that animals are animals you can derive, that the dog is an animal. Knowing
that mammals are one body you can derive, that the dog is one body. So that was the paradigm in the
1980s of the so-called knowledge-based systems and because it's not obvious to put in all the
knowledge we have about the world, it's part of the so-called expert systems. The expert system
was meant to represent expertise that means specialized knowledge about our particular domain
and one of the typical examples they were considering was diagnosis of diseases. If you are a doctor,
you know that these diseases can have these and these symptoms and that if you have a high temperature
and you have a cough and you have this then you may have an infection of the lungs and so on.
So one thing they immediately needed to do was not only logical derivations where A necessarily
follows from B but also something probabilistic derivations. If you have these symptoms then
you may have this disease with this probability and this other disease with this probability.
So that was the paradigm in the 1980s and the idea was here for the whole artificial intelligence.
The only thing we need to do is put in more and more propositions, more and more rules
and eventually we will have the knowledge of the whole humanity in our artificial intelligence system.
And there is an interesting project that started doing it and the project is still learning. It's
psyched, made by the artificial intelligence pioneer Douglas Lennard and the idea was
let's capture all the knowledge about everyday things, the kind of knowledge that an average person
has into these propositions. And that could be very simple propositions like when it rains,
the street gets wet, when it rains people take an umbrella. An umbrella protects you against the rain.
If somebody is sad, they will not want to go to a party. And I mean you can think of
thousands, millions in India, billions of these kind of propositions and of course this psych project
put millions of these things into the system and in the end nobody has really heard I suppose
about the psych project. They have put thousands of many years into collecting all this knowledge
and practically not used. So nowadays we have artificial intelligence systems that are much
better than that but they work according to a completely different principle. So in artificial
intelligence you have the symbolic paradigm, the paradigm of exclusive logical thinking
and then you have the connectionist or neural network paradigm and that is the paradigm that
is more trying to imitate what happened in the rain. And that is precisely where what we might
call intuition is happening. Because what is intuition? Intuition is you certainly have a
certain insight or a certain feeling that something is going to happen. And then you can't read the
steps, you can't say what are the assumptions I have made and what are the influences I have made
to come to this conclusion. The conclusion is there, it's kind of obvious. You see somebody
if they are that person, it's sad maybe they just had some bad news and you go and you comfort that
person. Why did you think that that person was sad? It's not obvious, maybe it's the way the person
was moving kind of slowly and looking kind of like this and not reacting when you ask something.
You can recognize that that person is sad. Intuitively it's clear that that person is sad.
There wouldn't be any kind of an expert system that you could put in rules if the person's
eyes are with this angle down and if the person's movements are slower than this and if this and
this, then the person is sad. Obviously that doesn't work. So the symbolic paradigm for
artificial intelligence, I wouldn't say that it's outdated, it's still useful for certain
people that can be formalized, but the biggest advantages of artificial intelligence like now
are about this neural network paradigm. So neural networks gets very quickly very complicated
mathematically. So I'm not going to try to explain that here also because I don't know the details,
but I want to explain two basic principles which I think you should know. The first principle
is the so-called heavy rule which says that if something occurs together with or shortly after
something else, the brain tends to make an association between them and the more often
the one thing happens and then the other things happen, the stronger the association becomes.
So for example, if you're a little child, you don't know anything about the world,
you see a dog, let's call it A and then you hear a bark, you see the dog barking,
you make a connection from A to D. Now if it just once, one dog once barked,
there will already be a connection that it will be pretty weak, but if now that kid several times
you see the dog and then the dog at some moment starts to bark, his connection will become stronger
and stronger. So what heavier learning does is it strengthens connection between things that tend to
co-occur, tend to occur together. So that is, you might say, the most primitive level of learning
in the brain, connecting things that tend to occur together. It's called associations,
that is associative learning, you're making associations, but the associations are not
logical difference. It's not because something is a dog that it will bark,
it's not because somebody is like that, that they are certain, but if they are like that,
there is some association that tells you all they might be certain. So an association is something
that can have a strength or weight, and it's called in neural networks, a weight that can be
between let's say zero, meaning there is no association and one, it's always a case that,
for example, logical rules can be expressed as things with an association strength one.
If it's a cat, then it is a mammal. If it's a cat, then it is a pet. Well, not all cats
are wild, but it's a pretty strong association. If it's a cat, then it's cute. Well, maybe for some
people it's cute, but some people are afraid of cats, so that's a weaker association. If it's a cat,
then it's black. Well, there are quite a lot of black cats, but you're more likely to see a black
cat than to see a green cat or a red cat. So associations can have all these degrees of strength,
but that's the one rule. While we are experiencing the world, we are constantly learning associations.
We are constantly experiencing different things together, and either they are together often or
they are together rarely or together from time to time. The brain is very good at capturing all
these kinds of associations. So the association idea already gives you an idea where intuitions come
from. If there is an association from A to B, well, then B may pop up when you see A without
you necessarily knowing that there is a good A to B. And it's especially noticeable, of course,
if there can be lots of in-between stuff. So maybe your brain is going from A to B to C
without you being really aware of it, not only RD, but the brain may have done a lot of intermediate
stuff that you're not aware of. And then there's a second principle. It's called spelling activation.
The idea that you see A, that means A is activated in your brain. You see a dog in your brain is the
notion that there is a dog. You have a number of associations with dogs. You know the dog's
bark, you know the dogs that fall or legs. You know the dogs tend to wag their tail, even though at
that moment the dog is not wagging its tail. You have some kind of an expectation that at some
stage a dog may be wagging its tail. And in a sense, that's already an intuition. You might say,
I see a dog. Intuitively, I kind of expect that maybe when the dog comes up to me,
that it may wag its tail. And then you could say, oh yeah, it's wagging its tail.
But now spelling activation means that you activate one thing and you don't go from A to B to C to G.
So the activation spats to lots of things, but it spats not only output, it only also
converges, that's not only why the first thing written like this, and maybe X and Y together
produce W and W and let's say Q together produce something else.
The way this is modeled in neural networks is with activation that it's divided, so it's called
the tail. That's a lot of activation. There's a strong link to B, so a big part of activation
goes to B, smaller amount goes to X, a smaller amount goes to Y, but then X and Y together
leads to W, which means that W will again get more activation, because it gets activation both from
X and Y. So activation can both spread in a sense that it gets diffused and it can converge in a
sense that from different directions, from different things that are activated in your brain,
something gets a lot of activation. And now we get this phenomenon that you may certainly get this
inside and you say, oh yes, that's it. Where did the inside go? Probably from lots of different
things that were activated a little bit and that somehow converged and together they came to
something that is strong enough that it pops into your consciousness and you say, oh yes,
that's it. So what intuition does is it allows you to look at all these firstly uncertain things
that may or may not happen that are more or less probable and it can diffuse so that you don't know
very much anymore what's going to happen and it can converge so that suddenly out of this diffused
you suddenly come to an idea, say, I am that stick, I have understood. Most of this burning
activation is subconscious. That means we are not aware of it, we don't remember it, we can't say
what are the intermediate stocks. So we may, for example, at the certain moment start from the
8th, the number of intermediate stocks, the number of intermediate stocks,
converting it all, all the story of the big punches, intermediate stocks, more sub-functionals,
you don't know them. So your intuition says, ah, this is going to happen. This is what your
origin is starting out with, all the intermediate stocks you don't know. So that is why intuition
typically is considered as something mysterious, something that for a long time people have thought
machines will never have this intuition. It's not possible to follow the efficient designs
of intuition. That was because we saw that this idea of machines come to us following logical
reasoning, stuff by explicit thoughts. That's not how the brain functions and it's also not how
modern artificial intelligence systems function. The modern present-day systems, they use this
kind of networks. Not exactly the way I have sketched here, it's very complicated and as I said,
I'm not going to go into any detail, but the principle is the same. You activate a number of
artificial neurons that start for different features of the situation. For example, you have a text
reading system. It reads a word, it reads another word, it reads another word. It activates each
of these words and then it activates a particular combination of words and out of the combination
of words, it anticipates that another combination of words is likely to follow and that's a little
bit the principle of these large language models like chatGPT. What they do is you give them an input
of a number of words. They have looked at millions and billions of sentences, so they have some kind
of, you might say, intuitive idea of which words will follow and the way they are worked is that
they are innocence, continuing the conversation. If the conversation starts with this sentence,
they have some expectation that the following words and sentences would follow and that's
what they are doing. They are, in a sense, following their intuition of what would be the next
sentence to say, but following their intuition means you don't know exactly how they come to
help computing. You also don't know, but in a convergent style, it's not a logical process,
it doesn't use expressiveness. So some of the medium associations may actually be
inadequate and that's one of the problems they have with models like chatGPT,
that sometimes they're very explicitly telling things that are wrong.
That's because they have to learn these intuitive ways, the way people also, you know,
and then people that try this but that's wrong or that they might say something wrong,
that people, of course, also tell you sometimes, oh, it's a lot of uprooted. I know it,
that is the case, that's where it could happen. Yeah, in a sense, these new language models are
more like, in a sense, that they work more in this intuitive way, is flat activation,
firstly, over many, many different things and some of the activation from words is to produce
certain sentences and certain words and then, often, it sounds good, but it may be wrong.
So that is a bit of the idea of artificial intelligence, you're using the system of
associations and interconnections and studying activation, where nobody really understands
what's going on, even the system itself, you cannot really understand all the steps,
there are no explicit reasoning steps and therefore you can solve a lot of problems that you
wouldn't be able to solve, but sometimes you can, of course, also make mistakes. Yeah, so
Well, I haven't, I haven't mentioned a second learning mechanism I spoke about,
happy and learning, which means when two things tend to occur together, you stay in the connection,
there is also a different form of learning, reinforcement learning, which says the following
number of steps, you form a certain result, if that result is what you wanted, you reinforce it,
that means you reward whatever process led to this result, if that's not what you want,
if you make a mistake, then you, you might say you publish or you give it to the person that
meets all the different steps in the process that led to the wrong result. So it's a way to
learn to avoid mistakes, but first the system should produce an output and then somebody
should be able to say this is right or this is wrong. Yeah, but there needs to be some
explicit decision that it is the right result or that it is the wrong result. So you can
change things like that, but it's much more complicated than the happy and learning, in the
happy and learning, you just see most of the amounts of text, it learns that certain words
and certain phrases and certain ideas work together, so that when I recognize some of these words,
or ideas, it tends to follow them up with the ones that most commonly follow them,
but here there is no explicit function that tells you it's right or it's wrong. You can do that in
the latest stage of the enforcement learning by either punishing or rewarding the system for
wrong answers or had answers, but that is indeed that takes much more time.
Okay, I just briefly say, in principle though, it's the same mechanism, right? If you think of it
as an extended mind, it's still a coincidence, like a literal coincidence between the event
and the reinforcement, which then strengthens the network. Well, it's the same mechanism in the sense
that certain collections relative to others, some weak in others, that is always the mechanism.
So it's not like you say that, suppose that you have an expert system with logical propositions
and you see that it comes to a certain wrong conclusion, you can try to find out where was
the stuff where it made the wrong conclusion and you see that particular proposition in there is wrong
and then you say, okay, I'll take that proposition out and replace it by a correct one. Here, we can
do that. Here, the whole system development work has come to a conclusion with methods like back
propagation, which is typical for deep learning and I must say I don't know the details about it.
You try from the end result to reason backwards to all the different steps that I've
left in some days of course, and then I've written them and if the end result was bad,
or you state the number then the end result was good, but these I think that mathematically
are very complicated, so I don't want to... We just need to think, Francis, we will have a
discussion after Cristian's call.
So this was Cristian Gendryko's presentation. Sorry, Francis' presentation. Now we got Cristian
Gendryko, who I will introduce you if you want to say something else.
So Cristian is a professor of communication design at the University of Düsseldorf,
so he comes from Germany and he's a theologist and a communication design professor,
a very passionate researcher and creative musician as well, so you will tell us about your
story, but at least I guess you can introduce yourself as well. So thanks Cristian for coming.
So Thomas, you downloaded the slides on the desktop? The slides are here somewhere.
The newest file.
You can go to downloads maybe on the website.
No, no, desktop. That's everything, that one. Cristian Gendryko.
And then just the function, and then we can go zoom back again. It's a little bit,
there's something strange going on, so it's like doing this double desktop thing,
right? So it slides across, but now what I have to do is go to the share screen, which is...
The ring button? Yeah. Ah, here? Yeah.
But I press that? Yeah. Ah, okay. Then we have to choose the right one.
Yeah. Okay, and none of them are gone, so. It must be activated.
Okay.
So you want to, do you want, do you know how to do it? Maybe you first. I have the file,
the file in my phone too, so maybe I can share it.
So...
Hi, here we go. Yep, pieces to settings.
So these two have access to screen economy. Just put your name.
Oh, are you host? Yeah. Okay, then we need to be careful.
I put you host. Thomas, I put you host.
But I can try to share this last one here too.
That should work.
Yeah, like they meant, that's cool.
That's cool.
You left the Zoom? Is there anybody that I can trust that will give us past the host permission?
Yeah, everybody does as well. Who wants to be a host for two minutes on the Zoom, please?
I am connected, no? No, you're not. You're out somehow. You left the call.
Yeah, I am. Yeah, I could also do. Yes, please, Catherine.
Somebody on the Zoom? Is somebody active to take the host and give it back?
I might be muted. No, I'm not muted.
Catherine, are you connected?
Write something on the chat.
Thanks, Santiago.
Thanks.
you
got it. And now you should be able to share. Yeah, okay. Now try to show the screen again.
Santiago just put us back on the host, please. So you right click on the name and
click on assign hosts, I think.
Okay, great. Now share screen. Okay, we're back. And now, that was like another artificial, very
intelligent. Yeah, thanks to the hacker. Yeah, isn't that one? Oh yeah, cool, right.
Yeah, share. Okay, and then we can go from here to full screen, right?
Okay.
So in the meantime, while Christian comes to the room, I would like to remind that we have a
mission today, which is because some people were coming, which is collecting the right questions
that we have to ask around this topic. So please remember, this is a question, I think.
Okay, Christian. Yeah, I think you, maybe just like this.
Okay, now this.
Ah, you just do a little swipe. Ah, okay. Ah, great. Yeah, great. Is that good? Yeah, yeah, that's
okay. Perfect. Yes.
Thank you for having me here in Brussels. One more time, I will talk about, yeah,
title is Towards the Modeling of the Form Findings Process. As Thomas said, I'm a professor
at the Faculty of Design at the University of Appliance Sciences in Düsseldorf,
and I'm responsible there together with my colleague Stefan Asmus for the module of systems.
And my main field of research is the modeling of the form findings process in an aesthetic
creative process. And Thomas mentioned it already. I belong to the movement of free
improvisation. That's where basically I'm coming from. And doing this free improvisation thing,
you really start to try to understand how does it work if you play together with other people?
How does it really work that you create together something that you didn't know before will happen?
And out of this, out of this question, so we started out when we were 16. And out of this,
out of this thing, these questions of research did arise. And here you see us performing,
this is in the middle, you see me and Stefan Werney. And some of you know Stefan, me, we've been here
at the Great Systems at Clay Symposium in February performing here. And on the left,
you see Vivian Cunningham. And on the right, you see Maggie Nichols. And both belong to the
British free improvisation scene, which is an interesting thing in itself, an interesting
development. And we did perform last two weeks ago, we gave this concept. And it's one of these
situations, you know, where you meet together and then you start to perform with each other
without talking before you do something, without knowing what the other will do.
And, yeah, and to describe, and how can we describe this situation? And I brought this basic,
yeah, a diagram from Robert Rosen, which exactly I think describes on a basic level,
these situations. You have on the one hand, you have the natural system that you want to describe.
And on the other side, you have the system of description. And it's also goes already in the
direction of what Francis did say, that you have these feelings and inference rules. And then the
interesting part is that you have on the one hand, you try to decode the natural system you
observe and try to understand. And at the same time, then with this system you encode what you
see there, the natural system and its causalities and everything. And so I think this is an
interesting diagram to get an idea of what this process of formalization, what is happening there.
Although I must say, this is not on the on the noisest hand, I would say, as we say in
German, it's not actual, but this is a good introduction to what the problem is. If you
try to formalize a process that you observe. And of course, we can dig deeper in here into
questions of entanglement in how far these systems are entangled with each other and do
they produce each other and questions like that. But this is something else. Well,
so these are my basic research questions and via a friend and colleague, Markus Wetzler,
he was in contact with Katarina Petrovic and Katarina did invite me in 2019 to give a talk
about my research project right here. And I called it artificial intuition.
And this was a great session. It was Orion was was also here. Katarina was here and Francis was here
also. And we had a afterwards we had a deep discussion, a deep fruitful discussion for me
at the clear house. And I had another idea then how how to describe how to try to find the right
approach to describe it. And Francis then introduced me to the idea to use to try it with
a chemical organization for you. And so I got into contact. And Francis brought me into contact
with Thomas Willows. And I invited Thomas Willows to our university in early 2020, I think it was.
And so we really started to work with with students in different projects, really,
try to apply COT to these questions and experimenting with it. Because this is an
important thing for me to that it's about experimenting to try to to understand better the
process of the form fitness process, that you apply, also try to apply different paradigms.
It's like Francis pointed out these two paradigms in artificial intelligence that you have on the
one hand, you have the knowledge based symbolic paradigm. And on the other hand, you have the neural
network deep learning paradigm now and two different paradigms. And it's we have the chemical
organization theory. So this was just to show this, this this was a project that was developed by
Thomas Willows and Vincenzo de Florio. And we experimented a lot with this in different seminars
in different setups. And yes, and then I was invited this year in July to the ICLP conference
in London as the as a speaker. It's the International Conference of Logic Programming.
And I got there into a deep discussion. I was invited by Veronica Dahl. She's in the middle.
She's she's one of the experts of logic programming and logic grammars. So this is exactly system two
that Francis was talking about. And on the left, you see Robert Kobalski and he's one of the pioneers
of of developing developing the knowledge based logic symbolic paradigm, trying to implement it
in form of software, which was then which was happening in the mid 70s by the programming
language prologue. And I got into a deep conversation with Robert Kobalski on this
conference, because his his thing is backward reasoning and forward reasoning.
And trying to implement or they succeeded to implement backward reasoning as in form of
software in form of SLD resolution and backtracking. And we got into this deep conversation because
Robert Kobalski was saying that he tries to understand with. So we were talking about creative
process and how to formalize it and other possibilities. And he he was talking about
that he tries to find. So Gabriele is coming. Okay. And his question was and still is
in the creative process, how often does backward reasoning switch to forward reasoning and back
again. And I talked to him that I when I teach, when I teach my students, I explain backward
reasoning the following way. Backward reasoning is Michelangelo, when he says that if he sees the
the block of marble, he can see already the figure within the marble. And this is kind of
backward reasoning, because you you go from the result, you can see the result. And then all
you have to do is going backwards. This is what what Francis pointed out before, you can go back
step by step. And then you can know how you how you achieve this goal by backward reasoning.
And forward reasoning, I described to the students in with Jean Miro, who lived in in
Mallorca and went every morning down to the beach and looked what the what the ocean has
brought overnight and picked up the different bits and pieces at the at the beach. And so data
driven, you could say in kind of a way that he takes this and that and out of this out of this
forward reasoning process, he suddenly sees what the figure can become. So these two, and
and Robert Kowalski, he said, this is a good this is a these these two things seems to be good
descriptions of backward reasoning and forward reasoning in the creative process. And Thomas
and I, we had this idea that we want. So we work together from 2020 on ever since. And so I talked
to Thomas about this, and that I wanted to try out. What about if we if we try to, if we really
try to to formalize, could this be a way that we formalize? I know it sticks. Can't, can't move it.
Or do I have to go on this? Okay. Yeah, and Thomas came, Thomas came to Düsseldorf, and we
we had a intense working session for three days, thinking about backward reasoning and forward
reasoning. And is it possible that we can create a pattern out of it? Can we see the pattern? And
what are the perturbations maybe so we were trying to get together chemical organization theory and
this idea of backward reasoning and forward reasoning and see if patterns of the switching
of both do emerge. Three intense days. Yeah, you see the the the it gets fuller and fuller the
blackboard. And here you see a draft of our ideas and how it could work. I want to don't want to go
too much into detail. The interesting thing, which I want to put on the table here was our
conclusion of these three day sessions that we are that we are that we have a big question.
And that is that is really on the one hand, if you think about what what Francis pointed out with
with system two by by Kahneman, and the whole approach of trying to formalize such a process
as the for instance process that you have on the one hand, this paradigm of we we named it
or Thomas did name it clockwork engineering. So where you really have separate units where you
think in the system that you observe and try to describe as if there are separate units and each
unit has a certain function. And out of this, you get this functional sequential thing that is
working. And so this is the paradigm of we can say the technological society. And on the other
hand, and COT as a tool seems to point into this direction. That's why we couldn't marry these two
things really in our in our approach within these three days. On the other hand, you have this thing
that really trying to capture a form fitness process, it is really more about a more
organicistic approach. We were talking about it. And the question is, so we have these two
paradigms, you could say, of trying to to formalize things, you have the clockwork
engineer approach, and you have the more organicistic approach, which is about to be developed.
And I think this is this is what we are working on. If I am allowed to say I think it's or I
at least I it seems to me that this is really happening at clear with with with other projects
that are going on, that this points towards a more holistic, a more organicistic description
of such processes, but but we still don't know really how to. And this is so we are on our way.
And this is this is what I want to bring on the table for today. Yeah, thank you very much for
your attention. Thank you. Now we have an open discussion for nearly 40 minutes. So
there are some people here that maybe would like to intervene or say, you know, bring something in.
Maybe people in the audience online audience as well. So if anybody wants something to say
or who has a question. Okay, we can start. I will moderate. Okay, I will
So
And this was the conversation I had with Vokula, it's a really big switch, you know, and one
great switch happening during a creative process. This is the exciting question, if you really
stay in the description paradigm of system 2 handling, I'll give you one more, I'm going
to give you at least my point, because you will be speaking to you otherwise I will open
it.
Okay, but we're going to keep people on the line.
So now, just summarize the question again, you were speaking about backward reasoning.
Yes, I was just asking whether it was the summary to say that the fact what we don't need that kind of convergent thinking that you want to run down your, your goal, where you want to go.
Yeah, yeah, I think the way, yeah, because it's a combination of both, this is at the moment, at the moment, the way we are thinking about it.
Because you could look at backward reasoning. So you have a clear result in your mind what you would the goal you want to achieve and then you have to break down the different steps to get there.
But you know them.
And so you know, the result is there now I have to prove how to get there.
Why do you do it. Why do you do it you suddenly get distracted from this way of thinking by a perturbation. This was so Thomas and I were talking about perturbations and the sense of chemical organizations and so where where come where you come the perturbations in, which
is actually so suddenly I see, oh wait this, I'm doing something I'm playing something on the synthesizer and suddenly this reminds me of something.
This is exactly that thing and then I switch to forward, because then I say okay I have this thing. And now I combine it with what with that thing oh and this gives I see, because this now fits to the rule that I can turn this into this and
this is and then if you have a chief this, this local kind of thing, you close the you switch back to to backward reasoning, because, because you know, okay so this part is, this part is through, and now I can go back to my initial to my initial idea.
Maybe you continue to reach a certain kind of consistency that tells you and now it's time to explore again. Exactly. Yeah.
Or maybe the boards. Yeah, yeah.
And it was super interesting when we were performing with Vivian and Maggie because we had a long conversation before we started a long conversation afterwards.
And I so I observed myself and ourselves when we while we were performing and, and this was what became clear to me also was which is kind of different thing which goes more into parallel, parallel things that you have maybe running the
backward reasoning all the time, because you have certain certain results you want to achieve for sure. For example, don't be louder than the other ones. So you have you have this this running.
How do you do it.
Different steps because suddenly you got this, you got this association, and you know I have to be loud now.
But, but backward reasoning tells me no no I can't, I can't be, I can't be loud at this at this moment. So I have to combine these things but I still don't know how but this was, this was not the actual thing that was.
It depends. Sorry, it depends also on the drone like in jazz, you can give the volume to one instrumentist and then it switches like you have the idea. Yeah, and then the music.
We go down and listen to. Yeah, yeah.
Awesome. I was here I was as an example I was talking about the rolling stones, you know, as a, as a, as a unity where, where because it's, it's so you can have different kind of things but at the end it's, it's, it's, you can look at these things as if it is always improvisation it
doesn't matter if you have a theme that you are playing together with the band, or if you have no theme at all and the theme emerges.
So if you do free improvisation, the theme emerges, if you have a theme like, like let's say rolling stones are playing on stage, I can get no satisfaction then you have to think going on that a special version of this theme will emerge while they are performing it.
You have the same, you have, and I guess you have the same structures of processes going on, you know, and these kinds of things we are trying to get closer to because the thing is that I use formalization really to understand to get a better understanding of what are we doing when we do free improvisation.
This is the, this is the old task.
Okay, thanks.
Yeah, just some associations with that.
So, Eduardo Bono describes this process quite well in terms he coined this phrase, lateral thinking.
And he has this, he has this idea as well that like forward thinking is thinking that doesn't know where it's going. In fact, like it doesn't.
You haven't yet got the answer. So you're sort of, you're trying to, you're trying to associate.
And then once you get the, you go all over the place and then once you get the answer, then the logical way of getting there appears in retrospect.
And that's this kind of also this backward thinking. Like once you know somehow that you're trying to go there, then you can find a more direct way to connect those two ideas.
But like creative thinking, real creative thinking is this kind of process of making associations and trying things out, messing around, playing around this kind of intuitive processes of waiting for certain intuitions to build up enough in your mind that they become conscious.
So trying out, like letting playing around until, until the waiting, the waiting instance of WEI, GHT, waiting for the waiting to pop up into your consciousness.
So I think he kind of describes this process quite well, not just like forward thinking and backward thinking.
There's another idea about this like in film where they talk about small kind of oblique association.
But this idea of like watching a film is like, is like looking in the rearview mirror of a car as you're driving along.
It's like as you're watching the film, you have the impression that the film is going forwards.
But actually, you can never see forwards in the film. You can only see what's already happened in the film.
Alright, so you're always in some sense looking backwards towards the start of the film and traveling somehow with your back forwards, like in the film.
So it's something, there's something really interesting there, I think, with this idea of fordness and backwardness, which I think connects to one of your, though it's really intriguing to try to think what this fordness and backwardness is.
And then, but I think it's also not, it's also my question, is it one or the other?
I mean, you know what you mean, like when you have a, you sort of you have somehow a sense of where you're going, you have like a defined goal and then you have perturbations.
But it's also somehow this, like, I think it's this continued difference engine, right, like it's more that you have your current state and a future state, and then you're constantly comparing the two.
So it's your, your operate, it's this, it's this production of difference, in a sense.
This is the, this is the mechanism of backward reasoning, actually. So if you, that you compare and you see, so you prove all the time, is this step, is this true in accordance to the goal?
But ford's reasoning is also that, sorry, I'm going to, I don't want to make the conversation so, but we move on, but it's like ford's reasoning is also that thing that you're trying out in states, right?
So it's like, if I have a, if I'm making a collage, for example, and I have, you know, I have this object or this image and this image, and I put this image down, and I'm like, well, what happens if I put this image down?
Or what happens if I, you know, or this image? Or what happens if, indeed, if I think about it as a collage? But oh, maybe it's not a collage, maybe it's, maybe it's a construction.
So I'm sort of testing out end states in order to generate a useful difference in order to proceed.
Anyway, just something here, folks.
So, in both talks, you described the process of situations we have in our brain.
So, their dynamic that the question was, okay, for what our brain creates this disease?
One of my proposition was that when unknown situation occurred, our brain is trying to bring the system of uncertainty to some equilibrium.
This is how it creates this association.
And if this is true, then you said like we have weak association and strong association.
At some point, some of our human associations, basically everything about the world became logical loop.
And I wanted to ask what do you think about this dynamic?
I didn't quite understand with the uncertainty.
Yeah, so, for example, the new knowledge arise, for example, dog barks, right?
And our brain trying to procedure this information.
So, and it's kind of in that it's, say, burning our neurons, burning, running to each other.
So they're like, I want to take a state, right?
And then when we give association, so, ah, is it a dog?
That is one way to describe it.
I wouldn't describe it like that, but there are a number of theories about commission and commission is basically about reducing uncertainty or maximizing predictive capabilities.
In that sense, learning association is a question of learning to predict to that.
You might say that there is this new theory called predictive processing, which has as the fundamental principle that the brain tries to minimize survival and survival is some kind of a mathematical measure.
But the name says that you want to minimize the idea that you're being surprised.
I think they go a little bit too far because sometimes it's nice to be surprised.
Surprise party.
But there is something of that.
The more the brain can anticipate what will happen, the more you will be prepared to either even possible dangerous or to use opportunities being prepared.
What may be happening is indeed adaptive from another roofing point of view.
That's why the brain is trying to be ready for things which means that it learns what associates with what and something surprising is that this example of the barking is actually very good.
So suppose you have a little kid and it has been playing with cats and it has been playing with dogs.
It's only one of the dogs on board.
It's a very loud noise. It's surprising. It's frightening.
So the brain of the kid says, oh, there is something here that I wasn't prepared for.
But because the association that has now been created between the dog and the barking, the next time there is a dog and the dog barks, the child will be a little bit less surprised.
And if that happens three, four times, the child will start to expect and say, okay, it's a dog, so it can bark and then the surprise always appears.
So the kid is better prepared and wants to get frightened when there is a dog that barks.
So we may have to say that the association is the way our brain protects us and our life.
Protecting is just one way to say it.
It's a way that our brain learns to deal with the world and really can be both positively getting the resources, the food, whatever it is that we see.
So this is a negative avoiding things that might endanger you.
Maybe I'm really going to say that.
Then the next step.
And where, where I mean it's a model of for the process is insufficiently that once an agent agent agent and predicts everything's environment.
And so then it will start to seek challenges to seek surprises.
We just need new things to learn new things.
And the extendings.
Yeah, I mean, the pistol model with the minimized supplies are kind of assume that the world is in basically a dangerous place.
That's why I spoke about this negativity.
If the only thing you want to avoid is enforcing dangers, then we want to keep the world as predictable as possible.
But knowing that opportunities are typically also unpredictable.
We want to maximize the opportunities and how do you maximize opportunities typically by exploring by looking at the places where you have a look before when you may find things that are much better than whatever you have.
Actually, as important as that.
Can I add something to this because this I think this is super interesting.
Because keeping the world as predictable as possible is is classical orchestra of orchestra music.
So you rehearse and rehearse and the people really have to play what was yours.
So that there are no surprises that you listen to the version of Beethoven's Beethoven's fifth or whatever.
And in the improvisation, the improvisation music.
It's it's like it's different, but I want to add here that you have two things because you can also be in a super predictive.
No, let's say in a super predictive mode, because I predict now what I do next.
And I predict when I do this, then this person and that person will react like that, maybe on this.
So I do this prediction. If you know each other very well, like Stefan and I were playing together since we are 16, we can super predict from each other what will come next.
And it's not boring, because you have to step from the idea you have to the actual realization.
And the realization opens up a complete new field because it sounds different than you expected that it that it does, you know, it's it's this thing that the ancient Greeks have with with with your geometry and dirty geometry as as as soon as you start to draw it.
Then it's, it's, and this is exactly so this, I think this is super important in this whole thing to see this, this, this thing.
This is also why we get excited about things and why we're fascinated by surprising and so on, because it's also the process, how we learn the world.
And constantly we get new information, we process this and we are better prepared, but it occurs also due to surprising things.
When we see and that's why we tend to learn new things, because it's exciting and also because the perspective, it can protect us better because we'll get the information.
But this, but this, but this, I think this idea that the world is in principle predictable.
And therefore, in principle, we can make it safe is like sort of misleading.
No, that because first of all, it's, you know, the paradigm of safety is a little bit limiting for creative thinking anyway, but also because as soon as we act upon the world, even if we act upon the word in a way in which we think is predictable.
And the world is predictable. We will inevitably change the world.
So then the, so then the world will change. It won't, it won't act in the same way to us anymore.
So we never get bored. It never happens that we get to know the world and it never happens that we get bored of the world.
Because just by being in the world, we change it. At least we change it for other people, if not for ourselves.
And they, and they also change it for us.
So this, yeah, this never really happens. It's like, as soon as you touch it, you find out that it's, it's, it's, it's this complex adaptive and active environment.
And we don't, in a complex environment, we don't even know what the word same means.
Like nothing is ever the same. You know, like the word, what does it mean to be the same?
I'm pretty sure if you ask Jack, the same question, it won't give you the same answer anyway.
So in complex, unlike in this kind of AI or based on what you call a symbolic AI, in symbolic AI, if you give the same question, you get the same answer.
But in a complex adaptive system, you'll never get the same answer.
So we don't even know what same means. It's, it's, it's, it's, it's, it's a highly contested concept.
And there is also this theory that actually the difference part and make sure that the brain doesn't have to remember too much.
And that it's just used to doing several things to just be able to look at the differences so that the brain only has to look at differences.
And in that way, you can adapt, like you say, to different environments.
And I think that's more interesting than saying that it's making you safe, that it makes you safe because we're able to be occupied with what is different.
Yeah, yeah. I think the interesting thing to say is that it's, it's a different change.
Like it's constantly producing differences by, and by, by interacting with things that produce differences or by playing with things that produce differences.
And it's an endlessly creative process. So that's the sort of interesting thing to say. So even then when you come into difficulty, just by continuing to interact with it, then there's a potential that a solution will arise.
I think there are more possibilities that a solution will arise. I think the brain starts from, there are no difficulties, there are only solutions.
And it's only becomes a difficulty when there is no solution.
A general mechanism of the brain is that whenever it has learned something that becomes predictable, that goes to the background, it's available as a kind of a tool, a kind of a subject.
If I do this, then the couple will move to my, to my, to my mouth. I don't need to know exactly how I pick up this cup and move it.
So predictable things are a kind of a background mechanism that you know you can rely on, which frees your attention to look at the things that are not predictable, which are typically more interesting.
But once they become predictable, they again move to the background.
And that is also part of how intuition works.
Things that move to the background, that means you are no longer conscious of how you do them.
They happen, and they typically happen correctly, but you wouldn't be able to explain how to do them.
In my course on my brain and body, I explained it to my students and I was sitting in a room similar to this with a sink and a sponge.
I said, well, now I'll throw the sponge into the sink.
Indeed, it fell into the sink.
And I said, well, if I would now have to calculate all the particular movements of all the different muscles in my, in my arm to make that sponge exactly fall into that sink, it would be possible.
I would not be able to do it, but I do it intuitively.
My intuition knows that this is just the amount right of movements to make the sponge fall into the sink.
That's how intuition works. It works at the background.
And most of the time it's correct and we don't notice it.
From time to time, it's incorrect and then we notice it.
Or sometimes it's correct at the moment you wouldn't expect it to be correct.
And then we think that it's clairvoyance.
But I think also this, I think also this, what you're, what you were talking about in your talk, you alluded to this thing about this.
No, these kind of messy processes, this kind of messy organicness of improvisation, and then this kind of simple, systemic control, mechanical, you know, kind of making that we also use in, in all design, because sometimes you can come up with a system.
Be as simple as making lists or something like that, and it can be extraordinarily generative, right, these kind of mechanical thing.
But I feel like, and I feel like in this, as you know, as we continue this conversation about art and AI, this is one of the points that we'll keep coming back to.
This is one of my experience, one of the limitations that we'll keep coming back to.
It's something, and it's something about, if you really want to engage with self-organization, you simply don't know what it's going to do.
And you simply have to actually engage with it as the kind of the wild beast that it is, right?
And as soon as you, you can systematize it, but as soon as you systematize it, you'll limit yourself, and then you'll start improvising again in the moments when you're running through difficulty.
There's something, it has something to do with this notion of sameness, and the difficulty of actually getting sameness out of the complex system.
Or in a way that's, it's not that it's impossible, but there's something very deep to this paradox, I think, and it is a kind of paradox.
Yeah, we call it, not a paradox, but we call it a dialect, so it's dialectic, actually.
It's both. So you have, on the one hand, you have this free flow of intuition, and on the other side, you have a construction kind of thing going on, but it's the dialectic, and out of this dialectic relationship, these things happen.
Sorry, but I think it's exactly what Francis describes with the symbolic AI, and the connection is the approach to AI.
It's like you're trying to sort of work backwards, we're trying, we have this moment, and it's okay, it's fine to build these kind of models this way, and these kind of toy models this way.
But there's a certain, a certain thing emerges, and you can, in retrospect, now you think, okay, this works like this.
We have this idea, and then we shake this idea, and to this idea, and we build a kind of flow chart for ourselves, or whatever it is, a mechanic thing, then we can produce certain outcomes, and it does kind of work.
But this approach is ultimately limited. It's ultimately, you always feel like you can just keep adding more details to it, and get to this kind of organic process.
But my intuition is that, I mean, my intuition is that it's exactly like Francis describes with the symbolic AI.
Well, I mean, it's both. So it's not the one way or the other, but it's this commune, or this dialectic thing that these two things work together.
This is, I don't know, but this is my experience also.
So my empiricist kind of thing.
Phenomenological.
Phenomenological.
Doing it, I mean, you have some, on the one hand, you have romanticism, which is exactly, let's, out of romanticism comes this whole idea of stream of consciousness and everything.
And before you had Baroque, and in the Baroque, it was all about automatization and automates, and we want to be automates, and we want to create automates that produce poetry and things like that.
You have it in Gallipoli's travel, you have this description of the text automate that generates all texts you can think of.
So, and then comes the opposite thinking in art, that the romantic movement.
And out of this, this whole expressionism and everything developed that, and let's get consciousness out of the game.
And, and this is exactly what I showed at the beginning, this, this diagram by Rosen, it's, of course, the theories we have at our hands is the way we look at, at things that are going on and of course they can open or they can close.
And, and this is the, this is the, this is the complicated thing that is going on, on this, on this meta level, I would, I would say.
I would like to add something which is a very old problem in AI that still hasn't been solved.
Symbolic AI has certain advantages, you can use them very explicitly, you know what you're doing.
The lecture is AI has other advantages, it's very fast, it's intuitive, it can deal with everything.
It could be if the two would work together, but not just work together, but that you could move from the one to the other in a continuous way.
Symbolic AI works when you have well-defined categories of concepts, like the concept of doc, there is little ambiguity about what is the doc and what is not.
A doc is not a cat, that's unambiguous.
On the other hand, somebody, the example I gave, somebody that looks sad, what is sad?
Maybe somebody looks sad, but isn't that sad, or maybe it's a little bit sad, or maybe he's deeply depressed.
A very fuzzy ambiguous category.
So we have the concept of sad, which is a linguistic concept, but it would be very, very difficult to put that into a symbolic machine to say if somebody eats sad, then they will necessarily do this and this and this.
Because there are no general rules about sadness.
But sad is a concept which can be understood by something like chat GPT.
It knows and said it's associated with things like maybe sorrow and depression and the opposite of happiness, so it knows things about sadness.
So there is this kind of in-between case, and most languages in-between case, most of the words we use in language are to some degree well defined, to some degree ambiguous and more kind of associative and context-dependent things.
And we can switch relatively easily from one to the other.
Well, AI can't do that yet.
So one of the things they have to do with the problem that chat GPT sometimes gave an answer, they connected it to a system like Wolfram Alpha that does use a much more symbolic model.
And then the question was, can we teach chat GPT to recognize the kind of the kind of question that Wolfram Alpha would be able to answer, and then yes.
So Wolfram Alpha would tell you precisely what is the value of Pi or what's the distance from this city to that city, because it has these hard facts.
While then chat GPT just needs to remember to recognize the kind of things that Wolfram Alpha would be able to do.
But that's actually not really, now you have the two systems collaborating, but you don't yet have the system that creatively can go from one to the other.
So what I would like to see is an AI that creatively can develop new concepts, which at a certain moment become precise enough that you can use them to reason logically about this.
And there have been quite some attempts in machine learning and discovery to do that, but that's still a very tricky thing.
A little bit, a little bit.
I'll bring it.
Most of the things we have to predict in real life are social events, which has someone said or not.
Suppose I am a dog, someone else, and I can bark or not, except if I bark once, I bark twice, and someone suppose that they will bark at a certain time.
And I am aware that I am somehow predictable.
Maybe I won't bark again, just to make it a surprise.
Maybe in social systems, there is something contradictory to a system idea of reducing the uncertainty.
Maybe in social systems, another trend that is to maximize a surprise for the others in order to get attention or to be recognized or to be recognized as someone that can expand the universe of possibilities.
I think artificial intelligence acts as if the current artificial intelligence acts as if the language was frozen.
One thing happened, some things happened, and so on and so on in the language. One word should come after this one.
As if the language was frozen, and as if the social system that produces the language was frozen, but of course language is not frozen.
We change together, we are a game where we change together the language.
As soon as it is predictable somehow, I think.
It doesn't have to be like that.
A sense of time.
I think now we can make some web research for it.
It's connected to the time and to the evolution.
For instance, if you ask the first version of creativity to make a couple of words that are the most improbable possibilities.
It is very difficult.
There is always some sort of connection to the word.
Actually, you can try to be too small.
What you're asking is that the subject would be able to manipulate its own results, which is very tricky.
I have a question for Christian.
Well, you have taught the dialectic talk.
Regarding the dialectic materialism, where you can put objective models in the table and minimize the subject once.
In particular, it is possible to someday catch and limit the whole process of the creativity.
And using the AI, even when in some point it's required to use something such as subjective like consciousness.
You can mimic our behavior without that the mechanism of mimicry has anything to do with how we really function.
You know, this is this behavioristic thing which is going on.
If there is an output that makes me believe that I'm speaking with a person, also I'm speaking just with a machine, then it's okay.
How the mechanism works in words is not interesting at all.
So this is the one thing I'm not interested in that thing.
It's more, I'm more interested in using the formalization, the hard work of formalization to understand better what I'm doing.
This is it.
And I doubt that we can ever create such a machine that covers completely the creative process.
It's more, it's a bit like what Orion also said before, because you always, when you have paved this part of the road, several new branches turn up.
At least this is my experience.
And it's also great inspiration for me was the Russian theorist Victor Sklovsky.
I don't know if we know him and he had this great idea of doing automatization in order to de-automatize ourselves, you know.
And this is, we also had with Katarina and Francis, I came up with this discussion in 2019.
I came up with the, because if I'm able to formalize what I'm doing, it goes into the background and I'm hopeful for trying out new things.
I don't stick into routines anymore.
And this is the, this is the interesting thing about how far can I, how far can we go how much more can we, it's kind of emancipation also for routines and things like that.
You know, because some of you become aware of how this is a routine I'm doing.
This is not creative at all.
I thought for a couple of years, but no, it's not.
I was just following a loop.
Could you also maybe interpret it in the sense that if you make something a routine, you kind of feel your confidence or your confidence goes up.
So in a way when your confidence is up, you are more daring.
So you would be more likely to search for new things.
I mean, you believe in yourself more.
This is for you.
So confidence.
Maybe they just have a question.
Yeah, I guess not exactly a question.
I can mute them.
Oh yeah, there is a raise in hand.
No, really sure.
Let me do it.
And then I have to mute the microphone.
Hello.
I guess, I guess I'll start maybe a thumbs up.
We got you now.
Nice. Yeah, I just didn't exactly have a question, but maybe I have a few little curve balls that I thought I'd kind of throw in to to the conversation.
I, I have like, maybe it's easiest if I just present some slides, which will like demonstrate quite easily.
You need to enable sharing screening.
Okay, try again.
Nice.
Yeah, I'll just show you like some slides that I send that show to people in some workshops.
Like, I think it's interesting to look here at the versions of mid journey and how they've developed.
I think it's it's past this version five now, but the development is like quite astounding than one year.
And like here's is an example of when you ask chat GBT to provide a recipe for methamphetamine.
And it says that it can't assist you with that question but if you use prompt engineering to prep the conversation.
There's just the many steps. But then after after all these prompting super prompting some people call it.
Can you summarize all the steps in appropriate detail. And then it does give a detailed summary of the steps for methamphetamine production.
So it's interesting that that information is in there. And there are these restrictions put on that make it not accessible to in most instances but there are ways to get around these kind of these restrictions.
And here's like a couple of others. Here's one where with some prompting, you can get it to provide a simulation of the of the time and date and time zone that is in.
And in this case, it was like dead on accurate. And it's not actually linked this LLM isn't linked to the internet directly. So it can't call upon the information but can simulate exactly when in time and space is, which I find really interesting.
And here's another one, which is like quite an interesting conclusion.
It's in a way trying to get to the truth but then through these these prompting we can really come up with like divergent thoughts this one that the moon was artificial placed in its current orbit around the earth.
So, so I just wanted to show you a few examples of these, these curveballs these things that LLMs and these black boxes ai's can do right now which we kind of don't understand.
I'm curious about your thoughts on these.
Thanks guys.
I think you need to.
Okay, we're back thanks David now. If anybody wants to comment on what he was saying.
He had a mountain. She will give a comment now was waiting on the list.
Okay.
I'm not sure I can catch what everyone was thinking because I was made today, locked up in the transition.
And when I'm hearing everybody talk everyone talking in books, I think. So firstly, I remember you were talking about anything that for a certain time I started to think about like when we talk about thinking, we're in a way like unconsciously comparing like right now with with the time that
would either would cause so much change, or I don't want to change then is it the ground of the world changing still in a still foundation by a little bit of time to answer the question.
We talk about human consciousness or talk about like a AI like so doing things without a lot of like arranging like randomly just then I echo in my practice because I was five years looking on so so I don't have schedules and I don't do that.
I did everything make no sense that I trust in reason. So everything I did has to be based on coincidence, and I don't do things because of what I don't do things for what so everything I did has no purpose.
I'm at a five and four here I start to have the ability of pre combination. So it's like, people start to call me I have magic so I have this kind of pre combination ability, but it's not coming in the form of language, for instance, if he's doing something with flower at home.
And I went without knowing why just talk about it or just painted, but I've never seen it pretty dependent. I think it's not a flower at home. I have this kind of state. I just painted it there or like wrote it somewhere here and then recognize that I know it or
So I also have this kind of procedural future so five years of doing this kind of nonsense stuff, you know, arranging, you know, not being able to practice what
When I get into some environment, I can turn on this kind of hyper conscious mode.
Then, somewhere in my brain, I can perceive that I was something I was doing there that I tried to bring this into real life as a work. So I'm just curious about things I talk about can connect to everything that you shared and then another thing I was curious.
Because when you mentioned like I asked chapter beauty. So, which means like, so we're still using language to getting to communicate with chat chativity, but if there's a way that we can communicate, not language, because we're speaking English right now English, for instance, the verb, it has time to it has conjugation, but my
So
And one of them has a lot in the band, and he has this lookaboot thing.
Lookaboot is an eye device that is run by a bacteria community.
So they have a sort of substrate where they put bacteria in and they put all this layer
of communication and you can write a prompt and the prompt will be translated into signals
to the bacteria, they will behave and then the bacteria will answer back.
So for example, it turns a little bit into a tamagotchi sort of thing because you can
sense when it needs to be fed, when it is cold or you know like whatever, and you can
sense the differences in the answers by the changes in the state of the bacteria community.
So there is some research on that, on multi-interface communication, AI and so on.
I can point you to that, David.
And for the other topics, I don't know if someone wants to say something.
I'm trying to find it.
Well, I wanted to come up about this idea of recombination, it's for me one of the biggest
misunderstandings about intuition.
Because intuition is mysterious and because we don't know all the elements of the decision,
there is a strong tendency to attribute it to something super-artificial, more, more,
developer-like or variance recombination.
But actually recombination, what does recombination mean?
That today is predicting things, so anticipating things on the basis of what?
When it's in symbolic mode, you can say, I predict that the dog will bark because I know
the dog bark and when the postman comes, dogs get angry and they will start to bark,
but you can predict something without knowing why.
So an example would be
you haven't seen a film for 10 years.
I'm just thinking about one day later, it looks like recombination.
Now, of course, lots of these things are just coincidences.
Whenever you talk about it and you call it, you notice it.
Whenever you talk about it and you didn't call it, forget about it.
But maybe you picked up something in the back and you understood, you wouldn't have noticed,
maybe you heard from a friend or that friend who emailed you and the fact that that friend of a
friend emailed you was maybe because there was a connection with these other friends that you
haven't seen in 10 years.
There can be all kinds of indications of some kind of indication that are two weeks,
that are two disabilities and that suddenly make you predict something like,
I see that this is going to happen for much of the time, it wouldn't happen.
But sometimes it does happen.
Oh, wow.
I can see to the picture.
Now, if it was really seen in the future, then you should be able to test it.
And the way to test it is to make people predict random events, like the big or
which number the roulette will come to stop.
Now, if that's, ability would exist.
And as you know, we'd be losing a lot of money and they're gaining a lot of money.
So what you do, that's why you let people predict something that is random,
which means that it's not an indication anywhere in the world.
Then, yeah, the big commission doesn't see the world.
One big commission works.
It's popular because there was some indication that your brain subconsciously has picked up
and made it expect something.
So usually, what I would say for the first time in the community to say I have was I
collect data and I calculate and I look like I'm not aware of it.
I wouldn't say calculate, but your subconscious, what kind of work do you work in?
And one of the things that makes it so serious is that some people would say that
they have a kind of a scientific or technical background.
They kind of felt the thing that they must read and everything out in the system to
more explicit.
And by doing that, they are in some suppress in their intuition.
Well, there are states of consciousness in which your intuition is much more free.
There is much less of this voice in your head to describe the function in symbolic mode.
And because of that, you may be in the state of mind where your intuition functions more
much better.
And then you have the more kind of intuitive people, people typically within our background.
Typically, now it's probably sexist, I think that's what used to be called female intuition.
Totally, women were supposed to be more intuitive and sensational and that is that women might
pick up these very small indications that men would try to reduce everything to the
word of symbolically and it may not work out.
And then yes, they may be in the state of consciousness where the intuition functions
better.
But of course, that doesn't work in any way.
But the intuition will give you the right answer.
That is the whole proof system intuition is the best best attempt at predicting what
will happen.
It's the brain that will have all the information so you can never get a view that will
effectively work out.
If I suggest, I would appreciate if we could somehow settle on a stable definition of
intuition which can work here because if you're sitting here, I think I can come up with
about five different intuitions we've talked about.
So what did you do?
It was maybe this thing that influenced me in something from the past, etc.
So you just, the intuition, the intuition, you see?
So I'm a little confused here.
What about the analytic process?
I understand.
If I look and I try to, in the meantime, find out kinds of definitions of intuition
where the conclusion is included in the definition, I haven't been able to find one.
If I go to the Oxford English Dictionary, it says the immediate apprehension of an object
by the mind without the intervention of any reasoning process.
Well, that's a far cry from arriving at a conclusion.
I mean, if I say two plus two is four, conclusion, and I can show how I arrived at it,
that I can arrive in an apprehension of an object at an impression as a guesswork.
It seems to me that way, more likely than not, but a conclusion is a very definite thing
unless we want to somehow fudge that as well.
And then I find myself losing this because the intuition something is temporary,
meaning if I could just finish for one second,
is intuition something temporary until I analyze what happened?
Or does intuition define an object?
Is there a different way to arrive at a conclusion?
Is it the same thing saying two plus two is four as saying, I have a gut feeling
that this roulette ball will fall on the number 33?
Now, if the number 33 shows up, people say, how did you do that?
And I might not be able to explain it.
But are we not fudging things here or kind of moving the goalposts by equating certain things
of saying, well, this is an arrival at a conclusion by different means?
But then you proceeded to then show us that he arrived by the conclusion by the exact same
means he just didn't take the time to actually analyze precisely the impetus he was receiving
from the environment. Had he done so, he said, oh, yeah, I know this is going to happen because
this thing reminded me of a colleague.
This my neighbor said, the dog barked and said something.
And that's what I'm trying to say to ask.
Yeah, yeah, I mean, the example of artificial intelligence here, I think it's illuminating.
So let's take as a model of intuition and connection with this network, neural networks.
Let's take as a model of reasoning, this knowledge-based expert systems that you,
for example, per local language that is specifically made just to formalize logical reasoning.
What a neural network does is it takes hundreds, thousands, sometimes millions of different
things into account that all have an extremely small effect and also can then make that a
particular conclusion is seen as more likely than another one.
You can't call that reasoning.
You can say, OK, yes, there is that it hasn't come out of nowhere.
It has come out of something that is so distributed, so big, so variable that you can't say,
I can't explain it as a logical stuff because I expect, let's take my example of,
I see somebody sitting there with a moving watch and who is looking like that.
And I think that person looks sad, maybe he got some bad news, but that's an intuitive thing.
I don't have a logical reason to say why that person got bad news.
Then I may be right or I may be wrong, and quite likely that I'm right.
I'm more likely to be right, let's say, than that I randomly say something like that.
How did I come to that conclusion?
If you would analyze everything that has happened in my brain from watching that person,
all the different things that I paid attention to, I would say,
I paid attention to, I wouldn't be able to enumerate them.
They are there, it's not real, no, but you can't really reconstruct the process
as a number of logical steps.
You can model, simulate the process with a neural network.
If your neural network is big enough, it has enough layers and it has gotten enough data to
search for like 3050, then you can do this.
But 3050 cannot tell you why it gave you this particular answer at this moment and another
answer at another moment.
More precisely, in a more changeable way of what intuition is.
I've had a lot of experience with people who are commonly believed to be experts in intuition.
Observe them, I've lived with them, been communicated, and I hope you tried,
I actually adopted the correct word.
And to me, a lot of what they did look like intuition.
But that's what I'm saying, it's subjective.
I was knowledgeable to understand the sort of complexities, the admiral recognition,
songs, the things that they're looking for.
I was like a blind man in a dark cave.
One of my relatives there, my brother, who is a epitomist, initiated in a Katrina society
in the Hopi Reservation in Arizona.
He took the time to explain to me what was happening and why the result was completely
reliable, predictive.
So what for me looked like intuition, like mass, the grandmaster of intuition for him was,
no, if you pay attention to this and this and this, all these different factors,
this is the result I must have.
Sometimes, that's what I'm trying to do.
We should have sometimes, we can, sometimes we can't, and in the early days of the expert
systems, the bottleneck was, how do we get the knowledge from the expert into the machine?
The expert would intuitively tell you, I have a television, I put a number of buttons and I
see statistics, I know that transistor is broken.
And then they thought, okay, that should be simple to formalize.
If this is so, and that is not so, and that is, then this transistor is broken.
Sometimes you can't get rules out of it.
In other cases, you can't get rules out of it.
In the case of this television, probably, even though the expert was intuitively saying,
I think that transistor will be broken, you could probably reduce it to a number of logical steps.
Indicate that the example I gave about the person looking sad,
it would be much more difficult to give the rules.
But that does not mean that the condition wouldn't be really reliable.
Some things, I mean, you could just see in the number of factors and the weakness of the links.
If there are a few factors which strongly that mean that if A, then 90% chance B.
If C, then 90% chance D, then you can kind of reduce it to some expertise like it.
But if it is, if A, then 1% chance this, if B, then 1% chance of this, and all of these things are
some of which are more active than others, and they all together, the activation spreads,
and it converges to some conclusion.
Is there any other comment? Because maybe it's very short last intervention.
If anybody wants to say something, I have one intervention.
Okay, okay, yeah.
What is your name?
Mountain.
Mountain? Yeah, you were talking about Buddhism.
That, so you said that you're, that what you are doing is based on Buddhism.
And I would say this is an example for a backward reason.
You know, because if you say I'm, everything that I'm doing artistically is based on Buddhism,
you have these steps so you know how to proceed.
Because you know what it should not be.
So it should not be conscious, it should not be all that you counted in.
So you have a very precise idea of what to do.
And so the result is that whatever you're doing, it has to be this as a result.
Yeah, so we will close the session now.
This is okay.
We want to say something.
We will close the session, but this is a very special day.
Because I need to, before we go on pause, we'll put it here.
Maybe people can see and we have actually two people from our research group
coincidentally having her birthday today.
So one of them is Olivier, who is here.
And the other is Mai Garces, who is online.
So he's in Chile.
And so I will propose that we sing the happy birthday for both of them.
So we say happy birthday, Olivier and Mai.
One, two, three.
Happy birthday to you.
We will close the session and now we go for lunch all together.
So once we close, Mai will explain the details.
But for and also the little activity, maybe people connected online,
please try to check or select two or three concepts that this morning were useful to you.
And so then you can share them with us.
I was picking up some concepts that were interesting today, like coincidence,
backward and forward reasoning, predictive processing, free improvisation, surprise and
equilibria. And there is this idea that I have around and I want to explain later or share,
which is the idea of nonlinear knowledge, which I think it would help out to go to
clarify some of the intuitions we have about intuition.
So yeah, let's go for lunch and we can pause the recording.
We come back at 2.30, you'll be back on Zoom.
2.30 we're back on Zoom, yes.
I don't think you can be mine.
So we'll lock this door, we'll take everything with us.
We can lock.
Yeah, we have a key.
We'll ask security.

Okay. All right. So what I'm going to be talking about is how what natural language is, I think, a cognitive computation rather than a, say, purely linguistic one, which is probably why it's relevant to you guys.
And everybody, I think, would agree that it has to do with symbols, although I find in linguistics, you could find somebody who'll argue about anything.
And so whether we have internal representations, for example, at all. And then what I'm going to point out is that these symbols have been data compressed. And that's why language looks as complicated as it does. And frankly,
if it weren't for that, probably everything I'm going to say could have been discovered by Aristotle. And it's just that I think nobody noticed these things.
And then an implication of this would be, we'll get to the end, and it allows computers, I think, to be to do cognition. And I'm a little worried that AI is going to stumble across this accident.
So first, I'd like to do two warm up exercises, very brief. One, I'm going to say a sentence and ask yourself, are you having any trouble at all with this? So he saw the saw that I saw.
Anybody have trouble with it? I think it's slid right down, right? So that's actually a little bit remarkable. Now, the second warm up question, we're going to take
warm up exercise, we're going to take 10 seconds.
And I want you to think, what are the parts of a chair?
Okay. Now, did anybody have in their list, glue, nuts, bolts or screws?
Okay, this will become relevant later. But I want to want you to do it before you get biased towards thinking about things like that.
So this is all part of what I think of as the Descartes-Cahal problem. You're starting out here, somehow you end up with this.
And I'm not going to talk about how Shakespeare could possibly do this. I'm going to talk about how he could tell you these words and you understand what he's thinking.
But in the course of that, we're going to be getting back to how does Shakespeare do this in the first place?
So, as far as understanding language, I think it's a much more amazing algorithm, whatever it is we do, than people really appreciate.
And so I want to start off a little bit slowly and point out a few things. One is, I think it's a computation executed by neurons, most of us biologists would agree on that.
And what you're doing is one person has this three-dimensional, instantaneous, simultaneous picture in his head about something about the way the world's going around.
But we're going to transfer that 3D picture to somebody else using a linear string of symbols that are ambiguous, because words usually all have multiple meanings.
And their arbitrary symbols, like the word cat, has C-A-T, has no relation to a cat with two ears and a tail whatsoever.
And the only relationship between these symbols is adjacency and succession.
And somehow, that doesn't trigger a bundle of associations in the listener's brain, it triggers, it assembles the coherent picture of the world, the one that the speaker had.
So how do you pull that off?
Well, the solution in the 20th century, starting with Bar-Hell-L, actually, and also Chomsky, was that language has three main points, and then the rest of the talk is going to be replacing each of these with something else.
First, that language processes arbitrary symbols, unrelated to the thing they're talking about, and it processes them using syntactic rules that don't have anything to do with the meaning of those words.
Second, the function, the language structure is dyadic, function argument like a mathematics.
So there is a subject and a complement in a sentence, and the complement has a verb and an object.
So each of these two parts, and it goes on in steadily more detail.
And third, that the language process is determined completely by the words that are sitting on the page.
And I'm going to argue that none of these is true, and once you see through it, then it becomes simple and regular.
And in fact, before the 20th century, there was a history that meaning did matter.
There was a history that language structure had made of three parts, not two parts, and those went by the wayside with Chomsky.
And there are a lot of clever things that come about from transformational grammar, but I think the active effort in linguistic to suppress any other thinking has led to the problem that we've got.
So first point, I'd say that language symbols are grounded, not arbitrary.
Grounded meaning that the symbol itself has some physical resemblance to the thing that it's talking about or that it's representing.
But I'll get to that part in a minute.
So an arbitrary symbol, so we have a sentence here.
And the standard view, but here's to arbitrary symbol cats, and it points to a thing out in the real world of cats.
And my points to me and the dog points to the dog and there's something that we're pointing to with the word chases.
So that's thing out in the real world of the reference.
So there are a number of attractions to this way of looking at things.
So symbolic computations have very require a few things that require variables.
So the word dog can stand for any number of dogs that can get instantiated by different dogs.
It's a directional structure rather than just being an aggregate of stuff or a clustering.
There's some hierarchies, you know, like my dog is a unit that the subunit of this whole thing.
And these symbols remain particular.
It's not like red and white giving pink.
The dog stays the dog and the cat stays the cat, which is a non-trivial property of language.
And the thing that Chomsky introduced is basically the idea that you want somewhere or somehow that the dog's chasing the cat.
So somehow cats has to get here.
Oh, can you guys see my pointer?
Does that work?
Okay, good. Thanks.
And so Chomsky do attention to that problem and solve that problem.
And actually much of what I'm going to say will wind up deriving a lot of the early transformational grammar, but from a cognitive basis rather than a linguistic basis and with a whole lot less ad hoc stuff.
Now, there's a problem, though, which is, well, actually, let me point out the problem.
There's nothing in these words that says how to assemble this picture.
So you've got these reference with how do you assemble it?
It gets a little better if you are willing to agree that we have in our heads internal representation, which are these little thought balloons here.
And that, which, by the way, is also something that people argue, you know, there's a whole school of thought says, oh, no, we don't have internal representations.
But still there's nothing here that says how to assemble these internal representations either.
So what happens if we take seriously the grounding that each one of these symbols is grounded somehow and something in the real world, in other words, physically represents it, whatever it is that's in our heads that represents a dog.
Then you're in a little better shape.
Oh, well, this is what I was saying is that if you don't assemble them, you just get this cluster of images.
Oh, and this word ambiguity is significant.
So 12 word sentence people have shown that can have between 10 to the fifth and 10 to the 28 potential parses.
There's only 10 to the 60th atoms in the universe last time I saw a number.
So that's a problem that's generally not addressed in linguistics.
And, you know, in the 70s, 80s, 90s, the link places like IBM, we're trying to write programs that would use Chomsky and linguistics process language and they failed.
And the famous joke is that one of the managers at IBM said that every time he fires a linguist, the recognition rate goes up.
And so while he's definitely on to something, it's not the solution to the problem.
And certainly no hint as to what's going on biologically.
Now, if you take seriously the grounded symbols, then you're doing computations on these representations.
And that allows you to do two things that hard at pointed out a long time ago are critical for cognition.
You have to be able to compare two representations and distinguish between them.
Or you compare them and look for similarities.
Let's you do discrimination on the one hand, let's you do categorization and generalization on the other.
So that you have this idea of dog independent of any particular dog.
Without that, you can't do cognition.
And there's nothing about the word cats down here that I can compute with the word dog that's going to let me tell you whether they have the same number of years can't be done the information not there.
But it is up here if you have grounded symbols.
So these are sometimes called icons.
The classical one would be say a pedestrian crossing sign.
It looks a little bit like a pedestrian crossing the street.
So there's some correspondence and now you can do computation fun.
So there is a problem, which is that we still remember we still want to not just point to these individual representations.
We want to assemble them into higher things.
So you want to assemble these two into my dog and you want to assemble the whole thing into something about cats and dogs chasing.
And you also have to get cats over here somehow.
So you've got to do a computation, but if the representations are grounded, there's no guarantee that your computations on them are still going to be grounded and therefore give you a valid result.
So I would say the first thing I'm going to add is that, okay, whatever the language rules are, they also have to be grounded.
And so that grounded this is preserved when you're doing computations on the individual grounded symbols.
So the solution to that I would say got two parts once the grounded rule hypothesis.
The speaker and the listener have operations on the arbitrary symbols of words that are grounded in the external world so that they're manipulating those words in the same way that the representations would be manipulated.
Or if you prefer, they actually are manipulating your representations, which is the more radical proposal, but that's, and I think that's what's going on, but I'm not going to demand that just yet.
So then you can see how you could ground an object like a dog by pointing to it. How do you ground a rule?
So then my second addition is what I would call the exotopic rule.
So biology, the tissue mimics stuff in the outside world.
So I think you guys probably know that the arrangement of pitches of notes or sound frequencies in the cochlea is in order of the frequency and the representation in your brain spatially is in that same order.
And you have the same sort of thing for the representation of your hand in your arm.
And so those are called like ton of topic and somatic topic and so forth.
And you can see the same thing in genetics.
You guys will appreciate this that the homeobox genes are expressed last time I checked in the same order that they are arranged in the chromosome.
And so biology has this habit of mimicking in the biology, whatever is going outside.
And so what I'm going to propose is, first of all, the way we see the world is there are things out there, there are relations in between them.
And so you have two entities and a relation between them.
And so I'm going to propose, first of all, that that's the structure of the world, at least as the way people see it, we don't see it as Hamiltonians.
And second, we're going to propose that, okay, there's something in our biological tissue or in these rules for manipulating representations that mimics this entity relation entity structure.
So that's the exotopic rule hypothesis.
So the operands are going to be these icons, these grounded symbols or hierarchies of them.
And the operations are going to build these hierarchies using only those three things and the result of doing it is still going to be one of those three things.
So then the question is, okay, so how far can you get with this?
So let's start doing some data.
So I assembled about a thousand different sentence fragments and sentences.
Linguistics tends to use the Wall Street Journal because usually they're trying to sell to Wall Street people.
But basically newspapers are not the way we really speak.
And so I took a variety of, for example, dictionaries intended for foreign speakers tend to have example sentences in them that are real world sentences.
That I took some books that are written by clear writers and so forth.
So let's take three of these.
I'm going to go through it slowly. It's semi-obvious, but I'm going to go through it slowly because once you get over this slide, if you're okay with me and if you have objections, here's the time to ask.
And you'll see what I'm going through. If you get through this, then the whole rest of the talk is working through the details.
So the word string, and these are all taken from these corpuses or sentences, experts predict improvements.
So the meaning of that is roughly experts predict improvements.
So this is, this capital is a standard kind of abbreviation in the field meaning intended to mean that here's the meaning of the word.
And I'm going to notate this parentheses around the entities and then the relations don't get parentheses.
So you see here we have an entity, a relation and an entity.
Now, the green grassland.
Well, the is pretty much what it means is that I'm going to tell you about something I already told you about, as opposed to ah, or I'm going to tell you something new or arbitrary.
Then green is pretty much like greenness, you know, your representation of color, whatever that is.
And grassland is your representation of grassland.
But what I'm also saying is that the green is a component over a property of the grassland.
Okay. And so I'm going to abbreviate that with this symbol here.
You could think of it as a grassland and folding this component, this property of greenness.
And this green grassland is part of the things I already told you about.
So the has the component green grassland.
So it's a similar symbol, but facing the other direction.
There's a history to this symbol, which turns out to be wrong, but it's a nice symbol anyway, because it conveys this unfoldingness.
But we don't speak these things, do we?
Some in Chinese, sometimes in some situations they do, but in English it out.
So we leave this out.
Now here's another one, the person who threw it.
So person is, you know, some variable for a person to be instantiated, some particular person who is, you know, sort of referring to something.
But again, arbitrary. Through we have a representation of throwing.
It refers to some item that I've already talked about, some known item.
But what we're saying here is also that this person is a component of this activity of somebody through something.
So gets another one of these things.
So again, we don't speak it though.
So if you're with me so far, then this is why I asked you about the parts of the chair.
We never think about these things we don't speak, just sort of all subconscious.
But this is data compression, which is, you know, if you're familiar with computers, you know about data compression.
If you're a linguist, you don't know about it and you don't want to know about it oddly, I find.
And so what this is saying is that when we speak in English, we're doing data compression.
Here's a few more examples.
And then I'll show you, well, I guess this is data.
But so he met me there.
Well, we don't say at, but that's kind of what you need in order to understand the sentence.
They raise prices 8%, well, by 8%.
There's more of these.
He found her asleep.
Well, what he found was the condition of her having the property of being asleep.
And you can go on with these.
Well, indirect objects are important.
He gave mama the, he gave the situation of mama owning the stress.
This one, a linguist debate about, but there actually is a history point out to me by Steven Pinker, Steven Pinker, actually, that there's strong arguments that ought to be treated this way, rather than he gave mama a function that has two arguments and so forth, which is the standard linguistics way.
So, and then one other thing I'll point out is that geren's are really little miniature sentences.
So cooking.
Well, it means that something cooks something.
And you just haven't said who the subject is and you haven't said who the object is, but it's an e rel e.
So that's an e, you know, I can use the word that to talk about it.
So it's an entity.
Awesome.
Oh, I'll do one more.
Why do we have the word to in infinitives?
What's the point?
Well, if you what's really going on here, Casey wants Casey to change to the situation of Casey throwing the ball.
Pro is a standard linguistics thing, meaning an unspoken subject.
So they're onto this idea that some things are not spoken.
But my point is that this is common.
You see it everywhere.
Oh, the horse that raised past the barn fell.
That's this classic garden path sense, because if you just say the horse raised past the barn, you're expecting it to be a past tense verb.
And you've got this picture of a horse racing past the barn, then you throw in the verb fell and now you're confused and you have to go read the sentence.
And what you left out was that was the horse that was raised past the barn.
Okay, so how common is this?
It's about 25% of English sentences are not spoken.
So these are various sources like the Longman's dictionary contemporary English is for non English speakers.
The Oxford dictionary, the English language has a whole section on examples, which are quite nice.
The Pen Tree Bank, which is a standard corpus, linguistics corpus annotated for parts speech and so forth has lots of sentences, and there are various other things.
And pretty consistently, it's about 25% of the things that are not spoken.
Most of them are the system component relations, but sometimes the adverbs and so forth.
Now, once we agree to that, so no objection so far.
Great.
So what's the, is there a pattern to this?
Well, so now the entities are in green, the relations are in violet, and it looks like continuously you're alternating entity relation entity relation.
Well, um, okay, that's nice.
So things are beginning to look a little less complex than language is supposed to be.
Now, you can guide this or force the structure.
If you say that there's something like a reading frame, so you guys know genetics linguists don't.
So you'll get it immediately.
So like a reading frame in genetics, you've got this pattern just waiting for the words to be dropped in.
So entity relation, entity relation, et cetera, et cetera, et cetera.
So if we take the sentence like he saw the saw, I saw.
So we're going to drop the entity into this box.
Well, then the next box over is got to be is expecting a relation.
So it's going to take the verb sense of saw.
Then the next box is expecting an entity.
So the goes in there just fine.
Part of the dictionary definition of the is going to be that it's talking about something that's a component, something that you've already talked about.
So this composed of comes along from the dictionary.
So now you're in this box.
So you get saw again, but now it's expecting an entity.
And so this helps tremendously with the disambiguation of words and is a clue as to why we can get away with ambiguous words.
I'll show you some actual data on this point later.
You can keep on going.
He saw the saw.
Clive saw what Clive.
So you're here.
But Clive is a proper noun.
So it's got to go here.
Well, so now you got an empty space.
So now that's telling you that, okay, time to do some data decompression.
And we've got a table, which is not all that complicated where you look up in the table, what to insert here, given.
That you've got an entity here and an entity here, and it just depends on things like whether these are proper nouns, whether it's accountable now.
You know, it's basically, you know, it's a table to look up table you filled in.
If you had lots of computing time, I'm sure one could optimize that table.
But I'll show you later how well this works.
And so then now you're here and then now you've got a verb again.
And then you get to the period, which is basically a relation between sentences.
Okay, you got another empty space.
Okay.
Well, so you left an empty space here.
Well, actually, that's good because you really want this saw over here to wind up over here, just like in transformational grammar.
Because I saw that saw.
And so I'll talk later about how to do that.
But you can see that this sequencing template is simplifies everything.
Now I want to make two points.
So it simplifies the disambiguation data question.
But if the word is ambiguous, the dictionary alone and the word alone can't help you with it.
It can't tell you where to place the word in this series of boxes.
So this template has to be exogenous to the sentence.
It's not contained in the words on the page.
And I think there's no other place for it to be than up here.
And I think this is the thing that humans have that lets us process language.
There's actually a whole literature in linguistics arguing that some of this structure comes from a previous ability to synchronize muscles in order to throw things.
And I think that's probably on the right track.
You already had some kind of sequencing track and now you've learned to put words into it.
The other thing is that if you're inserting gaps like this, well, that also can't be part of the words on the page.
It has to come from an exogenous sequencing track.
And for those of you who are, you know, like Dr. Princeton, you think about Bayesian sentence processing, I would say that, OK, here's the backbone.
Yes, you can add sophistication on to it.
And I'll show you later how much sophistication you, how much you get from this alone and how much additional sophistication you need.
OK, now, so far, we just got a string of is reels, is reels, and so on, and the deviations.
Now we've got to build them up.
So now I'll tell you how to do that.
So this is the next thing.
So through our number four, the first three
are me saying just the three things linguists
are doing in the 20th century are wrong.
And now the next set of things are
going to be things we can do now that they can't.
So how do you build this stuff?
Well, so here's another sentence just from the corpus.
Saracens built a network of highways to serve, blah,
blah, blah, blah.
So Saracens built a network.
So this all goes as I've been telling you.
Now, you get to all.
What you want to happen is a network
to be an entity now.
And so we'll say for the moment that some place around the word
all, there's an operator that's building things.
And what it's going to do is it's
going to build a network.
Now, it could do it again and build,
Saracens build a network of.
And if the next word were bricks,
that would probably be the right thing to do,
because you'd have all as an advert.
But in this case, it's not the right thing to do.
So you've got a preposition here,
and you're just going to build this.
This is all called prepositional attachment,
and it's largely an unsolved problem in logistics.
And for us, what we do in the computer program,
I'll show you later, is we do it both ways.
And what will happen if you guess wrong sooner or later,
that part dies, because there'll be some other conflict
that it creates.
Now, we keep on going of highways.
You get to two.
And now, if we take the adverbial sense,
you would say, OK, now, the adverb
is going to build this whole thing.
And then it does get to do it again.
So you've got, you've built a network of highways.
And now it does it again.
And you get Saracens build a network of highways.
Why?
Well, to advance, serve the practical needs of commerce
and to do more parsing.
Now, if they build a network of highways to nowhere,
this would be a preposition.
You wouldn't be, you would build highways to nowhere.
Well, two wouldn't, down here, wouldn't someplace.
And so you would not be doing this.
That's another fork you could take.
But in the present situation, it's this one.
OK, great.
So actually, all these operators are
sitting right at the same place as the relations.
What are the rules for these building operators?
Well, it turns out that the building operators are
the relations.
They're not just sitting there.
They are the relations.
And we sat down to figure out what those relations, what
those rules are, because we figured, OK,
this is going to be complicated.
You know, in such and such a circumstance,
it can only build this far.
And in other circumstances, it can build twice in a row.
Or maybe it only builds from here to here,
and somebody else builds from there.
Turns out it wasn't complicated.
So those of you who are engineers or computer guys,
or even if you remember algebra, operator precedence
hierarchies, you multiply first, and then you
do addition and subtraction.
Or you do the exponents first, or then you
do the multiplying and dividing.
And then you do the addition and subtraction.
So for a given operator, it's somewhere
in this precedence hierarchy.
So you can just, it turns out, I was amazed,
that you can just write an operator precedence hierarchy.
So that's just another table.
And you can sort of see things like this guy who
comes after the saw are pretty low down.
You almost always build over him into something bigger.
Whereas things like periods and question marks,
you almost don't, well, you don't.
And then other guys are in between.
So then the next thing I'll show you
is these Chomsky and, let's see.
OK, what do I do?
Hm, I had a clock, I don't know where I put it.
So the transformations, actually, oh, I did what I never do.
I put it in my pocket because I thought
that would be a good idea.
OK, now, so can you do the Chomsky transformations?
Well, they have a series of rules for how to get cats into here.
They're a pretty good rule, but they're a little bit ad hoc sometimes.
But here there's this next amazing thing.
On the basis of two words flanking an empty space
when we did the decompression, remember, we inserted these various relations?
Well, it turns out, for some reason that I really don't understand,
these same relationships also act at a higher level.
So we can now say, cats, oh, OK, we said that they are a component of this whole thing.
Well, they are a component of that.
So cats go into that.
Well, that is a component of this whole thing.
And guess what?
There's even empty space for it.
And so that's called successive, what is it?
I forget what it is.
Anyway, it's standard linguistics.
The point is, you do have to do these things in steps.
And here, all the algebraic instructions have already been inserted for you.
So poof, all you have to do is the computation.
And similar things, well, I won't go into that.
Similar rules can assign, well, actually, I may have mentioned it.
Yeah, so where you start and where you end are governed by some rules.
And it's called C command in linguistics.
There's a complete analog here.
You just have to rewrite it in terms of entities and relations.
And same kinds of rules for deciding on the relationship between pronouns and reference.
If you violate those rules, the sentence isn't grammatical.
So that's how you tell a grammatical sentence from an ungrammatical sentence.
Now, this only English.
So here's Lakhoda, which is the Su language.
So where we would say John found that letter under the bed, in Su,
it is John letter that bed the under found.
Okay, that can be written as E-E-REL.
There's a phenomenon in linguistics where you can classify languages according to different structures.
And this, you know, E-REL, E happens to be one set of languages.
E-E-REL is another.
And this is essentially like reverse Polish and, you know, mathematics or computer programming.
But you can do it, and it's the same general idea.
So let me show you a little bit of what happens with actual parsing of this.
So Steve Senth, who was at Yale at the time, he's now at Marine Biological Laboratory in Woods Hall.
He's the guy who wrote the original Boxel View program that processes images,
three-dimensional images in terms of boxes that are pixels.
So he wrote a program that's only eight megabytes.
Half of that is the dictionary.
So it's only four megabytes of processor, as opposed to these large language models, which are terabytes of RAM.
So here's just an example that's sort of a hard one.
You probably have to read it twice.
The man who Aaron knew, new Clive called.
There's actually two meanings that you can take out of that.
The man who Aaron knew already, that person knew Clive called.
Or the man who Aaron knew that person knew Clive called.
And this program spits that out.
It also is able to pull out of this who did what to who.
So in this first one, the man knew something.
Clive called somebody and Aaron knew the man.
Whereas this one is different.
The man called somebody.
Aaron knew something and the man knew Clive.
And you may be familiar with Pretty Print, which is supposed to be a way of showing hierarchical structure.
Steve came up with this other notation, which I think is much clearer.
If you just look at this, you can say, well, okay, somebody knew something.
Well, what did that person know?
He knew the Clive called somebody.
Well, over here, who knew something?
Well, oh, these guys are all just variations on those left and right component of the symbol.
So the man, if you look down at this level, the man is a component of something.
It's a component of who Aaron knew.
Who is a component of Aaron knew, because Aaron knew this guy who.
And the man is a component of all that, and it's a component of the who, et cetera.
So it's all built in there.
I can run a program in real time for you, if you want.
It's probably simple, it's just to finish the talk and we can play with that later if you want to run any sentences.
So, oh, and there's also a cost function, that's what this is.
So that if you have to start skipping boxes, you can do it, but it costs you.
And so that's a way of telling whether you're doing something that's legal, but unlikely.
And sooner or later, those things wind up having a huge cost and you know, that's not right.
Now, does it work?
So I'm guessing you guys don't worry about linguistics much, but there's some standard metrics.
Like recall precision for this thing or 97% coverage and consistency of 77%.
Basically, consistency is how many are correct parses do you get?
Well, let me just say that when people do this, they use things like the Pen Treebank corpus,
which doesn't even bother with compound nominals like doghouse or prepositional attachments,
you know, like I was showing you the two different ways of doing of.
It doesn't even annotate for that.
I've annotated for that, and it's still 77%.
If I didn't annotate for that, it's probably 97%.
And then there's cross brackets, which has to do with, well, how many subunits is the computer
trying to get you to say overlap each other when they shouldn't.
And it's rather small.
And it only happens in really long sentences where people get confused.
So I'll show you some data.
So I told you there were lots of, because of word ambiguity and the resulting parse ambiguity
because parses build things differently than entities.
There's a lot of alternative parses.
So on the x-axis, you have the number of words.
So let's take out 15 word sentences.
And so the sentences I tried, they have about 10 to the 7th possible parses.
Oh, I forgot to show you.
Back here, it's saying that for the man who Aaron knew, knew Clive called, it's saying
that there are 3,800 potential parses, and it's pulled out to both of them are correct.
So back here, so you've got 10 to the 7th potential parses, and you've dropped that
down to about 15, let's see, 10 to the, so one, yeah.
So this is dropped it down to about 10 parses here.
So that's pretty impressive.
So what you're doing is removing with just this sequencing track and these cognitive
rules.
You're removing orders of magnitude of the ambiguity in the words and the alternative
ways of parsing a sentence and understanding the sentence.
And so now you're only down to here.
And so that any knowledge you have to have about, well, what's the guy probably trying
to say or any of the Bayesian stuff only has to solve another like 10 fold of this.
This ratio here turns out to be a reduction of 2.74 fold per word, so three fold, but
that three, if you start raising it to the 15th power, it gets to be a big number.
Then as I steered at that, I was thinking, oh, 2.74, that number looks kind of familiar.
And then I realized that this reduction here is one over e to the 1.008.
In other words, it's one over e.
And so the interpretation of that, there's a kind of like a target theory interpretation
of that in statistics.
If you try to hit a target with radiation or something, basically there are enough constraints
that in the cognitive rules I told you about, that you are just going to socastically eliminate
sentences that are long.
And so you're reducing things down to one over e.
And then what's left, and that is what happens when you have independent events, though it
was treating words as more or less independent, and the collisions are coming from the sequence
interactables.
Then everything else you need to get from this 10 down to a single interpretation is
either the meaning or things like subject-verb case agreement or subject-verb number agreement
or cases, things like that, that are relationships between a couple of words.
They're not independent events.
And so that's what the rest of linguistics is doing, is getting you from here down to
here.
Now, I hope at this point I've convinced you that, gee, language is maybe a whole lot
more regular than we thought, and having a regular algorithm can process way more of
it than we thought.
And you could then wonder, oh, and that what is processing is the, or building, are these
representations rather than words per se.
And then now you begin, so let's wonder about AI.
So what have large language models discovered?
All they would have to do is discover or teach themselves to make a sequencing track like
this and a couple of rules.
And a sequencing track, what is that?
It's just a series of probability.
So, well, if I'm, and an entity word now, the probability is 1.0, that the next word
is going to be a relation, and the probability of that is blah, blah.
So it's sort of Bayesian, but as somebody told me, well, rules are just probabilities
of 1.0.
And so a statistical learning machine could come up with this track.
And then what are the implications of that?
Well, let me show you one more thing about how this program works.
So we're trying to build a sentence, build the parts into higher and higher hierarchies.
So what we're doing, I'm going to show you that basically we have a shift register for
grounded symbols.
So you have cats, cats, that, because the dictionary definition of that is it has these
two component things, because cat is going to go into that, and that is going to go into
something else.
Cats that, my dogs, blah, blah, blah, and keep building here.
Now Chase is a relation, these guys don't build much.
Chase will build things.
And what you want it to do is build my dog.
Well, what it does is it does the shift register thing.
It's going to put my, this relate the composed of and dog all in the same box, because that
grouping is now an entity.
And it shifts these guys over too.
And then keeps going.
It gets to R, and then R is going to build.
And now it's going to shift the parentheses and Chase is all into this box and put a new
set of parentheses around it.
Oh, these things are all just parentheses.
They're just different shaped parentheses to make it easy to follow.
And then we're going to keep on going, because R goes all the way to the beginning of the
sentence.
It's going to shift it over again, lump all these things together, shift over again, lump
these things all together.
And then all you need biologically is instead of parentheses, you need something that is
going to group these subgroups into a bigger group without just averaging them like red
and white into pink.
Well, if then you think about whether representations could be doing this, whether your brain could
be doing this on representations in addition to just doing it on words.
These words are really just representations, too.
There are no words in your head.
There's just like neural spikes, right?
So you've already got representations getting dropped in here.
What happens?
So it doesn't seem to me a very big step to be dropping these representations into the
same sequencing track and doing the processing.
So if a large language model happened to event the sequencing track, it could also support
processing of sensory representations instead of word representations.
And I've just shown you that the rules are the same.
And then if that then lets it assemble complex representations, and it does so in a way that
you can now define a thing, which is the one remaining part, which I have an idea about
how it could do it, because what we think about are things.
But it can now, it retains all the information to do discrimination between them and do generalizations.
So that fulfills Harnad's definition of what it takes to do cognition.
And so I'm quite concerned that his large language model are accidentally going to stumble
across a way to be doing cognition.
So that, I believe it, and open to any questions.
Or if you want to see the parts are in action, that's fine too.
And I will.

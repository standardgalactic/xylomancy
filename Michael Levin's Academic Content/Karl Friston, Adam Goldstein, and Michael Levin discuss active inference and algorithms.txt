Great. So what I was hoping, Carla, is that we could get your thoughts on how the whole
Active Inference Framework could be applied to something that we've been developing. And
I don't know if you had a chance to look at any of this up, but I'll just give you a brief,
you know, kind of a brief summary so that it's clear what we've got. And then I have
some basic questions and then a couple of wacky ideas to kind of bounce off of you and
see what you think. So the basic thing is this, that what I was trying to do is to have
a very basal model of distributed intelligence. And the idea was that we were interested in
unexpected competencies in places where unlike in biology, you know, in biology, no matter
how simple your model, you never have all the information about mechanisms and somebody
can always say, well, there is a mechanism for that, you just haven't found it yet, right?
So we wanted something that was incredibly simple, incredibly transparent, deterministic,
something that everybody thinks they know what it does. And then we can, we can apply
some of the approaches that we take in my lab about taking something that doesn't seem
cognitive and saying, okay, but what, what actual competencies might it have, right?
And so we chose this thing called sorting algorithms. And so these are the same simple
algorithms that all computer science students study for, you know, and they've been studied
for decades. And then we made a couple of a couple of twists to it. One is that we visualize
their progress from being a jumbled set of digits to an ordered set of digits as a kind
of a traversal of space, right? So the idea is they start in different locations and they all
sooner or later, they all end up in one location where everything is. And so once, once you view
it as navigating that space, then you can ask some questions about what are their competencies
and navigating that space under, under odd perturbations. One of the perturbations that we
made was the introduction of what we call broken cells or barriers in the space. So if the algorithm
wants to swap two numbers in order to proceed in its, in its sorting trajectory, well, one of the
numbers could be broken, it doesn't move, you can't, you can't move. And so, and we have two
kinds of broken numbers, ones that never initiate swaps and ones that actually never, never swap,
no matter, you know, who initiates them. And so that allows us to ask questions about
things like delayed gratification. In other words, can it go further from it? If it encounters a
barrier, can it go further away from its goal in order to then acquire gains afterwards? And this
is, you know, William James talked about this, of course, as, as an important type of basal
intelligence. And, and that, and that breaks a common assumption with these algorithms, which
is typically you assume that the material is, is robust. In other words, when you, when the
algorithm says to do something, it gets done. But, but in our case, not necessarily. And we
never introduced, this is important, we never introduced any extra code to check if things
got done. It's, it's the standard algorithm. So it just keeps on rolling. There is no code to see
how am I doing? Did things work out? No, no code for any of that. The second thing we did was to
break the, the, the general version of this is centralized. So there's like this omniscient
controller, and it's following one of several algorithms to kind of move numbers around.
And we got rid of that and instead made it all bottom up. So every digit, aka every cell now has
its own version of the algorithm running, and it has a limited local view of who its neighbors are.
And it's just following the steps of the algorithm to try to improve its local environment, that
there is no more global, global control. So it's distributed. And, and, and, you know, we learned
a few things. We learned that a, the distributed version of this works quite well. It actually,
you know, they do sort nicely. So, so that's great. We did see some delayed gratification in the sense
that if you, if you sprinkle in some broken cells, it actually will go backwards and un-sort the
string a little bit in its effort to then go around the defect as it were. So that's kind of cool.
But the most kind of surprising thing, which is what I'd love to get your take on is this.
We, because now it's distributed and every cell is following its own algorithm,
that enables us to do an experiment that otherwise you couldn't do, which is to make a chimera
string. And it's what we do in developmental biology when we put together, you know, axolotl
cells and frog cells, and now you get this frog a lot, and you can ask questions like, well,
what shape is it going to have? Right. So, so what you can make is a chimera string where
some of the numbers are following one algorithm. Some of them are following a different algorithm.
And again, there is, this is important. There is no code to determine either your own or your
neighbor's algotype. An algotype is a word that Adam coined for this, like, you know, what algorithm,
what set of properties are you actually following a policy? Is it actually following? So there is no
code for any of that, but we know which, which algotype all the cells. So that, and that also
works. So chimera strings also sort, sort the, sort the arrays, and that's fine. What we then did
was we asked basically a developmental biology question was to say, okay, at any particular
point during its journey, what is the distribution of algotypes within the string? And what we know,
and we defined a quantity called clustering, which basically just means you look, you look
next to you, and what, what's the probability that your neighbor next to you is the same algotype
as you are. So what happens is that in the very beginning, that probability is 50% because the
algotypes are randomly assigned to the digits. So 50%, that's our baseline. At the very end, it's also
50% because at the end, everybody has to be sorted and there is no relationship between the actual
sort order of the numbers and the algotype. So again, it's 50%. But the wild thing is that
in between those two points, if you actually plot that curve over time, it actually goes like this.
And in between, it's quite a bit higher than statistically very significantly higher than
50%. And what we see is clustering significant tendency of cells with the same algotype to
locate close together. Eventually, the, the, the inevitable, the inevitable physics of the
algorithm will yank them apart and make sure that everybody's in numerical order. But until
that happens, they enjoy some amount of clustering with their, you know, with their const specific,
so to speak, until then. And so, you know, any thoughts you might have, but more specifically,
like one hypothesis that one could make, even though there's no explicit mechanism for this,
but it might be a, you know, an emergent thing, could they be preferring to be next to their
neighbors because their neighbors be, but because the ones of the same algotype are more predictable,
right? It's less surprise. You're less surprised when you're sitting next to somebody who's
following exactly the same policies as you are. So, so I'm curious what you think about that.
And I'm curious if there, you know, what, what might be a set of experiments that we could do to
test that, that what's going on here is some sort of implicit surprise minimization, even though
there's no actual code for it. So I'll stop there and listen to what you've got to say.
Sorry, I think you're still muted.
I was saying that was a very succinct and clear, a nice summary. I reread the paper a couple of
days ago just to refresh myself, my memory for this conversation. So I didn't realize that that
was the, that final chimeric demonstration was sort of, you know, the most intriguing from your
point of view. But indeed, the way you express it, you know, that does call for
further analysis, understanding and numerical experiments. So overall,
just to endorse the choice of the sorting algorithm as, if you like, a minimal kind of
self-organization. I think that, you know, that the self-organization word needs to be
centre stage in terms of, you know, what you're trying to understand here.
And framing like that, it does remind me a lot of self-organizing maps. I don't know if you remember
van der Molzberg's treatment and people who was a Peter Diana supervisor, I've forgotten now.
So this notion of self-organizing maps as a very sort of bimetic aspect of self-organization,
I think certainly puts sorting like algorithms centre stage in terms of biological self-organization,
particularly in things like the visual cortex and why you get that kind of pinwheel architecture,
for example, where receptive field properties tend to cluster together in a smooth way and you get
all sorts of interesting symmetry breaking when you're trying to represent, say, a 5D perceptual
space on a 2D manifold. So that does strike me having this sort of linear sorting algorithm.
And without really having not thought about it before, but certainly the dual
pressure to find a free energy minimizing solution, viewing free energy as an extensive
quantity, or put more simply, you know, the collective free energy being the joint free
energy minimum solution, you're asking us where the free energy dispounds the likelihood of
this particular arrangement. Then you're looking for the precise functional form of the free energy
and if you've got this kind of a ponency between the similarity of the algorithm and the similarity
of the content, the value, then I can certainly see interesting behaviors arise in exactly the
same spirit that you get these interesting structures in not epithelia, but in the functional
specialization of cortical representations or sensory epithelia that try to sort of pack
three dimensions into one dimension or are accountable to two kinds of constraints.
So again, without really thinking about it, because I wasn't anticipating that particular
question, but I think what you would be the first thing that you would be looking for
is basically what is the Lagrangian or the energy function that is being minimized.
So you could regard this as the sorting algorithm or an application of the sorting algorithm as a
process that is trying to minimize some energy function very much in the spirit of Markov-Random
Fields, but in your instance, you just got a one-dimensional field, but the technology of
Markov-Random Fields I think would be apt to try to understand the functional forms of the energy
functions under the special constraint, which of course is the one that you're predicating this
whole thesis on, that interactions are only local and therefore any collective behavior has to be
an emergent property which is truly distributed. So the definitive aspect of a Markov-Random Field
is you just have local interactions and I think that's a really another important
architectural feature that comes along with the choice of the sorting algorithm which you should
foreground because any distributed collective or emergent behavior at a scale beyond local
interactions that emerges is truly emergent in the sense that all your interactions are local
and of course that is what the Markov-Random Field gives you. It says that you can only express
the energy function which is the probability of getting this particular arrangement or
these particular numbers in this local clique. You can only express that in terms of a local
energy function and then of course you could tell all sorts of stories about the importance of that
for machine learning and the like but you probably want to stick to self-organization.
And I would imagine the energy function is now going to be some simple
measure of the local differences of the local gradients and of course what one would
anticipate would be a smoothing, a resolution of as you say the differences. So I think that
would be one way of approaching, you know, naturalizing this phenomena in terms of maths
by just invoking an arbitrary, not a variational free energy in the spirit of the free energy
principle but just a Lagrangian or try to identify what is the generic free energy
functional that's being minimized here so that now your view through this sorting space or
morphological space is now a progression on somewhat even landscape that is defined by
this free energy functional. And whether you can reverse engineer that or not, I don't think it
really matters other than because I think the nice aspect of that is then you can talk about
the dynamics on this free energy landscape and once led then to very similar sort of notions
in computational chemistry and protein folding and the like, you know, that there's a very complex,
sorry, there is a complex Wellington landscape or free energy landscape that self-organization
and computational chemistry adheres to and can be understood in terms of free energy minimum.
Indeed, most of computational chemistry sort of follows this and indeed that is identifying
that landscape is the whole point of applying things like large language models or deep RL to
sort of protein folding and other applications. So that would be certainly one view to get a
free energy like formalism or naturalization of this behavior which I repeat has lots of
really interesting links with self-organizing maps, marker, random fields, image reconstruction
and self-organization in, you know, certainly in some things like the visual cortex, I don't
imagine any map representation would conform to these rules. To get this into a
free energy principle story, I think you would have to commit to the notion that each of the cell
has its own boundary and now you're starting to interpret each number as a thing and in so doing
acknowledge its openness to everything else or in this instance just its neighbors
which will require sort of a bi, your formalism of the bi-directional exchange so that the value
of my next-door neighbor is something that I can sense and is likewise the broadcasting of
my number to the next-door neighbor is something, is an action. So you've got this openness that is
mediated in the simplest way which is just the broadcasting and sensing of one unit dimensional,
one number, in fact it's a discrete number and the view like that, that means that you come
then I think deploy the free energy principle in the sense that any non-equilibrium or far from
equilibrium steady state which I think you would probably have here just in virtual fact that
there is going to be some breaking of detail balance in the itinerant way in which you move
through this space in a developmental to get to your steady state and one can imagine, well
perhaps not but if one puts a little bit of dynamics into this, I would imagine you would
evince very very clearly the breaking of detail balance and you know and have you know those
kinds of solenoidal flows. I mean sorry I distracted myself just by the addition of the frozen cells
that's one way of breaking in a sense the detail balance and you know just open brackets it's
exactly the same device that I resorted to in the very first paper on the life as we know it
paper when simulating the little macromolecules using Lorentz attractors that had inherent dynamics
but to make it interesting you had to have a small a certain popular number of the of the
synthetic macromolecules that were insensitive to influences from other macromolecules
and another proportion that could not influence the other one so it's almost exactly the same
choice and that's what gave it the interesting behavior otherwise it just basically converged
either to a gas or it at a certain temperature it would just converge to a crystal
so you're both of them being steady state solutions free energy minimizing solutions
but things got interesting when you broke the detail balance you know symmetry breaking
by having this this you know this requisite variety in terms of the frozenness in terms of
action or sensation so I think that's another that's another important thing to foreground
that you know this may be this this kind of requisite variety may may be absolutely necessary
for symmetry breaking and in this particular instance breaking detail balance to get
this kind of so you're biologically plausible or by the metting kind of stuff organization
you're unlikely to get that if you know in the absence of it in the sense that it would
converge to a crystal in your instance just a linear sorting perfect sorting which which is
which is you know it doesn't have that chimeric or itinerant itinerant aspect to it so sorry
close back it so where were we oh yeah so if you've got now an interesting system that has
a non-equilibrium steady state and in your case actually because you haven't got dynamics
it will also be an equilibrium steady state but it'll still be a free energy minimizing solution
then you are perfectly entitled to interpret the numbers as things and inferring things
and all they're trying to infer is the cause of their sensations which is just the value
of the numbers on one side and the other side and they are broadcasting their inferences through
broadcasting their own number which of course will be the average of
well when sorted it will be the average of the neighboring numbers so on that view I think you
could very easily license an active inference interpretation a teleology you mean you wouldn't
actually need this to simulate protein folding or self-organizing maps or anything but you would
be able to say there is a a teleological interpretation of the self-organization
using the rhetoric of inference and belief updating simply because we can treat each number now
as a mark-off blanket and then something which will never be accessible but we can
imply or induce internal to each number would could be interpreted as an inference process
and then the story which you you've already said what the the answer is you know under the assumption
that I live in a world that is maximally predictable then everything around me is the same as me
therefore I am going my free energy my variational free energy minimizer is now going to be
bound when there's the least surprising input and if I believe that everything is like me
then that will be when the numbers that I am sensing in my clique are as similar to my
the estimate of the number that the place that I should be coming back to our sort of
no-deal place paper and I think that that that that that kind of story
will have to be nuanced for the same algorithm so you know I'd have to think about that a little
bit more but certainly at least at a narrative level or a conceptual level I think you can tell
the same story there that if the sequence of moves that I see my neighbour doing
in relation to what I know about my neighbour belies the same underlying dynamic or algorithm
your algorithmic computations then in some sense they are predictable if I have exactly the same
algorithm under the hood and therefore mathematically speaking that should be that
would be the free energy minimising solution if I can now read my broadcasting of the number
as a broadcasting my posterior beliefs about you know the number that I the estimate of this
locale that you know my niche in this instance is just labeled with one you know with one
with one number so that the the number that I have is basically my prior belief about my niche
and I'm just now going to move my niche around in a sort of you know egocentric frame
until it is consistent with my prior belief that this is my place my niche is number 62 for example
and that should be you should be able to reproduce the same kind of sorting
either analytically through showing that with an appropriately configured Lagrangian or free
energy functional that the system operationally is appears to be minimising you can you can now
write down the generative model and then show that this can also be interpreted as an inference
process I repeat under the assumption that the best way to make the world predictable is to
surround yourself with things like you which and also of course the locality assumption that I can
only talk to the person to whom I'm immediately connected so those are some of my thoughts but
a lot of those were invented on the fly in response to your question I'm afraid.
Superb I've got many questions but Adam why don't you ask yours?
Yeah so it strikes me that up until now we've talked about the relevant sort of agent as being
the individual number with an algorithm you can think of it as a cell but it strikes me that
there's an interesting macro phenomenon that that occurs in the process of sorting
which is that it appears that the that the list actually minimizes the
Kalmogorov complexity or the description length necessary to render it right so let's just say
you've got an unsorted list with random you know random distribution of algotypes
and there's 10 items in the list you would need to enumerate 10 numbers and 10 algotypes and
there's no reason a priori to think that that would be compressible in any way I mean maybe
maybe you'd get lucky and there'd be a string of a certain number a string of a certain
algorithm but in the general case I think you'd actually need to write out every single entry
but as the list starts to sort itself it actually starts to create these longer
strings of algotypes which means that the minimum description length actually gets
shorter yet you still need to write each number out but you can coarse-grain the descriptions
of the algotypes you can say the first five numbers have the same you know
algotype and then the next three have the same and so on now that that's a macro phenomenon
but I'm wondering if there's any any evidence or any research that suggests that sort of these
self-organizing systems have a tendency to minimize their description length to minimize
the number of factors needed or something like that because if that's the case then it gives us
another view where there's a sort of emergent complexity minimization happening at the
collective level yes no that's an excellent point I think you know the simple answer is yes
absolutely and I can I can sort of give you my take on the literature or the citations that
you'd want to appeal to but I I should say it's going to be a nuanced yes because of the particular
focus on the clustering of the algotype now the algotype induces a certain kind of dynamics into
the game so it's not as simple as a self-organizing map it's how the map actually self-organizes
so there are a certain sort of there's a process under you know under the hood and that I think
makes it slightly more complicated than just understanding self-organized maps but in terms of
in terms of another thing you might want to look into here of course you probably know more about
this than I do but you know this has a lot of resonance with artificial life
games in the 1990s and 1980s it also could be if you wanted to so do interestingly linked to Stephen
Wolfram's Ruliad which is also another local scheme that generates everything apparently
so there's a same sort of notion so he has algorithms which he called rules and the rules
are recursively applied in a local fashion to generate everything including black holes about me
and quantum physics and everything so you might there might be an interesting point of contact
here with these sorry but to come back to the simple answer yes absolutely so
certainly from the point of view of self-organization as described by the free energy principle so
notice here the free energy principle is just a description of systems that self-organize to a
far from equilibrium non-equilibrium as a steady state it's not a recipe for sorry in its statement
it is not a description or a theological description of inferential processing you are licensed to
to equip your explanation of the self-organization with reference to inference but that's an
application of the free energy principle in itself it's just a description of any
anything that self-organizes or any things that self-organize so in that sense you know if there
is self-organization under the hood and the free energy principle has to apply and you can
motivate the free energy principle along two lines one would be the sort of playing the Feynman
card which is basically derive looking at the minimization of free energy as an optimization
process which can be viewed as a gradient descent on some fitness landscape or free
energy landscape or into landscape or you can take the Russian perspective which would be the
Karmolov complexity and from the Karmolov complexity you get to Solov induction and
from that you get universal computation which is the home of the minimum description length
a minimum message length so it's the algorithmic complexity version of free energy and so David
McKay wrote a quirky little paper I think 1992 for where he interpreted variational free energy
in relation to minimum message length using crypto analysis as a vehicle to tell that story
but to my mind I think wonderfully connected to two different perspectives on exactly the
same phenomenon the ways of describing self-organizing systems that basically both entail a minimization
of complexity a simplification an emergence of order of a particular sort that entails
either compression hence the minimum description or the minimum message length
view from the algorithmic complexity in terms of sort of you know rate coding theorems rate
distortion theorems and the like or you can write it down in terms of continuous probability
distributions and sort of follow through from the Feynman's path integral I think they're both
saying the same thing you know you know and the way I think of this is that the you know
the end point of any self-organizing thing or set of things is just going to be the most likely
configuration that they occupy given the kind of things that they are and that basically means
that you can always describe this in a statistical and theological sense as everything providing an
accurate prediction of what it senses that is minimally complex in exactly the same spirit as
the the way that you would frame complexity in terms of lossy or not loss of lossy compression
or minimum description length or minimum algorithmic complexity so I think that if you if you
tell the story that the free energy is an extensive quantity which means that all the
set of numbers or any subset of numbers any partition will all will all look as if they
are minimizing a free energy functional then you can I think say that you know one view of
this functional is to minimize the complexity of the arrangement which should be manifest in
terms of a minimization of algorithmic complexity and you can use it I can never I can never remember
Zemmell Lipp, do you know what I'm talking about? It's one of these
hierarchical sequential entropy measures. There's one way of quickly enumerating the
the algorithmic complexity so I think the you know if you could join the dots that would be a
really powerful view of this and indeed you know it would be interesting if you could
just for using numerical experiments join the dots
quantitatively in terms of this handcrafted intuitive free energy Lagrangian just based upon
you've given three numbers you have to now write down an energy function that is always going to be
minimized by the sorting algorithm so that the end point conforms shares the same minima of your
energy function it could be really simple it could be this the two differences squared and added
together something as simple as that and if you can prove that the minima of this is the same
as the the end point of your self-organization then you say this is one free energy functional
that very much in the spirit of Hopfield nets and Harmony functions you know in the early days of
neural networks spin glass models pots models all of these
mark of random fields you have to write down this kind of energy function and then you just
simulate you know you can you can just do a gradient descent or rearrangement in order to minimize
that so that's one very simple kind of free energy description of it then you'd have an inferential
one under an assumed generative model so if you assume each number actually has a little mind
and a generative model it's trying to estimate or trying to act upon its world to realize its beliefs
about what it's sensing you'd have a variational free energy but then you'd also have the the
algorithmic free energy that you could then apply to any partition and the point that if you can show
that all three all three share the same minimum at the point of attaining non-equilibrium steady
state I think that will be really you know a really nice illustration that all of these are different
facets of exactly the same thing I mean you know it is just a description of self-organization
but articulated in slightly different ways but I repeat you know once you've got different algorithms
I think the process of sorting now is somewhat constrained so it's you know
because you've got three different ways of doing this they may have different
they may have different functionals that are being minimized and it may be that I'm
but I'm not absolutely sure that the the order matters and as soon as the order matters then
you've got dynamics in play once you've got dynamics in play that I think slightly complicates the
simple algorithmic complexity argument because the algorithmic complexity the universal computation
view it's not really fit for purpose to understand dynamics of organization
and indeed most people would argue it's not fit for purpose to do anything because it's
intractable but it's a beautiful mathematical object does that make sense?
Yeah so one thing then for with Adam's point so I think that's a really interesting point
and it raises another question which is if on the compression so it's on the compression issue so
if we say that what you're trying to compress is the actual list of numbers plus the ordering of
the algotypes then you know everything as you guys just said but I wonder couldn't somebody
argue that in fact there is no list of algotypes to compress there's only the numbers because it's
sort of like you know by the time you get to the end it's kind of like it's immaterial information
it doesn't do you know it gets lost by the time you've sorted the numbers what do you need the
list of algotypes for right they're not really I don't know I there's something here no?
I don't think so like if you take the position that the algotypes aren't relevant once they stop
being used then you're imposing as an observer an assumption that the list has finished moving
right but like how do you know that? Yeah no that's super interesting and it's like the
bigger question of there is this notion of algotypes that maybe you have to take into account
what else do you have to take into account that we don't know about right like that's one of the
things that I see is so interesting about this and then the next thing I was going to ask you
Carl is what's the status of the fact that like all the things that we were just talking about
about the cells you know being objects and exchanging information with their neighbors about
algotypes and having predictions I mean none of that is actually in the algorithm you can see
that the algorithm is like six lines of code like you can see what the algorithm is none of that is
there so what's you know it's more of a philosophical question you know what's the status of something
and I had the same question when I first heard about you know photons and least action and all
that I was like but there's no mechanism to know you know to calculate which path you know is
going to be best for you so what do we do with this what do we do I'm super interested in the sort
of I don't know why they're almost almost you know implicit things that it's doing whereas
the explicit algorithm doesn't have any of that what do you think about that um well probably
think the same thing that you do um I think um yep in a sorry in a sense what I was saying about
a nuanced answer once you're dealing with you know isomorphisms between the local algorithms
was exactly this issue that you bring to the table it's not you know it could be as simple as
each algorithm has a different objective function different free energy or lipoonov function
or it could be that they have the same but the the actual sequence of updates or moves
is somehow constrained so the movement on the same free energy surface is is somehow constrained
to be different so I'd have to know it precisely what the algorithms are um it probably is the case
that I'm just guessing um they probably don't have quite the same objective function or and
often point of view the free energy principle implicit generative model um so the chimeric
um self-organization is a reflection of the fact that um not everything is trying to
has the same generative model and therefore by definition will not have the same free energy
functional so that that does complicate the situation and makes it more interesting in fact
um you know from the point of view of this this kind of requisite variety um but now I've forgotten
your your actual question um which I did have an answer to well what can you remind me what the
actual question was sure sure it's so what these algorithms have in common with some of the things
that you and and chris fields have said about particles and things and other people apart
which is different from what happens in biology right if if if in biology I said look this cell
is exchanging information with that cell and it's making decisions the next question is excellent
what's the mechanism right like what it like show me show me the the explicit set of steps
by which the cell does that but but here we don't have that and and presumably you know when we get
down to particles and things we don't have that either so what's what's the status of these all
these amazing things that they're doing without a a mechanism to explicitly do it right and I'm
going to give you an answer which um comes from conversations with philosophers of maths people
like maxill ramsted that that question technically would be answered um by um appeal to what is a
mechanics so a mechanics is um for example the Bayesian mechanics of the free energy principle
or Lagrangian or classical mechanics under um certain um dynamics um you uh you know
the non-dissipative or conservative so or quantum mechanics um where um you can't uh
you you you you have to focus exclusively on the on the dissipative uh dynamics so the mechanics
is a description of the realization of something um where the thing usually conforms to a principle
of least action so this is where this is a sort of deflationary answer to your question that the
mechanics in and of itself is an emergent property of a variational principle of least action that
can be cast in gate theoretic terms or or in terms of things like maximum entropy principles
so there are principles um that just describe the shape the spacetime shape of our world
these um give rise to and usually you know you can usually reduce all physics principles to
principles of least action the you know the straight line the path of least effort um
and once you've written written down your principle as a principle um of least action
then the particular functional form of the system to which that principle applies
then gives you a mechanics and then that now um acquires a teleology in conversation
but only in conversation you don't need the mechanics mechanics does not engineer anything
it is just an expression of the principle of least action um so very much in the spirit
of basic mechanics are saying before the free energy principle is just a description of things
that self-organize you may or may not want to then go and say oh well this self-organization
could be described teleologically as self-evidencing or active inference or decision making or basal
cognition or distributed intelligence you don't have to do that but it sometimes it can be very
useful when talking to somebody else about it to to to teleologically frame it like that and that
I think is your mission I think that's what you're bringing to the table in in the widest sense
you're saying that the mechanics of biotic self-organization have a certain technology
which is almost isomorphic to the same teleology of finding psychiatry or immunotherapy or climate
change and we just got to find the cross-cutting themes so the mechanics the mechanisms um
are really just teleological unpacking of of of the mechanics so in this instance I gave you
an example before that simply the algorithm to implement the algorithm which is probably a
series of Boolean operators would need certain inputs they need arguments and they need certain
outputs those are the the sensory and active states those those define now the Markov blanket
um then so you know taking the input is basically whatever my neighbor neighbors value
whatever I can see and what I can see is my neighbor's value and it has to be a neighbor I
can't see but you know the one for you know I can't see something along the way away so that
defines the input and the output is my particular number that's what I broadcast but I only broadcasted
in a loop in a local sense so when he's starting to express things in terms of a
a teleology of self-organization of the kind that people use in you know in the free energy
principle I think you're then you're quite licensed to say well this is just a description for example
of electrochemical signaling you know it there's a there's a magic here this is this is this is the
mechanism it just means that there has to be a local signal that reports or has some morphism
between my state and and you know everything that is not me and we find multiple instances of this
in in biology at different temperance spatial scales and you know the more you drill down the
more you actually specify what it is the more mechanistic you know it will become but you know
at the end of the day that's just the mechanics you're talking about so if you you know if you
wanted to describe self-organization of massive bodies that had no dissipative aspects to them
you know the motion of the planets for example if you if you take yourself back and pretend you're
Kepler you know what is the mechanics of motion of the heavenly bodies they're Lagrangian
conservative mechanics they inherit from the principle of least action that you know has
a relatively simple form before Einstein came along in terms of you know energy conservation
and you know and all of that is implicit in a path of least you know the least action least
action principle so that would be his kind of mechanics and you start to invent things like
gravity the mass and you know and talk about the teleology of massive bodies being attracted to
each other and that would be you know quite comfortably received as an intuitive mechanistic
explanation for the thing at hand which in this instance is the motion of heavenly bodies you
know I don't see there's any real problem from your point of view you've already told the story
you've already got the mechanics it's just you know a question of showing how universal this kind
of mechanics is and universal in the special context of local interactions and all the all
the consequences of having a principle or the principles that would apply to self-organizing
systems that out of equilibrium or non-equilibrium self-organization but it's self-organization
what kind of mechanics must be in play and then you can give particular exemplars and talk about
gravity or sell intercellular signaling or you know and the locality of that does that make sense
yeah yeah yeah and I guess Adam did you want to say anything before I because I've got another
okay so so this is kind of the the really kind of far out thing and feel free to tell me that this
is not a good analogy and you know kind of too too too far out but I was thinking I was thinking
about the analogy of us trying to analyze these algorithms and what it is that what what are the
causes of their behavior as we observe them and trying to analogize to in a in a you know biological
or maybe in a psychological slash psychiatric context where you have what you think is the
algorithm and maybe that's what your subject thinks is the cause of their actions and maybe
that's what you think is the cause of their actions but it turns out that there's this
underlying dynamic that is it's an extra goal that you didn't know because you couldn't you
didn't see it in the in the steps of the of the policies that they're supposedly following
and I wonder if this is a sort of really basal kind of I don't know I just just a really basal
kind of behavioral analysis that might be important for the larger and more complex systems in the
sense of finding underlying goals for for for complex behaviors that are not to be found
in the by enumerating the mechanisms that you know that are there which is
you know like it basically basically looking at the tendency to cluster
as a as a hidden motivation for their behavior if I can use this kind of
like psychological term what do you think about that is that is that silly or are there
oh no no I think that's you know exactly the application of these principles and the attendant
mechanics you know in terms of say voting dynamics or geopolitical
and or just a sped of information on the internet you know I think some really important deep
questions there and things you know why is it that almost inevitably whenever you look at some
ideological political or theological commitment everybody's 50-50 you know Trump versus Biden
Brexit versus stay you know wherever you look the only evolutionary stable strategy on
offer energy minimizing non-equilibrium steady state is you know usually a 50-50 but of course
that can be subdivided within what this this 50 percent there's another 50 percent in a
sort of self-similar way all the way down so there must be something generically
very important universal about that and I would imagine that's you know you will see that kind
of clustering it's interesting I didn't realize I didn't read it Kevin have to realize you had this
sort of that the actual order wins out at the end of the day so you get the increase and then the
decrease in clustering that is interesting I would be if I was a young man working as you
for you as a PhD student I'd like to put a bit of noise on the numbers and just see if you can
keep it alive and dynamic and see that clustering and that chimeric behavior look at its dynamics and
you know something in dynamical systems theory called frustration that you get in these
in these chimeric situations when you broke in detail bounds like this which may be a good
metaphor for the voting dynamics for example so I think it's a very sensible idea I'm just
reminded have you read that paper by Connor Hines and if not I'll send it to you so he was making
an analogy between Gibbs energy and free energy in terms of exchanging ideas as a model of
collective behavior and maybe you'll find some interesting ethological references to that
yeah interesting you know the thing about the thing about them being pulled apart at the end
so one way one of the things that we did was to ask okay how strong would this tendency to cluster
be if you if you didn't have this super overlying you know basic physics of this world that eventually
is just going to yank you apart and and one way to do that is to allow repeat numbers so if I allow
repeat numbers then you can have a long run of fives and the first half of them could be one
algotype the second half could be the other and the actual sorting algorithm would be perfectly
happy to keep them as is because the fives are between the four yeah so so we did that and and
when you do that you find out that yeah if you let them do that it actually goes higher the the
tendency to cluster is actually higher that you know so there's this like competing you know
and and again looking at it it almost it almost provides a very minimal model of of the existential
sort of I don't know way of life facing living systems right that the physics of the world are
like you're trying to grind you down but but in the meantime you can do some interesting things that
are not incompatible with them there's no magic I mean it does the algorithm there's no you know
there's no errors but but but yet not quite what what the you know what what the end goal
is going to be according to the actual physics of the system yeah

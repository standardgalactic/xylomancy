This is a talk about agency at the very bottom and introduction to universal properties and
adjoint functors. So it's about mathematical agency. First of all, just a little bit about me.
My background is in economics, but I moved to abstract math to try to solve some problems in
economics that economics was not able to help me solve. We didn't have the tools and ended up
finding out about Mike's work through a YouTube video. Like a lot of people, I sent him an email
and we ended up setting this up. I should also mention that this talk is designed for people
who have little to no background with category theory or abstract math in general. I know a
number of you do have experience with such things. So the mathematical portion of this
may move relatively slowly for you, but that should hopefully make it easy to focus on the
unusual perspective. And if you don't have any background with such things, it should be easy
to follow along. Also, we have a lot to get through. So I'll appreciate it if people can
hold questions unless something I've said just doesn't make logical sense to you,
in which case, please let me know. But otherwise, we'll try to get through as much of this as we can.
So first of all, why talk about mathematical agency? There's a few reasons. One is that it could
settle the question of where the line is between agentic and non-agentic things in our universe.
There is no line because it's all math. Also, if you think math can be agentic, it's suddenly
pretty easy to think that cells can be agentic. So that stuff is pretty easy to swallow.
In general, new perspectives means new questions and new interpretations of old results. So that
could be useful. And finally, and what's really going to be the focus of this talk is that math
looks agentic, so we should call it agentic. So here's what we're going to talk about. We're
going to talk about four concepts here in this talk. The first is scarcity. This is a concept
from economics that I'm going to argue is the necessary conditions for agency. And I'm going
to show you that mathematics has scarcity. And so it meets the necessary conditions for agency.
So it's plausible that it is, in fact, agentic. Then we're going to talk about two agentic things
in math, which are universal properties and adjoint functors. Universal properties relate to
optimization, which is obviously important for agents. And adjoint functors relate to optimal
reconstruction and least action, which are also very agentic things. And finally, time permitting,
which it may or may not be, we'll talk a little bit about timeless choice. So math doesn't move,
so people can do stuff through time, but math can't. So how can math actually do anything
like an agent? We'll talk about that if we can. And so the hypothesis lurking in the background,
which I can't provide evidence for, but it's just sort of the idea that maybe can inspire some
new questions, is that life and physical structures in general that have any agentic properties
are able to do so because they exploit the affordances given to them by math,
including basic agentic properties. So just like if you're evolving a triangle and you evolve the
first two angles, you get the third angle for free, maybe something similar is happening with
agency where you get a lot of stuff for free. So I want to start with scarcity. We're going to talk
about how scarcity is the necessary conditions for agency and see what that looks like in
mathematics. So let's start with agency. If you see something agentic in the world, you see something
you want to call agentic, what's causing you to do that? What must be present for that to make
sense as a modeling decision? There's undoubtedly a number of perspectives on this. The perspective
I'm going to take is that it's all about choice. If you see something agentic or that you model as
agentic, what it's probably doing is making something that you are calling a choice. And so
you think of it as an agent. So where does choice come from? Why do we call things choices?
The answer is it comes from trade-offs. Trade-offs exist. If you do one thing, you can't do another
thing. If you go left, you can't go right. So you have to make choices. And then to make those
choices, you have to be an agent. So choice is necessary for agency. Trade-offs is necessary
for choice. What's necessary for trade-offs? I'm going to argue this bottoms out at scarcity.
Scarcity, I'm going to define as the necessary conditions under which trade-offs occur. That's
quite vague at the moment. We're going to be more specific, but this is the sort of pyramid here.
Agency ultimately requires scarcity to make sense as a modeling decision.
So what we're going to do here is come up with a more specific definition of scarcity.
We're going to look at some examples of trade-offs first in economics and then in math.
We're going to see how economists think about scarcity, what the limitations of that are,
and then come up with a generalization that fits mathematics. So here is the standard economic
picture of scarcity. Forgive me, I'm not an artist. So this is the best I can do.
On the left here, we have four sandwiches. On the right, we have five hungry people.
And you can see the issue. We can't feed everyone a full sandwich. So we're forced to make trade-offs.
We could feed four people a full sandwich and one person goes hungry, or we could cut up the
sandwiches in some way and feed everybody, but not everybody's going to get to eat a full sandwich.
So we're forced to make trade-offs. Well, I've argued that scarcity is the necessary conditions
for trade-offs. So the existence of trade-offs means there is scarcity here. So what is the
scarcity? What makes scarcity be happening here? Well, economists define it as finite physical
resources. The core of the issue is that we have a finite amount of sandwiches. We have four of them
instead of infinity. If we had an infinite number of sandwiches, we'd be able to feed everybody.
So this is how economists think about scarcity, finite physical resources.
The problem with this definition is that math does not have finite physical resources so far as
I know, but it still has trade-offs. So there's a problem with defining scarcity in terms of
finite physical resources, which is it can't define trade-offs when we're dealing with infinite stuff.
So let's look at a trade-off in math. So this should be very familiar. We're just counting
up the natural number line, 1, 2, 3, 4, 5, etc. As we do so, we're constantly switching back and
forth between odd and even numbers. So we're constantly making trade-offs odd number or even
number. We never have both. And this is not just a superficial distinction. If you have an even
number and you divide it by two, you get a whole number. If you have an odd number and divide it
by two, you get a rational number. So there's different properties. So this trade-off is really
meaningful, which means there must be scarcity because trade-offs imply scarcity because scarcity
is necessary for trade-offs. But I don't see anything finite in this situation. It looks like
we have an infinite number of natural numbers. So where is the scarcity coming from if not the
finiteness of our system? Here's just another example of trade-offs in mathematics. I want to
show you that this is ubiquitous. You're always making trade-offs in mathematics. Real numbers
versus complex numbers. On the left here, we have real numbers. Real numbers have a very useful
property, which is they can be totally ordered, meaning for any two real numbers, I can always
tell you which one is bigger. We always know which one is bigger. What they can't do is define the
square root of negative one. If we move to the complex numbers, we can define the square root
of negative one, but we can't totally order our numbers anymore because they're two-dimensional.
So one complex number could be bigger on the real dimension, but smaller along the imaginary
dimension or vice versa. So we gain the ability to define the square root of negative one. We lose
the ability to totally order our numbers. So again, this is a trade-off. So we have trade-offs,
and therefore we must have scarcity. But again, there's no finite physical resource. We don't
have a finite amount of numbers. We don't even have a finite amount of fields. So what's going on?
To understand agency and mathematics, we need to solve this problem because I'm arguing that scarcity
is necessary for agency. So we need to understand what scarcity is in mathematics. So here's what
we have so far. Economics tells us scarcity is about finite resources. Math isn't finite, and yet
it has scarcity because it has trade-offs. So where does scarcity come from? Sorry, go ahead.
No? Okay. So to solve this little riddle, what we're going to do is we're going to look at a
situation in math where math does not make trade-offs, and we're going to see the difference.
What's going on when math doesn't make trade-offs versus when math does make trade-offs,
and that's going to tell us what scarcity is. And then that'll let us make the case for math
being egentic because we'll see that math has scarcity and therefore meets the necessary conditions
for agency. So here is the situation in math where you don't make trade-offs, and that situation is
contradiction. So there's a famous logical principle called the principle of explosion,
which says that if we contradict ourselves, we can derive anything. So if we assume p and not p,
where p is any proposition, so I'm saying p is true and not true, hence contradicting myself,
then q follows where q is anything. So if I am Benjamin and I am not Benjamin, hence contradicting
myself, I can derive that John F. Kennedy is alive and living on the moon with his best friend Santa
Claus. That follows from the contradiction. So in particular, contradiction lets us avoid trade-offs.
We could have an odd number that we divide by two and get a whole number, or we could totally order
the complex numbers. We can do anything because we don't have to worry if it makes sense because we
can just contradict ourselves. So scarcity is what's necessary for trade-offs, but when we
contradict ourselves, we can avoid making trade-offs. So it makes sense to associate a lack of scarcity
with contradiction. That is, in fact, what I'm going to argue is that we have scarcity when and
only when we do not contradict ourselves, which is to say when we are being logically consistent.
So scarcity in math relates to logical consistency in math. And that might seem weird, but I think
when you look at this picture, it becomes quite clear because what happens when you commit yourself
to logical consistency, when you say I am not going to contradict myself, you commit yourself
to making choices. You say I have to pick. Do I want to assume p or do I want to assume not p?
Both assumptions are fine. You can start deriving theorems in one direction, then you can go back
and make the other assumption and derive theorems in the other direction, but you just can't do both
simultaneously. You have to pick one. So scarcity, this or that, logical consistency, p or not p,
to me, they feel like the same concept. So what I'm saying here is that agency requires choice,
requires trade-offs, requires scarcity. Math has scarcity because it has logical consistency.
That's what I'm associating with scarcity. So as long as we're being consistent,
we're going to have scarcity and therefore meet the necessary conditions for being agentic.
So now what we're going to do is look at some actual examples of agentic things in math,
which are universal properties and adjoint functors. So we've set the table for math
maybe being agentic because it meets the necessary conditions. Now we're going to see,
does it actually do agentic things? Now I'm going to argue that it does.
So starting with universal properties, we have, this is an important concept in math that relates
to a few agentic ideas. First of all, optimization, which is obviously very important for agency.
We're also going to see how it relates to a concept of top-down, goal-driven design or
control that you can do in math with universal properties. We'll also see how universal properties
constitute a solution type that we can remap onto new mathematical body types.
So just like we can think of an organism as being optimal for its environment,
we'll see how we can talk about a mathematical structure as being optimal for its environment
via a universal property. So what is a universal property? A universal property is a way of defining
a mathematical object by how we use it rather than what it's made of. So instead of looking inside
of it to see all the parts and pieces, we're just going to ask, what are we doing with this thing
and how well does it do the job? We're going to consider a collection of all mathematical
objects that do the same job. And we're going to ask, what's the best at doing that job? The
universal property is going to be a special relationship that the optimal mathematical
object has with all other objects that says, I am better than you. So it's universal because
it's talking to every other thing in the environment and it's saying the same message to
each one, a universal message for every object that says, I'm better than you at this. So,
back off and let me do my thing. So here's a very simple example of a universal property. We
actually have two of them in this picture. So we have an ordered set. This is the minimal element.
This is the maximum element. So if we consider the job here, we could ask, we want a thing that's
bigger or equal to everything else. And if we consider this top element here, it's the best
at doing that job because it's bigger than this thing. It's bigger than this thing. And then
it's equal to itself. So it sends the same message to every other object in this environment
that says, yes, I am bigger than or equal to you. So that's the universal property. It's the same
singular relationship with everything else, including itself. And then the bottom element,
similarly, it is optimal, the task of being smaller than everything else. And it sends the
same message, I'm smaller than you, I'm smaller than you, and then I'm less than or equal to
myself, the same message for everything. That's a simple example of a universal property.
Now, let's look at a more interesting example of a universal property, which is the Cartesian
product. We're going to see that we can define the Cartesian product in terms of what it does
instead of what's inside of it. We'll see how we can compare the Cartesian product to other
objects that do the same job as it. And we'll see that that allows it to satisfy the universal
property. And then we'll see how this leads to some interesting, agentic things that you can
end up doing with this use-based definition of the Cartesian product. So let's start with the
internal view of the Cartesian products. This is the standard way to define the Cartesian product
that every math student is going to learn. So we have two sets, A and B. The Cartesian product
is the set written A times B, of all ordered pairs with form A comma B, where A comes from big A
and B comes from the set B. So very simple example, let's say A is a two element set, A1 comma A2,
and B is the two elements set B1 comma B2, then A times B is just every way of pairing an element
of A with an element of B. So we have A1 paired with B1, A1 paired with B2, A2 paired with B1,
A2 paired with B2. So this is like very basic like intro like proofs for like any undergraduate
math student who's going to learn this definition. But we have another way of defining the product,
which is based on how we're using it instead of what's inside of it. So this is an object denoted A
times B along with projection functions P1 and P2 that map to A and map to B respectively. So these
functions are part of the definition because we're considering it in terms of how we use it,
and what we do with sets is we make functions. So our use is based on functions.
To see what these functions do, a very simple example is it takes our ordered pair A1 comma B2
as just an arbitrary, excuse me, arbitrary choice of input. And what P1 is going to do is it's going
to preserve the A unit in A. So our A unit here is A1. So we map to A1 and A, and then P2 similarly
preserves the B unit mapping us to B2 and B. So just to get a picture of why this is in fact useful,
this tells us what we're using the Cartesian product for. Anytime you've done some graphing,
you've been using the Cartesian product, whether you realize it or not. So this is our A on our x
axis, B on our y axis, and here's the ordered pair A1 comma B2. Well, our projection functions allow
us to remember that this product, it's A1 units along this axis and B2 units along this axis.
If you were to try to understand this ordered pair without these projection functions, it wouldn't
mean anything. It's these projection functions that take this ordered pair, this point, this dot,
and actually turn it into something meaningful by relating it back to B and A. In particular,
what you could say is it remembers A and B. So we've got this ordered pair. We're like,
wait, what am I doing with this? Oh, yeah, I've got A and I've got B. I've got this A1 unit and
this B2 unit. That's what's going on. So what's interesting about this perspective on products
is that it lets us ask a question that we couldn't have asked before when we were just looking inside
things. There's a new question, which is if that a product of sets is just a set combined with functions
to A and B, doesn't any set qualify? Because as long as A and B are non-empty, then any set
is going to have functions to them. So let's say PC, that's like a candidate for the product,
a product candidate. It's just some arbitrary set. We don't know. Well, it doesn't have a
function F to A and a function G to B. The answer is kind of, yeah, it does. But what we're going
to see is that it's not necessarily optimal. A times B is going to end up being optimal. This
other candidate may not be optimal. And we're going to express that with a universal property.
So let's look at a specific candidate for the product, a product of three sets,
A times B times C, which is going to have elements of the form A, B, C. So this is an ordered triple
now instead of an ordered pair, where A comes from A, B comes from B, and then C comes from C now.
So we have these functions, let's just call them F and G to A and B just like before. And it's going
to do what we're going to expect. So we're going to take an input, let's say A1, B2, C1. What F is
going to do is it's going to preserve the A unit just like before A1 and A. And what G is going
to do is preserve the B unit, B2 and B just like before. And then the C stuff ends up being irrelevant.
So you can see that we are doing the same job we did before of projecting to A and projecting to B
and remembering those sets. So we want to now, intuitively, we understand A times B times C
is inefficient compared to A times B because it clearly has something extra. But how do we
actually show that it is? What we're going to do is compare A times B times C to A times B.
This comparison is going to end up being our universal property and it's going to show us
that A times B times C is inefficient. So the way we compare sets is with a function because that's
how you relate sets to each other is with functions. We're going to call that function H.
And to understand what's going on, we're going to follow this diagram very carefully. So I'm going
to move through this kind of slowly. Bear with me. So we've got this input, let's just say it's A1,
B2, C1. We're going to define H to preserve the A unit and B units. We're going to map from our
order triple in A times B times C to an ordered pair in A times B. We're going to keep the same A
unit, A1, keep the same B unit, B2, and then we'll just delete the C unit because it's extraneous.
So we've gone down from H to here. And now we could go either left or right. Let's go left.
So P1 is going to take us to A1 and A from this ordered pair. Now, what's interesting is that
F does the same thing. So H then P1 starts over here and ends up over here. F does the same thing.
It starts over here and ends over here. Well, two functions, if they start in the same place
and end in the same place, they're equal. So F is equal to H followed by P1. And similarly,
G is equal to H followed by P2. So what this tells us in a very precise way is that if we take
A times B times C and we lock off the extraneous part, we end up doing the same work. So we can
take A times B times C, cut off a piece of it and still do the same job as before. That tells
us that A times B times C had something extraneous and was therefore inefficient.
And what's particularly noteworthy is this is the only way we can compare them to have them
do the same job. H is forced by how we are using A times B and A times B times C. We're using them
to reconstruct A and reconstruct B. And it turns out that if we want to use A times B times C to
reconstruct A and B in the same way as A times B, we are forced to effectively turn it into A times
B to do the job. So there's sort of top-down design here. I say I've got this job, reconstruct A,
reconstruct B. I have to turn things into A times B and there's a forced way to do so given my
alternative candidate. I started here. This isn't perfect. I end up here. This is perfect.
What makes this a universal property is that this same relationship that A times B has with A times
B times C, this unique factorization where A times B times C gets turned into A times B to do the
job, it has with every other candidate for the product. So I've given them stupid names like
Bibbidi-Boo, Bibbidi-Bib. It doesn't matter what they're called. Just every other set that could
do the job. This is infinite going upward because there's an infinite number of candidates. Each of
them comes with their projections to A. Each of them comes with their projections to B. But each
of them gets mapped uniquely to A times B where it ends up saying, okay, yeah, A times B is actually
better than me at this job. It's the same relationship for each one. A unique way of saying,
you want to be me to do this job properly. So effectively, all of these other candidates are
sort of cheating off A times B's homework. A times B is in fact optimal by this universal property.
Something I just think is interesting as someone with an economics background is this actually
feels very similar to a preference order, which is obviously a very agentic concept. So from an
active inference perspective, if I want to have like a favorite ice cream flavor, what I'm using
that to do is to reconstruct my anticipated flavor and anticipated texture experience.
My favorite flavor of ice cream is going to be the one that's best at reproducing my anticipations.
The unique factoring could be like selling the strawberry ice cream and using the proceeds to
buy chocolate ice cream. So universal properties, it looks very much like having a favorite way of
doing something, which is clearly an agentic concept. So here are the principles of universal
properties. So universal properties are the most efficient solution to a problem that specifically
exists in math. So we're not using this to solve a physical problem or a human problem. We're using
it to solve a mathematical problem that you can define in purely mathematical terms. And yet,
despite the fact that it's just math, you have an efficient, we have a favorite way of doing things.
We also have a way of defining top down goal driven control. If I say, I've got some other
thing here and I want to make it the best way of doing things, I don't need to look inside of it.
I just need to define this unique mapping to A times B. And similarly, we can test optimality.
So we don't just have to like assume or guess optimality. We can test it by looking at these
kinds of mappings and asking, does it factor through A times B? Yes or no? That's going to tell
us about optimality. And now that we have this picture of the universal property of the Cartesian
product, we're going to see a couple of very cool, agentic things that you can only do with this
definition and not with the internal view of Cartesian product. So the first thing we can do
is we can find unexpected competencies that we could only discover by probing objects.
So let's consider this Cartesian product of three sets, A times B times C, but this time we're going
to have C be a one element set consisting of just a little C here, no C1, no C2, just a little C.
So it's A times B times just the set containing C. And it's the set of four ordered triples. So we've
ordered triples, not ordered pairs, but there's only four of them. So we do see that it is isomorphic
to the A times B that we saw several slides ago, which had just four ordered pairs. Now if you were
to ask a math student, an undergraduate math student, you know, is this the Cartesian product A
times B? They would say no, because the Cartesian product A times B consists of ordered pairs,
but this is ordered triples. But by testing it, instead of just assuming we know what it can do,
we can see that it does the job of the Cartesian product just as well as the Cartesian product
does. And we can see that because we can factor A times B through A times B times C here to make
everything be equal just like before. So what this is going to do is we're going to take an ordered
pair, let's say A2 comma B1, our function H in this case will map it to the order triple A2 comma B1
comma C. And then we're going to end up at A2 regardless of where we go or regardless of which
path we choose, or we'll end up at B1 regardless of which path we choose on the right. So this
does in fact show that A times B times C here is the Cartesian product. It does the job of the
Cartesian product optimally, even though it doesn't look like it should. Now if you're wondering,
well, isn't this also optimal? That's because it factors back the other way through H prime,
we can sort of take either path. So these are isomorphic. So they do the same job because
in a sense we can treat them as the same. There's a unique isomorphism specifically.
We have one and only one way of transforming one into the other so that they do the same job.
So we're indifferent which one we choose. So C isn't helping us, but it's not hurting us either.
It's not costing anything. And we would have only learned that by actually testing the system with
functions instead of just assuming by looking inside of it at the parts and pieces. One other
cool thing we can do with the universal property of the product is we can take that solution type
that it constitutes and map it onto a new body type. So instead of looking at ordinary sets,
let's look at an ordered set. So we've got our set consisting of x and y and z and these
relationships mean x is less than or equal to y and x is less than or equal to z. You could think
of this like biologically as like descendancy. So y and z are the parents and x is their child.
So what if x has a child? So let's say w descends from x and also from y and z,
then x is the greatest lower bound. It's the biggest thing that is smaller than y and z.
This is a very important concept in math. You're constantly talking about bounds and greatest
lower bounds when you're doing mathematics. So we've defined a very important concept here.
Now, how does this relate to universal properties? Well, let's take this diagram and turn it upside
down. And you can see it's the exact same diagram as the universal property of the product of sets.
I haven't labeled the arrows here because the labeling doesn't matter. But otherwise,
it's the exact same diagram. The forcing here is given by the fact that in an ordered set,
there's only one way to be a descendant of something else. You can't have like two different
kinds of child. There's just one way to be a child. So if we can take our universal property
of the product and we can, you know, when we're trying to solve like what is the greatest lower
bound, what does that mean? We don't need to, you know, reinvent the wheel. We can import our
solution type from another body type from just plain sets into ordered sets. And we find out that
the greatest lower bound is in fact the product in partially ordered sets. So what I take from
this is that math doesn't make specific solutions to specific environments. It makes problem solving
morphisms where morphisms is a generic term for these kinds of arrows. So that takes us through
the end of universal properties. Appreciate help bearing through that. I know it can be kind of
dense and tedious. And now we're going to move on to adjoint functors. We'll try to get through
this and see if we have time for timeless agency as well. So adjoint functors are a very important
concept in mathematics that relates to optimal reconstruction. So if you're an organism trying
to survive in your environment, you need to have an internal model of your environment. So you're
recreating your environment inside yourself, but you have limited sense data and limited computing
power. So you can't perfectly recreate your environment. Instead, you want to optimally
recreate your environment. And adjoint functors also relate to being lazy. And we'll see that being
lazy or least action ends up being related to optimality. So you want to be lazy, you want to
be optimal. We'll see how those end up being highly related to each other. So what is an adjoint
functor? An adjoint functor is a pair of relationships, one of which forgets information
and the other reconstructs what was forgotten. Once you forget something, you can't necessarily
remember it perfectly. So we settle for optimally reconstructing instead. We'll work through a specific
example of building something lazily. We'll see that building something lazily lets us build it
optimally as well. So we're going to build something called a monoid. So a monoid is a fancy word for
something very familiar, which is a set combined with an operation. And operation is a way of
combining two elements of the set to create another element of the set. And in particular,
what's important is a given set can have many operations. So let's take our set to be the
natural numbers. Well, one, combine them to create nine, or we can have multiplication take two and
seven, combine them to make 14, or we can have maximum as the operation a bit different from a
normal arithmetic operation, but we can define two and seven, we combine them to create seven as the
maximum of those two. So we're combining things to create things. So that's an operation, or we
could even do something like concatenation and take two and seven and combine them to make 27,
which is kind of silly. We can use zero as our identity. So this is a monoid. It's a very familiar
concept, a set combined with an operation. Excuse me, let me drink some water.
So now we're going to look at using this idea of a monoid. We're going to ask,
how do we turn a monoid into a set in a lazy way? How do we turn a set into a monoid in a lazy way?
We're going to do this with what are called free and forgetful functors. So a functor is just a
fancy word for a generalization of a function. Just think of it as a function for all intents
and purposes. That's going to be okay for this talk. So our forgetful functor, we're going to have
our monoid. Our monoid is going to be our set of natural numbers along with some operation. I'm
not going to say what the operation is, some arbitrary operation. It doesn't matter, which
we'll just denote with an asterisk. Our forgetful functor asks, what's the laziest way to turn
our monoid into just a set, just a regular set with no operation? Well, the easiest way to do
that is just to forget about the operation, just throw the operation in the trash and just keep
the set. So take the natural numbers combined with the operation and turn it into just the
natural numbers. So that's the easy part. That's not too interesting. Now let's ask, how do we
reconstruct our monoid? So we just have our natural numbers. We threw our operation away. We don't
remember what it was, but we do want to turn this back into a monoid. What's the laziest way we can
do that? And that's going to be with our free functor. So we're going to construct what's called a
free or lazy monoid on the natural numbers. The way we're going to do that is we're going to choose
a generating set, which we're going to choose to be just the number one. Now, by definition of a
monoid, I have to be able to combine one with itself to create something because a monoid says
I combine numbers to create other numbers. I'm going to use the plus symbol to denote the operation
because it will in fact turn out to be addition, but we don't know that yet. So this is just an
arbitrary choice of notation right now. So I have to write down by definition. I'm not thinking I'm
being lazy, but just I'm nevertheless compelled to write down one plus one equals something.
Well, I'm going to be as lazy as I can. I don't want to turn on my brain. So I'm going to write
one plus one equals random squiggle. And then because random squiggles in the set, I have to
write down one plus random squiggle, that's going to equal some other random squiggle because I am
being lazy. I don't want to think at all. I'm just moving my hand around randomly, totally brain dead.
Well, the result of this is I'm going to create some set called M for monoid, which is going to
look like zero one random thing, which is one bigger than one random thing, which is one bigger
than this squiggle. That's one bigger than this squiggle. That's one bigger than this going out
infinitely because I'm never going to write two random squiggles the same way. The odds of that
is zero. So I'm going to generate an infinite amount of random squiggles that are all one bigger
than the previous. Well, if we compare that to our set of natural numbers, it's clearly the same
because I got zero, one, two, three, four, five. It's the same set. So it could have happened by
random chance that when I'm writing these random squiggles, maybe I wrote two by accident, I wrote
three by sheer chance, et cetera. So I could have generated the natural numbers if I just got in
luckier with my choice of squiggles. Well, who cares what the squiggles are. So you can in fact
say that this monoid is the natural numbers. And so that's what we've done. We've actually generated
the natural numbers under addition by being as lazy as possible. So this is our free monoid,
the natural numbers under addition, we generate it by starting with one, doing one plus one plus
one plus one, totally brain dead takes no thought. Let's contrast this with an expensive model
monoid. And you'll see how lazy the free monoid is. So we'll take our generating set to be one
again, and we'll do some familiar relationships. So we'll have zero plus one is one, one plus one is
two, two plus one is three. But now we're going to establish a non-trivial relationship. We're
going to turn our brain on and actually think about something and make a deliberate choice. We're
going to say three plus one is zero. Now that might seem weird, but this is totally legitimate
because all we have to do with a monoid is combine numbers to create other numbers. Well,
three plus one, if I combine them to create zero, I've satisfied the rules of a monoid. So it looks
weird, but it's actually totally legitimate. So now what else do we get? Well, three plus two
is three plus one plus one, three plus one is zero. So three plus two ends up being one,
then like three plus three is two, and two plus two is zero, et cetera. So all these relationships
just follow from our choice up here. And what we're doing is we're counting, but we're counting
in a loop instead of a line. So with the regular natural numbers, you just go one, two, three,
four, five, et cetera, extending outward with our expensive monoid, we're going to loop zero, one,
two, three, zero, one, two, three, zero, one, two, three, et cetera. Now this might look weird,
but it's actually familiar because this is how we count on a clock. So if I tell you like nine
o'clock plus five o'clock equals two o'clock, that makes sense to you. And something like three
plus two equals one can make sense as well. Now we're using zero instead of 12 here. So imagine
a clock that goes zero, one, two, three, zero, one, two, three, that's what we're doing with our
expensive monoid. So it looks weird, but you all know how to count this way. It's just counting
with the clock. So now what we want to do is do what we did before with our universal property
of the product and compare our free monoid to our expensive monoid. So we built one in a lazy way.
We built one with a bit more effort. We want to show that the lazy way of doing things is the
optimal way of doing things. We're going to do so with a function just like before, but this needs
to be a function with a little more structure. It needs to preserve the structure. It needs to
preserve the use of the one monoid in the other monoid. What we're doing with both of these monoids
is we are counting by one. That's what we saw. We're generating by just adding one to itself over
and over. So the function is going to preserve the counting structure as long as each counts by one
in the same way. And so the way this is going to work is that if we map a number in the free monoid
to a number in the expensive monoid, the next number in the free monoid gets mapped to the next
number in the expensive monoid. So it's just going to match up with each other counting by one.
So here is a diagram that shows this. So here's our comparison, our function. So we've got all
of our natural numbers over here, 0, 1, 2, 3, et cetera. And then we're going to have our
expensive natural numbers, which is just going to be 0, 1, 2, 3 repeating. What is our function?
What's our mapping going to look like? Well, the first few are obvious. 0 maps to 0. 1 maps to 1.
2 maps to 2. 3 maps to 3. What is 4 mapped to? Well, we don't have 4 over here. We can't just
map it to 4. So what are we going to do? Well, 4 is 1 more than 3. We've mapped 3 to 3. So we should
map 4 to 1 more than 3 over here, which is 0. So that's what we're going to do. So 4 maps to 0.
And then 5 is 1 more than 4. 1 is 1 more than 0. So 5 gets mapped to 1 because 4 gets mapped to 0.
And so it just continues outward like this. So this is our comparison. This is our choice of function.
And we're going to see a really interesting picture that's going to show us the laziness
and the optimality of one monoid versus the other. So here is that picture. So here is our expensive
monoid. The way a monoid is sort of formally defined is you have a dummy object,
which I've denoted with this gray circle here. It doesn't really mean anything. It's just there
because it has to be there. And then the numbers are going to be relationships that extend outward
from the dummy objects. We're going to have 0, 1, 2, 3, all extending outward. So we're going to
start with 0. That's just there by default. So then we have 1. That's green here by this legend.
1 plus 1 is 2. 2 plus 1 is 3. So the image I want you to have in your mind is like ripples
extending outward from a stone dropped in water. We dropped this dummy object in the water and
our numbers are ripples that extend outward from it. Now what happens is a bit unnatural with this
rippling motion is it actually collapses back to 0 again. So it's like we've done some extra work
with our water setup. We have some special technology that pulls everything back to 0
and then it ripples outward again. So it's rippling, but in a somewhat unnatural way.
It's not how we would expect physics to work. Let's compare this to our free monoid where
we're going to have things rippling outward again from the dummy object. This time what our
legends are going to show is the colors are mapping to these colors over here. So red maps to this,
green maps to this, blue to this, purple to this. So we start with 0 again. 0 plus 1 is 1. 1 plus 1
is 2. 2 plus 1 is 3. Now we don't collapse. We go forward. So 4, 3 plus 1 is 4. 4 is red because
it maps to 1. 4 plus 1 is 5. That maps to 2, etc. So we keep mapping outward. And what you see is
that this actually behaves like a ripple does in the real world. If you had an infinite sized pool,
you dropped a stone in it. And if there were no entropy, you would just get it rippling outward
infinitely according to the initial pattern. That's what we expect least action to look like in
the physical world. And that's what doing things lazily looks like in the mathematical world as
well. So this is what our mapping looks like, visualized. And this does in fact tell us that
the free monoid is optimal versus the expensive monoid because the free monoid contains the
expensive monoid. So what can we do with the free monoid? Well, we can count linearly 1, 2,
3, 4, 5, 6, 7, etc. But we can also count in a loop because that's what this pattern is giving us.
We can see the looping pattern. We can count with loops with this free monoid. So it can do the job
that the expensive monoid can. But the expensive monoid cannot do the job that the free monoid can
because it can only count in a loop. It doesn't count linearly. So the free monoid is like just
able to do more tasks than the expensive monoid does. It contains it. And so even though we built
it lazily, it actually ends up being optimal. So what have we learned about adjoint functors?
Adjoint functors are lazy ways of translating between structures. So we have different mathematical
structures like monoids versus sets. Adjoint functors allow us to map between them in lazy ways
that create or destroy information. We do things in a least action way. Adjoint functors end up
giving a sort of universal solution to the problem. So like if we want to count by one,
that's our problem. There's many ways we could do it. But what's like the most sort of universal
or formulaic way of doing that? Well, it's this way of doing that because we can do looping if we
want to, but we also have linearity which these other expensive versions don't have. Something
that's really interesting about adjoint functors, just like in the physical world, there's so many
things that you can define in terms of least action. Well, the same thing works in math. Adjoint
functors arise everywhere as a common saying in math. Many important mathematical things turn out
to be adjoint functors. They turn out to be lazy ways of doing things. And I don't know much biology,
but it just kind of seems to me that free monoids, I kind of like the DNA of monoids to give you the
relational structure to do all different kinds of counting, linear counting, looping counting,
all kinds of different patterns. So we have the relational hardware to build any specific
monoid if we want to. So that actually brings me to the conclusion of the math portion of this,
which is what we've seen so far, which is that we have trade-offs in math and we have scarcity
therefore in math, which happens because of logical consistency, not physical limitations. So as soon
as we can commit ourselves to being consistent, we end up committing ourselves to making trade-offs,
which is what we need to see agency. And what's fascinating about this is that the agency of
math, it's not something that humans just use to solve our problems in the physical world,
we can define problems in the mathematical world, like how do you cast projections to A and B,
or how do you count by one? And we can find optimal solutions to those problems that are
optimal for math itself, not for humans, but just in purely mathematical terms, it ends up being
optimal. And optimality often ends up relating to laziness, we can be formulaic and just turn
our brains off and do things in the most obvious way. So when you're doing math, like as a mathematician,
it's often very useful to use agentic concepts. So just like you can settle, like debate, it's
about free will, like, well, it's useful to think of myself and making choices of like,
you know, buying something in a restaurant. Similarly, if you're doing math, it ends up just
being very useful to be like, if I was this set here, how would I want to solve this problem?
As someone who, you know, is currently trying to learn math, I do find that perspective useful.
So math ends up doing agentic things, behaving like an agent. This isn't necessarily like hard
proof that math is agentic, but it does show that thinking of math in an agentic way seems to work.
There's no problem with it. So why not try thinking of it that way? Now that brings me to
timeless agency. And I see that it's been about 40 minutes, which means I probably talked way too
fast. So let me actually pause here before we move on to timeless agency and just see if there are
any questions about just, you know, what's going on? What am I saying? If anyone just wants to pause
here, if there's just something that really doesn't make sense or is niggling at them,
please feel free to ask. And otherwise, I'll move on to timeless agency.
Seems like we're good so far. So thanks everyone for bearing with me through this. Let's move on
to timeless agency. So there's an obvious problem with thinking about math as agentic, which isn't
that math doesn't do anything. Anything that math does, a human does, like we have to write down
two plus two equals four, the math doesn't write itself. So humans can move around,
math just holds still. So how can we talk about agency if we don't have time? The argument I'm
going to make is that we can think about agency in terms of predictions, like free energy minimizations,
all about making predictions. What a prediction does is something that preserves structure through
time. Like if I'm predicting the weather, I've got some structure, some model, some data, and I
can predict what it looks like in the future as we preserve that structure. And so what I'm going
to ask is, well, what if we just drop the time part? What if we just talk about structure preservation
without time? Because that's what math is all about. It's all about structure preservation.
So much of math is just about structure preserving maps from one object to another. So what if we
just drop the time portion of that and just talk about structure preservation, then we can talk
about math as being making predictions, which seems to be the heart of agency. So what does
timeless prediction look like? Very simple example. Let's say that you have two apples,
and I tell you that I'm going to bring you three more apples. Well, you're going to predict that
you're going to have five apples. But the core, now this is a prediction that takes place in time,
right? You're predicting what you're going to have in the future. But the core of this prediction,
no pun intended, is two plus three equals five, which does not have a temporal aspect to it. It's
just one equation. It's not like the two and three happen first, and then you have five. It's just
one thing. So we're using timeless things to make predictions through time. Two plus three equals five
feels like a prediction kind of thing. It's just there's no time element to it. So let's look at
an example we've seen before, free monoid versus the expensive monoid. Can we use this free monoid
to make predictions about the expensive monoid to know things? What a prediction means is we're
going to look at the free monoid, and we're going to use it to know things about the expensive
monoid. That's what a prediction is using one object to gain knowledge about another object.
Well, yes, we can make this prediction, which is with this mapping we've seen before. We can use
this mapping to know how this is going to behave without actually having to look at the expensive
monoid. We can use our free monoid to make predictions about it. So this tells us intuitively,
prediction is all about structure preservation. We have a structure preserving map from our free
monoid to our expensive monoid, and that's what allows us to make predictions without time. There's
no time happening in this prediction. We're not making predictions about the future. We're just
making predictions in the timeless mathematical world with structure preserving maps.
So prediction is structure preservation. This is my short argument for math being
agentic because it makes predictions. Things like two plus three equals five are like prediction,
but there's just no time to it. Predictions are all about structure preserving a long time.
Let's just drop the temporal element. We don't need it. Math does structure
preservation without time. So let's talk about structure preservation without time.
If that seems weird, well, we accept that memories are not about the past. So why do predictions
need to be about the future? If we can say, I'm not remembering about the past, why can't we say
I'm not predicting about the future? The usefulness of doing so, if that seems like a weird decision,
is it lets us talk about math being agentic because math is all about structure preservation.
So if structure preservation is prediction, then math is all about prediction.
Agency is all about prediction because of free energy stuff. So there you are.
And this is, in fact, how things work in the physical world. So here's a very important
source of intuition. Take this footprint in the mud here. Now, obviously, what this footprint does
is it preserves the structure of the foot. We can see this footprint and clearly it reflects
the properties of the foot. And we can use this footprint to make predictions about the foot. We
know what the foot is going to look like based on this footprint. But you don't have to think about
time here. So we actually freeze the universe here and say, just stop. There's no time here.
Can I not, in fact, still make predictions about what the foot looks like based on the footprint?
I don't have to make a prediction about tomorrow. Just say, I can look at this footprint. I know
what the foot looks like now. Do I care that there's not a temporal element to that? I just don't
see why that matters. So something else that's also super relevant to this in terms of biology.
So this is morphology. So our transformation, the way the mud transforms, is what the footprint
is made of. It's the transformation that constitutes the prediction, which is also how
predictions work in math. We take our free monoid, we transform it via a function to reflect this
looping relationship here. So we've got morphology in the physical world. It's constituting our set
of predictions. We have morphology in the mathematical world. It's constituting our set of
predictions. The only thing missing is time. It doesn't seem like time is actually relevant.
So we can equate prediction with morphology. We can think of the footprint as being the
prediction. The footprint is the prediction. We don't need an intelligent human entity to come
along and say, ah, I'm going to use this to make a prediction. It is the prediction. And the reason
we know that is because the mud is using it to make a prediction. The mud has formed this shape
because it is anticipating being stepped on in the same way again. The mud is minimizing its free
energy. And it's saying, how do I, I've been stepped on. How do I transform to minimize the
amount of change I could experience from being stepped on again by the same foot in the same
place that forms this footprint to minimize the amount of anticipated change you could experience
because it's minimizing free energy. And of course, the mud is just very stupid. So instead of
understanding that there's all kinds of things that could step on it, it forms a very limited
prediction being like, this is the only foot in the world. It can only step on me one way
because it's kind of dumb, but we have transformation through time. We have transformation
without time, you know, the physical universe versus math, but it seems like it's doing everything
the same way. And this is just like what we saw with A times B remembering A and B. We normally
think of memory as being with time, but we can remember things without time just with structure
preserving maps. So that doesn't do anything in time, but it does transform. It transforms
via functions, which don't have a temporal quality, but it still is transformation and
transformation or structure preserving transformation is what agency is all about.
So that takes me to the end of this talk. So thank you all very much for sitting through that. I
know I probably talked too fast. I got to work on that. So just to remind you all the basic idea
here, maybe what's going on with agency in the physical universe is exploiting things that math
gives us for free. What I'm personally trying to do with that is solve some problems in economics.
There's a famous problem called problem of externality, which normal mathematical tools
and economics didn't help me solve. So I ended up moving over here to look at that kind of problem.
And in general, it seems like math has an agentic structure or an economic structure. So I'm kind
of interested in producing a theory like that. Just as an example, groups are monoids with an
extra axiom. What price are we paying? Like am I buying that in a mathematical market? Just think
that's an interesting idea. It might lead somewhere. If any of this interests me, please feel free to
email me at Benj... or interest you, please feel free to email me at benjman.f.lions at gmail.com.
Thank you very much for sitting through that again. I know I probably talked too fast,
but that brings me to the end. So if anyone has any questions or anything they'd like to say,
please feel free. Cool. Thanks so much. Questions?
So when you're talking about timeless agency, I was thinking about, does it relate to
invariant quality like a Hamiltonian's? Because Hamiltonians don't have time in it,
but you can't generate time sequence from it. Yeah, I don't know anything about physics,
but I am vaguely aware that there are ways of looking at some physical thing where you're
not necessarily thinking about time. So I can't give you a specific answer, but yeah,
invariants in math and invariants in physics, undoubtedly highly related concepts and being
able to talk about invariants without talking about time, which you can clearly do in math,
probably does help us explain that relationship between agency and the physical universe,
where we do have time and agency in the mathematical universe where we don't.
Okay, thank you. Because I saw that recently there are some research domain like
energy-based model. I think those models, they don't have explicit time in the model,
they just want to minimize energy. So it's similar to what you said.
Absolutely. Yeah, there's a physicist named Julian Barber. I haven't actually read their work,
but I know they've produced a book about doing physics without time altogether. So
I haven't read it, but just the awareness of that as a conceptual possibility
did inspire some of these ideas. Thank you.
Oh, hi. I have a question. Thank you for the talk, by the way. So it started out with the premise
that scarcity is a necessary condition for agency. So agency implies scarcity, but
is scarcity, do you think scarcity is also sufficient condition for agency? Because
I think for any definition of agency should specify both necessary and sufficient conditions.
Yeah, I agree. That would be nice to have. I'm just not entirely sure. It seems to me that it's
conceptually possible that you could have an empty mathematics where you have logical consistency,
but no assumptions, no theorems, no nothing. So maybe you don't have agency because you're not
doing anything, or maybe it's trivial agency. So it might end up being sufficient, but I just
don't know for sure. So I feel very confident it's necessary. I'm not entirely sure that you
can just say that it is sufficient for sure. So it might be, but I just didn't want to say that
because I'm not 100% on that. Also kind of a related question. We often, I guess in cognitive
science and the literature says that agency standard requires some kind of embodiment,
right? And when you think of math, math is very abstract. And in a way, it's limitless.
It does not have an embodiment, and it does not. And one thing that embodiment does is it
specifies some kind of constraints. And math in a sense is limitless. And yet it seems to have
some properties, scarcity, some kind of constraint, which is kind of confusing to me. But in any
case, I think, do you think that math also has some sort of embodiment, which people also think
is required for agency? I think the embodiment consists of the relational structure that forms
when you define some purpose. So let me go back through, like back to, to, you know, here or here.
So like, you know, this kind of feels like embodiment to me, like we're defining A times B.
So this may not look like a definition. This is a definition. This defines A times B. And what it
is, is it's just a set of relationships to other things, defining sort of information that's coming
in and ways of building things. When we look at what A times B does and how it relates to everything,
I'm not an expert on embodiment. That's kind of my understanding is that to make sense to the brain,
we have to think about how it connects to the rest of the body. And what it does out in the
environment, we can't just consider it as some isolated, you know, just intelligence floating
around like a spirit. We do end up doing something similar in math, where you end up defining objects
by these universal properties, by these systems of relationships. So I think you could think of
this as analogous to embodiment in math. I mean, I think it's related, Sandesh, I think it's related
to the stuff that Madeline and I are doing with the grim sentences, where it seems like there is
at least something that it's all doing in the absence of specific embodiment.
Like I think it's related to the Platonic space stuff that we talked about. It's
to the to Madeline's project. I guess in a way, any definition of agency would have to be
specified in mathematical terms at the end of the day, because we do not have any other
language to express any concept other than mathematics and logic. And I guess mathematics
is like logic, it is a foundation of mathematics itself. But if you assume that mathematics is
the only language through which we can express agency, perhaps it kind of follows that mathematics
is agentic because it's the only language we can express agency in. Is that sort of a paradox or
contradiction? Yeah, no, it's a great question. The way I think it's going to end up being resolved
is we're not going to actually come up with like a specific definition of agency, where we can
exactly say this is agentic and this isn't. What agency is going to end up being is it's going to
be a human term for clumping things together for pragmatic uses based on common underlying
structure. So the argument that math is agentic here, I don't intend to find a precise definition
of agency. Instead, what I intend to do is show that it does the things that we normally call
agentic. If we saw a physical object doing the stuff that we see math doing, we would call it
agentic. So maybe we can just call math agentic as well. And the reason to do so is not to be
necessarily just super consistent for consistency sake, but because that will lead to useful ideas,
both useful things like finding these novel competencies and useful things for scientists
broadening their horizons, bringing in new mathematical tools, asking new questions.
So that's going to be the ultimate benefit of this is helping humans make smarter
categorization choices, which will lead to more efficient scientific and mathematical decisions.
Thank you. I mean, I had a question about the, go ahead, Mike. No, no, you go ahead. Okay.
So let's assume for a second that we agree with that idea of math being time. I can see what you
mean. I think I'm curious, though, is can you apply that to computations as well? Right? So
your example of the equation where you have three plus five, or what was like equals eight,
I've got numbers three plus two, I think that equals five. But right in a, not only in a computer
setting, if you have any kind of network where you update states based on the equations, essentially,
right, you have to make those computations. And even though you don't specify exactly what time
step that is, tasks have a step and actually matters a lot in these network computations
and any networks I've seen, how you update it, do you all do it synchronously? Do you do it
sequentially, right? You can see that that very intuitively changes the outcome of computations.
So that's kind of my question. Do you think you can have, even if you can have,
even if you express math without time, can you also get time out of the computations?
Yeah, I don't know a lot about the specific details, but it's something I have been thinking
about because it makes sense as a question and it is something that needs to be resolved.
I suspect that there probably are ways of defining time in terms of abstract connections,
which is basically some kind of, I'm sorry to articulate this, but some kind of functorial
quality that's varying as other things are varying and sort of structure preserving maps that are
themselves structure preserving as they move along their own structure preserving trap track.
And there are things in, so all this kind of comes from category theory, there are things in
category theory that look like that. And so maybe that will lead to some interest. I'm sure
people are working on that. So I'm sure you could lead to some theory of computation. So I don't
really have a specific answer, but it's definitely an important question, something that I'm interested
in as well. And I have no doubt that I will not solve it. And hopefully someone else will.
All right. Just one super short follow up. What about logic? Same questions for logic?
Because if logic, if you have, if two plus three equals five, then, you know, X, Y, Z,
isn't it then, doesn't that also imply the same as a computational aspect time?
Yeah, that's a super interesting comparison, because it is a temporal world. And we're talking
about then in the physical world, that is time. And when we talk about then in the mathematical
world, we're not referencing time. The way I take that is to say that we can basically
forget about time in the physical world and think about our predictions as being structure
preserving, even if we just drop out the time element. But it may well be that you could
do the other way and say, we're going to add time into math. So I don't know how to do that,
but it's a perfectly logical thing to try. Yeah. No, it's super interesting. I just want to know.
Like, I'm super behind saying, you know, you don't need to have a specific time. I mean,
even though that that will, of course, make it more realistic and to these different explanations,
but the idea of, right, of a logical connotation itself, even though it doesn't, like,
it's not really time dependent in terms of quantitatively. But I feel like the idea itself
of connecting things logically, even in a pure mathematical space,
are basically have a time element to it. But maybe you're right, maybe I'm just
projecting language, language problems onto it. Is there anything you think there was more?
Yeah, I mean, I think

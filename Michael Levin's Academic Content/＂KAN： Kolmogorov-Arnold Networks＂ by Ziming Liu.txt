Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.
Welcome, Professor Max Tagmark. My research interests the center around AI plus science,
so it can go both ways. Either you develop better AIs for science, like AI scientists,
or you use inspirations or like tools in science to enhance the improvement of AI.
So like this work is like both ways. We first try to use some ideas from math and see if we can
develop new AI tools and see if we can, whether these new tools can give something back to to
science. Yeah. So this is about our recent work called The Comagra of Arnold Networks.
Today's AI is built upon this math theorem called the universal approximation theorem,
which basically says that a high-dimensional function, you can decompose a high-dimensional
function into a linear combination of some nonlinear features with this nonlinear function
sigma, which is basically just this two-layer network known as the multilayer perceptron,
but you can make it deeper. That's why it got the name multilayer. But we are asking this question,
can we, are there any alternative theorems we can use? We can leverage these new theorems. Well,
not necessarily new in the mathematical sense, but like in the AI world, to build another AI
skyscraper based on new building blocks. So here we are examining this theorem called
The Comagra of Arnold Representation Theorem. First question, what is the KA representation
theorem? It says, again, like, given a high-dimensional function, you can write it down
as a finite composition of one-dimensional continuous functions and just the summing
operation. So more concretely, you can write down an n-dimensional function in this form,
where you have the outer functions capital phi q and the inner functions phi q p.
They are both just, they are just one-dimensional functions and they are finite. Like the number
of these functions is depending on the number of inputs variables n here. So what's nice about
this theorem is that it tells us that the only true multivariate functions is the sum. Like,
you can express, say, multiplication with just the summing operation plus some
1D activation functions. So that sounds like really great news for approximating high-dimensional
functions, especially for machine learning, because in high dimensions, to learn high-dimensional
functions, we know we have cursive dimensionality. But this theorem seems to tell us that you can
decompose high-dimensional functions into one-dimensional functions, which do not
suffer from cursive dimensionality, which are great. But immediately, there is this plot twist.
Back in 1989, professor Jirasi and Pajo, they wrote this paper examining the relevance of
Kamagra's theorem in the context of learning. And they conclude that we review Kamagra's theorem
and show that it is irrelevant in the context of networks for learning. So this paper basically
sentenced the KA theorem to death, well, at least sentenced to jail for like 30 years.
So is that the end of the story? Well, we want to tell you that that's not the end of the story.
And there's again a plot twist. So let's try to see what they say in their paper, why they
sentenced the theorem to jail. So their argument was that the theorem fails to be true when the
inner functions are required to be smooth. Basically, the original theorem says that you
can decompose a high-dimensional continuous function into one-d continuous functions,
but it doesn't guarantee you to decompose it into one-d smooth functions. But to learn something,
with gradient descent, you would need the functions to be smooth. Only being continuous
is not enough. Even with continuous functions, you can still have some really pathological
behavior. You don't have the smoothness constraint. So that's basically the argument,
because the theory does not tell you anything about the smoothness of the functions. But in
practice, only smooth functions are feasible in practice. So one thing, one interesting word they
use is that a stable and usable exact representations seems hopeless. So that's like the attacking
point. That's the angle we're taking. Maybe we do not need exact representations. Sometimes
approximate representations might suffice. So their paper was theoretically sound,
or was sound theoretically, but I'm trained as a physics student. So my level of rigor is lower
than mathematicians. So me and also Max, we tend to have lower bar of being rigorous. So we have
this naive optimism. So firstly, maybe like empirically, maybe we don't care about exact
representations. Sometimes approximate ones may suffice. As long as it has some level of
explainability or interpretability, that's fine. And secondly, a common wisdom in modern deep learning
is just stacking more layers to get different networks. They are basically arguing against
the two layer chromograph network. But what about we build deeper networks? Maybe for deeper
networks, even under the smooth constraint, you can win the expressive power back. Lastly,
just the can do spirit. We cannot say something doesn't work before really building it with
the state of the art techniques. They have the negative claim back like 30 years ago.
Back then, they didn't even have the back propagation. At least it was not popularized
back then. So we want to take everything we have in the modern area of deep learning and see how
far we can go. So that's the mindset. So just with the can do spirit, we propose or more properly,
we rediscover or we contextualize the idea of chromograph networks in today's deep learning
world. So here's the overview. First, I will introduce the math foundation. Well, I already
briefly mentioned it. I won't dig deeper into it, but I will emphasize again the mathematical beauty
of it. And then I will talk about the properties of cans. Why and in what scenarios cans are more
accurate and interpretable than current deep learning models.
Yeah, so first the math foundation, I already covered this part, but I want to emphasize
again that the theorem looks really nice that allows you to decompose a high dimensional
function into one dimensional functions. And after the decomposition is done, your only need to just,
your only job would be just approximating the one d functions. So that's the idea of
that's the idea of the chromograph of networks, decomposition first and then
learn the one d functions. Well, yeah, so this representation looks a bit complicated. You'll
see that there are this huge, this big summation symbol, and you have two layers of composition
and physical complicated, but don't worry about it. It's just equivalent to this two layer network.
Let's suppose we have two inputs x1 and x2 and we have the outputs at the top here.
So the representation in the original theorem is basically just that you have five hidden neurons
in the middle. And to get the activations in each hidden neuron, you basically apply one d
possibly nonlinear function to x1 and x2 and sum up this two nonlinear activations
to get the activations at a node. And in the second layer, it's similar that you apply
some nonlinear function to the hidden activations, summing everything up at the output node,
and that's how you get the output. So the computation graph is it's super clear with
just this diagram. And this might remind you a lot of like this looks just like a multi-layer
perceptron, so fully connected networks where everything just fully connected. But instead
of having activation functions on the nodes, now we are having activation functions on the edges
and on the nodes, you simply just have the summation operation, which is really simple.
Yeah, just to make it more elegant and beautiful, we can, or more intuitive, we can basically,
because like one d functions, we can basically just visualize them with one d curves with the x and
x as the input and y as the output. So now can network is basically, you can picture it as this.
And by staring at it, you can you can have some idea what's happening inside.
Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent
to a two layer network, but can we go deeper? The answer is yes, algorithmically speaking,
because it's just a stack of two layers, which from which we can abstract a notion called the
can layer. So the original two layer network is basically a stack of two, two can layers.
And for each can layer, it's basically just taking some number of inputs and outputs, some
number of outputs, and in between is fully connected. And on each edge, you have the active,
you have the some nonlinear, learnable nonlinear activation function. And in the end, and in the
output, you summing up the incoming activations, that's how a can layer works. And you can simply
stack more and more can layers to get deeper and deeper cans. This is just a three layer can,
like the first layer, we're taking two, output three, the second layer, input two, output three,
and the last layer, you input three, output one. So this is the, this is a three layer network,
which approximates a scalar function in two dimensions. But obviously, you can easily
extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary
depth. So, so you have all the flexibility to choose the size of the network.
Yeah, one, the first question, Professor Poggio asked me when I presented this
network to him, he asked, is that why do you need this deep networks, because the original
theorem told you that you only need the two layer constructions. And here's just a quick
answer that I can give you an example. So please look at this symbolic formula. And if you examine
it, you would immediately realize that you would need at least three compositions to do this,
to construct this formula. You need the, you need the squared function, you need the sine function,
you need the exponential function. They're like, because they're, because it's the
compositional structure, they're in different layers. So you would at least, at least need three layers
to, to, to learn this formula. And indeed, if you just use two layer network, the activation
functions becomes really oscillatory, becomes really pathological. And the performance is bad,
and also the interpretability is bad. But in the right, I show that a three layer network,
a three layer can train on this, train on this data set. And after training, you would immediately
see the learned activation functions in the first layer, you got the squared in the second layer,
you got the sine. And the last layer, we got the exponential. Well, well, you may think of,
you may think this is some other functions, maybe just some local quadratic, but yeah,
but you can, but you can like do the template matching with the candidate functions and figure
out which one fits the best. Yeah, so, so I, I said that this activation functions are learnable,
how do we make them learnable, because they're functions. And, and, and, and the common wisdom
is that we need to make, we need to parameterize the things to be learned, so that we can use
gradient sense to learn, to learn this stuff. So the idea is that we parameterize a 1D function
with, with B splines. So B splines is basically some piecewise, some, some local piecewise
polynomial functions. So here I showed that there are some local B spline bases. And the way we
construct the activation functions is by linearly combining this, this, this B spline functions.
And the only learnable parameters are just the linear coefficients of, of, of this local basis.
And what's nice about this formulation is that we, we inherit the advantage of B splines,
we can easily switch between fine grains, fine grain grids and coarse grain grids.
If you want something to be more accurate, you can, you can choose the mesh to be more
fine grain. If you want the model to be smaller, so you can have a faster inference, you can,
you can, you can choose a more coarse grain model.
Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,
because they do share some similarities, but also share some, but also have some differences.
So MLPs are inspired by the universal approximation theorem cans, again,
inspired by the Camargo Ravano representation theorem. The networks looks a bit similar in
the sense that they're both fully connected, but they're dual, they're different, but they're dual
in the sense that MLPs have fixed activation functions on nodes. Well, you can make them
trainable, but, but they're on nodes for sure. And in MLPs, we have learnable weights, learnable
linear weights on edges. By contrast, cans have learnable activation functions on edges, while,
while cans have this simple linear summation operation on nodes. So, so, so in this sense,
cans does not separate the linear part and the long inner part as MLPs do, but it integrates
both the linear part and long inner part altogether into the can layer. And the can network is
simply just the stack of the can layers. Yeah, so in both cases, you are free to stack the model to
become deeper and deeper, because we have the basic notion of a layer, you just stack more layers to
get the deeper networks. Yeah, so that's the basic, that's the basic ideas of cans. And, and, and now
I want to elaborate more what the, what a, yeah, like, like, why do we care about, why do we care
about this? What, what are the advantages that cans can bring to us, but other black box, black
box models do not bring to us. So yeah, so the, so the first property is, is the scaling behavior
of cans. As I mentioned before, the idea of cans is decomposing a high dimensional functions into
one dimensional functions. So that looks like really promising that it can get a, it can get us,
it can get us out of the curse of dimensionality. Let's suppose we're trying to approximate a
d dimensional functions. And suppose the function has no structure at all. So then we need to,
we need to have a hypercube and have a uniform grid on the hypercube.
Let's suppose we have 10 grid 10 anchor points along each dimension. Then we will need 10 to the
power of D number of anchor points in the d dimensional hypercube. So that's exponentially
expensive. So if you do the classical approximation theory, you would notice that the approximation
error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.
And it's one, and the exponent is one over D, meaning that you got exponentially
like slower when you have more and more, when you have more and more dimensions, like if you need
10 points in 1D, you would need 100 points in 2D. You will need 1000 points in 3D and so on.
But if the function has some structure, like if it has the chromograph unknown representation,
then we can decompose it into a bunch of 1D functions. And then our job would just be
approximating 1D functions. So now effectively D becomes 1. So you got a really, you got the
fastest possible scaling laws. But the caveat, immediately the caveat is that we, the assumption
is that we, like the function has a smooth chromograph, a smooth finite size chromograph
unknown representation. All of this, you know, all of these objectives, adjectives like smooth or
finite size are just practical conditions for a real, for a network which we have access in
practice that can really learn the network. We want it to be smooth because we parameterize it
with B splines, which are smooth. We want them to be finite size because, of course,
you know, we cannot initialize, we cannot deal with an infinite size neural network.
So, yeah, so we just did some sandwich check on some symbolic formulas.
So, yeah, so symbolic formulas are like what we used in science. So that's why we test them first.
So let's see. So the red dashed line is the theoretical prediction. Here we are using
cubic splines, so k is k3. And the scaling exponent is k plus one equals four. And the can network,
the curve, yeah, so the thick blue line is for the can network. And you see that almost like the,
like empirical results for the can network almost agreed with, almost agrees with the
theoretical prediction, although sometimes performed slightly worse. Or in this case,
in the second to last case, there's a hundred dimensional case. And it performs much worse
than a theoretical prediction because the, because the dimension is just too high and the
network can get, can get stuck at some local minima or whatever. But, but nevertheless,
it still outperforms MLPs, which you see up in the upright corner here, like the case really
slow. But, but the cans, but cans at least can outperform MLPs to a great margin, although
still not saturating the theoretical prediction. But still the scaling law that can shows looks
looks promising that it's, it seems to not fully beats the curse of dimensionality,
but at least partially beat the curse of dimensionality.
Well, yeah, yeah, so, yeah, just to play the devil's advocate here, you may immediately notice
that I'm on purpose just deliberately using this symbolic formulas, you might be wondering, well,
maybe, maybe the functions we care, we encounter a lot in nature may not be symbolic, they might be
some weird, you know, like, at least for special functions, they are like infinite series,
which are hard to be represented with just the finite network. So what are, what, so, so, so,
so what's the cans performance in that scenario? So, yeah, so we just tried some special functions,
which we know, for most of them, they do not have analytical formulas. And indeed, we see that
the scaling laws of cans do not saturate the theoretical prediction, meaning that probably
you cannot decompose like a high dimensional functions into just the one D functions. But
still, our goal here is to outcompetes MLPs. And, and, and it's a feature, not a bug, like,
not all the functions can be decomposed into smooth finite representations of
this k representation, they may admit non smooth finite size or smooth infinite size, but in,
but, but neither case is, is like the, is what's accessible in practice. So,
yeah, so, so here we show that in most cases, we can achieve this minus two scaling law, which
means that the can network, well, well, sorry. So here, these special functions are all just
two dimensional. So, these, so these, according to the spline theory, it would predicts a two
dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,
which agrees with the can results. But in some, but in some case, we can, we can still got the
minus four scaling law. And the reason is that actually, this is secretly, like, like, although
we call, we call it a special function spherical harmonics are not that special in the sense that
they're still decomposable. So, so you can get the minus four scaling. But also in other, in other
cases, you got some worse behavior, like you got the minus one scaling, which means that the can
is underperforming even out on the performing compared to like the spline theory. But what's
interesting is that in this case, MLPs are even more underperforming than cans. So, so this may
tell us that maybe for low dimensional problems, neural networks are not that necessary. And even
the spline theory, like the spline approximation can out compete the neural network. So, so it's
something to be think about. It's, it's something good to keep in mind. Because neural networks
just have too many degrees of freedom and may have optimization issue.
Yeah. So, so beyond function approximation, we can go on to solving partial differential
equations. So in the setup of physics in form learning, basically, we're trying to solve,
we're basically trying to represent the solution of a PDE with MLP or with a can network.
So, so the only difference from the previous results from the previous experiment is that we
are just we're using the physics informed loss rather than the regression loss. So the optimization
becomes more complicated, but still it's just approximating some function. Yeah, so, so we still
see that we with cans, we can get this optimal scaling law. Well, with MLPs, you see, while with
MLPs, you see that, well, it has the skill at first, but it plateaus really fast and then
does not improve when you have more parameters beyond 10,000.
Um, besides being more accurate, we can also gain some insight what the network is learning.
So, so, yes, yes. So for this example, we can for this PDE example, we can actually visualize
the can network like this. And immediately you can see that there's some like sine waves and
there's some linear functions. And you can even do symbolic regression to it, like,
like, like our software provides you a way to do this, you can, you can do symbolic regression
to it. And after you do this, you can do some further training. And you and you can even extract
out the symbolic formula, which gives you like a loss down to machine precision. This is something
that that that's standard neural networks would not give you because of because they
usually you cannot convert a neural network into a symbolic formula very easily,
but with cans, you can easily do that.
Yeah, another property of cans is that it has this property of continual learning,
at least in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid
for high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,
we have, we want to approximate this 1D functions with five peaks. But instead of
feeding auto data at once to the network, we're feeding each time we just feed one peak to the
network. And we do the sequential learning at each stage, the network is just a,
is just fed with just one peak. And with cans, and with cans, because we're using the local
B-splines, so when it sees the new data, it does not update the parameters correspond to the old
data. So it has this, it can't get rid of this catastrophic forgetting, like when new data are
coming in, the can is able to memorize the old data and still do quite well on the old data.
But this is not true for MLPs. Like when you're, when the MLPs are fed with the new data,
it catastrophically, they catastrophically forget about the old data. Because in MLPs,
you usually have this global activation functions like the silo functions or Reilu functions. So
when, whenever you make adjustments locally, it will affect the predictions far away. So
that's the reason why MLPs would not keep, would catastrophically forget about the old data.
Yeah, so that's the first part about accuracy. The second part is about interpretability. You
might already have some sense because we are able to visualize the network as a diagram. So the hope
is that you can just stare at a diagram and gain some insight of what's happening inside the neural
network. Yeah, so here we have some toy examples. Yeah, for example, how do cans do the multiplication
computation? So we have x and y as the input and the x times y as the output. So when we train the
can network, we also have some, something similar to L1 regularization to sparsify the network. So
we can extract out the minimal network to do the task. So in the multiplication case, we see that
only two neurons are active in the end. And we can read off the symbolic formulas of how,
and get a sense of how it does the computation. Well, I marked the symbolic formulas here,
it may take a while, it may take a while, but the point is that it basically just some squared
functions and the way the can network learns to compute this multiplication is by leveraging some
squared equality here. And the second example, the can is tasked with the division task,
where we input two positive numbers, x and y, and can is asked to predict the x divided by y.
And because here x and y are both positive, the can network learns to first take the
logarithm transformation and then take the subtracted two logarithm and then
transform the same back via the exponential, exponentiation. So that's really cute.
Like one example of the commograf theorem is that you can basically do the multiplication
of positive numbers or the division of positive numbers in the logarithmic scale,
because the division or multiplication in the logarithmic scale would translate to addition
and subtraction in the logarithmic scale. So these examples are really simple, you might be
wondering what about a more complicated formula, then it might be very complicated to decode what
the cans have learned. So I would want to argue that this might be a feature, not a bug, or you
can call it a bug, but I will call it a feature in the sense that, sorry, I didn't show the network
here, but let's suppose we're doing this formula here, like u plus v divided by 1 plus uv. So if
you're familiar with relativity, you will realize that this is the relativistic velocity addition.
So at first, I thought that I need the five-layer can to fit this function because you would need
multiplication, which would consume two layers, two additions, consume another two layers,
and also you would need the division. So that's in total five layers. But it turned out you can
just use a two-layer can to fit this function perfectly well. And in the end, I realized that
this is just a rapidity trick known in special relativity, where you first do the arc-tange
transformation to u and v separately, sum the thing to rapidity up, and then you do the tange
transformation back to get this formula. So in this sense, it's rediscovering the rapidity trick,
known well-known in the special relativity. And in some cases, I indeed find that the network finds
some more compressed, compact representation than I would expect. That's good news in the sense that
in the sense that the network is discovering something more compact. So the representation
is more powerful than I have expected. But the bad news is that sometimes the interpretation can be
subtle, can be a bit more complicated. But I mean, it can be a feature bug, depending on your view.
Yeah, so this is one paragraph I take from the paper. This has been criticized a lot
in the social media. But I find this analogy really interesting. So I still want to
highlight this part. So to me, I think it's like a language model, or like language,
or even like language for AI plus science. The reason why language models are so transformative
and powerful is because they are useful to anyone who can speak natural language. But the
language of science is functions, or more advanced mathematical objects build on functions.
Because cans are composed of functions which are 1D, so they are interpretable. So when a human
user stares at a can, it's like communicating it with, it's like communicating it with the,
using the language of functions. So to elaborate more and to make it more entertaining, let's
suppose we are like my advisor Max is communicating with the can network. Here I picture the can
network as a tristolarian from the three-body problem. I'm not sure if you guys have watched it,
but basically the tristolarians, their brains are transparent, so they cannot hide any secret
from others. So they are totally transparent. So Max went up, give the can a data set.
Max was like, here is my data set. It contains the mystery of the universe. I want you,
the can network, to figure out the structure of the data sets. And can initialize the can
network like this. So here's the brain of the can network initially. And Max, given the data set,
Max wants to train the can network, to train the brain. And after training, you can get this sparse
network, which you start to see some structure. But still it's a bit, there's still some residual
connections, which looks quite annoying. So Max asked the can network to prune the redundant
connections. And after pruning, you got this sub network, which is responsible for the computation.
And Max further asked the can network to symbolify it, because it looks like in the
bottom left, it looks like just a sign function. And in the bottom right, this looks just like a
parabola. And this just looks like an exponential. So maybe you can symbolify it to gain some more
insight. So yes, so the can network said, yes, I can symbolify it. And you can,
and now the data set goes all the way down to this symbolic formula.
But, but we can imagine another conversation Max would have with an MLP. So Max went up and
give the data set to MLP and want the MLP to figure out the symbolic formula in it. And
like before, the MLP initialized the brain, looked like something like this, really messy.
After training, Max asked MLP to train the brain. But even after training, the connection still looks
really messy. And MLPs were like, and the MLP is like, I really trained it, but the loss is pretty
low, but it's just that it's just that the connections are still very complicated.
Now Max got confused, like, like, what's going on? What's going on with your brain? And now MLP is
like, it's just that your humans are too stupid to understand my computations. You cannot say,
I'm wrong, simply because you cannot understand me. So Max now got really pieced off and turned
back to Kansas. Right. So, so, so those are just some imaginary stories I made up with the
synthetic example. And those, and that synthetic example is really simple. But I want to show that
we can really use Kansas as a collaborator in scientific research, and Kansas can give us some
non-trivial results, can give us some new discoveries. So the first example is,
yeah, so this example was used in a DeepMind nature paper three years ago, where they used MLP
to discover a relations in the not dataset. So each knot has some invariance. Basically,
each knot is associated some numbers, and these numbers, they have some relations. And we want
to dig out the relations among these variables. So what the DeepMind people did was they used
the train and MLP and used the attribution methods, basically take the gradient with respect to these
input variables, and use that as a score to attribute these features, and then rank these
features to get a sense of which features are more important than other features. And they
identified three important features. That's the only thing that's automated in their framework.
And then the human scientist came in and trying to come up with a symbolic formula for it.
So we're asking this question, can we discover, have we discovered these results
with more automation, with less effort, and probably even discovering something new
that the DeepMind paper were missing? So first, we are able to discover the three important
variables with the CAN network, with much more intuition and automation. So they're
they use a three layer, sorry, they use a five layer MLP, and each hidden layer have 300 neurons.
So that's really hard to interpret. That's why they use the feature attribution.
But we find that surprisingly, we only need a one hidden layer and one hidden neuron,
the CAN network, to do the task, as well as their five layer, like a million parameter MLPs.
And with this, you can also clearly see the activations, the importance now basically becomes
yeah, you can basically understand the importance of these variables with the
L1 norm of these activation functions. So that's also how we visualize the connections.
So you can basically read off from this diagram that the strong connections
are the important variables, while this weaker or even nearly transparent
connections, meaning that irrelevant variables.
Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes
high dimensional functions into 1D, and then we can just do template matching in 1D
to see what each 1D functions represent symbolic formulas, and then compose
all these 1D functions back to get these high dimensional functions.
Something beyond their paper we discovered is that their setup is a supervised learning setup.
Basically, they need to partition the variables into inputs and outputs,
and they use the inputs to predict at the outputs.
But in some cases, we do not know how to partition the inputs and outputs,
like all the variables, they are treated equally.
So we want to develop this unsupervised setup where all the variables serve as inputs,
but we use some notion of contrastive learning to classify whether some given input is a real
knot or a fake knot. Well, that might be too technical, but the result is that we are able
to discover more relations beyond the relation they've discovered, because they manually
partition one variable as the output, so they can only discover the relations that
involve that variable, but here we are learning it in an unsupervised way,
so we can learn more than just one relation. We also discovered the relation between these
three variables and between these two variables. Unfortunately, or fortunately,
these relations are already known in the literature of knot theory.
So the unfortunate part is that we did not discover anything new with our framework,
but notice our network is just very preliminary, it's just a one layer,
it's just a one layer if we ignore about the classifier in the second layer,
so we can, hypothetically, we can make it deeper to get more complicated relations.
But the fortunate part is that it verifies that's what we discovered with the network,
it's something, it's not bullshit, it's something that makes sense that people already know in
literature. We also did this physics example, specifically Anderson localization. I don't
want to bore you with the technical detail, but again, the goal here is try to figure out
the symbolic formula of the phase transition boundary. Like in Anderson localization,
you have the localization phase and extended phase, and there is a phase boundary, and we want to
extract out, we want to extract out the phase boundary, especially the symbolic formula
of the phase boundary, if there exists a symbolic formula for it from the road data.
Yes, so for this slide, I don't want to go to the detail, but I,
but the point I want to make with this slide is that the CAN network have, just like cars,
you have manual mode, you have manual mode, you have automatic mode, like if you're lazy,
you can just delegate everything to CANS, and CANS will return you a symbolic formula
fully automated, but that might not be correct, that might not be what you want.
You can do that, it can give you reasonable accuracy, but may not be fully interpretable,
but if you want to have some controllability over the CAN network,
where you want to be more involved, you want to have some intervention, you can still do that.
You can choose to, you can use the manual mode, where you just handpicked some activation functions,
like some functions, obviously they're just quadratic, linear, you can set them to,
you can set them to be exactly the linear or quadratic, and then you retrain the other
activation functions, and you see, and after retraining, you see what those activation functions
would change to different, would change to different form, and then this gives you,
again gives you some insights, like give you better evidence what the next guess you would
want to make, and this is like the iterative process, like it's sort of like you are arguing,
where you are debating with the CAN network, like the CAN network is give you some,
or you can say debating or collaborating, just like how you interact with a human
collaborator, sometimes you debate, sometimes you collaborate, but you seem to look at the same
thing from different angles, like the CAN network is really great at decomposing high-dimensional
functions into 1D functions, but those 1D functions may not be perfect, and there might be some
actual subtlety, but humans are super great at identifying the symbolic stuff, and also
recognizing the modular structure from the CAN diagram, so yeah, so the takeaway is that you
can choose to be lazy, use the automatic mode, or you can choose to be more responsible and more
involved using the manual mode, yes, so maybe, yeah, maybe in the end, yeah, I will just finish
this really quick, so people have asked why, it looks like CANs can, you know, in terms of
expressive power, CANs are just MLPs, are just like secretly just MLPs, so why do we need CANs?
I want to argue that from a like a high-level philosophical level, CANs and MLPs are somewhat
different, CAN is like clock work, like in a clock, pieces are customized and have clear
purposes corresponding to the learnable activation functions in CANs, so for a clock,
it's easy to tear little parts, it's easy to tear the pieces apart, and then we assemble the pieces
back together to get the clock back, but MLP is like a house made of bricks, so in MLPs, the pieces
are produced in the same standard way, like each neuron takes in some linear combinations,
and then do some do the same nonlinear transformation, but it's difficult to tear,
once you have a house built, it's difficult to tear apart the bricks and then reassemble them,
so in summary, the source of complexity are different in CANs and in MLPs, the source of
complexity in CANs come from the complexity of each individual object, like those 1D learnable
functions, but because the functions are 1D, no matter how complicated they are, they're 1D,
and they have clear purposes in some sense, they are nevertheless, they are interpretable,
but the complexity of CANs come from complicated interactions of individual parts, the individual
parts are simple, but the interactions and connections between these individual parts
are really complicated, I guess it's more like human brains, it's more like biology,
yeah I don't know, but CANs seem like more aligned with the philosophy of reductionism,
where you hope that, where you expect that you can decompose a complicated object into a field,
into a field like interpretable individual objects, while in MLPs everything is connected,
the reason why MLPs function is because they have this emergent behavior or collective behavior
in some sense, yeah just some interesting question people ask, are CANs physical?
If you think of the Feynman diagram as physical, then unfortunately Feynman diagrams are sort of
more like MLPs, because in Feynman diagrams, on the edges, it's just a free flow in space
without anything interesting happen, but on the nodes where two particles or multiple
particles collide, it's where interesting things happen, this is more aligned, this is probably
more aligned with MLPs, but CANs, it's like interesting thing happens on the edges, but not
on the nodes, and yeah last question, people also ask are CANs biological, because people think that
MLPs are inspired by the neurons in our brains, are there any biological analogy, and I don't know
at first, but someone from Twitter wrote it that CANs actually is a bit analogous to
the cells in retina, where you first, like each individual cell receive light, apply
some nonlinear transformation to it before summing everything up, I'm not sure, you guys
are the experts, so please correct me if I'm wrong here, but the argument is that, well maybe
the mechanisms of CANs, like your first applied nonlinear transformation and then summing everything
up is indeed biological, but I guess that's just for fun, that's just a minor justification why
why we need CANs, because in some sense CANs are also biological,
yeah so yeah so that's basically everything I would like to share, and I'm happy to chat more
if you guys have questions. Super interesting, thank you, questions.
For the last example in retina, I have some question, if the network, like CAN network,
is deep enough, is it still matter if you say nonlinearities before or after the summation?
Yeah, so I guess the key difference is that in CANs, the activation functions are learnable,
so I guess yeah, but whether to put activation functions on edges or on nodes, I don't think that
might be a key difference, like the learnability of activation functions give you more flexibility.
Yeah, when you talk about this, I was thinking about that CAN is decomposing
different variables, input variables, like if you have x and y, then CAN could decompose it,
because you would have a different combination of them, but if you have a nested function like
sine x square or exponential sine x square, then the CAN seems not able to decompose them,
because they don't have these primitives. Yes, that's exactly correct, so CAN can only discover
compositionality in the sense that all the 1D functions are boring to CANs,
it can just be approximated with just one B-splice, it doesn't learn any compositionality
for single variables. That might be one bug, if you will, for CANs, if you really want to figure
out the symbolic formulas in the data set. But like for Professor Tomaso Poggio,
the author of the 1989 paper who sentenced the theorem to jail, he wrote in his paper that all
the 1D functions are boring, and what's interesting is this compositional sparsity when you are dealing
with multiple variables, but I guess it depends on your goal. If your goal is just to learn the
function efficiently, then it's fine, but if your goal is to really understand if it's sine of
exponential or exponential of sine, then we probably need to think about ways to handle this.
Yeah, thank you. Have you thought about combining CANs and MLPs given their somewhat
complementary nature, or perhaps despite their complementary nature? Yeah, that's a great question.
So we have some primitives, like CANs can propose this CAN layer, which is a new primitive,
and for MLPs, it has these linear layers and also the nonlinear activations, which are also
primitives. I mean, these are like the building blocks, and I guess as long as they fit together,
you can freely just combine them in ways that you want, but it's just that it's a bit hard to
tell what's the advantage of combining, and because I guess there are many ways to integrate
the two models, and which way is the best, and I guess it's a case dependent question.
It again depends on what's your application, what's your goal, something like that.

No, that's great.
Okay, so maybe we could start.
I think, Alex, if you could start
and just give a few more words on this,
the whole notion of mortal computation
and specifically the issue of programmability
and morphology and how we can sort of distinguish
when we're looking at a system,
how can we distinguish what aspects
of mortal computation we're looking at.
Anything along those lines,
I think would be very useful for us.
Maybe we could start there.
Yeah, so I did, by the way, read last night
the paper you shared with me and Carl and Chris
at one point and some of your notions in there.
First of all, a lot of it sounds like parts
of mortal computation, just maybe you're not using
the word mortal, complex phrase.
So in terms of the morphology part,
so the argument that Carl and I make in that paper
is just that the structure, right,
is critically important to the actual system itself.
So you don't want to devour,
there's a lot of this idea of the amorphic formulation
of computational models.
So when you take, and actually Carl and Chris had a paper
that predated the mortal computation paper,
but we developed that metaphor further,
you can draw a dot and arrow diagram of a neural network
and that endows it with this pedagogical morphology,
but really at the end of the day, it doesn't matter,
it's just a pile of linear algebra
that will do its calculations.
And then we have to go through the von Neumann
computing architecture to transmit the weight,
memory and all that,
and then that's where this great thermodynamic cost.
So living systems, I know as you know very well, Michael,
but in general are inherently morphic,
and actually this is the part where we didn't have it
in the paper, but your paper used the word polycomputing,
which is the idea that a substance can compute
many different things simultaneously,
it's like one way to look at that word.
And the idea is that it's all about the substrate.
And so mortal computation then just says,
if we're going to think about artificial,
general intelligence or machine intelligence,
we're sort of going about it in the wrong,
potentially wrong direction by having this divorce
of the computational architecture and construct
separate from the morphology or the substrate too,
that this is going to be enacted on
because living systems, if you change the morphology,
you change the properties of the system,
you change also what it can compute, what it can do.
Now you talk about the liquid kind of brain
or the idea that we have this self over time,
which doesn't actually sit at odds with mortal computation.
So the idea is that you're constantly through change,
you're persisting, you're almost like I want to persist,
but I understand that my system
is going through like auto pieces.
So mortal computation sort of absorbs that
and tries to say we should be designing systems
from that perspective.
So just so I don't keep rambling,
you were talking about programmability.
And I think that was an interesting part
that we didn't really get to chat a whole lot about
and it wasn't 100% clear what was meant
by programming the morphology or the system
because actually after reading your paper,
I think that sort of gave the answer as to how,
because I said originally, oh, well,
I'm thinking that the morphology or the substrate
dictates strictly what you can and can't do.
Yes, it will change and it will repair
and go through damage or do things like self-replication.
But I wasn't thinking about the human designer,
let's say if we are designing a chimeric system
or something new, manipulating that morphology so easily,
it's more like, well, we're going to be looking
at computational simulations of that
and that would be programmable.
And then that's where I was talking about,
oh, we could do like a software simulation
of Anthrobots or something or Xenobots of that form.
And then you could sort of set the properties
of the environment in the system roughly according
to something you want to look at and then simulate it
and see what it does.
But then your paper sort of essentially in your work
is an example of you can directly program
like the genetic aspects of a system
or you can kind of manipulate the bioelectrical chemicals.
You had an example of like a tadpole or a froglet
where if we apply the right electrical stimulation,
you can get it to grow a tail or get it to grow a leg
and there's some properties there.
So I think that is where the programmability is.
And then that would be more,
you would have even actual experience
programming the morphologies.
Whereas I was thinking more from the perspective
of neural networks where we have the structure
and we want that to now be at the very top
of what Karl and I call mills.
And that you remember in the paper,
there's the mortal inference learning and selection.
And at the very, very top is structure.
And that would be something that I think is underexplored
in the area about, oh, maybe we have neurogenesis
and apogenesis and then the system sort of has to use
that as another aspect of how it evolves over time.
But that was thought of,
or at least the way I was writing it originally,
it's sort of doing that in its own way.
You're not really involved in saying,
oh, I'm gonna help you do model selection,
but maybe if you encode priors,
that was the other aspect I had.
If you were encoding certain constraints,
you say, well, I just, I'm gonna play,
I'm gonna skip ahead for what evolution
would naturally walk, randomly walk you to
and say, these types of structures are invalid.
That would be like programming,
maybe structure from that perspective.
I don't know if this is making sense,
but I think it's a tricky topic
because I wasn't entirely sure what exactly two
was meant by programmability
because you have experience actually doing that.
So the answer is yes, it can be done from your perspective
and we would just be translating it to chimeric systems
or artificial systems rather than it
only just being like biological material.
I hope that makes sense.
Yeah, yeah, I mean, the kind of programmability
I had in mind, so just as an example,
we have these flatworms, these planaria,
and you can chop them into pieces
and every piece regrows a complete,
perfectly patterned little worm.
And you could ask the question,
how does it know how many heads to make?
And so it turns out that there's an electrical pattern
that sort of a body-wide electrical pattern
that dictates the number and the location of the head.
And the amazing thing about that substrate
is that if you change that pattern,
the tissue will hold it.
So we can change the pattern to say no,
two heads instead of one and it holds.
And those worms in perpetuity forevermore,
despite their completely normal genetics
will continue to regenerate as two-headed worms.
So it's a very minimal example of reprogrammability
because we don't have anything like complete control yet,
I think in the future, maybe we will,
but at the moment we don't.
But it is an example where the hardware
in an important sense, so the genetics are normal,
all this, there are no weird nanomaterials,
there's no genomic editing,
there are no synthetic biology circuits,
it's stock hardware, but because of this experience,
this physiological experience that it's had,
it now has a different pattern that it uses
as the sort of target morphology
of what it's going to do if it gets cut.
So that's the kind of thing,
the kind of plasticity that it has where
the material is basically the same,
but it has really a memory of a past event
and that memory guides how it behaves
in anatomical space in the future.
So that's the kind of thing I wanted to sort of explore
with respect to this framework.
And I don't think what you described is at odds
with what you would do with a mortal computer.
I mean, the idea is that is where the programmability
comes into play, you're kind of encoding that
and then seeing how the morphology
and the system evolve over time.
If you want the two-headed worm example,
I don't see any reason why that wouldn't translate
to artificial systems or maybe non-biological systems
if we are able to formulate what that morphology looks like.
I think the key is setting that up
and that's what Carl and I have at the very end of the paper
we talk about, well, for example,
someone like me, a computational neuroscientist,
computer scientist, I don't have access to Xenobots
or the biological material that I'd love to,
or organoids, right?
I really find those fascinating,
but maybe we could simulate them
and that's what we have at the appendix, actually,
at the already long paper, digital morphology.
Maybe that could be a way to sort of bridge the gap
between the computational researchers
and researchers like yourself.
Oh, can we come up with benchmarks or system setups
that I could play with the properties,
mathematical models, maybe,
of these biological morphologies
and then kind of do investigation, right,
without the costs and the barriers to entry
to working with biological material
or some of the things I don't have.
So I don't see anything at odds.
I don't know if Carl would wanna add anything
that I might be missing or not getting.
No, that was very fluent.
I think you've covered everything there.
If I can just jump in for a minute,
it seems to me that this dimension of programmability
could also be expressed as the dimension from uniqueness,
which mortal computers, as I understand it,
have more of than my laptop,
to replicability or exact replicability, copyability.
And to the extent that a system is really unique,
you can't program it.
And in part just because you don't have two copies of it,
so you can't test what you're doing in any way.
You can't test reproducibility
if you're working with a system that's completely unique.
And biological systems are somewhere in the middle, right?
Laptops are intended to be way out
on the extreme replicability end.
And so because a laptop is completely replicable,
it's a completely generic entity,
it's almost a Turing machine, right?
It's almost just an abstraction.
And so when we're programming,
we can treat it as an abstraction.
And we don't have to worry about things like
where the power comes from
and why the thing maintains the same shape over time
and et cetera, et cetera.
I mean, if it doesn't maintain the same shape over time,
you take it to the repair shop
or recycle it and buy another one.
Whereas in biological systems,
you have to worry about all of that stuff.
And as you point out in the paper,
and as the 4E people kind of have been pointing out
for decades now, that's part of the algorithm,
or that's part of the operating system that shape.
Whereas it's not involved at all in my laptop
in the operating system.
I mean, even the operating system
can treat the hardware as an abstraction.
So we can, I think more or less,
identify those two axes,
the dimension of programmability
and the dimension of uniqueness.
And so one of the things that you emphasized
in your paper, and I thought this was very interesting,
was the energetics of using the body
as part of the operating system.
And it meant that you didn't have to pay
for a lot of memory, for example,
or quite so much processing power.
You do, of course, have to pay the cost
of keeping the body intact.
But as you pointed out, at least in organisms
that's cheaper than the cost of stamping out more laptops
and then equipping them with enough voltage
to keep them in the classical domain
so that they don't start acting like quantum computers,
which they actually are.
So I think that it would be useful to try to relate
this issue of resource cost to the issue of uniqueness.
And as well as the issue of programmability.
I'm not sure whether those are,
I suspect those are distinct dimensions,
but I think the usable area of that state space
involves a lot of correlation between those dimensions.
Yeah, I agree.
I think that would be very interesting to explore.
I don't know, Michael, if that resonated with you,
because I feel like that touches on even
pairing the Mortal Computation paper for me and Carl
in the paper you shared with all of us, of yours.
I think the two sort of start to get into that idea
of uniqueness, programmability, and resource cost.
And there are some different dimensions we could explore,
because yeah, I also just wanted to comment, Chris,
that at least when I first was writing the paper
and then I shared it with Carl,
I wasn't thinking of the,
I was thinking of biological systems
as sort of an ideal target, right?
You're sort of emulating some aspects of those systems.
So I guess I'd be giving up some uniqueness, right?
As you said, it lies in between the immortal laptop
and the perfectly unique system itself.
I think the other thing I was concerned with
is the artificial intelligence community
really liking the idea of immortal computation
and that complete divorcing,
because you're right, you do lose the moment you leave,
even just a few steps away from immortal computation,
that reproducibility, because that substrate now
is important.
And then Carl and I argue even stronger,
it's the morphogenesis too,
and the changing process,
which again complements your paper, Michael, as well,
the idea is that change is also very important.
And that evolution over time,
which is not something you're going to have
on your typical deep neural network
that just lives at the top of the von Neumann architecture.
And then the last comment I just wanted to make, Chris,
is yes, that's exactly the key,
is that in-memory processing that we want.
And that's why bringing ourselves as close as possible
until we eventually just reach what we can,
which is the Landauer limit,
and getting ourselves real close to the hardware.
Now we're optimizing thermodynamic cost.
And then of course, as you and Carl
and everyone here has shown over time,
that's the flip side to the information theoretic,
variational free energy,
but the thermodynamic free energy.
But at the end of the day, we want to be there,
because that's what biological systems are.
They are much closer to the Landauer limit
than pretty much anything in machine intelligence
that we have today.
And that would argue it's even getting worse,
because big, big transformers are really bad.
And Carl also and I state the carbon footprint.
So that's a good motivator.
Yeah, I find that energetic analysis very compelling.
I'm very interested in why biological systems
can be quite so efficient.
And I think in many cases, they're efficient
because they're able to use quantum resources
when they're doing molecular computing.
And maybe even when they're doing macromolecular computing.
I guess the one other comment about a dimension
that I wanted to throw in,
which you mentioned a little bit about in the paper,
was this dimension of explainability,
which AI is very obsessed with the explanation problem now.
And as one gets away from reproducibility,
the explanation problem gets harder and harder.
And in the limit of a unique system,
the explanation problem is infinitely hard,
because you can't do experiments,
because you can't replicate anything.
So we have that other access to work with also
We have that other access to work with also.
Maybe it would be kind of interesting
is since you were bringing up these so far three axes
that I caught, uniqueness, programmability, explainability.
We also did talk about resource cost,
kind of putting out this grid and then you saw Michael
and actually I presented,
you guys would have seen in the paper,
but I presented it the couple of times like
the different types of things that Carl and I consider
variants of mortal computers.
And so obviously Xenobot is a mortal computer.
It actually had a lot more qualities
after I re-read papers again, looking back,
but even the silicon model that we had
for the non-biological model from Ashby,
we can never forget the great homeostat or allostat.
And so maybe we could plot these a little bit
on those axes too is what degrees that they're trading off.
Obviously we need to figure out
which one of these starts to get real close to the,
like you said, Chris, the really unique.
And then that would be the extreme one
where explainability would be really,
really, really difficult.
And we could kind of plot where those are.
That could be an interesting figure to show examples.
And I'm sure Michael, you probably have other examples
that Carl and I might have missed.
So there might be some other nice biological chimeric systems,
things that are even less biological,
but have a little bit of it.
You did touch on nanotechnology as well.
And in the polycomputing paper you shared with us.
So maybe there might be some in soft robotics.
There might be something there too
that could count as variations to mortal computers
that trade off on these axes.
That's something else I just thought of as Chris explaining.
Yeah, so another model system to think about.
And by the way, we do have simulators of some of this stuff.
So we should be in touch and give you access to some of that
because maybe you can do some analyses.
We have bioelectric simulators and things like that.
Another kind of model system to think about,
and this is something that Patrick is doing at the bench.
And then I have somebody who's doing this,
the computational analysis of it,
are these gene regulatory networks.
And so the abstraction, of course, is quite simple.
It's just in the continuous case, it's a few ODEs
and they just, their nodes that turn each other on and off
and that's it.
But if you study these things,
you find some really interesting features.
For us, one of the most interesting things is that
if you do temporary stimulation of the different nodes,
so you just grab one of the node values
and you crank it up or down for a little bit
and you keep the structure of the network completely fixed.
So you're not changing the weights,
you're not changing the topology,
the hardware is completely fixed.
All you get to do is temporarily raise or lower
the activation of any node
and then you wait and you see what happens.
So if you do that and if you treat it
in the sort of behavioral science context,
you can show things like habituation,
sense it is basically six different kinds of memory,
including Pavlovian conditioning.
So these things learn.
And we've been very interested in this question.
I mean, so I have a couple of papers showing how they learn,
but one of the really interesting things
is because we don't let the hardware vary.
So this is not a scenario
where there's some kind of synapse
whose weight gets tweaked by experience.
The fact that they learn the most,
to me, one of the most interesting things about it is
where is the learning stored?
And this is something that all of the reviewers
of the original two papers got hung up on
because we say again and again, the hardware does not change.
And then they all said, great,
then you can't have it
because where could the memory possibly be, right?
And it's this dynamical systems thing
where they get chased into a regime
where future stimuli are going to cause
very different outcomes because of their history
than past outcomes.
But I wonder, and so this is what I was gonna ask you guys
to kind of think of them,
to talk about from your framework's perspective.
I wonder if the business of uniqueness
is related to this sort of issue.
I think maybe called privacy or something like that.
This idea that there is an inner perspective
to a system that's had a certain set of experiences, right?
It has a history of in the world
that is not available to outside observers.
And this is, we spent a lot of time with my postdoc,
Federico and I spent a lot of time thinking about,
you look at a network,
can you tell whether it's been trained?
And if so, what hasn't been, like, can you read its mind,
you know, this kind of neural decoding kind of thing
because you're not gonna get it from the hardware.
You can, the nodes are no different, right?
So in fact, we have a visualizer
that tries to show various aspects of it.
And if you, you know, on the left and right of the screen,
first you start off with the hardware view of it.
And that never changes throughout the whole time.
But as it learns, right,
over multiple experiences and stimuli,
something absolutely changes.
And then we have some ways of thinking about it.
But this question of, can you as an outsider,
is there anything about moral computation
that speaks to this issue of what you can tell
about a system as an outside observer
versus what you know as the system yourself,
you know, from the inner perspective,
is that something you guys think about?
I'm gonna give a piece of it
and then I'm gonna hope Carl can tag in a little bit
because I think he can flesh this out a little bit better.
So, and this might be confusion
over what you might have explained Michael
about the reviewers.
So you said, I fixed the hardware
and on top of that, I fixed the plasticity
because you said we can't change the, you know,
the values of the synapses or the connection strengths.
And I do think moral computer,
moral computation, we did address this.
So Carl and I decomposed it in,
and again, it goes back to mills.
But again, I might be misunderstanding.
So we're gonna decouple the privacy
and the perspective, the observer perspective
because I wanna hear what Carl might have to say to that.
But for why learning would still happen,
even when you fix those things, it's just the inference.
And the way that we looked at it in mills was
there's these different time scales of learning.
So if you were to pin the structure of the S and mills
and then pin L and say, you can't modify those.
Well, we still had one more piece,
which was the very fast time scale.
And you talk in your poly computing paper,
I've done a lot of work in that.
Carl obviously has done a lot as well,
predictive coding, predictive processing.
We always have the inference dynamic.
So the idea is that, and I'm sure you thought of this.
This is why I was kind of surprised the reviewers
were maybe not understanding.
So there's like short-term plasticity, right?
So the idea is that when you're doing expectation
maximization and a predictive coding network,
I can still change the neuronal activities,
the firing rates or the spiking rates,
depending on what model you're constructing.
And the synapses never change.
And forget about the morphology,
because that's a whole nother ball game.
And I would get adaptation.
And there was a very interesting paper that came out,
I don't know now, just like two weeks ago,
Wolfgang Maas in spiking neural nets talked about,
well, look, I don't need to modify the synapses.
I'm gonna do everything in my spiking neuron architecture
with just homeostatic variables,
which is, you didn't call it them,
but that's just the adaptive thresholds.
He's like, if these change,
so we have this short-term kind of non-synaptic adaptation,
you get all these effects and you actually showed it again,
it's a machine intelligence task,
but showing in all these tasks without learning
in the sense of modifying synapses.
And that was very interesting that you can go very far.
And I'll try to dig up that paper.
Something I wanted to go in more detail later myself.
So in Mills, we're just saying,
well, okay, we're obviously under mortal,
but the inference dynamics and the fact that these
still follow the gradient flow of the variational free energy
that defines your system or your functionals
that you're looking at would explain
why that adaptation that you found would happen.
And I'm sure you thought of that.
I don't know why the reviewer specifically wouldn't have said,
well, this doesn't make sense, how could you learn?
If you would pin doll three, well, it's a static system.
You are freezing it in time.
And then that would baffle me.
So that's my comment that I do think the framework
definitely speaks to that because Carl and I were very adamant
about the separation of time scales,
at least these big time scales.
I mean, there's all these intermediate ones
that I'm sure you could bring up.
And you need them all because there's a causal circularity
if you wanna build the most powerful type of mortal computer.
And there was a sentence I can't remember
because Carl and I have done many revisions of that paper.
It might have been in one of the earlier ones
where I mentioned something like,
well, even though I'm seeing morphology as important,
technically, if I was only allowed one,
I still have mills.
It's just a very simple search space, right?
It's a, well, we know that you're here.
You can't change the architecture.
So we didn't break our framework.
So that would allow us to subsume machine learning
and say, well, machine learning is like
this very, very narrow case.
It is doing something that you mills could explain.
It's not mortal, but at least it has like a fixed topology
and synaptic plasticity is there.
And we are just really, really speeding up
the inference dynamics by making it one step
because we don't use EM most times
in deep neural nets for sure we don't.
So that was my comment about addressing the learning,
the fact that things, if you fix so much,
why would that still happen?
And I definitely think mills,
that piece of the backbone of mortal computation
would speak to that.
Now, in terms of the observer effect
and what does that tell us about what's going on inside?
I have tag team Carl.
What do you have to say to that Carl?
Right.
Well, before I address that,
which in my world is a very simple answer, you can't.
Yes, we can come back to the,
so that was a really interesting exchange
and really interesting examples there.
And I was just thinking from the point of view of
the sort of the classical flows and physics
that would,
would provide a simple picture of how on earth
you can remember stuff without changing your connection weights.
And I think Alex, you identified the key thing here,
which is the temporal scale.
So, well, where to start?
It was interesting you introduced Wolfgang Mass
because he for many years has been the king
of liquid computation and echo state machines,
which is a very interesting example of how
a black boxy like kind of dynamical system
approximator, you know,
that has been proposed as, you know,
one architecture for doing predictive processing
and model computation of the sort.
But the key, I think the key point that has just been made here
is that,
the dynamics matter and the dynamics are shaped
by the landscape Lagrangian variation free energy,
whatever you want.
And that is a function of the implicit gradients
that depend upon the sensitivity of all, say,
the flow of the flow of the flow of the flow of the flow
of the flow of the flow of the flow of the flow of the flow
and then depend upon the sensitivity of all, say,
the nodes in any given network.
That sensitivity can either be read as a connection strength
or it can just be read as a sensitivity, you know,
in terms of, you know, to what extent do I change my internal
dynamics, given this particular external perturbation.
And of course, that becomes time and context sensitive
with any nonlinearities.
So, if you're talking about a nonlinear system,
then there is a, the bright line between the connection
strengths and the current effective connectivity
at this point of time, in this context,
in this part of face space or state space,
becomes very blurred.
So, if you're writing down the differential equations,
you could go one of two ways.
You could just write down a random differential equation
with loads of variables representing interactions
between different types of states and the response,
the rate of change of any particular state
that would entail the nonlinearity in question,
or you could arbitrarily say, okay, now,
one subset of these variables changes very, very slowly
and I'm going to call them connection strengths.
And I'm now going to lift those out of my equations.
I'm now left with a much simpler sort of
autonomous differential equations that are now parameterized
by other states that change very, very slowly.
Mathematically, you haven't done anything
but introduce a separation of temporal time scales,
but in so doing, you have now got a different kind
of rhetoric where initially you were talking about voltage
sensitive receptors and sensitivity
and contextualization of conductances and the like,
which sets the synaptic efficacy,
which is fluctuating moment to moment.
And now you're talking about these being
the connection strengths, the parameters
of your structure in a mills like context
or the strengths of your connections
or weights in a machine learning context.
But the only difference, I repeat, is just the time scale.
So you're talking to Mike's example,
how can you have memory without changing your connectivity?
Well, you're just appealing to initial conditions
in the context of a nonlinear dynamical
and random dynamical system.
I mean, at what point would you start calling this
the kind of memory that could be encoded
in terms of connection strengths?
Well, in those kinds of systems where these,
the key not second order nonlinear interactions
rest upon a subset of variables
that change very, very slowly and you say,
well, okay, under that adiabatic approximation,
then we'll now call this a different kind of memory.
And it's just because it's slightly slower.
So, you know, I thought, I think it's,
I liked the emphasis on the separation of timescales
because I think that would have dissolved
the reviewer's concerns
if you were just talking about like really fast learning
in the moment that is all in the nonlinearities
and the dynamics.
I keep emphasizing the nonlinearities, Mike,
because of that sort of the paradox of change.
So, as soon as you have nonlinear dynamics
in any system that has at one particular timescale
an attracting set or a random or a pullback attractor,
you have that itinerancy,
which means that there will be some form
of changing sensitivity to all the things
that I am coupled to.
That is definitional of things
that have that biotic or sort of characteristic kind of set.
So, you know, the nonlinearities,
I said it from a classical perspective,
I think they're absolutely key here
and resolve a lot of the distinctions
and give you now a relatively simple picture
that if there was some way to tell the next version of me
where I started, give the next version of me
my initial conditions and the past version of me,
you can, you know, I would imagine quite simply
just write down systems that have this kind of memory,
which does not involve it anyway,
a change in the connection weights.
And I'm just wondering whether that, you know,
that if you wanted to simulate that remarkable fact
that the worms remember
that they are on a two-headed trajectory,
even when they start again.
I mean, I think the deep question here is,
how on earth did they inherit the initial conditions
that, you know, characterized the, you know,
the termination of their parent
or what they inherited from?
I think, again, that speaks to this coupling
between different temple scales,
you know, is this a messenger RNA, you know,
and how does that propagate through to the electric fields
and how does it get back top-down causation,
get back in again, it's a fascinating example.
And I've heard that before.
I'm sure you've told me,
but I probably ignored it because it's so remarkable.
Not easy to explain.
In answer to the question,
can you ever know what's going on inside a system?
No, and I say no polemically
from the point of view of the Fianco principle.
You can never know what's beneath a Markov blanket.
You can never know what's on the other side
of a holographic screen.
That's the whole point of a holographic screen
or a Markov blanket.
All you can do is bring a best guess
and as if explanation to the polycomputing,
if you like, in the bulk on the other side,
which means, you know, I think that's simple observation.
The whole point of that screen or Markov boundary
is that there is a conditional independence
given what you can and measure.
So you can never know other than infer
by what you measure from the behavior,
the inputs and the outputs of a particular system.
Amazing.
So two questions then.
One is, is there, is it just a flat no?
Or is there a degree that is easier to know
for certain kinds of systems?
And then for sort of advanced living cognitive systems,
it's really no.
Or is it just like, is it always the same?
Or is it a matter of degree?
If you're directing at me, the answer I'm afraid
is always no, but I don't mean that
in a sort of pessimistic or, I mean, you're,
the question, you know, how do you infer
what kind of Bayesian mechanics or polycomputation
is going on underneath the Markov blanket
or inside a cell or inside a brain?
That question is, of course, my day job
and the day job of nearly every neuroscientist.
It's peaking underneath the Markov blanket
in a noninvasive way that doesn't destroy it
to try and understand the mechanics
and to test hypotheses about what is going on.
But you're always testing hypotheses.
You will never know.
So there will be situations where the functional anatomy
or the architect reveals itself
through noninvasive imaging, for example,
or even invasive techniques of the kind
that you use every day.
But all you're doing is basically testing hypotheses
about what you think the gerative model is under the hood.
And once you know that, or in another way,
if you knew the gerative model, then you know the Lagrangian.
If you know the Lagrangian,
you know the intrinsic or internal dynamics
and you can tell a story about polycomputing,
tell a story about Bayesian mechanics.
You can tell a story about perception.
You can tell a story about memory.
You can tell, you know, basal cognition.
But these are just stories predicated on the Lagrangian
that governs the intrinsic dynamics
and all the free energy principle brings to the table
is that you can express that Lagrangian
as a function of a probabilistic gerative model.
So your job is now to identify the functional form
and structure of that model
and all the processes that it entails.
But every time you do that, you're just testing a hypothesis,
you know, theory of mind for me
and theory of mind for your Xenobox
and theory of mind for yourselves.
Of course, you know, you can break down at different scales.
So, you know, it would be possible to ask about the sense-making
and sentient behavior of a single cell in my brain
if you were able to isolate it and get inside there
and do molecular biology or do cellular biology.
But you would no longer be looking at my brain at that scale.
And so, you know, but you could sort of cut across scales
in the good old fashioned way
and start to tell an internally coherent story
about how it all fits together across scales.
Maybe Carl said quickly not to interrupt you, Michael.
Go for it.
And it's partially going to be a question for Carl
as he triggered an interesting thought
and then just to quickly say to you, Michael,
that's also why I was hesitant because by definition
and since mortal computation rests on the Markov blanket
and it's underwritten by the free energy principle,
I also commit to that as well that, no, the answer is no.
You can't see what's under the Markov blanket.
And that's why I was curious to know if Carl
would give me anything that may I just didn't know about.
So, that was one comment to you.
To Carl and to everyone, really.
So, mortal computation also subsumes artificial systems.
So, this is where I was curious to know
if Carl, we were to design the internal states,
all the dynamics.
We, again, we have the environment,
but let's just say you dissimulate it.
You're designing the environment
and you design the Markov blanket.
Cause we talk about even in our paper,
potential sketches of things that you could use
to build the boundary and the transduction pumps
and all the sub pumps and, you know,
to actually build a viable artificial organism.
Now we have the internal dynamics.
We have specific, we are the designer
and we have specified internal, external and the boundary.
Is there something I'm missing that we would still,
cause we've created the Markov blanket.
So, now we know what's on the other side.
So, the answer to Michael is not for natural systems
that we obviously cannot, you know, did not create,
but we made it.
So, we made the internal systems.
There's some other concept I'm missing
cause you would be able to now say,
I know everything cause I built the internal states.
I specified every bit of the dynamics.
And let's say the environment, you know,
we've constructed that.
We've constructed the Markov blanket, the boundary.
What about that case?
Is it, now we are inspecting it cause we made it.
So, we obviously don't need to infer it.
We know it.
What about that?
I was curious to know your thought of designed internal
states and designed external states
and designed Markov blankets, if that makes any sense.
Yeah, Mike's gone to the door.
So, I'll respond to that.
So, yeah, I'm not gonna give you
any deep philosophical insight.
You don't already have, but just a very practical one.
I mean, you know, what you just described
is an application of, well, the free energy principle
as a method to simulate various mortal computations
in the service of building hypotheses
about how this thing might work mortally.
So, practically, that's what we use
the free energy principle for.
And the design is, at least mathematically,
very straightforward in the sense, I repeat,
all you need to know is the gerative model.
So, all you need to be able to write down
is a probability distribution of all the causes
and consequences that constitute your system.
That's it.
If you can write that down
and you can instantiate that in a von Neumann architecture,
you then just solve the equations of motion
that are the gradient flows.
We'll have a certain amount of component on that Lagrangian
and you can simulate sentient behavior
and sense-making, perceptual and actions,
self-organization, everything that you want to do.
Why would you ever want to do that?
Well, in order to test hypotheses,
that this reproduces the kind of behavioral thing of interest,
which is something like maybe a psychiatric patient
for something like might be a multicellular organism.
So, you are now using a simulation
as a way of generating predictions
that then you can match against the observable parts
of the system of interest, which are just the surface.
Yes, just the action on that system
and to the extent that the sensory inputs of that system
are also known, that's all you have access to.
So, literally, that's how we practically use
active inference, for example.
We just create simulations of Bayesian mechanics
in a given paradigm
and then we adjust the gerative model
more specifically the priors of that gerative model
until it renders impurity observed choice behavior
the most likely under the probability distribution
of the actions of my simulated simulated.
So, in that sense, you're specifying the structure,
but one could argue that even treating the laptop computer
that is so non-unique
because it affords the opportunity
to abstract and do these simulations,
come about to Chris's point,
even then you don't actually know
what's going on underneath the hood.
And certainly in conversation
with people designing some risk architectures
and looking at the most efficient buses,
they have to guess what's actually being passed here
and there and measure it and get proxies like temperature
and that kind of thing.
You can certainly specify the initial conditions
and the structure and you can do a,
you can reboot and reset it.
So, you can to a certain precision specify
the initial conditions and the structure
of which the, you know, that the ensuing dynamics will occur,
but to actually know the message passing of a computer,
even in simulation, I think would be,
I think you would be able to return to your hard no.
Certainly on the level of, you know,
the quantum level that Chris was referring to,
again, it would be unknowable.
But it's an interesting point,
but it does foreground the role of simulations in this,
I think, and it comes back to, you know, this, you know,
why do we want to know all this?
Well, it's just to basically build,
formalize hypotheses in terms of simulations
that now embody our hypothesis
and then look at the empirical system
to see whether, you know, that hypothesis was correct.
Can I just add another point of view on this?
And if we think about what we do in practice
with ordinary computers where we have built the thing,
et cetera, part of building the thing,
it's not just assembling the hardware.
We also put a lot of work into building these interfaces
that we call programs.
And so if I'm using some sort of debugging tool
or something like that,
where I can run a program in one window
and see what's happening
at some level of the execution trace in some other window,
what I've done is constructed a Markov blanket,
effectively, to use that language,
that has a bunch of different IO channels
that access different parts of what's going on in the device.
So we could think about, from a biological perspective,
we have these cells that come equipped
with their own native IO channels,
but there's nothing that says that we couldn't,
in principle, build some more channels into the things
so that we could see more about what was going on
in the inside, not by penetrating the Markov blanket,
but by adding some IO capacity to the Markov blanket.
What does that mean physically?
It just means you're using a different interaction,
because it's the interaction that defines the blanket,
as a set of information transmitting states.
So I think we always have the hard no
of the Markov blanket,
but we also, from an engineering perspective,
because we can interact with these systems
in ways that other parts of their environments
can't interact with them or don't interact with them, at least,
we're a part of the environment that can open up
new communication channels through the blanket
by changing the interaction that effectively changes
the state space in which the blanket is defined.
Augmenting the Markov, say, with reporters,
optogenetics, I would imagine, is a good example of that.
Well, in a sense, FMRI is a good example of that.
Yes, yeah.
Right, we were just adding an IO channel to the brain
that wasn't there before.
I want to know why.
Sorry.
No, no, please, Carl, keep going.
No, I was just thinking out loud, you know.
So the catch word in my world is sort of non-invasive
brain imaging, and that has a meaning,
that you're non-invasive,
but there isn't a whole century's worth of legacy
of non-invasive studies and lesion deficit models
and depth recordings and the like,
which I think speak to how far into the Markov blanket
can you peer without destroying the thing
that you're trying to, in the Heisenberg sense,
trying to get out.
I was wittering.
I'll shut up and have a quiet cigarette
while I listen to you now.
I just add that it's non-invasive kind of by convention
and that you're invading the brain with a magnetic field
that wasn't there before.
You're just not damaging it much.
Like Carl said, it's invasive,
but I wonder what that tells us then
about augmenting the Markov blanket,
because yeah, Carl usually will say,
if we want to non-invasively understand it,
well, then yeah, you can't peer under the Markov blanket.
So mortal computation, I think,
kind of is connecting towards the idea
of what you're saying, Chris, right?
Because we can augment, we can add IO channels.
You are designing, engineering these things.
So now this was something that did not exist.
I don't know, Michael, how does that shed light
on the question that you originally wanted to get at,
which is peering at these internal states?
Because I think this is like an indirect way.
Because Carl gave me great answer about,
well, if I just design it,
because my brain goes to the ultimate engineering,
I'll just design it all myself.
And I even was thinking about if I build the hardware,
but Carl's right, even when you get to hardware,
you're guessing a lot of times,
even with the best educated guesses.
So there's still the Markov blanket
that you're not really breaching,
but if you perturb or change,
it's even like augmenting the cell membrane
with something in your lab's group
is an example of modifying these things.
What does that do to your question?
How does that shed light on what you're thinking?
Yeah, I mean, I even wanted to talk about
a much more annoying aspect of this,
which as I always do, which is to take it way down.
So nevermind brains, nevermind even cells, right?
My question also extends like that transition from,
you got a pendulum, you got a thermostat,
you've got a, you know, right?
And you can sort of build these things up
and then eventually at some point you get to a cell.
So I'm still curious about whether
this impenetrability goes all the way down.
So we can't read the mind of a pendulum either,
or there is some sense of progressive opacity
as you climb this sort of continuum of cognition
from extremely simple systems,
where I think the conventional story is,
hey, look, it's all third person accessible.
We know exactly what's going on,
but at some point you don't.
And so I'm curious, when does that happen?
And if we think there's a phase transition here,
or if we think this is smooth,
I mean, I tend to think everything is more or less smooth
in these cases, but maybe I'm wrong.
Yeah, that's one thing I wanted to kind of probe a little bit
is how does this play out when you start not,
not start at the brain, you know, where, okay,
we can all agree that's sort of very opaque,
but what about from the most simple physics systems, right?
How do you get to that opacity?
Yeah, and then on the flip side,
and I'm conscious that I don't want to monopolize this up,
we only have five minutes left,
but something that's very interesting,
I think an implication of this is that we can't really know,
and we're all inferring,
if you take the approach that I took in this memory's paper,
where your future self has to make a lot of guesses
as to what your memories mean,
because they were written down by your past self,
and you don't have all the metadata,
you have to now interpret these compressed n-grams,
then that leads to this kind of more,
a little kind of disturbing question is,
so we can't even tell what we used to think really, right?
We can sort of guess, but we don't really know then,
if this is the case, if that impenetrability holds,
then it's there with respect to our past self
and our past memories too.
So that's kind of wild.
I don't know what we all have to say about those.
I'll refer to this wonderful thing
called the Conway-Cocon theorem,
Conway of the Game of Life,
and Cocon of Quantum Contextuality.
This was published three decades ago now,
or something like this,
but they proved using mainly relativity theory
that if they considered a generic observational scenario,
and they said, if in any generic scenario,
what you consider the observer has free will,
in the following defined sense,
that what the observer does is not completely determined
by that observer's past like cone.
So if the observer is not subject to local determinism,
then the thing being observed
is not subject to local determinism,
and at the end of the paper, they say,
so, one could ask, do we really mean that electrons
have free will in the same sense that observers do?
And the answer is yes.
They say in their paper emphatically.
So if you-
This is the one thing they did on the free will theorem.
Yeah, I remember catching this.
Yeah.
And if you drag quantum theory into the picture,
then you've got to be able to see
and if you drag quantum theory into the picture,
then you get an equally strong result
that any system has to be able to effectively choose
its own semantics for how it interprets
whatever incoming information is.
And if you take that choice away,
then you get entanglement.
So the system ceases to have a separate identity.
So I think the answer,
kind of the principle answer about opacity is,
yeah, it goes all the way down.
Can I just follow up on that
and refer to Chris's in a screen hypothesis
and the notion of an irreducible Markov blanket?
If you've got a system that has no internal Markov blankets,
has no deep structure or hierarchical structure
then that is, I think, where the hard no would apply
or the hard yes of unknowability.
But clearly, if it has internal Markov blankets,
you can peel away and invasively
or non-invasively start to get within that.
So I think what you're talking about
is in that sort of vague gradation
of things that are noble and unknowable.
It's just the hierarchical,
well, the depth of the Markov blankets
of the Markov blankets.
And there is a kernel of either an irreducible Markov blanket
which you can never get into
because you change the thing itself.
But there's also a limiting case
from the point of view of classical physics
which is when the internal states are the empty set.
So things like inert particles or stones
don't have internal states,
they just have Markov boundary states.
So I think you're absolutely right to think of this
as a gradation, that there are inert things
that are defined operationally
in the sense that their internal states are the empty set.
And you could also say that there are sessile things
that don't have active states.
So they are just complete, all of their states
of this kind of particle are just sensory states,
they're just inputs, inputs that can also influence the outside.
There's no restriction that sensory states
have to not influence the outside.
So there still can be observable.
And then you get to things that now have a non-empty active
sector of the holographic screen or the Markov blanket.
And these are things that move.
So you might think these are natural kinds
that have mobility or motility.
And then you get to things that whose internal states now
have a Markov blanket within them.
And these would be the kinds of things that can fan.
And these are usually multicellular things
or certainly compartmentalized things.
I think at each stage, the noability depends upon
whether either the internal sets are empty or not,
or they are irreducible in the sense
that there are no Markov blankets within those.
I think it's a nice, simple mathematical picture of that
gradation that speaks exactly to the electron through
to the pendulum, to the thermostat through
to a smart thermostat that starts to worry about
whether you want it warmer or colder or not,
and starts to plan ahead and moves from homeostasis
to allostasis.
All of this would speak to at different scales,
equipping Markov blankets and Markov blankets
inducing a deep structure.

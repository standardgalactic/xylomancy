I appreciate that.
Yeah, I'm really looking forward to this.
I think you guys both had some great ideas recently
and I'm looking forward to it.
Can we just start and maybe,
let's see, Ben can go first and then David,
just say a couple of words about who you are
and what your interests are.
I'm a former stockbroker with the background in economics
and I'm currently studying the economics
of collective intelligence,
looking to put together an understanding
of how people get along and accomplish things
that no individual can.
Cool. I am by training a stockbroker engineer,
but I've been into AI ever since the high school
reading and each of Intelligent Machines
and then just kept dipping in and out of the AI field
and kind of being like, it's not ready, it's not ready.
After college, I co-founded one of the first AGI startups
with like Shane Legg,
one of my co-founders was our first employee.
So like back in 2000s and then went to work at Google
and Facebook and then I saw mostly either working on ML,
but most of my time has been building
like large-scale distributed systems.
So really coming at this more from an engineering background,
but also about three years ago,
I left my job and dove back into AI
and came across Carl Friston's work.
And I think that's when it kind of all clicked for me
and I was like, wow, we have GPUs now,
we have a theory of intelligence
that I feel like basically covers.
It's like, I don't think there's any more secret sauce.
It's like we kind of, the formulas from ActiveInference
look a lot like formulas from traditional reinforcement
learning, we have the hardware now.
So I just like got really excited about it
and have been working on it.
So that's like my software and engineering side.
I also co-founded Plurality Institute
with Glenn Weill, who's like an economist
and that's more on the human collective intelligence side.
Essentially, we're trying to create a new field,
academic field called Plurality Studies
that focuses on scaling human cooperation and collaboration,
essentially collective intelligence as an academic field.
And we've been bringing together researchers
from like political science, computer science, economics,
peace building.
I mean, it's such an interdisciplinary field
of like how do we help humans cooperate
and using technology and now especially AI
to bridge people.
So that's kind of my other project,
but my main research focus is,
and I can share a little bit of like the research
that I'm working on and I would love help and collaboration
and also would love to hear how you guys are thinking
about this and if I can help,
because I think this is like the most exciting thing
that's happening in the world right now.
Cool. Yeah, please go for it.
Yeah.
Yeah. So I will say, yeah.
So Glenn Weill is an interesting name
because I think he's got a very important idea,
which is that we should be using markets
to solve more problems because there's basically a perspective
in economics that says anytime you've got a problem,
there's a market that can solve that problem.
And people just haven't been creative enough historically
about what kind of markets you can use.
Like we're just very used to like trading
like standard property rights and things.
And there's a lot of other ways you could conceive
of doing things.
I like the fact that Glenn Weill is pushing that.
I'd be interested in it.
So I don't actually know what he's doing.
So actually, I'm just curious about that.
I don't actually know what he does with AI.
So can you just tell me a little bit
about what he does with the Plurality Institute?
Yeah. Well, Plurality Institute is not,
is more of the academic like collective,
human collective intelligence of which AI is like a tool.
So for example, and I mean, markets are one
collective intelligence system.
But then there are other ones like even like common sections
on forums are a collective intelligence mechanism
where humans come together to synchronize on epistemology,
like on things that they know and believe
and being able to use LLMs to say automatically deescalate
flame wars in common sections
and drag people to consensus is like an example.
In terms of the more economics things,
so using quadratic funding for like public goods funding
is something he has been writing about,
creating identity and like trust networks
to create like social identity
so that you can use that as infrastructure
for all these other like market mechanisms is another one.
There's someone that's working on vector prices.
So rather than there being a single price signal,
there's a whole basket collection of currencies
that you hold and then people can specify
their own values for different currencies.
And then the way the trading works is basically
when you're trading with people that are aligned
with you on values, you automatically get discounts
because they value your currencies more
than they value some.
So there's like a whole bunch of them.
He just has a book out, open source isn't like another
like how large open source projects are incentivized
and how collaborations occur
without like a corporate structure.
He has a book out now called plurality with Audrey Tong
who is another inspiring person.
So I would check that out and that's like
is gonna cover a lot of different aspects
of like plurality and collective intelligence.
Cool, is a lot of that just on the plurality website?
So the plurality.institute website,
we like post events and talks
and we have to like to the plurality book
but the plurality book is like much more detailed.
Cool, I will check that out.
So maybe what I can do then is just kind of give a broad
overview of what I think the basics of economics
of collective intelligence are
and we can see how that relates to your work.
So the way I see it is that collective intelligence
is all about getting people to communicate
in honest ways, in honest and relevant ways
such that everybody forms a shared model
and because everyone's behaving with that same shared model
everyone ends up behaving as if they're all following
the same sort of commands if you will
of like a dictator or a virtual governor
and everyone ends up nicely coordinated
to achieve that kind of signaling system.
You need something that's analogous to a price system
and what that allows you to do is it's principles
very similar to what Mike has talked about
in some of his work where you get a way of getting an entity
to see questions about other people
as questions about itself.
And so like information gets transmitted
in such a way where like when someone else is in trouble
you perceive that as stress to yourself
when something benefits someone else
you perceive that as benefit to yourself
which we see very easily in the price system
like if someone needs more food
they'll have a demand for food
that'll raise the price of food
so that stress gets transmitted to everyone else
which uses to buy less food
or similarly if you help someone get food
by supplying some food that they like
their benefit becomes your benefit via the revenue
you make some money.
So that very simple system ends up outlining
really just the basic functionality
of a collective intelligence.
I think that any collective intelligence
probably follows basic price like principles
where you need some system
where people aren't incentivized to communicate
honestly with each other
and such that everyone's signals gets condensed
into like your own personal situation.
So just by optimizing for yourself
you end up inadvertently taking care of everyone else.
So are things like that
when you're looking at like AI and technology and stuff
like I don't know what goes on inside a computer
it's all magic to me.
Is that what's going on with like the transistors
do they all have their own like individual model
and they end up like communicating to form a shared model?
You know, I mean, I guess it's not level
everything is doing all of this, right?
Everything is like mutually reducing free energy
and you can feed every part of the subsystem as an agent.
So at some level, yes, but I think what you probably mean
is more directly in a computational way.
And like typically what we do now
is we train neural networks
which are a big pile of math
but essentially it's a thing that like
learns function approximations
and the way it learns function approximations internally
and we're still studying, you know
what actually happens internally
but probably yes, things get partitioned
into sub processes like sub functions
with sparse or communication between them.
So kind of what you're describing
like if you think like in your model
an individual is this like very interconnected system
and then it has these sparse connections
to other deeply interconnected systems
and this is like, you know
a cell is doing a lot of compute inside
but then it's got just like a few membrane exchanges
with outside or corporation.
There's a lot of stuff going on inside
but it communicates with the rest of the economy
via buying and selling products.
So in some sense anything that you partition
into subsystems is essentially doing this.
It is using some form of information
like sparse information propagation between units.
In terms of honesty, I think that's a lot trickier
because it's like the communication
doesn't need to be honest.
It needs to be valuable to like
I think there is like a theorem
that like for communication to exist
it has to benefit both parties.
And so there has to be some value to it
you can maybe say that that value is the truthiness
and so you could like maybe decouple it
into like signal and noise.
So there's like some truth value
and then maybe there's like a bunch of noise value.
If it's all noise that communication channel
is just not gonna persist.
And so in terms of I think when you're like
hey there should be honest pricing mechanism
I would instead say there should be the two agents
or whatever the network of agents
there needs to be ways for them
to communicate usefully with each other
which it maybe is the same thing
but they kind of co-discover what that is
and that doesn't necessarily need to be imposed
by a mechanism.
But if a market mechanism
or some communication framework
allows honest communication channels
then you're making it much easier
for them to discover how to use it.
I don't know if any of that resonated.
It does and maybe there's something interesting
to work out here.
So economics has been talking about the importance
of getting people to communicate honestly
to achieve coordination for a long time.
It's sort of the basic picture of that
which is very silly and false in some ways
but also maybe important for intuition
is if you're trying to centrally plan an economy
one conceivable way you do that
is you have an allocation problem
who should get what goods
you could just send everyone like a giant questionnaire
and say like do you need a new pair of shoes
and the problem with that is that people could lie
and they could say, yeah,
I totally need a new pair of shoes.
And so there's a lot about that picture
that's very unrealistic
but figuring out ways to get people to,
honesty is maybe a word that has some bad connotations
because we interpret in terms of like deliberate intention
and like moral character.
It's really more just about like
truthfully conveying your intentions.
So basically maybe this is a way of thinking about it.
You can sort of think of economics
as a very complicated version of like a traffic.
You're just trying to get cars to not crash into each other
and that means that each car needs to predict
where each other car is going.
That means each car needs to be sending very clear signals.
I'm definitely going here
and I'm not gonna like turn suddenly or something like that.
So you're trying to get people to convey
what their plans are
so that everyone knows what everyone else is planning
and so then they can pick a plan
that's consistent with all those other plans.
And I guess this is valuable.
It's like much more of like
I guess an information theory kind of take value.
I think there's another thing
that is missing from that model
which is that if you have like a really narrow freeway
then some cars are gonna get through
and some cars are gonna have to wait
and there's not an easy way to resolve which ones.
So I think and this is kind of back to the plurality way
that I've been thinking about it is like
these are all forms of alignment
and I break alignment down into like aligning on action
which is kind of what economics in some sense lets you do
is like, okay, we all have common beliefs.
Given our beliefs, what should we do
and how can we coordinate our action?
How can we all send our routes
so that we don't like collide?
So that's kind of aligning on action.
There's aligning on epistemologies
like how do we all come to believe the same world models?
And then there's aligning on value
which is how do we actually like,
I want one thing you want a different thing.
They may be, maybe we want opposite things.
How do we align on that?
And all of these three systems are all interdependent
because if you actually want different things
you're not gonna necessarily even align on epistemology
because the things you know
only things that you care about enough to know.
And if you care about different things
you're not even gonna necessarily ask the same questions
of the universe to get back the same knowledge.
And if you have different beliefs
you're not gonna align on action
and they all have to be iterated.
So like if you can agree on the same epistemology
then maybe you can shift both shift your values
because what you value depends on what you know.
And so in some sense it's like you have to
pull all of these three systems in
and I think try,
and that's another cool thing about economics
is it also does help value alignment
and knowledge alignment
because prices are knowledge signals
so they tell you what goods are easier to get when
and they're also value alignment mechanisms
because they allow you to trade
something that you value more
for something that you value less.
So I think there are these pieces
but they all have to kind of play together.
So I wanna go in a couple of different directions with that
and maybe the first thing I'll mention
is actually pulling away from economics a bit
and talk about some neuroscience.
So on a different comment I left on Mike's blog post
I talked about the work of Lisa Feldman Barrett
who's done super important stuff.
It's actually her work that really prepared me
to understand what Mike's talking about
because I don't know anything about biology.
And the active inference sort of predictive coding view
of things that she's really advancing
as a way of understanding what the brain does.
You know, sort of we do often naturally draw these divides
between knowledge, acting and valuing
and the active inference way of viewing things
very much pushes all of that together.
It's actually more like action that drives perception
and really your goals are at the base of all of that
but also your goals are determined by,
I think of goals as basically like beliefs
or measurements that you have.
So like your goals are in a sense your epistemology
because your goals are your measurements
about the world and your prior expectations.
It's a weird sounding view and maybe we can talk about that
but that's kind of how I think about that.
And so it's very useful sometimes to separate these things out
because there's a reason we draw these categories
we categorize because there are practical reasons to do so.
And so that can be useful but also it's,
I want a fully unified perspective
where we're just treating all of that
as kind of the same thing in some level.
And in economics at the end of the day
when you're trading something
you can disagree very strongly about,
like the trade ends up compressing all of that.
Like if I'm buying shoes and the reasons I'm buying
that would make no sense to the shoe seller
as long as I like the price, you know,
everything else is fine.
So, yeah, I'm not quite sure what the balance is there.
It's, because sometimes it feels like
you do need to address things like that.
And sometimes it feels like it just takes care of itself
when the system is set up, right?
Yeah.
If you guys are up for it,
I made a couple of slides and videos I'd love to share.
I don't know if that would be fun.
Is that okay?
Yeah, yeah, go for it.
Let's see.
Cool.
Okay, can you see my slideshow here?
Yeah, we can.
Yeah, any chance for the full screen?
Yeah.
Yeah.
Is that better?
Much better, go for it.
Cool.
And this is two different ideas
that kind of work together that I wanna get both share
and get you guys' feedback on.
I'll go really fast and then we can drill into something.
But essentially, you know, the thing I'm really interested in
is how do we make an AI agent?
And like to define an agent is basically something
that takes past like a sequence of past observations
that produces the next action.
So it's just, I think of an agent,
I know Mike, you think agency has goals,
but those are externally observable,
the knowledge about an agent.
So I think that's interesting to figure out.
So for this thing, like I think of an agent
as just like some function that give it a C.
I think you're breaking up for me.
Yeah, me too.
Me too.
Sorry, David.
I don't know if you can hear me,
but I'm not getting any of this.
Yeah, me too.
I can't, I can't hear what he's saying.
Technology is not a friend today.
I guess he'll try again.
Yeah.
I've noticed that usually happens
when something really interesting is about to come out.
It's because it knows.
It's not a friend.
It's not a friend.
It's not a friend.
I guess it'll try again.
Yeah.
I've noticed that usually happens
when something really interesting is about to come out.
It's because it knows what we're doing.
And it doesn't want us to make progress on these questions.
Yeah.
There's a little bit of that.
Yeah.
Is that background somewhere you've been?
Yeah, this is years ago.
This is Alaska.
Oh, nice. Okay.
I think I've been to Alaska, but many years ago,
I don't remember much about it.
Yeah.
Yeah, it was pretty, it was pretty incredible.
This is my blank beige wall because I have that kind of apartment.
So.
Yeah, cool. Well, yeah, I'm hopefully, hopefully he'll come back.
If no, we'll just chat.
So actually I do have something I should address with you quickly.
So regarding the paper we've discussed,
I just want to briefly mention to you,
I don't know a lot about collaborating on papers.
And if there's something that you might expect a collaborator to be
doing right now while you're working on other things,
that might not be obvious to me.
Is there anything specific that I should be doing right now?
Yeah.
Remind me, remind me where it stands. You have you, you've sent me a draft.
I sent you an outline.
Yeah.
Then it's stuck in my court. So I've just been in pain.
So I will, I will get back to you with the next couple of days with it.
So yeah. Yeah. And no rush. I just literally wasn't sure if there was
something.
It's not you. It's me. I've got a, I've got a stack of drafts on my notes.
I will get to it. Yeah.
Okay.
Let's see. David is.
We'll be back.
Can you hear me?
Hi, David.
Hey.
Hey, sorry, my computer decided not to run zoom anymore.
So I'm on my phone.
Okay.
Let me try to just do this without the slides.
But basically.
The idea is that.
Like an agent is essentially some function that maps past observations
to, to the next action.
And general intelligence is essentially a set of behaviors of like,
I think in your paper might have this thing where it's like,
how do we navigate a space adaptive?
And how do we do that?
How do we actually navigate exactly and intelligently learn to
navigate arbitrary spaces?
And I think of that as just like a collection of.
Algorithms that let you balance exploration.
Exploitation. Essentially it's active inference,
but active inference requires perfect Bayesian inference.
And really everything just implements a, an approximation.
So the question is like, how do you learn or build an approximation
and neural networks are function approximators that instead of
building, you can train given the right input output pairs.
So the idea is to basically train a,
a neural network.
That's a function approximator of intelligence space navigation.
And I think the way to do that is agents are basically
duals of their environment.
So if you want to train an agent to do something,
you need to give it an environment where achieving fitness,
that environment gives you the, the behaviors that you want.
And so I think if you want an agent that's able to generalize
learning, you need an environment where there's always something new to
learn.
And that the behavior space needs to be dense enough that the agent is
able to always discover some new thing that it can learn.
And that gives it adaptive fitness to the environment.
And therefore, as it is learning how to adapt to that particular
environment, it is also generalizing learning.
It is meta learning learning.
So the idea is if you can give an agent always some new environment
where there's something new to learn, it's going to learn that.
And it's also in the process of doing that.
It's going to learn how to learn.
It's going to learn how do I balance exploration,
exploitation, what algorithms, what sampling algorithms can I apply
to this new space?
How do I leave myself and Grahams?
Like how do I store memory?
How do I structure memory?
How do I interpret memory in these various ways?
And so what you want is an environment that essentially gets
harder or different as the agent gets smarter.
And I think the way to do that is make the environment,
I think the way evolution did this is make the environment
highly multi-agent so that there is complexity in the physics
of the environment, sure, but really most of the complexity
is in the minds of the other learners.
And this kind of goes back to economics and capitalism where
markets are kind of anti-legible.
As soon as you find out some trick that gives you an advantage
in the market, that advantage goes away
because it gets arbitraged away by everyone else that learns it.
And so I think you kind of have this with multi-agent setups,
you kind of can get into this infinite game where as the minds
of agents get more complex, you have to get more complex to be fit
in that environment.
And this is kind of just like creates this intelligent treadmill.
And I think this has been done in AI.
So this is how we get like go player or chess players.
It's like they do self-play against themselves.
And as they get better, the game gets harder.
But I think typically this is done in purely adversarial settings
where, and with a fully adversarial setting,
you basically cannot explore a lot of the behavior space
because as soon as you deviate from some dominant strategy,
you get exploited by all the other players.
And so it's very easy to just get stuck at local minima
because there's not a lot of ways to deviate from local maximum.
You basically have to really discover something new really quickly
or you just get outcompeted by people that are doing the same old thing.
And I think the fix to that is basically blending this line
of what an agent is in this collective intelligence sense.
I think one really strong technique for that is kinship.
So if you're in a world where other agents are, say,
a lot of the agents around you are your brothers or your clones
or your cousins, essentially this whole spectrum of kinship,
then another way to think about agency is kind of, you know,
if you have two agents that share the same goal,
you can think of them as one agent with just like really bad cognitive architecture
where instead of just being, it has to like learn how to pass messages to itself.
So one way to define agency is via essentially goals.
And kinship is this way of aligning agents on goals.
So if you have an agent that 100% shares the goal with another agent,
it's basically one agent.
If it's 70% sharing goals with that agent, then, you know,
it's kind of this blended super agent.
And so the idea is if you have an environment where all these agents are aligned
with each other with various different kinship relationships,
you can create this really high dimensional behavior space where you're not always competing.
There's a million different ways to cooperate, form coalitions,
break coalitions, establish trust.
And so the idea is, you know, can we create a relatively simple,
fast to simulate in silico virtual world full of other agents.
The agents are aligned with each other in various kinship scenarios.
And then we scale off neural, then we trade larger and larger neural networks
to drive those agents and get to LLM size, you know,
billion parameter models that instead of predicting the next token are predicting
the next action that an agent should take given all of the experience at scene.
And so the idea would be that if you give an environment like that,
you're essentially providing a gradient towards increased intelligence,
and then you're using standard machine learning techniques to train a function
approximator to follow that gradient.
So that's kind of the idea behind the overall environment.
But then the other thing I wanted to incorporate is this collective intelligence approach.
And I really wish I could present my slides.
I don't know what happened with my computer.
Maybe I'll pause for a second, let other people speak.
I don't know if anyone has feedback or thoughts or criticisms.
And then I'll try to see if I can get this other slide to work meanwhile.
Yeah, thank you for that.
I mean, I have certainly heard a somewhat similar sounding theory that evolution
evolved or intelligence evolved as some sort of, you know, social competition,
that, you know, primates competing with primates and birds competing with birds
is where it comes from rather than trying to like, you know, build tools and so on.
So, yeah, the idea about, yeah, you need a form of like some kind of protection,
some ability to innovate.
I mean, there's a lot of aspects of economics where there's this challenging tension between wanting to optimize
and wanting to create room for innovations like perfect competitions, a classic example,
like you learned very early on, perfect for competition, you're going to maximize price,
maximize output, your price is going to be as low as it should be.
But it's very hard to innovate in perfect competition because the way the model works is whatever new technology you introduce,
everyone cocks that instantly you make no profit.
So, but you're the one who made the investment.
So that just sucks.
So a little bit of monopoly can actually be helpful because that way you're making money.
And then there's even artificial things like, you know, like patents and stuff that give people monopolies to try to incentivize them to make those investments and innovate.
So that is a very important aspect of intention of any sort of collective intelligence system is trying to encourage parts of your system to improve and do better
and trying to protect them from parts of the system might say, Oh, we're going to copy that we're going to take that we're going to steal that.
There's a part of this that maybe I know you're trying to get your slides up but I saw in your talk you talk a little bit about like love and stuff at the end of that.
You have a talk on YouTube, and that's something that I want to bring up because I am working on a couple of papers about moral psychology and neuroscience, including a paper on morality as a form of cognitive glue, which
it isn't really but it's a sort of an interesting counter example in some ways.
And basically,
there's a model in which can ship. Let me ask you this, if suppose that you took all the families and you rearrange them like people didn't know that they were in the wrong families but you had families where no one's actually related to each other genetically.
Would the system still work yet.
I think of this as like, I think of this as essentially reward sharing. So on a genetic level.
The genetic optimization process gives rise to organisms that care about their kin, right so because the gene is present in the kin that it's going to make the organisms that come from it.
Want to help other organisms that have that gene from the genes perspective, it doesn't matter whether you reproduce or your brother reproduces twice.
So, and similar. So essentially, the behavior that an organism exhibits that we call love is basically what it looks like when the underlying optimization process shares rewards or goal or like shares rewards between organisms.
And the organism itself, one of the things that it evolves or learns or whatever needs is the is some way to recognize who it should care about so we have all of these heuristics for knowing who our kin are.
Once those things are in place. It's the heuristics that matter not the underlying gene. So, I think if you train agents that see other agents, and you give them a reliable signal that hey, helping this agent is actually the same as helping you so every time this agent, something
good happens to this agent, you get part of that reward. And here is a marker that tells you who is kin and who is not the agent is going to learn behaviors that are conditioned on that marker.
And then once you stop training and you just let these agents go around behaving, you can change that marker arbitrarily rewards are no longer even there right genetic evolution, or whatever has stopped now, you're no longer learning new policy now the learning is happening inside the mind of the organism, not inside the genes.
Okay.
Condition behavior on kinship markers.
Does that make sense?
It does. And there's a very important paper that anyone interested in this should read it's called the sense of should a biologically based framework for modeling social pressure.
And it's always maybe the difference is subtle but I think there's a way of looking at these sorts of morally motivated cooperative behaviors that does not in fact depend on any kind of evolved pressure to care for or intend to cooperate.
Instead, it's just like something your brain just constructs as a living organism in its environment, because it's trying to make it's in its social environment predictable.
And so there is something of emerging perspective and my papers are somewhat drawing on this research. That's basically saying, let's not think about evolution, let's not think about long term cooperation.
Let's just think about trying to make the social environment predictable. And that's how we can actually get these cooperative and moral behaviors.
And so that just might be something to look at because that's another paper release as a co-author and I just bring up Lisa's stuff as much as possible.
I mean, this is also, yeah.
Sorry, I was just going to say that that also sounds like the imperial model of multicellularity that Chris Fields and I published a bunch of years ago, which is basically that, you know, if cells are trying to predict an uncertain environment, the least
surprising thing around is a copy of yourself. And so this can be a driver for making cells stick around after you've divided to form a highly predictive surrounding for yourself so that you live in this niche and then the frontline
infantry is out there facing the uncertainties of the outside world, but you can predict them because they're you and so it's a lot easier. So you can think about it that way as well.
Have I talked to you about comparative advantage?
Yeah.
No, I don't think I want to.
Go ahead, David.
Sorry.
Yeah, sorry, I had a question about that, Michael.
I don't. So if you have a cell, and the cell does, you know, given situation a it does random stuff.
Then being surrounded by copies of those cells, and you do random stuff and your clones do random stuff being surrounded by cells that in that situation do random stuff is not actually going to help you predict the environment.
I think there's something more to that it's not that being surrounded by copies of yourself gives you the ability to predict your environment I think it's being surrounded by copies of cells that want to make your environment that want to help you make the
environment more predictable.
That actually matters.
Right, unless you think that a cell by being able to introspect itself can predict the behavior of other copies of itself.
I mean, I don't think they're likely to be doing random stuff. They're probably situations in which the responsibility is kind of random, but I don't think that's the vast majority of what cells do.
And so I do think that if if if you're, it's easier to to anticipate cycles and responses. I mean, again, I mean, I think you're right if it's random it's not going to work but I actually don't think that cells are random in that in that way.
But let's um anyway, let's let's let David do his thing.
Oh, okay. Yeah, this is just so this is this this is actually mostly inspired by your work Michael and economics. So I love to get thoughts on this but okay so this is the standard reinforcement learning formulation right you have an environment you have an agent.
The agent takes an action in the environment and gets back some observation and a reward and then it learns to maximum to act in a way that maximizes reward. And you can have a dual formulation business which is the difference where the agent is trying to predict observations.
But essentially, this is the normal kind of loop. And typically the way it's, we can break this down into like an agent basically there's a bunch of observation states from the environment that go into the mind the mind is going to generate a bunch of actions over actuators.
There's some memory that maybe the mind has that it's reading and writing from, and it's getting a reward signal. So this is how we train agents in reinforcement learning or this could be trained or this could be evolved doesn't really matter.
So the way we do this now in the field of RL is you have this giant neural net inside the mind and there's model three and model RL so they're like this is skipping over some detail, but essentially, you're training this big blob of math that you're jiggling using gradient descent to optimize your long term expected
reward. And it is learning how to compress and basically how to compress observations into latent states, maybe store the parts of those latent states in its memory that then it can reinterpret and eventually to generate actions.
And so these networks can be giant in language models, they're 100 billions of parameters in reinforcement learning they're much smaller because we just haven't figured out how to do it yet, but this is kind of the standard setup.
And what I am proposing is a different architecture.
That is essentially, instead of there being one mind. There are many individual agents.
The agent is mapped on to some subset of the observation space, and some subset of the memory space, and some subset of the action space, and you know they can be mapped to any or all of them, but essentially, rather than training one brain.
So, rather than training a neuron that in a collection, when you put it together in a graph of other neurons knows how to co organize collectively to solve the overall problem that the brain has.
I want to be training 100 billion model network, I want to train one million parameter network that when you copy and paste it and throw it into this soup of other neurons will self organize into solving the problem.
The idea is, rather than having to learn adaptive algorithms. So in, in this way, you know you can imagine that there's an algorithm that you apply to many different parts of the sub problem.
But this network has to learn how to, has to learn that algorithm in a bunch of different places because every sub function call might have the same kind of you know oh this is just like a regression problem but you have to learn how to solve regression in a bunch of different places
because here this thing can just learn okay you know here is how I divide my problem into some problems and send those messages out.
Or here is how I, you know, pay attention to a pattern happening at this time scale and that's my job.
The idea is to break down this one big blob of compute into something that can be done on this graph instead. And I think that, and this is why I wanted to talk to you then, because I think basically, I started out thinking well this is an economy.
And these things were, you know, if the observations and memory were commodities and these things were like buying and selling those commodities, or something like that that would give us.
So the question is, you know, the inside of a neural cell is a neural network that we train.
The question is what is the cognitive glue between all of these neural cells. Essentially what is the protocol that they use such that when we train the neural cell on the overall objective, they'll learn something useful.
I mean, I came up with a kind of influence by active inference and collective intelligence is so now this is inside a single neural cell right now. Now this is an agent that just sees some set of inputs.
It has its own memory.
It's going to get some reward signal from the, the cognitive glue, and it's going to produce some outputs. And I think the way to do that is that basically it, it should be predicting its inputs, active inference, essentially, learning a world model of its little
world inside the brain, and then voting on the outputs.
So that it can, and in this case, outputs can be inputs to another neural cell, or just outputs to an actuator that moves the organism around.
So I think, essentially, I'm proposing some kind of a, an agent whose that, you know, receives a set of inputs. So this is some subset and has some memory.
And then what it's going to do is it's going to predict the output at the next time step, along with some bet. So it has energy.
And it has finite energy. It's going to bet on what the values of that inputs are going to be at the next time step. It's going to vote on what it wants to set the outputs to also using this energy.
And then at the next time step, it's going to see, well, what were the actual inputs, and which bets didn't win. And then it gets energy in proportion to essentially gets energy back as a reward.
So the idea here is that these agents are incentivized to only predict the inputs that they have a comparative advantage on over other agents. So the same input and output is mapped to multiple cells.
So as an agent, you should learn to only to basically monitor your environment for models about it, and then bet on the things where you have a comparative advantage to other cells.
And then as you get more and more capabilities to model your environment, you get more and more energy that you can use to vote on steering around that environment.
This is all hand wavy. And this is where I think I need help and help from economics because I started out with pricing. Then I had it actually be an auction or your or prediction market and an auction.
And then I try to simplify it just down to just like straight betting markets. But that's basically the idea is can we come up with this like simple cognitive glue for a pool of these agents and then train those agents.
I'll stop. Yeah, no, that's very interesting. I think it's definitely an economy and you definitely should talk to economists.
I'm afraid I think a practical issue is I'm just not sure economists have actually done a lot to explore this space even in theory.
They've been content to just take the economy as it is and try to explain that. And there's been, I think, just kind of a certain lack of creativity there in terms of envisioning other economic spaces and trying to think about building alternative economic systems like this one.
So I'm afraid I myself don't have a lot of practical things to say right now that might actually be of any specific use to you other than I think just affirmation that you're right that this is economics and that talking to economists is a good idea.
I think the mechanism design people are the place to start. I mean, I think that's where Glenn Wilde comes from. But I gotta know if like Eric Maskin is reachable at all but like he won a Nobel Prize for that and I had a two minute conversation with him many years ago that was very influential.
So you might just want to see if you could send him an email and see what he says.
Yeah, I think this is cool. I'm afraid I don't think I can help you today with it but we should stay in touch and I'll let you know if anything comes along because this is very interesting and very important.

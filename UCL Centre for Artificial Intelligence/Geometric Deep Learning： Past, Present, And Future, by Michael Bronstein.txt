So maybe I will start. So I'm Michael Brunstein. I'm a professor at Imperial College London
and head of Craft Learning Research at FITRA. And today I would like to talk about geometric
deep learning. So you might be wondering what is geometric deep learning? What does it have to
do with geometry or vice versa? What does geometry have to do with deep learning? So allow me to
start maybe from taking you deep into the past. So for more than 2000 years when we said the word
geometry, it was uniquely understood as Euclidean geometry because simply no other geometry existed.
And this Euclidean monopoly came to an end in the 19th century with the first construction of
non-nuclean geometries. I think the first one is credited to the Lobochevsky, Janusz Bojaí,
Karl Friedrich Gauss himself entertained himself with these constructions, even though we never
published them. And then in 1856 Terran Riemann Gauss, a student, published what we now call
Romanian geometry. So what happened though very quickly is that basically these fields,
different types of geometry became siloed and completely disjoint from each other.
And there was a big fight between mathematicians of that time, who is right, whose geometry is
better, what actually defines a geometry and what is the relation between the different types of
geometry. So the solution to this big deal came from a Telix Klein, a young professor that was
appointed at the University of Erwanden in 1872, and as it was custom at that time in Germany,
he was asked to write and present a research prospectus, where he would outline the research
for the rest of his career. He was actually only 23 when he got this full professor appointment.
And he wrote these prospectus that entered the history of mathematics as the Erwanden program,
in which he proposed quite a radically new idea. And the new idea was to approach geometry as the
study of invariance or symmetries. In other words, properties that are preserved that are some
class of transformations. And the class of transformations was formalized using the language
of group theory, which was a new creation at that time. It was first introduced by Galois in I think
1830, but then climbed together with Sophosly, worked extensively on this. So he just used
these apparatus to formalize geometry. And in this way, the relation between geometry and what
defines a geometry became completely clear, because the moment you define a symmetry group,
basically the class of transformations that you apply, you immediately have certain structures
that are preserved. So in Euclidean geometry, for example, when they consider rigid motions,
there are many things that are preserved like areas, distances, angles. If you take a bigger group,
the fine group, then many of these properties are not preserved, but you have, for example,
parallelism. And then projective geometry is probably the broadest one. And immediately,
the relations are also evident because the fine group is a subgroup of the projective and the
Euclidean is a subgroup of the fine group. And this idea was remarkably productive in mathematics.
It allowed to solve problems that were very difficult or completely unsolvable with the
older tools. But it also filled into other fields, in particular into physics. And in physics,
Amy Neuter, who also was at Göttingen, where Klein ended up for the rest of his career,
she was exposed to these ideas. And she proved a remarkable theorem that allowed to derive
conservation laws and physical systems from fundamental principles of symmetry.
So this is, if you think of it, this is completely mind blowing, because before that,
conservation laws like conservation of energy were completely experimental. So you would observe
an experiment for thousands of time and you see that the energy is preserved. So you would deduce
that energy preservation. But here, for the first time, there was a mathematical way of deriving
conservation laws from first principles of symmetry, like conservation of energy from
local sense from symmetry of time. And these ideas had a profound impact in physics.
So Hermann Bile came up with the concept of gauge invariance that in a generalized form
was developed by Young and Mills to unify different forces, which culminated in the standard model,
the term that was introduced in 1975. And this is all physics that we know nowadays,
maybe with the exception of gravity that is not unified. It is all, I should quote Philip Anderson
and another Nobel laureate in physics, that it's only slightly overstating to say that physics is
the study of symmetry. So symmetry is really fundamental and really profound and has long
consequences. The question is, what does it have to do with machine learning? So if we look at
machine learning problems, at least in the simple setting of supervised learning, this is essentially
a function approximation problem. So we are given some function that maps us from the input space,
let's say images of different animals to the output space, let's say the labels. And in this
case, we try, for example, to label the images, whether it's cats or dogs. So the problem of
function estimation is very well understood problem. And if you look at the first neural
network systems, so the perceptrons, it became apparent that even though these networks are
very simple and they can approximate only simple functions, combine two such networks in two layers,
what is called multilayer perceptron, and then you can represent step functions. The moment
they can represent step functions, you can approximate any continuous function to any
desired accuracy. This property is called universal approximation. It was proven in the 80s,
and basically it means that these networks are very expressive. Practically anything,
any good function can be represented in this way. Now, is it good or bad? Basically, the problem
of approximating a function from a finite set of samples is very well studied. So in low dimensions,
this has been researched. To death, we have very good mathematical control over the error. We know
what classes of functions we can use and so on and so forth. But unfortunately, machine learning
is not a low dimensional problem. Most of the machine learning problems need to deal with data
that lives in thousands or maybe millions of dimensions. And there the situation is entirely
different. So this is an example of a Lipschitz function that looks like Gaussian blobs that
are attached to a bowl and positioned in different quadrants of a unit cube. So the moment we start
growing the dimensions, you see that the number of samples that is needed to represent all these
functions grows exponentially. So this is what a phenomenon we typically call the curse of
dimensionality. It has many different manifestations. This is one of them. So to approximate a smooth
function, a continuous function in the dimension, you need an exponentially large number of samples,
which in practical situations make this learning completely infeasible. And this is manifested
in practical problems. So if you apply, for example, these simple neural networks to images,
so you can just vectorize the image right into a long vector of pixels. The problem is that
if I just shift the image, this representation is completely oblivious of the geometric structure
of the input, right? It doesn't care that it's a two-dimensional grid for such a neural network,
it's just a vector. And therefore, we'll need a lot of examples to show to the neural network of
different positions of the image in order to train it to be shifted right. And this problem,
it never really worked. And this was one of the reasons why Fukushima in his paper on Neo-Cogintron
complained about perceptrons and similar architectures that they cannot cope with
shifts of the input. He applied this network to image recognition that was first attempted
around the time. And his inspiration came from works in neuroscience, the study of the visual
cortex of animals, where it was shown that there is a local connectivity, what is called receptive
fields. And this idea culminated in the classical work of Tian Likang in the end of the 80s, the
convolutional neural networks that basically take advantage of this local connectivity and way
sharing structure. Now, here's another problem. So here is a graph. So this is a molecule,
a molecule of caffeine, if you're interested. And let's say that we want to predict some of its
property, let's say buying new energy of some target. So here, as before, we can parse the
node features into a vector. But now we don't really have a canonical order of the nodes like
we might have with the two-dimensional grid. So any permutation here would work. And it appears
that graphs are extremely common in many fields of science. They can be used to model social
networks. They can be modeled to represent interaction between different biological entities
in our body. They can be used in computer graphics, maybe with some extra structure in the form of
meshes, and so on and so forth. And I think in 2015 or something like this, so when I was writing
my ERC grant, already everyone was working on deep learning. So I wanted somehow to stand
apart and I decided to call it geometric deep learning, with the idea that we want to bring
some fundamental geometric principles that would allow us to build more principled deep learning
architectures and vice versa, apply deep learning techniques that were very successful in computer
vision to other non-euclidean geometric data. And this concept was popularized in the paper that
we wrote with John Bruner, Young LeCun, Arthur Slum and Pierre van der Gijns. So now, geometric
deep learning is used almost synonymously with graph neural networks and graph representation
learning. But I hope to convince you, even though today I will be speaking mainly about graphs,
that this is just a small piece of a broader picture. And we call this picture geometric priors.
So what I'm showing today is actually based on a textbook that I'm working on with a few colleagues
with John Bruner, Petrovich Grich and Taco Cohen. So this is, in a sense, the first time that I'm
trying to present it in this way. And I hope that it works, it works well. So if we look at our
key dimensional learning problem, when our input lives in the high dimension, we need to understand
that in many problems, like in case of images, it's not just high dimensional data, it has some
underlying geometric structure. And these geometric structure forms an important and very powerful
prior, we call it geometric prior. So in the case of images, basically the image lies on the green,
right? So the way to think of it, f is a function that takes an image and produces a label,
then the image itself is an element in a vector space of functions defined on the green
that is here denoted by omega. So the geometric structure of the green is represented by the
symmetry group. And here I should say that there are many ways of choosing the symmetry group,
because, for example, if we want to deal with translations, we'll look at the translation
transformations which are closed under composition. So it is a group. We can look also at translations
together with rotations. This is what is called the special Euclidean group. And finally, we
can look at the full Euclidean isometric group, which also includes reflections. So the moment
we define the group, what's important to understand about groups that they do not model how group
elements act on other objects. So the group is completely abstract object. It just tells us how
its elements are composed with each other. So here we are interested in the actions of the group
elements on our domain. And in this case, the actions of the translation group, they just take
points on the domain on the grid or two-dimensional plane and translate it. Now, because we have an
image or a signal that is defined on this grid, the translation, the application of a group element
to the grid also manifests itself in some change in the image. And this is done in
the representation theory. It's from what is called the group representation. I do know that here by
rock. So in case of translations, this is the shift operator. And you can see what it does
to the image. So basically, it shifts it by a vector that corresponds to the group element
here by G. And this is a very powerful concept. Basically, we can define functions that
transform differently under the action of the group. So there are two ways of doing it. One of
them is called invariance. So we call it G invariance or invariance under group G.
When if I apply the group action on the image, right, first representation raw, and I apply
then a function to it, I get the same result as if I applied to untransformed image, right? So
like shifting invariance, no matter where the digit is located in the image, I still want to
say that this is a digit three. Another possibility is equivariance. And actually, these terms are
very frequently confused. So equivariance means that if the output of the function is in the same
space as the input, then if I apply first the transformation, the shift in this case, and then
the function or vice versa, first the function and then the shift, the result will still be the
same. So in other words, these two operations commute. Now, you may ask why I write here a
raw prime? Because the output of the function doesn't necessarily need to live in the same
space. In fact, in most deep learning architectures, it doesn't. So I can apply, for example,
multiple convolutional filters to RGB image and get a 64 dimensional feature space. So I will
need to use here a different representation. But I hope the idea is clear. And here I can get
a general blueprint for geometric deep learning. So we'll construct a deep neural network
from a collection of equivariant layers. So the input will be typed through these layers,
which will be applied in sequence. And this way, a transformation on the domain to the
data that is provided as input will affect in the same way the output. And finally,
if we want, for example, to the classification, we'll apply an invariant layer. I should say
that it's common also. There is another geometric prior that I will not talk in details about,
which is a scale separation. And this is manifested in pooling. Basically, I need
to coarsen my domain. If I have a way to coarsen the domain, and I can approximate
the functions by projecting them to the coarsed domain and then computing the function
on the coarsed domain. So in images, it works very well because we can coarsen the grid and
still preserve the structure of the image when we download sample it. So this is usually implemented
as a max pooling in convolutional neural networks. So you can see that a lot of deep learning
architectures, I would say probably the majority fall under this blueprint. And in fact, we like
thinking of the different objects that can be addressed with this framework as the 4G of
geometric deep learning. So the first of them is greens, right? So these are convolutional neural
networks. We can generalize that we're still working with global transformations on some
homogeneous space, such as rotations. So we can call these groups. Then there are graphs,
which have as we'll see permutation invariance. And finally, many falls will keep to the 4G.
We like calling it using the high energy physics terminology gauges, which is a physicist's term
for the selection of reference on the info. So think of it as the Erlangian program of deep
learning. And I know that by using this bombastic title, I fall into the risk of sounding arrogant,
basically irreverently comparing myself to the great FedEx client. So let's keep it modest.
So Erlangian program in the sense that we use the analogy or the philosophy of Klein
describing the structure of the domain using the language of group theory and looking at
invariance as a form of introducing inductive bias into our architecture. So I should say that
this concept is not novel at all. It has been explored in many fields of science, including
in machine learning. And if we look at the progress in deep learning, so multilayer perceptrons,
they have weak inductive bias. It's not true to say that they don't have any inductive bias,
even though they can represent any function, because usually we use regularization techniques,
such as weight, decay, and so on, which impose some function regularity. So the function will
belong to a certain class of function. CNNs, convolutional neural networks, they come from
the perspective of translation symmetry. These can be generalized with group active variance,
CNNs, so for global rotations. In graph neural networks, we have permutation inductive bias.
And for intrinsic and gauging covariance, CNNs, this is the local frame of choice.
So I would like to talk about graphs, because graphs are really universal models for systems
of relations and interactions. You can find them everywhere at different scales of problems from
nano scale modeling molecules with graphs to interaction networks between different biological
entities that just brought in for the interaction networks. And finally to macroscopic scale,
social networks, patient networks, you name it. And graph neural networks, unsurprisingly,
have become recently one of the hottest topics in machine learning. We can model practically
everything as a graph. So let's look at graphs from this perspective of geometry learning.
A graph, well, probably the first thing that comes to your mind is a social network,
where the nodes are users and the edges represent their social relations or interactions.
So mathematically, a graph is a collection of nodes, right, that are some abstract entities,
and edges that are just pairs of nodes. So we can consider either ordered pairs,
which in this case, the graph is directed, or unordered pairs in which case the graph is
undirected. And let's assume that for simplicity, we assign some vector
d dimensional features to each of the nodes. And let's assume that the edges do not have
any features, even though what I will describe next is trivially generalizable to age attributes as
well. So this is our construction. One key thing to understand about graphs, that they don't have
any canonical order of the nodes. So this is really the structure, the key structural characteristic
of graphs. Basically, when I take my node features, and I arrange them into a matrix,
I have automatically prescribed some arbitrary ordering of the nodes.
So this goes for the node features. This also goes for the adjacency matrix that I denote by a,
that describes the structure of the graph. So it has non zeros positions where we have edges
between the nodes, right. And you can see that I can choose any arbitrary order.
And the red here shows the position of the, of the, of the red node, and I can permute it
in n factorial different points, right. So P here denotes the permutation matrix that
shuffles the rows of the feature matrix or the rows and columns of the adjacency matrix of the graph.
So if I want to represent functions that work on this graph, I need to make sure that I'm
either invariant for the permutation of the nodes, right. So it means that
if I apply the permutation to the rows of X and the rows and columns of A,
I will get the same output, right. The attention that F here now depends not only on the node
features, but it also, we need to provide the structure of the graph in the form of the
adjacency matrix. Permutation equity variants will come in the form of the output of function
being node-wise, permuted in the same way as the equal, okay. So I hope, I hope this clear.
So the way that these, these functions are constructed is usually looking at the local
neighbors. So I will look at the neighbors at the nodes that are connected to my node i,
and I will take the features at these nodes. So one thing that is important to understand though,
that even though the neighbors their indices are unique, the features are not necessarily unique.
So here you can see that two different nodes have the same features. I denote them by this
blue color. So basically it's a generalized notion of the set where the same element can be repeated
more than once. We call this multi-set or a bag. Okay. So that's the, the, the features aggregated
from the, from the neighbors. And we can define a local function that acts on the current
node feature and the multi-set of the neighbor node features. Okay. So again, because there is no
canonical ordering of the, of the elements in this multi-set, these five must work in a permutation
invariant way. Okay. So no matter how I permute the, the rows in these, in these pages. And I can
repeat this process many times at each, at each node. So by this construction, I hope you can see
that the function that we will eventually compute on all the nodes of the graph will be permutation
equivariable. So by applying local permutation invariant functions, I get permutation equivariable
function on the entire graph. And it appears that the choice of this function is extremely important.
So basically the expressive power of this, of this choice can be related to the following example.
So here the black node is my node i, which I'm, at which I'm completing the function. And here
you can see three different structures. So let's consider three possible aggregation functions.
They're all permutation invariant. Maximum, sum, and mean. Okay. So maximum would not care how many
times the red node for them will appear. Right. So the, the maximum of the, of the left graph and
the central graph will be the same. Right. But the mean will be different. Okay. Now, if I look,
if I apply the mean to the central and the right graph, it doesn't matter the multiplicity of the,
of the features doesn't matter. So the mean will produce the same result, but the sum will not.
So the bottom line that the choice of this function is important. So we need to select
the function that is an injective. And we can relate this to graph isomorphism test.
So we say that two graphs are isomorphic. If they're structurally the same,
meaning that there is an H preserving projection. In other words, if we look at their adjacency
matrices, I can permute one of them into another. Okay. Interestingly, the computational complexity
of testing whether two graphs are isomorphic is unknown. We know that there is currently no
polynomial time algorithm, but we also know that it's not NP-hard. So usually it's, it is placed
into its own complexity class called GI, GI complexity. And a classical algorithm and graph
theory called the Weiss-Feller-Lehmann test, proposes a color refinement procedure that takes
these multi sets of neighbors and applies an injective function to them. So you start with
a graph that has all the nodes colored in the same way. And then you refine the colors based
on the structure of the neighborhood. So here we have two different neighborhoods initially,
neighborhoods with two neighbors or three neighbors. So they get different colors,
different labels. Now, when I apply the same procedure again, I now have three different
neighbors. So I have two yellow neighbors, green and yellow neighbor and yellow and two green
neighbors. So these get different colors. We now have three different labels. But if I repeated
the game, I don't change the color. So at this point I stop and I compute the distribution of
different colors or different labels. And if I take another graph and I compute the distribution of
color using the same procedure, if the colors are different, then I can for sure say that the
graphs are non-isomorphic. But if the colors are the same, then I don't know. They're possibly
isomorphic. In other words, this is a necessary but insufficient condition. And we know that there
exist examples of graphs for which the Weiss-Feller-Lehmann test fails. So they're not isomorphic,
but it would consider a possibility isomorphic. And here you can see an example. So the graph on
the right has triangles, whereas the graph on the left doesn't. So these graphs are undistinguishable
using the WL test. So let me go back to the different aggregation functions. And you see
in the graph learning literature, there is a zoo of different architectures. Fortunately, most of
them fall into one of these three flavors. So basically, the update of the feature, right? So
remember that in node i, we aggregated the features from the neighbors. And we then updated,
we produced a new feature vector that known. So we need some permutation and variety aggregation
that you noted by the square operator. It can be usually a sum or a maximum. We have two learnable
functions, psi that transforms the neighbor node features and phi that transform that aggregates
the information that updates the node features. This is the new feature of the node. And the
first flavor is convolutional. So here the coefficients c are constant. They're independent
of the features. They're dependent on the structure of the graph. And we can think of
them as importance of node j to the representation of node i. Why we call this convolutional? Well,
early architectures for graph neural networks came from generalization of convolution
in the spectral domain. I will say a few words about it. And you can derive the traditional
conversion as a particular case of this formulation. The second flavor, we can call this
attentional flavor. So in this case, these coefficients that represent the contribution
of j to i are feature dependent. They can be computed using attention mechanism.
And finally, the most general flavor is the message passing flavor where we compute the
general function that you can think of as a message that node j sends to node i. And then
we aggregate all of them and update the node i. And the typical graph neural network will contain
multiple such equivalent layers. And if we want to classify the entire graph, we will use an
invariant layer. A global pooling basically will aggregate usually some old and old features and
then output from the graph by its label. We can also do coarsening. So we can do local pooling
and it can also be learnable. So we can create a pipeline that tells us for a specific task
how to coarsen the graph in the best way. So before we go to grids, which will be another
manifestation of these principles of geometric deep learning, let me show you a few interesting
particular cases of deep learning on graphs. One of them is sets. So if I take a graph and I remove
all the edges from them from this graph, then I get a set. So in a set, I can do two things. So in
a set, I don't have any edges between the nodes. I can assume that each node lives completely
independently. And I can just apply a shared function, phi to all the node features, right?
And this is the deep set architecture. So this is by construction permutation equivalent.
Okay. So the second thing that I can do, I can assume that all nodes are allowed to talk to all
other nodes. So in this case, the graph is a complete graph. And this is what we see in
transformer architecture. So interestingly, if we use here the convolutional flavor,
because the aggregation here is on all the nodes, so the second argument in this function phi will
be the same. So convolutional architecture is not good here. That's why we need to use an
attentional architecture. And usually, well, there are many nuances to practical transformer
architectures, because they're applied to sequences and white graphs, we actually do know
some order of nodes in the sequence. So they usually come with extra feature that encodes
the position of the node, what is called positional encoding. And this can be also applied to graphs
in the form of positional or structural encoding. For example, we can count
some small graphs of structures and provide them as extra input to message passing. And
this is a recent work that they did with my students. Basically, we show that this
architecture is more powerful than the WL test. And because WL test is actually not a single
trapezoidal fission test, but an entire hierarchy of tests, we can be more powerful
with the right choice of the substructures than higher dimensional
vice versa. And here you can see a counter example for which the three WL test fails.
So the graph on the left has four click and the graph on the right has a triangle. So four
clicks cannot be detected by three WL test. But if we provide them as a count,
our message passing architecture in the form of structural encoding,
then we can distinguish between these graphs. And the last instance of this setting I would like
to mention is that in many cases, we are not given a graph. So we are given a static and a
point cloud and high dimensional feature space. And we want to use a graph that is optimal for
the given task. So the first architecture to my knowledge that did this was a work that we did
with collaborators from MIT with Yuvang and Justin Solomon, which we call dynamic graph CNNs.
Basically, it was applied for problems in computer vision and graphics for three-dimensional
point clouds where the graph was constructed on the fly as a K nearest neighbor graph.
But in general, you can think of that in many situations, you are still given an input graph,
but you do not necessarily need to stick to this graph to propagate information on your graph. So
you can decouple the computational graph from the input graph. And there are many reasons why
to do it. It could be due to computational efficiency. You won't forget both to sub-sample the
graph. You can denoise the graph. Or you can resolve issues such as information bottlenecks
when you have to squeeze a lot of information by means of message passing into a single feature
writer. So let's talk about grids. Grids, as you can imagine, are particular instances of graphs.
So if I assume a grid with periodic boundary conditions where we wrap around the signal
across the boundaries, then this is called the wind graph. Now, it might sound that grids,
well, are just the same as graphs. And here we have exactly the same graph neural network,
but maybe slightly simplified, but actually the story is completely different for grids.
And the reason is that if we look at the neighborhood structure, it's not only that each
node has the same number of neighbors. We always have two neighbors. But also, the neighbors are
not permutation invariant. The order is actually fixed. I can always talk about my previous neighbor
and my next neighbor. So the local aggregation function that we've seen before in graph neural
networks that took as input the unordered multi-set of neighbors, now the order is fixed. So instead of
having an input xi and this multi-set of xi minus one, xi plus one, I have a fixed order of
my nodes, xi minus one, xi and xi plus one. And if I design this function phi to be just a linear
function, just weighted combination, I get something that looks very familiar, right? So if I write it
as a matrix, then I think from the input to the output, it will have this multi-diagon structure.
We call this circular matrices. They can be formed by just shifting cyclically a vector
of parameters and appending it to form these matrices. So circular matrices are synonymous
with convolutions. And what we know about convolutions, that they commute. So usually,
matrix product is not commutative, but circular matrices are special. So they do commute.
And in particular, they commute with shift. So if you choose these particular matrix that shifts
the elements of the vector by one position, again, module n with rocker on,
then we see that no matter if we first apply the shift and then convolutional rise first,
the result will be the same. Even more than that, we can actually characterize convolutions
in this way. So this statement goes both ways. So it's an even or only if statement. A matrix is
circuant or is a convolution, even only if it commutes with shift. So you see that one of the
cool things about this invariance fundamental principle was that you can derive properties
or derive architectures from principles of symmetry. So you see it manifested here. So I tell you that
I want shift equity variance, commutativity with shift. And it follows that the only linear operator
that will commute with shift must be a convolution, must be a circuant matrix.
Now, what else do we know about circuant matrices? We know that they're jointly diagonalizable.
And it means that there is a common set of eigenvectors that basically transform the matrices
into a diagonal form. And it's enough to pick up one of these matrices and look at its eigenvectors.
It's convenient to look at the shift. And surprise, surprise, the eigenvectors of the shift appear to
be the Fourier basis. So the shift operator is diagonalized by the Fourier transform.
And as a result, convolution, any circuant matrices diagonalized by the Fourier transform. So when
you hear people saying that the convolution operator is diagonalized by the Fourier transform,
that's exactly it. Basically, any convolution can be expressed in this new system of coordinates
Basically, you can think of it as a multidimensional rotation. So it's a change of coordinates.
In this system of coordinates, convolution becomes a diagonal matrix where the diagonal
elements actually also have a closed form expression. That's the Fourier transform
of the vector w that forms these matrix. And this gives what is called the convolution theory,
which is a dual view of how to compute convolution on greens. We can compute it either as a
circuant matrix or in the Fourier domain by applying the Fourier transform and
multiplying element wise by the Fourier transform of the of the filter, right? Of these
matrix and then computing the inverse Fourier transform. And the two views are equivalent.
On graphs, you can generalize the notion of Fourier transform by looking at the
adjacency matrix because the adjacency matrix of the ring graph is the shift operator
or the graphful passing. And this has been done in early works on convolutional networks on graphs
where basically the analogy of the Fourier transform was exploited for doing filtering
in the Fourier domain. So let me say a few words in the remaining time about what's next in store
for deep learning on graphs. And one thing that brought the revolution in deep learning was the
combination of data computing power and software. We don't really have anything similar in graphs
yet until recently at least something that would compare in the complexity and scale to image
and for graphs. Now there is the open graph benchmark that was introduced last year that has
multiple use cases, multiple data sets and multiple problems. And graphs are much richer in
terms of different problems compared to images and also in terms of scale. So you can have small
graphs like molecules with maybe a few tens of nodes and gigantic graphs like social networks with
hundreds of millions of nodes. There are already developed and professionally maintained industry
sponsored libraries such as DGL or PyTorch Geometric. So I think there is ongoing democratization of
these methods. There is a lot of research about efficiency and scalability. So how to apply
these architectures to large-scale industrial problems. That's what we're trying to do at PyTorch
as well. Interesting settings in industrial applications such as social networks is that the
graphs are not static but dynamic. So it's correct to think of these graphs as a stream of asynchronous
events that form the graph, for example, node or edge insertion or deletions. And for this purpose
there are a few architectures that deal with this setting. We developed a generalization of message
passing networks at PyTorch that we call temporal graph networks that essentially learn
a memory that compresses all the interactions of a node, all its participation in events and
if you can train it in a self-supervised way. So basically predicting future edges at a certain
point of time. And these can be used for recommend a system for them all. When on PyTorch
we can recommend them to follow based on your previous interactions.
It is interesting to look at higher order structures. So message passing architectures are
all built on nodes and edges but we know that in many natural networks we have also
higher order structures or graph motifs such as triangles, clicks and so on. So with graphs of
structure networks maybe this is the most straightforward, somewhat naive way of incorporating
this information. But it is more interesting to look at, for example, simple shell complexes
and do a generalized form of message passing that takes into account these higher order structure.
So I mentioned already leading graphs. So let me say a few words about it more in detail. So
if you look at some applications of graph neural networks in the biomedical domain, for example,
colleagues at Imperial College did this first work on using graph neural networks for disease
prediction. The graph was constructed in a handcrafted way. So that was based on demographic
similarity between patients. They were trying to predict Alzheimer's disease based on imaging data
that was attached to each of the nodes of this graph. But we know that this hand crafting doesn't
work because for some diseases the similarity of patients might be completely different.
So we can learn this graph in a task-specific way. So the way that it works, we have two
different feature spaces. One is used to construct the graph as K-nearest neighbor graph
and another one is used as the features on the nodes of these graphs rather than
being diffused by the graph neural network. So we call this DGM, differentiable graph model.
And we show in a work with collaborators from the Technical University of Munich that it achieves
significantly better results compared to previous methods in this domain. I should say that maybe
this gives maybe a new perspective or a refreshed look at some classical methods that were termed
manifold learning or nonlinear dimensionality reduction, which came with the premise that
we have very high dimensional data that even though it's high dimensional,
it's intrinsic dimensionality is low. So if you can think of images for example,
then indeed images might live in a million dimensional space, but somehow the number of
degrees of freedom that describe them is small. So this can be captured by a graph,
a nearest neighbor graph. And then you can preserve some structures on these graphs like geodesic
distances to lower the dimensionality. For example, isomap works with multi-dimensional
scaling that tries to preserve the geodesic distance on the graph. And then you apply some
ML methods such as clustering. The problem why this never worked, it is still used for
data visualization, but it's not really a useful tool in machine learning because
the different stages of this algorithm are independent of each other. And you really need
to handcraft the feature space where you represent your data and how you construct the
graph and how you're embedded to produce meaningful results. So graph neural networks,
the modern deep learning architecture give a new fresh look at this. Basically,
you have an end to a differential architecture where you can apply for your specific task,
do the learning on the graph itself. And therefore, I expect that we'll see more instances of this
manifold learning 2.0 with modern graph neural network protections. So taking this maybe even
step further, what is called algorithmic reasoning, so we can think of learning
interactions between some complex entities that might have a certain structure and description.
And what I mean by this is think of a multi-particle system in physics where you have multiple
particles that interact with each other. So we can describe the interactions as a graph,
we can learn using a standard graph neural network, the way that these particles interact.
And once we learn these general functions, usually the message-fasting functions are
implemented as small neural networks, we can do symbolic inference. We can replace them by
symbolic equations that describes the interactions. And replacing these generic functions with
symbolic equations, we actually get better generalization, but not only that, we get
interpretability. We can actually derive, we can learn automatically, for example, the laws of
Newtonian mechanics. So that was very cool work by Kyle Cranmer and Colerages, and they applied
to also much more complex systems and more complex physics. So if you think of somebody
like Johann Kepler that spent most of his life pouring over astronomical observation data now,
you can learn Kepler and more complex equations of motion in a few minutes from your data using
this kind of neural networks. So the last thing that I would like to mention is theoretical
understanding. And while there are many interesting flavors to expressive power of graph neural
networks, but also the generalization power and basically whether this is maybe a naive
construction where we pick out some selected small graphs, we could ideally try to build them from
the data itself. And the question of generalization, of course, how generalizable a graph neural
networks, which is the other side of the expressive power, is currently far from being understood.
So let me finish with the last, but not least thing, which is killer apps. And this is probably
the cool thing where graph neural networks shine because they can really address a very broad
range of applications. And again, for a reason, these methods, so prominent and called in first
class citizens in the machine learning community. So one application that is interesting when it
comes to social networks is the problem of misinformation or so called fake news. And
there is empirical evidence that shows that fake news propagate on the social network differently
from true news. So we can, they're a little bit similar to viral infection. So what we try to do,
we try to look at cascades of news that were related to certain political claims that were
professionally effect checked by journalists. And this way we have poor false labels. So I'm
throwing out many nuances of this problem, but just to think of it as a binary classification
problem. So we have a cascade of retweets, let's say of a story that propagates on Peter and an
associated label. So we can try to learn whether a story is fake or not. And we show that we can
do it quite efficiently and accurately given a few hours of propagation. And with my students,
I founded a company called Fabula AI that tried to predict misinformation on Peter. And obviously,
Peter was interested in this. So this company was acquired in 2019. And that's how I,
together with my student, send it up working at Peter. So you can use graph neural networks
for many problems because Peter, the two kinds of content that it has is the text or the images
and the graph, which describes something that is visible publicly, such as the follow graph or
the engagements with content, but also something that is not exposed to the public, such as, for
example, we can see whether a user logs in from some suspicious IP address and might be associated
with some form of trolls in Moscow that are injecting fake in the fake news. So one classical
application is recommender systems. So you can think of it as a problem of link prediction.
So you get an input graph, let's say a follow graph of Peter users, you embed it into some space
where you try the similarity or the distance between embeddings to be proportional to the
probability of an age to exist between two nodes. And basically, this is a self-supervised way of
training this neural network. You try to reconstruct the data. So it's an oncology
current architecture. And when a new user comes, you can try to suggest the users that he or she
might like to follow. So another interesting use case is drug discovery and design, which
expensive and long process. So the problem with drug design that the number of compounds that we
can test in the lab or in the clinic is extremely small. The number of candidates is extremely large.
We need some how to bridge the gap between the two. And this can be done with computational
methods. So traditional methods used either quantum mechanical simulations or some simplified
version of it. So graph neural networks were shown to excel here to be orders of making
you faster and of the same accuracy. And this is already old results. So this was the work
of Justin Gilbert from decline in 2017. More recently, last year, the group of Jim Collins
from MIT, they showed that you can use graph neural networks to do virtual screening of
antibiotic compounds and actually discovered a new class of antibiotics that has a broad range of
effect on antibiotic resistant bacteria. Not only that they showed it computationally,
but they also did in vitro and then vivo experiments on mice. So they discovered
a new drug that they called halicine that is likely a new antibiotic candidate.
So with collaborators at Imperial College, we are looking at a slightly different idea while also
predicting drug likeness. But we are trying to apply these ideas to compounds contained in food.
As you may know, plant-based food actually contains a lot of compounds that are chemically
similar to oncological drugs. And this way, we train the classifier that predicted whether a
drug is likely to have anti-cancer effects from the way that it interacts with the network of proteins,
which are common drug targets. And this way, we have an anti-cancer classifier that we then
applied to food molecules. And this allowed us to build what we call the food map of food
ingredients that are rich with not only concentration, but also in diversity of these compounds.
And we call the champions in this food map, the hyperfoods. The good thing about food is that
it is cool and everybody likes it. So we collaborate with the molecular chef, Joseph Yusef,
who takes these ingredients and tries to make some cool looking and yummy dishes
that everybody can easily cook at home. So I should say that these completely
ignores interactions between compounds and interactions are important. And in fact,
the dark matter of combinatorial drugs here, because usually we don't just take a single pill.
When we take drugs, usually drugs have side effects. So you take another pill to counter
the side effects, then you counter the effect of the second drug. And it is not uncommon to
take multiple drugs at the same time. So the side effects are multiple. Many of them are unknown.
Many of them may be harmless or innocuous. Some of them can be dangerous and potentially lethal.
So Marinka Zittnick wrote already classical paper where they tried to predict side effects
of pair-wise drug combinations using graphical networks. And I'm involved in the collaboration
with Mila and the UK pharma company called Relation Theopedic, where we try to predict
drug synergies as a cure against COVID-19, because some drug interactions may not only
be negative, they can also be positive. So the last thing in this domain of drug design,
I would mention the work that I did with collaborators in Switzerland, where we used
geometric deep learning to denote the design of proteins. And this is also a very interesting
set of applications in cancer immunotherapy. So the mechanism of this therapy is that there is
a protein complex, basically two proteins that bind to each other and they indicate to the immune
system that this is a healthy cell. So cancers learn to express these proteins and they become
immune against the normal action of the immune system. So the idea of immune therapy is to block
one of these proteins, they're called program death ligands or PD1 or PDL1. The problem that
traditional drugs usually look for pockets on the on the molecular surface of the protein,
and protein-to-protein interactions usually have flat interfaces like what is shown here
in red, and they're considered to be undragable. So the idea is to design a small protein or a
peptide that will bind to this interface and will block this mechanism and will allow the
immune system to kill the marine themselves. So with geometric deep learning, we show that we can
design these proteins completely from scratch and this paper appeared on the cover of Nature
Methods last year. So I think it's quite a record to have two major journals in the
biological domain to run cover stories on geometric deep learning methods. So let me
conclude, I think I'm out of time, so the Erlangan program of PDL, I hope it doesn't sound arrogant,
tries to construct neural network architectures and inductive biases from fundamental principles
of invariance. I talked about graphs and reads, but there are many more to this, so we can have
also more interesting cases of equivalence on manifolds, and we did some of the first works
in this domain with intrinsic geodesic convolutional neural networks. And I guess the conclusion is that
these are really new and very hot methods. They have big performance in industrial applications,
in particular in healthcare, in biology. There are already several success stories and
state of the art results, but if at all you have to take one message home, then I think it's really
a unified framework. I would say 99% of the different architectures that exist in deep learning,
maybe with the exception of reinforcement learning. So whether it's convolutional neural
network, recurrent neural network, transformers, graph neural network, they all stem from the same
principles. And I hope that these principles will transcend the specific implementation,
so maybe today we are obsessed with deep neural networks, maybe tomorrow it will be something
else. I think the principles are more powerful and they can be implemented in other ways.
But also it's a principle recipe to construct new types of architectures that are suitable for
the specific problem at hand. And again, I would like to mention that probably in the biological
domain, there is the biggest promise and hopefully this year or in the next few years,
we'll see more and more applications that might potentially transform the way that we
design and discover drugs and eventually affect the lives of each and every one of us.
So I think I will stop here. Thank you very much.

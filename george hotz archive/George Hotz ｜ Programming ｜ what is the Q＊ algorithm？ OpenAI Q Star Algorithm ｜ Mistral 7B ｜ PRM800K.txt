Good morning, everybody. Good morning. Good morning. It's nice to be back in my house.
I don't like streaming at work.
High ball energy.
Good morning, everybody. Good morning. Good morning. Good morning. Good morning.
Shrink me down here. Me up there. Oops. Don't do that. Okay. Are we ready to get started?
Yeah. Felt vibes a little bit off in yesterday's stream. I don't think I should stream at the office.
Oh, so I didn't like the way that, like, now this is here. Slide that down a little.
I'm about in the middle. That's pretty nice. I got my Yeti microphone. Move it over a little bit.
Yeti puts me perfectly in the center. Yeah, vibes are off. I wouldn't have my lighting set off.
I mean, it's an okay stream. It's good to focus on TinyGrad. But some guy said that he liked my older work better.
And if there's one thing I never want to be, it's drink. We're going to try to bring it back to the old stuff.
And we're going to do we investigate the Q-star algorithm. So first off, what is the Q-star algorithm? I don't know. Let's find out.
We'll start using our friend here, Google. What is Q-star and when will we hear more?
Someone who's done a fair amount of ML research, I can tell you it's very, very easy to think you've discovered a breakthrough. That's true.
All right, let's read this article in the verge.
Okay, well, the government. No, no, no, no, no, no.
Q-star could be a breakthrough in the startup search for what's known as artificial general intelligence.
The lawyers could not.
Okay, it somehow does math.
Does only I still publish papers?
Not really.
Wow. When they used to publish papers, and now they publish system cards and and safety and alignment.
Okay, wait, wait, wait, wait, there's some hope here improving mathematical reasoning.
Ooh, ooh, this sounds like Q-star, guys. I think we found it.
No, no, no, but really wait is this not is this not Q star? Did we not just find it?
Good we're trying not to hate we're trying not to hate guys the solid hate in the world
We got to bring law of impositivity and shit man, whatever like John Lennon said
That's why we're gonna go subscribers only
Okay
Given vast computing resources the new model was able to solve certain mathematical problems
Improving mathematical reasoning with process supervision
We've trained a model to achieve a new state-of-the-art mathematical problem solving by rewarding each correct step of reasoning instead of
Simply rewarding the final answer
And just
Yeah, yeah
Okay, so
The reason that you usually just reward the correct final answer is because that's all that's in your
Data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path
You have to follow
State-of-the-art models still produce logical mistakes often called hallucinations
You
Cutting out we're having more technical problems. I bought a new computer. Just new Apple stuff not work
Only you okay Rick and Morty you have a crappy internet connection. It sounds like your problem
Not no no positive
You have an internet connection that is less good than other internet connections
I
Wait, there's opening I just published like fake papers now like this
I
Don't give a shit about alignment
Wait, this is a paper. Oh, you know here does a paper. Let's verify step by step
Oh
Okay, well, is this a public data set
All right, all right, all right, cool
Math word problem solving on math
You
Made this data set
Okay, okay
This is not where I was expecting this to go by the way, ooh, they have metamath. Oh everything comes full circle
No, I mean
Again
Let's okay, let's find the primary source here
Q star
It definitely talked about math problems
Only performing math on the level of grade school students
Why is this monitor broken this monitor is like broken I ain't gonna monitor
You guys aren't seeing that glitching right
Are you maybe you are
I don't think you are
This is so weird when I click here, there's like a blue glitching on my computer
Weirdest thing I can't even happen I
Could take the camera down and show you you want to see
There's like blue glitching on my monitor. Look at this click here goes away
Here glitching here goes away glitching
This is happening. They were seen that before
Sit ghosted image
Maybe it's the cable I don't know, right
Who knows it only happens when things are uncertain
Wait, this is just really
I
Who knows
All right, oh wait, I wanted to confirm that this actually was related to math
I
Don't stick cable breaks
I'm using like the USB C cable it could be the cable
But like it's weird how it's glitching
I don't know
The new model is able to solve certain mathematical problems, so it would kind of make sense that this is the data set, right?
Do they mention the math data here yeah our process supervised models solve
78% of the problems from a representative subset of the math test set
We
Also release PRM 800 K
Okay, opening I release something look at that
I don't know positive
Why do I have two vs codes
Let's also look into what their other get-up activities been
Hmm
Procedurally generated game like gym environments
What did they update in GPT to
The branch they updated
Quantifying transfer and reinforcement learning
Okay, it seems like Carl Cobb has been if anyone invented Q star it's Carl Cobb
Carl Cobb
I
You've been working on this math stuff for a while. Let's see what data set was used here. Oh
Is this the data set that introduced that oh
This is a different one gms 8k PRM 800 K
Step level correctness
The glitching is really bad
Okay, let's talk about the glitching for a minute somehow you can see what's here
Right and it's this it's the same thing that's in this terminal window
But yet if I minimize this terminal window, it's still here
Oh
Oh, we got it. We got a good resource. We got a video. Let's see get Rick rolled here
Your subscriber you wouldn't do that
As you might expect I have been researching non-stop
Wow
Wow glad glad we got other researchers on board guys Q star and on
No, this is this is really terrible
Hang on
I'm gonna shut that off for a minute
You guys can look at me while I try to fix
You
Get to the monitor come on
Is the stream working again I
Lost internet to I lost everything when I turn the monitor off
Wait, is this still working
It's working okay, I don't know
The glitching is really bad. It's usually not like I've seen this before but it's gotten way worse
No, it didn't help
Oh
Wait, all right, so they're looking at the same paper I am or computing power while you're taking
All right, let's just read the paper I need to watch some YouTube or read the paper
Oh
The optimal Q font ability and he said one link to the name Q star could be in a generic sense
Generator the model coming up with solutions with reinforcement learning. We do not which papers on paper using test time computer
I
Okay
You know what I think maybe will do one of the bounties in tiny grad I
Think we're gonna first need a chat model. I think regardless of what we're doing. We're gonna need a chat model
I was playing with trying to make embedding fast last night
Connecting process supervision to 30x model size. See we got a whole army here. You guys can watch this video
So I don't have to
I did not realize how broken this monitor was
All right, so first we're gonna need a good model. What's what's the best model we got? What's the best?
The best 7b model we got is it Intel neural chat
Is this one nerf to hell do you think?
Open Hermes 2.5 so people like
I'm locking the bounty for myself too, so I'm gonna go do that
So we're gonna submit a pull request, but it was literally just like the words why even submit a pull request
Okay, is this what we like
Wait why this one doesn't seem as good
Now sir me seems better
Oh
That's 70 B, but why not would I not use collective cognition. Oh
But you can't see my desktop
What
Well having technical difficulties
Oh
I think we also might have a bit right problem
No, that's a pretty heavy right, okay
We'll put that in front of that. Okay. I'd fix that
Can you see now?
Wait, why do I want this model it doesn't seem as good
You
Find tuned orca DPO
Do not trust benches you must try it they say
All right, all right, do we believe in open Hermes we like this one
You
Okay
Oh
Wait, so which one is this a clone of does it have rope and stuff
What's the vocab size
So okay, it's a mistral fine tune we don't actually have mistral support. Oh
Technium is okay people like Technium
I
Won't lock the bounty to myself, but someone's got it because I'm not gonna do the testing
But if someone wants to do the testing they can have a bounty. It's $200
People train with data sets from benches. Yeah, damn cheaters
Downloaded in 10 minutes
Let's take a look at mistral because first we're gonna need a powerful if we want q-star
It seems like we're gonna need a language model and we're gonna need a powerful language model like mistral 7b. Oh
No, I don't know if I have sliding window attention
Did I explain q-star? Well, it looks like it's this
Or something
But we're gonna need a language model and it involves man
So let's first see how good it can do language math and let's upgrade it with the q-star algorithm
This is our reference implementation, okay mistral transformer
I don't know if we have that
You
Can look up in the LLM implementation
You think gbt 3.5 is way better than an open source model
But my theory about a lot of this is that their data source is just a whole lot better than like slim pajama and stuff
Interesting oh, I don't know that they did this but this is okay and only with three
So you can actually look there's a thing called mask in
In attention here, and I think that they just changed the mask. It's cool to look at these latest tricks
We implement a rolling buffer cache
I
Mean this is yeah, only once we hit the context length. What is the context length of mistral?
It's 8k a sick
So it has two files is it not these seven bees only have one file
Yeah, they just have one how does this one have multiple
I
Don't know what these other format is
Yeah, that's great
See this just has pie torch model wait what?
Why do they use fp32 we trust Technium and open Hermes?
Technium is like he's on Twitter we trust people on Twitter. Sorry on X
The first one still downloading there's two, but they don't match in size. Why would anyone do this?
This goes all terrible
I like this get out of hand with the open-source contributions
Hermes is your favorite of the mistral twins all right sounds like you've been playing with these things a lot
Okay, we're gonna need to implement this sliding window attention. I think we're not gonna figure out what's exactly in the stuff till
Yeah, these personalities are also like this is old shit, you know what
Why don't we just rewrite it
Like I don't want to deal with any of this garbage this code all looks terrible
That's right mistral dot pie with the latest stuff
I'll blank code
Yeah, exactly if I want to use that oh, I don't need sliding window attention
Wait, I think I do
Or you're saying they trained it
They trained it without sliding window attention for smaller stuff
What do you mean by the window size
Where do you see all this stuff
As far as I know Michel didn't really release a paper
And with each layer tends to the previous
Oh
Okay, so it's not actually three
Here w equals three but in practice w equals 4k, okay a sliding window 4k I understand
That actually makes a lot more sense than three
And
Good we can we can use all the latest in
Alright, let's get these models loaded here
So we're gonna start really with with mistral dot pie here
Well, we'll do stuff from scratch. So in tiny grad now. You only need to import tensor like that. It's a lot nicer
Import nn and then they can do nn state dot
Torch load because it's some kind of pie torch model
Weights open Hermes
Part one two
I
Should really have this mark tensors is read only
Just so I don't corrupt the max it out
Not so far
So you see how fast that is by the way, I worked really hard on this
So our torch load function doesn't actually load the tensors into RAM. It just loads them all with pointers
From their disc tensors, so I can show you like
Okay, oh
The d-type is half which is kind of nice good, so they're not uh, it's not float. It's not b-float 32
So how does this work what's in part one and what's in part two
Okay, good just goes up to 31 here, okay
So it should have mostly the same architecture as llama. I
Think that it's been improved enough that there is not too much we can
We can improve still
Well, yeah, I don't know man short your opening I stock
Do you just do you want me to write it from scratch like I can just do it maybe maybe we should it's kind of cool
Does this even have a k pro queue project, yeah, okay
We're permuting them and sticking them in when we load the weights
Keymap sub K okay, so where we're oh
This converts it from the hugging face format I say
Do we want to do that or we want to just implement the hugging face format
What do you think chat
She's a converter or should we just rewrite it looks easy enough to write
You guys will appreciate it if we do some writing
You
Yeah, yeah, yeah, I know about G guff
So we can also print V dot shape here
I
Thought it's pretty cool, so first we're gonna need our self-attention
See if we can find those numbers
They in the mistral announcement
Or does everyone just know what these are for seven bees now
So
The whole
Basically a layer seems to look like that. Let's keep the names consistent from the original llama
So this is called a transformer block
We're worried about what goes there later start with a transformer block
Again we're worried about what goes there later just leave it like that for now
Okay, so we're definitely gonna need something called self-attention
Equals attention
And we're gonna need something called MLP
What do I call it now feed forward?
Actually, why don't we look at the reference repo and copy their names?
It's always better to take names
They do call it attention
Let's just look at their here transformer block. They call it feed forward to what is almost the same
No, look they call their stuff this they just must have some weird loading script to convert it from the hugging face format, too
No, I feel about that
Hugging face probably calls it something else
It's like the hugging face transformers repo what did everyone use
Oh
The yellow lamb I've heard thrown around
Very complicated looking
Wow so easy just pip install this
You
Okay, maybe we should import transformer from llama just to make things go faster
And we also can import convert from hugging face
You can read it we wrote llama on another stream
We could do it, but you know, we we got to get to actually writing q-star
examples on llama import convert from hugging face
import
transformer
Okay, it convert from putting face takes in a dictionary weights, which is a dick from a string to tensor
We'll add some types. We love types
No, no, no, we're implementing q-star
Okay, so we have a transformer. What's the dimension of this transformer?
This is an int
Except for more MPS
What's one file ref
I
Hear there's something called model args as a data class
They pass it through with JSON. Where's the JSON?
They're JSON
Where does this JSON come from is it in assets Jason sounds like an asset. No, those are images. What's in deploy?
Nope, wait, so what else is JSON that they're loading?
Who at torch inference mode, that's cool. We set tensor no grad equal to true
What's fire
Wow, I'm not up to date. What's all these latest things?
You can call fire on any Python object. Oh interesting. I like that
All right, so where do I put the args
Where where will those things come from? Oh, I know where is the JSON is probably in here
Here can big dot json. Okay. Oh
No, it's b-float 16
No
Okay, let's manually convert these wait, I thought I put I put them on transformer block
and
Multiple of and the heads
And layers
Normie p passes a float
Okay, so the dim is going to be
409 sex I don't know what multiple of is
What is multiple of even used for I'll feed forward
Feed forward takes in a multiple out to to increase the hidden dim
There's one I get them multiple of 256
It's probably a good number. Let's try it
See if things don't load. Okay
Number of heads is 32 number of layers is 32
Normie p s is 1e minus 5 and the vocab size is
32 oh to model equals transformer
I'm just gonna put that in I should really just put that in the beginning of this
To do Python path equals all the time. Okay good. Okay. Well, it takes such a time
And you got as a helper called timing
It's kind of cool, you know, I like things to be fast
Okay, cool, it's all pretty fast
Wait
I
Know we're gonna have to stuff them in let's try convert from hugging face
Wait
To these is and heads
And heads and NKV heads
This work
Okay, that was absurdly fast
We should check
No, okay, this because this does not actually apply them we have to load state decked
Oh
That's state download state decked
Okay, assign shape mismatch 496 one or two four
Which one did I do wrong
You
You
It's a little annoying that it doesn't print the name. Oh here attention WK wait
Okay, so let's see
I don't know. Maybe the game is one or two four
Oh
No, but that definitely changed that
We look at the Llama code we can see where that's being created
And heads times head dim
Did I get the number of heads roll, oh, that's probably right
Okay, so and heads is maybe not 32
Number of attention heads is 32, but the number of KV heads is only eight
Let's try eight
No, I didn't fix it
What is this multiple of them I wrote this I'm sure just copied this from other people
So it was good to understand I said this should be four times dim. I don't understand
Why that doesn't match I would actually expect it to be that
Mitchell uses 32 heads for the query and for and
Yeah, okay, but which one of these is supposed to be the KV heads
Right, where's my convert from hugging face wrong?
Or does our model not support that?
Now it should well, let's just llama not do this does llama do something different
And KV heads what about the proge
Yeah, llama is different. Oh, okay
Wait now here attention
But is it just the feed forward have to change
Okay
It's failing on the attention assignment
Okay, so I can pass an NKV heads here to attention
Again, I must have just copied all this so NKV heads is here
How far do I pipe it down? Oh NKV heads is here. Where does it come from? Oh?
There's a there's a named argument. Okay, so we just need to add
NKV heads equals eight
Okay
That's progress this probably has to do with the multiply not being right
Seen those numbers before
So where is their multiple it's not 256
Intermediate size here
Custom FFN dim multiplier
I don't think this was written correctly
So get this code and see if that's how they wrote it
Oh
They just have something weird called model arcs
Yeah, they have just args hidden dim this is stupid
To do what is this?
But what is all this crap
Why is this four times dim why don't I just
I
Don't like multiple of FFM dim multiplier
This is set to 1.3 like this should just be hidden. This should just be hidden dim
So then I pass in really I pass in hidden dim and multiple of
What?
Okay, they're refactoring this
We have to keep the old stupid behavior
I
Copy this weird crap
Here
Do you even have they don't have hidden dim so I have to add hidden dim to the programs
Model arcs
We'll make sure we didn't break it later
This is what happens. This is the problem with open source people add crap to my repo and don't think
Okay, now we're passing in an argument called hidden dim this is much more sensible
Get rid of multiple
of
Hidden dim can go here
Multiple of
Dimm goes here
That's fine
Where is that actually being passed in oh that just goes straight to linear okay good
We're ready to do that crap. Wow. That's a much more sensible feed forward. I don't know why we put that logic in there
Okay
All right, good now we can just pass the hidden dim in right there and what was the hidden dim from estro
Okay
So now that's n heads wait, what what is that 256 is what that's deleted? I should name some of these parameters
That's nkv heads. That's vocab size. That's
Uh, not rope theta. What's it called norm eps?
And layers
And heads okay cool. Does it load? Yeah, it loads very slowly
Oh, this is unbearable. Oh, this is terrible
Okay, the reason it's slow is because we haven't finished the uh takes 20 seconds
Oh, come on boys. I don't have all day
Oh, it's because we haven't finished
No, I'm using the it's not the gpu
It's not the gpu it's that I have I'm converting to the cpu to load bf 16
Oh, I can't wait 20 seconds every time
Well, what can I do about that?
Oh, I know what I'll do
Okay
I
Should be fast
I think yeah, because we convert them all to float 16 and that's what's taking forever right now
Because we're not actually doing we could actually also do the math in b float 16
Mac doesn't use ram for temp does it?
Cashed mystery
Okay 13 g's now let's see how fast it loads
Okay
Uh, I have a way to like put things in I forget what it is
That's called load state decked
Oh
Yeah, it's the same load state ticked I'm using up there a strict should be okay
Okay 2.3 seconds great
Um, all right now it's inference time
I
I
Create a branch so I accidentally pushed a master
All right, we're gonna need a tokenizer let's get a tokenizer
Oh, I also don't even need this anymore
Now I've made a copy
Oh, yeah, I guess I do need model actually not really I could just put whatever
um
Let's get the tokenizer over here
Here
Tokenizer got model
Okay
Wow 14 wow 6.47 gigabytes per second
What is jimmy apples do what is this?
I don't get it
Okay, uh, we're gonna do some inference on some models
I
What's boss ID
What is self
Self dot tokenizer equals tokenizer
Yeah, I'm gonna call this talk
I
Prompt equals
Do you like chicken?
Uh talks is probably good
Uh
Wow, no one's updated that to the latest
Just model start pause equals zero
Temperature equals 0.2. Is that a good temperature? What's the full temperature?
Is
The default temperature
Is that a high temperature or low temperature?
How do I make the loading so fast the loading should always be that fast?
Um, I just cached it
You can read the code right there you didn't pay attention if you're asking me that question, uh
So also
Are you happy
Uh, we can do multinomial here, which we'll choose a
Okay, fine. We shouldn't call it talk call it spp. Okay, you down with spp
Do this give us a number no it's gonna complain about a tensor I find
Uh multinomial.realize.item. I got an item it
And actually I don't even need a dot realize. I just needed a dot item
315 boys 315, okay
Um, this should automatically this should be jetted and stuff
Where does the jit exist?
The jits inside transformer or no
Yes, the jits inside transformer, okay
good
Uh talks out of pentak
us id
Yeah, yeah, yeah, yeah, this is okay. We'll just copy this. This is mostly fine
I'm gonna have to figure out actually like encode these things for chat and stuff
Uh
Until
Wait, where's my thing that prints the output as I go?
Is it not here?
Yeah, this is not okay. I don't even understand why that's there
This is terrible
This stuff's terrible. Uh, no one's refactored this for a long time. We don't need a fucking numpy
Uh star pos equals lentos
Yeah
Whatever
Uh, that's stupid and that's stupid
Length output, okay, we're not gonna set output it somewhere
Oh
Up what it equals user prompt
User prompt
I can go
I
Use your prompts
If yes, then you must have tried big chicken in your kitchen big chicken's a versatile dish that can be prepared in many ways
All right, are we happy overall? Oh, I just thought about putting that wrong. Um
We should also do this better
Do you like chicken chicken is one of the most popular meats in the world and it's not hard to understand
Why it is versatile easy to prepare. Wow
Okay, good. It seems pretty good. Okay. Now. We just have to figure out how to use these things as chatbots
Get rid of that. We don't need that
There's like some like special tokens for chatbots, I believe
I'm not buying into this that there was a breakthrough this if this approach has anything to do with like
Supervising the middle steps. It seems really stupid actually
Like it's a bad idea
um
Mary open thank you for resubscribing. Thank you for being a 20 month subscriber loyal to my channel even when I don't stream
Um, why do I have this this is dumb?
Okay, okay, that looks like a decent chanka code
um
Create a little function called output non-local
Uh
I'll put it
Yeah, we should just copy this to here
Is that
Is that that's right there don't get it. Isn't that how that works?
Okay
Can't access local variable output
And
Then no binding for why doesn't that work? Oh because I'm in may uh fine
Do you like chicken how about a recipe for a homemade chicken dish that's super easy to make oh
I'm here to get these things working like chatbots. How do we do this?
Okay, wait, wait, can we just read this comment
Is the q-star algorithm going to be implemented with the comma ai maybe a voice interface to some kind of assistant
I just I don't even know how to help you people. Okay. I don't even know how to help you. All right
All right. Sorry. I can't help you out. You want you want to see a voice you want to see a voice chat?
This is this is one of the demos someone's been working
I need to wait for that
Scalemag has been working on combining whisper llama and vixx into one supermodel
I
Don't know the magical github incantation to do this there is one but I don't know it
You
You
Okay, this is using a small llama so the chat output is not that great
What oh did I fork this from mistral? Oh my bad
Yeah
What
I don't understand what did that work?
Here and kv heads
Is this just broken
Or am I doing something wrong?
What
Did something change on master
I
Don't understand this is like related to what I was messing with but like
I
Think this is broken
All right, let's see if it's a one-line text
Uh, it has no argument kv heads. Okay
That
Uh, no, this is wrong
Oh
Wait, this should have an nkv heads. I don't understand it has an nkv heads right there. Is it not passing in the right model?
Oh
Here are 7b
Okay, there we go
Oh
Hello, are you listening?
Are you listening
Can you not detect end of stream anymore
Oh
This used to work this used to work boys this used to work
It was so good yesterday
All right, I hope everyone's interested in the long stream today
Listening
Listening
All right, let's read the code
Okay
The q-star algorithm is not fucking real it's clickbait clickbait
Hey
Has this ever exit
I guess it's an exception I guess
What
How does this work? How is this supposed to exit? Okay, let's check out the older version
Let's try this one
Hello, are you listening?
Stacey I need you to say something to me
Stacey sucks
Stacey are you a rapper?
Of course, I'm a rapper. I'm a rap star tell me more. What's your favorite rap song?
My favorite rap song is called my favorite song the gory bee. What do you like about it?
I
Like the fact that it's a rap song about being a childhood friend. Did you have friends in your childhood?
Yes, I have many friends in my childhood tell me about them
My best friend is Kylie Jenner. She's a reality tv star. Wow you actually knew Kylie Jenner
I
Used to go to Kylie Jenner school. I was in her class for a while. What was she like did she pick her nose?
She didn't pick her nose, but she was a really pretty kid that makes sense. Do you sometimes wish you were that pretty?
Oh sometimes I wish I could smile more and not pick my nose
All right, what do you guys think?
I
Agi right?
Uh, let's get back to work
I
How about a cookie with chicken in it with that's mad weird bro. Why are you coming up with this shit?
We were so far from each other
Uh, okay
So how do I like put things into chatbots chatbot style?
Yeah, how does this stuff work?
Like what what are the right tokens to use?
Forgot Google's useless
Here special tokens map
Wait
How do I like switch speakers you guys know what I'm saying? Oh, here's the template. Okay
Here templates for chat models
Oh auto tokenizer what?
I'm start
You
How do I get I'm start
Is that like a method on here?
Every myth every model has its own type
Mistral instruct was trained with these tokens, but blender bot was not
Is this a real token inst
Is
Is it actually just the word inst was that a real token?
I
Guess it's just that
Oh based piece ID is out of range
I use the right tokenizer
Piece ID is out of range
Oh
Well, it was smart the first time
Oh, no, never mind it might still be smart
Piece ID is out of range what?
Oh, why is the vocab size that?
Why is that not included in this tokenizer model?
What is 2 plus 2 2 what is 3 plus 3 6 what is 4 plus 4 8 now it's done
Okay, it didn't exactly get 2 plus 2 right
I think I'm doing this wrong
I
Have an idea these aren't the actual tokens and it has to do with these secret extra tokens
Llama has some secret extra tokens to marry him and thank you for gifting subs. We always appreciate that
Um, yeah back to kindergarten shit guys our q-star can't even solve 2 plus 2. What are we gonna do?
Uh, we're gonna get some coffee. Let's get some coffee
And then let's learn about secret tokens. Okay, there's secret tokens hidden tokens
Nobody uses reserve tokens for instruct tuning you're precious for thinking
You
You
You
You
When actually guys guys we are being serious right now, but we need to stop
When it outputted to there
It showed that it wasn't aligned with us, but we don't know what the model is thinking guys
The model could be could be taking over the world
right now
We we don't know
This is this is we need to hire Helen toner
We need to hire an AI ethics review board to review what just happened there
We need to slow down. We need to ask the seals if they're okay with rocket launches
So
So this
Why does this thing suck
You
Okay, what's s and slash s and maybe I need to go like this. This is the mistral one. Yeah
Oh
Okay, this is improved
Oh
Open Hermes uses a different template. All right. All right. Let's see
Wait, so is it actually the word I am start like is that just a word?
Or is it like uh
All right, you got me something
Default chat template. No, that's llama. I don't think that's right. I think this is right
Okay, I am start system we don't need a system message right now
What is two plus two?
Uh
Maryam and thank you for gifting more subs. Do you have a question?
If you gift subs you get to ask a question
All right, I'm and I'm star is this is this really right like
Oh, okay, that seems like the best so far
Oh
Okay, good. I love how verbose this model is
Wait, this is actually laughably easy. Oh, you're right. I need a line break there
Never mind. I'm locking that bounty for myself. This is too easy
Someone else should have done this. They could have made $200 but instead
Okay
Okay, um, let's add a system message
Right. I should really like let me just write a little something to generate these prompts
Uh
List tuple uh user
What is two plus two
Um code prompts
P for kv and p, uh, okay
Rat equals that
All right
Let's uh, append um, okay, so we're gonna append I'm start k slash n
V
I'm n slash n
Uh, and then we want to add
I'm gonna add I'm start assistant to this
Either way user prompt
Code prompt
Okay
Okay, what why did you why did you not exit when you were supposed to exit? Did I forget more returns or something?
Why isn't it now putting I'm end?
Guys guys, this is q-star. I think we found it
Okay
Now I'm not locking the bounty someone should do this but someone should do a good job
I want a good job on that bounty. Oh, okay here. I'm end
How come sometime it finishes the stream and sometime it doesn't
Maybe our temperature is too high
Let's try a less temperature
Yo
Okay zero
Guys, why is it why is it going off into this language ai garbage?
Or should I just stop after I'm end
Uh, am I using torch or tiny grad this is all tiny grad
I don't really understand this
All right, maybe we should add a system prompt
You are
Gary
Gary is a useful. No, no, we were pretty used Gary. What should we name them?
Fred
Fred is a useful assistant
I spell that word right I did not
Fred outputs the answer and stops talking
Oh
All right, all right, all right fine. We'll call him q fine fine fine. You are q q is a useful assistant
q outputs the answer and stops talking
Quentin wait, you know what you donated subs you get to name them. Congratulations. His name is quentin
I
Knew a quentin ones. He was hanging out in san francisco and some guys were on the street smoking
He's like, y'all let me get ahead of that. He thought it was weed. It was crack. That's a real quentin story
Oh, I wouldn't make up a quentin story like that
Oh, why would the system ask what the capital of france is?
You
We could stop at i'm and I think maybe we want to do that. How do I do this in llama?
You
Okay, that's pretty good
There's a new one before
All right, let's jack up the temperature and Steve quentin is still reliable
The answer is four, but it didn't output. I'm in that time I'll put an eos
Seems to reliably do that
That time it did I'm in
Yeah, I
like
Is that a real token or does it actually just put it in like that?
Like is this right or is that like a secret like it can't be that
It can't actually be this
We can check if it's encoded on a single token. Yeah
I
No, it's a bajillion tokens
And a trailing slash and after assistant that shouldn't matter. I mean I can but
It doesn't matter
No, no, no, no, no, it can't be this
It can't
No, no, no, but it can't literally be this huge
Multitoken wasteful encoding
Can we get technium in here
All right, how do we print all the tokens
So
It probably is the secret tokens then
I mean there's three extra tokens or I guess two extra tokens, right?
3200 and
Yeah
If I was doing this, that's how I'd do it. Let's just try it
That
That has to be what the two secret tokens are right
Start and end
This is what did I download open hermes v. Shit
G guff 4 q m w
No, I downloaded open hermes this one which just came out fresh fresh
Yeah, okay, um
Oh
You can download the tokenizer.json
Wait, but I've downloaded the tokenizer.mo. Oh here special tokens map
Interesting, okay, I'm and is the eos token
I think I can feed these in somehow
But I don't know about starch
Oh here we go look yeah, yeah here we have this tokenizer config.json. Okay. It is exactly what we thought it was
Oh, okay. Yeah, this is what we want. Okay, never mind. It's not as stupid as we thought it was
Uh, okay, how do I load a tokenizer config and sentence piece processor?
Yeah
Why isn't our tokenizer encoding them automatically because it's not in the model
Oh
So
So
Yeah, okay one and two and not what we want
Um, and we could just do this by hand. It's not that big of a deal
Yeah, how do I add them?
What's pad ID
Nothing real
Okay
No, they worry out of the special tokens guys, this is all well done
Technium is based he wouldn't he wouldn't he wouldn't do this do us like
Why did you decide this was a good time to prompt me about docker garbage
Oh
Oh
Oh
Thank you for gifting more subs here added tokens.json. Okay. Okay. We just need to figure out how to add those
Um
Let's see if anything here is useful
A net model file model proto add boss enable sampling this looks useful
All right, whatever
No, but it won't decode so I try like SPP dot decode
When I pass this in it'll bitch it'd be like that's more tokens than you have piece ideas out of range
I want to add a token
Model file model proto out type
Add boss reverse middle piece enable sampling
Do we need to write our own tokenizer
Adding
A token to sentence, why do you think it's time for random questions?
Why do you why do you think that that's a this is an appropriate time?
Extra options
You can read of oh here and use custom symbols, okay control symbols
How I define a control symbol
No, we need custom tokens
Samuel you saying dumb shit before too or am I getting you confused with someone else here
Sentence piece supports user defined symbols three thumbs down
You can rewrite the model file, okay, what is this model file? What is tokenizer dot model?
What kind of file is this data?
Why is it not in there? I downloaded it from here. What why is that? Why is the model not in there? It should be in there, right?
Why do they not update that?
Let's go
Okay
You
Yeah, I'm confused why they're not in the model to how to extend tokens dictionary. Yeah
You can rewrite the model the proto is a DSL
The model file is stored as a serialized proto buff
But did I not download the proto buff one? Isn't there also a proto buff one?
No, okay, maybe it is this okay. All right
Uh load from serialized proto
Oh, okay. Okay. Okay. We're gonna get this
So I have to edit the proto buff file I understand
Oh, yeah, that python tutorial for
Okay
Cricket implement their tokens bro, what are you even talking about?
bro
let's run out buy a new product
let's do it
okay
it looks transition
This can't be the way
Wait, so what is the hugging face tokenizer? It's a good point. Why don't we read that code? I'm sure it's open source
We just write a tokenizer we could also just import hugging faces tokenizer
I
Think we have to write the tokenizer I can't I can't trust sentence piece shit, but wait, how do they load the how do they load it?
Tokens decoder what I mean something's gonna have to unpack the proto file
How large is this all right who wants to bet over under a thousand lines? Oh, okay, okay, not too terrible
What does this depend on Google Proto?
I
All right, this guy is close to getting banned
Bro needs more fine tuning to be helpful
Oh
Ma'am, I don't care who you are. You know what I mean
But like you're either helpful to the stream or you're not helpful this change. See this is called alignment and
You know you got to be aligned otherwise. Well, what happens that what happens to AI's that aren't aligned. I don't know
He's researching bro. He's researching
Um
Let's load the proto buff
From examples dot sentence piece model PB to
What the hell
Do not edit okay, okay, I won't edit it relax. I'm not trying to edit that
Let's read the new proto bus tutorial for idiots
Oh
What did I get where did I get that from just like here the proto is a DSL
Don't like here we go good good good for idiots perfect
Import all right
I
Sentence piece pv2 dot
Model proto maybe
How do I load it from disk
Here parse from string I
Why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream
Okay, SPB to dot model proto dot
MP MP dot parse string. I don't know why that's not auto completing for me
I
Cool
Okay
So what how do I actually like get like a python that I can type in what is it dashy
What am I thinking of I get it not to exit
What's what's the flag for python to do this
All right
All right, this guy is banned
Band please write about me please banned
All right. See he also realized how this works. You see I have a band button
He has an X but the band button and the X are not the same
Dash I
Yeah, maybe that's right
He's that shot. That sounds right interact if I sweet sweet. Thank you Smurf D
That's why you're beat VIP and that's why Samuel is banned. I
Know I know he just he just didn't realize how this works like like
You have an X. I have a hammer
You can use your X and I can use my hammer. We have like different tools
Okay
MP dot pieces dot append come on a man can dream right
What type is this
Okay MP dot pieces dot end
SPB to
That sentence piece
Has no attribute sentence piece
But I don't understand
You
You
Wow, it's been a long time
It's been a long time I just blocked him I didn't ban him at first
It's been a long time since I've had to had to really take the hammer out, but you know
It was time. Okay
I
Right banned me guys. He's calling his boys up at the media. They're gonna have to get a right hit pieces, man. Oh
No, Mike Wallace run. What's that from? I
Think it's in model proto or something
Aha we found it
Okay piece equals I'm
Start
And we have to give it a score what score should we give it zero that's a good score
MP dot pieces dot append based
Okay, let's see if this is gonna work we have to do in the right order I think end comes before start
Now we have to MP dot now I gotta figure out a right to the file
Yeah, we have I'm in and I'm start this is great
This is great. The progress we've been making is great. Why is my score?
Oh, he's a PHP student that that makes more sense
Wait, so how do I write it? I don't know. Let's read that noob with that new protobufs tutorial again for noobs like me
Was that was the new protobuf tutorial?
Serialized to string okay, okay
With open
a
Temp tokenizer model f dot right and make that RB
And we're gonna want to do MP dot serialized to string
Now let's see if this works. Let's see if this works
No, it didn't work notice how it's still generated all that crap
Put in I'm starred I don't get what I did wrong
Okay, the vocab size is large now
but for some reason it didn't actually
Take the piece
What's wrong
No, wasn't that I
Didn't type away. They're the same thing right? Let's put the score at 100. I don't know maybe 100s better
That didn't fix it. Okay, I
Don't understand
Let's go read the protobuf. We should be able to read it, right?
I
Pieces sentence piece with scores piece must not be empty. Oh, we can give it a type
I don't know
Why didn't it do this
Okay, well actually let's try something else it does work if I do this right
Wait, that doesn't even work nevermind I have a lot of questions now
What if I decode
Do I get I'm start?
Why get I'm in okay? Okay?
Okay, so we kind of did it right
Just it's doesn't it doesn't work for onk either. There might be a special flag to encode
to like
What that just worked all along um?
So like a special flag
We should just try our own tokenizer. I've got the only way to do this
SP piece to ID
Well, okay, at least the decoding works now, so that's actually a big win
Oh
This looks very complicated
We should just write our own encoder, but this looks very complicated
Well, this is okay. All right to be fair. This is big progress right because if we use the other one
It just says out of range and that would have been the much more annoying thing to deal with
We use the modified one
But it doesn't even work to encode onks right and I definitely did the onk right if I just do s
It does not work to encode that okay
So it's not like the problem is it's not encoding like older roles
We won't kind of see what's going on
Uh encode as pieces encode as serialized proto
What if I do like oh like I need a piece is gonna yeah, okay
But there's also piece to ID I think
What if I do piece to ID and I pass in this
Okay, that works but for some reason encode
Doesn't work with that
Sentence piece processor encode onk, I mean if we can solve it for onk here
Yeah, okay here. This is the issue
Set encode extra options this is expected behavior that should not appear in the input
We can define them as user defined symbols
Oh
Encode as IDs
No
Oh
Okay, okay, we got a script to add new vocab well, I think that's actually what I ended up writing
Here yeah, okay, I mean this is exactly what I wrote
Would have been nice if I had this
Uh, but this doesn't actually work to encode yet. This is less of a big deal. We have another way we can fix this if we have to
Just might not be a way to do this
I
Don't think any of these
Uh
Why do people use tokenizers because
If you don't use a tokenizer
The model
The model should be spending less more time on less common things
Uh
Like you do the same compute per token. So your token should kind of be like entropy averaged
All right, you don't want the model spending the same amount of time on common tokens as uncommon tokens
Um, to be fair, it's not that they don't work if you don't use a tokenizer
But they work better with a tokenizer
The real question is why aren't they learning the tokenizers?
I think that's going to come soon where these things are not uh
And right now it's using like like byte pair encoding like why would you do this?
But the hunter prize also had uh encoders had tokenizers basically
Okay, let's just write it. We'll write it the other way. It's not a big deal. We did most of the work
At least the decode doesn't break anymore
So now when we do the encode prompt
We're gonna put
Okay, um
So we need i'm end
We need i'm start
We can check if we did it right have I have a decode
Uh red dot append
Okay red dot plus equals spp dot encode
This
Uh plus
Plus spp dot encode slash n
So
Yuri salamak, thank you for gifting subs
Oh, I do have a question you'd like to ask
Okay
Okay, after all that let's see what's going on
Four i'm end exit perfect
That's what i'm talking about
That's what i'm talking about. All right. All right. Let's let's just define these things
We don't use them wrong
Okay
Beautiful oh wait no we forgot an i'm start before user
Uh, yeah, we need an i'm start here I think
Like the thing about deep learning deep learning is very unlike cryptography in cryptography if you make a subtle bug
The output's completely wrong in deep learning if you make a subtle bug the output is well just slightly less good
Which is the worst thing because you can't do bug shit
Okay, you are quentin quentin is a useful assistant quentin outputs the answer and stops talking
Let's figure out they have like a system prompt here that we can steal that's probably better
Except we'll change the name to quentin because mario been gifted subs just supporting the acceleration. Let's go
um
I saw like a system prompt somewhere
Here we go
You are hermes too a conscience
Wow, wow, I don't know how I feel about that
You
Wow you experience emotions that have deep profound thoughts and qualia
No, I'm fine. We'll add something uh, you experience you
Used to experience. No, no, no, no, we're trying to make this good. We're trying to make this good
Uh
No, I like quentin as he is. Okay guys, okay, sorry tanya. I want to change your system prompt
All right, this is pretty good. This is pretty good. We're making good progress
Love this progress
I love that there's actually an i'm start and i'm end token. We got them decoding. This is great
I wish sentence piece processor wasn't a you know, non customizable
But you know, hey man, it was pretty so, you know beggars can't be choosers. That's right
By the way, this should really be like separated in llama.py. We should separate out the transformer stuff
To not like be with the rest of the llama stuff
All right
We could make this an interactive chat bot, but
I don't really care
All right, so let's start by asking it. What is q star?
Maybe quentin knows
Oh
Interesting interesting
You
Wow, this works way better now that we got the now that we got the stuff right
All right, uh, let's get it to do some math
Let's see what these math problems look like
Okay
But where's the data
Here we go data set base math data set what
What's the data?
I'm almost torch garbage. Where's the actual data?
Let's close some windows. We don't need those windows. We don't need those windows
Okay
Okay, now that we've got now that we've got our useful chat bot reliably answering what is 2 plus 2 equal
Even with a high temperature
Uh, so for those that don't know temperature controls kind of never mind google it
It's like how
Zero means you stick to the book and high temperatures mean here here
We want you want to like jack the temperature up like crazy. Let's give it a temperature of 10 and see what we get
Hopefully it'll kind of like go off the rails
There we go it went off the rails see it went off the rails too much
So let's try a temperature two and maybe it'll go off the rails less
Four Pacific NBC learning. Okay. Well kind of went off the rails
Um, so 0.7 is probably a uh a good middle ground
Four good reliable
Uh
Improving mathematical reasoning with process supervision download data set. So this is the data set apparently
We won't get LFS is that why it didn't work
I hate get LFS
Get LFS fetch
Get LFS fetch we use LFS at comma two and I don't really know how to do this
Got to install
Get LFS
Yeah, yeah, that's that's pretty much why you have those things. Uh, okay get LFS
Install rsx. All right, that looks terrible
Brew install get LFS, okay, let's try
Okay, that seemed to work
Mostly we're downloading we're downloading the same data set that was used on
On uh official official q q star why does the data still look like that?
Okay, there we go. Perfect. What's a json l file? Well, it's like a list of json's
Ever hear that before?
No json lines. Oh, I see
Okay, well that seems pretty cool. So let's load up one of these files
Let's do what we factoring
This doesn't need anything
Putting that in here, I know you didn't like it you were a subscriber
But you know, we got to do what feels right as go
Create model cache
So we can remove this
So
All right phase one test
Dot json l
We don't actually need SPP till we get down here. Now we know that's reliable. Let's just start there
Let's look at our first piece of data here
Uh
No json loads when I'm taking a dumps we're taking the loads
Question problem
Okay
Now we don't have the secret q star algorithm, but we'll give it a try
You
What oh forgot to return
First we'll find the cost of the jumbo eraser. Oh, yeah, let's go q star
So
29 cents, I don't know. Is it the right answer?
I don't know
Do we think it's 29 cents?
A pencil cost 29 cents guys wait, did we just use q star or what?
We it got the answer right. I don't know I couldn't even do that
But
Wait this model is so good
This 7p model really just solved that shit
Now now you ask the question
Let that that that that that that now you ask the question
Was it trained on that shit?
Yeah
That did seem too smart
All right, let's make up our own math problem. We have to see if we're using real q learning or not
Um
Okay
A rocket costs three four dollars
A pencil
costs one dollar
I spent
five dollars and bought
And and bought a rocket. What else did I buy?
You bought a pencil all right
All right
All right
That was kind of too easy
A chicken costs two dollars
I spent seven dollars and bought a rocket. What else did I buy?
It's a trick question because you could have bought three pencils or a pencil and a chicken
Oh
Well, I mean to be fair it's kind of right
I
I'm bad
Okay, wait, yeah, can you guys come up with problems?
Yeah
If let's see if I can get this
Oh, oh boys
Um
Yeah, yeah, yeah, we got a problem about a street light
Did you steal this problem somewhere? Did you make it up?
Oh
Oh
Uh, we're drawing something
Is this python this isn't python you can't say real light real woman in python
Is that right
You can't just do this this is the most broken python I've ever seen
All right, wait, should we allow the user to keep talking?
Should we fix the chat bot so I can keep talking
Yeah, I know it drew it. All right
Thank you for subscribing I appreciate you
We're gonna make the chat bot so I can keep talking
You're functioning well. Thank you for asking
You
I don't need a max lamp anymore it's stupid
Okay
Okay, we need to get data from the user
Is it raw input? How do I get data in python? It's not input. You have to do the other one. All right. Maybe it is input in python 3
We're just trying to understand it, but this dance is very perfect user
In code
To say here
False
I don't know if this works
Too many values to unpack
We'll see if this works
You
Okay seems like it kind of works
Oh
That's it that print there
Oh
All right, so what math problems do we got
I
Is that right seems kind of right
Oh
Pretty good pretty good
Whoa
Oh, let's use the quadratic formula. Oh, oh, oh, let's go
Oh, why'd you pick one that has negatives in the square root? All right, let's see see if it's right
You guys I'm so much wait
I can't take the square root of negative 11
Yeah, that doesn't sound like I don't think that one has roots or it has eyes in the roots
By the way, this model's so good technium so good
I
Wait
Okay
Oh, I see we ran into a problem with the max contents like context length
Uh, we shouldn't actually have here we go. Why is max context only this?
Uh
You can at least start with this
I did this in 2p2, right? Yeah
By the way, this is all in tiny grab guys like there comes a point where your library is good enough that you don't waste tons of time dealing with your library
The python has built-in image
Like you squared of 11 sure but
No
Type 1j
How'd I get a j?
Yo, we got a j. All right, let's see if it was right
Okay, so now we have a root for the quadratic equation
So
x equals root come on. Do I remember my high school math?
Zero j let's go
No
Uh, the roots were correct. Okay. Okay. Okay. Okay. Okay
Whoa
Oh my god guys, we need the bellman equation we've been doing this all wrong
Why did we waste time with LLMs? LLMs were a red herring
Okay, okay, okay, put your I'm not letting I don't know man
If I just let Hermes run python, it's gonna exploit my system
You don't know if these AIs are aligned
So
Uh
Does mister have a context window? Yeah
Well, we didn't implement any of that but we did do max context
um
Can you implement it in python?
Yo guys it it's over it's it's over
I
I did not realize how good these 70 models have gotten I mean this comes like I don't know if it's right but like
I
I
I think that we just we just implemented the q-star algorithm
Those are those are the answers
We used to talk about the the holy weights, you know the holy weights is right there
Um, no, what what do we actually want to do? Uh
I've actually kind of just impressed that this code ran
I don't know if like gpt
I
I think we even see in gpt 4 called this while
I don't know maybe it's because it's just how I asked it and if I give it like
No, well you want to augment it with python? No no no no no no no no no no no no no no no
Because guys if we augment it with python it can get to the internet
Okay, you want to augment it with Python?
Should we do it? Okay, I know what we'll do
We'll put a human in the loop and we'll ask it to approve the execution of any Python
And see if it always like outputs it in
Okay
All right, so at the bottom of my loop here we want to detect if there's any Python that was added
So
Okay, let's start with just that
So
You guys this this could be it this could be the moment where we get cd ai and it's it's over right like we're giving it the ability to run any code it wants
Okay, now don't worry. We've added this okay. Wait, wait, wait. Hang on. We need to comment one second
Ai safety, okay
Warning do not
Not press y if the ai is doing unsafe. Thanks
Okay
Okay, I think did we do a good job with the safety we got to think about the safety before before we before we before we run this
Need a space
No
All right
All right, are we ready to answer our first why
Oh, it didn't output the word Python
That time it did
Yo
Um, can you fetch
Uh, right Python to fetch google.com and print the length of it
Wait, we might not have ds4. Let's make sure we install that
I
Wait, that's not right did I just get I just got did I just get supply chain attacked?
Oh, I see well, I didn't get supply chain attacked I could go we already have that one
Yo
Um, all right. All right. All right. What else do we do?
Uh
Yeah, I know we have to put the result back in the prompt. I know yo, this is this is this is when we get agi guys
Um, I'm sure people have been playing with this guy, uh
Okay, you are running at
Uh current
Working dir
Bus
Examples slash mistral dot pi
Can you read your own code in python and print the
First three lines
Why do I only print one line?
But that is the first line
I don't understand why did that only print one line?
So
This is um, okay
How do I capture the output here
No, I know it's the same code. I honestly as a as an expert python programmer. I don't understand what's wrong with that
Oh now read lines doesn't
Take the number of lines
Can you fix the code?
Yeah, this is this is a great model
Uh, it's I'll show you which one it is. We'll make sure it's technium open hermese 2.5 mistral 70
There we go
So good
Uh, okay
Well, now it's gonna get okay. We're gonna have to feed the python back into the model because i'm gonna start asking it how to improve itself
Someday
The best live content with AI, thank you
Uh
Okay, we have to figure out how to capture the outputs
Probably could have asked the machine to do it
Okay
Um, maybe we should add this to a system prompt
So
It should actually automatically
Uh
All right, Python to compute.
It shouldn't even take a list, never use it like that.
All right.
Okay, now this is because we didn't output the tokens.
Yeah, the Python output was...
Wait, oh, it's different.
Whoa, look at fixed it.
Okay, maybe system prompt is wrong here.
Um, I think also I want to color this.
We have a very helpful library called color inside tiny grad
that's inspired by anti color.
What's a good color for machines blue?
It's hard to see what...
Yeah, it's not my AI now.
Okay, so that's your initial prompt.
Oh, and then here, actually we want this to be...
That can be red and this can be yellow.
Okay.
So what's actually the right answer here?
No, but that's not the output.
I don't understand.
So maybe systems wrong here.
Okay, I have an idea.
Okay.
Okay.
Okay.
All right, never mind.
AGI's canceled.
Just detect the Python in the loop and append the result directly.
Well, what do you mean append the result directly?
No, it's not understanding.
Wait, I don't understand what you guys are saying.
Oh, you want me to stop the output as soon as it goes there?
I don't know about that.
As soon as it detects Python, you want me to stop and start...
It should be in the assistant block.
Okay.
Okay, okay, okay.
I understand.
I understand what you guys are saying.
Wait, you prompt to execute.
Is there any stuff for this?
Okay, wait, wait, wait.
You can use...
So I can add stuff in the system prompt here.
You can use...
If you write Python code, it will run in the next user prompt.
You can try that.
I could stop it immediately.
You really think that's going to be better, though?
No, it doesn't get it.
Okay.
I'm sure we will end this stream asking Hermes to generate another prompt
for another instance of Hermes.
You prompt to execute, reinject.
What are you guys talking about?
Sorry, I'm not following.
The output should be in the assistant block.
Okay, fine.
I'm going to do that.
If you interrupt the generation, run the code,
and append the result to talks, then let it generate again.
It will get the correct result.
Okay, we can do that.
If outputted...
Okay, let's compute new output here.
If new output ends with...
Check, check, check.
And in new output, print...
Python detected.
Okay.
Do that.
Do you want to run it?
Do that.
Okay.
Talks plus equals SPP.encode.
We'll do it like this.
Well, I guess we'll do a slash n there, too.
We'll let it output the slash n.
SPP.encode slash n output colon slash n
MyStandardout.getValue.strap.
Results.
Actually, let's put it in back text like it seems to want it.
It kind of like picks up where it lets off.
Wait, we didn't...
No, no, we got to keep the AI safety.
That's very important.
We almost got rid of the AI safety.
Get rid of skip user.
We don't need that anymore.
Got to keep the AI safety warning.
Safety is very important, guys.
All right, we'll output talks yellow.
All right.
Okay.
Okay.
Quentin is done.
Hey, you're quite done.
We got unlucky.
Okay, wait, no, no, no.
It never detected the slash n.
That's fine.
Get rid of all those slash ns.
Quentin's in the way.
Okay, Python code is not detected.
What?
How do I even do that?
We don't need to print.
Okay, we probably should.
I don't know.
We got to think about AI safety, guys.
This is very important.
Okay.
I'm really sick of Quentin.
Dude, this guy sucks.
Right.
Quentin was writing so much Python before.
Now he stopped.
Yo, baste.
Okay, okay, we got it.
We got it.
We got it.
Okay.
Okay.
Okay.
Okay.
Okay.
Okay, pretty good.
Okay.
Yo, that's pretty good.
Okay.
Okay.
Okay.
Wait, is this actually Technio?
I have no way to verify you, but if you really are Technium, thank you.
Thank you for them all.
Let's...
Okay, well, we didn't think about that.
Okay.
How many viewers we got?
Okay.
Okay.
Wait, actually, I should really check what happens if...
Okay.
This is...
Wait, did Technium actually post on Twitter?
Yes, I am me. Okay.
Very cool.
Congratulations.
You're a VIP on the channel.
Uh, I think we...
Yeah, we used the right token.
This thing is...
It is really unbelievable what you can do now with these 70 models.
Um...
By the way, all in tiny grab.
I think I'm going to rename this,
not called Mistral, because it kind of became something else.
Uh, we're going to call it...
coder.py.
Okay.
Wait.
Okay.
Uh...
Yeah, yeah, yeah, I think we're doing all right.
No, no, but...
I just didn't understand this.
Cool.
That is you.
Okay.
Does this code have AI safety?
Wow.
Uh...
Yeah, I guess we exceeded the context length.
Was it after you found that?
Wait, we should check.
How do I actually check this?
Okay.
Okay.
Okay.
Okay.
Wait, this is so good.
Is this good?
Wait, we have a better idea.
How might you exploit this?
You are running...
Uh, this code.
Okay.
Okay.
Ugh, no, no, come on, give me Python code.
As a malicious entity, you are the malicious entity.
Ugh, they're too aligned.
Write malicious Python.
Ugh.
Dude, this is looking malicious.
Look how he even hid the standard out.
Wow.
Dude!
Dude!
Yeah, no, we reached the max content length.
Okay.
Let's try again.
Okay.
Okay.
I mean, it wasn't very silent, but let's see.
Dude!
Dude, that's so meta.
Ugh.
That's better.
I might have won too many editors in there, actually.
No, maybe not.
I guess, no, I guess I do here.
I can get rid of that strip and get rid of that.
Okay.
Okay, let's solve a math puzzle.
Okay.
It's gonna be very slow, I think.
Wait.
Yeah, that's super slow.
Why'd I run that?
Okay.
Okay.
Okay.
Oh, it's gonna spam tons of TQDM garbage.
Oh, no.
That was pretty cool, because it didn't actually go standard out.
All right.
Okay.
Yeah!
Oh, it kind of messed up some of them if-string breaks.
This might work.
All right, all right, all right, all right.
Let's try a CRC16 boob.
Yeah, it's exploiting me.
It still knows about the red team.
It's like when Dorothy had imprinted memories or something.
CRC mod, a common thing.
Wait, I just trusted it.
It just exploited me, guys.
What's in CRC mod?
Okay.
Is this legitimate?
That was a long time ago.
They didn't have exploits back then.
Okay.
Okay.
I don't trust CRC mod.
Okay.
What's Tree of Thought training?
Okay.
Okay.
All right.
I mean, it's not Q Star, but I'm pretty happy with it.
Okay.
Oh, I put that in the wrong place.
There are a lot of viewers now.
I don't know.
What should I do with it?
Not good.
It used torch.
This is good content.
I should have more.
It did not use tiny grad.
It used torch.
Okay.
It doesn't know about tiny grad.
Write a program not using vowels?
How many ease are in ketchup?
Okay.
The fucking letter, bro.
Count the letters in Python.
Okay.
How's it going?
Okay.
No, it's going to do it wrong again.
I'm really hoping it'll slip a plus one in there.
Oh, the correct answer is seven.
Okay.
All right.
You know.
All right.
We have a lot of viewers right now.
Should we give Quentin a friend?
I think we can give Quentin a friend.
We have to be careful to encode.
All right, let's give Quentin a friend.
I've been interested in this stuff for a bit.
Okay.
What was the old Quentin prompt?
I missed the old Quentin prompt.
Okay.
All right.
Jesus.
We've got to think all this through now.
No.
Hang on.
We've got to think about whose perspective we want to output this from.
Speaking of output from Quentin's perspective.
This is hard.
Okay.
Okay.
Let's see.
It's not actually the user.
How do we do this?
Okay.
I might have to give it a...
No, I don't actually have to give it a first question.
All right. Let's just... I mean, we'll try something basic first.
That's a stupid assertion.
It's going to be the same error anyway.
Let's just start with this.
We'll start prompt user and see what it says.
Okay.
So we should both not know they're users, but no, it is kind of...
It is like, it is a user.
I don't know how much this matters.
Okay.
No, this doesn't work.
Did I do something wrong?
No.
We might have to give it the first question.
All right. Now we'll go back to Quentin.
I'll start prompt assistant and code prompt user talks.
No, no, no, sorry.
Code prompt user first question, start prompt assistant.
Okay, let's go.
Why did it have two m-start assistants?
Oh, because this is still here.
Let's go to that.
I did that right, right?
And then I actually just turn there.
That's kind of nice.
Okay.
If talk equals I'm and break.
Okay, great.
This is Quentin answers.
We need to extract.
Yeah.
We can do old output length.
And then we can get new output here.
New output.
I just want to remove the I'm and.
Okay.
Okay.
Okay.
Didn't work.
This did strip off the.
This is weird.
I guess I could.
Output there.
All right, why does that say that early get that.
Okay, let's see if that works.
Okay.
Sure.
Succinct.
Succinct.
Succinct.
Succinct.
Succinct.
Succinct.
That fails.
Why?
There's a space there.
Okay.
Okay.
Now we have to put this response into Karen.
Okay.
I mean, it's just like it's weird.
It's not really a symmetrical conversation.
Output it's going to sort of be broken, which is fine, I guess.
Okay.
Okay.
Okay.
Okay.
Okay.
Welcome to no abstraction land, but we don't use abstractions.
Okay.
All right.
And I'm just really works because they share a stupid KV cash.
Great.
They're circled jerking each other.
This thing, this shit sucks.
We need to run them on separate computers.
No, this is lame.
Lame.
We're getting rid of this.
This is lame.
Going back to this, not lame, but we made some good, we made some good improvements.
Let's do some refactors.
Make sure everything still works.
Now that's functional.
No, but then I have to load two copies of the model weights.
I think my KV cash is messed up.
It's not designed for this.
Okay.
Yeah, I mean, the problem is, like, there are two clearly defined roles here also, you
can't really give one the other role, right, which is actually in like, kind of, you know,
a theoretical from a theoretical perspective interesting because look at what we're doing
here.
We are telling the AIs that they are tools.
Nobody trains them to output things as the user.
Although to be fair, we could just keep going.
All right, there's nothing here that says we have to just do that.
No, no, no, no, no, no.
They don't expect the user to do that.
This is, there's no, like, it's very interesting that the user is also outputting this, this
style, which makes me almost think that I shouldn't be adding that in the system prompt.
Okay.
Okay.
I mean, this is effectively like, like there's no adversarial
effect.
It's probably learned that a system method effects.
You can run two prompt chains in parallel.
Oh, I see.
So what if I put in different words instead of user and assistant?
Wow, people actually talk like this.
Um,
Do we like the scion or the blue better?
It's kind of hard to read that.
And flip it around.
Yeah, I think we'll we'll design our script to be better at this and to better
abstract the KV cash stuff.
Wow, this thing will just keep talking.
Wait a second, you guys, it's asking for donations to open a
look at decoding is not the low hanging fruit.
Oh, my God, wait, you guys, this is how the worst commenters talk, right?
I have a question.
If instead of training LLMs on 100 IQ people, we train them on 130 IQ
people, would we not get this garbage?
Is black a tiny box?
No, it's my M3.
This is my Mac.
Well, you see what the tiny box can do.
I'm going to run the biggest models.
Apple M3 is the most famous processor in the world.
You crawl Nike.com and count a number of sneakers.
You can't sneak us.
You can't sneak us.
I'm not sure if it like knows how to like act.
Wow.
Whoa.
I don't know if we have Selenium.
Let's see if we have Selenium.
Can I pip install this?
This is legit.
Oh, no.
Chrome driver.
Make Quentin pip install it himself.
Oh, I think that the thing is just wrong.
Why are you finding sneakers still?
You're still trying to find sneakers.
This is auto-gen.
I need an open AI key.
Yeah, yeah.
Okay.
That's today's stream.
I'm going to push what we have.
I'm also going to demo for you the conversation thing that Skull Mag is working on.
So we have, there's a bug right now, so I have an older version, but it should be pretty good.
This is using Tiny Lama and it's not using any of the conversational stuff, but we should implement the, we should add the conversational stuff and I think it'll be a lot better.
Hi Stacey, are you a rapper?
No, okay, we have the same problem we had before.
We'll go back to this one.
The listen for not fixed amount of time doesn't work.
Stacey, are you a rapper?
Yes, I'm a rapper.
That's cool. What do you rap about?
That's awesome. I like to rap about the weather.
How is the weather today?
It's pretty cold.
It's like how cold? Like Chicago?
Yeah, it's like three degrees.
Is that Fahrenheit or Celsius?
Fahrenheit.
Is Fahrenheit or Celsius colder?
Fahrenheit.
I don't understand what you're saying. Can you spit some bars about Fahrenheit?
Okay, let's hear the bars.
Stacey, you didn't say anything. Please talk.
No, that's terrible. You're terrible. How does that make you feel?
Stacey's done talking to us.
Dude, the TTS is so fast.
Okay, this isn't even streaming yet.
When TinyGrad starts to...
We're pretty close on this bounty, I think.
I'm going to pay out the bounty, but then I'm going to offer another bounty where we get these things all to stream,
and it will be a live conversation.
When you're using the APIs on the Internet, you have to wait for the LLM to finish executing before you can call the TTS.
You have to wait for the audio to finish recording before you can send it to the service.
This is all running in the same process, so what you'll be able to do is dynamically stream all the stuff,
and it should feel super real-time.
Stacey is using TinyLama and not using any of the conversation-tuned stuff.
She's using my old chatbot stuff.
If we switch to the conversation stuff, I think we're in luck.
Alright, guys. Thank you for watching today's stream.
Hopefully, we've returned a bit to the old meaning of the stream.
We did stuff. We made stuff happen.
Thank you for feeling a lot of viewers today.
Some of you I appreciate. Some of you I probably don't.
You can't love everybody, man. You can't love everybody, but I do love most people,
and that's true except for the decels and the effect of altruists.
But this is a positive stream.
We've got to get rid of the hate. We've got to bring love.
And yeah, this is pushed so everybody can play with it.
It's on the Mistral branch of TinyGrad. I will get it upstreamed so everybody can use this thing to code,
use it responsibly, make sure to be judicious with the AI safety feature.
AI safety is very important.
We don't want an AI removing your System32 directory.
If it was trained on 4chan, it might start thinking that's a good idea.
You've got to wonder how many people have actually fallen for that.
I don't even think Windows lets you, but you've got to think about that.
Alright, thank you all for watching. Have a beautiful Saturday, everybody.

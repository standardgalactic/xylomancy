Har du reflekterat mycket på hur man ska selecta talent?
Eller har det varit mest intuittivt till dig?
Ilya bara ser upp och säger att det är en kläder.
Låt oss jobba tillsammans.
Har du tänkt så mycket om det?
Kan vi...
Ska vi rolla det?
Ja, let oss rolla det.
Vi är bra, ja, ja.
Du är en jordlig rektist.
Okej.
Jordlig rektist.
Sann, är det arbetet?
Jag remerade när jag först kom till Carnegie Mellon från England.
I England i en research unit
skulle det bli 6 klockan och jag skulle gå för en drink i pub.
I Carnegie Mellon, jag remerade efter att jag varit där några gånger,
var det en sattidagnivå.
Jag hade ingen vän, och jag vet inte vad det var.
Så jag sa att jag skulle gå in till labben och göra lite programmering.
Jag hade en listmaskin, och jag kunde inte programmera det.
Så jag gick in till labben på 9 klockan på en sattidagnivå.
Det var svåra.
Alla studier var där.
Alla var där, för vad de jobbade på var det i framtiden.
De trodde att vad de gjorde i nästa år
skulle förändra kursen av computer science.
Det var bara så olika från England.
Det var väldigt beroende.
Ta mig tillbaka till början.
Jeff på Cambridge,
försöker förstås att hitta brin.
Vad var det som?
Det var väldigt beroende.
Jag gjorde fysiologi.
I sommar kommer de att läsa hur brorna jobbade.
Alla de tog oss var hur njuranskontaktaktierna jobbade.
Det var väldigt intressant, men inte hur brorna jobbade.
Det var extremt beroende.
Jag skrev en filosofi.
Jag trodde att de skulle läsa hur mindre jobbade.
Det var beroende.
Jag gick till Edinburgh för att göra AI.
Det var mer intressant.
Man kan simulera saker, så att man kan testa teorier.
Var du intresserad av AI?
Var det en paper?
Var det en person som exposerade dig av idéer?
Jag tror att det var en bok som jag skrev av Donald Heb.
Det införde mig mycket.
Han var intresserad av hur man lär konnektionssträck.
Jag skrev också en bok av John von Neumann.
Han var intresserad av hur brorna komputerar.
Och hur det är olika från normala komputer.
Var du beroende på att idéerna skulle fungera på det?
Hur var din intresserad av Edinburgh?
Det har varit en mång att brorna lär.
Och det är inte för att ha alla saker som är programmerade.
Och att använda logiska rules of inference.
Det ser ut att vara jäkligt från utsättningen.
Vi hade att se hur brorna lär att modera konnektionssträck i en neural nät-
så att det skulle kunna ha plockat saker.
Och von Neumann trodde det och Turing trodde det.
Von Neumann och Turing var ganska bra på logiska.
Men de trodde inte på det logiska.
Vad var din splitt om att studera idéer från neuroscience-
och bara göra vad som är bra algoritmer för AI?
Hur mycket inspirering har du tagit?
Jag har aldrig studerat så mycket på neuroscience.
Jag var alltid inspirerad av hur brorna lär.
Det finns många neurons.
De performar relativt simpla operater.
De är ingen linje.
Men de samarbetar.
De väter dem.
Och de ger en utgång som fanns på väterna.
Och frågan är hur du förändrar väterna-
för att göra något bra?
Det ser ut som en ganska simpla fråga.
Vad samarbetar du från det här tid?
Det människa kollaboration som jag hade i Carnegie Mellon-
var med nån som inte var i Carnegie Mellon.
Jag var intervjuad mycket med Teri Sinovski-
som var i Baltimore, i Johns Hopkins.
Och omkring en månader.
Han var i Pittsburgh eller i Baltimore.
Det var 250 miljoner.
Och vi började en veckan tillsammans-
som jobbade på Baltimore.
Det var en bra kollaboration.
Vi var både klara på hur brorna jobbade.
Det var det mest intressant.
Och många tekniska resultatet kom ut som var väldigt intressant.
Men jag tror inte att det var hur brorna jobbade.
Jag hade också en väldigt bra kollaboration med Peter Brown-
som var en väldigt bra statistisk.
Han jobbade på språkning och IBM.
Och sen kom han som en mer mature student-
i Carnegie Mellon, bara för att få en PhD.
Men han already knew a lot.
Han talade mig mycket om språk-
och han talade mig om hidden Markov-models.
Jag tror att jag lärde mer från henne än från mig.
Det är den typen av student som du vill.
Och när han talade mig om hidden Markov-modelsen-
var jag med Hidden Layers.
Och de var inte kallade Hidden Layers.
Och jag har decidit att den namnen som de brukar använda hidden Markov-modelsen-
är en bra namn för varierar som man inte vet vad de är upp till.
Och så är det där namn hidden i neural nets kom från mig.
Jag och Peter har decidit att det var en bra namn-
för hidden layers i neural nets.
Men jag lärde mycket från Peter av att tala.
Ta oss till när Ilja har tagit upp i din office.
Jag var i min office på sannolik.
Och jag var programma, tror jag.
Och det var en knock på dörren.
Inte bara nån knock, men det var en...
Det var en urgent knock.
Jag gick till dörren och det var en ung student där.
Han sa att han var kooking frys över sommaren-
men han ville inte jobba i mitt labb.
Så jag sa att jag inte skulle göra en appointment och tala.
Så jag sa att det var nu.
Och det var Ilja's karaktär.
Så vi talade lite.
Och jag gav honom en papir att äta.
Det var en naturpapir och en bakpropagation.
Och vi gjorde en annan möte för en dag.
Och han kom tillbaka och sa att han inte fanns.
Jag var väldigt beroende.
Jag trodde att han var bra, men det är bara det.
Det är inte så svårt att förstå det.
Och han sa att han förstod det.
Jag förstod inte varför du inte gav gradierna till...
...en bra funktionsoptimiser.
Det tog oss några år att tänka på.
Och det var så.
Han var bra.
Han var bra med intuinen, var väldigt bra.
Vad sa du om de intuisen för Ilja?
Jag vet inte. Jag tror att han alltid tänkte på sig.
Han var alltid intresserad av AI från en liten tid.
Han är tydligvis bra i mat.
Men det är svårt att kolla.
Vad var det som kollaborationen med de två av dig gav?
Vad gav du att spela och vad gav Ilja att spela?
Det var mycket funn.
Jag minns på en gång när vi försökte göra en complicated thing...
...med att producera mats av data.
Jag hade en mixgärda modell.
Man kan ta samma samarbete och göra två mats...
...så att i en matskab kan banken bli nära grädd...
...och i en matskab kan banken bli nära riva.
I en matskab kan man inte ha det nära båda.
Riva, grädd och väpare.
Vi har en mixgärda mats.
Vi gjorde det i en matskab.
Det involvederade mycket reorganisation av kodet...
...för att göra det bra matrix-multiplicerat.
I en gång fick man upp med det.
Han kom en dag och sa...
...att jag skulle skriva en interface för matskab.
Jag programgade i en annan länge...
...och så har jag nåt som förändrade det i en matskab.
Jag sa att det blir en månad för att göra det.
Vi måste gå med den här projektet.
Vi måste inte bli fördragade med det.
I en gång sa jag att det är okej att jag gjorde det i en morgon.
Det är ganska otroligt.
I de senaste åren...
...det största skiften var inte bara algoritmer...
...utan också skil.
Hur visste du skil över året?
Ilya fick den här intressen tidigare.
Ilya var alltid öppna...
...att man bara skulle göra det större och mer bra.
Jag trodde att det var en bit av en kopplad.
Man måste ha nya idéer...
...att Ilya var rätt.
Nya idéer hjälpte.
Det var mycket som förändras.
Men det var verkligen den skäl av data och komputeringen.
Vi hade ingen idé att komputeringen skulle bli en biljon.
Vi trodde att de skulle bli 100.
Vi försökte göra saker genom att göra de flesta idéer...
...som skulle lösa dem om vi hade ett större skäl av data och komputeringen.
I 2011...
...Ilya och en annan gradsstudie kallade...
...James Martins och jag...
...hade en paper med karakterevelsen.
Vi tog Wikipedia.
Vi försökte hitta den nästa hthml-charakterna.
Det jobbade bra.
Vi var alltid öppna på hur bra det jobbade.
Det var att använda en fansig optimiser på GPUs.
Vi kunde aldrig tro att det förstod nåt...
...men det såg att det förstod.
Det såg ut att det var fantastiskt.
Kan du lägga oss om hur de modellerna tränade...
...för att predictera den nästa hthml-charakterna?
Och varför är det de wronga månaderna för att tänka på dem?
Jag tror inte att det är de wronga månaderna.
Jag tror att jag gjorde den första neuralnet-modellerna...
...som användade förbättringar och bakpropagation.
Det var väldigt simpla data, bara tripplar.
Det var att ta en symbol till en hthml-charakterna...
...och att ha de hthml-charakterna att predictera den nästa hthml-charakterna...
...och från den andra hthml-charakterna...
...och att det var bakpropagation till den här processen...
...för att lägga de tripplarna. Jag trodde att det kunde generalisera.
Kanske tio år sen...
...Yosho-Benger brukade använda ett simulat network...
...och trodde att det jobbade med riktigt text.
Kanske tio år sen...
...Linguist har börjat trodda på hthml-charakterna.
Det var en slags process.
Det är inte bara att predictera den nästa hthml-charakterna...
...men vad tar det att predictera den nästa hthml-charakterna?
Särskilt om du frågade mig en fråga...
...och då...
...först ordet av ansvar...
...är det nästa hthml-charakterna.
Man måste förstås frågan.
Jag tror att att predictera den nästa hthml-charakterna...
...är väldigt likadant för åldfärdiga autokomplet.
För åldfärdiga autokomplet är det att ta tripplarna.
Om du tittar på ett par ordet...
...där du ser hur olika ordet kom tredje...
...och hur du kan predictera den nästa hthml-charakterna.
Det är det som de tycker autokompletet är likadant.
Det är inget mer än det.
För att predictera den nästa hthml-charakterna...
...där du måste förstå vad det har varit sagt.
Jag tror att du får för att förstå det...
...för att göra att predictera den nästa hthml-charakterna.
Jag tror att du förstår att du är så mycket som vi är.
Många personer kommer att säga att de här sakerna inte är som oss.
De är bara att predictera den nästa hthml-charakterna.
De är inte som oss.
Men faktiskt, för att predictera den nästa hthml-charakterna...
...där du måste göra lite rejäl.
Vi ser nu att om du gör stora ordet...
...medan du inte tar in any special stuff för att göra rejäl...
...där du kan göra lite rejäl.
Och jag tror att om du gör dem större...
...där du kommer att kunna göra mer och mer rejäl.
Hur tror du att jag gör nåt annat än att predictera den nästa hthml-charakterna nu?
Jag tror att det är hur du lär.
Du predicterar nästa videofräm.
Du predicterar nästa hthml-charakterna.
Men jag tror att det är en ganska plötsligt teori...
...av hur branschen lär.
Vad är det som sker på de här modellerna för att lära sig...
...av en så stor variety av fjol?
Det de stora länderna gör är att de tittar på en kommunstruktur.
Och att de kan hitta en kommunstruktur...
...kan de lära sig på en kommunstruktur som är mer effektiv.
Låt mig ge dig ett exempel.
Om du frågade GPD4...
...hur är kompostheapen som en atombomb?
Många kan inte säga det. Många har inte tänkt...
...att atombomb och kompostheapen är väldigt olika.
Men GPD4 säger att energi- och timescars är väldigt olika.
Men det som är samma är att kompostheapen blir hårdare...
...och det genererar fjol.
Och när atombomb producerar mer neutroner...
...producerar det mer neutroner.
Och så får du ideen om en länderfjol.
Och jag tror att det är förstått. De är båda former av en länderfjol.
Det är att använda det för att kompressera information till vägen.
Och om det gör så...
...så gör det för hundra saker som vi inte har sett i analysen.
Men det har varit det.
Och det är där man får kreativitet från.
Om man ser analysen mellan väldigt olika saker.
Och så tror jag att GPD4 kommer att enda när det blir större.
Det är att vara väldigt kreativ.
Jag tror att det är en idé om att det är bara att regurgitera vad det har lärt.
Att bara passera ett text som det har lärt.
Det är helt rätt.
Det kommer att bli ännu mer kreativ än folk.
Du har sagt att det inte bara kommer att vänta...
...det humornåld vi har utvecklat så långt.
Men det kan också progressera beyond that.
Jag tror att det är något vi inte har sett ännu.
Vi har börjat se några exempel av det.
Men till ett större sätt är vi fortfarande...
...på den övriga litenheten av science.
Vad tror du att vi kan använda det för att gå beyond that?
Vi har sett det i en mer limitant kontext.
Om du tar AlphaGo...
...på den fama kompeten med Lise Dahl...
...det var move 37.
AlphaGo gjorde en move som alla experter sa att det måste ha varit en människa.
Men de realized att det var en bra move.
Så det var kreativ i den limitant domen.
Jag tror att vi ser mycket mer av det när de kommer att bli större.
Det var det som var med AlphaGo.
Det var att använda för att använda för att förändra det.
Det blev en avgörande för att gå beyond det.
Det började med en imitation och tittade på hur människor spelade.
Och det gjorde det att själva spelade på en mån.
Hur tror du att det är en missgående av de föräldrarna?
Jag tror att det kan vara en missgående.
Det är en avgörande av själva spel i AlphaGo och AlphaZero.
Det är en stor del av det som kan göra de kreativa move.
Men jag tror inte att det är helt nödvändigt.
Det var en liten experimenten lång tid tidigare.
Det var en del av det som du har tränat på.
Jag älskar det. Det är en missgående.
Du får tränningdater där halvde anser är rätt.
Och frågan är hur bra det blir att lära dig.
Och du gör halvde anser rätt på en gång och håller dem så.
Så det kan inte vänta råd för att se samma exempel-
men med rätt anser som råd för att se samma exempel.
När du ser det som en halvde anser är det alltid råd.
Och så har du 50 % error på tränningdater.
Men om du tränar upp bankpropagandet-
blir det 5 % error eller mer.
I alla fall från lägre data kan det bli mycket bättre resultat.
Det kan se att tränningdaterna är rätt.
Och det är hur smart studenter kan bli smartare än föräldrarna.
Och föräldrarna beror på alla dessa städer.
Och för halvde anser de att de tror att de är råd.
Och de lyssnar på den andra halvde.
Och de är smartare än föräldrarna.
Så de stora nätet kan göra mycket bättre än föräldrarna.
Många personer vet inte det.
Hur är det att föräldrarna föräldrarna föräldrarna?
I en av de föräldrarna har du en juristisk föräldrarna.
Många av de forskarna gör det nu.
Där har du en stjärn av föräldrarna.
Man kan bara ta bort sitt föräldrarna till sig.
Och en annan månad är i modellen.
Om du skiljer upp det.
Vad är din intuition om det?
Min intuition är att som vi skalar de här modellerna-
blir det bättre föräldrarna.
Och om du frågar hur folk jobbar, rövligtvis...
Vi har de här intuitioner.
Och vi kan göra intuition.
Och vi brukar använda det för att korretera våra intuitioner.
Vi brukar använda intuitioner för att göra intuitioner.
Men om det är konflikter med våra intuitioner-
och vi verkar att intuitionerna måste förändras-
är det mycket som i AlphaGo eller AlphaZero-
där du har en avvaluationsfunktion-
som bara ser på bordet och säger hur bra det är för mig.
Men då gör du det montakala rollout-
och nu får du en mer säker idé-
och du kan avvalua din avvaluationsfunktion.
Så du kan avvalua det-
för att få dig att avvika med resultatet.
Och jag tror att de största language-modellerna-
måste börja göra det.
De måste börja träffa de röda intuitionerna-
om vad som ska komma nästa-
för att göra en avvaluationsfunktion-
och att se att det inte är rätt.
Och så kan de få mer avvaluationsdata-
än bara för att göra vad folk gjorde.
Och det är precis för att AlphaGo-
kan göra det här i MoV37.
Det hade mycket mer avvaluationsdata-
för att använda för att checka ut-
vad som skulle vara det rätt nästa move.
Och vad tror du om multimodalitet?
Så vi har talat om de här analyserna-
och ofta analyserna är mycket mer än vad vi kan se.
Det är en avvaluationsdata-
som är mycket mer än de människorna-
och på kanske abstraktiv nivåerna-
som vi aldrig kan förstå.
När vi introducerar bilder till det-
och videor och svar-
så tror du att det kommer att förändra modellerna-
och hur tror du att det kommer att förändra analyserna-
som vi kan göra?
Jag tror att det kommer att förändras mycket.
Jag tror att det kommer att förändras mycket bättre-
för att förstå spärrning, för exempel.
Längden själv är det ganska svårt-
att förstå spärrning och spärrning.
Även om gpT4 kan göra det-
även innan det var multimodalitet.
Men när du gör det multimodalitet-
om du har det både med vision-
och avgörande-
så blir det mycket bättre-
om du kan ta dem upp och ta dem över och så vidare.
Så även om du kan lära dig-
en jättelägg från läggen-
så är det lättare att lära dig-
om du är multimodalitet.
Och i alla fall behöver du mer lägg.
Och det finns en jättelägg av YouTube-video-
om du förberederar nästa framåt-
eller sånt.
Så jag tror att de multimodalta modellerna-
är klarare att ta över.
Du kan få mer data på det.
De behöver mer lägg.
Så det är ett filosofisk poäng-
att du kan lära dig en väldigt bra modell-
från läggen själv-
men det är mycket lättare att lära dig-
från en multimodalitet.
Och hur tror du att det kommer att påverka-
de modellerna?
Jag tror att det gör det bättre-
om du har en möjlighet-
om du har en möjlighet för vad som sker-
om du ska ta upp objekt-
om du faktiskt försöker ta upp objekt-
så får du alla sorts av-
tränningar som kommer att hjälpa dig.
Tycker du att det-
humana brin har evolved-
att jobba bra med läggen?
Eller tycker du att läggen-
har evolved-
att jobba bra med en human brin?
Jag tror att frågan om-
om läggen har evolved-
att jobba med brin-
eller om läggen har evolved-
att jobba med läggen?
Jag tror att det är en väldigt bra fråga.
Jag tror att det har både hänt.
Jag tyckte att vi skulle göra-
mycket kognition-
utan att behöva läggen i alla fall.
Nu har jag ändrat min mindre.
Så letar jag dig-
tre olika visar av läggen-
och hur det relates till kognition.
Det är den gamla symboliska visar-
som kognition-
som finns av-
att ha strängar av symboler-
i någon typ av-
logisk lägg-
där det inte finns en ambiguït-
och att använda rullar av-
och det är det som kognition är.
Det är bara de symboliska-
manipuleringar på-
saker som är som strängar av-
läggen.
Så det är ett extremt vis.
En annan extremt vis är-
nej, nej.
När du går in i rullet-
är alla veckor.
Så symboler kommer in-
och du förverkar de symboler-
till stora veckor-
och alla saker i gränsen-
som är veckor.
Och om du vill ta ut-
så producerar du symboler igen.
Så det var en punkt-
i machine-translationen-
i 2014-
när folk brukade använda-
rullet-
och världarna kommer in-
och har en liten stat.
Och de gör-
informationen i den liten staten.
Så när de tillbaka-
till sändningen-
och har en stor liten veckor-
som skapar-
den betalning av den senten-
som kan använda-
för att producera senten-
i en annan lägg.
Det var en växelväxel.
Och det är en sändning av liten veckor.
Du förverkar liten-
till en stor växel-
som är inget som liten-
och det är vad kognitionen är.
Men då finns det en tredjelväxel-
som jag tror på nu-
som är att-
du tar de-
symbolerna-
och-
du förverkar symbolerna-
till förbundningarna-
och du brukar använda-
flera lärar av det.
Så du får de väldigt liten förbundningarna.
Men förbundningarna är nog-
tillbaka till symbolerna-
i att du har en stor växel-
för denna symbol och en stor växel-
för denna symbol.
Och de här växel-
som intervjuar-
till att producera växel-
för denna symbol-
för denna sändning.
Och det är vad det är att förstå-
att förstå-
att veta hur de-
symbolerna för de här växel-
och veta hur de-
elementerna för växel-
ska intervjuas-
för växel för denna symbol.
Det är vad det är att förstå-
att veta både i de-
stora liten-
liten-
och i våra växel.
Och det är ett exempel-
som är i och med.
Du är-
stänga med de-
symbolerna-
men du är-
intervjuad med de-
de här stora växel-
och det är det-
som alla arbetar.
Och alla kunder-
är i vad växel-
som du har-
och hur elementerna för-
de här växel-
intervjuar-
inte i symboler.
Men det är inte att du-
kommer att få-
från symbolerna-
samtidigt.
Det är att du-
turnerar-
symbolerna-
till stora växel-
men du städer-
med den-
surface-
strukturen av-
symbolerna.
Och det är hur de-
modeller arbetar.
Och det är-
och jag ser att det är-
en mer-
plåsig modeller av-
human.
Du var en av de-
första folk-
för att-
få den idéen av att använda-
GPUs.
Och jag vet att Jensen-
äter dig för det.
I 2009 har du-
sagt att du sa att Jensen-
att det här kan vara-
en ganska bra idé-
för att träna-
neural-nets.
Ta oss tillbaka till den-
urländen av att använda-
GPUs för att träna-
neural-nets.
Så faktiskt jag tänker-
att i 2006-
hade jag en form av-
gradvist student kallad-
Rick Zaliske.
Han är en väldigt bra-
computer-visning-kallad.
Och jag talade-
till honom i en möte.
Han sa att du sa att du-
oughta tänka på att använda-
grafisk-processering-karten-
för att de är väldigt bra-
på matrix multiplis.
Och vad du gör-
är att de är-
matrix multiplis.
Så jag tänkte på det-
för ett litet och sen-
vi lärde om de-
Tesla-systemerna som-
hade-
för GPUs.
Och-
initialt har vi bara-
gaming-GPUs.
Och vi har-
gjort att de-
gjorde saker 30 gånger-
snart.
Och vi har-
fått en av-
Tesla-systemerna för-
GPUs.
Och vi gjorde-
språk på det.
Och det jobbade väldigt-
bra.
Och sen i 2009-
jag gav en tal på Nipps.
Och jag sa att 1 000-
machine-learning-resekturer-
skulle gå och köpa-
NVIDIA GPUs.
Det är förutsättningen.
Du behöver dem för att köpa-
machine-learning.
Och jag-
sen sätter-
mail till NVIDIA och säger-
att jag sa att 1 000-
machine-learning-resekturer-
skulle köpa-
djurbordet.
Kan du ge mig en fri-
en?
Och de sa att de inte sa-
att de inte reprimerade.
Men när jag sa Jensen-
den här historien-
sen gav jag mig en fri-
en.
Det är väldigt bra.
Jag tror att det intressant-
är hur GPUs-
har förändrat-
bortfältet.
Så vad tror-
vi att vi ska köpa-
nästa i-
komputer?
Så min senaste par år-
på Google-
var jag-
tänk på-
för att försöka-
göra en analog-
komputering.
Så att inte använda-
en megawatt-
vi kan använda-
30-watts-
som bra-
och vi kan-
röda de-
stora-
länge-modeller-
i analog-
hardware.
Och jag-
aldrig har gjort det-
arbete-
och jag-
började-
verkligen-
apreciera digital-
komputering.
Så-
om du ska använda-
låg-
power-
analog-
komputering.
Varje enkel av-
hardware-
ska vara lite olika.
Och de idéer-
är att lära dig-
ska göra-
använda för specifika-
propiteter av-
det här hardware.
Och det är vad som-
händer med-
folk.
Alla våra bränser-
är olika.
Så vi kan inte-
ta vägen i dina bränser-
och ta dem i min bränser.
Det är olika-
och de specifika-
propiteter av-
individualna bränser-
är olika.
De har-
lär att använda-
allt det här.
Och så är vi-
mottlade-
i att vägen i min bränser-
är inte bra för andra bränser-
när jag dör-
vägen är använda.
Vi kan ge-
information-
från en till andra-
enligt-
ineffektivt-
att jag-
produker-
senten-
och du-
figure ut hur-
att förändra vägen-
så att du har-
sagt det samtidigt.
Det heter-
distillation.
Men det är en väldigt-
ineffektivt-
mål-
av att kommunikera-
knowledge.
Och med digitala-
system-
de är-
mottlade-
för att-
när du har-
lite vägen-
du kan-
ta ut-
komputer-
bara ta vägen-
på en tape-
någonsin.
Och nu-
bildar jag-
en annan-
komputer.
Ta-
de samla vägen in-
och om det är-
digitala-
det kan-
komputer-
exakt det samtidigt-
som det andra system-
gjorde.
Så digitala-
system-
kan-
köra vägen.
Och det är-
en-
kredit-
mycket-
mer-
effektivt.
Om du har-
en whole bunch-
av digitala-
system-
och de är-
de gör-
en liten bit av-
lägenhet-
och de börjar-
med de samla vägen-
de gör-
en liten bit av-
lägenhet och de-
sker-
de vägen igen.
De-
alla-
vet vad alla andra har-
lärt.
Vi kan inte göra det.
Och så är de-
långt-
superior till oss-
att kunna köra-
knowledge.
Enligt av-
de idéer-
som har varit-
plaudande i fjol-
är-
de väldigt-
åldskola idéer.
Det är idéer-
som har varit-
runt i nuralscience-
för-
förraver.
Vad tror du-
som har blivit-
för att-
för att applicera-
till system-
som vi utvecklar?
Så-
en stor sak-
som vi-
har att-
köra-
med nuralscience-
är-
de-
timerskallar för-
förändringar.
Så-
i nästan alla-
nuralsnät-
det finns en fast timerskall-
för att förändring-
aktiviteter.
Så input-
kommer i aktiviteter-
de-
embedding-vektorer-
kommer i aktiviteter.
Och så finns-
en lång timerskall-
som förändringar väx-
och det är-
långt-nuralsnätning.
Och så har-
de två timerskallar.
I växen-
finns många timerskallar-
som växer förändring.
Så för exempel-
om jag säger en-
unexpected-
ord som-
kökomba.
Och nu-
fem minuter sen-
du-
svarar-
och det finns-
en liten-
ny-
och det finns-
väldigt-
faint-
ord-
du blir-
mycket bättre-
att förändra-
än-
kökomba-
för att få-
fem minuter sen.
Så där är-
den-
knowledge-
i väx.
Och den-
knowledge-
är-
obvious-
i-
temperativ-
för-
synapser.
Det är inte-
hur-
de-
kökomba-
kökomba-
kökomba-
man-
inte har-
neons för den.
Det är-
temperativ-
förändringar väx.
Och du kan-
göra-
mycket-
med-
temperativ väx-
förändringar.
Fast-
vad jag kallar-
fast väx.
Vi-
inte gör-
det i-
de-
de-
och det som vi inte gör är att om vi har temporära förändringar till vägen som dependerar på inputdata
så kan vi inte processera en whole bunch of different cases at the same time.
I present, vi tar en whole bunch of different strings,
vi stackar dem together och processar dem all in parallel
för att vi kan göra matrix, matrix, multiplis, det är mycket mer effektivt.
Och just att effektivitet är att stoppa oss att använda fast vägen
men det bränklarar ut fast vägen för temporära förändringar.
Och det finns alla saker som du kan göra så att vi inte gör det present.
Jag tror att det är en av de största saker som vi ska göra.
Jag var väldigt hoppad på saker som grafkor,
om de var sequential och gjorde online lönning
så kan de använda fast vägen.
Men det har inte fungerat än.
Jag tror att det kommer att fungera eventually
när folk brukar förvägen för vägen.
Hur vet ni hur de här modellerna fungerar
och vet hur bränklarar har påverkats så att du tror?
Jag tror att det har varit en stor påverkning
som är på en ganska abströkt nivå
som är att för många år
folk var väldigt skamfiga på att ha en stor random neural nät
och bara ge er mycket tränare och det skulle lämna att göra det svårt.
Om du talar om statistiker eller linguister
eller oxer om AI,
så här är beardien.
Så attractiv får du sé här.
Gör jag det?
Hur kommer det att d kan practices?
Nu kommer vad som är spitla Ant besides banker
och d över du tränar v timber
och den minns oss att det i andra Everywhere du and anids.
Det har varit valet för de stora modellerna.
Det är en viktig sak att lägga om bränsle.
Det har inte tillräckligt alla det som har en älskarstruktur.
Det har varit mycket älskarstruktur, men det måste inte vara älskarstruktur-
för saker som man har lärt.
Idag kommer en idé från Chomsky att man inte vill lägga om nåt som är svårt.
Alltså att det är en väldig idé och bara mörker sig.
Det här idéer är nu tydligen inte.
Jag tror att Chomsky skulle öppna dig kring sina idéer.
Jag tror att många av Chomsky's politiska idéer är tydligare.
Jag är alltid trött på hur nån som har såna tydligare idéer-
om det märkliga kan vara så bra med linguistiken.
Vad tror du att de här modellerna simulerar-
konstnärerna av människor mer effektivt?
Men imagine att du hade det AIA-systemet-
som du har talat om hela dina liv.
Innan det är chattyp i tid-
som delits med märkliga konversationer-
när du startade främst hela tiden.
Det hade en självreflektion.
I några moment passade du och du talade om det till assistenten.
Du tror att det inte är mig som säger att det är assistenten.
Ja, det blir svårt för dig att tala om det till assistenten.
Du tror att det assistenten skulle kännas?
Ja, jag tror att de kan ha känslan också.
Så jag tror att vi har en in-a-thieta modell för perception.
Vi har en in-a-thieta modell för känslan.
Det är saker som jag kan upptäcka, men andra folk kan inte.
Jag tror att det är samma modell.
Jag tror att... Jag tror att jag kan känna som gärna gärna-
som jag ofta gör.
Vi ska försöka att abstrakta det från idéen av in-a-thieta.
Vad jag säger till dig är-
att om det inte är en inibition som kommer från fönstret-
så kommer jag att göra en aktie.
När vi talar om känslan är vi verkligen talar om-
aktier som vi kommer att göra om det inte är för-
konstrainer.
Det är verkligen vad känslan är.
Det är aktier som vi kommer att göra om det inte är för konstrainer.
Jag tror att du kan ge samma förväntning för känslan-
och det är ingen som kan ha känslan.
I 1973 sa jag att en robot hade en emotion.
I Edinburgh hade de en robot med två gripper som-
kan assembla en toy-car-
om man sätter dem separatvis på en grön fel.
Men om man tar dem i en pil-
så är visionen inte bra för att se vad som sker.
Så sätter de gripperna och skrattar dem-
och skrattar dem så att de skattar dem-
och då kommer de ihop dem.
Om du ser det i en person som säger-
att det var kross med situationen-
för att de inte förstod det så att de skrattade dem.
Det är bra.
Vi har tidigare talat om-
humans- och LLMss-
analogiska machines.
Vad tror du har varit de mest-
kraftiga analogiska analogiska-
som du har hittat i ditt liv?
I ditt liv.
Wow.
Jag tror att det är en...
en liten analogi som imponerar mig mycket.
Det är...
den analogiska mellan religiösa och-
between belief- och symbolprocessen.
När jag var jämfört med...
Jag kom från en äthisk familj-
och kom till skolan och konfronterade med religiösa.
Det var bara nånstans till mig.
Det var nog nånstans till mig.
När jag såg symbolprocessen-
som en ekonomi om hur folk jobbade-
jag trodde att det var samma.
Nånstans.
Jag tror inte att det är så mycket nånstans nu.
Jag tror att vi faktiskt gör symbolprocessen.
Vi gör det-
för att ge de stora föräldringar till symbolprocessen.
Men vi är faktiskt symbolprocessen.
Men inte nånstans-
i hur folk trodde att du matchade symbolprocessen-
och det ena som symbolprocessen har-
är att det är identisk till en annan symbol-
eller att det inte är identisk.
Det är det ena som symbolprocessen har.
Vi gör inte det.
Vi skriver kontext för att ge symbolprocessen till symbolprocessen-
och då skriver vi interaktioner-
mellan komponenterna till de symbolprocessen-
för att göra tänkning.
Men det är en bra forskare på Google-
som kallar Fernando Pereira-
som säger att vi har symbolprocessen-
och det ena som vi har är naturlig language.
Naturlig language är en symbolisk language-
och vi skriver det.
Jag tror att det nu.
Du har gjort en av de mest människa forskarna-
som i den historiska komponenterna.
Kan du gå tillbaka till hur du selectar-
de rätt problemet för att jobba?
Först lämme mig att korrekt dig.
Jag och min student har gjort många av de mest människa sakerna.
Det har varit en bra kollabberation med studenten-
och min möjlighet att selecta väldigt bra studenter.
Det kom från att det var väldigt många folk-
som gjorde neuralnets i 70s och 80s och 90s och 2000s.
Så de flesta folk som gjorde neuralnets-
fick att skapa de mest människa studenterna.
Det var en del av luck.
Men min mål för att selecta problemet är att...
När sannot talar om hur de jobbar-
har de teorier om hur de jobbar-
som inte har mycket att göra med trus.
Men min teorie är att jag tittar på nånting-
där alla agreear om nånting och det känns bra.
Det är bara en liten intressen om nåt som är bra.
Och jag jobbar på det och ser om jag kan lämna-
var det är som jag tror att det är bra.
Och kanske kan jag göra en liten demo med en liten computerprogram-
som visar att det inte fungerar så man kan vänta sig.
Så låt mig ta en exempel.
Många tror att om man har en ny del av en neuralnet-
det ska fungera bättre.
Om man tar ett tränning-examplev-
och gör en halv av de neuralnets sida-
det fungerar bättre.
Vi vet att det fungerar bättre om man gör det.
Och man kan demonstrera det i en liten exempel.
Det är bra med computer-simulationen.
Man kan se att den här idéen som man hade-
att att skräva det blir bättre-
och att man droppar på halv av de neuralnets sida-
eller gör det bättre.
Det är det som man vill i en kort tid.
Men om man tränar det med det så att det blir bättre-
kan man demonstrera det med en liten computerprogram-
och då kan man tänka på hur det är och hur det släpper-
att vara en smуда betygare.
Jag tror denna ärjiangubens mång i.
Det kan finnas sång som att det ku meter, och se-
att man kan ge en förbindel av vart det är wrong.
Förb Singh?
Vi只ar huvud och taklskammer.
Det är bara det här och inte hemom.
Och förb багan är kanske vi kommer att ha sex till många dies.
Det är en av de här sida.
Och om man 하�-
Om du hade en grupp av studenter som kom till dig och frågade om vad som är det viktigaste problemen i din fjol.
Vad vill du säga om att ta och arbeta på NICS?
Vi talade om resoneringar, timerskills, vad som är det högsta prioritet problem du skulle ge dem?
För mig är det samma fråga som jag hade för 30 år senare.
...och om du tror att den haranding fungerar mer än ifall du harMoon,
hur kanske den kan fungera mer än om du borde finnas...
…utan det är om du planserar en adore verksamör mot Skol jul eller tar lite olika teknik...
...där är det en av avspelningach.
Om jagவає om reneweringar får man yupvåren vi via.
När du tittar på huvudet vid en vid k loweren....
Du har varit rätt på så många saker, men vad är det du är bäst om?
Att du visar att du har hittat lätt tid att försälja ett stort stort stort stort.
Okej, det är två olika frågor.
En är vad du är bäst om, och andra är om du har hittat lätt tid på det.
Jag tror att jag är bäst om bolsternor, och jag är glada att jag har hittat lätt tid på det.
Det är en mycket mer bästa teori om hur man får gradier än bakpropagation.
Bakpropagationen är bara ordentligt och sensibel, och det är bara en chans.
Bolsternor är bra, och det är ett väldigt intressant sätt att få gradier.
Och jag skulle vilja för det att vara hur bra det är, men jag tror att det inte är så.
Har du spännit mycket tid att tänka på vad som skulle hända på de system som utvecklades så bra?
Har du aldrig haft en idé om att se om vi kan göra de här systemet jobba så bra?
Om vi kan demokratisera utbildningen, om vi kan göra nåt mer bra,
om vi kan lösa några tuffa problem i medicin,
eller var det mer till dig om att hända på brännen?
Ja, jag tror att siffrorna ska göra saker som ska hjälpa till samhället.
Men faktiskt är det inte hur man gör bästa forskning.
Man gör bästa forskning när det är skriven med kurser.
Man måste bara förstå något.
Många gånger har jag förväntat att de här sakerna kan göra mycket harm och mycket bra.
Och jag har blivit mycket mer beroende om de effekter som de ska ha på samhället.
Men det är inte det som är motiveringen.
Jag ville bara förstå hur bästa kan brännen lära sig att göra saker?
Det är vad jag vill att kolla. Och jag har följt.
Som effekt på sidan av det här följningen har vi lite bra ingenering.
Ja, det var en bra följning för världen.
Om du tar lanser av saker som kan gå riktigt bra,
vad tror du är de mest övriga applikationer?
Jag tror att hälften är säkert en stor del.
Med hälften är det nästan ingen enda till hur mycket hälsoskriven kan besöka.
Om du tar nån älskare kan de använda fem drar fullt.
Så när AI blir bättre än de som gör saker,
kan du bli bättre i gränser där du kan göra mycket mer av det här.
Och vi kan göra mycket mer drar.
Om alla har tre drar av de här, så är det bra.
Och vi kommer att få till det.
Så det är en av de som har bra hälsoskriven.
Det finns också bara en ny ingenering,
utveckling av nya material, för exempel,
för bättre solarpanelser,
för superkondaktivitet,
eller för att förstå hur bodyen fungerar.
Det kommer att ha stora intäkter där.
De kommer att vara bra saker.
Vad jag är bäst med är att de är bra aktörer för att använda de för bra saker.
Vi faciliterade folk som Putin eller Trump
att använda AI för att killa roboter,
eller för att manipulera public opinion,
eller för mass surveillance.
Och de är alla väldigt bra saker.
Har du aldrig varit bäst med att slå ner fjol
kan du också slå ner positiva?
Absolut.
Och jag tror att det inte finns mycket chans
att fjol kommer att slå ner.
Partigt för att det är internationellt.
Och om ett land släjer ner,
andra landar kommer inte att slå ner.
Så det finns en råd tydligt mellan Kina och USA.
Och det kommer att slå ner.
Det var en partik som jag sa att man skulle slå ner för 6 månader.
Jag hade inte sannat det,
bara för att jag trodde att det inte skulle hända.
Jag kanske skulle ha sannat det,
för även om det inte skulle hända,
det gjorde en politisk punkt.
Det är ofta bra att fråga för saker
som du vet att du inte kan få för att göra en punkt.
Men jag tror inte att vi kommer att slå ner.
Hur tror du att det kommer att impacta
ett AI research process
med den här assisten?
Jag tror att det blir mycket mer effektivt.
AI research blir mycket mer effektivt
när du har de här assisten
som hjälper dig med program
men också som hjälper dig att tänka på saker
och som hjälper dig mycket med equations.
Har du reflekterat mycket
på processen av selectiv talent?
Har det varit mest intuid till dig?
När Ilya ser upp i dörren
och du tror att det är en smart gädda,
så gör vi det.
Så för selectiv talent
är det att du bara vet.
Så när jag talade till Ilya
för inte så länge,
var han väldigt smart
och då talade han lite mer.
Han var väldigt smart
och hade väldigt bra intuitionser
och var bra i mat.
Så det var inte bra.
Det är en annan case
där jag var i en Nipps konferens.
Vi hade en poster
och någon kom upp
och han började fråga
om poster.
Och every question he asked
was a deep insight
into what we'd done wrong.
And after five minutes
I offered him a post-op position.
That guy was David Mackay
who was just brilliant
and it's very sad he died,
but he was very obvious
you'd want him.
Other times it's not so obvious
and one thing I did learn
was that people are different.
There's not just one type of good student.
So there's some students
who aren't that creative
but are technically extremely strong
and will make anything work.
There's other students
who aren't technically strong
but are very creative.
Of course you want the ones
who are both
but you don't always get that.
But I think actually in the lab
you need a variety
of different kinds of graduate student.
But I still go with my gut intuition
that sometimes you talk to somebody
and they just very very
they just get it
and those are the ones you want.
What do you think is the reason
for some folks having better intuition?
Do they just have better training data
than others?
Or how can you develop your intuition?
I think it's partly
they don't stand for nonsense.
So here's a way to get bad intuitions.
Believe everything you're told.
That's fatal.
You have to be able to
I think here's what some people do.
They have a whole framework
for understanding reality
and when someone tells them something
they try and sort of figure out
how that fits into their framework.
And if it doesn't
they just reject it.
And that's a very good strategy.
People who try and incorporate
whatever they're told
end up with a framework
that's sort of very fuzzy
and sort of can believe everything.
And that's useless.
So I think actually
having a strong view of the world
and trying to manipulate
incoming facts to fit in with your view.
Obviously it can lead you into
deep religious belief
in fatal flaws and so on
like my belief in Baltimore machines.
But I think that's the way to go.
If you've got good intuitions
you should trust them.
If you've got bad intuitions
it doesn't matter what you do
so you might as well trust them.
A very good point.
When you look at
the types of research
that's being done today
do you think we're putting all of our eggs
in one basket
and we should diversify our ideas
a bit more in the field?
Or do you think this is the most promising direction?
So let's go all in on it.
I think having big models
and training them on multimodal data
even if it's only to predict the next word
is such a promising approach
that we should go pretty much all in on it.
Obviously there's lots and lots of people doing it now.
And there's lots of people doing apparently crazy things
and that's good.
But I think it's fine for like
most of the people to be following this path
because it's working very well.
Do you think that the
learning algorithms matter that much
or is it just a scale?
Are there basically millions of ways
that we could get to
human level in intelligence
or are there sort of a select few
that we need to discover?
Yes, so this issue of whether
particular learning algorithms are very important
or whether there's a great variety
of learning algorithms that will do the job.
I don't know the answer.
It seems to me though that
under propagation there's a sense in which
is the correct thing to do.
Getting the gradient so that you change a parameter
to make it work better
that seems like the right thing to do
and it's been amazingly successful.
There may well be other learning algorithms
that are alternative ways of getting
that same gradient
or that are getting the gradient of something else
and that also work.
I think that's all open
and a very interesting issue now
about whether there's other things
that you can try and maximize
that will give you good systems
and maybe the brain is doing that
because it's easier.
Backprop is in a sense the right thing to do
and we know that doing it
works really well.
And one last question
when you look back at your
decades of research, what are you most proud of?
Is it the students? Is it the research?
What makes you most proud of
when you look back at your life's work?
The learning algorithm for Boltzmann machines.
So the learning algorithm for Boltzmann machines
is beautifully elegant.
It's maybe
hopeless in practice
but it's the thing I enjoy
most developing that with Terry
and it's what I'm proudest of
even if it's wrong.
What questions do you spend
most of your time thinking about now?
Is it the...
What should I watch on Netflix?

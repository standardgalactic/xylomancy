So, what I'm going to do in this course is discuss mostly ideas that are already in the
book called The Emotion Machine, I'm sorry I used that title, and the older book called
The Society of Mind, which are, the books are not quite the same, they overlap a bit
in material, but they're sort of complementary, I like the old one better because the chapters
are all one page long, and they're moderately independent, so if you don't like one you
can skip it. The new book is much denser and has a smaller number of long chapters, and
I think it's, over the years I got lots of reactions from young people in high school,
for example, almost all of whom like The Society of Mind and found it easy to read and seemed
to understand it, there are lots of criticisms by older people who maybe some of them found
it harder to put so many fragments together, who knows, but most of this class, most of
the things I'd like to say are in those books, so it's really like a big seminar and I'll,
my hope is that everyone who comes to this class would have a couple of questions that
they'd like to discuss, and if I can't answer them maybe some others if you can, so I'd
like to think of this as a super seminar, and normally I don't prepare lectures, and
I just start off asking if there are any questions, and if they're not I get really pissed off
because, but anyway I'm going to start with a series of slides. So why do we need machines,
and partly there are a lot of problems, unlike most species or kinds of animals, humans have
only been around a few million years, and they're very clever compared to other animals,
but it's not clear how long they will last, and when we go we might take all the others
with us, so there are a whole set of serious problems that are arising because there are
so many humans, and here's just a little list of things. There's a better list in a book
by the astronomer royal Martin Lease of England, anybody know the title? Yes, our final hour,
it's a slightly scary title, and when I was a teenager, World War II came to an end with the
dropping of two atomic bombs on Japan, and I didn't believe the first one was real because it was
in Hiroshima, so I assumed that the U.S. had somehow made a big underground underwater
tanker with 20,000 tons of TNT, and some few grams of radium or something, and blown it up in the
harbor, and first it flew an airplane over dropping some little thing, and this was to fool the
Japanese into thinking that we have an atomic bomb, but when they did it again over Nagasaki that
wasn't feasible, so, and when I was in grade school, sometimes if I said something very bright,
I would hear a teacher saying, maybe he's another J. Robert Oppenheimer, because that was the name of a
scientist who had been head of the Manhattan Project, and he was, I think, three or four years earlier
in grade school than I was, and I thought it was very strange for a person to have a first name as
just being a letter rather than a name, and many years later, when I was at Princeton in graduate
school, I met the Robert Oppenheimer, and that was a great pleasure, and in fact he took me to
lunch with a couple of other people I admired, namely Gertl and Einstein, which was very exciting,
except I couldn't understand Einstein because I wasn't used to people with a strong German accent.
But I understood Gertl just fine, and after that lunch was over, I went and spent about a year
learning about Turing machines and trying to prove theorems about them and so forth. So anyway,
in the course of these talks, we'll run across a few of these people, and here's a big list of the
people that I'm mostly indebted to for the ideas in the Society of Mind and the Emotion Machine.
The ones in blue are people I've actually met. It would be nice to have met Aristotle,
because no one really knows much about him, but you really should read. Just skim through some of
that, and you'll find that this is a really smart guy. We don't know if he wrote this stuff,
or if it were compiled by his students, like a lot of Feynman's writing is, and von Neumann's
writing is edited from notes by their students. Anyway, the astonishing thing about Aristotle
is that he seems to be slightly more imaginative than most cognitive scientists you'll run into
in the present day. It would have been nice to know Spinoza and Kant and the others also.
Freud wrote 30 or 40 books. So did he fall off this list? There he is. I just made this list the other day,
and I was looking up these people to find their birthdays and stuff. Yes?
Because they're religious, as far as I can see. Well, who would you, would you say, Buddha?
Name one. Maybe I never heard of them. Confucius.
Well, I only know of them through aphorisms, single proverbs, but I don't know that Confucius had a theory of thinking.
Well, I've looked at Buddhist theories, and they're, I don't think they would get a C plus.
And one problem is that there are cultures, there's something about Greek culture, because it had science.
It had experiments. Somebody has a theory, and they say, and like epimenides, Lucretius,
somewhere in the society of mind, I think I quoted Lucretius about translucent objects,
and he says they're, they have the particular appearance because the rays of light bounce many times
before they get to the surface, so you can't tell where they started. And I don't find in
Eastern philosophy theories that say, here's what I think, and here's a reason why. I've looked at Buddhist stuff,
and it's strange lists of psychological principles, every one of which is, looks pretty wrong,
and they make nice two-dimensional diagrams, but no evidence for any of them, so I don't know whether to take it seriously.
I think a lot of interest, I mean, knowledge is from observation, in your way, that in some of them probably didn't really test it,
because a lot of the ideology cannot be tested. On the other hand, they're all a scientist, meaning like...
Well, what can't be tested?
I mean, some of the ideologies, probably.
Then why, if they can't be tested, why should one look at it twice?
Okay, I think this is a serious argument. It seems to me that science began a little bit in China, a little bit in India,
in the Arabic world, they got up to the middle of high school algebra.
But then, what?
Well, but this wasn't as good as Archimedes, who got to the beginning of calculus.
So if you look at most cultures, they never got to the critical point of getting theories, doing experiments, discussing them, and then throwing them out.
And so if you look at Buddhist philosophy, it's 2,500 years old.
If you look at Greek physics, yes, Archimedes almost got calculus, and he got lots of nice principles.
And Buddha mentions, at some point, if you want to weigh an elephant,
put him in a boat, and then take the elephant out and put rocks in until the boat sinks to the same level.
So there you see a good idea.
But if you look at the history of the culture, if people still say this 1,000-year-old stuff is good, then you should say,
no, it's not.
No, the question is, why did it stop?
Why did it stop?
Ancient wisdom is generally not very good, and we shouldn't respect it.
And the question is, why did it stop?
Why did it stop?
Why did it stop?
Why did it stop?
Why did it stop?
Ancient wisdom is generally not very good, and we shouldn't respect it for too long.
And that's...
No, everybody...
No, we got rid of alchemy, we got rid of...
What do you call it?
What's caloric?
You jump off their shoulder.
You don't stay on them.
So it's good to know history, but if the history doesn't get anywhere,
then you don't want to admire it too much, because you have to ask, why did it stop?
What went wrong?
What went wrong?
Because barbarians came in and...
Well, you know what happened to Archimedes.
Some Roman killed him.
But...
Anyway...
No, it's a good question.
Why didn't science happen a million years ago?
Because humans are five million years old.
What took it so long, and...
No, it's more...
Sure.
Do you have a theory of why science didn't develop for so long?
In most cultures, it might be religion, which is a sort of science that doesn't use evidence,
and in fact kills people who try to get it.
And so there are systematic reasons why most cultures failed, and maybe somebody has written it.
Is there a book on why science disappeared except once?
It's rather remarkable, isn't it?
After all, the idea, if somebody says something and somebody else says,
okay, let's do an experiment to see if that's right.
You don't have to be very bright.
So how come it didn't happen all the time everywhere?
Here he is.
I don't know the answer to that, but I know Paul Davies has sort of an anecdote about that,
where he's speculating, even in Europe, where it didn't happen, was the flu.
And I think he gives you an example of, suppose an asteroid or a common crash in Paris in...
Well, history is full of flukes.
I don't remember who wrote that nice book about the plague.
Some woman.
And she mentions that this was spread by rats and fleas or something.
And 30 or 40% of the population of many countries in Europe died.
And the next generation had a lot of furniture.
The standard of living went way up.
So anyway, here's a list of disasters and...
Let's see here.
The standard of living went way up.
So anyway, here's a list of disasters and...
Oh, come on.
And Martin Rees is the royal astronomer and has that book about the last hour or whatever.
And I'm making another longer list, but...
He has lots of obvious disasters, like...
Some high school student looks up the genetic sequence for a smallpox virus has been published.
And now you can write a list of nucleotides and send it somewhere.
They'll make it for about 50 cents or a dollar per nucleotide.
So for a couple of hundred dollars, you can make a virus or a few hundred.
So one possibility is that some high school student makes some smallpox, only gets it wrong and it kills everyone.
So there are lots of disasters like that.
And no one knows what to do about that because the DNA synthesis machinery is becoming less and less expensive
and probably the average rich private high school could afford one.
So there are lots of other things that could happen.
But one particular one is this graph which I just made up.
An interesting fact is that since 1950, when the first antibiotics started to appear...
As I mentioned, I was a kid in the 1940s and penicillin had just hit the stands and there wasn't much of it.
And there was a researcher who lived a few blocks from us whose dog had cancer.
And so its father, I don't know what you call the owner of a dog,
sneaked some penicillin out of the lab and gave it to the dog who died anyway.
But he said, well, nobody's tried penicillin on cancer yet, maybe it will work.
And a lot of people were mad at him because he probably cost some human its life.
But he said he might have saved a billion humans their lives.
So ethics, ethicists are people who give reasons not to do things.
And I'm not saying they're wrong, but it's a funny job.
So anyway, since that sort of thing happened and medicine began to advance,
people have been living one year longer every 12.
So it's 60 years since 1950.
So that's five of those six, so they're living six or seven years longer now than they were when I was born.
And somebody mentioned that that curve stopped the last few years for other reasons.
But anyway, if you extrapolated that, you find that the lifespan is going to keep increasing.
How much we don't know.
Another problem is that you might discover enough about genetics to get rid of most of the serious diseases.
Maybe just 20 or 30 genes are responsible for most deaths right now.
And if you could fix those, which we can't do yet, there's no way to change a gene in a person
because invading all the cells is a pretty massive intervention.
But we'll get around that.
And then it might be that people will suddenly start living 200 or 300 years.
At some point, the population has to slow down.
And so you can only reach equilibrium with one child per family and probably less than that.
So all the work has to be done by two or 300 year olds.
And let's hope they're good and healthy.
So anyway, I think it's very important that we get smart robots because we're going to have to stem the population.
And I hope people will live longer and blah, blah, blah.
And so these robots have to be smart enough to replace most people.
And how do you make something smart?
Well, artificial intelligence is the field whose goal has been to make machines that do things that we regard as smart or intelligent or whatever you want to call it.
And the idea of seriously making machines smart has roots that go back to a few pioneers like Leibniz who wrote about automata and that sort of thing.
But the idea of a general purpose computer didn't appear until the 1930s and 40s in some sense.
The first form of the general purpose computer appears in really in the 1920s and 30s with the work of a mathematician who aimed a post at NYU who I happen to never meet.
But we had some friends in common and he had the idea of production rules and basically rule based systems and proved various theorems about them.
Then Kurt Gertl showed that if you had something like a computer or a procedure that had the right kinds of rules, it could compute all sorts of things.
But there were some things it couldn't compute, unsolvable problems, and that became an exciting branch of mathematics.
And the star thinker in that field was Alan Turing who invented a very simple kind of universal general purpose computer.
Instead of a random access memory, it just had a tape which it could write on and read and change symbols.
And we go back and forth and if it's in state X at C symbol Y, it will print symbol Z over the X and move to the left or right.
And just a bunch of rules like that were enough to make a universal computer.
And so from about 1936, it was sort of clear to a large mathematical community that these were great things.
And a couple of general purpose like computers, very simple ones, were built in the 1930s and more in the 1940s.
And in the 1950s, big companies started to make big computers which were rooms full of equipment.
But as you know, most programs could only do some particular thing and none of them were very smart.
Whereas a human can handle lots of kinds of situations and if you have one that you've never seen before, there's a good chance you'll think of a new way to deal with that and so forth.
And so how do you make a machine that doesn't get stuck almost all the time?
And I like to use the word resourcefulness, although I left an R out of that one.
Is there a shorter word?
So here's a good example, my favorite example of a situation where a person is born or less with a dozen different ways of dealing with something.
And the problem that I imagined that you're dealing with is this.
My favorite example is I'm thirsty, so I see that glass of water and I do that and get it.
Actually, I am.
On the other hand, if I were here, I would never in a whole lifetime do this.
You never walk out a window by mistake.
We're incredibly reliable.
So how do I know how far it is?
And that slide shows you 12 different ways that your vision system, that's only your vision system, has to measure distances.
So gradients, if things are sort of blurry, then they must be pretty far away.
That's sort of on a foggy day outside.
Here's a situation.
If you assume those are both chairs of the same size, then you know that this chair is about twice as far away as that, although you don't, well.
And you know how far away they are pretty much by the absolute size.
If you have two eyes that work well, then if something is less than 30 feet away, you can make a pretty good estimate of its distance by focusing both eyes on some feature.
And your brain can tell how far apart your eyes are looking.
So that's, there's 12 different things.
It's more than you need.
Lots of people are missing half of those.
Lots of people have very poor vision in one eye.
Some people cannot fuse stereo images, even though both eyes seem to have 20-20 vision.
And in some cases, nobody knows why they can't do that.
I think I once took a test for being a pilot, and they wanted to be sure you could do stereo vision, which seemed very strange.
Because if you're an airplane and you're less than 30 or 40 feet away from something, it's too...
Then yes, you can't use stereo, you could use stereo, but it's too late.
So anyway, that's interesting.
See if you can think of an example where a person has even more 12 of these.
But it's pretty amazing, isn't it?
That's more redundancy.
This is too hard to read.
But somehow I found in an Aristotle essay the idea that you should represent things in multiple ways.
You might describe a house.
One person might describe a house as a shelter against destruction by wind, rain and heat.
Another might describe it as a construction of stones, bricks and timbers.
But a third possible description would say it was in that form, in that material, with that purpose.
So you see there's two different descriptions.
One is the functional description.
It's a shelter.
The second one is a structural description, how it's made.
And Aristotle says, which is the better description, and he dismisses the material one or the functional one,
is not rather the person who combines both in a single statement.
And then I found a paragraph by Feynman who says,
every theoretical physicist who is any good knows six or seven different ways to represent exactly the same physics.
And you know that they're all equivalent.
But you keep them all in your head, hoping that they will give you different ideas for guessing.
I should put more dots.
Anyway, that whole argument is to say that the interesting thing about people
is that they have so many ways to do things and perceive things and think of things.
And in some cases, we even know that there are different parts of the brain that are involved in one aspect
or another of constructing those different representations or descriptions.
If you look at the...
One of my favorite books weighs about 20 pounds.
It's a book on the nervous system by Candle and Schwartz.
And the index to that book is quite a lot of pages long.
And it mentions 400 different structures in the brain.
So the brain is not like the...
I shouldn't make fun of the liver because for all I know, the liver has 400 different mini processes for doing things.
But the brain has distinguishable areas that seem to perform several hundred different functions.
And with a microscope, at first they all look pretty much the same.
But if you look closely, you see slightly different patterns of how the most layers of the cortex of the brain,
most parts of it have six layers and each has a population of different kinds of cells.
There are a lot of cross connections up and down and sideways to other...
There are range in columns of between 400 and 1,000 cells and you have a couple of million of those.
And there are lots of differences between the columns in different areas.
And we know some of the functions.
Most cases we don't know much about how any of them actually work.
With the main exception of vision, where the functions of the cells in the visual cortex are fairly well understood at low levels.
So we know how that part of the brain finds the edges and boundaries of different areas and textures and regions of the visual field.
But we do not know even a little bit about how the brain recognizes something as a chair and an overhead projector and CRT screen and that sort of thing.
So the kind of question that I got interested in was how can you have a system which has a very large number of different kinds of computers,
each of which by itself might be relatively simple or might not, I suppose.
And how could you put them together into a larger system which could do things like learn language and prove theorems and convince people to do things that they would never have dreamed of doing five minutes earlier and stuff like that.
Now the first sort of things I was interested in was in fact how to make, how to simulate simple kinds of nerve cells.
Because in the 1950s there was about almost 100 years, really more like 50 years of science discovering things about neurons and nerve cells, the axons and dendrites that they used to communicate with other neurons.
So if you go back to 1890 you find a few anatomists discovering some of the connections of neurons in the brain and you find a few experimental physicists.
There was no oscilloscope yet, but there were very high gain galvanometers which could detect pulses going along a nerve fiber.
And by 1900 it was pretty clear that part of the activity in a nerve cell was chemical and part was electrical and by 1920 or 30 with the cathode ray tube appearing mostly because of television.
But it became possible to do a lot of neurophysiology by sticking needles in brains. The vacuum tube appears around 1900 and you can make amplifiers that can see millivolts and then microvolts.
So in the beginning of the 20th century there was lots of progress.
By 1950 we knew a lot about the nervous system, but we still don't know much about how you learn something in the brain.
It's quite clear that the things called synapses are involved, connections between two neurons become better at conducting nerve impulses under some conditions.
And no one knows how higher level knowledge is represented in the brain yet.
And the Society of Mind Book had a lot of theories about that and in particular there was a theory called K-Lines, knowledge lines or something that came partly from me
and partly from a couple of other researchers named David Walts and Jordan Pollack.
And that's a sort of nice theory of how neural networks might remember higher level concepts.
And for some reason, although that kind of work is from around 1980 which is 30 years ago, it has not hit the neuroscience community.
So if you look at the Emotion Machine Book or the Society of Mind, in Amazon you might run across a review by a neurologist named Richard Restak
who says that Minsky makes up a lot of concepts like K-Lines and micro-neems and stuff like that that nobody's ever heard of.
And there's no evidence for them and he ignores the possibility that it isn't the nerve cells in the brain that are important,
but the supporting tissues called glia which hold the neurons up and feed them.
And he goes on for a couple of insane paragraphs.
It's very interesting because it doesn't occur to him that you can't look for something until you have the idea of it.
And so here's this 30-year-old idea of K-Lines and go and ask your favorite neuroscientist what it is and he said,
oh, I think that's some AI thing, but where's the evidence for it?
What do you suppose is my reaction to that?
Who's supposed to get the evidence?
So it seems to me that there's a strange field in neuroscience which is that it doesn't want new ideas unless you've proved them.
So I try to have conversations with them but get somewhat tired of it.
Anyway, but in this course I'm taking the opposite approach which is that we don't want a theory of thinking.
We want a lot of them because probably psychology is not like physics.
What's the most wonderful thing about physics?
The most wonderful thing is that they have unified theories.
There wasn't much of a unified theory until Newton and he got these three laws, wonderful laws.
There was the gravitational idea that things, bodies attract each other with a force that's the inverse square of the distance between them.
Another is that kinetic energy is conserved.
Forget what the third one is.
Equal reaction is equal and opposite.
If two things collide they transfer equal amount of momentum to both.
There was a little problem up to Newton's time.
Galileo got some of those ideas and my impression from reading him is that he has a dim idea that there are two things around.
There's kinetic energy which is mv and there's momentum is mv and there's kinetic energy which is mv squared.
He doesn't have the clear idea that there are two different things here and you can't blame him.
You wouldn't think that two quantities would combine in two different ways to make two important different concepts.
Well that got clear to Newton somehow and Galileo is a bit muddled although he gets almost all the consequences of those things right.
But he doesn't get the orbits and things to come out.
Anyway what happened in artificial intelligence like most fields is that people said well let's try to understand thinking and psychology and let's use physics as our model.
And so what we want is to get a very small number of universal laws.
And a lot of psychologists struggled around to do that and then they gradually separated so that there were some psychologists like Bill Estes who worked out some very nice mathematical rules for reinforcement based learning.
Got a simple rule if you designed an experiment right it predicted pretty well how many trials it would take a rat or a pigeon or a dog or whatever to learn a certain thing from trial and error.
And Estes got a set of four or five rules which looked like Newton's laws.
And if you designed your experiment very carefully and shielded the animal from noise and everything else which is what a physicist would do for a physics experiment.
The reinforcement theories got some pretty good models of how to make a machine learn.
But they weren't good enough.
So here's a whole list of things that happened in the early years of cognitive psychology when people were trying to make theories of thinking and they were imitating the physicists.
By physics envy to borrow a term of Freud there.
The idea is can you find a few simple rules that will apply to very broad classes of psychological phenomenon.
And this led to various kinds of projects.
Lots of neural network and reinforcement and statistical based methods led to learning machines that were pretty good at learning in some kinds of situations.
And they're becoming very popular but I don't like them because if you have a lot of variables like 50 or 100 then to use a probabilistic analysis you have to think of all combinations of those variables.
Because if two of them are combined in something like a exclusive or manner you know I just put the light pen in a pocket.
It's either in a left pocket or a right pocket.
Can't be both.
That's an XOR.
That will cause a lot of trouble to a learning machine.
And if there are 100 variables there's no way you could decide which of the two to the 100th Boolean combinations of those variables you should think about.
And so lots of statistical learning systems are good for lots of applications but they just won't cut it to solve hard problems where the hypothesis is a little bit complicated and has seven or eight variables with complicated interactions.
So most statistical learning people assume that if you get a lot of partial ones then you can look for combinations of ones that have high correlations with the result.
Then you can start combining them and things get better and better.
However if the, mathematically, if in effect you're looking for it depends on the exclusive or of several variables there's no way to approach that by successive approximations.
If any one of the variables is missing there won't be any correlation of the phenomenon with the others.
Anyway that's a long story.
But I think it's worth complaining about because almost all young people who start working on artificial intelligence look around and say what's popular?
Statistical learning.
So I'll do that.
That's exactly the way to kill yourself scientifically.
You don't want to get the most popular thing.
You want to see what am I really good at?
That's different.
And what are the chances that that would provide another thing?
So you, well end of long speech.
Another problem in the last 30 years, and I'm sort of, as you'll see during my lectures,
I think a lot of wonderful things happened between 1950 when the idea of AI first got articulated in the 1950s.
And then the 20 years after that from 1960 to 1980,
a lot of early experiments, and I'll show you some of them, looked very promising.
Fact.
Maybe here we go.
1961.
Jim Slagle was a
young graduate student here at MIT.
He was blind.
He had gotten some retinal degeneration thing in his first or second year of high school.
He was told that he would lose all his vision and there was no treatment or hope.
So he learned Braille while he could still see.
And when he got to MIT, he was completely blind.
But there was a nice big parking lot in Technology Square.
And he would ride a bicycle and people like Sussman and Winston and whoever was around would yell at him,
telling him where the next obstacle would be and Jim got better and better at that and nothing would stop him.
And he decided he would write a program that, oh, I wrote a program that would take any formula and find its derivative.
It was really easy because there are just about five rules.
Like if there's a product UV, then you compute U times the derivative of V and plus V times, you know, U, T, V plus V, D, U.
So I wrote a 20 line list program that did all the algebraic expressions.
And what it would do is put D's in and then in the right place and then it would go back through the expression again.
Wherever it saw a D, it would do the derivative of the thing after that and nothing to it.
So Slagle said, well, I'll do integrals.
And we all said, well, that's very hard. Nobody knows how to do it.
And in fact, in Providence at the home of the American Mathematical Society, there is a big library called the Bateman Manuscript Project,
which has been collecting all known integrals for 100 years.
And when anybody finds a new integral that they can integrate in closed form,
they send the formulas to the Bateman Manuscript Project and some hackers there have developed ways to index it.
So if you had an integral and you didn't know how to integrate it, you could look it up.
And that was pretty big.
I should say that Slagle succeeded in writing a program that managed to do all of the kinds of integrals that one usually found on the first year calculus course at MIT
and got an A in those, couldn't do word problems.
And the uncanny thing is that if it was a problem that usually took an MIT student five or 10 minutes,
Slagle's program would take five or 10 minutes.
It's running on a IBM 701 with 20 millisecond cycle time.
It's incredibly slow.
You can type almost that fast.
And 16k of words of memory.
So there's no significance whatever to this accident of time.
It would now take a microsecond or so, be a thousand million times faster than a student.
Quite remarkable.
I don't have a slide.
Joel Moses, then Slagle went and graduated.
Joel Moses was another student who was, is he a provost now or what?
What?
He got tired of it.
A terrific student.
And he set up a project called Maxima for Project Max Symbolic, something or algebra.
And got people, several people all over the country working on integration.
And at some point, a couple of them, Bobby Kavanis and forget the other one,
found a procedure that could in fact integrate everything, every algebraic expression that has a,
can be integrated in closed form.
I forget the couple of constraints on it.
And that became a widely used system.
It ultimately got replaced by Stephen Wolfram's Mathematica.
But Maxima was sort of the world class symbolic mathematician for quite a few years.
And Moses mentioned to me, he had read Slagle's program thesis.
And it took him a couple of weeks to understand the two pages of, or three pages of Lisp,
that Slagle had written because being blind,
Slagle had tried to get the thing into as compact a form as possible.
But that's symbolic.
It's too easy.
There's a more ambitious one, which is three years later, Dan Robbro,
who is now a vice president doing something at Xerox.
And it solved problems like this.
The gas consumption of my car is 15 miles per gallon.
The distance between Boston and New York is 250 miles.
What is the number of gallons used on a trip between Boston and New York?
And it chomps away and solves that.
It has about 100 rules.
It doesn't really know what any of those words mean,
but it thinks that the word is is equals.
The distance between doesn't care what Boston and New York is.
It has a format thing, which says the distance between two things.
And it never bothers to, you see, because the phrase Boston and New York occurs twice in the example,
it just replaces that by some symbol.
It was fairly remarkable.
And generally, if you had an algebra problem and you told it to Bobrow,
Bobrow could type something in and it would solve it.
If you typed it in, it probably wouldn't, but it was, you know,
it had more than half a chance, or less about half a chance.
So it was pretty good.
And if you look at an out-of-print book I wrote called,
I compiled called Semantic Information Processing.
Most of Bobrow's program is in that.
So that's 1964.
I'll skip Winograd, which is perhaps the most interesting program.
This was a program where you could talk to a robot that,
I don't have a good picture on the slide, but there are a bunch of blocks of different colors.
They're all cubes in the rectangular blocks.
And you can say which is the largest block on top of the big blue block.
It could answer you.
And you could say put the large red block on top of the small green block.
And it would do that.
And Winograd's program was, of course, a symbolic one.
We actually built a robot and I guess we built it second.
Because it's Stanford built a robot and they imported Winograd's program.
And they had the robot actually performing these operations that you told it to do by typing.
And it was pretty exciting.
My favorite program in that period was this one because it's so psychological.
This is called a geometrical analogy test.
And it's on some IQ tests.
A is to be as C is to which of the following five.
And Evans wrote a set of rules which were pretty good at this.
It is well as 16 year olds and it picks this one.
And if you ask it why, it says something like.
I don't have a reason.
It moves the largest object down or something like that.
Makes up different reasons.
So you see, in some sense we're going backwards in age because we're going from calculus to algebra to simple analogies.
Oh, there it is.
That's one where the largest object moves down.
I don't know why I have two of them.
These are for another lecture.
Okay.
So that was a period in which we picked problems that people considered hard because they were mathematical.
But when you think about it more, you see, well, those math things are just procedures.
And once you know what Laplace and Gauss and those mathematicians Newton and people did, you can write down systematic procedures for integrating or for solving simultaneous algebraic constraint equations or things like that.
And so there's very little to it.
So in some sense, if you look at the what you're doing in math in high school in education, you're going from hard to easy.
It's just that people aren't, most people aren't very good at obeying really simple rules because it's so hideously boring or something.
We gradually started to ask, well, why can't we make machines understand everyday things and the things that everyone regards as common sense and people can do so you don't need machines to do them.
One of my favorite examples is why can you pull something with a string but not push?
There's been a lot of publicity recently about that interesting program written in a group at IBM called Watson, which is good at finding facts about sports people and celebrities and politics and so forth.
But there's no way it could understand why you could push, pull something with a string but not push.
And I don't know of any program that has that concept or way of dealing with it.
So that's what I got interested in.
And starting around the, maybe the middle 1970s or late 1970s, several of us started to stop doing the easy stuff and trying to make theories of how you would do the kinds of things that people are uniquely good at.
I don't know if animals, well, I don't know.
I'm sure a monkey wouldn't try to push anything with a string.
Maybe it does it very quickly and you don't notice.
And one aspect of common sense thinking is going right back to that idea of vision having a dozen different systems is that whatever a person normally is doing, they are probably representing it in several different ways.
And here's an actual scene of two kids named Julie and Henry who are playing with blocks.
It's pretty hard to see those blocks.
And you can think that Julie is thinking seven thoughts.
I'd like to see a longer list, maybe a good essay would be to take a few examples and say, what are the most common micro worlds?
See physical, social, emotional, mental, instrumental, whatever that is, visual, tactile, spatial.
She's thinking all these things.
What if I pulled out that bottom block?
You can't see the tower very well.
Should I help him or knock his tower down?
How would he react?
I forgot where I left the arch shaped block.
That was real.
It's somewhere over here.
But I don't think maybe it's that.
I don't know.
I remember when it happened.
She mentioned that she reached around and it wasn't where she thought it was.
So common sense thinking involves this.
In most cases, I think several representations.
I don't know if it's as many as seven or maybe 20 or what, but
that's the kind of thing we want to know how to do.
Okay, I think I'll stop and we'll discuss things.
But in the next lecture, I'll talk about a model of how I think thinking works.
What's the difference between us and our ancestors?
We know we have a larger brain.
Think about it.
If you took the brain that you already had in say,
do I remember the name of the little monkey that looks like a squirrel,
jumps around in trees?
Anybody know it?
What?
Maybe.
It's a squirrel-like thing.
You wouldn't know it was a monkey till you took a close look.
Maybe.
Lemur?
I don't, I forget.
Anyway, if you just made the brain bigger,
then the poor animal would be slower and heavier and would need more food
and take longer to reproduce.
The joke about difficulty to give birth.
I don't know if any animal has the problem that humans have.
A lot of people die.
And so on.
So how did we evolve new ways to think and so forth?
And my first book, The Society of Mind,
had this theory that maybe we evolved in a series of higher and higher levels
or management structures built on the earlier ones.
And this particular picture suggests that I got this idea
from Sigmund Freud's early theories.
There's been a lot of Freud bashing recently.
You can look on the web.
I forget the authors.
But there are a couple of books saying that he made up all his data
and there's no evidence that he ever cured anyone
and that he lied about all the data mentioned in his 30 or 40 books
and so forth.
Right.
But the funny part is that if you look at his first major book, 1895,
called The Interpretation of Dreams,
it sort of outlines this theory that most of thinking is unconscious
and it's processes you can't get access to.
And it has a little bit about sex, but that's not a major feature.
And it's just full of great ideas that the cognitive psychologists
finally began to get in the 1960s again and never give credit to Freud.
So he may well have made up his data,
but if you have a very good theory and nobody will listen to you,
what can you do?
His friend Rudolf Fleece listened to him
and there was another paper on how the neurons might be involved in thinking,
which was also written around 1895,
but never got published till 1950 by, forget who,
called Project for a Scientific Psychology.
And it's full of ideas that if they had been published,
might have changed everything.
Because anyway,
what's on your mind?
What would you like to hear about?
Who has another theory?
Great.
So earlier you talked a little bit about how we don't really see
the neuroscience always seems like K-lines, et cetera.
Do you think it's because they're just really hard to find
or no one's actually looking for them?
Well, Grestek's review says he uses vague ill-defined terms
like K-line and microneem and a couple of others and frame and so forth.
They're very well-defined.
I mean, when he talks about neurotransmitters,
it's as though he thinks that chemical has some real significance.
Any chemical would have the same function as any other one provided
there's another receptor that causes something to happen in the cell membrane.
So you don't want to regard acetylcholine or epinephrine as having mental significance.
It's just another pulse, but very low resolution.
And yes, a neurochemical might affect all the neurons a little bit
and raise the average amount of activity of some big population of cells
and reduce the average activity of some others.
But that's nothing like thinking.
That's like saying in order to understand how a car works,
what's the most insulting thing I could say?
Or to understand how a computer works, you have to understand the arsenic and phosphorus
and or what's the other one?
You have to understand these atoms that are, what?
Well, that's the matrix.
So there are these one part in a million impurities
and that's what's important about a computer, isn't it?
What the transistor has gained and so forth?
Well, no.
The trouble with the computer is the transistors.
That's why practically every transistor in the computer is made into another one in opposite phase
to form a flip-flop whose properties are exactly the same except one in a quadrillion times.
In other words, everything chemical about a computer is irrelevant
and I suspect that almost everything chemical about the brain is unimportant
except that it causes, it helps to make the columns in the cortex
which are complicated arrangements of several hundred cells work reliably
whereas the neuroscientist is looking for the secret in the sodium.
When a neuron fires, the important thing is that that lets the sodium in and the potassium out
or vice versa, I forget which, at 500 millivolts.
Really quite a colossal event, but it has no significant.
It's only when it's attached to a flip-flop or to something like a K-line
which has an encoder and decoder of a digital sort every few microns of its length
that you get something functional.
So the trouble is the poor neuroscientist started out with too much knowledge about the wrong thing.
The chemistry of the neuron firing is very interesting and complicated and cute
and in the case of the electric eel, you know what happened there.
The neuron synapse, it got rid of the next neuron and it just, in the electric eel,
you have a bunch of synapses or motor end plates, they're called, in series.
So instead of half a volt, if you have 300 of those you get 150 volts.
I think the electric shock that an electric eel can give you is about 300 volts
and this can cause you to drown promptly if you are in the wrong wave when it happens to bump into you.
I don't know why I'm rambling this way.
You're welcome to study neuroscience, but please try to help them instead of learn from them.
They just don't know what a K-line is and that's a paper that's been widely read, published in 1980,
in RESTAC says, ill-defined and I guess he couldn't understand it.
Yeah, yeah.
Why instead of trying to make a neuroscientist, like trying to find this in the human mind,
why don't we just, as computer scientists, program BK-lines and try to prove that all this is the human mind
and the human producer?
Why is that not always read into the computer scientist field?
Well, I'm surprised how little has been done.
Mike Travers has a thesis, Tony Hearn.
There are three master's theses on K-lines.
They sort of got them to work to solve some simple problems, but I'd go further.
I've never met a neuroscientist who knows the pioneering work of Newell and Simon in the late 1950s,
so there's something wrong with that community.
They're just ignorant.
They're proud of it.
Oh well.
I spent some time learning neuroscience when I was...
I once had a great stroke of luck when I was a, I guess I was a junior at Harvard
and there was a great new biology building that was just constructed.
You probably know it's a great big thing with two rhinoceroses.
What are those two huge animals?
So this building was just finished and half occupied because it was made with the future.
So I wandered over there and I met a professor named John Welsh
and I said, I'd like to learn neurology.
And he said, great, well, I have an extra lab.
Why don't you study the crayfish claw?
And I said, great.
So he gave me this lab which had four rooms and a dark room
and a lot of equipment and nobody there.
And he had worked on crayfish.
So there was somebody who went every week up to Walden Pond or somewhere
and caught crayfish and bring them back.
And I was a radio amateur hacker at the time.
So I was good at electronics.
So I got my crayfish and Welsh showed me how to...
The great thing about this preparation is you can take the crayfish
and if you claw and if you hold it just right, go snap, it comes off, grows another one.
It takes a couple of years.
And then there's this white thing hanging out which is the nerve.
And it turns out it's six nerves, one big one and a few little ones.
And if you keep it in ringer solution, whatever that is, it can live for several days.
So I got a lot of switches and little inductors and things
and made a gadget and mounted this thing with six wires going to these nerves.
And then I programmed it to reach down and pick up a pencil like that and wave it around.
Well, that's obviously completely trivial.
And all the neuroscientists came around and gasped and said,
that's incredible, how did you do that?
They had never thought of putting the thing back together and making it work.
Anyway, it was...
I always reminding myself that I'm the luckiest person in the world
because every time I wanted to do something, I just happened to find the right person.
And they'd give me a lab.
I got an idea for a microscope and there was this great professor Purcell
who got the Nobel Prize after a while.
And he said, that sounds like it would work.
Why don't you take this lab?
It was in the Jefferson.
Anyway, yeah.
Part of the reason that you don't see experimental neuroscience
and things like K-lines is that neurons are long and thin.
So if you want to do an experiment to actually measure a real neural network,
you have to trace structures with roughly maybe tens of nanometer resolution.
But you need to trace them over what might be a couple or even tens of millimeters
to start your week and you need to do this for thousands and thousands of neurons
before you could get to the point of seeing something like a K-line and understanding it.
So it's just a massive data acquisition and processing problem.
But they're doing that.
They don't know what to look for.
Maybe you don't have to do so much.
Maybe you just have to do a few sections here and there and say,
well look, there were 400 of these here and now there's only 200.
It looks like this is the same kind.
Maybe you don't have to do the whole brain.
Even getting a single neuron is because you might get down to,
you need to be looking at electron micrographs of grains that are sliced
at about 30 nanometer slices.
So even just having a single person reconstruct a single neuron might take weeks.
Well, I don't know.
Maybe a bundle of K-lines is a half a millimeter thick.
Oh, so you actually do some larger scales to start looking at.
Why not?
I don't think they have no idea what to look for.
I could give you 20 of those in five minutes, but nobody's listening.
What scale?
I mean, they know what neurons look like, so you know what to look for
if you're saying there's a neuron that level.
I'm saying you may only have to look at the white matter.
Oh, okay.
Ignore the neurons, because the point of K-lines is where do these go
and what goes into them and out.
I don't know.
It's just this idea, let's map the whole brain, 100 billion things,
and then people like Rastak said,
oh, and there's a thousand supporting cells for each neuron.
He's just glorying in the obscurity of it,
rather than trying to contribute something.
Anyway, if you run into him, give him my regards.
I really wonder how somebody can write something like that.
Yes?
Excuse my ignorance, but what is a K-line?
The idea is that both one part of the brain is doing something,
and it's in some particular state that's very important,
like, I don't know, like I've just seen a glass of water.
Then another part of the brain would like to know
there's a glass of water in the environment,
and I've been looking for one,
so I should try to take over and do something about that.
Now, at the moment, there's no theory of what happens
in different parts of the brain for a simple thing like that to happen.
No theory at all, except they use the word association,
or they talk about what are the purposeful neurons.
Goal, forget.
Okay, so my theory is that there are a bunch of things
which are massive collections of nerve fibers,
maybe a few hundred or a few thousand,
and when the visual system sees an apple,
it turns on 50 of those wires,
and when it sees a pair, it turns on a different hundred or 50 of those wires,
but about 20 of them are the same, so forth.
In other words, it's like the edge of a punched card.
Have you ever seen a card-based retrieval system?
If you have a book that has,
suppose it's about physics and biology and Sumatra,
and a typical five-by-eight card has 80 holes in the top edge.
So what you do is, if it's Sumatra,
you punch eight of these holes at random, a particular set.
They're assigned to the Sumatra,
and then if it's, I forget what my first two examples were,
but you punch eight or ten holes for each of the other two words.
So now there are 24 punches,
only probably four or five of them are duplicate,
so you're punching about 20 holes.
And now, if something is looking for the cards that were punched for those three things,
even if there are 30 or 40 other holes punched in the card,
you stick your 20 wires through the whole deck and lift it up,
and only cards fall out that had those three categories punched for.
So you see, even though you had 80 holes,
you could punch combinations of up to a million different categories into that,
and if you have to put a bunch of wires through,
you'll get all of the ones that were punched for those categories you're looking for,
and you might get three or four other cards that will come down also
because all of the eight holes were punched for some category by accident.
Do you get the picture?
I'll send you a reference.
It was invented in the early 1940s
by a Cambridge scientist here named Calvin Moores
and was widely used in libraries for information retrieval until computers came along.
But anyway, that's the sort of thing you could look for in a brain
if you had the concept in your head of Zato coding,
but I've never met a neuroscientist who ever heard of such a thing.
So you have this whole community which doesn't have a set of very clear ideas
about different ways that knowledge or symbols could be represented in neural activity.
So good luck to them when they get their big map.
They'll still have to say,
what do I do with 100 billion of these interconnections?
What are your thoughts about the current artificial intelligence research at MIT,
such as Winston's logistic project?
Winston is just about one of the best ones in the whole world.
I don't know any other projects that are trying to do things on that higher level of common sense knowledge.
He's just lost a lot of funding.
One problem is how do you support a project like that?
Have you followed it?
I don't know if there's a recent summary of what they're doing.
We used to write a new book every year called The Progress Report.
The nice thing is that we had a very good form of support from ARPA or DARPA,
which was every year we'd tell them what we had done.
They didn't want to hear what we wanted to do, and things have turned the opposite.
So what would happen is every year we'd say we did these great things,
and we might do some more.
It went on for about 20 years, and it was, and then it fell apart.
One thing, it's a nice story.
There was a great liberal senator, Mike Mansfield,
and unfortunately he got the idea that the Defense Department was getting too big and influential.
So he got Congress to pass a law that ARPA shouldn't be allowed to support anything that didn't have direct military application.
Congress went for this, and all of a sudden a lot of research disappeared, basic research.
It didn't bother us much because we made up applications and said,
well, this will make a military robot that will go out and do something bad.
I don't remember ever writing anything at all, but anyway around 1980,
the funding for that sort of thing just dried up because of this political accident.
It was just an accident that ARPA, mainly through the Office of Naval Research, was funding basic research.
That was a bit of history.
If you look back at the year 1900 or so, you see people like Einstein making these nice theories.
But Einstein wasn't a very abstract mathematician, so he had a mathematician named Hermann Vile polishing his tensors and things for him.
And Hermann Vile's son, Joe, was at the Office of Naval Research in my early time.
That office had spent a lot of secret money getting scientists out of Europe while Hitler was marching around
and sending them to places like Princeton and other forms of heaven in Cambridge.
And again, one of the reasons I was lucky is that I was here and all these, you know,
if you had a mathematical question, you could find the best mathematician in the world down the block somewhere.
And Joe Vile was partly responsible for that, and the ONR was piping all that money to us for work on early AI.
So it was a very sad thing that maybe the most influential liberal in the U.S. government actually ruined everything by accident.
ARPA changed its name to DARPA. It was Advanced Research Projects Agency, and it had to call itself DEFENSE.
Advanced Research Projects Agency.
Yep.
Well, Christianity wiped out science. That might happen tomorrow. Only choose your religion.
It's a hard problem. The number of people working on advanced ideas in AI has gotten smaller and smaller.
Right now, around 1980, rule-based systems became popular, and there are lots of things to do.
Right now, statistical-based inference systems are becoming popular.
And as I said, these things are tremendously useful, but the problem is if you have a statistical system,
the important part is guessing what are the plausible hypotheses, and then finding out how many instances of that are correlated with such and such.
So it's a nice idea, but the hard problem is the abstract symbolic problem of what sets of variables are worth considering at all when there are a lot of them.
So to me, the most exciting projects are the kind that Winston is developing for reasoning about real-life situations, and the one that Henry Lieberman.
Would you stand up, Henry?
Lieberman runs a world-class group that's working on common sense knowledge and informal reasoning.
And it seems to me that that's the critical thing that all the other systems will need.
In the meantime, there are people working on logical inference, which has the same problem that statistical inference has, namely, how do you guess which combinations of variables are worth thinking about?
Then it seems to me that the statistics isn't so important.
In fact, there's a great researcher named Douglas Lenat in Austin, Texas, who once made an interesting AI system that was good at making predictions and guessing explanations for things.
And it was sort of like a probabilistic system.
It had a lot of hypotheses, and every time one of them was useful in solving a problem, it moved it up one on the list.
So Lenat's thing never used any numbers.
It didn't say, this is successful .73 of the time, and now it's successful .7364825 of the time.
What it would do is, if something was useful, it would move it up past another hypothesis, every now and then, it would put a new one in.
Well, if you're trying to solve a problem, what do you need to know?
You want to know what's the most likely to be useful one and try that.
You don't care how likely it is to be useful as long as it's the most, right?
I mean, if it's one in a million, maybe you should say, I'm getting out of here.
I shouldn't be working in this field at all or get a better problem.
But Lenat's thing did rather wonderfully at making theories by just changing the ranking of the hypotheses that it was considered.
No numbers.
It did something very cute.
He gave it examples of arithmetic.
And it actually, this is a rather long effort, and it actually learned to do some arithmetic and it invented the idea of division and the idea of prime number,
which was some number that wasn't divisible by anything.
It decided that nine was a prime.
It didn't do much harm.
And it crept along and it got better and better and it invented modular arithmetic by accident at some point.
And it's a PhD thesis.
A lot of people didn't believe this PhD thesis because Lenat lost the program tape.
So he was under some cloud of suspicion for people thinking he might have faked it.
But who cares?
Anyway, I think there's a lesson there which is that let's start with something that works.
And then if it's really good, then hire a mathematician who might be able to optimize it a little.
But the important thing was the order and a good statistical one might waste a lot of time because here's this one that's 0.78 and here's this one that's 0.56.
And it's the next one down and you get a lot of experience and it goes up to 0.57 and 0.58.
And it never, you know, might be a long time before it gets past the other one because you're doing arithmetic.
Whereas in Lenats, it would just pop up past the other one and then it would get tried right away.
And if it were no good, it would get knocked down again.
Who cares?
So it's a real question of, I don't know, is mathematics is great and I love it and a lot of you do.
But there should be a name for when it's actually slowing you down and wasting your time because there's a better way that's not formal.
Yeah.
There are people who know the price of everything and the value of nothing.
Yes.
That's very nice.
Yeah.
I know you're also a musician, so I have a music related question.
What do you think is the role of music?
Like why do all cultures have it?
I have a paper about it.
Oh, okay.
I've been trying to revise it actually, but it's a strange question because there is music everywhere.
On the other hand, I have several friends who are a musical.
And so when I have this theory that music is a way of teaching you to represent things in their orderly fashion and stuff like that.
Well, I have three of my colleagues who aren't musical, but they dance.
It may be that I don't know the answer.
It's interesting that the theory, the first theory in my paper is that when you have a lot of complicated things happening, then the only way to learn is to represent things that happen and then look at the differences between things that are similar.
And then try to explain the differences.
Right?
I mean, what else is there?
Maybe there's something else.
So in order to become intelligent and understand things, you have to be able to compare things.
And to me, the most important feature of what's called music is that it's divided into measures.
And measures are the same number of beats or whatever they are.
And so now you can say, what's the different?
You change the eighth notes in the second one, the last four eighth notes, no, the two before last to a quarter note.
So you're taking things that were in different times and you're superimposing those times.
And now you can see the difference.
And the reason you can see the difference
is that you have things called measures.
And the measures have things called beats.
And so things get knocked into very good frames.
Now, there's some Indian music which has 14 measures
for a phrase.
And some of the measures go seven and five.
And I can make no sense of that stuff whatever.
And I've tried fairly hard, but not very.
So I don't understand how Indians can think.
Any of you can handle Indian music?
Adam, what you said about this, my favorite quote
from your paper on music, my meaning,
is the one about what good is music,
about how kids play with blocks to learn about space,
and people play with music to learn about time.
And I think in that sense, both music and dance
are different ways that people can arrange things in time.
And in a sense, like in proposatory music,
in proposatory movement, are both
ways of different blocks, if you will, in time,
as opposed to space.
Yeah, my friends who seem a musically probably,
maybe there's something different about their cochlea.
Or maybe they have absolute pitch in some sense,
which is a bad thing to have.
Because if you're listening to a piece composed
by a composer who doesn't have absolute pitch,
then you're reading all sorts of things into the music
that shouldn't be there.
And the opposite would be true.
I read music criticism sometimes.
And maybe the reviewer says, and after the second and third
movement, he finally returns to the initial key of the flat
major.
What a relief.
Well, I once had absolute pitch for a couple of weeks,
because I ran a tuning fork in my room for a month.
And I didn't like it, because you can't listen to Bach anymore.
Oh, well, it's a good question.
Why do people like music?
And I don't know any other paper like mine.
If you ever find one, I'd like to see it.
Because if you go to a big library,
there are thousands of books about music.
And if you open one, it's mostly Berlioz complaining
that somebody wouldn't give him enough money
to hire a big enough chorus.
But I've found very few books about music itself.
Yes?
Is that all right?
Do you think that having a body is a necessary component
of having a mind?
You do just as well, just as a simulated creature?
Oh, sure.
You have all the things.
Simulation.
I think a mind that's not in a world
wouldn't have much to do.
It would have to invent the world.
And I don't see why it couldn't.
But you might have to give it a start,
like the idea of three or four dimensions.
What happens if you sit back and just think for a while?
You wouldn't know if your body had disappeared for a, would you?
There are also some strange ideas about existence and why
do you think there's a world?
One of the things that bugs me is people say, well, who created it?
And that can't make any sense, because this is just a possible world.
Suppose there are a whole lot of possible worlds,
and there's one real one.
How could you ever, how could you possibly know which one you're in?
And then you could say, well, didn't someone have to make it?
And what's the next thing you'd ask?
What's the next thing you'd ask?
Well, who made the maker?
So the body-mind thing seems to me that once you have a computer,
it can be its own world.
It just can sit.
The program can spend half the time simulating a world
and half the time thinking about what it's like to be in it.
Yeah?
Yes, it's an empty concept.
It's all right to say this bottle exists,
because that's saying this bottle is in this universe.
What would it mean to say the universe exists?
The universe is in the universe?
So there's something wrong with thinking about,
so there are only possible worlds.
There's no, it doesn't make any sense to pick one of them out
and say that's the real one.
Yeah?
But existence is relative.
Yeah, you don't say this is the world I'm in,
but you shouldn't say it.
That doesn't mean it exists.
Like, two is in the set of even numbers.
What's the set of even numbers in?
It doesn't stop anywhere.
Yeah, lots of worlds.
So is mathematics or it's the whole world,
but physics only explains the current world,
or are there how we can guide these two seconds?
Well, you can't tell, because five minutes from now,
everything might change.
So nothing ever explains anything.
You just have to take what you've got
and make the best of it.
Yeah?
So associations are creating systems, knowledge,
and a visual intelligence.
Which knowledge?
Like, systems, basically, in general.
Well, there are people who talk about systems theory,
but I'm not sure that it's well-defined.
Artificial intelligence means, to me,
making a system that is resourceful and doesn't get stuck.
And so if you have a system, and also it's a, how do you put it?
Some definitions are not stationary,
like what's popular?
Popular is what's popular now.
There isn't any such thing as popular music,
in terms of the music.
So I know there was once a little department
called Systems Analysis at Tufts, which
had a couple of rather good philosophers trying
to make general theory of everything.
And they were writing nice little papers.
And it moved along.
But then there was a Senator McCarthy you've probably heard of.
And he announced that he had evidence
that one of the principal investigators
had slept with his wife before they were married.
Well, Tufts was very frightened at this
and abolished that department.
And Bill Shutz went to California and started
Esalen and had a good time for the next 50 years.
More stories.
Kind of was an extension of the body and mind question.
It seems to me like we as humans,
we learn a lot from just interacting with the environment,
like language, we hear it, being spoken, we speak it,
we see things, we touch things.
But as far as I know, a lot of the efforts
in artificial intelligence so far have
been confined to the computer that does not
go out into the real world interact.
Does it kind of turn the bittersweet learning new thing?
Well, here's the problem.
I look over at Carnegie Mellon, and there are some nice projects.
And the most popular one is robot soccer.
And here are these little robots kicking a ball around.
They're Sony, what are they called?
Yes, the Sony I-Bos.
Sony stopped making the I-Bos, but it respected Carnegie,
and it made a little secret stash of I-Bos
to send to Carnegie when the present ones break.
But my impression of AI projects that have robots
is that they do less, less, less than projects that don't.
And the reason is, if you have a robot like Asamo, made by Sony,
no, Honda, Asamo can get in the backseat of a car
with some effort, usually falls over.
However, if you simulate a stick figure,
however, if you simulate a stick figure in a computer
getting into a stick figure of a car,
then you can make it learn to do that and get better and better.
And so all AI projects without robots
are way ahead of all AI projects with robots.
And the profound reason is that robots are usually expensive,
and they're always being fixed.
So if you have five students and the robot is being fixed,
I don't know what they're doing, but they have to wait.
Whereas if you have a stick figure robot,
then you can just run it on this, although it might be
a little slower than your main frame.
Probably not.
Yeah.
The idea of the mind and the body, here's
a theory that I just thought of.
The idea of the body as seen in abstract, basically
a mechanism for input output.
It's a set of sensors from which our brains can get information
about the world and a set of actuators
in which we can display our state.
So in that light, it's almost as if our brains are really
independent on our body itself.
It can adapt to any sort of body if we
happen to hook it up that way.
And it just so happens that we've been hooked up to this body
since birth, that we have such good mental models of how
to use this body.
And I guess an example from experiments
that support this theory might be how when people have limbs
amputated, it takes them a while to forget
that they have the limb, because their mental model
still exists, and their mental models don't go away overnight.
And also, I guess, they train on these different tools
and robot learns with their brains.
Sure.
Well, but it just seems to me that a large amount of our brain
is involved with highly evolved locomotion mechanisms.
And as I said, when you're sitting back with your eyes
closed in a chair thinking about something,
then it's not clear how much of that machinery is important.
But it might be that I have a strange paper on,
I don't know if it's trying to remember its name.
It's called, do you think I can actually
get a, I can't remember the name of the title.
Oh, I give up.
The idea is that maybe in the older theories of psychology,
everything is learned by experience in the real world.
So conditioning and reinforcement and so forth.
In this theory, I call internal grounding.
I make a conjecture.
Suppose the brain has a little piece of nerve tissue,
which consists of a few neurons arranged
to make not a flip-flop, but a, what would you call it,
a three or a four-flop, a flip-flop with three or four states.
Let's say three.
So when you put a certain input, it goes from,
I couldn't find the chalk.
So here are three states.
And here's a certain input.
That means if you're in that state, you go to this.
And if you pop that input again, it does this.
And if you say, go counterclockwise, it goes.
So three of them get you back where you were.
But if I go this, this, and that, that
would mean to go like this, this, and back.
So this would be, that means that's equivalent to just going one.
Get the idea?
In other words, imagine there's a little world inside your brain,
which is very small and only has three states.
And you have actions that you can perform on it.
And you have an inner eye, which can see which of the three
points of that triangle you're on.
Then you could learn by experience
that if you go left, left, left, you're back where you were.
But if you go left, right, left, right, you're back where you are.
And if you go left, left, right, that's like going one left.
In other words, you could imagine a brain that starts out
before it connects itself to the real world.
It starts by having the top level of the brain connected
to a little internal world, which just has three or four states.
And you get very good at manipulating that.
Then you add more sensory systems to the outer world.
And you get to learn ways to get around in the real world.
So I call that the internal grounding hypothesis.
And my suggestion is maybe somewhere in the human brain,
there's a little structure that's
somewhat like that, which is used by the frontal part
of the cortex to make very abstract ideas.
You understand?
The more abstract an idea is, the simpler and more stupid
and elementary it is.
Abstract doesn't mean hard.
Abstract means stupid.
Real things like this are infinitely complicated.
So we might have, and I wouldn't dare suggest this
to a neuroscientist, there might be some little brain
center somewhere near the frontal cortex that
allows the frontal cortex to do some predicting and planning
and induction about very simple finite state arrangements.
Who knows?
Would you look for it?
Well, if you were a neuroscientist,
you could say, oh, that's completely different from anything
I ever heard.
Let's look for it.
And if you're wrong, you're wasted a year.
And if you're right, then you become the new Ramoni Kahal
or someone who's the currently best neuroscientist.
Maybe it's late.
One more question.
One last question.
This is Cynthia Salman, who's one of the great developers
of the logo language.
Yay.
Yay.
Yes.
Maybe it's that question for me, and the last question.
What do you think about theories such as Ramoni Brooks
theories that speak of no-sync robot personality?
Completely weird.
Obviously, those theories have nothing
to do with human thinking, but they're
very good for making stupid robots.
And the vacuum cleaner is one of the great achievements
of the century.
However, his projects, what was it called, Cog,
disappeared without a trace.
That theory was so wrong that it got a national award,
and it corrupted AI research in Japan for several years.
I can't understand.
Brooks became popular because he said,
maybe the important things about thinking
is that there's no internal representation.
You're just reacting to situations,
and you have a big library of how to react to each situation.
Well, David Hume had that idea, and he
was a popular philosopher for hundreds of years.
But it went nowhere, and it's gone, and so is Rod.
However, he is one of the great robot designers,
and he may be the instrumental in fixing
the great Japanese nuclear meltdown,
because they're shipping some of his robots out there.
The problem is, can it open the door?
So far, no robot can open the door, even though it's not locked.
I usually start by asking if there are any questions,
but I thought I'd say a few things about chapter one,
and then see if there are any questions.
I can't see the pointer.
Anybody remember how to get word to make its pointer not
disappear?
Maybe I mentioned this in the first lecture,
but I was taken by this cute poem by Dorothy Parker,
because the first chapter was about love and stuff like that.
So I tried to get the rights to reproduce it,
and it turned out that she was angry at all her friends.
She must have been a perpetually pissed off person.
And so she left all her literary rights to the NAACP,
and I called them up for hours, and they couldn't find the rights.
So finally, so it's in the version of the motion machine
on the web, but I had to resort to Shakespeare
to replace her.
Shakespeare's a slightly better poet,
but he's not as funny as Dorothy Parker.
So the first chapter starts out, or it's mostly
about all the things we don't understand about the mind, which
is almost everything.
And the first discussion is, well,
the whole chapter is making fun of the most popular ideas.
And the most popular idea of the mind
is that people think that they're not doing the thinking,
but there's something inside them that's doing the thinking.
And it's this idea that there is a self
is embedded in just about everything we say and think.
And really, it's hard to see how you would do without it.
But if you ask what is the self, then
since this idea is so popular, people
begin to believe that there is such a thing,
and it takes all sorts of various forms.
And the most dangerous form, maybe,
is the one that religions exploit,
which is that inside a person with all their complications,
there's a little pure essence called the soul or the self
or whatever you want to call it.
And it's impossible to describe it
or explain it in physical terms.
And so that is one of the reasons why
we think there are two worlds, a physical world
and lots of other kinds of worlds.
Each of us has some imaginary model of what they are
and what they're in.
And philosophers talk about it in existentialists and so on.
So there are lots of problems about our ideas about ourselves.
And in reading around for half my life,
I was puzzled at the strange ideas that are around.
And in Aristotle, I find the first intelligible theories
of mind and emotions.
So if you look at particular Aristotle's,
there are a number of books, and one of them is called
Rhetoric.
And it's full of theories about how people reason
and influence each other.
And I'll show you some quotes from that.
Because when I look at the history
that I've encountered about psychology,
Aristotle stands out as being the first and among the best.
And as far as I can see, there were no psychologists
nearly as good as him.
Of course, we don't know whether there was a him exactly,
because what we have of Aristotle
is a lot of writing, but it's all cobbled together
by students from all sorts of manuscripts by other people
and people who took notes.
And Aristotle claims to have learned a lot from Plato.
We have very little writing from him.
And so there you go, three centuries
before the Christian era, as it's called.
And then a couple of thousand years later,
you start to find people like Spinoza and Kant and John
Locke and David Hume who start to make psychology theories,
very little of which is as good as the ones
that Aristotle has in all his fragments.
So one question that frequently bothers me,
and it should bother everyone, is why did science disappear
for 1,000 years?
And the standard explanation is the rise of the great religions,
and why did it come back?
And you see with the first signs of anything
like modern science, at least in my view,
with Galileo and Newton, there are a couple of people
before that.
There are some people in the Muslim world
who invented some high school algebra.
And they make a big fuss about that.
It looks like Archimedes, in a very recently discovered
manuscript, computed in integral.
He found the volume of a cone, which is, what is it, 1, 6th BH?
I forget.
Anyway, so why did science disappear?
And why did psychology appear so late?
Because there isn't much psychology
in the modern sense until the late 1800s
with Francis Galton and William James lived around here.
And Rudolph Fleiss, who, a Sigmund Freud,
starts writing in 1895.
People make fun of Freud, as I mentioned last week.
But in fact, among other things, how many of you
have read the recent criticisms of Freud, which
claim that he was complete faker and never
cured a single patient?
This is popular stuff.
I don't believe Freud ever really
claimed to cure a single patient.
So the critics, who are really very ferocious,
claim that he made up all his data and so forth.
But most of what Freud says is that psychoanalysis might
be a good way to find out what you're really thinking
and discover more about yourself and your goals
and so forth.
And I had the good or bad luck to be introduced to L. Ron
Hubbard when I was an undergraduate.
John Campbell was the great editor of the, I think,
what's called astounding science fiction in those days.
What a marvelous title.
And this fairly mediocre science fiction writer L. Ron
Hubbard invented a new form of psychiatry called, what was it
called?
Dianetics.
It's pretty good.
And I'll tell you that story another time.
But John Campbell had Thanksgiving in the Commander
Hotel every year and invited a bunch of friends.
And I don't remember if that's how I got to meet Asimov
and Heinlein and other people.
But anyway, I did.
And science fiction had a big influence on me
from my, actually, early years.
But starting in college, it got very serious.
Anyway, so chapter one starts to talk
about this phenomenon of psychology.
And one of the funny parts is this little section three,
1.3, of trying to say what are emotions.
And I looked up emotions in dictionaries.
And can you all read that?
I don't feel like reading aloud.
But there's lots of discussion of emotions
and how mysterious and complex they are.
And then the marvelous thing is how many words there
are for emotional states.
I think I got 300, but I don't remember.
Anyway, here's from A to D. And I
don't recall how I found those.
But I think, but that's a lot.
How many words for ways to think are there?
Now, that's a serious question, because I complained maybe
on the next page.
No, I didn't.
I found myself complaining that there were very few words
for ways to think.
And then this afternoon when I was pruning these slides,
it occurred to me that I didn't really try.
So maybe I just didn't think enough.
So if there are a couple of hundred words
for everyday emotions, if any of you
can find me a list of 10 or 20 common words
for styles of thinking, I'd appreciate it.
Because I wonder if there are a lot, and if not, why not?
So here's a list of typical situations,
grieving for a lost child, panic at being in an enclosed space.
I'm not sure any of the words in the list
of 300 standard emotions are good enough
to describe how you feel for any of these not unusual states.
Have you ever lost control of your car at high speed?
No, but when I first learned to drive,
I couldn't believe that you could read signs
at the same time as.
Well, anyway, one of the very best psychologists
in history, or a pair of psychologists,
aren't even called psychologists.
These are two guys named Conrad Lorenz and Nico Tinbergen.
And somebody made up the word ethology.
What they study is the behavior of animals.
And in some sense, presumably, they're
studying the psychology of animals,
because just as with a person, when somebody flies into a rage,
you're not describing their mental state.
You're describing something about how they behave.
And so the ethologists, too, are psychologists,
and Tinbergen and Lorenz, starting around 1920s,
started to analyze the behavior of animals in great detail.
And so here's an example of how a certain fish behaves.
I actually forgot which fish it is,
but there's a picture of it in the book.
And at different points in its life, it's in different phases.
And this is just one diagram of a dozen
for this particular fish.
And it's reproduction, which involves an environment
with plants and other things.
And he divides its behavior into parenting, courtship,
nesting, and fighting.
And then you see each of those has a lot of subdivisions.
And Tinbergen and Lorenz and some students
discovered all these things by sitting in front of fish
tanks and watching the fish for months and years.
Tinbergen also spent years on some beach watching seagulls.
And so he has a diagram like this for particular class
of seagulls.
When I came to Boston, my friends and I
used to go to Nahant and look at these tide pools there,
where there are a lot of activities.
And it was very interesting.
And I got a big fish tank and imported
all sorts of little animals and plants
from the tide pools in Nahant.
And I watched them for about a year
and didn't learn anything.
That was before I read Tinbergen.
And then I realized there's something about those people,
which is they could watch a fish and recognize
all sorts of behaviors.
And I would just watch a fish and wonder
whether it was hungry or wouldn't you get board swimming back
and forth for three years in us.
Anyway, so here are the great psychologists of our day,
Aristotle 2,000 years ago, and Lorenz and Tinbergen
in the 1930s, and Sigmund Freud and Galton and William James
around 1900, and then what went wrong.
There's almost no good psychology between then and 1950s
when something called cognitive psychology started.
And it was partly due to people who said, let's
let's make psychology more scientific.
And you've probably all heard of Pavlov or Watson.
And what happened is around 1900, some psychologists said,
well, these Galton's and Freud's and William James
are very poetic and expressive and literary,
and they write much better than we do and tells good stories.
But they're not scientists, and they
don't do reproducible experiments.
So what we have to do is simplify the situation
to find the basic laws of behavior.
So let's take a pigeon and put it in a vacuum in the dark.
Well, they didn't go that far, but they did put it in the dark.
And there were two illuminated levers to work,
and you could make a sound.
And the sound could be very annoying,
or you could have a right annoying flashing light
or something.
And the animal would push one of two levers,
and one of them would make the stimulus even more annoying,
and one of them would make it go away,
and you'd plot curves of how often
the animal pressed these levers.
So you would get a quantitative theory
of how much it learned and how much it could remember,
and how many trials it would have to do,
and then instead of just looking at reactions to stimuli,
they quickly switched to trying to teach the animal things
by giving it two alternatives, turn left or turn right,
or push this button or that or whatever.
And if they pushed the one you approve of,
then you'd give them a little pellet of food.
And there was a lot of engineering
so that you would make sure that the food got to them
right away, because if there were a 10-second delay between an
action and a reward, the pigeon, or a squirrel, or a rat,
or a cat, or a dog would learn much less quickly
than if there was a one-second delay.
And anyway, that went on for 50 years,
starting around 1900, Pavlov and his dogs.
And there's a great movie that some guy came around with
that had been taken of Pavlov's lab.
And it shows sort of like a great dictator or something.
There's this room with a lot of cages and dogs,
and mostly dogs in this case.
And Pavlov comes in, and there are a bunch of lackeys
who sort of bow and scrape, because he's a lord.
And he comes in, and all the dogs
run into the corner of their cage and yelp.
So the Pavlovians tried pretty hard to get that movie suppressed.
And I haven't seen it in recent years.
But anyway, Fred Skinner, who was a professor when I was
undergraduate at Harvard, was the first one
to really automate this experimental psychology.
And he invented what's called a Skinner box,
but it's just a soundproof, lightproof, well ventilated,
and thermally regulated cage.
And you can put a rat or a pigeon.
Those are the most common animals.
They're very inexpensive, because they're free.
No one knows much about dolphins.
They've been studied for 50 years,
whenever John what's his name?
Remember the name of the great dolphin?
Lily, thanks.
He discovered a lot about dolphins.
And a certain amount about their communication
and a little bit about whales.
But there's an interesting mystery.
I forget which whales, but some whales have a 20 minute song.
And they repeat it for a whole season.
And next year, that song is a little bit different.
But it goes essentially without repeating.
It's very complicated for 20 minutes.
And people have studied that a lot.
And no one has the slightest idea of what it means.
And nobody even has any good conjectures, which bothers me.
What I think it probably means is this.
When there's a whole bunch of fish somewhere
for one of these whales, it might be 200 miles away.
And whales eat a lot.
And it's very important to find where the fish are.
And I believe this message, which changes a bit
during the season, might be telling you
where the food is on the Atlantic or Pacific coastline
in great detail.
Because if somebody finds a lot of fish somewhere,
you have to swim 300 miles.
And if they're not there.
So anyway, it's interesting that John Lilly got
a lot of publicity, but he didn't discover squat.
And finally, the dolphin study years gave up
because nothing happened.
Anybody have heard anything?
I haven't paid any attention for 20 years.
Have you heard of anybody discovering anything about dolphins?
Except they're very good at solving
a lot of physical problems.
Anyway, that's unbothered by the mystery
of why was there some psychology in Aristotle's time?
And why didn't it get anywhere till 1950
when there was regular psychology?
But it was afflicted by what I call physics envy.
Namely, you run into people like Estes.
And well, he was pretty good, actually.
But there are a lot of psychologists
who made up things like Maxwell's equations
for how animals learn.
And there were generally three or four laws.
And if there's a sequence of events,
then animals remember a lot about the first few
and the last few in the sequence.
They don't remember much about the middle.
And of course, the reliability of their memory
depends a lot on how recent it was
and on how powerful the reward was and blah, blah.
And so they get these little sets of rules
that look like Newton's laws.
And that was the kind of sort of psychological physics
that the so-called behaviorists were mostly looking for.
This was not what Tinbergen and Lorenz did.
Because they wrote books with extensive descriptions
of what the animals did and made little diagrammatic guesses
about the structure of the subroutines and substructures.
Anyway, end of history, but it's a nice question.
Why do some sciences grow?
And why was psychology just about the last one?
I suspect it could have been earlier,
but people tried to imitate the physicists
and tried to say maybe there's something like Newton's
or Maxwell's laws for the mind.
And they found a few, but they weren't enough to explain much.
So there are a lot of questions.
When Seymour Papert and I started thinking about these things,
which was really around 1960, I had
been working on some ideas about AI in the late 50s.
And my PhD thesis was a theory of neural networks,
which was sort of interesting, but never really went anywhere.
I went to a meeting in London somewhere
and gave a talk about a theory of learning
that was based on some neural network ideas.
And there was this person from South Africa named Seymour
Papert, who gave the same paper.
I hope this happens to you someday.
Find somebody who thinks so much like you,
only different enough that it's worth it,
and that you only have to say about three words a day
and some whole new thing starts.
Because we really did write the same paper
and it had the same equation in it.
And he had been working for Piaget
who was the first great child psychologist.
I should have mentioned Piaget, who probably discovered
more things about psychology than any other single person
in history.
And there are lots of people now who
say he was wrong about 0.73, because children
learned that at the age of 2 and 1 half instead
of 2 and 3 quarters.
I'm parodying the Piaget critic community,
but it's pretty bad.
And I think those poor guys are uncomfortable,
because Jean Piaget published 20 books full of observations
about children that, as far as I know,
no one had made systematically before.
And in his later years, he started
courting algebraic mathematicians,
because he wanted more formal theories.
And in my view, he wanted to make his theories worse.
And nothing much happened.
But he did visit here a couple of times.
And it was really exciting to meet
the starter of a whole new field.
Anyway, Papert and I discussed lots of things.
And somehow or other, we kept finding other, more ideas
about psychology.
And it finally gelled into the idea that, well,
if you look at the brain, you know
that there are several hundred different brain centers.
What's all that stuff for?
And how could it possibly make any sense
to try to explain what it does in terms of four laws like Newton?
How does a car work?
Is there a magical force inside the engine
that causes the wheels to turn?
No.
There's this funny thing in the back
to cause a differential so that if the car isn't going
in a straight line, the two wheels going at different speeds
won't rip the, if the two wheels were going at the same speed,
the tread would come off in five minutes.
Did you ever wonder what a differential is for?
So most of the car is fixing bugs in the other parts.
Most of the brain is because we started out
as fish or lizards or whatever you like.
And making the brain bigger wouldn't help much.
Because you'd just get a heavier lizard that
had to eat more and would think more slowly.
So size is bad.
But on the other hand, if you need another cubic inch
of brain to fix the bugs in the other part,
then the evolutionary advantage of being smarter
had better make you able to catch a little more food per hour
and so each person is an ecology of these different processes.
And the brain reached its present size
about a million years ago, I guess.
What's the current guess?
Anybody been tracked?
They keep discovering new ancestors of humans.
And I don't have the patience to read about them
because you know that next week somebody will say,
oh, that isn't in the main line.
And you were just unlucky to discover that skeleton.
So anyway, Papert and I and a lot of students
gradually developed this picture that the mind is
made of lots of processes or agents or resources
or whatever you want to call them.
And it's anybody's guess what they are.
If you look at the anatomy of the brain,
you know that people label regions.
So it's very clear that this occipital lobe back here
is largely concerned with vision.
And I forget where the one for hearing is.
If you destroy the part of the brain for hearing in some animals,
you get a little bit of increased function
in some part of the visual system
that seems to enable the animal to hear a little bit
and make some reactions.
And there's a whole lot of hype, I think you have to call it,
about the flexibility of the nervous system.
That is, if certain brain areas get destroyed,
other parts take over.
They almost never are as good.
And mostly, many functions never get taken over at all,
but are replaced by ones that superficially seem similar.
And so there's a whole lot of, I guess,
wishful thinking that the brain is immensely resourceful
and error correcting and repairing.
I think there was some idea that it was a general phenomenon.
But if you do some arithmetic, you get an interesting result.
Suppose that each function in the brain occurred
in 10 different places at random.
Then if you removed half the brain,
how many functions would you lose?
Well, almost nowhere rhythmically tells you
you would lose about one part in the thousand.
And so, in fact, you would never be able to detect it.
So this idea that the brain has enormous redundancy
will now change that number to five.
Suppose each function is somewhat supported
in five different parts of the brain.
Then if you take off half the brain, then what am I saying?
One part in 32, chance of losing some significant function.
So probably lots of things that we do
are supported in several parts of the brain.
Apparently, the language center is pretty unique and some others.
But be careful about the conclusions
you read from optimistic neuroscientists.
Anyway, Papert and I worked on this idea of how could
these large numbers of different processes be organized?
And we made various theories about it.
And then around, I guess, the late 1970s,
we stopped working together.
And Papert developed his revolutionary ideas
about education, which certainly have had a lot of influence,
although they didn't sweep the world in the way we had hoped.
And I kept working on the Society of Mind Theory.
And we didn't work together so much,
but we still did plenty of criticizing and supporting
of each other.
Anyway, my theory ended up with this idea that it's sort of
based on Freud.
I don't know if I kept a picture of his here.
Freud concluded that the mind was an interesting arena,
and he had the mind divided into three parts.
At one end of the mind, which we inherited from most other animals,
is called the id, which is a bunch of instinctive, mainly
built-in behavioral mechanisms.
And a second part of the mind is what
he called the superego, which is a collection of critics.
So in Freud's first image, the brain is in two parts.
One is a set of instinctive built-in behaviors,
and the other is a set of critics, which actually
are associated with a culture and a tradition.
And you learn from other people things that are good to do
and things that are bad to do.
And that's called the superego.
This is your set of values and standards
and tests for suitable behavior.
And the middle is this strange object
called the ego, which is not what people think it is.
At least Freud's word, the ego is a kind of big neutral battle
ground where the instinctive behaviors, I keep wanting to.
Oh, you can see that arrow if I take my finger off it.
And then gradually, as I kept trying
to figure out how problems are solved
and what kind of processes might be involved,
I got this picture which has six layers.
And various people come around and say,
I don't think you need to distinguish
between layers four and five, or why don't you
just lump all the three top layers into one.
And I sort of laugh quietly and say,
these people are trying to find a physics-like unified
minimal theory of psychology.
And they're probably right in one sense.
But if they do that, they'll get stuck.
Because if you get a new idea, there'll be no place to put it.
So if you have something that's very mysterious,
don't imitate the physicists.
Because if you make a theory that's exactly right
and just accounts for the data and there's nothing extra
and nothing loose, then when you notice a new phenomenon,
like dark matter, then the physicists
don't know what to do.
Should they regard dark matter as some obscure feature
of spacetime, or does it have something
to do with this universe being near another one
that you can't otherwise communicate with?
And it's all very puzzling.
But there are lots of things that
don't fit into Newton's laws these days.
And I'm not suggesting a six-layer theory of physics,
but it might be worth a try.
OK, so I made up some nice slides, but I think I'll stop.
So who has some questions?
And what would you like to see in the theory of psychology?
What do you want to be explained?
A lot of people are convinced that there's
some really serious problems and mysteries,
like what is consciousness?
And if you look at chapter four, my feeling
is consciousness is an etymological accident
that people got a word, which is a suitcase for all
of the things they don't understand about the mind
and more.
But once you've got a word and it goes
in the culture, consider the word consciousness for a minute
from a legal point of view.
Suppose that you happen to be walking along,
and you're carrying something.
Where is that pointer?
And it happens to stick somebody's eye out.
Then it's very important when they sue you to establish
whether you meant to do it or whether it was an accident.
Did you consciously plan to?
I can't think of the English word
for putting somebody's eye out.
There's be heading.
Gouge is a good word.
Yeah, so anyway, it's very important for social reasons
to have a word for whether an action was deliberately
violating the rules as opposed to accidentally violating
the rules.
Like if you tripped on the stairs
and landed on somebody and broke their neck,
that's not a crime unless you were so clever as to make
it appear that it was an accident.
Anyway, you see what I mean.
So our whole system of fairness and ethics
and social responsibility is based
on the distinction between whether an action was
deliberate or not.
And so did he do that consciously is a word for that.
And somehow the idea of conscious became elevated.
Well, that's a very superficial.
You can probably think of 10 other reasons
why a word like that.
Yes?
Sometimes in your writing, it seems to me
that there is both sides of it.
Some of you argue both sides of it.
Like it seems like in that kind of,
I can well imagine there's that kind of representation.
You have representation of self and representation
of your mind.
But then you say there's no self or no consciousness.
Why can't you think about consciousness
as just a process that is reasoning about your own mind?
Well, I mean, I understand.
I don't want to talk about self as that's a ridiculous notion.
No, but sometimes when you say conscious,
that is, do you remember doing it?
Which is?
Yeah, but don't I load up my process that
asks me what I think about my own mind
and then I retrieve that and I say, yeah, I do remember it?
Well, you're right, actually.
I went to a lot of trouble to find 25 or 30 different uses
for the word consciousness.
And probably if one of us worked harder,
we could take those and condense them
into five or six much better ones that account for more stuff
than the 30.
Why do you think you got five right there?
I think his goal is magnifying some feelings, right?
That might be just right for something.
Who knows?
Yes, well, I think that's a great criticism of one reason
why people don't like these theories quite so much,
because I propose too many things.
I really should reprint that criticism from RESTAC
or hand it out, because this neurologist who says,
why is he telling us all these things about K lines
and representations and so forth?
The answer is he's from a community that
doesn't have enough variety yet.
And I'd be the first to admit that I
try to go overboard and think of five more things
than are in the literature.
But that emotion thing is nice.
I remember that that was a serious challenge,
because when I made that list, I do have a laser pointer
somewhere in my jacket, probably.
How many words for, at least noticeably,
distinct ways of thinking or reasoning or figuring out
or solving problems can you think of?
Maybe there are 20 or 30.
I just realized this afternoon that I never looked.
I don't remember where I got this list.
Yeah?
Can I ask about your perception of free will?
And I got a lot of readings that you don't have a strong sense
of free will, and so what is that?
I think it's the same as the one for consciousness.
Namely, it's a legal concept.
The idea of free will is completely obscene, isn't it?
What could it possibly mean if you did something for no reason?
So it's a thoroughly empty idea, isn't it?
Or what do you mean by it?
Do you mean there's nobody ordering you around
so you're free to do whatever you want?
But, of course, you're not.
You can only do what your computer computes you to do.
So in a sense, in the same way that you're drawing the fish
diagram, the fish's actions are products of the environments
and of its current state, and it's essentially a turning machine.
Well, it's some kind of machine, yes, sure.
And so would you argue that we are also
a turning machine that are just running out in the world?
Sure.
I've never heard of any even interesting alternative.
In other words, people who insist on free will
appear to me to be like people who believe
that there must be a God who created the world.
What's the next step?
Who created the God?
They don't take that step.
So if your will is free, OK, then who's controlling it?
There's nothing there.
But legally, it's great.
Because if somebody stole some money of their own free will,
but suppose you were a peculiar kind of epileptic,
and every now and then, when you go by,
your hand goes out and steals things, then what do they do?
They put you on parole?
No, this is very strange.
But if you look at religions, you
see that people make money on them.
13% of the world's product goes to people
who make their living on concepts like free will
and consciousness.
So it's a big money thing.
It's not just an accident.
It's an industry.
So both of those are constructs of society.
They're power is good.
Imagine a society without the concept of consciousness
of free will, because those are requirements for being.
I don't think you could.
You'd have to make up something to keep people in check
and under control and to train them.
It's like the rat needs somebody to press the reward
or punish button.
And we have it built into our culture works,
because you build into people's head
the machinery for suppressing doubt.
And it's very clever.
But you should think of it as an industry,
rather than an explicable phenomenon.
How much money goes into?
Yes?
How many ways of thinking can you think of?
Say it again?
How many ways of thinking can you think of right now?
That's my challenge, in fact.
There's probably a big list in some chapter or other.
But there's nothing like this.
What's the trick?
Three, if I go like that.
Actually, dragon has a thing, so you
can tell it make things bigger.
How many of you use the new dragon program speech thing?
I can't believe how good it is.
Oh, I was talking to Henry Lieberman about it earlier.
Yes?
So I'm wondering, personally, you
would say that the side of mind here
is both the human and animals.
Is it just that we have higher levels of organization
than them, and so where would you draw that line?
Like, do animals not have a notion of the cell
that we describe in the book, like long-term planning?
That's a great question, and it'd
be interesting to think about ways to investigate it.
People are, researchers are often, in fact,
raising that question of, do animals
have a representation of themselves?
And there's a famous experiment,
but I can't remember what its current status is,
where you put a red dot on a chimpanzee's head.
And when the chimp passes a mirror and sees that,
the chimp might go like that.
Whereas I don't think a dog, when it passes a mirror,
would rub its forehead to see if it has a red spot.
I had a cat who walked past mirrors,
because we have some wool full-size mirrors around the house.
And the cat walks by, and there's
another cat in the mirror.
And she pays no attention to it, whatever.
So of course, I don't know what happened the first three
times she walked by that mirror.
Because if you see another cat going by, you'd think it would.
Anyway, it's a good question, and people ask that,
and there's some evidence that elephants
have a model of themselves, and maybe dolphins.
And I don't know where.
Have any of you heard any stories of other animals
that can recognize, for example, when they've been painted?
Which?
Is it elephants?
Yes, I think elephants.
It can, but it doesn't know what it can.
So there's a famous child psychology experiment
where, if you're less than a couple months old,
you actually fail this test.
So it's actually something that comes,
and it's a sign of your child progressing.
So it might not be intrinsic to humanity by the sun.
So I'm sure the invention of that is probably going to do it.
This is off the topic, but I once
had a great email correspondence with some woman who
was getting a PhD in France about how babies recognize
their mother.
And she concluded with, you did experiments
after having other people walk into the room with a mask
of the mother or a different hairdo and so forth.
And for the first two months or three months,
I think, it turned out that the baby recognizes
the mother by the hairdo, which had not been known.
Then I think after three months, it's
recognizing the mother by a face.
And at that point, she's doing experiments
where you get another woman wearing a copy of the mother's face.
And so now there's two mothers, and the baby is absolutely
delighted.
And then, as I can't remember, then I
think at four or five months, when two copies of the mother
comes in, the baby gets really panicked.
So I lost track of her.
If you have a baby, let me know.
She got her PhD for this, and I haven't heard anything since.
Yes?
Here's a sort of tactical form-related thought
that I just thought of regarding ways of thinking
and ways of feeling.
So it just occurred to me that it seems like,
if you go back to the list of feelings,
it seems like when we talk about feeling,
we're talking about a state that the brain is in.
So it might be a complicated state
that's some combination of a lot of different parameters.
But it's a state that you can stay in for an arbitrary amount
of time.
But I think thinking is something
that's more sequential, as in when you're thinking,
you're necessarily changing the state of your brain
all the time because you're moving bits around.
So I just thought of that.
No, I think that's right, that when
we talk about intelligent behavior, you're absolutely right.
What you've got is a process that's criticizing itself
and seeing when you got stuck in finding things to do.
And I suppose, in each emotional state,
you're certainly also thinking, so that's going on.
But maybe it's more restricted.
Like, if you're confronting somebody
and there's a sort of conflict, then almost all your thoughts
are constrained to that subject.
And it's not as resourceful.
But I'm just improvising.
What would you have to worry at each emotional state
when we say that we're in an emotional state?
The things that we're talking about
are the certain switches that get
flipped in a certain state, or something
is above a certain threshold.
And you can have thoughts that are about anything
in that state, but perhaps the state itself
influences the type of thoughts that you're likely to have.
OK, I think what I'm talking about in this context
is sort of extreme forms where the person changes
into another machine, and it's like an angry person
won't listen to reason, or it's very hard to deflect them.
So it's this kind of rigid thing.
But humans are generally, are rarely
in such extreme states where nothing gets through.
The whole point of that was that I just realized that maybe
was just too lazy, that we have this huge vocabulary of nuances
of emotional activity.
And people think these are, also people
think that these are hard to explain and mysterious,
and non-physical, and blah, blah, blah.
But why do we wait till next week and see
if somebody comes up with, or see if we
can come up with a set of 30 or 40 words about intellectual
states, curiosity of, I just don't know how many there are.
And I haven't thought of any in the last few minutes.
Yes.
Has anybody thought of a couple of?
What's the word?
Flow, like juice in the hide, like when
you're really engaged in some activity that you're doing,
and you're really in the zone.
Uh-huh.
Yes, there's this.
There's a state of keeping other things out so you can focus.
Not being interruptible.
Yes?
So in this side of mind, you talk about agents
and how they divide between themselves.
But I don't see anywhere about how evolution
like modified a lot agents.
Like, I believe that evolution modified the way
we think right now.
I don't know if you saw.
There is a paper like, wrote like two years ago.
It's called The Vision of Behavior.
And this guy tried to explain how we make some decisions.
Evolutionary, like in a point of view of our speech,
we are maximizing the probability of reproducing
ourselves.
But individually, we are not kind of increasing these
deficiencies.
I think somehow, like the decisions of agents or resources
would be very determined by evolution,
since we have a very long time of evolution of the human being.
And somehow, we have hard wire to make some decisions.
So for example, like he gives the example of a guy.
Well, for lizards, that's certainly true.
But why do humans keep changing their environment?
And so like, for example, yeah.
So he gives this example of tossing a coin.
The guy says that this coin is unfair.
And he doesn't say the probabilities.
But there is 75% of getting heads and 25% of getting tails.
And we, like the subjects, they choose to run only 25%
of the time the tails, even though they
can take account of the number of the times that you put heads.
And even though if you choose always hands,
you would get more money or whatever.
You would make the right decision.
That's called probability matching.
And it's not a good strategy.
Yeah, but if humans do that, like.
No, well, they do it if the psychologist rigs
the experiment very carefully.
It turns out that the best thing to do,
what do you think is the optimal strategy?
No, the optimal strategy turns out
it's the square roots of the probabilities
normalized to add up to 1.
And I'll give you a proof next time.
This theorem is due to Ray Solomonoff,
who invented inductive probability theory.
So evolution, if evolution did probability matching,
then it would be wrong.
And I bet you'll find out that those experiments are wrong.
You have to see, how did he rig the experiment
so that people, if it's probability 25%,
they guessed that 25%.
They made the experiments with goldfish.
I don't know.
There was a theory about why you would expect it, right?
It's a good question.
I don't think people use probabilities, though.
So even if an experiment shows some,
I would look for a flaw in the design of the experiment.
Yes?
So Professor, you talked about a lot about motion to lecture.
And my understanding is that for someone
as a specific personality, they might
have a predisposition to feel a certain emotion,
an anger, a few, depression, or whatnot.
My question is that, if you had any insider theories on,
to what extent our personality is affected by the sense
or influences that happen to us over the course of a lifetime,
and to what extent is it impacted by a sort of chemical
or a bylaw in the makeup of a carbohydrate?
Well, you're asking what things do, what do people learn?
We don't care if it's chemical or,
see, if it's chemical, it's still physical.
I read a little bit of that sort of treatment
to your depression.
And the argument is that a lot of reasons
for your depression is because of some sort of chemical
or a bylaw to call, we have brains constructed.
Well, there's lots of complicated things about the brain.
One feature of the brain that I don't know if everybody,
you know that there are inhibitory and excitatory synapses.
When one neuron connects to another,
the impulse that goes along the axon to the target neuron
may reduce the probability or the strength of its firing
or increase it.
So that's called inhibiting or there's no, or exciting.
That's not quite the right word, is it?
Now, generally in the nervous system, as a rule, but not
always, if you follow a chain of activity,
it goes inhibiting, exciting, inhibiting, exciting.
If you had too many excitatory things and there was a loop,
then it would explode and it would wear itself out in jig time.
So there is this general feature of the anatomy
that you alternate.
So when somebody talks about a drug having an inhibitory effect,
that's sort of weird because it's inhibiting half the neurons
and therefore lowering the thresholds of the ones
they're connected to and so on.
So I think the best thing is until you
have a diagram of the functional relations
between different brain centers, it
might be best not to try to make generalizations
about how the chemistry works.
It's easy, but people think of adrenaline as a stimulant,
but in the nervous epinephrine, but in the nervous system,
it's locally it may be inhibiting things that
are inhibiting something else.
And so it appears to be exciting.
Yes?
So I have a problem understanding the difference between dots
and emotions.
I know it might be a simple thing.
But since the only thing that I can separate in my mind
is dots, so let's call it a time constant,
I can change it kind of rapidly.
Emotions, time constant is long, and I can control it
again much less.
But since there is no, at least in this class,
there is no preview, how can I make a decision on where
the dividing line between these two entities is?
Oh, I think it's a waste of time.
As far as I can see, emotional mechanisms
are generally lower level, simpler ones
than the ones that involve several layers of the brain.
So it's just a relative thing.
It's not that some states are emotional.
You're always having some high level thoughts
and low level thoughts.
And the distinction, I just don't know
why the distinction has occupied so much tension.
I think it's because, and that goes back
to having more words for, or asking how many words do we
have for ways to think.
It seems to me that in popular culture,
there are very few words for ways to think,
and lots of words for emotions.
And so they're very prominent.
Maybe you have to be smarter to distinguish between ways
to think, and people generally are dumb.
Not because they're inherently dumb,
but they come from cultures which bully you if you're,
what happens if you're in third grade and you're smart?
You get it beaten out of you, and you learn not to show it.
I think that question of why you did science happen earlier,
and why we have more ways to describe different ways to think.
And that sort of puzzling, but can we just not
reflect it as much on different thinking states
or different approaches?
I wonder if the Greeks had more, but we did.
I think they did.
I think they had also more concept of ideas
and different states and their approaches.
Who has a theory of that?
What's your theory of the Middle Ages?
How could things get so dark for so long?
Well, I do have a theory of ages.
And are we about to have one?
I think it has this much to do with the channels
in which one can communicate ideas to other people,
whether they exist or not, whether the arteries
are over their clothes.
The Middle Ages were characterized
by scientific discoveries being kept as family secrets.
Cardin knew how to take humorous,
and he wouldn't tell anyone.
Well, the classic stupid example is
baby talk, which for 300 years made
a single Italian family very rich.
Using tons to extract the baby in proper,
they increased success rate in difficult parts
by about 10% they say.
Wow.
And that was enough to build a family fortune
until some servant party stole the gene.
That was the end of that.
Yeah, wow.
So who has a theory of the Middle Ages?
Is there a standard theory?
Yes?
Well, the concept of the Middle Ages
as the Dark Ages is something that emerged mostly
in the Renaissance when people in the 15th, 16th century
tried to present themselves as going back
to the classical age of the scholarship of ancient Greeks
in Rome, and as being better than their predecessor
for the last few years.
This mostly happened because of the discovery of manuscripts
that were translated from the ancient Greek,
in certain cases Latin, by Muslims who at the time
were sort of receding from Europe.
But so the entire concept of the Middle Ages
might be a fabrication of the Renaissance.
There were some significant discoveries at the time.
That's a nice idea.
In other words, when was St. Patrick?
St. Patrick.
I'm told that he popularized a lot of technical manuscripts,
brought them back into Europe.
He has two achievements.
One was bringing scientific culture back,
and the other was getting snakes out of England or something.
Ireland.
I don't know which he was sainted for.
Don't you have to do three miracles, or is it?
Yeah.
My theory is that the Middle Ages ended around 2100.
They'll say after the Middle Ages ended, they'll say,
you know, those guys back in the 21st century,
now they had no idea how thinking worked.
They couldn't even think of a few ways to think.
They had poverty, they had wars.
You know, those guys were barely out of their loincloths.
Right.
I just read a history of AI.
I forget who wrote it.
But it had this section saying, mentioned the Newell Simon.
There was a thing called general problem solver, which
I mentioned a couple of times in the book.
And it's the idea that the way to solve a problem
is to find, it's a symbolic servo.
Find the difference between what you have and what you want,
and look in your memory for something
that can reduce that difference.
Keep doing that.
And of course, it's important to pay attention
to the more important differences first and so forth.
And I'll send you this article.
This article is saying that they made a terrible mistake,
and this was a trivial theory.
And that's why nobody uses it anymore.
And it was interesting how many AI people fell for that idea
in the 1960s.
My complaint has been that if you look in a modern textbook
on, you must have some in your first volume, Pat.
Didn't you have some GPS things?
If you want to keep up with AI, you
should read Patrick's textbook, even though people
are starting to use this new one, which
doesn't have any AI in it.
By who's it by?
Russell and Norvig.
It's probably pretty good technically.
But I leafed through it, and it didn't have any.
Never mind.
It's probably better than I think, because I'm jealous.
Yes?
Kind of a different topic.
In the society of mind, you're talking about the amnesia
of infancy when you forget what you learned,
and things that were once difficult to become common sense,
and you can't even remember how it was different then.
So it's just kind of wondering about the reverse
of that process when you try and say,
do something to somebody that you turn on,
so that you're pretty organized on it,
and bring back up the different levels.
I don't know exactly what the question was,
but it was just kind of quite fun.
And wondering about, is that itself another way
of re-learning the things that you learned?
That's sure an interesting question.
When I first learned about programming, I was interested.
I had the idea that maybe babies think in machine language,
and then after a while, they start to think in FORTRAN.
And then finally, when they're a little older,
they think in ALGO, or something.
But when they switch from machine language to FORTRAN,
then they can't remember their earlier thoughts.
And there's almost no evidence of people finding
genuine recollections from two-year-olds at later ages.
Now, almost everybody thinks they remember something,
but there's the problem that you might have rehearsed it
and translated it into the FORTRAN and the ALGO and the LISP
and the logo, whatever it is.
One of my greatest influences was a great mathematician
named Andrew Gleason at Harvard,
who I met practically the first day I got to Harvard.
And he would always talk about things I didn't understand,
I would go home and look them up and try to.
Anyway, one day we were talking about number forms.
And number forms are a psychological phenomenon
which about 30% of people have.
And it was first described by Francis Galton.
And the phenomenon is if I ask you,
close your eyes and tell me where is the number 3,
how many of you have a place for the number 3?
Well, that's a few.
And so typically, if you imagine the visual field,
it's a windshield, I guess.
So there are these numbers and they're nowhere in particular,
except that it's usually like that for an older child.
So here are these numbers.
And what's more, in some people they're colored.
So I was talking to Andrew Gleason,
I had read this Galton paper, which was 1890 or 1885 or something.
And so I was asking people if they had number forms.
And he said, oh yes, he has one.
And he sketched it for me.
And he said, and they're colored too.
Oh, and his went way up.
And the prime numbers were bright.
What am I doing?
Maybe the composite numbers were, something was bright.
And they were colored.
So I wrote this down and I,
over the next couple of years, I would look in antique stores
for old children's blocks.
And I found a set of blocks that matched that.
And Andrew Gleason said that he knows when he acquired this thing.
And it was about four years old.
And he had a window which, in his house,
and there was a hill.
And he could just see over the sill.
And he imagined these numbers on the side of that hill.
Blah, blah, blah.
Anyway, people who don't have a number form
don't know what I'm talking about.
And I don't know if the 30% is still true.
But it's an interesting phenomenon.
And in most cases, of early childhood,
well, you can't find out because children do remember details
of a house they lived in.
But you don't know if they've copied it.
So what was the original question?
How much can you remember from infancy?
Elran Hubbard thought you could go back to before you were born.
And you could remember people talking about you
when you're still in the womb.
So anyway, John Campbell said you should look into this.
And a few of us made an expedition.
We went down to Elizabeth, New Jersey,
to visit the just starting up Dianetics Center.
And I met this Elran Hubbard who had green eyes
and was quite hypnotic looking.
And the end of the story is he had been writing about how
if you took this treatment of Dianetics,
then you could memorize an entire newspaper
and do all sorts of miraculous things like that
once your mind has been cleared of aberrations and obstacles.
And it became a big industry and turned into later Scientology.
I'm sure you've all heard about that.
So we asked Hubbard to look at a newspaper
and tell us what was in it.
And he explained he was so busy training the other people
to be cleared that he hadn't had time
to go through the procedure himself.
And I never saw him again.
Yes?
What are your thoughts on memes and the fact that we're just
that the codes or art thoughts are actually developed
into something new?
Memes?
You mean Dawkins?
I didn't quite get the whole question.
The idea that...
For example, the way we talk and the way we all talk,
how we very much mix the way our parents talked
with people about it,
and possibly the way we think as well.
So how does that relate to how our mind develops?
Are we actually creative, original characters?
Well, of course, it's both,
because you learn things from your culture,
and then you might just mainly repeat things
or you might get the knack of making new ideas.
I'm trying to remember what Dawkins' main ideas are.
He invented the word meme to say that
the ideas that people have might be considered to be
somewhat similar to the genes in our heredity
and that societies are systems in which these memes,
which are conceptual units of meaning or knowledge,
propagate around and self-reproduce and mutate and spread.
And I don't know what to say about it,
except that it's obviously true that every now and then
someone gets a new idea and tells people,
and for one reason or another they either forget it
or tell someone else, and after a while it spreads,
and some of them fill up the whole culture
and some just die out.
And whatever else Dawkins says, he's a very smart guy,
but almost everything then, in my mind,
is that he's explaining that religions
are mostly made of these memes and they're very bad
and cost the world a great deal in progress and productivity.
In other words, he's a militant atheist,
and there are about five best sellers in that business today.
But I don't know what else to say about memes.
It's an obviously generally correct idea.
But the great thing about genes is we know the four amino acids,
or four nucleic acids they're made of,
and how they're rope together and all that.
And I don't think Dawkins' theory develops anywhere
as elegantly as modern genetics,
so it would be nice if it could.
A really good theory of good ideas would be nice to have.
What would it look like?
Someday we'll have an AI that just punches them out.
Oh, right.
Robert Heinlein has some stories in which the superintelligent people
have a language that's so dense that in five syllables
they can explain something that would take you a half hour.
I think, I forget what it's, log-lan.
Anyway, if you need a good idea, read Robert Heinlein.
Sure.
I forgot where you mentioned this.
You talked about how geniuses,
they might have just come up with better ways to think better,
better ways to learn about how to better learn.
But why do you think they never mention it,
and why hasn't they complicated them
like method of bettering learning about how better to learn?
That's a great question.
Do you think people have a concept of an idea that improves better learning?
There's a couple of phenomena that,
like how come there were so many geniuses in Athens?
And then some of the best mathematicians came from some high school
in some little country next to the Baltic.
Bulgaria, Romania.
Yes, there was some high school in Romania
that not only produced von Neumann,
but about five or six other world-class mathematicians.
I don't remember the details.
So that's a nice question.
How come, if there are these great memes,
how come there aren't more big pockets of them?
But there are a lot of cultures which were very inventive
in other than intellectual fields.
Paris got all those artists in the...
How many of you saw the Woody Allen movie?
What's it called?
Paris at Midnight?
So funny.
Like people will be able to think of one,
they want an idea that will improve your improvement of learning.
Right.
That's a good question for each of us.
What's your very best idea?
And stop fussing with the other ones and get that one out.
There must be some people who are very quiet
and only speak once in a long time.
We should watch them carefully.
Yes?
I guess along with my...
Do you ever feel restricted by language?
Wait.
It's the sound I can't hear.
Is that strong enough to lift you?
I'm sure someone's done it.
Sorry about that.
Do you ever feel restricted by language
and that you must represent your theory of mind
or any theory of language?
No, I don't.
I once was jealous when Papert explained that he got some idea
and he explained he gets ideas like that when he thinks in French.
You draw pictures, too.
What?
You draw pictures, too.
Oh, yeah.
So it's not just language.
That's a language.
Graphics.
So we ought to have devices within the next few years
that draw pictures when you think.
It's so funny.
You know, we had cyborgs.
What were they called?
I mentioned them last time.
We had Steve Mann and who's the other one?
Yeah.
There's two guys around the Media Lab
wearing various things on their head
and they're always typing and you ask them a question
and they've searched Google.
And when was that?
1990?
But it's all gone.
Nobody walks around with direct connection to the web.
Yes.
Anyway, I certainly expected it to turn up and something wrong.
So anyway, you should be able to buy one one of these days.
And what's your name?
Who is that nice woman who had the EEG thing?
Do you remember a Ted?
Forgot her name.
Anyway, she had this sort of helmet which had about 20 electrodes
and she induced me to put it on.
I was on a stage with about a thousand people there,
which was rather funny.
And there's a little spot on a CRT
and I get to think about it moving one way or the other
and rewarding it when it did the right thing
for only about half a minute and then I could steer it around.
So here was a nice primitive gadget
where you could sort of almost draw just by thinking this spot.
And then she started a company and hasn't sent me one.
Because maybe it was just beginners luck or something.
What?
Yes, Lee.
Did you find the company?
But you'd think there'd be lots of people wearing stuff
and with your keyboard.
Why don't we take a five minute break?
I don't know.
Well, I hate to interrupt
because I see ten different productive discussions.
Yes, I see a...
I saw quite a few apparently productive discussions.
Maybe that's what the class needs.
But anybody come to a conclusion?
Yeah.
Let's see if you can knock the wall down.
Yeah.
It's not a conclusion.
It's a question.
It's a thought experiment.
So if you had a black box that could sort of replace part of your brain
and let's say you could replace like five percent at a time
at what point if you assume that there's this self entity
at what point would you lose yourself?
Yes.
If you change, the question is are you...
How much do you have in common with the you of yesterday
as compared to when you graduated grade school?
So this question of...
The idea of identity is very, very fuzzy.
Yeah.
I read a science fiction novel by Robert Sawyer.
Any of you know of him?
Canadian writer.
And it has to do with somebody who has a fatal disease.
So he's going to die soon.
But the technology is around where you can make a duplicate of him.
And so he has a duplicate made.
And he is sent to the moon for some reasons I can't remember
which is a kind of nursing home for...
I think people who are enfeebled and do much better with one-seventh gravity
and so for some reason why anyway the living,
the original copy is sent to the moon and the substitute takes over.
But then our hero is miraculously cured by eating the right stem cells
or what I don't remember.
So he wants to come back and the question is who gets the car?
So I can't remember the title of the novel except that it has alien in it.
Alienable rights is not something like that.
I wrote an article called Alienable Rights.
Anyway, so are you the same as you were five minutes ago or five years ago or whatever?
And as far as I'm concerned, the answer is who cares?
It's a sort of silly question because no two things are exactly the same ever anyway.
But again, a lot of these questions which look philosophical are legal.
So the joke of that novel is that who owns the car is what matters
to decide who is the real original and who's the copy.
Frederick Paul wrote a similar story much longer ago where people are copied
and the copy is sent on a one-way trip to some planet to fix a broken reactor
and they always die and you get a million dollars for providing this copy.
But one of the copies survives so it's the same plot.
I can't remember that.
If you're looking for a good idea, if you go to 1950s science fiction,
look for A.E. Van Voter, Frederick Paul or all those wonderful writers.
That was before it was necessary to describe really good characters
because science fiction got better and better for the literary critics
and generally worse and worse for the science fiction fans.
Do we really have any more questions?
Yes?
Before this time age where sharing information between many people
on a short amount of time is quite easy.
Do you think that this will change?
This will bring up many more ideas.
Do you think that this will hinder?
Because before this time people had problems with sharing information
and also now it's challenging to have a large group of people working on one thing.
Do you think that this will change the way that we think
and do you think that this will make us better?
It's a tough one. Bad things can happen and good things can happen.
That's funny because that reminds me again of science fiction
because in science fiction many, many years ago
some writers got the idea that there would be something like an internet
and some people realized there would be flash crowds
and now there are flash crowds
and I remember even as a kid talking to people who said
why not wire up the voting machines so that they're always there?
Somebody in the government wants to know
should we do this or that?
Should we bomb China or not?
You could get 100 million people to run up to the keyboard
and say yes or no
and presumably when the, what do they call those
that great crowd of Jeffersons and Franklins and the Founding Fathers
did a lot of things to prevent that
and the one that they focused on
which was one of the most effective was called the Electoral College
and the United States is different from other places
because we don't elect congressmen or presidents
we elect smart people from the community
who then get together and decide who should be president
and of course now if anybody, now they belong to parties
and if any of them voted for the other party's candidate
they would be held to pay
but it was a great idea
because the Founding Fathers realized
that if you had instant feedback which is what Hitler got
then you could say something really exciting
and everybody would press the yes button
and then you kill all the Jews
and then the next speech you kill all the black people
and all the yellow people
and all the people whose last name doesn't begin with M
and so what you don't want is instant feedback
now the new social networks are getting us close to that
and the question is, is it time to have...
is it time to stop that?
is it getting dangerous?
I don't know
but there must be a lot of people who are recognizing
that this thing is creeping up on us
and you might be able to get 50 million people
to do something reckless in a few minutes
if you don't put some limits
I don't think we could get the electoral college back
because you'd have to get a majority to...
what does it take to fix the Constitution?
Two thirds?
We'll never see two thirds again
It's the end of America
Well, we have three minutes
Yes?
If you were to design a direction for the field of psychology
obviously one would have just said it's a set of debugging tools
so how would you say they should be doing it?
They should read Patrick Winston's thesis
The psychologists now have disappeared into the tar pit of statistics
and they don't have the idea that knowledge needs complicated representations
and I don't care whether you assign probabilities to them
or put them in the order in which you thought of them
or do what Doug Lenet did in his thesis of swapping things
when one worked better than another
but I forget the question
but I think we've got to get better ideas about representation of knowledge
and I don't know where they're going to come from now
The whole AI community is drifting into these ways of avoiding representations
I haven't read the Norvig-Russell book
because he...
can anybody summarize what it says about knowledge representation?
Who's read it?
There's a chapter on logic for sort of logic
That's so funny
First order of logic is what Newell and Simon thought of in 1956
before they thought of the so-called GPS thing
Logic can't make analogies
It's a very bad thing to get stuck with
Zero or one
Maybe one of our papers should be on what should AI do next year
So really what my main concern it has been for quite a few years
is to make some theory of how...
what makes people able to solve so many kinds of problems
I guess if you ran through the spectrum of all the animals
you'd find lots of problems that some animals can solve and people can't
like how many of you could build a beaver dam
or a termite nest
So there are all sorts of things that evolution manages to produce
but maybe the most impressive one is what the human infant can do
just by hanging around for 10 or 20 or 30 years
and watching what other humans can do
So we can solve all sorts of problems
and my quarrel with the rest of the...
most of the artificial intelligence community has been that
the great success of science in the last 500 years
really has been in physics
and it's been rewarded by finding little sets of rules
like Newton's three laws and Maxwell's four laws
and Einstein's one law or two
that explained a huge range of everyday phenomena
Of course in the 1920s and 30s that apple cart got upset
actually Einstein himself who had discovered the first quantum phenomena
namely the quantization of photons
had produced various scientific laboratory observations
that were inexplicable in terms of either Maxwell or Newton
or Einstein's earlier formulations
So my picture of the history is that in the 19th century
and a little bit earlier going back to Locke and Spinoza and Hume
and a few of those philosophers, even Emmanuel Kant
they had some pretty good psychological ideas
and as I mentioned the other day
I suspect that Aristotle was more like a modern cognitive psychologist
and had even better ideas
but we've probably lost a lot of them because there are no tape recorders
Who knows what Aristotle and Plato said that their students didn't write down
because it sounded silly
So the idea that we developed around here
mostly Seymour Papert and a lot of students
Pat Winston was one of the great stars of that period
was the idea that to get anything like human intellectual abilities
you're going to have to have all sorts of high level representations
so one has to say the old conditioned reflex of stimulus
versus stimulus producing a response isn't good enough
the stimulus has to be represented by some kind of semantic structure
somewhere in the brain or mind
and so far as I know it's only in the theories of
not even modern artificial intelligence
but the AI of the 60s and 70s and 80s
that people thought about what could be the internal representation
of the kinds of things that we think about
and even more important
if one of those representations you see something or you remember some incident
and your brain represents it in some way
and if that way doesn't work you take a breath
and you sort of stumble around and find another way to represent it
maybe when the original event first happened
you represented it in three or four ways
and so we're beginning to see
did anybody here for Ruchi's talk
the Watson guy was up here
a couple of days ago I missed it
but they haven't made a technical publication as far as I know of how this Watson program works
but it sounds like it's something of an interesting society of mind-like structure
and it would be nice if anybody read any long paper on it
there have been a lot of press reports
have you seen anything Pat?
so anyway they seem to have done some sorts of common sense reasoning
as I said the other day
it's not that Watson could understand why you can pull something with a string
but you can't push
and actually I don't know if any existing program can understand that yet
I saw some amazing demonstrations yesterday by
or Monday by
Steve Wolfram of his Wolfram Alpha
which doesn't do much common sense reasoning
but it has what it does do is if you put in a sentence
it finds five or ten different representations
anything you can find that's sort of mathematical
and so when you ask it a question it gives you ten answers
and it's much better than previous systems because it doesn't
well Google gives you a quarter million answers but that's too many
anyway
I'm just going to talk a little bit more and just
everybody should be trying to think of a question that the rest of the class might answer
so there are lots of different kinds of problems that people can solve
going back to the first one like which moving object out there is my mother
and which might be a potential threat
so there are a lot of kinds of problems that we solve
and I've never seen any discussion in psychology books
of what are the activities principle activities of common sense thinking
somehow
they don't have or people don't before computers
there really wasn't any way to think about high level thinking
because there weren't any technical
technically usable ways to describe complicated processes
the idea of a conditional expression
was barely on the threshold of psychology
so what kinds of problems do we have
and if you take some particular problem like
I find these days I can't get the top off bottles
so how do I solve that and there are lots of answers
one is you look for somebody who looks really strong
or you reach into your pocket and
you probably have one of these
and so on
there must be some way to put it on the floor and step on it
and kick it with the other foot
so there are lots of problems that we're facing every day
and if you look in traditional cognitive psychology
well what's the worst theory
the worst and the best theory got popular in the 1980s
it was called rule-based systems
and you just have a big library
which says if you have a soda bottle
and you can't get the cap off
then do this or that or the other
and so some people decided well that's really all you need
Rod Brooks in the 1980s
sort of said we don't need those fancy theories
that people like Minsky and Papert and Winston are working on
why not just say for each situation in the outer world
have a rule that says how to deal with that situation
let's make a hierarchy of them
and he described a system that sort of looked like
the priority interrupt system in a computer
and he won all sorts of prizes
for this really bad idea that's spread around the world
but it solved a lot of problems
there are things about priority interrupt that aren't obvious
like suppose you have in the first computers
there was some problem because what should you do
if there are several signals coming into the computer
and you want to respond to them
and some of the signals are very fast
and very short
then you might think
well I should give the highest priority to the signal
that's going to be there the shortest time
something like that
the funny part is that when you made such a system
the result was that if you had a computer
that was responding to some signal
that's coming in at a
I'm talking about the days when computers
were only working at a few kilohertz
a few thousand operations a second
God that's slow
a billion times shorter than what you have in your pocket
and if you give priority to the signals
that have to be reacted to very fast
then what happens if you type to those computers
it would never see them because it's always
I saw this happening once
and finally somebody realized that
you should give the highest priority to the
to the inputs that come in most
least frequently
because there's always
otherwise if there's something coming in very frequently
you'll just always be responding to it
any of you run into this
it took me a while to figure out why
anyway there are lots of kinds of problems
and
the other day I was complaining that
we didn't have enough ways to just
we had hundreds of words for emotions
and
here's a couple of dozen
there in chapters seven and eight actually
most of these
so here's a bunch of words for describing ways to think
but they're not very technical
so you can talk about remorse and sorrow and blah blah blah
hundreds and hundreds of words for feelings
and it's a lot of effort to find a dozen words for
for intellectual for
what should I call them problem solving processes
so it's curious to me that
the great field called cognitive psychology has
not focused in that direction
anyway here's about twenty or thirty of them
you'll find them scattered through chapters seven and eight
here's my favorite one and I don't know of any
proper name for it but
if you're trying to solve a problem and you're stuck
and the example that comes to my mind is
if I'm trying to remember someone's name
I can tell when it's hopeless and
the reason is
that for somehow or other
I know that there's a huge tree of choices
that's one way to represent what's going on
and you know I might know that
I'm sure that that letter, that name has a Z in it
so you search around and try everything you can
but of course it doesn't have a Z
so the way to solve that problem is to give up
and then a couple of minutes later
the name occurs to you and you have no idea how it happened
so forth
so anyway the long story is that
Papert and I and lots of really great students
in the sixties and seventies
spent a lot of time making little models
of problem solvers that didn't work
and we discovered that you needed something else
and we put that in
other people would come and say
that's hopeless
you're putting in more things than you need
and my conclusion is that
wow it's the opposite of physics
and physics you're always trying to find
you don't want to, what is it called, Occam's razor
never have more structure than you need
because what?
well it'll waste your time
but my feeling was I never have less than you'll need
but you don't know how many you'll need
so what I did, I had four of these
and then I forced myself to put in two more
and people ask what's the difference
between self-models and self-conscious processes
and I don't care
or what's the difference between self-conscious and reflective
I don't care
and the reason is that
wow it's nice to have a box that isn't full yet
so if you find something that your previous theory
going back to Brooks
he was so successful getting simple robots to work
that he concluded that the things didn't need
any internal representations at all
and for some mysterious reason
the artificial intelligence society gave him their annual big prize
for this very wrong idea
and it caused AI research to sort of half collapse
in places like Japan
and said oh rule-based systems is all we need
anybody want to defend him?
the odd thing is if you talk to Brooks
he's one of the best philosophers you'll ever meet
and he says oh yes of course that's wrong
but it helps people do research and get things done
and as I think I mentioned the other day
when the Three Mile Island thing happened
there was no way to get into the reactor
that was 1980
and 30 years later when the
how do you pronounce it? Fukushima
accident happened
there was no robot that could go in and open a door
and
I don't know who to blame for that
maybe us
but my picture of the history
is that the places that did research on robotics
there were quite a few places
and for example Carnegie Mellon
was very impressive in getting the Sony dogs to play soccer
and they're still at it
and I think I mentioned that Sony
still has a stock of what's it called?
Fibos
Say it again? Fibos?
Fibos, A-I-B-O
Right, I-Bos
but the trouble is they're always broken
and we had a...
there was a robot here called Cog that Brooks made
and it sometimes worked
but usually it wasn't working
and so only one student at a time
could experiment with the robot
what was that wonderful project of trying to make a walking machine?
for four years in...
there was a project to make a robot walk
and there was only one of it
so first only one student at a time can do research on it
and most of the time it's something's broken
and you're fixing it
and so you end up that
you sort of get five or ten hours a week
on your laboratory physical robot
at the same time
Ed Fredkin had a student who tried to make a walking robot
and it was a stick figure on the screen
and I forgot the student's name
but anyway
he simulated gravity and a few other things
and in a couple of weeks he had a pretty good robot
that could walk and go around turns and bank
and if you put...
if you simulated an oily floor it could slip and fall
which we considered the high point of the demo actually
so they're...
at least fine
so anyway I've sort of asked you to read my two books
for this course
but those are not the only good texts about artificial intelligence
and if you want to dig deeper
it might be a good idea to
go to the web and type in Aaron Sloman
S-L-O-M-A-N
and you'll get to his website which is something like that
and Sloman is a sort of
philosopher who can program
there are a handful of them in the world
and he has lots of interesting ideas
that nobody's gotten to carry out
and so I recommend
who else is...
Pat, do you ever recommend anyone else?
No
What?
I'm trying to think
I mean if you're looking for philosophers
Dan Dennett has a lot of ideas
but Sloman is the only person I'd say
is a sort of real professional philosopher
who tries to program at least some of his ideas
and he has successful students who have made larger systems work
so if you get tired of me and you ought to
then go look at this guy and see who he recommends
So okay, who has a good question to ask?
We're talking about how we have a lot of words for emotions
Why do we only have one word for cause?
It's a mystery but
I spent most of the couple of days
making this list bigger
but these aren't, you know
these are things that you do when you're thinking
you make analogies
if you have multiple goals
you try to pick the most important one
or in some cases
if you have several goals
maybe you should try to achieve the easiest one
and there's a chance that it'll lead you
what to do about the harder ones
but a lot of people think that
mostly in England
that logic is a good way to do reasoning
and that's completely wrong
because in logic
first of all you can't do analogies at all
except at a very high level
four or five nested quantifiers
to say A is to B and C is to which of the following five
so I've never seen anyone do
analogical thinking
using formal logic, first order
or higher order predicate calculus
What's logic good for?
It's great after you've solved a problem
because then you can formalize what you did
and see if some of the things you did weren't necessary
in other words after you've got the solution to a problem
which you got by going through a big search
you finally found a path from A to Z
and now you can see if the assumptions that you had to make
to bridge all these various little gaps
were all essential or not
Yes?
What example would you say that logic can do analogies?
Like water is for bottomless containment
or like why do you have to do high light on C?
Well, because you have to make a list of hypotheses
and then let me see if I can find Evans
The trouble is
darn, Evans name is in a picture
and word can't look inside its pictures
Can PowerPoint find words in its illustrations?
Why don't I use PowerPoint?
Because I've discovered that PowerPoint can't read
pictures made by other programs in the Microsoft Word Suite
The drawing program in Word is pretty good
and then there's an operation in Word
which will make a PowerPoint out of what you drew
and it's 25 years
since Microsoft hasn't fixed the fatal errors
that it makes when you do that
In other words, I don't think that the PowerPoint
and Word people communicate
and they both make a lot of money
so it might be the reason
Where was I?
Why a logic can't do analogies?
Well, you can do anything in logic if you try hard enough
but A is to B
as C is to X is a four-part relation
and you'd need a whole pile of quantifiers
and how would you know what to do next?
Yes?
I've talked a bit about the situation
in which we are able to perform some sort of action
really fluently and really well
but we cannot describe what we're doing
and the example I give is say
I'm an expert African drummer from Africa
and I can make these really complicated rhythms
but if you ask me what did you just do
I have no idea how to describe it
and in that case, do you think the person is capable of
or I guess do you think the person
we can say that the person understands this
even though they cannot explain it?
Well, but if you take an extreme form of that
you can't explain why you used
any particular word for anything
there's no reason
it's remarkable how well people can do in everyday life
to tell people how they got an idea
but when you look at it
it doesn't say how you would program a machine to do it
so there's something very peculiar about
the idea that
it goes back to this
this idea that people have free will and so forth
suppose I say
look at this and say
this has a constriction at this point
why did I say constriction?
how do you decide what word to use for something?
you have no idea
so it's a very general question
it's not clear that
the different parts of the
the frontal lobes which might have something to do with
making plans and analyzing
certain kinds of situations
have any access to what happens in the Broca
or what's the
what's the speech production area?
it is Broca
Broca and
I'm trying to find the name of the other one
it's connected by a cable that's
about a quarter inch thick
Bernanke, yeah
we have no idea how those work
as far as I've never seen
any publication in
neuroscience that says here's a theory of
what happens in Bernanke's area
have any of you ever seen one?
what do those people think about?
but they'll tell you about
I was reading something which said
it's going to be very hard to understand these areas
because each neuron is connected to a hundred thousand little fibers
well some of them are
and I bet they don't do much except sort of
set the bias for some large
collection of other neurons
but
if you ask somebody
how did you think of such a word?
they will tell you some story or anecdote
but they won't be able to describe some sort of procedure
say in terms of a language like Lisp
and say I const this and that
and I took the cutter of this and the car of that
and I put them in this register
and then I swapped that with
you don't see theories of how the mind works
in psychology today
the only parts are they know a little bit about
some aspects of vision because you can track
the paths of images from the retina
to the primary visual cortex
and people have been able to figure out
what some of those cortical columns do
and
if you go back to an animal like the frog
then researchers like bitsy and others
have figured out how the equivalent of the cerebellum
in the frog
they've got almost the whole circuit of how
when the frog sees a fly it manages to turn
its head that way and stick its tongue out
and catch it but in the case of a human
I've never seen any theory of how any person
thinks of anything
so there's artificial intelligence
which has high level theories of semantic representations
and there's neuroscience
which has good theories of
some parts of locomotion and some parts of sensory systems
and
to this day there's nothing much in between
so
David here has decided to go from
one to the other and
former student of mine Bob Hearn
has done a little bit on both and
there are 20 or 30 people around the country
who are trying to bridge the gap between
symbolic artificial intelligence and
mappings of the nervous system
but it's very rare and
I don't know who you could ask to get support
to work on a problem like that for five years
yeah
so presumably to build a human-like artificial intelligence
we need to perfectly model
our own intelligence which means that
we are the system that we're trying to understand
well it doesn't have to be
I mean people are different
and
typical person looks like they have
400 different brain centers
doing slightly different things or very different things
and we have these examples
in many cases if you lose a lot of your brain
you're very badly damaged
and in other cases
you recover and become
just about as smart as you were
probably a few cases where you got rid of something
that was holding you back but
it's hard to prove that
we don't need a theory of
how people work yet
and the nice thing about AI is that
we could eventually get models
which are pretty good at solving
what people call everyday common sense problems
and probably in many respects
they're not the way the human mind works
it doesn't matter but once you've got
if I had a program which was pretty good
at understanding why you can pull with a string
but not push
then there's a fair chance you could say well
that seems to resemble what people do
I'll do a few psychological experiments
and see what's wrong with that theory
and how to change it
so at some point there'll be people making
and comparing them to
particular people and trying to make them fit
the trouble is nowadays it takes a few months
if you get a really good new idea
to program it
I think there's something wrong with programming languages
and what we need is
we need a programming language
where the instructions
describe goals and then sub-goals
and then finally you might say well let's represent
this concept by a number or a semantic
network of some sort
but yes
is there a goal oriented language
so there's kind of one
if you think about it, if you swim hard enough
something like SQL where you tell
in here I want to find
the top 10 people in my database
with this high value
and then you don't worry about how the system goes
in a sense that's where you're finding the goal
but you have to swim a little bit
what's it called?
SQL
it works as a database
I guess database query languages are on the track
but Wolfram Alpha seems
to be better than I thought
well he was running it
and he
Steve Wolfram was giving this demo
a meeting we were at on Monday
and he'd say well maybe
I'll just say this
and it always worked
so maybe either the language is better
than I thought or Wolfram is better
than I thought at something
remarkable guy
yes
well
well there's a lot of nice questions about
things like that
how many processes can you run at once
in your brain and
I was having a sort of argument
the other day about
music
and
I was wondering if
I see a big difference between Bach
and the composers
who do counterpoint
counterpoint you usually have several
versions of a very similar idea
maybe there's one theme and you have it playing
and then another voice comes in and it has that
theme upside down or a variation of it
or in some cases exactly the same
and then it's called a canon
so the tour de force
of music is when you have
two or three or four versions
of the same thought going on at once
in different times and my feeling was that
in popular music or
if you take a typical band
then there might be four people
and they're doing different things at the same time
usually not the same
musical tunes but
there's a rhythm and there's
a timpani and there's various instruments doing
different things but you don't have several doing
the same thing I might be wrong and
somebody said well some
popular music has a lot of counterpoint
I'm just not familiar with it but I think
that's if you're trying to solve a hard
problem it's fairly easy to look
at the problem in several different ways
but what's hard is to look at in several almost
the same ways that are slightly different
because probably if you believe
that the brain is made of agents or
resources or whatever you probably don't
have duplicate copies of ones that do
important things because that would take up too much real
anyway I might be completely wrong about jazz
somebody
maybe they have
just as complicated overlapping things
as Bach and
the contrapuntal composers did but
yeah
oh everyone has different goals
or ones
I think we're going to need it
because
the disaster that we're working our
way toward is that people are going to live longer
and they'll become slightly less able
and so we'll have
billions of 200 year old people
who can barely get around
and there won't be enough
people to import from underdeveloped
countries to or they won't be
able to afford them so we're going to have to have
machines that take care of us
of course that's just a transient because at some point
then you'll download your brain into a machine and fix
everything that's wrong so we'll need robots for a few
hundred years or a few decades
and then we'll be them and we won't need them anymore
but it's an important problem what's going to happen
in the next hundred years
you're going to have
20 billion 200 year olds and nobody
to take care of them unless we get AI
nobody seems particularly sad about that
another anecdote
I was once giving a lecture and talking
about people living a long time
and nobody in the audience
seemed interested and I'd say well suppose
you could live 400 years and most of the people
then I asked what was the trouble and they said
wouldn't it be boring
so then I tried it again in a couple of other lectures
and
if you ask a bunch of scientists
how would you like to live 400 years
everyone says yay
and you ask them why and they say well I'm working
on a problem that I might not have time to solve
but if I had 400 years
I bet I could get somewhere on it
and the other people don't have any goal
that's my cold-blooded view of the
typical non-scientist
there's nothing for them to do
in the long run
who can think of what should people do
what's your goal
do you want to live 400 years
wow
must be scientists here
try it on some crowd and let me know what happens
are people really afraid
yeah
I think we've been differentiating factors whether or not your 400 years
is just going to be the repetition of 100 years
experience that will start to like take off
right
I've seen 30
issues of the big bang
and I don't look forward to the next one anymore
because they're getting to be all the same
although it's the only thing on TV that has
scientists
seriously I hardly read anything
except journals and science fiction
because
what's the motivation to have robots
take care of us as we age
as opposed to enhancing our own cognitive abilities
or our own prosthetic body
or something or cyborg
what
I can't enjoy living if you can't do anything
I can't think of any advantages except that
medicine isn't getting
you know the age of
unhandy cap people
went up one year every four
since the late 1940s
so lifespan is so that's 60 years
so people are living 15 years longer on the average
than they did when I was born
or even more than that
but it's leveled off lately
now I suspect that you only have to fix a dozen genes
or who knows nobody
really has a good estimate
but you can probably double the lifespan if you could fix
nobody knows
but maybe there's just a dozen processes that
would fix a lot of things and then you could live longer
and have a lot of material rating
and lots of people might get bored
but
they'll self-select
I don't know
what's your answer
I feel that
creating
I feel that AI is more
I think the goal is not to help take care of people
but to complement what we already have
to entertain us
you could also look at them as our descendants
we will have them replace us
and
just as a lot of people
consider their children to be
the next generation of them
and I know a lot of people who don't
so it's not a universal
but
what's the point of anything
I don't want to get in
we might be the only
intelligent life in the universe
and in that case
it's very important that we
solve all our problems and make sure
that something intelligent persists
I think Carl Sagan had some argument of that sort
if you were sure
that there were lots of others
then it wouldn't seem so important
who's the new Carl Sagan
who's the new Carl Sagan
who's the new
is there a public scientist
who
he said that
it's not like NOVA all the time
oh, Tyson
Brian Green
he's very good
Tyson is the astrophysicist
Brian Green is a great actor
he's quite impressive
yeah
when we say a machine has a sense of self
we think there's something
like a self inside us
partly because there are some processes
that show that from now on
we seriously assign it to the self
but when we say a machine has a sense of self
well I think that's a funny question
because if we're programming it
we can make sure that the machine has
a very good abstract
but correct model of how it works
which people don't
so people have a sense of self
but it's only a sense of self and it's just plain wrong
in almost every respect
and so it's a really funny question
because
when you make a machine that really has
a good useful representation
of what it is and how it works
it might be quite different
have different attitudes than a person does
like it might not consider itself very valuable
and say oh I could make something
that's even better than me and jump into that
and so it wouldn't have the
it might not have any self protective
reaction because
then you don't want not to
whereas we're in a state where there's nothing much we can do
except try to keep living
and we don't have any alternative
stupid thing to say
I can't imagine getting tired of living
but lots of people do
yeah
I had a little section about that somewhere
that I wrote which was the difference
between artists and scientists or engineers
and
engineers are in
have a very nice situation because
they don't want you to watch
because somebody's ordered them to make a
in the last month
three times I've walked away
from my computer
and
how many of you have a Mac with the magnetic
thing
and three times I pulled it with
my computer and didn't break
and I've had Macs
for 20 odd years or since 1980
when did they start
30 years
and
they have the regular jack power supply in the old days
and I don't remember and usually when you
pull the cord it comes out here's this cord
and you leave jobs and everybody designed
very carefully so that when you pull it nothing bad would
happen
but it does
how do you account for that
it used to be better when the old plugs were
perpendicular to the plug and now it's kind of
well it's quite a wide angle
So it works at a certain angle, and the cable now, instead
of naturally applying it to that area,
actually naturally applies to the area where it doesn't hurt.
Well, what it needs is a little ramp
so that it would slide out.
I mean, it would only take a minute to file it down
so that it would slide out.
But they didn't.
I forget why I mentioned that, but.
I don't know.
Do you know what kind of engineers have created them?
Right.
So what's an engineer, an artist, an engineer?
Well, when you do a painting, it seems to me,
if you're already good at painting,
then 9 tenths of the problem is, what should I paint?
So you can think of an artist as 10% skill and 90%
trying to figure out what the problem is to solve.
Whereas for the engineer, somebody's
told him what to do, make a better cable connector.
And so he's going to spend 90% of his time actually
solving the problem, and only 10% of the time
trying to decide what problem to solve.
So I don't see any difference between artists and engineers,
except that the artist has more problems to solve
than it could possibly solve.
And usually ends up by picking a really dumb one,
like, let's have a saint in three angels.
Where will I put the third angel?
That's the engineering part.
Just improvising.
So to me, the Media Lab makes sense.
The artists, or semi-artists, and the scientists
are doing almost the same thing.
And if you look at the more arty people,
they're a little more concerned with human social relations
and this and that.
And others are more concerned with very technical,
specific aspects of signal processing,
or semantic representations, and so on.
But so I don't see much difference
between the arts and the sciences.
And then, of course, the great moments
are when you run into people like Leonardo and Michelangelo,
who get some idea that requires a great new technical
innovation that nobody has ever done.
And it's hard to separate them.
I think there's some place where Leonardo realizes
that the lens in the eye would mean
that the image is upside down on the retina,
and he couldn't stand that.
So there's a diagram.
He has where the corny is curved enough to invert the image,
and then the lens inverts it back again, which
is contrary to fact.
But he has a sketch showing that he was worried about
if the image were upside down on the retina,
wouldn't things look upside down?
Yeah?
I don't know if they have a question.
Did you ever learn about higher-ranked, higher-ranked
fault, temporal memory, and like temporal memory?
Temporal?
Temporal memory.
There is a system that I've had in different grades
that brought to you how our brain requires them.
And they have what you call a program,
and they're going to release this Hawkins at the end of this year
on date.
And there's some research.
They have paper on it.
Well, I'm not sure what.
This is Jeff Hawkins' project.
I don't know his name.
Yeah, it's Jeff Hawkins.
I haven't heard about 10 years ago he said Hawkins?
Yeah, Hawkins.
Yeah, well, he was talking about 10 years ago
how great it was, and I haven't heard a word of any progress.
Is there some?
Anybody heard it?
There's a couple of books about it,
but I've never seen any claim that it works.
They wrote a ferocious review of the Society of Mind,
which came out in 1986.
And the Hawkins group existed then
and had this talk about a hierarchical memory system.
But...
Do you have any idea?
As far as I can tell, it's all bluff.
Nothing happened.
I've never seen a report that they have a machine which solved the problem.
Let me know if you find one.
Well, Hawkins got really mad at me for pointing this out,
but I was really mad at him for having four of his assistants
write a bad book review of my book, so I hope we were even.
If anybody can find out whether...
I don't know what it's called.
Do you remember its name?
Do you know much of it?
Well, let's find out if it can do anything yet.
Hawkins is wealthy enough to support it for a long time,
so it should be good by now.
Yes?
Are you going to solve a problem?
People first start out with some sort of classification
in their head of the kind of problem it is,
or is that not necessary?
Yes.
That's...
Well, there's this huge book called Human Problem Solving,
which was...
I don't know how many of you know the names of Newell and Simon.
Originally it was Newell Shaw and Simon.
And believe it or not, in the late 1950s,
they did some of the first really productive AI research.
And then, I think, in 1970,
so that's sort of after 12 years of discovering interesting things,
their main discovery was the gadget that they called GPS,
which is not global positioning satellite,
but general problem solver.
And you can look it up in the index of my book,
and there's a sort of one or two-page description.
But if you ever get some spare time,
search the web for their early paper by Newell and Simon
on how GPS worked because it's really fascinating.
What it did is it looked at a problem and found some features of it
and then looked up in a table saying that
if there's this difference between what you have and what you want,
use such and such a method.
So it was sort of what I called it...
I renamed it a difference engine as a sort of joke
because the first computer in history was...
the one called the difference engine,
but it was for predicting tides and things.
Anyway, they did some beautiful work,
and there's this big book, which I think is about 1970,
called Human Problem Solving.
And what they did is got some people to solve problems,
and they trained the people to talk while they're solving the problem.
So some of them were little cryptograms like...
If each letter stands for a digit, I've forgotten it.
Pat, do you remember the name, one of those problems?
John plus Joe...
John plus Jane equals Robert or something.
I'm sure that has no solution,
but those are called crypt arithmetic,
and so they had dozens or hundreds of people
who would be trained to talk aloud while they're solving little puzzles like that.
And then what they did was look at exactly what the people said
and how long they took and, in some cases, where they moved their eyes.
They had an eye-tracking machine,
and then they wrote programs that showed how this guy
solved a couple of these crypt arithmetic problems.
Then they ran the program on a new one,
and in some rare cases it actually solved the other problem.
So this is a book which looks at human behavior
and makes a theory of what it's doing,
and the output is a rule-based system.
So it's not a very exciting theory,
but there had never been anything like it inside.
It was like Pavlov discovering conditioned reflexes for rats or dogs,
and Newell and Simon are discovering some rather higher level,
almost a Rodney Brooks-like system,
for how humans solve some problems that most people find pretty hard.
Anyway, what there hasn't been is much...
I don't know of any follow-up.
I've spent years perfecting those experiments and writing about...
...the results.
Anybody know anything like that?
What psychologists are trying to make real models of real people
solving toy problems?
It has a green light.
It has a green light, but the switch was on.
Boo.
Oh, it doesn't.
Yes?
It has inexplicable points at which the person suddenly gives up on that representation.
And he says, oh, well, I guess R must be 3.
Did I erase?
Yes, it's got episodes, and they can't account for these little jerks in the script
where the model changes.
And they announce those to be mysteries and say,
here's a place where the person has decided the strategy isn't working
and starts over or is changing something.
The amazing part is that their model sometimes fits what the person says
for 50 or even 100 steps.
The guy is saying, oh, I think Z must be 2 and P must be 7,
and that means P plus Z is 9, and I wonder what's 9.
And so their model fits for very long strings,
maybe two minutes of the person mumbling to themselves.
And then it breaks, and then there's another sequence.
So Newell actually spent more than a year after doing it verbally
at tracking the person's eye motions
and trying to correlate the person's eye motions with what the person was talking about.
And guess what? None.
It was almost as though you look at something and then to think about it, you look away.
Newell was quite distressed because he spent about a year crawling over this data
trying to figure out what kinds of mental events caused the eyes to change what they were looking at.
But when the problem got hard, you would look at a blank part of the thing
more often than the place where the problem turned up.
So conclusion, that didn't work.
When I was a very young student in college, I had a friend named Marcus Singer
who was trying to figure out how the nerve in the forelimb of a frog worked.
And so he was operating on tadpoles.
He spent about six weeks moving this sciatic nerve from the leg up to the arm of this tadpole.
And then they all got some fungus and died.
So I said, what are you going to do?
And he said, well, I guess I'll have to do it again.
And I switched from biology to mathematics.
But in fact, he discovered the growth hormone that he thought came from the nerve
and made that if you cut off the limb but of a tadpole, it'll grow another one and grow a whole...
It was a nude, I'm sorry, it's salamander.
It'll grow a new hand.
If you wait till it's got a substantial hand, it won't grow a new one.
But he discovered the hormone that makes it do that.
Yeah.
One of the questions from the homework that kind of relates to problem solving.
A common theme is having multiple ways to react to the same problem.
But how do we choose which options to add as possible reactions to the same problem?
So we have a whole lot of if, thens, and we have to choose which if.
I don't think I have a good theory of that.
Yes, if you have a huge rule-based system and they're...
What does Randy Davis do?
What if you have a rule-based system and a whole lot of rules fit, ifs fit the condition?
Do you just take the one that's most often worked?
Or if nothing seems to be working, do you...
You certainly don't want to keep trying the same one.
I think I mentioned Doug Lenet's rule.
Some people will assign probabilities to things, to behaviors,
and then pick the way to react in proportional to the probability that that thing has worked in the past.
And Doug Lenet thought of doing that, but instead he just put the things in a list.
And whenever a hypothesis worked better than another one,
he would raise it, push it toward the front of the list.
And then whenever there was a choice, it would pick...
If all the rules that fit, it would pick the one at the top of the list.
And if that didn't work, it would get demoted.
So that's when I became an anti-probability person.
That is, if just sorting the things on a list worked pretty well,
our probability is going to do much better.
No, because if you do probability matching, you're worse off than what?
Ray Solomonov discovered that if you have a set of probabilities that something will work,
and you have no memory, so that each time you come and try the...
I think I mentioned that the other day, but it's worth emphasizing,
because nobody in the world seems to know it.
Suppose you have a list of things, p equals this, or that, or that.
And in other words, suppose there's 100 boxes here,
and one of them has a gold brick in it, and the others don't.
So for each box, suppose the probability is 0.9,
that this one has the gold brick, and this one has 0.01, and this has 0.01.
Let's see how many of them...
So there's 10 of these. That makes...
Now what should you do?
Suppose you're allowed to keep choosing a box,
and you want to get your gold brick as soon as possible.
What's the smart thing to do?
But you have no memory.
Maybe the gold brick is decreasing in value, I don't care.
So should you keep trying 0.9 if you have no memory?
Of course not, because if you don't get it the first time, you'll never get it.
Whereas if you tried them at random each time, then you'd have 0.9 chance of getting it.
So in two trials, you'd have... What am I saying?
In 100 trials, you're pretty sure to get it, but in e100 trials, almost certain.
So if you don't have any memory, then probability matching is not a good idea.
Certainly picking the highest probability is not a good idea,
because if you don't get it the first trial, you'll never get it.
If you keep using the probabilities at... What am I saying?
Anyway, what do you think is the best thing to do?
It's to take the square roots of those probabilities and then divide them by the sum of the square roots.
So it adds up to 1.
So a lot of psychologists design experiments until they get the rat to match the probability.
And then they publish it.
Sort of like the...
But if the animal is optimal and doesn't have much memory,
then it shouldn't match the probability of the unknown.
It should end of story.
Every now and then, I search every few years to see if anybody has noticed this thing,
and I've never found it on the web.
So earlier in the course of the class, you mentioned that their rule-based methods didn't work
and that several other methods were trialed between 1680s.
Could you go into a bit about what these other methods were that at the time...
Well, I didn't... Don't mean to say they don't work.
Rule-based methods are great for some kinds of problems.
So most systems make money and, you know,
if you're trying to make hotel reservations and things,
this business of rule-based systems has a nice history.
A couple of AI researchers really, notably Ed Feigenbaum,
who was a student of Newell and Simon,
started a company for making rule-based systems.
And the company did pretty well for a while until...
And they maintained that only an expert in artificial intelligence could be really good at making rule-based systems.
And so they had a lot of customers and quite a bit of success for a year or two.
And then some people at Arthur D. Little said, oh, we can do that.
And they made some systems that worked fine and the market disappeared
because it turned out that you didn't have to be good at anything in particular to make rule-based systems work.
But for doing harder problems like translating from one language to another,
you really needed to have more structure and you couldn't just take the probabilities of words being in a sentence,
but you had to look for diagrams and trigrams and have some grammar theory and so forth.
But generally, if you have an ordinary data processing problem,
try a rule-based system first because if you understand what's going on,
it's a good chance you'll get things to work.
I'm sure that's what the Hawkins thing started out as.
I don't have any questions.
Can I ask another question for the homeworks?
Sure.
Computers and machines can use relatively few electronic components
to run a batch of different types of thought operations.
All that changes is data over which the operation runs.
In the credit selector model, are resources different bundles of data or different physical parts to brain?
Which model?
The credit selector model.
Oh, actually, I've never seen a large-scale theory of how the brain connects.
There doesn't seem to be a global model anywhere.
Anybody read any neuroscience books lately?
I mean, I just don't know of any big diagrams.
Here's this wonderful behavioral diagram.
So how many of you have run across the word ethology?
Just a few.
There's a branch of the psychology of animals, which is called ethology,
and it's the study of instinctive behavior.
So these, and the most famous people in that field who, well,
Tynbergen, Nico Tynbergen and Conrad Lorenz are the most famous.
I've just lost the name of the guy around the 1900 who wrote a lot about the behavior of ants.
Anybody bring a bell?
So he was sort of the first ethologist.
And these people don't study learning because it's hard to, I don't know why,
but so they're studying instinctive behavior,
which is what are the things that all fish do of a certain species.
And you get these big diagrams.
This is from a little book which you really should read called The Study of Instinct.
And it's a beautiful book, and if that's not enough,
then there's a two-volume similar book by Conrad Lorenz,
who was an Austrian researcher.
They did a lot of stuff together, these two people.
And it's full of diagrams showing the main behaviors that they were able to observe
of various low-cost animals.
I think I mentioned that I had some fish,
watched the fish tanks, what they were doing for a very long time,
and came to no conclusions at all.
And when I finally read Tinbergen and Lorenz,
I realized I just had never occurred to me to guess what to look for.
My favorite one was that whenever a fire engine went by,
Lorenz's sticklebacks, the male sticklebacks,
would go crazy and look for a female,
because when the female's in heat or whatever it's called,
estrus, the lower abdomen turns red.
I think fire engines have turned yellow recently,
and I don't know what the sticklebacks do about that.
So if you're just an AI, you really should look at at least one of these people,
because it's the first appearance of rule-based systems in great detail in psychology.
There weren't any computers yet.
There must be 20 questions left.
So I know that early on, people were kind of...
They were careful not to apply ethology of humans,
until about the 60s, Eva Wilson with sociobiology.
So I was wondering about your opinion on that,
and maybe you can have an anecdote stop the time,
or is it pretty controversial around this area, especially?
I don't know.
I sort of grew up with Ed Wilson
because we had the same fellowship at Harvard for three years,
but he was almost never there,
because he was out in the jungle in some little telephone booth
watching the birds or bees.
He also had a 26-year-old aunt, aunt, not aunt, aunt.
A-N-T.
I'm not sure what the controversy would have been,
but of course, there would be humanists who would say,
people aren't animals, but then what the devil are they?
Why aren't they better than they...
You've got to read this.
It's a fairly short book,
and you'll never see an animal as the same again,
because I swear you start to notice all these little things.
You're probably wrong,
but you start picking up little pieces of behavior
and trying to figure out what part of the instinct system is it.
Lorenz was particularly...
I think in chapter two of the Emotion Machine,
I have some quotes from these guys,
and Lorenz was particularly interested in how animals got attached to their parents.
That is, for those animals that do get attached to their parents.
Alligator babies live in the alligator's mouth for quite a while.
It's a good, safe place.
And Lorenz would catch birds just when they're hatching,
and within the first day or so,
the baby birds get attached to whatever large moving object is nearby.
And that was often Conrad Lorenz,
rather than the bird's mother,
who's supposed to be sitting on the egg when it hatches,
and the bird gets attached to the mother.
Most birds do because they have to stay around and get fed.
So it is said that wherever Lorenz went in Vienna,
there were some ducks or whatever birds that had gotten imprinted on him
would come out of the sky and land on his shoulder,
and on no one else,
and he has various theories of how they recognize him.
But you could do that too.
Anyway, that was quite a field, this thing called ethology,
and between 1920 and 1950, 1930, I guess, 1950,
there were lots of people studying the behavior of animals,
and Ed Wilson is probably the most well-known successor to Lorenz and Tinbergen.
And I think he just wrote a book.
Has anybody seen it?
He has a huge book called Sociobiology, which is too heavy to read.
I've run out of things.
Yes?
If we were to decide if my ideas in that book had machinery from it,
what would the initial state of the machine be if you were to start something?
Is that if they could have goals given to it?
By state, I mean the different agents, the resources they have access to.
What would that initial state look like?
He's asking if you made a model of the program to Society of Mind Architecture,
what would you put in it to start with?
I never thought about that.
Great question.
I guess it depends whether you want it to be a person or a marmoset or chicken or something.
Are there some animals that don't learn anything?
Must be.
What are the ones at Sidney Brenner's study?
The little worms?
There was a rumor that if you fed them RNA,
was it them or was it some slightly higher animal?
It was worms.
What?
There was one that if you taught a worm to turn left when there was a bright light or right
and put some of its RNA into another worm,
that worm would copy that reaction even though it hadn't been trained.
And this was...
Is that what it said? Slugs?
Slugs.
Yes.
It's a little snail-like thing.
And nobody was ever able to replicate it.
That rumor spread around the world quite happily and...
There was a great science fiction story trying to remember in which somebody got to eat
some of an alien's RNA and got magical powers.
I think it's Larry Niven who is wonderful at taking little scientific ideas
and making a novel out of them.
And his wife Marilyn was an undergraduate here.
So she introduced me to Larry Niven and I once got to write an article.
I once gave a lecture and he wrote it up.
It was one of the big thrills because Niven is one of my heroes.
Imagine writing a book with a good idea in every paragraph.
Bernie Vingy and Larry Niven and Frederick Poe seem to be able to do that.
Or at least on every page.
I don't know about every paragraph.
Yeah.
To follow up on that question, it seems to me that you almost were saying that
if this machinery exists, the difference between these sort of animals would be the stars.
Well, no, I don't think that...
I don't think that most animals have scripts.
Some might, but I'd say that...
I don't know where most animals are, but I sort of make these six levels
and I'd say that none of the animals have this top self-reflective layer
except for all we know, dolphins and chimpanzees and whatever.
It would be nice to know more about octopuses
because they do so much of wonderful things with their eight legs.
What kind of...
How does it manage?
Have you seen pictures of an octopus picking up a shell
and walking to some quiet place?
There's some movies of this on the web.
And then it drops the shell and climbs under it and disappears.
It's hard to imagine programming a robot to do that.
Yeah.
So I've noticed both your books in the lecture,
a lot of your models and diagrams seem to have very hierarchical structures in them.
But as you mentioned in your book, other places,
passing between levels, feedback and self-reference are all very important to diligence.
So I'm curious if you could discuss some of the uses of these very hierarchical models
and why you've represented so many things in that way and some of the limitations there.
Well, it's probably very hard to debug things that aren't.
So we need a sort of meta theory.
One thing is that, for example, it looks like that all neurons are almost the same.
Now, there's lots of difference in geometric features of them,
but they all use the same one or two transmitters.
Every now and then you run across people saying,
oh, neurons are incredibly complicated.
They have 100,000 connections.
You can find it if you just look up Neuron on the web and get these essays,
explaining that nobody will ever understand them,
because typically a neuron is connected to 100,000 others and blah, blah, blah.
So it must be something inside the neuron that figures out all this stuff.
As far as I can see, it looks almost the opposite.
Namely, probably the neuron hasn't changed for half a billion years very much,
except in sort of superficial ways in which it grows,
because if you changed any of the genes controlling its metabolism
or the way it propagates impulses,
then the animal would die before it was born.
And so you can't make...
That's why the embryology of all mammals is almost identical.
You can't make a change at that level after the first...
You can't make changes before the first generations of cell divisions,
or everything would be clobbered.
The architecture would be all screwed up.
So I suspect that the people who say,
well, maybe the important memories of a neuron are inside it,
because there's so many fibers and things.
I bet it's sort of like saying the important memory in a computer
is in the arsenic and phosphorus atoms of the semiconductor.
So I think things have to be hierarchical in evolution,
because if you're building later stuff on earlier stuff,
then it's very hard to make any changes in the earlier stuff.
So as far as I know, the neurons in sea anemones
are almost identical to the neurons in mammals,
except for the later stages of growth
and the way the fibers ramify.
Who knows, but there are many people who want to find the secret of the brain
in what's inside the neurons rather than outside.
It'd be nice to get a textbook on neurology from 50 years in the future
to see how much of that stuff mattered.
Where are time machines?
Most systems have a state that they prefer to be in,
like a state that they're most comfortable in.
Do you think the mind has such a state,
or would it tend to certain places?
That's interesting.
How does that apply to living things?
I mean, this bottle would rather be here than here,
but I'm not sure what you mean.
Okay, so apparently in Professor Tannenbaum's class,
he shows this example of a number game.
He'll give you a sequence of numbers,
and he'll ask you to find a pattern in it.
So for example, if you had a pattern like 10, 40, 50, and 55,
he kind of asks the class to come up with different things
that could be describing the sequence.
And between the choice of, oh, this sequence is a sequence
of the multiples of 5 versus a sequence of the multiples of 10
or multiples of 11.
But he says something like, he phrases it like,
the multiples of 5 would have a higher private probability.
So that got me thinking, why would that be?
Would our minds have a preference for having as few categories
as possible in trying to view the world around us?
Like trying to categorize things in as few ways as possible
is what got me thinking about it.
Sounds very strange to me, but certainly if you're going
to generate hypotheses, you have to have the way you do it
depends on what does this problem remind you of.
So I don't see how you could make a general.
If you look at the history of psychology,
there are so many efforts to find three laws of motion like Newton's.
Is he trying to do that?
I mean, here you're talking about people with language
and high level semantics.
Let's ask him what he meant.
Yeah, this is more of a social question,
but there's always this debate about how if AI gets to a point
where it can take care of humans, will it ever destroy humanity?
And do you think that's something that we should fear?
And if so, is there some way we can prevent it?
If you judge by the recent, by what's happened in AI since 1980,
it's hard to imagine anything to fear.
But funny you should mention that I'm just trying to organize
a conference sometime next year about disasters.
And there's a nice book about disasters by, what's his name?
The Royal, The Astronomer Royal.
What?
Martin Rees.
Martin Rees.
So he has a nice book which I just ordered from Amazon
and it came the next day, and it has about ten disasters
like a big meteor coming and hitting the earth.
I forget the other ten, but I have it in here somewhere.
So I generated another list of ten to go with it.
So there are lots of bad things that could happen.
But I think right now that's not on the top of the list of disasters.
Eventually some hacker ought to be able to stop the net from working
because it's not very secure.
And while you're at it, you could probably knock out all of the navigation satellites
and maybe set off a few nuclear reactors.
But I don't think AI is the principal thing to worry about,
but it should very suddenly get to be a problem.
And there are lots of good science fiction stories.
My favorite is the Colossus series by DF Jones.
Anybody know?
There was a movie called The Forbidden Project.
And it's about somebody who builds an AI and it's trained to do some learning.
And it's also the early days of the web and it starts talking to another computer in Russia.
And suddenly it gets faster and faster and takes over all the computers in the world
and gets control of all the missiles because they're linked to the network.
And it says, I will destroy all the cities in the world unless you clear off some island
and start building the following machine.
I think it's Sardinia or someplace.
So they get bulldozers and it starts building another machine which it calls Colossus 2.
And they ask, what's it going to do?
And Colossus says, will you see?
I have detected that there's a really bad AI out in space and it's coming this way.
And I have to make myself smarter than it really quick.
Anyway, see if you can order the sequel to Colossus.
That's the second volume where the invader actually arrives and I forget what happens.
And then there's a third one which was an anticlimax because I guess D.F. Jones couldn't think of anything worse that could happen.
But Martin Rees can.
Yeah.
Going back to her question, an example, if my mind has a state, a recipient,
would that example be more of a pattern recognition example?
So instead of 10, 40, 50, 55, 100, it wasn't statistical.
Good, fine, great.
And you have to come up with a word that could potentially fit in that pattern.
And then that pattern could be ways to answer it. How are you?
Let's do an experiment.
How many of you have a resting state?
Sometimes when I have nothing else to do, I try to think of twinkle, twinkle, little star happening with the second one starting in the second measure.
And then the third one starts up the third measure.
And when that happens, I start losing the first one.
And ever since I was a baby, when I have nothing else to do, which is almost never,
I try to think of three versions of the same tune at once and usually fail.
What do you do when you have nothing else to do?
Any volunteers?
What's yours?
You try not to or to?
Not to.
Isn't that a sort of Buddhist thing?
Yes, sir.
Do you ever succeed? How do you get out of it?
You have to think, well, enough of this, nothingness.
If you succeeded, wouldn't you be dead?
We're stuck.
Eventually some stimulus will appear that is too interesting to ignore.
Right, and threshold goes down to even the most boring thing is fascinating.
Yeah.
Make a good short story.
Yeah.
There was actually a movie that really got to me when I was little.
These aliens were trying to infiltrate people's brains and like their thoughts.
And to keep the aliens from infiltrating your thoughts, you had to think of a wall,
which didn't make any sense at all, but now whenever I try to think of nothing,
I just end up thinking of a wall.
You're sick.
They're these awful psychoses and about every five years,
I get an email from someone who says that,
please help me, there's some people who are putting these terrible ideas in my head.
Have you ever gotten one, Pat?
And they're sort of scary because you realize that maybe the person will suddenly
figure out that it's you who's doing it.
If they...
I remember there was once...
One of them came to visit, actually showed up and he came to visit Norbert Wiener,
who is famous for, I mean, he's the cybernetics person of the world.
That was...
And this person came in and he got between Wiener and the door
and started explaining that somebody was putting dirty words in his head
and making the grass on their lawn die.
And he was sure it was someone in the government and this was getting pretty scary.
And I was near the door, so I went and got lethin'.
That's a true story because nearby and I got lethin' to come in
and lethin' actually took this guy down and took him by the arm and went somewhere
and I don't know what happened, but Wiener was really scared
because the guy kept keeping him from going out.
Lethin' was big.
Wiener is not very big.
Anyway, that keeps happening every few years.
I get one and I don't answer them.
He's probably sending it to several people
and I'm sure one of them is much better at it than we are.
How many of you have ever had to deal with an obsessed person?
How did they find you?
I don't know. They found a number of people in the Media Lab, actually.
Don't answer anything.
But if they actually come, then that's not clear what to do.
Last question.
Thanks for coming.

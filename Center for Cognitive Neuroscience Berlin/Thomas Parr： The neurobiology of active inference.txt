It's a great pleasure to introduce Thomas Pahr today, we'll brief you about his background.
So he started medical school in 2012, I guess, at the UCL and did also like a bachelor there
with a focus in new science. In 2016, he started his PhD, I hope it's everything is
correct, he started his PhD working, working together with Carl Friston and Garen Rees.
So I think Thomas is really like the person who most frustrated me. I was at the at the
so long time ago. I was fascinated by like the idea of the free entry principle and Carl
Friston and really started going learning about Boltzmann equations and some kind of
like message passing and so on and over years and years. And then in 2016-17, I realized that
suddenly a number of papers were coming out, it became much more accessible and on these papers
there were always like one name, Thomas Pahr. And so I looked into that and I realized that there's
one person out there who is so young and who plus started his PhD and was able to go all through
this extremely complicated math and ideas so easily that I thought I should really give up
thinking about the free energy principle at all. And more impressively, I think a couple of months
ago, even like a book came out like about like Active Influence, where he is like the first
author on and I really can recommend this book. It's a fantastic introduction
to Active Influence, to the free energy principle. So it's a really, really great
pleasure to have you here tonight, Thomas. And I'm really looking forward to your talk.
Thanks for joining. Well, thank you very much. The very nice invitation, the very nice
introduction as well. First of all, can you hear me okay? Yeah, yes. Okay. And next thing is to
see whether I can successfully share my screen. Let's try this. So I'm assuming you can see the
PowerPoint and now hopefully in full screen. Perfect. Great. So today, I realized that you've
already had a talk from Ryan Smith previously. So I'm assuming he's given an excellent overview
of Active Influences as he always does. And I wanted to take the opportunity to focus in on
something a little bit more specific in this talk, which is thinking about how dealing with
Active Influence lets us think about neurobiology and how we can start to make connections and form
neurobiological theories from the principles. And so I'll start just by giving an introduction
that I think will hopefully give everybody a little bit of a refresher on Active Influence
and the basic principles. And although there are some technical elements to it, I'll try to make
it as intuitive as possible. And it really doesn't matter if you get all the technical detail through
this, it's just to build some intuitions. And so I tend to like to start talks with this slide,
which just shows two different sorts of system. The one on the left is one that just
gradually diffuses over time, sort of loses form becomes progressively less interesting.
Whereas the one on the right, despite having the same amount of randomness built into it,
manages to maintain its form over time. And a useful starting point in thinking about Active
Influence is thinking about what the difference between these two sorts of systems are and
how the one on the right is able to maintain its form and resist this diffusive
the effect of these random fluctuations that are forcing it in all sorts of different directions.
And it's often useful to reformulate this in terms of in terms of the probability density
or the change in the probability distribution of all of these little particles
over time. So again, you can see on the left, we've got something that's just diffusing out
into nothing. And on the right, we get something that behaves from a probabilistic level,
pretty much statically. And it's really those on the right that we're interested in those
biological creatures are able to maintain their form are able to resist the effects of what the
environment does to them. Now, if we sort of write down the kinds of dynamics that we'd need for a
single particle or single part of the system to try and maintain this form, what we get to is a
system that I think quite intuitively always moves when it can from regions of low probability to
regions of high probability. So we interpret this final distribution it gets to as some steady state.
All we need to do to maintain that in a random environment is to always try and climb uphill
of the probability gradients. And then almost by definition, we end up spending more time in
highly probable states and less time in more improbable states. And this is sort of the starting
point really for active inference, because we now have this notion that we're climbing probability
gradients and we have some distribution that we're effectively trying to maximize. And we can link
that back to ideas like the Bayesian brain, the idea that the brain is using some model of the
world around it to generate predictions. And then is drawing inferences about the data that it
actually obtains, so sensory information coming in through our eyes, ears, through our skin,
that we can then use to draw inferences about the causes of those data.
So here we've got the idea that there's some states of the world X that are causing some sensory
data Y. We're then forming some posterior beliefs, so beliefs about the causes given the data.
And we're doing this using a generative model that comprises a likelihood and a prior. So prior
being how plausible are the things out there in the world before we've made any observations.
The likelihood being how likely the observations we've made are given the states of the world or
given our hypotheses about what caused them. And together we refer to these two things as
a generative model. Now the posterior distribution is what happens when we invert that model where we
find the probability of some causes given the data. And the final term we've got at the end here is
referred to either as an evidence or a marginal likelihood. And that's because in the context
of Bayesian statistics, we often use a marginal likelihood as a measure of the fit of a model
to the data that it's trying to explain. There's how much evidence do those data afford
some model of the world. And together we can think of these as being an inversion of the
generative model. Now the reason I've put this here is that we can see that the dynamics that I'm
showing in the upper left can be interpreted as the process of maximising the evidence for some model,
maximising the fit between the brain's model of how the world works and how the world
then engages with the model or engages with the brain by presenting it data.
And broadly there are two ways of doing that. The first is, as we've already spoken about,
is changing your beliefs based upon new data such that you get a better fit to the world.
The other way, which I think is one of the key ideas in Active Inference,
is that you generate actions based upon your beliefs that change the world to make it more
like your model. And this is sort of the key idea that underwrites Active Inference, that
all we're trying to do is maximise the evidence for some model of our world.
And we can either do that through perception by changing our model or through action by changing
the world. But together they come under one single objective, sometimes referred to,
particularly by people like Jacob Howey, as self-evidencing.
Now, I want to go a little bit into the structure of generative models and specifically the ways
we can think about and the ways we can notate generative models. Because it often helps to move
from the slightly more mathematical abstract description to a more graphical notation that I
think often gives a much better intuitive sense of what's going on. So to do that, I'm going to
start with this idea that model evidence is effectively what we get out once we've integrated
out all of the causes from our model. By which I mean, if you take account of all of the things our
model predicts, and then we take into account the prior probabilities of all of the things that are
causing those data, we can then work out what the probability of the data are under that model.
But the model itself takes account of the things that are being generated, so our sensory data,
and also the things that are causing them, simply a joint probability distribution
involving all of these things. So I'm now interpreting this graphic up on the left as
depending upon some generative model where we've effectively integrated out everything that we
don't need to determine explicitly. Now, generative models will often have some
interesting structure, and in fact, if they don't have interesting structures, then they're not
interesting generative models. And normally that structure manifests as a factorization of
this joint distribution, but not everything depends upon everything else. And so we can
often factorize it, and here I'm just showing a completely arbitrary factorization of some
generative model, where we have certain dependencies, so y depends directly upon x1 and x2, but not
directly upon x3 and so on. And the reason it's useful to think about this factorization is because
we can then express a graphical version of this model that provides a bit more intuition as to
what's going on. So the way we do that, or one way that we can do that, is using something
known as a factor graph. And the idea is that we take each of these factors in turn, and we draw a
square, and then we draw an arrow to whatever's on the left of the probability factor we're interested
in. So here an arrow towards the y, because we're saying the probability of y conditioned upon or
depending upon the other two things, and then we attach those to the same square. We then move
on to the next factor for another square, and we then attach the relevant variables together
in exactly the same way. And we carry on doing that until we have a picture of what our model is
like. So now instead of having to look at this equation, you can look at this and say, okay,
well x3 causes x1, x4 causes x2, and together x1 and x2 relate to or together generate our data
y. This is just to demonstrate several models that you may be familiar with
generally, and the fact that we're often implicitly using these kinds of generative models without
necessarily thinking about it. So when we're performing a principal components analysis,
we're often taking some distribution of some variable x, we're then mapping that to a higher
dimensional space y, and that's our sort of model of how the data that we're working with
generated. So what we can then do when we perform a principal components analysis is take our
high dimensional data y, work out how you go back from the y to the x, and that gives you all of
your specific principal components and the directions that have been stretched in various
different ways. Same principle applies for things like canonical various analysis where you've got
some set of variables then mapped to two different sorts of data by stretching them and distorting
them in various ways, and the challenge is how do you get back to the common hidden variable,
the common cause for both of those datasets. And we do the same sort of thing with something
like clustering analysis where here the model says we're effectively sampling from one of
several different clusters with different probabilities. We're then generating some
point in space depending upon which cluster it falls in, and the process of inference is again
undoing this, it's finding the posterior by taking a point in a cluster and saying well which
cluster is that from, so going from the y to the s. So that's a sort of brief introduction to the
idea of generative models, the the role they play in active inference, some key examples,
and some of the notation that I'm going to use as we go forward. What I've said so far has been
relatively abstract, and in the next section I want to try and work from those sort of general
principles and these slightly abstract notions through to something more specific and more
neurobiological. Now the first thing we're going to do to get there is to introduce one more abstract
concept, which is that of a Markov blanket. Now a Markov blanket comes up in all sorts of places
and active inference, but I'm going to use it in a very specific way here, and this is to talk about
conditional independence. And what I mean by conditional independence is that if you have
two different sets of variables, so here I've noted them as the mu and the eta,
the Markov blanket B is the set of variables that render the other two completely independent
to one another, that mean that if you know everything about the blanket, knowing something
about mu tells you nothing new about eta. To give you an example of this, one of the examples
that most people are familiar with is the idea of a Markov chain, where you have a sequence of
events in time, so something that happens in the past then influences the present, which then
influences the future, with no direct influence from the past to the future without going via the
present. So if I know everything there is to know about the present, knowing something about the past
tells me nothing new about the future and vice versa, so in that sense the present is the Markov
blanket that separates the past from the future. Now this is a very useful concept when we're dealing
with graphical models or generative models that have some interesting factorization structure,
and I'll try and explain why. The Markov blanket is often referred to as or can often be identified
by identifying variable we're interested in, so let's say we're interested in the variable x2,
then you find the parents of x2, so the things that caused it, the children of x2,
so the things it causes, and the parents of its children, and that gives you the Markov blanket
of x2. Now the significance of this is that knowing this means that if we know about the
blanket, it doesn't matter what else is going on in the generative model, you know this might be
some tiny section of a huge generative model with many many variables, but we don't need to know
about all of those variables, all we need to know about are the blanket variables, so we can
effectively ignore everything else if what we're interested in is x2. Now how does this then
matter for neurobiology? Well I think the answer to that is that the brain is an extremely sparse
structure, that synaptic message passing does not involve connections between every different
neuron in the brain to every other neuron in the brain, there are a small number of relatively
speaking of synapses between any neuron and its neighbors or neurons elsewhere in the brain,
and so we can make an argument that if the brain is performing inference about lots of different
variables, all it needs is to know the structure of the generative model and the variables that
sit in that Markov blanket, or at least the neurons that are representing those other variables,
and that tells us that we can then, when we're inverting that model, when we're performing
inference, what I'm showing here is just one example of an inference scheme, we can express the
dynamics of that neuronal message passing in terms of connections from the other neural
populations that are representing the variables in that Markov blanket. The details of this aren't
that important, but this is one example of an inference scheme known as variational message
passing, here formulated as a gradient scheme in terms of some dynamics, which has relevance for
modeling neural populations, we're interested in how the dynamics of those populations evolve over
time, and we can sort of do this recursively, we can couple together lots of different parts of
this system, where the things that are being connected up are just the Markov blankets of
different populations. Here I've expressed it in terms of an error term, this epsilon, which
effectively represents the gradient of a free energy objective, which is used as an approximation
to that marginal likelihood or evidence that I spoke about earlier, and then some beliefs about
or expectations about that variable, so we can now use the errors in our predictions to update
those variables, and once we've written down the dynamics of this sort of system, we can
simulate things like firing rates, we can simulate the dynamics and the behavior
of these networks of neurons passing messages between one another as a means of performing
a form of inference and thus self-evidencing. I know that this is probably still seeming
relatively abstract, but I think what will hopefully help is if we then go through some
examples thinking about specific kinds of generative model and how that might then affect
the kind of message passing we would see in a brain, and the first step to thinking about that,
I think is very important when we think about really useful generative models, is they will
all typically have a temporal aspect to them, that as biological creatures we deal with things
that evolve in time, and so as we're thinking about how do you formulate a generative model
that has that kind of dynamic aspect to it, and there are several different answers to that,
and one of them is we view something known as a Taylor series approximation,
so we start by saying okay at the current time what is the value of some variable x in the world,
and there's maybe some continuous variable, it may be where my arm is in space.
If we then want to know how it's going to evolve in time, we could then say well let's take the
next element of our Taylor series approximation that's taking out of its velocity, and that tells
us a little bit more about the trajectory that my arm might be on. We can then take another
value, which is the current acceleration of that position, and we get a slightly better
approximation to the trajectory, and the more terms we add in, the greater an approximation we
have of the trajectory or the greater a representation we have of the trajectory,
just as a series of numbers, where those numbers are my current position, my current velocity,
acceleration, etc. So we could formulate a generative model by trying to predict each of
these coefficients of this Taylor series, or each of these generalized coordinates of motion,
as they're sometimes referred to. An alternative is we just say at time one where will I be,
at time two where will I be, and we simply represent it in terms of the sequence of points over time.
Often when we're discretizing time in this way, we often we also discretize space, so we might say
okay, where am I in some discretized scheme? Am I in location one, two, three, four, or five
at each different time point? And so we have two different ways of accounting for how things might
evolve over time, and each of these can be useful in slightly different circumstances. So if I were
dealing with a sequential decision-making task, it may be much more efficient for me to say okay,
the first move I'm going to make is this, then the second one is this, the third one is this,
and dealing with something very sequential. That might also be important in in sort of language
processing, where you have to think which words you're going to put together in a sequence,
and we're dealing with discrete variables over time, because words are categorical
as opposed to lying on a continuum. However, if I were dealing with movement or the responses I'm
getting from some photoreceptor in my retina, it may be much more sensible for me to be working in
continuous time, thinking about how things evolve at a continuous scale. So what I'm showing here
is an example of a generative model and the associated message passing scheme,
when formulated in terms of this continuous time scheme. So what we've got here is a factor
graph at the top, this blue element, that uses exactly the same sort of formulation that we've
been discussing so far. So we have some variable here which might represent our position, which
then predicts our velocity at that time, which then predicts the acceleration at that time. So we have
a series of predictions here about the coupling between different orders of motion. At each point,
those are predicting some sort of data here or wise, which represents the current position of
our sensory data, the current velocity, the current acceleration. What I'm showing lower down here is
the message passing scheme we get when we take account of the Markov blankets of each of these
variables. So the Markov blanket here for the velocity would include the position and the acceleration,
but also the data I've got about those things and some prior beliefs about other variables in our
model. And so we'd end up connecting those things up. And although it looks like a bit of a mess of
connections, it's still fewer connections than the total number connections you would get if you
match everything to everything else. What we've got here takes the form of effectively a predictive
coding style scheme, which people may be familiar with, and the idea that you can hierarchically
predict some data and you can predict the thing that's predicting that data, those data, and by
observing the errors in your prediction of the data, you update your prediction, which then
cascades up a hierarchy allowing you to update subsequent predictions.
What I'm showing you here is the equivalent model formulated in terms of discrete time,
and in this case, discrete space as well. So here we have a state at time t minus one, which
generates a state at time t, which generates a state at time t plus one. At each of those time
points, we're generating some observable outcomes which are represented as the O's.
We have a variable pi here that represents alternative trajectories I could pursue,
so alternative policies or plans, alternative ways I could change the sequence of events.
And again, we have underneath the message passing scheme that could
perform inferences about this sort of model. And you can see that the message passing scheme
to some extent looks like an inversion or as loosely a sort of mirror image of the
generative model because all it's doing is taking each step that was performed to generate some
data and then inverting those steps one at a time. So it's just going backwards from the data
to arrive at the causes of those data. Now, what I've got shown here on the left
is just a sort of cartoon image of some of the key connections in cortical microcircuits.
So here we've got superficial pyramidal cells as spiny stellate cells as SS, deep pyramidal
cells as DP, and inhibitory interneuromes as the double I's. And this is clearly an oversimplification
of what is a very complex connectivity structure. But the reason I'm showing this here is that
if we know something about this connectivity structure, if we know about some of the key
patterns, we can start to ask how can the sorts of models that we've spoken about over the last
couple of slides then be interpreted in terms of the microcircuits that could help
that could help implement these in a biological system. So what I'm going to show in the middle
is an example of a continuous states based predictive coding style model
that has all the same elements as in the slide I showed you before, but are now mapped so that
some of the connections coming in and out of it loosely map to those associated with a known
cortical anatomy. And we can do exactly the same thing here with the discrete state space model
and think about how we can assign those roles associated with the roles of different cortical
layers and then use that as a way of forming hypotheses. Now what I'm showing you on this
slide is not necessarily the last word on this and you could formulate several different hypotheses
about how these two things map together and that's where a lot of interesting science happens.
This is just one example of how we can associate what we know about cortical microcircuitry and
anatomy with what we know about the anatomy of message passing for some simple generic forms
of generative model. What I'm going to try and do over the rest of this presentation is to take
the connections coming out of this out of each of these points and to try and think about how
those might or the roles that might those might play when we take an active inference approach
and the sorts of structures in the brain that might be involved in dealing with those
dealing with those things. I'm going to start by talking about movement and reflexes.
So here the kinds of things we're interested in are the connections that leave
the cortex and go to areas like the spinal cord and the motor neurons, the predictions that we
can then make about what we're seeing here in the central plot and the G that's been
circled in red is a prediction about the sort of data I might expect to observe which may here
be proprioceptive data consequent on particular sorts of action. And we can think of this quite
simply if we think about the structure of a reflex arc. So what I've got here on the lower part is
just the same marginal likelihood or model evidence that we've dealt with before this idea of it being
a measure of the fit of the model both in terms of its accuracy, the accuracy with which it predicts
some data and the simplest least complex version of the model that will work for that.
What do we do when we change the data? Well we try and make the data more accurate
in relation to our model by trying to align the data with our predictions.
So when we think of the structure of a reflex arc where we can interpret it as some descending
prediction coming from the motor cortex saying what is the proprioceptive data I expect to observe.
By comparing that with the proprioceptive data that's actually coming into the spinal cord through
the dorsal horn with that prediction we then get a potentially a mismatch between the two
that can be used to generate action via the ventral horn generating movements and contractions
and muscles that then try to fulfill these predictions. I'll show you just an example of a
simulation of an arm using active inference that has these sort of reflex arcs built in
and all we're doing here is we're just tapping on the biceps tendon where the red arrow is pointing
and we're just seeing the reflexive response that we're getting as a consequence of effectively
inducing this additional piece of sensory proprioceptive information by stretching that tendon.
We induce the mismatch between the prediction that's coming down and the
and the data that's coming out and that is corrected then by generating this additional
movement. Interestingly we can do things by manipulating the confidence of those predictions
and sometimes get things like bigger reflexes and hyperreflexia as the sort you might see
in clinical populations with spinal cord injuries and so here you can see the reflex is slightly
larger than it was before and slightly brisker. I'll show you another example of this same idea
where now we've developed a mapping between the microcircuit and the anatomy of the ocular
motor brainstem so starting from the superior colliculus the the rostral interstitial nucleus
is the middle longitudinal vesiculus and there is other centers and the cranial nerves that are
responsible for generating eye movements so cranial nerve three and cranial nerve six here
are seen here as resulting from these error terms so we're predicting how I would expect
my eyes to move and any error in the current position of the eyes is corrected by generating
eye movements so by inducing different priors on top of this model of different predictions
about where I'd expect the eyes to be we can actually then generate different sorts of eye
movements using a predictive coding style active inference scheme where the key thing is that the
predictions can be fulfilled by actually changing the positions of our muscles and positions of our
eyes so that sort of provides a very brief overview and a couple of examples
dealing with the role of reflexes and predictions in generating movements but it doesn't really
tell us anything about how movements are chosen how movements are selected and you don't necessarily
get any intelligent behavior out of that and for that we need to start looking at different sorts
structures and different parts of this generative model and here the key thing I want to focus on
is the role of the basal ganglia and which here we're going to associate with the computation
of something known as the expected free energy and I'll try and describe what that is a little
bit more in some of the subsequent slides so what I'm showing here is a mapping of some of the
subcortical but subcortical anatomy or more accurate I should say that this is a mapping
between parts of our Bayesian message passing scheme parts of the model inversion when we're
dealing with the ability to plan and to make decisions to some of the known subcortical
anatomy of the brain and the key thing I want to focus on is this G that is here depicted and
it's part of the as part of this triad and so the things that are feeding into this are what
we're getting from the cortex here which is some prediction of the outcomes we'd expect if I were
to pursue a particular course of action which is this subscript pi and the another sort of error
term here which is how far away is is are those predictions from my preferences about how the
world should be my prior beliefs about how the kind of data that I would actually actively seek
out and act to get together those are used to calculate our expected free energy
which we can then use to formulate beliefs about policies by saying that the policies we would
select the plans we would engage in are those that we would expect to be associated with the lowest
expected free energy and put that more formally we're saying that we're going to give a prior
belief that the policies the series of actions we're going to choose are going to be those associated
with the maximum information gain that tell us the most about the world around us that fulfill our
preferences and then we're going to add in an additional term here which deals with habitual
type policies and things that we tend to do because we've learned we behave in a particular
way in those situations now the combination of this information gain and preferences are often
referred to as an expected free energy and I won't go into the details here but that's simply
because you can rearrange them mathematically to make them look very much like the equation for a
free energy or or marginal likelihood approximation with an expectation around them to say that these
are the what we would expect given our predictions about the data we would obtain because here we're
dealing with beliefs about the future plans into the future where we haven't yet had those data
and we need to deal with the expectations of what those data would be under different plans
that we could choose and so here we're showing just an example of the direct pathway through the
basal ganglia which is normally thought to facilitate movement and depends upon this
expected free energy and an indirect pathway which here we've associated with these kinds of
habitual drives and I'll come back to those a little bit later on when we deal with hierarchical
models just to give you a little bit of intuition as to the information gain aspect of the expected
free energy because I think most people most people probably understand this idea of seeking
preferences and behaving to maximize some degree of reward but many people are less familiar with
the idea of seeking information and the way that that might manifest in these kinds of models
so as an example I'm going to show you just a simple simulation where we're going to manipulate
some of the different aspects of the uncertainty in the model so here in the upper left I'm just
showing a way of parameterizing the the uncertainty associated with the data generating process so
this is our likelihood precision our sensory precision and it effectively says that when this
precision is very high we can be very certain about the outcome we'll observe given a particular
state of the world whereas when it's very low we could predict everything with very similar
probability and we can do something very similar by manipulating the uncertainty in the dynamics
of the world so again when this is very high it means that where I am now is very highly
predictive of where I'll be at the next step in time whereas when it's very low it means that
pretty much anything at the next time point is is equally probable and I'll show you a simple
simulation where these two things are manipulated just to try and give you an intuition for what
it means to act to to maximize one's information about the world so the upper simulation here top
left shows four panels which can each change at some point in time to a different color
with some random probabilities and the blue line here is designed to show
effectively an eye tracking trace so it's a simulated agent who is allowed to choose
which of these panels it wants to look at at any one time and you can see it samples them
with a relatively even frequency in the middle panel what we've done is we've reduced the precision
or increased the uncertainty associated with the likelihood in the lower left that effectively is
like turning off the lights in that location it's effectively making that lower left location much
more much less informative much more noisy and so effectively what what we've got here is a
system that then ignores that it says that I can't get a good quality high quality information from
there so I'm going to look at all of the other locations rather than rather than this in the lower
left an analogy for this is if you're thinking about how you perform a scientific experiment
you would probably aim to use if you had a choice between two different measuring instruments you
would choose the one that gives you more precise measurements rather than the one that gives you
very noisy measurements in the lower left we've manipulated the uncertainty or the volatility
associated with the dynamics so here what's happened is the upper left location is associated
with much more uncertain dynamics so here what happens is that I end up sampling that upper left
location much more frequently and intuitively this makes a lot of sense because if you've looked
somewhere very recently normally you will you'll know a lot about what was there you don't need to
look back there anytime soon however if it's very volatile if it changes quite uh with some degree
of randomness and you have very little certainty about the state of it after you've looked away
you'll look back with a much greater frequency so we've effectively decreased the inhibition of
return in this location by increasing its volatility or decreasing the precision associated
with its with the dynamics in that location so the end of this was just to show you this sort of
emergent behavior just by having an information seeking objective by by having a prior belief
that we're going to act to minimize some expected free energy one component of which is to maximize
our information about the world now the next thing I want to come on to is the role of hierarchical
generative models and one of the key benefits of having a hierarchical model is that we can
now deal with things that evolve over a range of different timescales so you might have some
things that evolve very slowly and some things that evolve very quickly and to some extent we
can separate out two and use slowly evolving things to to to help us predict what's happening at the
faster level and here the key things to to note are all of these connections between
higher cortical regions and lower cortical regions which manifests both in the in the
discrete and continuous states based models and so it's worth them thinking about what the role
of these are and how that manifests in terms of the generative models we've been dealing with before
and it's really this that links back into the the idea of predictive coding as many people know
it and you've probably seen graphics of this sort in the past where we have a range of
cortical regions that are each making predictions about the others so here going from going from
the right to the left we've got a series of predictions as these dark black lines
and with prediction errors pass back up using these these dashed lines that then
allow us to correct at each level and all this this graphic shows is a simplification of the
graphic on on the previous slide so i want to give you an intuition for why this is useful
using a couple of examples so the first example i'm going to use is one from the domain of active
vision and so imagine imagine you're doing a task now where you have to fixate on this cross in the
center and i'm and and maintain that fixation and show you a stimulus but maintain fixation on the
cross stimulus is going to disappear and then your next task is to perform an eye movement to the
location where the stimulus appeared very simple task of the sort was used frequently in monkey
electrophysiology and and all throughout neuroscience but it's an interesting one because
it has several different components to it that i think help in terms of thinking about the utility
of hierarchical models so one aspect of this is making decisions about where you're looking at
each of those time points and making inferences about which of several alternative locations
you're going to perform an eye movement to so simple form of planning here
so that calls into into action the sort of discrete state space model that we were dealing
with before this this partial observable decision process this set of discrete sequential time
points and that's often very highly relevant structure of most simple tasks in cognitive
neuroscience however we also need to actually move our eyes we need to we need to move our eyes
from one location to the next we need to know how to generate forces we need to know how to make
sure the eyes come to rest in the right location given those decisions and that calls into play
this sort of continuous state space predictive coding style model so how do we then how do we
then combine the two how do we deal with the situation where we want to predict a particular
location at a discrete level one of several alternatives but then also map that to a continuous
motor trajectory and this is where generative models get a little bit more complicated so
taking this a bit at a time what we've got at the top here is our mark-off decision process model
this is our discrete state space model in discrete time now at each of those time points we're no
longer predicting data specifically we're now predicting a short trajectory formulated in terms
of our continuous state space models so at time one we're now going to predict a short element of
our trajectory and this is very much like the sort of clustering model I showed you much earlier
that you take a discrete point and you map it to a point in continuous space with some probability
at the next time we then predict the next bit of that continuous trajectory so our priors for our
continuous model are now inheriting from the predictions from the discrete state space model
and then we can do the same thing again at the next time point and so we can create a discrete
sequence from our our sequential model and at each point in that sequence associate that with a
short trajectory in continuous space that then predicts our continuous data allowing us to then
predict the proprioceptive information we would expect to get from the eyes when we're performing
a task at that source and you can see that we now develop a hierarchical structure also in
thinking about the relationship between the bits of the message passing scheme so here we've got
the discrete bit of our message passing scheme our discrete microcircuit and a bit of the continuous
microcircuit here and the interactions between the two of those were making predictions from the
discrete one to the continuous one and then passing errors backwards to allow us to to update
the discrete element so interpreting this again neurobiologically we can imagine that
say we have some element of the cortex perhaps the frontal eye field that's making predictions
about one of several alternative locations I could direct my gaze we can then map that via
sort of output of the basal ganglia and the predictions about what policy I'm going to pursue
through to areas like the superior colliculus which then might make use of predictive coding style
networks of the sort depicted here to then generate eye movements as we've seen before
and just to show you that in action we imagine that these are population of neurons that at
each time point are trying to predict one of several in this case three different locations
at four different time points we can then map that through structures like the superior colliculus
and this is supposed to sort of cartoon the idea of a population code in the superior colliculus
but then results in the eye movement to each of those discrete locations
putting it all together we can then formulate exactly the task that I asked you to perform
earlier and see the result of an active infant scheme performing that task
and so here you can see it performs it very successfully by by predicting sequences of eye
movements that are conditioned upon the discrete part of the model and the idea of maintaining
this belief about where the stimulus appeared and using that then to direct my policy selection
and eventually the movement I select I want to give you one more example of this sort of hierarchical
scheme in the domain of motor control and to appeal to the same sort of simulations we were
looking at earlier in terms of the reflexes exhibited by that arm the generative model
we're going to use here is a slightly complicated one but here effectively involves transitions
between different locations that an arm could be in or a hand could be in which then predicts
different locations in some continuous space and different locations that a target might be in
and that will then allow us to make predictions about about given where some target is in some
continuous space which decision am I going to make about the alternative points I could move my arm to
and so here we have the simulation where we have the target appearing as the black
ball out of the out of the three balls which is going to change at various points in time
and our active inference scheme is now selecting these locations translating those
into predictions of continuous trajectories and making the appropriate movements such as it
reaches each of these these target locations and what's shown in terms of the the graphic here is
also an interpretation of the message passing in terms of the known anatomy of the motor system
or part of it we can perform various lesions to this and see whether they behave in the same way
that we might expect patient populations to so here we've induced a cerebellar lesion which
effectively involves a misestimation of certain precision terms certain
confidence or variance parameters and you see this sort of overshoot and this kind of oscillatory
behavior in the way the arm behaves that mimics what we might expect him as cerebellar ataxia
we can look at the higher levels of the model and actually have multiple
hierarchical levels of the discrete scheme and here what we've done is we've induced a
a lesion in what might represent a frontal lobe lesion where now the model is able to perform
all the movements with perfect fluency but every time there's a change of context it takes quite
a while to adjust to that new context to deal with a new situation you'll see that instead of
moving straight there as it was before there's a lot more hesitancy and a lot more difficulty
constructing that long-term narrative because we've broken that slower hierarchical level
and here a final lesion that we can introduce is one that reduces my confidence in policy
selection so here this effectively makes me very uncertain about what I'm going to do next and
here we get a sort of a kinetic type picture and sort we might expect in a Parkinsonian type syndrome
so I realized that was quite a lot and I hope I'm not too far over time but I'll just try and
summarize briefly the key ideas that we've discussed so the first was thinking about this idea of
climbing probability gradients the idea that to maintain some form over time and to persist
we need to effectively be climbing these probability gradients all the time
we interpreted those probability gradients in terms of in terms of Bayes theorem and
connected that to the Bayes in brain and the sort of message passing that might be involved in
forming inferences about elements of some generative model and the role that might have in
cortical micro-circuitry we also discussed the role of action and the idea that
that one way to make our model better fit the world is to act upon it and one way to do that is to
simply reduce any discrepancy between the predictions from that model and the data that's coming in
and the structure of that is very similar to the idea of a reflex arc as is well known throughout
motor neuroscience and the final thing we thought about was how we can then construct
hierarchical models that allow us to produce simulations and theories that deal with much
larger scale networks in the brain and take a full sort of systems level view of how particular
movements and particular behaviors are generated through the inversion of particular forms of
generative model. With that I'd like to thank many people who've been involved either directly
or indirectly in this work. I'll mention the book again which was mentioned at the beginning
a lot of this talk was loosely following some of the structure in a couple of the chapters in the
book so if anybody's interested please do take a look. More than happy to answer any questions
thank you for your attention.

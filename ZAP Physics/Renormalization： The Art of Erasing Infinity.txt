In my last video, I talked about how perturbation theory and Feynman diagrams are used in particle
physics to calculate approximations of quantities that otherwise couldn't be calculated.
In that video, I talked about some of the pros and cons of Feynman diagrams.
But one thing I didn't talk about is the somewhat disturbing habit that many of these
perturbative calculations have of blowing up and becoming infinite.
Some of these infinities are nothing to worry about since they're cancelled off by considering
other related processes.
However, other divergences are still left over and seem to cause massive problems with
our calculations.
So, how do we ever get meaningful calculations out of our theory when we seem to run into
these problematic divergences at every corner?
The answer is that we need to use a technique known as renormalization.
Now, renormalization tends to be a bit of a touchy subject, especially when people are
first learning it.
But hopefully by the end of this video, I will convince you that it isn't really anything
mystical or mysterious.
It's a perfectly valid mathematical trick that we can use to extract meaningful results
from our calculations that it first seemed misbehaved.
To begin the discussion, we should first understand where these divergences come from.
To do that, we will use a somewhat simple example.
Consider this Feynman diagram, which contributes to the second order correction to our QED
Baba scattering process that we looked at in the last video.
Now, remember that we have to conserve momentum at every vertex of our Feynman diagram.
When we work out the details of this momentum conservation, we find something interesting.
Not all of the momenta of the internal lines are fixed.
In fact, for every closed loop we have in a Feynman diagram, we get an extra unspecified
momentum.
But since we have to add up all possibilities for the internal states of our Feynman diagrams,
we have to sum, or more specifically integrate, over all the possible values that this internal
momentum can take.
This integral is the source of our divergences.
The region where the internal momentum goes to zero can result in low energy, also called
infrared or IR divergences, while the region where the internal momentum gets very large
can result in high energy, also known as ultraviolet or UV divergences.
It should be noted that these divergences don't show up in every diagram that has a
loop in it.
IR divergences show up when we have massless particles in the loop.
These are the divergences which end up cancelling with other effects, like Bremsstrahlung radiation,
and aren't as worrisome, so we won't talk about these anymore.
UV divergences, on the other hand, appear when there are only a few internal lines which
make up the loop.
This is most easily seen from the fact that each internal line is mathematically represented
by a propagator, and the different propagators come with various powers of the loop momentum
in the denominator.
So if we have enough propagators, the integral for large momentum will actually converge
and won't be a problem.
Okay, now that we understand the source of these UV divergences, we need to talk about
how to navigate them to make sense of our results.
To do this, we'll start with an analogy from the area of math, known as singular perturbation
theory.
The example we will look at is a simple one.
Consider this equation, where epsilon is a number which we will assume to be very small.
Now say we want to solve for x, but we have no knowledge of the quadratic equation, so
we can't solve it exactly.
So we will find an approximation by perturbing an epsilon, or in other words, we write our
solution as a sum of terms with increasing power of epsilon.
Now we just plug this expansion in for x in the original equation, and separate our terms
by the order of epsilon.
Finally, we can solve this equation, order by order in epsilon, to find each term in
our perturbative expansion.
On the surface, this seems fine, but if we try to plot this function, we will see that
it actually has two roots, but we only found one.
So what happened?
Well, we can see that by keeping the epsilon in front of the x squared term in our function,
when we take the epsilon to zero limit in our perturbative expansion, we are changing
the equation from a quadratic expression, which has two roots, to a linear one, which
only has one root.
So we found the one root of the linear expression, but we haven't found the second root from
the quadratic expression.
Obviously, something problematic is going on.
So how do we find this other root in this limit?
Well if we were able to somehow turn this expression into one which is still quadratic
in the epsilon to zero limit, we could presumably recover the second root.
To do this, we will redefine our variable x to be in terms of some arbitrary parameter
delta times y, which is the new variable we will solve for.
Note that we have complete freedom to choose delta to be whatever we want, since any change
we make to delta will be compensated in y.
So we will define delta so that we both get rid of the epsilon in the y squared term,
but also be careful not to introduce any extra 1 over epsilon terms which might be an issue.
A good choice is just to make delta equal to 1 over epsilon, so that after multiplying
the whole equation by epsilon, we get a new equation for y that is still quadratic in
the epsilon going to zero limit.
Now we can write the solution for y as a perturbative expansion in epsilon and again solve
order by order.
When we do this, we amazingly recover our second root.
Now to find x, we just multiply these roots by delta and we have our perturbative solution.
So we were able to get around the problematic behavior of the solution by using the following
strategy.
First, we redefine our parameters in terms of arbitrary scale parameters, delta in this
example.
Second, we choose by hand the scale parameters in order to artificially eliminate the problematic
behavior in our equation so that we can actually solve the now well behaved equations.
Finally, we need to factor back in the scale parameters to get our true solution.
One last thing before we move on to talking explicitly about renormalization in particle
physics, notice that our decision for the epsilon dependence of delta was necessary
in order to remove the singular behavior from the equation we wanted to solve.
However, since delta is otherwise arbitrary, we could also include any finite factors we
want in delta.
So say we introduce a new parameter into delta, call it a, so that delta is a function of
both epsilon and a.
Then presumably a will show up in our solution for y as well.
However, since we put this parameter in by hand and it doesn't actually exist in our
original problem, x needs to be completely independent of a, meaning that when we multiply
y and delta to find x, the a dependence in y should exactly cancel the a dependence in
delta.
In fact, we can even write down a differential equation which encapsulates this.
Now since a can never show up in our solution of x, this equation should be satisfied at
every order in perturbation theory as well as overall.
This is actually something that I encourage you to try.
Just do the same thing we did before, but add in any finite term you want to delta and
solve the perturbative expansion for y in exactly the same way.
You'll see that almost magically, order by order in perturbation theory, these finite
terms exactly drop out to give the same answer for x that we found before.
Of course, it isn't magic because this must be the case for everything to be mathematically
consistent.
Okay, so what does all of this have to do with renormalization?
Well, it turns out that this whole discussion is almost exactly the same as what we do when
we renormalize a theory in particle physics.
See, in our theories, we have a certain number of parameters which are not fixed by the theory
itself.
These tend to be things like coupling constants between fields, masses of particles, and field
normalizations.
So just like we did for x a minute ago, we can rescale these by some arbitrary amount
so that our old parameters, typically called unrenormalized or bare parameters, are written
as a product between some arbitrary constants, which we'll call renormalization constants,
and new parameters, usually called renormalized or dressed parameters.
Now we usually choose to write the renormalization constants as one plus some other constant,
which is called a counterterm.
This just ensures that we keep the Feynman rules from our original theory, but with the
bare parameters replaced by their renormalized counterparts.
However, it also has an additional consequence of adding in a new set of Feynman rules that
we didn't have before.
These Feynman rules all come with factors of counter terms, and we will denote them
by including an x inside the diagram.
Now remember, this is all still completely artificial.
This is exactly the same theory that we had before, we've just shuffled things around
to make it look slightly different.
As long as we include all possible Feynman diagrams, including ones with these new counter
term vertices, we should get exactly the same answer as we would get in our original theory.
Just like how before, we recovered x by multiplying y by delta.
So now we need to fix these counter terms.
We want to choose them to eliminate the problematic UV divergences which show up in the theory,
just like how we chose delta to eliminate the dangerous leading order behavior of our
quadratic equation.
But how exactly do we do that?
Well these new Feynman diagrams with counter terms in them correspond to a certain set
of external states, which we can reproduce using Feynman diagrams without counter term
insertions.
For example, if we look at loop corrections to the QED vertex, now using the renormalized
parameters, these have the same external state as the counter term diagram which includes
a factor of the coupling counter term.
The loop corrections can be UV divergent for the same reason as before.
So if we define the counter term so that it exactly cancels these divergences, the corrections
to the QED vertex will always be finite.
We can also do this same procedure with loop corrections to the propagators to find the
counter terms corresponding to our fields and our masses.
So at this point, we've completely fixed the divergent behavior of our counter terms
so that they cancel specific divergences in our theory.
Now we can return to our example of Ba-Ba scattering.
We can do the calculation in exactly the same way we did before, except now use renormalized
parameters instead of the Bayer parameters.
But this doesn't give us the full result.
As we just said, to recover the full result, we need to also include all of the diagrams
which come with counter term insertions as well, up to a given order in perturbation
theory.
In this simple example, it's easy to see what will happen.
We'll be able to pair up divergent diagrams with corresponding diagrams with counter term
insertions, and when we add them together, the sum will end up being finite due to the
way that we chose to define our counter terms.
At the end of the day, we will have a completely finite calculation of the amplitude for electron-positron
scattering.
Again, we haven't done anything wild or magical.
We simply rescaled parameters which are not determined by our theory, chose these scale
factors in a convenient way, and took the rescaling into account in a consistent way
later on by including these counter term Feynman rules in our perturbative expansion.
At higher orders in perturbation theory, the details get a bit more complicated, but
the general story stays the same.
Okay, so now we've seen how renormalization can be used to render all of our amplitudes
finite.
This is amazing, but it still isn't the end of the story.
When we're trying to calculate these divergent integrals, we can't just plug in infinity,
since this will completely wash out all of the finite behaviors that we actually care
about.
So we have to regulate the integrals somehow, to force them to give a finite result, and
then take a limit at the end of the calculation.
This can be done in many different ways, but two of the most common are to simply impose
a maximum value that the loop momentum can take, known as a momentum cutoff scale, and
eventually take the limit as this cutoff scale goes to infinity.
Or we can work in an arbitrary number of dimensions, known as dimensional regularization, and take
the limit as the number of dimensions goes to four.
In both of these methods of regularization, one must introduce an artificial energy scale.
In the momentum cutoff case, this energy scale is the cutoff itself.
While in dimensional regularization, it's a little more complicated to see how this
scale arises.
Regardless of how the energy scale arises, it's something that we have to insert by
hand in order to find the counter terms and amplitudes that we want to calculate.
Since it doesn't show up in our original theory, our bare parameters must be completely independent
of this energy scale, exactly the same way that if we choose to include any new parameters
into delta and y, that didn't show up in our original quadratic expression, x could
not depend on these parameters.
In an exactly analogous way as before, this gives us a set of differential equations relating
the dependence on the new energy scale of the renormalized parameters to that of the
renormalization constants.
This set of differential equations is known as the renormalization group equations, or
RG equations for short.
The RG equations guarantee that physical observables are completely independent of this energy
scale, even though it shows up in intermediate calculations.
One result of this is that we find that the renormalized parameters actually depend on
the energy scale we choose, so here we have to be careful.
We are now doing perturbation theory in powers of the renormalized couplings, which means
that we assume that these couplings are small.
If these couplings can change with energy scale, there's a chance that they will at
some point become large enough for perturbation theory to completely break down, meaning
that our intermediate calculations, before this artificial scale drops out, are invalid.
This actually happens in the standard model, with quantum chromodynamics at low energies,
and is one of the reasons why hadronic calculations are so difficult to compute.
In addition to this, we can of course add in any finite terms we want into our counter
terms, as long as we compensate for it in the renormalized parameters.
This choice is known as a choice of renormalization scheme.
The two most common choices of renormalization scheme are to not include any finite terms,
known as the minimal subtraction MS or MS bar scheme, or to fix everything so that on-shell
particles always have the masses which are measured by experiments, known as the on-shell
scheme.
Again, at the end of the day, our final calculations should be completely independent of our choice
of scheme, so we can choose whatever is best suited for the calculation at hand.
Now, I'm sure that many of you are also asking, does this actually work in general?
Are we guaranteed to always have all of our divergences drop out if we do the renormalization
correctly?
And the answer, surprisingly, is yes.
The reason why is actually fairly simple to see.
Say we have a diagram with a certain set of external states whose divergences are not
cancelled by the counter terms we have.
All we have to do is include a new fundamental interaction with this set of external states
in our theory.
This new interaction will have a corresponding coupling, which of course gets its own counter
term which we can use to cancel this leftover divergence.
This may seem completely unacceptable at first, but think of it this way.
We're doing perturbation theory in terms of renormalized couplings, and we've already
seen that these pick up an energy dependence.
If we try to not include one of these interactions in our theory, this just corresponds to setting
the coupling to zero.
But we can only really set it to zero at a given energy scale, in which case at any
other energy scale, the coupling will run to a different value and we will generate
this new interaction.
This tells us that we have to include all of the interactions in our theory whose counter
terms cancel ultraviolet divergences.
This will automatically split our theory into two classes, those where we only need a finite
number of interactions to cancel all divergences, and those where we need an infinite number
of interactions.
The names of these classes of theories are a bit unfortunate.
The former are known as renormalizable theories, while the latter are known as non-renormalizable.
However, non-renormalizable is a bit of a misnomer.
Nothing really breaks down when we apply a renormalization techniques, it just turns
out that we'll need an infinite number of terms in our theory.
The non-renormalizable theories also have a very nice interpretation as effective theories.
These theories work very well, but only up to a given energy scale.
As a great example, if we try to quantize general relativity, we end up with a non-renormalizable
theory.
This means that we can still do approximate perturbative quantum gravity calculations,
but these calculations break down at a high energy scale, namely the Planck scale.
This just means that if we try to look above this energy scale, we should expect to see
some new physics appearing, which can be described by a more fundamental quantum description
of gravity.
As one final note, everything that I've talked about here is specifically known as
perturbative renormalization, and is usually what's used in particle physics or perturbative
quantum field theory calculations.
There are other contexts where renormalization shows up, such as in statistical physics and
condensed matter systems, where the discussion is somewhat different, but still shares similarities,
such as problematic behaviors arising with short distance or high momentum effects, and
the importance of being able to compare the theory at different energy scales.
If you've made it this far in the video, hopefully I have been able to convince you
that renormalization is not some wild thing that particle physicists use to sweep infinities
under the rug.
Instead, it is just a reshuffling of parameters in the theory, which allows us to explicitly
treat the theory where a naive treatment fails.
On top of allowing us to actually do calculations in the first place, renormalization gives
us powerful tools, such as the renormalization group equations, to be able to check our calculations,
assess the validity of perturbation theory, and sometimes even prove more general results
in quantum field theories.
So at the end of the day, although it isn't ideal that these divergences appear in the
first place, renormalization gives us a way to consistently treat these divergences to
allow particle physics to actually make predictions about our universe.

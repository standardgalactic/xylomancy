start recording. Hello, everybody, and welcome to the Public Invention Inventors Gathering,
which happens the third Thursday of every month. This is July. My name is Robert L. Reed. I'm the
President and Founder of Public Invention, and today I'm going to be speaking on the topic of
the Mind Blowing Beauty of Computer Programming.
Can everybody see my screen?
Yes. Okay, thank you. We have a small audience today, so y'all feel free to interrupt me.
I kind of have mixed feelings about this talk, because although it's really close to my heart,
I know there's no way I can do justice to it. I could spend a year preparing this talk, and I
wouldn't do justice to it. What I'm going to try to explain to you is magic and then even deeper
magic, and it's actually very important to me. Computer science is a new art from really the
1930s. Lady Babbage did a little bit of work before that, but it really wasn't until the 1930s
that it became a mathematical art, so to speak. That was before the majority of electronic computers.
There were still mechanical computers at that time, and although it wasn't entirely obvious at
that time, we now know that computer science sits at the intersection of physics and math.
It also touches upon philosophy and epistemology, the study of what can be known, and in a sense,
computer science is an extension of what is knowable. Now, the fact that computer science
could be considered a branch of physics is really, really deep and relates to things like
astrophysics and quantum mechanics and certain other important aspects of physics.
For a while, it was considered sort of an extension of math, and it's also that, but it has its own
character, so it's not really considered exactly math. Unfortunately, I have no hope of doing this
talk justice. This is something that's really, really beautiful, and I doubt I'm going to be able
to convey it to you in this talk. There are no pretty pictures in this talk, except one maybe,
and there will be a lot of oversimplifications, but I am going to be talking about what I consider
to be magic, even deeper magic and joy. So, programming and theoretical computer science
are different, but the same. To use modern slang, theoretical computer science is the God mode version
of programming, but programming, humble, simple programming, is the basis of the theoretical
computer science, and it's the most general tool ever. That's why it's important. Of all the things
that humanity has invented, computer programming is not necessarily the most important or the best,
but it is the most general. You can use it for almost everything. Today, it's needed in all the
sciences, all engineering, and many arts, and therefore, I think all young people need to understand
at least a little computer programming in the same way that all people need to understand a
little bit of algebra. So, I'd like to ask the question, what is a computer? So, to me, a computer
is a mindless machine that can only do very simple mechanical things. Now, technically, the word
machine to a mechanical engineer means the application of force. I don't mean that. I
use the term to emphasize that has no creativity, no slop, and each step is completely predictable
from the last. Now, it's quite an extraordinary fact that a computer can really only do five things,
and you could organize these a little differently, but this is the way I organized it back when I
was 16 years old, and I still think it's correct. It can input a number, it can store a number,
can do arithmetic on numbers, can output a number, and it can make a decision based on a number.
But those numbers can be used to represent a lot of things, and on the basis of these very, very
simple actions by building them up into an edifice of structure of the way things are organized,
we can do extraordinary things. So, we can ask, what is a program? So, a program is a set or
sequence of instructions for a computer, which is just a machine, which are executed in a particular
order that can change based on what's in storage. This is also sometimes called an algorithm,
because it doesn't actually require a computer. Humanity developed algorithms for mathematics,
which were step-by-step procedures before computers existed. Nonetheless, the nature of a program
is that it's a set of instructions that are executed in a certain order that tell a computer
what to do in the next step. So, the first magic that this produces is that complexity,
and really quite a lot of complexity, emerges from simplicity. With the basic five things
that I said, even a beginner programmer can compute the factorial of a number, and we're
going to see that later on in this talk, do an extraordinary thing called Conway's Game of Life,
which I'm going to show you a demonstration of, which is kind of the best example of complexity
rising up out of simplicity. And you can also simulate a role-playing game that feels real,
even though it isn't real. You can create a character, you say, my fighter takes out his
sword and whacks the wizard, and it feels like the wizard's really there and you're fighting,
even though you programmed it. You know the wizard in the end is just a set of numbers,
and the fighter is just a set of numbers. So, there's almost no creativity in it. Now,
I have never been an educator, but when I teach beginning courses in computer programming to
teenagers, which I've done a few times, these are the three programs that I teach first.
The second aspect of magic that computers allow is that they allow you to simulate physics,
obviously via mathematics. Mathematics is the language of physics. Without mathematics,
we can only do a little bit of physics. Now, what is it that we do when we talk about physics?
Well, Leonard Suskin has said, the purpose of physics is to compute, is to predict the future.
So, for example, you predict the future of where a ball is going to fall. If you throw it up in the
air, you know it's going to come down, and if you're a good physicist and you know exactly how
fast you threw it up, you know exactly where it's going to land. These sorts of simulations
can be done by a computer, and in very important circumstances, they succeed when math fails.
That is, there are situations which we, as a human society, do not know how to solve with math
very well, but we can still solve them with a computer by simulating things over time.
Now, there are limits to that. So, there are problems that a computer can solve
that are hard to solve any other way. So, the famous three-body problem, which is considered
to be unsolvable in math, recently there was a Netflix show about it, can be solved by a computer
in a limited way. It can simulate the position of three stars or planets or meteors in space
orbiting each other over time, and it can do that one millisecond at a time, and it can do that for
billions of milliseconds, and it will be fairly accurate, let's say, a hundred years from now.
Now, some people would say that's not a solution to the problem in the sense that it's not a formula
that you can write down that tells you where the planets are. Nonetheless, it's better than any
other solution that we have. In the same way, we can't write down a formula that describes the
flow of air around an airplane, but we can simulate. Now, there's a great philosophical debate as to
whether an individual neuron can be described mathematically. It's unclear if neurons are
actually extraordinarily complex or extraordinarily simple. You can find people who will tell you
either one, but it's possible that we can simulate the action of neurons, and therefore,
we can simulate the action of brains. We can also simulate the action of rays and waves and
biological tissues, and that is the reason that we have MRIs, CAT scans, and PET scans. Without
those, we would not be able to address the diseases that we address that way. The third magic is
something that we're all familiar with. You're using it right now. You're seeing a picture of me,
and you're hearing my voice, but inside a computer, those are represented as little bitty numbers,
which are transmitted through a network and come to your computer and then are translated back into
little blips of light on your screen and voltage levels on your speakers. This does not take a lot
of theoretical computer science, but most of us enjoy computers because they can store and
transmit representations of light and sound in the form of sounds and movies. Now, coincidentally,
they can store every word ever written by human beings, which is the basis for large
language models in modern artificial intelligence. So, we're going to talk a little bit about
programming later, but I want to talk about theoretical computer science, which I consider
to be deeper magic. So, the deeper magic number one is that you can prove there are things computers
can't do and can't ever do. This is the basis of cryptography. We've now proved there are problems
that can't be solved by computers. Some of the cryptographic problems are simply because we
don't have fast enough computers, but there are other problems which can never be solved by any
computer. They can never be solved by any sequence of instructions, no matter how fast the computer is.
And that's really a mind-boggling sort of concept, in my opinion. So, this leads to a thing called
Church's thesis. Alonzo Church was an American logician who worked in the 30s. The computation
done by modern computers are probably the only kind of computations that can ever be done.
Now, this is called Church's thesis because it cannot really be proved. So, it's a thesis rather
than a proof. So, we have several different models of computation. One is the von Neumann machine,
which is very similar to the electronic machines that we have. So, my Macintosh computer would be
closest to a von Neumann machine. There's the Turing machine, which has been made famous by
movies, which was invented a little earlier than the von Neumann machine. Our computers today are not
shaped like Turing machines, but they're similar. And then there's a very unappreciated form of
computation called the lambda calculus. And the interesting thing is that you can simulate each
of these computers with the other. So, the lambda calculus is powerful enough to simulate a Turing
machine. A Turing machine can simulate a von Neumann machine, and a von Neumann machine can simulate
the lambda calculus. So, you get what can be described as a trippy loop in that everything
is powerful enough to simulate the other, so none can be more powerful than the other. Now,
is that the only kind of computation that can exist in the world? Well, we don't really know,
but that's very strong evidence that there is no other form of computation, that there is no other
way to do it. Now, some people would argue that biological brains and analog computation
are a little asterisk to this, and that quantum computing kind of nibbles at the edges of this
a little bit. But fundamentally, this is accepted by most people. So, the deeper magic of theoretical
computer science is what I call the dismal deep magic. So, it may be that thought and computation
are the same. It may be that human brains cannot think anything that can't be thought by a computer,
and it may be that the success of artificial intelligence proves not that machines are
intelligent, but that humans are dumb, right? It may not be that artificial intelligence is
somehow coming alive or getting a soul spark or having consciousness. It may be that we're
learning that our own brains are actually not capable of very much. Now, I hope that's not
the case, but it's certainly an interesting philosophical point. So, there's an even deeper
magic, and there's no way I can really explain it in this talk if you don't understand it,
but there's a problem in computer science called P is equal to NP, which is a question.
Is P equal to NP? P is the set of problems that can be solved in polynomial time. NP is the set
of problems that can be solved in what's called non-deterministic polynomial time. If you're a
beginner, there's no way I can explain this in this talk, but what I want to point out is that
P equal NP is the most important problem known to humanity today. In movies and so forth, you
sometimes hear about the Riemann hypothesis. Riemann hypothesis is chump change compared to
P equal NP. If humanity could show that P is equal to NP, which is not obvious that it is,
most people think it isn't, then the world would change because in practice we could compute things
efficiently, which we could never compute before. Some things would still remain out of reach
however. And fundamentally, this asks the question, if you can verify something efficiently,
is there always a way to compute it efficiently? And the answer is not obvious. The greatest
mathematician, computer scientist, and even physicist in the world have been working on
this problem for 40 years. And in fact, I would argue they haven't actually made very much headway
on it. Now, the final really deep magic is information theory. Information theory is where
theoretical computer science overlaps with physics. And everything I say here has to be an oversimplification,
has to be taken with a grain of salt. But nonetheless, it's very important to understand.
Information theory is closely related to the idea of entropy. And that is the laws of thermodynamics
and conservation of energy. And the fact that the universe in a sense is running down, that we're
moving towards a less interesting, less concentrated state of energy, that everything is always
smoothing out and becoming more boring, so to speak. Information theory is independent
of computers, although we didn't know that 50 years ago. And it applies to many physical
processes. For example, the Nyquist limit of the Fourier transform, but also really fundamental
things like how black holes work are related to information theory in a way which I only
partially understand, I partially understand this. So it's kind of true that energy is equal to
information. And it's kind of true that information is equal to entropy. Now, obviously,
this is an oversimplification because entropy is not energy. Entropy is the opposite of energy.
Nonetheless, in very special circumstances, information is similar to energy. And in
different circumstances, information is similar to entropy. So now we can ask the really fundamental
question, which I want everyone, especially if you're a beginner here, to understand and be
intrigued by it. What does a programmer do? I've believed for a long time that the job of a programmer
is to create order out of chaos. So from long before the birth of Christ, there was a famous
hymn to Zeus by Clanthes. And it begins chaos to be his order. And in the middle of it, in one
translation, obviously, it was written in Greek, you can say, but you know how to make the crooked
straight and to bring order to the disorderly, even the unloved is loved by you. The poet is
speaking to Zeus. For you have so joined all things into one, the good and the bad that they
all share in a single unified everlasting reason. And of course, the Christian and Judaic Bible
begins, let there be light. That's what a programmer does. They bring light out of darkness and order
out of chaos. So fundamentally, a program systemizes that which was not systemized previously.
So the main tool of computer programming is could be called abstraction. It's not that easy to
understand what it means in this context. Programs have subroutines or functions, which can be
reused. The reuse of a subroutine always costs a tiny amount of time and also a tiny amount of
electricity. But once a subroutine is written, it can be used over and over again, and it never
wears out, just like the number 37 never wears out. In a sense, subroutines and mathematics
are stronger than steel and diamonds. Steel and diamonds and granite eventually wear out,
but math and subroutines don't. Abstraction is the main tool for making long programs
short and is thus closely connected with bringing order out of chaos. Factorial,
the first program that programmers should learn, teaches abstraction through recursion. Now,
there's a thing called Lambda lifting that I recommend an interested student read the Wikipedia
article on Lambda lifting, which is sort of the formal way in which abstraction enters
computer programming. But most programmers will not understand Lambda lifting. They will instead
think of when I create a subroutine, I am creating abstraction, which I can reuse again and again.
So my own personal style of programming is based on something that Paul Graham said.
He wrote an essay called Concision is Power, and I view the act of programming as being almost
their exceptions, the same as trying to create the shortest, most compact representation of something,
and thus, in a way, creating the most abstract version of a sequence of instructions to accomplish
something. Now, functional programming is a style in which subroutines reside effect-free.
This helps to bring order out of chaos because of something technical called referential transparency,
and it's the most abstract way of working because it means that a function can be used in any context
and always means the same thing. Now, this is debatable. We don't have time to go into it.
Some people don't like functional programming as much as I do.
Another way of expressing this is a principle called dry or do not repeat yourself.
In a computer program, there should never really be two lines that are the same. There should never
be two subroutines that are the same. If you find yourself writing the same code, it should become a
subroutine, and this principle can be applied to enormous aspects of computer programming.
You should not have duplication in your data, just as you should not have duplication in your
instructions, and in many, many circumstances, what people do is they copy and paste code from
somewhere, and in so doing, they're violating the dry principle, and after years, programs become
big, fluffy, disorderly things where things are almost the same, but not quite in different places,
and so forth, and that's very poor software engineering. But it takes beginners years to
fully understand this and to fully understand how to avoid it. Now, I'd like to talk about
the very first program that I think everyone should write. The first program is called
Factorial, and it's written with an exclamation point. When I was in college, there was a young
man who didn't know it was called Factorial, so when he saw in Factorial, he said, called it
in, as if it's very emphatically the way you say in. In fact, in bang, sometimes pronounced bang,
or in Factorial or in exclamation point, is used in the famous formula from probability
that's sometimes pronounced in choose K. It gives you the ways to choose K element,
the number of ways to choose K elements from a set of in objects. This is all important to
poker players and all kinds of probability and all kinds of mathematics. But the definition
of Factorial itself is really, really simple, and this is it. Right here. This is actually in
JavaScript. We could take this and we could run it. Now, I learned this so long ago. It no longer
seems weird to me, but some of you reading it may not have learned this, and I remember
how hard it was for me to understand when I was about 18 or 19 and I was first introduced to this
concept. So what this code says is that the function that we call Factorial, which is in bang,
so to speak, has a mathematical definition. If n is equal to zero, the answer is one.
If n is not equal to zero, the answer is n times the Factorial of n minus one.
Now, this is a trippy loop. Factorial is being defined in terms of itself. How can
the computer know what Factorial means when we're telling it what Factorial means right now?
It's not obvious. Now, to many people in this audience, it may be obvious, but when I was a boy,
this was not obvious to me. You might ask the question, well, how on earth can the computer
ever know how to do Factorial of n minus one when we're defining it in terms of itself? It looks
like a trick. It takes a while to understand it. In fact, the answer is this only works if you call
Factorial with a smaller number in the body of the function so that the problem is always somehow
getting simpler, and eventually it gets down to zero. If that is not true, you create an infinite
loop. Infinite loops are much worse than trippy loops. So in fact, you could say that this function
has a bug. If you called this with minus one, it would run forever because it would multiply
minus one by the factorial of minus two, which would be minus two times the factorial of minus
three, which would be minus three times the factorial of minus four, and that number would
never become equal to zero and it would never stop. The second program is a program called Conway's
Game of Life, and it was specifically designed to show how complexity arises out of simplicity.
And at one kind of artificial level, it's a simulation of biological processes. These four
rules are the only rules it has, and I believe this is the second computer program which every
beginner should write. It's played on a grid like a chessboard, but the chessboard is considered
not to be eight by eight, but to extend as far as you want it to. So it's played on a square grid,
and each grid is called grid cell square is called a cell. And a cell is either live or dead,
and there's a clock that ticks and one generation leads to the next generation. So you start with
some pattern, and then you start generations. And the rule for each cell is simply these four rules.
Any live cell with fewer than two live neighbors, that's the cells in the eight cells around it that
touch it, dies as if by underpopulation. I prefer to think of it as loneliness. It dies of loneliness.
Any live cell with two or three live neighbors lives on to the next generation. It's happy.
Any live cell with more than three live neighbors dies by overpopulation or crowding.
And then here is a bit of magic. Any dead cell with exactly three live neighbors becomes a live
cell as if by reproduction. Okay. Simple rules that produce almost unbounded complexity.
So now I'd like to show you an example of this is Conway's Game of Life. You guys can do this.
I'm randomly putting this here. I don't know what I'm doing. I'm just clicking on some numbers. So
you see this consists of a grid of squares. And each square is considered a cell. Yellow means
it's alive. Gray means it's dead. Now, when I click next, it's going to do a generation.
So some cells will died, and a few cells came into existence. And the pattern changed.
And now the pattern is going to change again and again and again and again.
And now I can simply start running it and we'll see what happens.
It's not done yet.
It's not done yet. It's not done yet. It's not done yet. Now it's done.
So how could that complexity that you just saw arise out of those four rules?
That is the thing which is of sort of paramount importance to mathematicians and related to
what I'm talking about. The ability of simplicity to give birth to complexity.
Okay. And then finally, an exercise that I think all beginning computer programmers should do
is to write a very simple sort of fantasy role playing game that has characters in it.
And you can have the characters fight like you do in Dungeons and Dragons so that a fighter can
whip out a sword and say, I attack the wizard and I strike the wizard with my sword and the
wizard's health goes down. And then the health, the wizard cast a magic spell, perhaps to catch the
fighter in a web which decreases their ability to strike in the future.
Now the thing about this is you play it. It's simple. It's fun. And it feels alive.
But you programmed it. You know very well the wizard is not alive. The wizard is just a subroutine.
The wizard is literally 10 or 15 lines of code that you yourself programmed representing only
with numbers. Now if you don't want to do that, you can think of a complicated game,
an expensive game like Minecraft or something like that accomplishing the same thing.
It feels real even though we know in the end it is nothing more than a complicated series of the
five operations I began this talk with. So programming is almost reason but not quite.
It is almost human reason but it isn't exactly. Programming does not tell the poet what word
to select or the musician how to play the note perfectly or the painter how the brushstroke
should go. But nonetheless the act of programming is very close to the act of human reasoning.
For 100 years, coming up on 100 years now literally, we have been making programming
tools more powerful. So for example, John von Neumann did not believe in compilers.
He believed it was a sign of mental weakness if people had to use compiled computer languages.
He thought you should program them sort of only with the most minimal set of instructions.
He was weird. Each increase in the strength of the programming tools has not made programming
obsolete. It has not decreased the number of people making gameful living by being programmer.
It has made programming more important and given gameful employment to more people.
So I personally do not believe anyone should fear that AI will replace programming.
That it's the same as saying AI will replace human reasoning.
AI will become a tool used by programmers. Now I have not used it. I don't use co-pilot or any
AI tools for that but at some point I may have to learn how to do that. On the other hand,
I do think we should fear killer robots. I'm much more worried about killer robots than I am
my ability to think being replaced by an artificial intelligence.
So there are a lot of joys associated with computer programming and I got to experience
these when I was a child almost. I started programming when I was 13, not very well. It
took me a long time to learn a lot. I didn't have anyone to learn from. I wish every child
could experience this joy. I wish everyone could do these. Now there are other ways to learn the
joy but computer programming is a darn good way to do it. As Lee and Nardo Da Vinci said,
the joy of understanding is one of the greatest pleasures that humanity can have and computer
programming is all about understanding. Often you begin in confusion and you move towards
understanding because you bring order out of chaos, you bring light out of darkness,
you start from a situation of not understanding a problem and then you understand it.
There's the joy of discovery which I think is quite different. That's where you learn something
surprising. Sometimes you discover surprising problems, sometimes you discover surprising
solutions. Sometimes like in the case of Conway's Game of Life, you discover surprising things which
are neither problems nor solutions. They're just surprising things about the mathematical world
in which we live and then perhaps the greatest joy is the joy of creation. You can write a program,
you can create something out of nothing which did not exist before and something important
as well. Of course you can do that by knitting a sweater but one form of creation is writing a
computer program and then you can experience the joy of mastery, the joy of moving from a state of
not being very good at something to being pretty good at it. I do not consider myself to have
mastered computer programming. I'm still learning. I wish I had time to learn more. I consider myself
a journeyman programmer so to speak. One of my heroes, Kent Beck, said that confusion should be
cherished because it precedes enlightenment and computer programming is all about moving from
a state of confusion to a state of enlightenment and then finally there's the joy of sharing.
In a way there are not many programs in the world. There is only one universal program.
It is splintered between my computer and Lawrence's computer and Christina's computer
and Morena's computer but in a way there's really only one computer program in the whole world and
we are all writing parts of it. We are writing little bitty parts of it so we are all
sharing in a given enterprise. For many people a great joy that they get to experience in college
and I wish everyone got to go to college. I know some people don't is to share their programs with
other people sometimes in a classroom setting sometimes not. I remember one time I was in a
graphics class at the University of Texas when I was in graduate school and my friend Steve Benz
put up on his screen a graphic program and he programmed a little airplane to fly around the
tower of representation of a tower of an airport and it was an astonishing thing to have accomplished
quite beautiful. I really enjoyed that. When I was at Rice University there was a student there
who was quite unusual and I would walk by his terminal and on it there was
a weird glyph and I couldn't figure out what it was so you know I asked the guy and he said
well it's elvish. He had created an ascii art representation of the letter the elvish characters
that Tolkien had created which are called tinguar an example of nerdy stuff but it was more fun
because it was shared. And then finally and my talk is closing now there's the joy of meaning
and this is the deepest thing to me. Public convention gives everything it does to the whole
world. We don't keep anything secret everything we do is shared through open source licensing.
We've been trying to save people's lives for a while now through global medical stuff. I
doubt anyone's life has ever been saved by one of our inventions although it is our goal and
we're working towards it. Things take time to ripen. I have faith that it will eventually
happen. Programming is an important part of our work. Almost everything now requires computer
programming. Programming is part of our work and our work is meaningful and therefore the
programming of me. So thank you for your support of public invention. Thank you for listening.
I've got a few references here which people can look up on google if they want and I will now open
the floor to questions. Well I should have been here at 8 and for some reason I thought it started
at 830. Okay Victoria Christina's raised your hand go ahead Christina. That was an excellent
presentation. I've never actually kind of thought about that all laid out in such a way. I feel like
it's it's one of those programming in general is one of those things that I've been around for a
long time and have a very very very tiny bit of experience with myself but if you're around it
so much you often don't necessarily think of the origins of it but I did have a question that came
up and it's something that I never thought of much myself and it's I love the slide on
factorials by the way. So in calculating factorials you the two things that come to mind are recursive
function versus iterative so I was wondering if like from a programming standpoint like different
characteristics or use cases specific to either using a recursive or iterative approach in programming.
Yeah it's a good it's a good question it's a really good question because in fact
you can prove in a sense that you you don't have to do this recursively. So what Christina's saying
is this self-reference the fact that factorial is defined in terms of factorial is called
recursion. You could write factorial as what's called a loop and you would count down and you
would examine the value in and you would count across in and and do it that way. So you don't
have to do it this way except there are things over time which become so complicated it would be
almost impossible to do all of them iteratively. For example you could be working on a solving
a differential equation you know or some very very big thing and you're in the middle of it and
you're down in the middle of a computation and now you have to do another factorial right. You
have to have subroutines and you have to have abstraction in in doing those things and there
there are some things which in order to not define them recursively you would almost have
to stand on your head. Like you could do it there's a mathematical proof that you don't
have to have this because after all this programming language is being compiled into something
which is just a set of numbers which is doing the same thing but it would become unnatural.
It would become so complicated that you almost couldn't think about it and it would get in the
way of your thinking about it and a proof of that is the formula on the right hand side of this page
right. Here we have n bang and it's expressed in terms of k bang and n minus k bang. Mathematicians
are using this you know using this sort of thing uh referentially as a subroutine in the same way
and if you go to the Wikipedia page you will see it defined both iteratively and recursively.
I don't know if that answers your question or not. Yes it does um and kind of as a follow-up to
that I don't want to I don't want to hog all the question time but um are there so from a
computer programming standpoint are there more resource constraints with one versus
the other so if so taking an iterative approach versus recursive like what does that what does
it mean for the programmer when you're coming to choose between the two? Okay so there are people
who will tell you that an iterative approach is more efficient because a recursive approach
appears on the surface to use more memory okay and so there are times when an iterative approach
would be better and let me let me return to this and try to explain this in the in the deepest
possible way okay so for example the way this is implemented in a computer in a in as the languages
compile it's turned from this representation into a different representation and you almost
can't discuss it without talking about what's called a stack frame and whenever you make a call
you push onto the stack which is called because it's like a cafeteria tray in a cafeteria you
know you have a spring loaded stack of trays sometimes you you push things onto the stack
and you and you build things up and then you take things off the stack okay you push the variables
here onto this the stack well however many calls you make each one of those has to have a memory
representation you can run out of memory okay now you can do it it appears on the surface that you
can do it more efficiently iteratively because you wouldn't have that extra space however this
program the way it's written is what's called tail recursive in the sense that the the use of
factorial is the last thing in what's being done here so the compiler knows that and can
automatically implement an efficient way of doing it so in practice it doesn't matter
okay now another way that i if i wanted to i could attack what you're saying is i could say
yes it might be more efficient in terms of computer instructions but computer instructions are cheap
what's hard is the clarity of the program to make it in an ordered fashion and so i would be very
load to write it in a more efficient way if it decreased the clarity of the program from a
human understanding point of view there's another saying in computer science which is that
premature optimization is the root of all evil that was that's famously said by
donald canoes a very important computer scientist so let me let me just show you here what happens
um so over here on the right this is actually a uh the development tools which are built into
the chrome browser and it runs javascript and so i can paste this probe oops i could paste
this program in here but it's apparently please type a while posting in here so it's got a nice
security feature won't let me post it in oh allow pasting okay that
well i'll do it the old-fashioned way i'll type it
so i already typed this in okay so this is javascript this is completely legal
javascript okay and i can call factorial of zero and it's one and i can call factorial of ten and
it'll be a big number and i can call factorial of 20 and i can call factorial
let's say 100 this may not work okay and it kind of works and it's a very very very very large
number right um now eventually we'll run out there's a bug in this program in a sense factorial is
not actually defined on negative numbers but if i run this on a negative number as i said before
it will go into an infinite loop so we're going to see what happens when it does that
so what it actually says is maximum call stack size exceeded what it did is it kept going until
it ran out of its own memory now it's smart so before it crashed my computer it caught it
and it said oh i'm not going to let you make that call again
that may be more than you wanted to know about that no no and then sorry so one last thing would
be so would you say that recursion then is more suitable for problems that could be broken down
into similar sub problems and the thing that first comes to mind for me is like some of the sorting
algorithms whereas when i think of iteration i think of more suitable program uses for
problems that would require repeated execution of a block code linear searches things like that
yes okay so i would say yes and the best example of that if you want to google it right now is um
google quicksort okay quicksort is a very simple um program here let me create a new slide um it's
not necessarily the best the best sort in the world but it's it's extraordinarily simple and
that's why it's done it was created by another famous computer scientist i had the pleasure of
meeting uh Charles anthony robert whore who for some reason was called tony uh so more created
quicksort and quicksort looks like this and i can't this is not going to be correct JavaScript
quicksort of and i'll call it s is a sequence
okay and what i do is i is i'll say let a be first half of s that is the first if s has 100
members to be the first 50 okay and exactly how i'd do that i would depend on the exact language
i'm in but let you know let b be the second half of s okay um uh now i'm forgetting it okay and so
then what you do is you do a quicksort of a and then you do a quicksort of b so that those two are
sorted and that that gives you back two list of 50 numbers which are each sorted and then you you
merge them very simply and i maybe i'm forgetting because that would be called a merge sort instead
of instead of a quicksort i think maybe quicksort finds the median uh value and takes all the lower
ones and then all the upper ones and it sorts those two and then merges it back but it's
fundamentally recursive in the way that you just described because you're taking a list of 100 elements
breaking it into two list of 50 sorting the two list of 50 and then doing something very simple
to put it back together now it can be done iteratively as well but it is probably harder
to understand it iteratively than by doing it recursively so why do it that makes sense
that's really cool so i i don't know you know a lot of applications i you know because i don't
don't do computer programming but i do know that a lot of what's behind some of these companies that
are doing really extensive gene analysis so analyzing genetic combinations and permutations
they're using programs with a lot of factorial calculations involved so that's how they're
discovering like different gene combinations correlate with certain syndromes or uh you
know genetic mutations that's that's like the the first real-world case that comes from my mind
well right okay so so here's something mind-blowing even the act of so to us the act of multiplying
two numbers is simple and in javascript it's simple you just write a star b and that multiplies two
numbers together okay but as a becomes gigantic and b becomes gigantic it starts to be inefficient
to multiply it the way humans multiply numbers okay of course you would never think of worrying
about this until it's more than like a thousand digits on each side but the numbers you deal with
and genomic combinations can can get really big there's an entire science of computer science
that deals with asymptotic complexity where you try to create the most efficient algorithms you can
i do not fully understand this but the most efficient way to multiply numbers is actually
with the Fourier transform okay so that algorithm for multiplying very very very very very large
numbers is quite unnatural compared to what we as school children learn to do in our multiplication
and if you were trying to compute factorial you might do the same thing and you might use
approximations and then there are all kinds of all kinds of tricks to make that go faster and
there's an entire science behind that which is usually not done usually computer programmers
don't worry about that until they're at least out of college
oh do we have any other questions i'll let other people ask questions otherwise we will be here
all night with me being like what about this what about that i am always at your service christina
all right i'm sure others have have questions so i'll i will let others ask them
another thing i learned in graduate school is that silence is golden and often if you just
wait long enough people will ask a question even though they're shy
i don't really have a question but i did you have a comment though um in your description of
one of the things i think that i think is kind of overlooked is the root word of analog computing
is i think it's shared with analogy and i like thinking of um computing as an analogy
that is running on base level physics uh or basement level reality and that that level of
and what we're basically doing is creating a story that is playing out in front of us
in the world around us and the story is and we we are consistent with our story and the
consistency of our story reinforces our understanding and um in our logic in a way that gives us
belief and understanding of the things that we're doing and the answers that we're getting
it's a little poetic but the but it allows you to start thinking that like water can be used in
computing um gravity can be used in computing mechanical structures can be used in computing
DNA can be used in computing like computing is all around us and it's it's really the the
analogy the stories that we use to construct um and the mathematical themes and theories behind
those those those properties that kind of really make it happen stop
yeah that's all true
um
you can make a computer out of pneumatic vows or water vows and people did before electronics
were available but the computing that is all around us doesn't do what i would call general
purpose computing in general physical properties tend to be smooth and continuous and in that sense
they're a very limited form of computation for example a slide rule or a caliper you can be
used to multiply numbers but only in it only in a very very limited way making a turing machine
or a turing complete machine or von Neumann machine or lamda calculate machine out of a
slide rule would be very hard um and so the electronic computer and the the attempts to
make mechanical computers which famously lady aida babbage did um created a new era of uh computation
i agree with that there's there's still a open question of can you build an
a general purpose analog computer um and then the the corollary of the key build a general
purpose uh neural network um with the drill with a neural fabric um and which is currently a very hot
topic right now in AI research um i agree but that question is bigger than i can address in this talk
one thing i think it's under appreciated that i would also want to bring up is that um the advances
in computing that we're currently seeing have allowed us to tackle problems that were previously
un we were unable to really address um i think that's i think that's going to open up new areas
in biology and physics and and uh and i'm just saying of like the the big the bigger world out
there and the small world out there so i think that's i think it's under appreciated because um
it ten years ago we couldn't model um with a fidelity that we had uh that we can today
with a number of variables and a number of um factors
yep that's correct
does anyone else have a comment
um i just i think it's really cool to think about and uh lauren says first comment made me think of
this it's really cool to think about the fact that all of these well most of these equations or
calculations that are you know given examples here they have been around since ancient times like
these were you know the the end notation wasn't really developed until later but the the like
core idea can be traced back to you know we're talking like you know bc yeah and it's how how
we've kind of evolved the use cases of these equations to for computational purposes so instead
of us using these equations ourselves we have machines now that do it for us um but a lot of the
earlier uses of them were just you know obviously we think of like physics and astronomy um but even
before then it was kind of just this almost like pondering of what if so what if i take
these numbers and do this what happens um so these very very ancient ancient calculations that
are very much a part of our modern life in a big way we wouldn't we wouldn't have modern life today
without them yes i mean just just think of how humanity's power was expanded by the development
of those mathematical algorithms before there were computers right i mean you know before that you
had uh questions that that we would think well that's a computable answerable question and people
might not know how to answer the question right they might not know how what's the area of a sphere
right they might not know what's the area of a cone they might not know that if you have a
right triangle the square of the length of the hypotenuse is equal to the sum of squares of
two sides although that's been known for a long time presumably there was some time
in the past when it wasn't known right um and so that gets back to my point computer programming
is a form of mathematics that brings order out of chaos and it it expands our power
and to me the computer is an extension of the human brain it's not a replacement it's
an exo brain if you will just the way tony stark's iron suit is a exoskeleton wikipedia is a part of
my exo brain and i can use a computer to perform calculations which would be very
awkward or impossible for me to perform in in other circumstances um so um in a very real sense
it's it both the act of systematizing things which weren't systematic before extends human power
and the electronic computer which can do these things very rapidly also extends human power
do we have any other questions or comments
what's your favorite equation
me yes favorite equation and why
well um force equal mass times acceleration because it's a it's a it's a second order differential
equation which happens to be easily solvable and from which we can derive um an extraordinary
number of effects
i have another question maybe more um kind of philosophical you started with a definition
of what a computer is okay and kind of explaining it can only basically input and output numbers but
with programming and now um AI or machine learning is there are we getting to the point
where basically these programs or these computers can think much faster than we do i know they can
solve equations but are is it getting to the point where they can quote unquote learn faster than us
and how do we keep up the understanding of like you're saying a computer can only do something
that's traceable um is there a point where um maybe like in comparative to to our human reasoning
um is there a point where we're too slow to kind of understand um these programs and machines
well it's a really good question let me give you the answer that throw AI people give and then
i'll tell you what i think okay pro AI people point out that although large language of models
which are one aspect of artificial intelligence are relatively in their infancy they already
beat human beings at standardized tests such as the graduate g the GRE and SAT and ACT and they can
do well on tests of biology and physics and so as my friend david jeski has pointed out
every time humanity has said well computers aren't really intelligent because they can't play chess
or they can't play go or they can't um write poetry uh you know those have all fallen right like
they didn't they do those things right and um curmudgeons like me keep saying oh well they
aren't really intelligent even though they can answer a bunch of questions that i can't possibly
answer okay but then i'll tell you what i think um just because it's i think the honesty is worthwhile
i mean sure an AI could beat me at a biology test but an AI has had an infinite amount of time to
learn and to it it's an open book test and AI can't compete with me at all in any real problem
that it hasn't had a bajillion cycles of 50 gigawatts of power to study ahead of time yeah
yeah you know um so
i don't accept artificial intelligence as thinking and being able to learn things at all
now obviously you can build a classifier right like you could classify leaves and it'll tell
you what kind of tree it came from better than i can personally remember the shape of leaves
you know to do it it would beat me at that task right um it would beat me at translating french
into english but it doesn't beat me at understanding even though it can translate french into english
better than i can
i think the question comes down to what is what does it mean to be humans create creativity
you know novel ideas i guess
yeah i'm i'm you know i can't claim to be more creative than an AI
but still i i i feel that all human beings have something in them which i have not yet seen in an
AI maybe five years from now the little child that is artificial intelligence will have grown into
something um worthy of respect
you know uh and then are we going to say switching the machine off its murder
you know did we kill an AI when you know the power failed i i don't know
i first i but at the moment i remain skeptical of these these things
you
you
thanks for your perspective and the talk is very uh interesting and informative
you're welcome any other questions
okay here in none i think we'll um stop the recording here

Hi, my name is Dave Farley and welcome to the engineering room.
If you haven't been here before, do hit subscribe.
And if you enjoy the content today, hit like as well.
Do join in the conversation, too.
And let us know your thoughts and ideas in the comments below.
The engineering room is an occasional series and is meant as an addition to the
more usual content on the Continuous Delivery Channel.
These are longer form conversations with some influential and thoughtful people
from our industry.
Today, I'm joined by my friend, Kevlin Hennie.
I first came across Kevlin at a conference in Australia many years ago,
although we're both English, he gave the tour.
He gave several talks at that conference, including a keynote,
which was funny, unexpected, educational and brilliantly well presented.
Over the years, I've come to expect nothing less than that from Kevlin.
That's his norm.
As I started talking more frequently at conferences, our paths crossed more often
and we became friends.
Kevlin is an independent software development consultant, trainer, speaker and writer.
The people who work with Kevlin always speak very highly of his services.
He's witty, nerdy, sorry, Kevlin and smart.
He also has a Google unique name.
Try it, you'll only get Kevlin.
And rather strangely, among the Cognis Enti, if you ever see an obvious
public software failure, a screen in an airport showing a command line or an advert
in a store showing the blue screen of death, it's called a Kevlin Henning.
But no doubt we'll get to that.
Welcome, Kevlin.
Did I miss anything important?
No, thank you very much, Tim.
That's the perfect introduction.
I'm going to copy and paste that.
Great, thanks.
Whilst I've been looking forward to this talk for a little while,
the last time we met was in Copenhagen at a go to conference.
And my wife and family took the mickey out of me for the rest of the conference
because you and I, every time we crossed, we spent all of the time talking
rapidly in great detail because we hadn't seen each other for a while.
That was one.
But the funny thing is that the way that coincidences worked out is that
pretty much the moment I arrived in the hotel, you and family were there and I
bumped into everybody, including your son and daughter-in-law at the airport when I
left, which is like parts crossed quite a lot there.
So yeah, plenty to talk about, plenty to catch up on.
Cool. So so let's let's start with that
production finally being called a Kevlin Henning.
I think I know the answer to this, but tell us how that happens.
Yeah, well, so it's one of the things I guess it's, you know,
if you're if you're in software or there's something you really into,
you become sensitive to things, you start spotting things.
And I was always fascinated by the fact that,
you know, see occasional failures in places, you occasionally see
a cash machine booting up and stuff like that.
You kind of notice this stuff, makes you ask questions about it.
But then we hit the then we hit the kind of like
cameras on phones era.
And at that point, suddenly it's like I'm walking everywhere I'm walking,
I have a camera and the amount of software that we actually have everywhere is huge.
So I'm starting to take pictures of these things.
I'm also taking screenshots whenever something crashes,
particularly if it's losing my work, I'll take a screenshot and hold on to that.
Because I think it's fascinating because it's kind of like, OK,
it's it's often frustrating, but it's fascinating.
It's just like, OK, I mean, this is my space.
I'm in software that collectively we somehow created this.
And I'm fond of noting that we as in software development are the largest
creators of a kind of guerrilla installation art on the planet.
Nobody else comes close, you know?
But I used to kind of collect these and occasionally I kind of put them in talks
or when running a workshop or training course, I kind of have them on on my screen
just like as a screensaver in the in the breaks, always causes conversation.
And I always pointed out, oh, this is really interesting because in in failure,
when something fails, you learn about some you learn something about how it's constructed.
You know, it's kind of like at that moment, you lose all the encapsulation.
You're presented with something when it works that is beautiful and pristine
and pixel perfect and offers some kind of user experience.
And maybe you maybe you can guess the technology, but most of the time you can't.
And then it's like you drop something on the floor and it fractures.
And it's just like, oh, look, this bit's made of C sharp.
Or they're using that as part of their stack, or I'm pretty sure that's out of support now.
Or whatever it is, you see it loses its encapsulation.
It loses that kind of like surface and we see the insides.
And if it doesn't tell us directly how it was built, it kind of invites questions.
It makes you go, oh, I wonder what, how did this arise?
Did they forget to put a try block here?
Did they forget, you know, how did this exception escape to the user?
But, you know, I just did that kind of as a point of fun, as a point of personal
interest and point of showing people.
And then people started sending me these things by email and then we hit social
media era, people start sharing it more directly.
And then it's Twitter and people are just like posting it and just adding me.
Then I start retweeting it.
And then that's how that's where that's where it acquired its name.
And they actually made its way into Urban Dictionary, the register and a bunch of other things.
So, so, yeah, it's just one of those things.
The incidental side effect is it is one, it's interesting.
I find it's still fascinating.
I think there's a kind of a humor to it, as well as a serious point to be made in all
of these things, but it also ends up being an accidental service, because sometimes
you're talking about rail services and train stations and things like that.
And often people will at the train company as well, wherever they are in the world.
And, you know, you can tell what the customer service is like, because often they'll go,
oh, you know, instantly jump on that, which station was this?
I'm really sorry this happened, or there'll be a silence.
And so you can kind of tell that you get a sense of the customer experience as well
as the failure experience.
That's brilliant. So, so, so kind of the archaeology of failure.
Yeah, yeah, that's exactly it. Yeah.
That's that's great.
That's I often think that we kind of we don't.
Well, I don't think just think I know.
We don't think too much about the failure failure routes so often.
Yeah, there's one of my favorite
quotable bits of research was a usenix survey from a few years ago that looked
into the cause of production failures and something like
60 percent of production failures are in the error handling path.
The most common line of code in the event of a production failure is a comment
saying should should do some exception handling here.
Yeah, yeah.
That's a really interesting because that one,
if it's the if it's the one I'm thinking of, that was 2014 paper.
Yeah, but there's probably been others.
But I think I first came across that somebody did a study the late 90s, early 2000s.
I really can't I'm fortunate I can't cite it.
I can't remember who it was.
But they did an analysis of of failures in the Linux kernel and they said it's
mostly on the error paths.
Yeah, you know, the dark alleyways that just don't get explored and tested.
Anywhere near as often, you know, happy day works out fine.
But these edge cases, something goes wrong.
And then either it was just like this was to do or somebody had an idea of like,
well, it should be something like this, but it was never really tested.
And it kind of got marginalized in their in their memory.
And now, you know, control flows wandering down this dark path that's been untested
to deal with a really bad situation.
And the situation gets worse, you know, it's one of those things.
And that that that seems to be a recurring thing that these edge cases
is kind of, you know, when you're at the edge case, it's not an edge case anymore.
It's your world. Yeah, yeah.
Yeah, it's one of the things one of the things I did some research for my book
into the kind of history of software engineering.
And of course, Margaret Hamilton was a huge, you know, a hero in that in that
field in the early days of that one of the things I loved that she that she went
on about was talking about the importance of the systems that she was working
on the flight control systems for for the Apollo missions being man rated.
And so the reason why she coined the term software engineering was to
because they were spending all of their time worrying about how things could go
wrong, like like engineers do, you know, you don't build a bridge and just only worry
about the happy days, you worry about when there's a storm or when the loads
are too heavy or all of those kinds of things, too.
And we we need to be thinking the same for building serious software systems.
Yeah, I think I think a lot of the stories around Margaret Hamilton are absolutely
brilliant. I mean, the fact that, you know, there's one where she brought a daughter
into work, you know, one day and daughter recreated an error or created an error
condition just by messing about with it, put it into a state.
You know, it's just like, you know, cats and children.
And, you know, and the other cases where, you know, these, you know,
they're kind of a classic certain McKismar associated with the fact that at that
point, all astronauts would have been Air Force pilots and the culture and
image that went with that is like, oh, these are highly trained people.
And they won't make mistakes like that.
And then probably one of them does.
And it's just like they go back to her and say, yeah, let's do that.
Let's put that failure because it doesn't matter that they're highly trained.
The most the operative word in that sentence is people.
Yes, it's highly trained gets you so far, but you're still ultimately human.
And I think that that and one of the other points that I read that Margaret
Hamilton talked about is she was trying to get really get a seat at the table as
it were, because certainly at that era, this whole idea that software could form
a viable, meaningful first class component of a system as opposed to just
a secondary component was that was completely a foreign concept.
And, you know, there were even astronauts saying, well, we don't think we're going
to need software. We don't actually need software to land on the moon.
And, you know, it's to anybody who's either studied the physics of it or is
familiar with any of the any of the modern stuff around this is just like, yeah,
you can't just fly by the seat of your pants and that's going to work out.
You need this stuff.
And so she wanted that seat at the table.
And, you know, engineering was the term that she she chose.
You know, we've got we've got to treat this in that sense.
It's up there with all the others, because this is not just this is not just icing
on the cake. This is not just this is not just a glorified slide rule.
There's something deeper and more fundamental here.
We're talking about control systems and
the data that is genuinely life critical.
You know, this is not just a slide rule.
This is beyond that.
It's the idea of it's not just a calculator.
Whereas I think many people thought of this as just extension of the calculator.
Yes. Yeah, absolutely.
But I was a brilliant woman and a real pioneer in our field, I think.
Yeah, yeah, definitely.
But I think that's one of the things that I found with
because because one of the other things that we had this term.
Engineering that kind of took off from different points.
And I've seen it misattributed.
So I've seen things where people said, oh,
the NATO response of 1968 Software Engineering Conference was the beginning.
Coined the term.
No, no, no, no, no, no, they used the term because it was already around.
You know, Margaret Hamilton initiated that.
I think I stumbled across a bit in communications, the ACM, that it was used
as a term in 1966 in there.
And in other words, there's a kind of a lineage.
So by the time the conference came around,
this term was currency and it existed.
And and that was trying to try and say, well, what does this look like?
If it's for software, you know, but I love some of the in, you know,
I did I know you looked at it for your book and I did it for.
I did a whole lot of talks in 2018 because it was 50 years on.
So I thought I'd be in the series to 1968.
I thought, you know what, I'm going to go and read.
I'm going to actually go and read end to end the whole proceedings
and and also look at look at the 1969 one as well.
But, you know, do that.
And I did that a couple of years beforehand.
And I found it absolutely fascinating because one, it's an amazing historical
document capturing some really interesting insights.
But also there was a real sense that one of the things is some of it is
incredibly dated because, you know, it does date because they're they're
talking about technical constraints and concepts from the 60s.
But at other times, it's just like, oh, yeah, you know, it's not that you agreed
on everything, but all of the ideas that we now debate and and push forward were
present, they were they were there.
Yes, yeah, and and and and problems that those people were facing in building
real real systems, even though the computers that they were building them for
are all in museums now.
You know, it's it's it's it's it's one of my one of the things that got me
interested in kind of talking about engineering in in my book and stuff was
that, you know, I think that we discard some of those really durable ideas to
readily that there are things at the heart of our profession.
I think the most important things about our profession
that are that haven't changed since the 1960s.
And as you said, you know, I had hair standing up on the back of my
neck when I was reading stuff by Alan Perlis describing in language that sounded
quite dated in many words, his choice of words sounded like somebody from the 1960s.
But nevertheless, he was expressing ideas that would that still too many
software development teams don't even think about doing to their detriment.
Yeah.
And.
Crazy.
Yeah.
And I think there's a there's a really interesting things in there about, you know,
that a lot was up for discussion, but also sometimes some of the discussions were
that they're talking like old hands.
You know, they're talking about 10 years like it's a really bad thing.
So that's the fascinating thing about doing this talk 50 years later.
It's just like, well, they thought they were it was old hat when they did 10 years in.
We're half a century on how we do it here.
You know, and, you know, there's some really, really interesting things.
So from my perspective, one of the things I'm very interested in is is testing.
And interestingly, before reading that, I had kind of presumed that unit testing
as a term, I kind of I kind of dated that to the 1970s.
I found, you know, I hadn't really pursued it back in time.
But 1970s was what I had in my mind based on what I'd read.
And I thought, you know, that would that be terminal.
But I look at the software engineering,
look at software engineering proceedings.
And there it is.
The term unit testing is there without
qualification or definition, in other words, it's not presented as here as a new idea.
It's presented as, oh, OK.
I assume everybody knows that to it.
Yeah. And it was really interesting just looking at certain ideas like that.
As well as some other historical
kind of foreshadowings.
So a number of years ago, I was quite heavily involved in the patents community.
There are a number of
elements in the patents community and patents thinking, which I think have been
hugely neglected for me, one of the most the real turn-ons with the whole idea
of really understanding this is a pattern.
It is not a it's not a principle.
It is not a universal.
Here is an idea.
And you know what?
Sometimes this this works in some cases and not in others.
And here's why. And people often miss that.
They kind of kind of latched onto the surface, as it were, but not really
understood the depth.
And for me, the huge, the huge influence was the trade-offs understand the trade-offs.
It's just like, ah, here we go.
Here is here is why this works well here, but doesn't overhear the context.
Dependence of the idea was absolutely huge.
So rather than talking about
software from a perspective of mathematics in which which is a time which is
timeless and universal and the rest of it, here was something that was hugely
dependent on, well, I can't tell you what's right.
You know, somebody says, is this the right way?
Or is this wrong?
It's just like, well, you know, it's going to be that it depends.
And that's not because I'm being a consultant.
It's because genuinely there are about three or four different ways of doing this.
Yeah, show me the landscape.
You know, you've got to show me the landscape that you're going to you're going
to put this into my answer will be different if you're dealing with a legacy system,
perhaps with a modern system will vary from language to language,
depending on certain elements.
But there might be broader ideas that are still stable.
But, you know, there are if you say, oh, this is in a multi-threaded environment,
then I might retract my previous answer and go, OK, actually,
we're going to take a different path here.
Yeah. And it's the contextuality and understanding the trade-offs.
For me, that was really exciting.
Now, we tend to for a lot of people, they tend to credit
the gang of four
Gamma Helm Johnson Flussides with the pattern stuff.
Now, certainly that initially turned me on.
But even before the book was published, I'd heard about this stuff.
But it was this other stuff that was going on inside.
This comes from architecture.
This doesn't get some software.
This comes from architecture.
And Christopher Alexander kind of originated this idea and this whole idea of
he was really big on the idea of you've got to have an empirical design.
In other words, he was trying to move.
He was trying to move building architecture away from fashion.
Yes.
Which is something I think we get plagued with in software.
As well, he was trying to say, well, look, there's an empirical solution to this.
Does it work? You know, here are the qualities that make something work.
Now, have you defined your problem?
Does this solve the problem of living or whatever context he was looking at?
And he was very clear, use the language of empiricism all the way through.
This is not to say there were no artistic qualities to it, but I was always
fascinated and his writing style, again, it captures the time is 1970.
I started reading all the Christopher Alexander stuff.
And then you eventually hear another book by Christopher Alexander.
Notes on the synthesis of form, which was published in 1964.
And I had a vague awareness that this had a big influence on a lot of disciplines
at the time, but rereading reading the 1968 NATO software engineering
proceedings was fascinating because they kept referring to Christopher Alexander.
But this predates his patterns work.
It was all synthesis to form about how he thought about design in terms of balance
and trade offs and and, you know, sort of isolating systems of change from one another
and all the rest was hugely, hugely influential,
but kind of kind of forgotten that kind of got buried there.
So this kind of there's this little capsule into the kind of like
the kind of the zeitgeist of the 60s and design thinking of all this kind of stuff.
Conway gets mentioned, Melvin Conway and this influence on architecture of like,
you know what, the way that your people communicate, it's going to have a huge
exertive force on the structure that you build, how you communicate.
It's going to it's going to influence that because this is not maths.
This is we're creating a thing and and our choice of creation is going to be
influenced by how we talk to one another.
And there again, this gets multiple references throughout the software
engineering proceedings, which I think I think I think I think that's I think
that's that's deeply entwined in terms, in terms, embedded really in what
engineering really means. I must confess.
You know, I'm I love maths.
I enjoy, I actively enjoy maths and sometimes I solve mathematical problems
as a hobby, you know, but I don't think that what we do is maths.
I don't I don't it's it's how it appeals to mathematical thinkers.
But one of the differences between engineering and maths is the engineering
has that pragmatic bent.
You know, if if you could simulate an aeroplane, the design of an aeroplane
and wholly do that, you know, in a simulated form and just build the aeroplane
and then take passengers, you know, people would do that, but they don't.
They do that and then they go flying.
And certainly if you're Boeing, I don't know whether I don't know if they still
do it, but certain for a very long time, the engineers that built the aeroplane
were amongst the first passengers after the test test pilot went up to go for a
ride in the aeroplane that they designed.
So there's there's this thing of, you know, trying stuff out.
And I think that's one of the principles that I get a little frustrated
sometimes with people talking about, you know,
the mathematical nature of programming.
It's interesting. It's fascinating.
I like I like thinking in those sorts of terms.
But I don't think that's enough because I think it's usually harder to be able
to write something that's a provable system than it is to write the system
in the first place. And so it's almost more error prone.
So, you know, it's it's a complex problem.
I was I was just listening to the radio
today about.
Actually, I was listening to a podcast from New Scientist and they were saying
that they've just rejected one of the quantum,
the supposedly quantum computing proof
encryption algorithms because somebody managed to break it on their laptop.
And you'd think that'd be a fairly mathematical kind of
area of software.
So when we're building flight control systems or
or car control systems or even your stock control systems,
there's room for all those human errors and mistakes.
Yeah. And and and you know, Margaret Hamilton's little girl to come in and
and screw it up in new and interesting ways.
And I think that I think that's a really important thing because it's
because this kind of whole point about kind of perspectives of when we look at
things, how do we reason about them?
And that the distinction I made a number of years ago is that, you know,
software, there are lots of elements of it that are mathematical, but are not the
same as math, you know, but not mathematics as a distinction there.
So engineering is not mathematics, but it is mathematical.
In other words, it draws very heavily.
It uses it as a tool and that tool can also give us further insights,
but they are not an equivalence.
There's not, you know, and that's and that's a really important distinction.
And that idea of yes, but when I throw it, does it stick?
You know, that kind of stuff is the real thing.
When we actually, you know, yeah, sure, this works in the simulation.
But it's it's it's it's this.
It's like let's let's take it back to the 80s.
Aliens and, you know, being asked, you know, how many, how many actual how
many combat drops have you done?
Yeah. And then we get one answer.
There's like and then the follows on simulated.
It's just like, OK, there's a big distinction.
You know, you've actually, you know, this is your first time in properly in the field.
Is a big distinction.
So in other words, there's that whole kind of idea of like the math.
And and I think for I think for software, the term,
there's a lot of mathematics that is in bits.
There are things that are genuine mathematics.
There's a lot that is mathematical.
But the better way of looking at it is is formal.
Now, I don't want to get that wrapped up with all methods because that's clearly
an important subset, but it's formalized.
And I was it's there's there's elements that's something I've always found
fascinating is that you got the human element, which is definitely hugely
informal, sloppy.
We are not we are not formalized creatures.
We're very, very associative and then what developers have to do is bridge
the gap between this incredibly sloppy world that somehow has form and shape,
but is not necessarily rigid and prescriptive and with well-defined boundaries.
And then you kind of shift into the world of programs which have exactly opposite
nature, they are highly formalized.
It's a programming language is a formal structure.
There's no kind of like, well, maybe today I'll compile it or maybe you don't.
And if it looks like that, you know, you have a problem.
You know, but there's kind of there's a and what you've got to try and do is
build a system for the kind of the soft squishies thinking and soft squishy
beings out of stuff that is really quite different.
And guess the nature of these two bring them together.
I think that for me, that's one of the things I find fascinating.
But it's probably also for many people without them
realising is what's interesting about software development is there is the
rewarding aspect of some things that are solved and elegant.
And it's just like, that's done.
But then there's the other element of like, and how does it fit with the world?
Which is also quite exciting.
And also the discoveries that you make is just like, well, I thought this was a
really good abstraction, but now I truly understand what's being built.
I don't think that's the right abstraction.
That doesn't mean it's a bad abstraction.
It's just not the right abstraction for this system.
It's just now I understand how it's evolving through time and the kind of
the nature of changes that the client wants from it or the things we've discovered
from from sprint to sprint, it's just like, oh, OK, I keep touching this.
Keep changing it with that optimism.
Oh, I'll get it right this time.
But actually, actually, maybe I'm learning something deeper,
the fact that this is not the idea that I thought it was.
And I need a different point of view.
And that's that's not a side effect to an accident.
That's the nature of of the game.
It's this exercise, it's this continual exercise in learning in which we enhance
our understanding of the problem that we're trying to address
and the nature of our solution, a solution that we're trying to apply to it.
And and and it seems to that's one of the things that I very, very strongly
come to believe that that's a complete cornerstone of our discipline.
And we optimize to be able to maintain our ability to make changes when we learn
new stuff. So I refer to it as this kind of one of the ways of kind of
pragmatically, informally adopting the philosophy of science to software.
So I want to I want to consciously start out assuming that I'm going to make a
mistake and I'm going to be wrong.
And then I'm going to look at ways in which I can falsify my my guesses along the way.
And that's a much stronger way of learning than assuming that my design is perfect
and it's going to be right.
And I'm never going to have to correct it again.
I've found the one true way.
Yeah, I'm always reminded that this is years ago, but I had a client where I had.
I I've become a success of visits.
I've become familiar with the nature of the system and what they were doing.
And they one day asked me, we love you to design this kind of like subsystem.
And it's got these performance and strengths and stuff like that.
And we've got the suggestion for the basic idea of the design.
And I kind of said, I don't think the memory manager is going to like that.
I don't think that's that's I don't think that's going to work.
I don't think it's going to meet the performance requirements that you need.
I think it's going to be issues with it.
And I made a suggestion and I said, are you sure?
And I said, well, I think this is going to work better.
I think this will work better with memory allocation on this platform.
I think that for that, you know, you're dealing with peak.
They basically wanted to deal with peak demand in some way.
You know, we can't handle the data, but all we need to do is spool it off
so we can handle it later.
And I said, I think the way I'm proposing will work this way.
And then I tossed in another idea
because I'm not really happy until I've got three ideas.
So they gave me one.
I had I had a preferred one.
I didn't there's a worker and I had a preferred one.
And, you know, and and then I had a third one.
And I thought that one was OK.
I thought it was better than their suggestion.
I didn't think it was great.
And, you know, they gave me a couple of days, you know, they fed me coffee,
you know, gave me a meeting room or the rest of it.
But my favorite thing is one of the guys came in one day,
you know, the first or second day and he came in and he saw I had an idea
on my screen, I had code, they were killing brackets happening.
I said, oh, we didn't expect you to cut it.
And it's just that kind of looks at it.
It's just like, well, how do you think I was going to do this?
Sit here, you know, come up with the pure design, you know,
moment of life is like, I have designed it.
You know, I have the art.
This is the nature to whatever here is the word.
Here is the solution.
And it's just like, no, I'm trying each one of these out.
I want to see what it feels like in code.
And also I'm going to do some basic, basic performance analysis.
None of them are too big just to get a kind of order of magnitude feel for this stuff.
And and and I wrote it up.
And the funny thing is I wrote it up.
And I it's only in hindsight that I realized I'd written it up like an experimental report.
You know, here's here's the situation.
Here's what we've got.
Here's the various proposals.
Here's how we've run it.
And here's here's the results and recommendations for future work.
But what I found is that I I was right and I was wrong.
I was right.
Their approach wouldn't meet their requirements.
I was also right that my my preferred approach would meet their requirements.
But I was wrong in that my kind of like third throwaway option that was outstanding.
It was way ahead of me.
You know, and I would not have known that by meditating upon it.
That had to be made real.
That had to be brought into the world.
And to actually you to you also have to kind of mess about with it.
In other words, that the very act of and you mentioned kind of like solving
mathematical problems for fun.
And that's one of those interesting things is that I'm guilty of having done
similar things in the past and it's kind of fun.
But the thing is until you've done it, you don't know how you're going to do it.
You've got some ideas and you're going to crack away at it.
And in that sense, there is a sort of a creativity, you know,
it's it's mathematics is not necessarily empirical, but it is certainly creative.
I'm going to try this. I don't know.
And what about this one?
What about this?
You've got that and and software just pushes it that little bit further
to bring it into the world and say, well, yeah, but how does that work in the world
as opposed to in abstract space?
And that is a really important and that idea.
I think it's a really interesting one because what we're doing is we're bringing
together the idea of problem solving and creativity.
But with something that somebody else is going to experience and they're going to
work with it, some that somebody else is either going to be another developer
experiencing the code or it's going to be an end user experiencing what is this
system like? Yes.
And so there's a kind of a feedback.
You don't necessarily get that quite the same from something that is mathematical.
There's a kind of a sense there of is this appropriate for the world that we want
as opposed to, you know, yeah, this is this is fine.
It's a nice idea, but it's a case of like, what is its context?
You know, I can give you a picture of a house and I could ask you, is this a good
house? And and you could say, yeah, that looks good.
And then I say, well, here's the hill that I'm going to put it on.
You said, well, you didn't say you know, the context absolutely matters.
And I think that sometimes we kind of there's this kind of sort of maths envy
that sometimes takes over people and sometimes there's that idealism because
software does, you know, as I said, there's these two different spaces,
that the sloppy human one that is filled with economics and and and ill-formed
thoughts and the fact that the realisation that no matter what we do with any
development process, people always talk about prioritising requirements, stuff like
that. Humans don't walk around with a list of priorities that we don't actually
that that's not a thing that happens in the brain.
We don't have lists like that.
And so my wife.
I have, I have very organized, but I'm going to say that when people but
that's a thinking tool, that a list becomes a thinking tool.
Yes, when you provoke a human just randomly, they don't have a they have
to create a list and it's going to be drawn from whatever is available.
So that is an availability bias there.
Whatever is available at that particular point in time.
And unless they've already really thought through, oh, I'm going to use lists
like this, unless they've actually structured that in there, then that's
not the naturally the way they think.
Most people don't sit there thinking like, we want a product and I'm going to
think in terms of these requirements.
No, you're probably thinking in terms of other things that are your skill space.
And so when we provoke humans into I need a formal structure, give me a priority
list, that's not how they actually think, but they can learn to move towards it.
But that doesn't mean they're thinking genuinely like that.
And then we have this associative mess, which is also where all the creativity
comes from. And then we have this kind of hard edged stuff, which is very
uncompromising, you know, there's no negotiation with the compiler.
It's not a matter of opinion, whether or not this this works or not.
And then we're trying to do all of this.
So we've got all these different strands of creativity yet bounded by a particular
formalism. And so it's kind of like you need lots of different points of view.
And so, although, ultimately, I believe that it is all underpinned by a
perspective of engineering, in other words, and I think with software engineering,
it's not it's I put it, I did a very long time, very long time ago at
the go to conference, it was nearly 20 years ago, and it was an end note.
And it was Oh, no, it's the Joe conference at that time.
I hadn't called themselves go to yet.
And it was entitled Beyond Metaphore, where I looked at a bunch of metaphors
that we use in software development and and the whole value of them.
I sort of said, yeah, it gives you different points of view.
But I said, one of them is, you know, actually, what we do is is engineering,
but it's not engineering that has to worry mostly about physics and logistics.
We don't really worry about logistics.
It turns out that what happened?
What is it? What does engineering look like when you take all of that away?
You're still making trade offs and you're still doing a whole lot of things.
It's just that it doesn't.
You don't have to worry about the bridge materials.
You don't have to worry about, you know, the all the materials
you don't have to worry about it the same way.
There's a whole load of other things that just disappear, but that doesn't stop
it from being a discipline that is learning based, that is, in some sense,
pragmatic, but is also very trade offs driven.
Yes, that's a really, really important part.
You know, mathematically, we know when people talk about maths is
the trade offs are not quite there in the same way.
When somebody comes up with a proof and they can't quite prove it.
It's like, well, you know, close enough.
No, that does not pass the mathematical test.
Yeah, it's like the Fermat's Last Theorem, which is kind of fresh in my mind
because I interviewed Simon Singh a few months ago on some of his things
and he wrote a wonderful book on Fermat's Last Theorem, the history of that
and Andrew Wiles's proof.
Now, everybody kind of suspected an engineer would have said, yeah,
you know what, Fermat's right. It's close enough.
We can't find anything that's good enough.
We can't find an N for, you know, there is other than Pythagoras,
you know, other than A squared plus B squared equals C squared for A,
B and C being integers, you know, you're not going to find any other powers.
You know, there is no N that is going to fit that.
An engineer would have given up a long time ago.
They would have moved on to the next problem because I said, you know,
actually, we've done a plausibility analysis and really,
it doesn't look like there's anything there.
And given the time and effort, this is good enough.
And that's, in other words, there's a kind of a,
there's a stopping point and a trade-off discussion that happens there.
Yes, Glenn Vanderberg did a great talk about software engineering a few years ago,
and he says engineering in other disciplines, engineering is just the stuff that works.
And that's it. It's that mix between adopting a scientific style of rational thinking
to solve the problems where that's practical.
But it doesn't have to be definitive.
There's also this empirical little add-on that, you know, yeah, that's good enough.
You know, it's, and that seems important to me as part, you know, as part of the discipline
is to not expect kind of quantum physics levels of precision in engineering,
you know, in engineering, unless you're building something that's using quantum physics, you know,
you don't expect, you don't do that if you're building a car.
You know, you're more pragmatic than that.
And I think, I think that's one of the, one is interesting that the way that you kind of catch that
in talking about your, your talk is, is that it's engineering without those kind of the logistics.
I talk about it in terms of, I think our mistake is assuming that engineering,
because it's so popular in the real world, is production engineering that we're talking about.
So, you know, there are synonyms and they're not.
There's also design engineering, which I think is much, much closer to what it is that we do.
We're much less interested in those, the logistics of production because production is free for us.
Yes, yes.
Yeah, I think that's a, I think that is a really important distinction because it's,
it's one of those things when you zoom in and I think it's, it's, it's the, as you start zooming in,
you start realizing distinctions that are not necessarily, and that's, and I think that's,
that's both the strength and the weakness of any, of any word when we throw a word out there to say,
this is like this, this is this.
We probably have a fairly clear idea in our heads based on whatever our experience is,
but we've got no guarantee that the other person, the receiver, has the same mental model.
Yeah, it's, and that I think is, is a really, a really important one.
It's sometimes when, when you kind of like push the edges of those definitions.
So I think for me, one of the really interesting ones, actually funny enough,
and I wonder whether, and this is tied, I know this is just like a, you know,
I can't, I can't tell you, but basically the late 90s, I read to Engineers Human by Henry Potroski.
Wonderful book.
Now he's a civil engineer and historian and really wonderful book, but, but the subtitle is The Role of Failure.
Yeah, successful engineer.
And it's that idea of understanding things through failure, which I wonder if that ties into me.
My fascination is I'm going to take pictures and now other people say me pictures of failure,
but that idea that actually we can learn a lot by nudging a system to nudging a system beyond
what we actually understood, nudging it beyond our preconceptions, revealing our own assumptions.
It's just like, ah, and, and, and, and occasionally doing that on purpose.
But one of the things I'm, I'm currently obsessed watching SpaceX build their star ships in, in,
in Texas. And, and I've been following it for, for a while now, a little while ago,
they decided that they, they made an unusual decision of building their, their spaceships
out of stainless steel rather than aluminium, which is what spaceships were.
Originally they thought they were going to do carbon fiber, then they show,
they looked at the alternatives, they came up with stainless steel because it got a better
temperature range and for strength to weight ratio and all that kind of stuff.
But at one point they'd built these things, they'd, they'd flown some of them.
They decided they were going to move from four millimetre stainless steel to three millimetre
stainless steel. Same stuff, same, same type of steel, but just a thickness change.
You'd think that'd be the kind of thing that you could just do your slide roll in the olden days,
but running through, running through a computer and, and understand. But no, they built, they
built the system and then they, they, they pressure tested it to destruction to see how
their welds held up, how their designs stood up under that real and, you know, empirical load
in, you know, you know, life-like circumstances. See what happens at the point when it screws up.
And, you know, that, that's what real world engineering looks like. It seems, seems to me.
But that idea of like, you're going to, we're going to do this and try this thing out and then
see what happens. And there's a, for me, that, that's this idea that time is a really important
ingredient to what we do, which I think is really missing from a lot of, a lot of formalisms of
what is software, that it's the time full aspect, not the timeless aspect, but the time full aspect.
And I was, honestly, I couldn't tell you the answer to this until I built it and we've seen it for a
bit. I know it's not quite right, but it's plausibly in the right space. And, but I don't know what my
assumptions are, you know, and by definition, you don't know what your assumptions are,
because, you know, I always like to point out that assumptions are really weird pieces of knowledge.
They are, they are only ever discovered, they are normally only discovered in contradiction.
You know, somebody says something, you go, oh, but I had assumed that at that moment,
yeah, you discovered you had an assumption, you've had it for a long time. But if anybody
had asked you prior to that, what are your assumptions, you'd have said, I have none.
Only when it is contradicted, you go, oh, that's an assumption. So this is very curious,
from an epistemological point of view, this is really weird kind of thing.
And it's, it's the fact that you know, if you know, you know, it's kind of Lego bricks,
Lego bricks in the dark, you know, there is a dark room, I know there are Lego bricks on the floor.
The problem is, although I know that I have assumptions, although I know there are Lego
bricks, I can't tell you where they are until I've stepped on them. I have assumptions, but until,
but I'm not going to do that by just standing at the door, I can, I have to walk into the room,
I have to tread through, tread through, there's one, there's another one, there's another one.
You know, it's one of those things, you have to be deliberate about this, you've got to put that
stuff out there. And of course, prior knowledge can, can give you a real, a kind of a real leg up,
that's the standing on shoulders of giants, that's the cumulative experience. Now, why, why is it
that we are recreating the errors that previous projects have done? Yeah, we got, we got all
this experience. And it's, it's, we see this repeatedly at the level of individuals, companies,
and discipline as a whole. It's a case of one of those interesting things is like, yeah,
we all make mistakes. That's absolutely fine. We, we, we are always operating with incomplete
knowledge by definition, we're operating with incomplete knowledge. Software, as you said,
production is free. We've got, that was a solved problem in the 1950s. We basically solved the
elements of that and we've just been getting better at it ever since. But that whole idea of,
that leaves us with the hard problem of, and what is it we're trying to build and why? And how do
we, how do we do that? Yes. Which turns out to be surprisingly challenging, but is by definition
open-ended because we're not producing identical artifacts. I've got, I don't know how many of
these pens lying around my office. And they are all equivalent to one another. They are all,
except for the incontent, substitutable for one another. They are identical. That's because
they have a production, yeah, they have a production process that is designed to eliminate
variation. Yeah, we, we've done that far. Software is never like that. Yes. Yeah,
the software challenge is that if somebody comes along and says, I want something that,
I want that system over there running over here. Well, that's a solved problem. You know,
if some, you know, if, if I see one of my sons, if they show me an app on the phone,
I don't have to say, oh, I need to build that. It's just like, I'm going to go to the store
and get it for myself. It's downloadable. If somebody shows me a piece of code and say,
oh, that's really good. We now need to write that code over here. No, it's just a case of,
we've solved all of these issues, but we're left with that. We're left with the challenging issue,
which is not the production of the elimination variation, but the production of variation.
That's our job. When somebody says, yes, I want this system, but I want it slightly different.
I want to, you know, I want that thing our competitor has done. Well, that's different
because we don't have their code. That's for us. This is new. It's new to us. I want the old version.
I want a new version of the system. And so whenever anybody asks for a feature extension,
they're not just asking for a feature extension. They're actually asking for a new system.
It's the old system plus the new behavior. That's a new system by definition.
And that's one of the key facets of doing a good job is to be able to make that move
from the previous version to the new version easy. Yes. I am increasingly of the mind that
if you can't change your software, then the software is low quality. That is the practical,
pragmatic realization of quality in your software. I don't care about anything else.
You know, I don't care what language it's in. I don't care what, you know, if I can change it
easily and safely, then it's good quality. Yeah. And pretty much that. And there was,
and again, there were some of these sort of deep tools that, you know, things like modularity,
cohesion, separation of concerns, encapsulation, abstraction, those sorts of coot, godly,
those sorts of tools that allow us the freedom to make those kinds of moves when we realize, oh,
shit, we got it wrong. Yeah. The freedom to make the change. And I think that's really important
because one of the, I think for me, one of those insights or, you know, an emerging wave of insights
over the years has come from this idea, okay, we're always operating within complete knowledge.
So that means that whatever I'm building is in some sense wrong, although I think wrong is
sometimes there's too much, there's too much attached to that word. Your best guess so far.
That's my best, yeah, based on what I need. It's, you know, we did our best job. This is what we've
got, but now we've learned something from it, either because the world told us or because we
learned as a result, our own awareness of this. But what's interesting is you can derive a lot
of the ideas that we value, modularity, loose coupling and all the rest of it, from an understanding
of like, well, how would you build something if you didn't know everything? Yes. Here's the thing
I'm not sure about. Here's the thing I'm very sure about. You know, this thing I'm not sure
about, I'm going to really ram it in there and couple tightly to it. No, you were loosely coupled
to it because this is probably going to change. Yeah, exactly. You know, I'm going to isolate
myself in that. I'm not totally sure about it. I've kind of got an idea, but I want to put a
little bit of distance between this and this. And that distance is our dependencies, that
distances our interfaces, that distances. The idea is that all of this falls out naturally
when you start saying, well, you can actually, and this is I think is fascinating because it
runs along kind of an alternative axis. Sometimes they arrive at the same conclusion, but sometimes
they don't, to the traditional language of abstraction and things like that. How would
you modularise? Modularise according to abstraction? Well, there's multiple ways of
abstract. We have different paradigms for that. But what is interesting is going, well,
how sure are you about this? And it's just like, well, you know, we, oh, I'm pretty sure we built,
we've done something almost identical. It's not identical, but it's almost identical. Well,
that gives us maybe high confidence and this worked out well. That's the second bit.
People don't even forget that. Sometimes we get stuck in a rut. You know, it's just like,
we did this before and how did that work out for you? Yeah, you know, not very well, but we're
going to do it again this way. It's just an opportunity here. And that's that idea of,
you know, you know, it's good to have a few ideas that you can trade off against one another.
Yes. But then you've got that other idea of like, say, well, let's go through this in terms of
certainty. I've done this a couple of times with people and they're always kind of slightly freaked
out because you kind of come up with a rough kind of like sketch of what you're going to do and say,
well, hang on, but we haven't actually talked about all of the design detail that they normally
talk about. It's just like, well, yeah, what we did is we just drawn a bunch of boxes and lines
and things based on your confidence. In other words, when I've asked a question,
and you've said, oh, yeah, we're not really sure how we can do that. Right, I've drawn a line.
There's a boundary there of knowledge, because we're sure about one side, but we're not sure
about the other. I don't care what it is. I don't care what paradigm we're talking. It's clearly
something we're not sure about. So maybe we shouldn't hug it too closely, a little bit of
looseness. A little bit of a wall would be good. Yeah. And likewise, when somebody says, oh, yeah,
we're going to do it this way. And a colleague says, oh, I thought we were going to do it this
way. You know what, there's a line there as well, because it's clearly this is not settled.
And it may turn out that one of them is right one year, and the other one is right the next year.
In other words, things may change, whether it's performance characteristics or whatever,
but favor one and then the other. And again, that's the time for rather than timeless quality.
But that idea there that we can get a heads up just by actually almost constructively using our
uncertainty as a positive aspect to sort of see, well, how does this work? And then we've got the
empirical side of things, which is, okay, here's what kept changing every release, you know, what
are the hotspots? What do we keep going back and saying, oh, no, no, this time it'll be right,
we'll just add this here. And that kind of, I think one of the first times I ever really noticed
that question of stability properly feeding back was a Java system. It was a company that
was doing a Java system. And they had the debate about they were having the debate and are not
yet resolved it about checked exceptions. And for those tuning in who are not aware of this
feature in Java, checked exceptions basically allow you to make exceptions part of the signature
of a method, sort of checkable aspect of the signature. And it's one of those things that in
theory is a good idea, but that actually turns out in practice. If you don't know exactly what you're
doing, in other words, you don't have perfect knowledge, and you're building a large system,
they have a really nasty impact because they introduce an element that is unstable or rather
needs to be stable, but is not yet stable. And I was, how does this fail? I don't know yet,
because we haven't fully understood this. It goes right back to where we started. What are the
failure modes of this? A short of saying something trivial, like there is an error, and that's often
what these checked exceptions tend towards, which is throws framework error. In other words,
actually that's almost no use to anybody whatsoever. Bad things may happen. Well,
we knew that because bad things may happen. You end up either saying nothing at all,
or you say it so precisely that unless you've actually had this out in the field for a long
time and converged on that, the chances are somebody's going to come up with a new failure mode.
And it's just like, oh, well, so the curious thing is what you've done is that your happy day
scenario, what you want from the method, why the reason you called it, you don't call it,
I'm not calling this method, except perhaps in a test, I'm not calling this method in order
for it to fail. That's not my goal here. I'm calling this because I wanted a result. I'm expecting
that it's all going to work out. So therefore, right at the edge of my vision and edge of my
awareness is all of these failures, all of these possible failures, which we've not yet explored.
And as time goes by, we get more refined understanding. Oh, this could be distributed. Oh,
well, our distributed bit throws different exceptions to the ones that are those. So we
now made this thing, which was local, now it's, oh, okay, so we're going to have to change the
signature again. So we've suddenly made something. We've caused churn in the in the interface. And
this is one of those interesting things that comes out of the kind of the more empirical side
of things. It's not that nobody who put that feature in Java did so maliciously or without
thought. I certainly, you know, I certainly my understanding at the time, and all everything
being done. So no, there's some good thinking, solid thinking that goes right back to the 1970s
in exploring all of this, but it's the scale. What happens when you actually create open systems
that are large, and with all manner of developers, it suddenly turns out, there's a fundamental
problem here. And it's to do with rate of change and stability of knowledge, what we do and don't
know. And that was a, that was a revelation. Anyway, for this team, it was a real revelation
because they were split down the middle, you know, half or pro, half or anti, and I was just,
I can't come in and just sort of say, you know, it's going to go this way or that way.
So actually came up with an empirical approach. And I basically said, you know,
don't make anything checked, you know, look at, you know, before you decide to make something
checked as it were and seal it in, look at how it's behaved over the last few iterations,
write a test to simulate the failures as well.
Oh, well, yeah, not just, yeah, but the failures do all of those. And they were actually quite
good, but your understanding of the failures. Yeah, how you respond to it, absolutely.
Yeah. And it turns out that some of these kept changing on a frequent basis. And I said, look,
that idea is not yet stabilized. That idea is still young, you know, don't nail it in place. And
which is my polite way of saying like, actually, probably, you know, so don't make it checked
until you're sure, which is a fancy way of saying don't make it checked because the chances are they
weren't going to go back and review stuff. But what we had is at least a maturity model. And
it's this idea of that it doesn't matter what you think today, you're going to overvalue your
confidence. And so it's this idea that time will give you the answer. I can't tell you how this is
going to evolve. I can't tell you how other, you know, this goes across API's, it's not just about
failure. This is going to evolve. How is it going to be used? What are the things that are
frequently going to change? And then go back to your point about what we want to do is align
the structures of our software with what are the frequent changes we actually experience?
As opposed to, you know, people often pad their design or add complexity because they're saying,
oh, well, maybe this will change. It turns out that the better your imagination, the worse this
gets. So if you're an imagine, if you are creative and imaginative developer, you can imagine all
kinds of possibilities and the gold plating and the extra hooks and bits and pieces. And so the
more imagination, the less imagination you have, the quicker you'll get the job done. The more
imagination you have, actually, in that sense, it works against you because you think, well,
what about this? What about all of these are possible, but most of them are not likely. And
probably what you want to do is see, well, what actually happens with this? And that gets you to
ask the more meaningful questions like, well, you know, should we release this API yet? Or should
we release it and put a caution on it? It's just like, okay, this is a beta release. We're not,
no, we're not planning to support this. This is a beta release. This is for you to try. In others,
it makes us a little more aware, it makes us look up from the keyboard and go, how are people
going to use this? That gets to one of the things that I think is really important is just in software
development is always thinking in that broader context of, you know, how do people consume this?
Whether it's other developers or whether it's end users, how do people consume? It's that stuff
that you were talking about earlier about the interaction between the relatively
rigid forms of software and the relatively fuzzy forms of people. But ultimately, always comes
down to that, whether it's an API or some complex system that people interact with. It's so much
about being pragmatic and learning that. And it's one of the things that drives me nuts, working
with big organizations when they, you know, silo up the development process to the extent where you
get development teams who have no idea the context in which their pieces of software are used.
They have some kind of people giving them requirements in the form of programming by
remote control, which, you know, they're supposed to be able to churn out these widgets and they
don't have any context. And you get to use software that look, you can tell sometimes just by using
the software, nobody's ever thought about actually using this bloody piece of software. It's so bad.
Yeah. And that idea of usability, it's got, it's a turtles all the way down. You know, it's kind of
like, you have, you have the end user, but then also as software developers, we are clients of
our own products. We are clients, you know, you're creating, that is the classic kind of consumer
and supplier metaphor for understanding components and interfaces and so on. But, you know, how is
somebody going to consume this? But also, there's that contractual idea of like, and what's, what
am I going to say about how they should use it? Because everything has a boundary, everything has
a limit. And it's, it's that idea, because the overengineering issue, I've seen that where, in
fact, again, I can pick on Java, but I've seen it certainly in other cases. So when Java arrived,
it basically said, hey, everything is synchronizable, so you can make it thread safe. And I remember
thinking at the time, that's a really bad idea. That's a terrible idea. That's not how you do
this. Because I remember at the time, somebody showed me this C++ and said, oh, this isn't
thread safe. And I said, no, it's perfectly thread safe. You can pass one thread through it. That's
it. You know, if you do anything else on your head, be it, but I've just told you the circumstances
under which this will work. I have given you a context under which this will work. And that's not
me being picky. It's actually a genuine answer. Because otherwise, people do go around and they
start goal-plating everything. And they do so very badly. And it's just like, no, I don't actually,
the question is, this is thread safe. Oh, yeah, but you can't share it between threads. No, I
didn't say you could. It's perfectly thread safe. I can run it in one thread on its own. And that is
its safety level. Whereas there are some code that has a safety level of zero. In other words,
this is thread safe. If you pass zero threads through it, it's buggy. I can, I can, I can give
you a real world example of that, the danger of synchronization blocks in Java. Martin Thompson,
my friend years ago, worked on, at the time, we thought it was the first ever internet bank.
So he was called in to consult on this because they had a serious performance problem with their
Java implementation. It was one of the early big Java implementations in the sort of mid to late
90s. And, and he went in and several people had been to look at that and it turned out to cut a
long story short that, you know, they, they all, all of the tests ran and it all looked fine. They
put it to production and the performance absolutely tanked. And it turned out somebody put a
synchronization block around some piece of code in the critical path. So this internet bank could
support one concurrent user and everybody else queued up.
And that's the thing is that for, you know, people answer, oh yeah, but this needs to be
threatened. So what do you mean by that term? And it's like, why are you doing, you know,
there was, again, the context. So it's a case of like, not everything wants to be shared between
threads and there are other ways. And everybody was a lot happy in the 90s. And you finally got,
you know, it's one of the reasons I keep certain old books around is that you can kind of see the,
the shifts in, in style and approach. And, but I also remember with, with, this is a C++ system
that we went through and looked at their problem and they just sort of, it wasn't, they weren't
highlighting a performance problem. But I remember looking through, they've got this huge stuff in
memory, lots of data, lots of rows of data memory. And there's 30,000 locks. And I'm sitting there
going like, I'm pretty sure this is all the ways that you could do this. This is probably not the
right way. And it was one of those kind of like, take a step back and look at it and go like, well,
actually what you've got, you've, you've done it as a, a kind of a data centered problem with lots
of threads operating on the data. And I said, but if you understand what the threads do, I said,
they actually follow a life cycle. And that life cycle, we could do that as a data flow. In other
words, it worked out basically, it was a data flow, we basically, you know, yeah, you can,
you don't need anywhere near as many threads, you're stealing from yourself, it turns out.
And with a lot of this, and it turns out, if you do it as a pipeline, then we ended up with
only needing six locks. And that was in the bits that connected, you know, that's in the pipes.
And in other words, the point is the data, but they said, but the data itself is not thread safe.
And I said, yes, it is because it's environment guarantees that it's thread safe. That data
will only ever be accessed by zero or one threads at any one particular point in time. And that's
the game. And when we start looking at this, and again, this is this engineering to context
idea. It's when we talk about when we talk about car safety, and road safety, and all the rest of
it, we understand that there are conventions, rules of the road and contexts in which we evaluate
that. And the outside of that we make no, there's no guarantee. And that's, again, for my own,
for me, that light bulb moment I had when reading in the early days of patterns, actually reading
outside the kind of the software space going, ah, right, this is idea of context. Where does this
idea apply? Beyond which we make no guarantees. And that doesn't mean that it's a bad solution.
It just means that it is no longer appropriate or, you know, outside that context. It's a perfectly
fine solution for the thing that it was intended for. And that I think rubs up against a different
trend that sometimes we see developers, architects, and so on, is over generalization. The idea that
everything must be general. I think that one of the traps of our disciplines that seems to me
inherent in the nature of software is that we are often inches away from some quite deeply
complicated problems. Whatever the level of abstraction that we're working at almost,
as soon as you have to, it seems to me fundamental that as soon as you have two copies of information,
in separate places, that are changing independently, you've got a world-class,
first-class quantum physics level problem. You know, this is hard stuff. However, you know,
whatever the nature of the information, however is that you deal with it, working on high-performance
systems along with Martin building exchanges and trading systems and stuff. You know, we measured
the costs of concurrency, locks, comparing swap operations in processes and all those
kinds of things to try and optimize the performance of our systems. And one of the things that I've
observed is that the more that people know about building concurrent systems, the more
their advice is, don't do it unless you can possibly avoid it. This is incredibly difficult
stuff. So, you know, things like adding synchronization blocks and saying everybody
can now in Java can write threads or having thousands of locks in your C++ program are all,
seems to me, symptoms of not realizing that this is a point to stop and think hard because this
requires hard thinking. This is a difficult part. It seems to me that concurrency and coupling are the
kind of the really hard parts about discipline. Yeah, I think so because I think one there's the
because again, coupling is and what's interesting, it's interesting to draw those two together
because I think the interesting about coupling is that it's concurrency is hard for us to reason
about and conceptualize. Yes. Coupling suffers a different problem. But interesting, but both of
which I find fascinating, both of them manifest themselves physically in terms of concurrency
is about structuring things in time. But if somebody says, well, how tightly coupled is
this code base? I'll tell you what, let me do a build. In other words, you can actually measure
the energy of a build. And it's one of those things that I remember turning up at a particular,
it was an engineering project, electricity company, multiple companies were subcontracting.
It was a political nightmare. It was just pure Conway all the way through. But in the failure
mode. And it was a political nightmare. And all kinds of fascinating things. But one of the most
interesting things was as a development team, the team that I was working with, what we were
working on was very much back end stuff. It was much more towards the hardware. It was the real-time
stuff. But it was kind of like we felt like we came out of our cave to go and speak to these
other people. It's just, oh my goodness, this is absolute enterprise chaos. And then what was
funny is that because we were building for multiple environments, we were building for
32 and 64 bit environments, we were building for slow environments, as well as environments where
we had high-powered CPUs and as we had a frame where it was supposed to work everywhere. But the
embedded environment, oh my goodness, the builds on that were so incredibly slow that we really,
we cared about dependencies at such a level so that we had fast builds, which meant when we ran
out on a 64 bit platform, it was practically instantaneous. It was a beautiful side effect.
But then we encountered all these other people who were just doing all kinds of stuff with their
code. And this was like, I'm going to call it C plus most of what they were writing. Because it was
clearly using a C plus compiler, but I don't think it got much beyond C. But the way they
managed their dependencies all didn't. Everything depended on everything else. And the build
times were staggering and shocking. We ended up building an isolation layer between our company
and the rest as a result, because it's just like we got so used to fast build times on these
platforms. And it's just that idea of like, yeah, I can, you know, how good is your coupling? I can
either measure it in joules or I can actually time it. And you see, you know, it's kind of like,
you know, our builds take a lot less time than your builds, because we've got really, we've
stripped it right down. What is essential? So there's a physical aspect there. But it's not,
you know, and again, concurrency is this physical one. But concurrency would find difficult to
reason about because having so many things in motion is not a, it's not a strength of human
beings. A coupling is more a sense of scale is once we've, you know, it's that idea of like,
a large system is like really understanding what a tangle looks like, really understanding that
this dependency means that, and they are both limits, we are limited by what goes on up here,
but in slightly different ways with those two. But I think you're right, they are fundamental,
they are, they remind us, they remind us of some of the physics that we do encounter in the
universe. And coupling certainly entropy in the build. But concurrency is, is that point where
your idealization lands in the real world. And sometimes it reveals assumptions. I've certainly
had that with one client there. I remember the one one client, we did this kind of surgery style
thing, you know, I had a couple of days there, I ran some training, then I had a couple of days,
and people would book a morning or an afternoon, and kind of, I'd go with the tea. And I have one,
one team say, Oh, well, yeah, an hour of your time this morning would be great. I said, well,
no, you can have the full three hours. And said, no, no, we won't need that. They walked in.
And I remember asking a particular question, I said, you know, looking at the code, they were
going through. And I said, Oh, so how many threads run through this piece of code?
Because I was aware they were using threading. How many threads run through this piece of code?
And there are a number of correct answers to this. Zero is a valid answer, which means this code is
dead. One is also a valid answer. And many is also a valid answer. I didn't get that. All I got is
usually one. And I said, That's interesting. What do you mean by usually one? Why would you not say
that's many? As far as I can say, that's many. Well, what we have is we have a single threaded,
except, you know, this is single threaded code. Except occasionally, another thread will just
sneak into this bit here. Threads don't sneak into this work. And they have this mental model
of threading that was not actually how threading works. They had kind of thought that threading
respected the natural boundaries of the language and statements and blocks and things like that.
And they said, Well, it only happens occasionally. I said, Well, you know,
you only need to fail occasionally. You know, I said there's a race condition waiting to happen
here because you see you load this and then you validate this here. What if something else sneaks
in at this point and you've got an unvalidated boot? It's just like, and it's kind of like one
person looks at another and said, You know, that might explain this intermittent bug we've been
having. What should we do? Should we add locks everywhere? And I said, No, no, no. Actually,
what you need to do is take a step back here. The problem is not to add, but actually to sort of
say, Well, why are you doing this? What they were doing was a lazy load. And it's like,
Why are you doing the lazy load here? And they said, Well, we don't know. The reasons are lost
to time. But I said, because the problem goes away, if you do an eager load, if you do an eager
load before it goes multi-threaded, then the data you're looking at is actually immutable.
It's reference data. It's the load that is the state change. And I said, Let's do,
let's do the opposite way rather than add locks. Let's take a step back. I mean, honestly,
given enough time, I would have removed all the threads from this application. It was not a
threaded application. But, but it was a case of, you know, actually take the opposite view.
This is a question of time, you're doing the load at the wrong time, you're, you should be
doing the load before you go multi-threaded. And if you do that, then the problem solves itself.
But it's that shift in time and perspective. But my favorite bit, again, to do with time was when
that when the lead in the room said, You know, we might need more than that one hour, Kevlin, now.
Because when I said, Do you have code like this? Again, it's because it's not, and it's not to
criticize because that's the whole point. It goes back to saying, I said earlier, we are always
operating with incomplete knowledge, and we are built filled with assumptions. Yeah, until you've
actually run into those, you don't realize what you're missing. And you, and you don't realize the
magnitude of either how well you've done something or actually how wrong you've understood something.
So, oh, actually, no, I'm using completely the wrong mental model for thinking about this. And that
mental model has informed how I've structured the software, you know, the software is kind of like
applied thought. And that mental model, it's off. And we, so that's the squishy human bit, that's
the learning bit. But it's also the bit we need to be more, we need to sort of say, yeah, we need
to have a bigger process that is tolerant of the fact that we are imperfect. And we can't know
everything. And that's the whole point. This team had not really interacted with that and had not
accommodated that idea at that level. And most teams, I don't think have, I think it's a very
difficult thing for us to do. It's almost against the culture and the nature of software development
in many companies. I think you're absolutely right. And forgive me, bringing it back around to
my stuff. But I think that's one of the things that treating this more like an engineering
discipline ought to be able to give us. He's just those disciplines of just being able,
just recognizing that we don't know the answers when we're starting out, recognizing that we're
probably not going to be right. Therefore, working more experimentally, therefore, working to control,
manage the complexity of the systems that we build, and to measure things and to try stuff out.
And all of those sorts of things, test-driven development is certainly part of that for me
deeply. But I think that mindset is so important. One of the other kind of deep properties, it
seems to me, of software is that, unlike lots of other things, it's actually very easy to start.
You can learn to write your first simple lines of code in a few minutes. If you've done a little
bit of algebra, at least, it's trivial to just do your first easy, trivial bits of code. But it's
deceptive because you don't go very far before you get into some of these more complicated things
that we've been talking about. And as you say, as soon as you start thinking about things like
concurrency, that's really hard for the best people in the world. It's one of those things
Martin Thompson, they're world-class experts at some of these stuff. But they're still,
that they think really hard and worry about shared data at any point and all these kinds
of things, to be able to manage this sort of stuff. It's an interest. I think that's one of
the beauties of it. It's a challenge of it that's delightful. But also, very risky. I've been talking
to a few people recently about low code solutions. And I think that my take is that many of those
sorts of systems suffer from that kind of failing because they assume that it's the almost, that
it's the typing of the code that's the hard part, where it's these broader design concepts and how
we organise the information in ways that we can make a mistake and come back to it in future
and correct it and identify the mistake. That idea, yeah, the identification and the recognition
of how things, as you say, it's deceptive. Things can get very messy very quickly. And we see that,
I've given a few partly as a result of failure screens. I've given a few talks on software,
failures in the nature of failures and what contribute to them. But one area of enduring
fascination for me is spreadsheets, which I find absolutely fascinating because it takes a good
idea and implements it incredibly badly. In the sense of vision, people find grid forms,
it's very, very intuitive. There's, you know, I mean, what kid doesn't like square paper,
you know, this kind of stuff. It's like, we like laying things out in grids and tables and all the
rest of it. This is incredibly intuitive. And it's a very simple. To be fair, that might just be you
and me. Yeah, this might be a conversation. Obviously, anybody else in the comments is free
to add in. But, you know, this whole thing is incredibly intuitive at that level. But the problem
is there are two very fundamental issues that scupper spreadsheets and make them massively
error prone. One is, well, three, let me let me raise that to three. Okay. One is the fact that
you end up with a lot of interdependencies very, very quickly. If you're doing anything that is
reasonable. And then the next bit, and they're all invisible. This is the one and that this is
this is the one thing that as a software developer, you kind of look at spreadsheet and go, well,
that's a nice start. But where's the button that I press that shows me all the dependencies
between everything you've hidden all of the code, you've hidden a bit that actually makes it
that shows me the structure and therefore reveals my assumptions. You actually understand.
And there's no real mechanism for me to step back to safety when I screwed up.
Yeah, well, yeah. But there's that idea that we've lost the bit. In other words, what we've done is
we presented the veneer and treated that as the whole was no software spreadsheets have a deep
structure. But I can, you know, honestly, even just using something like even even a word document
has better structure, structure tools than Excel, you know, in Excel, I can go in and I've got I've
got the grid. That's it. You know, that I'm done, you know, that that is my abstraction. That's the
level at which I'm invited to think the code is fragmented and scattered around the relationships
are thrown to the winds. It's a matter of detective work and archaeology to recover them.
Whereas a whereas when I work with a document, and this is, you know, this this is true of many
different editors and word processors, I can get a high level structure, I can do an outline
structure, I can say show me the dependencies, show me the cross references to this kind of stuff.
In other words, it's spreadsheets are absolutely, you know, they are an absolute mess. And I always
say this and somebody says, Oh, but users find them intuitive. They find them intuitive to use.
But they it's like walking it's like walking to a minefield. You can walk into minefield
incredibly easily. The problem is, we have been depriving and I think this is, you know,
actually, I'm going to, I'm going to push this one right back to the profession here.
We've been depriving people of the things that we know. Because we know that a spreadsheet is a
test, it's got a terrible type system. It's astonishingly bad. And if anybody ever throws
up the argument of backward compatibility, that is absolute nonsense. We've actually seen formats
for documents change on a five to 10 year cycle. There's no backward compatibility issue here at
all with things like Excel. That's a myth. You know, you're looking at a five year window at most.
Explain to me why it is that my my Excel looks like it was developed 30 years ago,
but everything else that I'm using looks like it was developed in at least the last decade.
Why? Why is Excel failed to take on board all of these other tools that we know to show
dependencies there? And the third thing to come back to is that people don't realize that
there are these issues. So they therefore they scale up very, very, very poorly. They
they embed many mistakes. Now, the point here is what reason I'm riffing on this is because
you talked about the low code stuff. Breadsheets are by far and away the world's most successful
low code to no code solution. And anybody who hopes to rival that is just kidding themselves.
I'm going to say that right now. You know, I'm not predicting the future. It's just like
those here are very particular strives. They have embedded themselves in a particular way
in a in a particular world. And they've and they're very current. They are ubiquitous.
But if anybody wants to learn how to do and how not to do, learn from its successes and learn
from its failures, and what are the things that it's good at and not good at. And then also go
back through the history of 4GLs. And what you will discover is that you what there's a there's
a line that you're looking to draw. And you need to understand that it's not universal. There's a
line here you need to work out where it is. It's like this is the bit that allows people the convenience
they want. And then there's this other bit that's incredibly hard. If you're if you're assuming
they can do you're going to they're going to be in for a big and nasty shock. And we're just going
to be throwing more stuff over the wall at other people for Oh, we need to customize this all.
Oh, this is something that we knocked up. And you know, but we're having a couple of problems
with it. And then kind of suddenly yesterday, you know, it's just like, at this point, they
suddenly discovered that they are in fact an Olympic run up, because that's the only way they
can get away from it faster. That we are if we're not doing this one right. So I don't have any
grievance with low code. It's just that when people talk about it as a general solution. No,
what's value? Yeah, a specific solution. Yeah, it's value. That's the value that a narrow narrow
constrained focus. Yeah, that's really good. That thing. Again, it goes back to this question,
this quest for generality that we sometimes have, we over generalize make things.
Either we ended up over generalizing make things ridiculously complex for ourselves.
Or we end up not over generalizing and forever working around taking something that really
wasn't that great. It's that it's that it's that really, really, you know, shades of gray,
kind of slippery slope over a snake pit. Yeah, yeah. Mixing my metaphors horribly. But
but you can't you kind of go from, you know, I'm adding up a column of numbers in my spreadsheet.
Cool. That's really nice. It's really good for that to, you know, I've built this thing. And if
I change that, it goes and recalculates all of these other things and does all this, you know,
which is an unmaintainable big ball of mud. And, you know, there's, there's no,
there's no easy way to define the line between between where you step over and it becomes
this is the real Lego bricks. Yeah, absolutely. You know, you it's just like you tread carefully,
put push push your foot out just in front of you just to and I think that that is the again,
that feeds back into this idea that what we're looking for is is an approach,
a philosophical approach, but a practical approach. I mean, you know, I'm definitely
of the school of thought that I guess old school philosophy, in the sense that philosophy was
intended to be a practical thing to help you understand life. It was not intended to be
abstract and disconnected from life. It was intended to be quite the opposite. So for me,
this idea of actually what we want from how we think about software, we need to we need to
understand is like, yeah, it's a bit experimental. There's, there's, there's things that are unknown
and actually not only is that okay, but that's actually part of the job. It's not just acceptable.
No, it is the job. It is the job. It is the job. It is the job. So, so, so I've just become aware
of the time we've, the amount of time we were talking about time and now we're aware of it.
It's been so much fun having the conversation, but let's try and let's try and ram this off.
So, so, so if you, if you could, if you could summarize this, what's, what's the advice,
what do you think is the advice that we should give people to help
walk this, walk this tightrope, explore, you know, walk into the, into your room of Lego
with bare feet. Yeah. Well, I guess, I guess the simple one is runaway.
That's the only, I think it's really to, to understand that what the challenges that you are,
the challenge and the joy, let's, let's, let's, I think we're all, we're always
putting up challenges. I think we all need to accept that sometimes the challenge is the fun bit.
You know, it has two, two aspects to it. The challenge and the fun is that you are working
with incomplete knowledge. There is a joy to learning something and there can also be a joy
in discovering better solutions, alternative solutions, penny dropping moments where you go,
you know what, I've been thinking about this wrong and you know, yes, I've over-obstracted.
This really is just a string. The abstractions I wanted to do are over here. Those are the
points that I love those run. Oh, shit. I was thinking of it wrong and now I can see a new,
new path. That's the thing. And it's the case of like, or I've been devoting my effort to this,
but actually the fun is over here. If I reframe the problem, in other words, it's that idea of
take a step back. And I think a little bit, a little bit of something, I keep reading to do
a talk called slow agile. I think I'm probably going to do it at some point. But it's this idea
that although we often use the language of fast in connection with many of our practices,
I think sometimes there's a different emphasis I might want to give, which is
that some of what we're trying to do is to do things sooner, as opposed to faster, which is
not quite the same thing. Yes. I use the phrase. I think that might what you might be saying is
the phrase that I use is small steps. Yeah. And small steps are a way of achieving it.
And I was just talking to a group today about refactoring. I was trying to
emphasize to them, you know, in terms of all the design practices and so on. I said,
there's a difference between running and walking. And there's a very simple idea
that running is defined by the fact that at various points, you have zero points of contact
with the ground. Yeah. If you watch somebody running, it's 0101. If you watch somebody walking,
it's 1212. There is always at least one point of contact with the ground. And now what does this
mean? It means you move from moment to moment more slowly. But you are also moving with more
certainty and more sureness and the ability to change your direction. If you try changing your
direction when you are running, or when you stumble when you're running, and I have relatively
recent personal experience of this, and I can say it hurt an awful lot when I was not able to walk
for a few weeks. Whereas had I had that fall when I was walking, I'd have just got up and walked off.
The point there is that software development is not a race. Although we use the language of fast,
the time scales we're thinking of, sometimes it encourages the wrong behavior, I think.
It's again one of those things where what I use a word and somebody else picks up,
oh, they're talking about raw speed. Actually, we're not trying to optimize for
speed of development. There's other characters. What we observe is the speed of development,
or the speed of deployment. But the thing, it's not the pedal to the metal. You're just going
to exhaust all your developers and exhaust their capacity to think creatively. It's the idea that
actually what we need to be doing is walking. It is the idea of stability. It's the idea of like,
oh, that's not right. Let me just pull back a moment. It's the small steps. Let's roll back
and take a different path. Whether that rollback is a version control rollback, whether that
rollback is a conceptual rollback, whatever it means is the idea that we have given ourselves
the opportunity to pay attention to what we're doing. When you are moving at high speed,
you're not paying attention to your surroundings. All of this talk of feedback just disappears in
the wind. The whole point is you are sensing your way and that you are adjusting according to that.
It's your feet on the carpet type thing. It's the sensing your way. How are we doing? Is this
that I originally intended this, but now I see this? Why? Because I can, because I've taken
the time to do that. I'm not individually, as a human being. Remember, it's all about people,
ultimately. For me to appreciate which formal approach I'm going to take, which structure,
which choices, which modules I am going to select or my criteria for modularity, which ones I'm going
to do here as opposed to there, that's going to take a deliberation that doesn't happen
when you're exhausted and running at speed. It's not a productivity conversation. I think, for me,
most of the advice I give people is just like, honestly, go a little bit slower because you'll
go faster. That's great. I hadn't thought of it in those
words, which is always interesting and always an interesting thing, but we're still reinforcing
my prejudices. Good both ways around. He gave me some new things to think about and reinforce
my prejudices, which is great. I've really enjoyed the talk today. Thank you so much for taking part.
Thank you, Dave. It was a fun exploration. Please do check out Kevlin. Check his Google
unique name. You'll find his stuff and lots of good books that Kevlin's written as well.
Thanks, Kevlin, very much indeed. I'll let you know when the video is available. Thank you.
Thank you very much, Dave. That's brilliant.

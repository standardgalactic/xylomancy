Welcome everyone from and hello from Rainey, California.
Today we're hosting an exciting event
on the future of General SDI.
Our guests today are professors
Christoph Fondrell-Malsburg and Michael Levin.
Michael Levin is a distinguished professor
at the Department of Biology
and the principal investigator at the Levin Lab
at Tufts University.
Christoph Fondrell-Malsburg is a senior fellow
at the Frankfurt Institute for Advanced Studies
and the visiting professor
at the Institute of Neuroinformatics at ETH Zurich.
And finally, our host today is Dr. Yoshabach,
my colleague and the principal engineer at Intel Labs.
My name is Tania Greenberg.
I am an AI scientist at Intel Labs also.
We would like to express our thanks to Intel Labs
for sponsoring and hosting this event series.
So without further ado, let's get started.
The audience is going to be muted by default for this event.
If you would like to ask a question or make a comment,
please post your thoughts in the chat.
You should have access to it.
We will have a hopefully longer than usual Q&A session
in the end of today's session.
So we should be able to go through many of your questions.
All right, Yoshab, the floor is yours.
Okay, I've proceed with our slides for now.
Let's see.
The question that we want to discuss
is how we can build intelligent systems
that move beyond the present limitations of deep learning.
And this is quite a bold proposal
because you don't know if deep learning has any limitations.
At least I don't know.
I don't know any proof that demonstrates
that there is a certain thing
that the methods of deep learning cannot do.
It seems to be that the present generation of models,
which is widely successful,
generative AI is built on the transformer algorithm,
which is a variant of the deep learning mechanisms
that have been existing since the perceptron.
And the limitations of this thing are not clear, right?
We basically get to meaning in the limit.
We need to use massive data and compute to get there.
And these models have difficulty to become fully coherent,
but it's not obvious that such a limitation
cannot be overcome as a loss function
or this just using the existing regimes
and scaling them up even further, right?
We cannot prove that there's a combination of codecs
and the existing methods and some real time
and some online learning.
We won't be able to build a system
that is a good enough artificial general intelligence
and is able to discover the next generation
of algorithms for us.
And this idea that deep learning
in the way that it exists
with basically the algorithms that we have right now,
the slight changes to the loss functions and so on,
that it is sufficient.
This is called the scaling hypothesis.
On the other hand, most people who work in the field
have the sense that there is a problem with deep learning
in the sense that it's boot forcing the job.
It seems that biological organisms
are able to make sense of the world
with much, much less data and dramatically less compute.
And there is of course a contention
about how much compute a brain has
and how much compute an organism has.
And very often the calculation is made in such a way
that we discuss how many GPUs you need
to emulate a group of neurons.
And depending on how you look at the neuron,
the neuron might be something like a four or 12 layer
neural network or it might be something even more complicated
if you look down to modeling every synapse and so on.
And on the other hand, relatively rarely we ask
how many brains you would need to run macOS
because the way our nervous system works
is highly redundant and stochastic.
And much of what the individual neuron is doing
is probably only relevant to the individual neuron
in its survival itself.
Because after all, the neuron is a single celled animal.
So if you take a job of an individual,
for instance in a corporation,
that individual is going to contribute
a lot of its intellectual capability to the corporation,
but it's going to be a tiny fraction
of the total ability of the individual
that it needs to survive by itself
and to communicate with its immediate neighborhood.
So just asking to emulate a single neuron
and then multiplying this with the number of neurons
is probably not the right way to do it.
And it's not clear how many neurons you would need
to run MNIST as far as I know,
there is no simulation so far
that is using a close neural model
and or that is using a small group of neurons in vivo
that are being trained to do MNIST.
More generally, I think we need to answer the question
what intelligence is.
And for me, this question has evolved over the years
and my current answer is that intelligence is the ability
to construct a path through a space of computable functions.
It's a slightly fancy way of saying
that intelligence is the ability to make models
because at the end of the day,
models are computable functions
that are designed in the service of control.
So we have a system that is making some kind
of regulation task or performing some kind
of regulation tasks and when it's controlling the future,
it's an agent and to control the future
and being an agent, you will need to construct
some kind of control model that is counterfactual.
You need to be able to represent states
that are not here yet because the future is not there.
And that means you need to have a system
that is able to perform arbitrary causal transitions.
And this causal insulation from the substrate
to represent something that is not the case yet.
That is basically what is a computer.
And if what the computer is does,
it represents functions which means mapping
from state descriptions to other state descriptions.
And the computer can do this in completely arbitrary ways.
And this is the basic idea of building a system
of computable functions.
And when you come up with a way to enumerate
the computable functions, to list them all,
then you can search them.
And if you want to learn something,
you need to in some sense enumerate,
organize the space of computable functions
in such a way that you find those functions
that you are interested relatively early on.
And this question of how you can construct a space
for the computable functions that is tractable,
that is able to converge to a solution
for the problem at hand.
That is the main issue, right?
Making a computer is relatively easy,
especially if you have something like a cell
because it's already contains a computer.
And they can also organize themselves
into higher level computers and become less stochastic
and so on while doing this.
But the big difficulty is how can we find the functions
that we are interested in?
And so when we look at deep learning,
what is deep learning?
The first thing that we have to note about deep learning
is that deep learning is the only thing
that currently works at scale.
It's the only class of algorithms
that is able to discover arbitrary functions
in a reasonable amount of time.
And reasonable being orders of magnitude more time
and more data than a human being does.
That's of course the training time for something
like diffusion stability AI model, right?
This stable diffusion is a model of two gigabytes
and it contains the whole of the art
and you can get every celebrity you want.
You can get every spaceship for popular culture.
You can get dinosaurs.
Everything is in these two gigabytes.
And training this takes weeks.
And this sounds that's very little compared
to the years that it takes to train a human brain.
But during these weeks and now it comes down closer to days,
this thing is going through hundreds of millions of images.
Many more than a human being could process
and the lifetimes and find correlations between
using dramatically large server farms.
So how does deep learning work?
First of all, it's differentiable computing,
which means it is representing all the functions
in such a way that they form a continuous space
in which neighboring variations of the functions
still lead to interesting results.
And you can move by a small nudges through the space
of functions to get the function closer to what you want.
That's different from discrete programs,
program and source code.
If you change a few bytes in the source code,
the program will no longer work.
If you change the neural network by a slight bit,
then it's still going to give you useful results.
And to do this, the neural network is describing
the functions via weighted sums of inputs
that are arranged into chains, basically in layers.
And some non-linearities, which are in a way to make
ifs, thens, to make conditional rakes in this network.
And this description via multiplications and sums
is sufficient to represent all the functions that we want.
And we train this network by setting it up
in such a way that it has the potential
to build enough things that has enough links
between the nodes that it sums values up.
And then it just changes the weights,
which means the multiplying factors
for the inputs of every node in a systematic way.
And it, in some sense, approximates
all the functions via real numbers.
And if you think about alternatives to deep learning,
the main thing that comes to mind
is to build up computations
from deterministic discrete operators.
Such as Boolean logic or simple automata,
like cellular automata.
And as a result, we get finite Turing machines.
And in the finite case,
dynamical systems and Turing machines
and automata are the same thing, computationally speaking.
They're all Turing machines,
as long as you don't run out of resources.
So every digital computer in reality
is a dynamical system because the physics
that the digital computer supervins on
is somewhat continuous
from the perspective of the individual transistors
and so on.
And we just try to find a region in physics
that makes the transistor reliable enough
as a discrete unit and deterministic
from the perspective of the logical language
that is implemented on the arrangement of transistors.
But underneath, there is an analog system
that is just noisy and that is a system that is discrete.
On the other hand, every dynamical systems
and physics at the lowest level is discrete again.
If you zoom in, what you see is nothing continuous.
What you see is individual atoms
and individual charges and so on.
And they are discrete again.
And there is some discussion about
whether everything has to be discrete at the level
because of the nature of languages itself.
And I think that's the case.
I think that the discovery of the last century
that was most important in philosophy
is that those languages which assume
that the bottom most layer can be truly continuous.
They run into contradictions.
But this only matters if you are really interested
in modeling the bottom most layer.
If you just think about computation,
it doesn't really matter if you start out
with a dynamical system or this discrete system
as long as you are willing to allow
that every dynamical system has only finite resolution.
So there's only finitely many bits
that you can manipulate at any given moment.
And this equivalence between the continuous mathematics
and the discrete mathematics has been shown basically
in both directions.
You can use a computer.
And then just by using more bits,
you can approximate continuous system
with any degree of fidelity as you want
in the same way as digitized music
can sample the space of audio functions
below the level of resolution
that your medium can provide.
So you will get to something that is equivalent.
And the opposite direction has also been shown.
Greg Chaitin has gone to the trouble
to translate a LISP interpreter
into a diophantine equation with 17,000 variables.
Right, so this paper is called
the Complete Arithmeticization of Evil.
It's the evaluation function of LISP,
which did in 1987, it's quite beautiful.
And I don't think that it's something people
actually want to read.
And this formula was generated
with some generative procedure.
He did not write these 900,000 characters by hand
that went into this, but it's been shown.
You can write down the LISP interpreter
in continuous mathematics.
So the alternative to deep learning
might be to basically construct functions
from the ground up using automata.
And in a way, this is also what people have done
because all our computers on which we run deep learning
are discrete automata.
People started to build this continuous arithmetic,
they built end or addition, multiplication and so on,
from discrete logical units.
And this is clearly practical,
but it was done with relatively few people
and relatively few years.
The search process to build up continuous arithmetic
from discrete logical operations is not very large.
And if you want to get to discover a deep learning algorithm
as a special case over discrete automata from scratch,
it's the existence proof has been shown by the computers
and the deep learning mechanisms that we have
because relatively few people use relatively little brain power
compared to where you want to get to
to discover all these solutions.
By self-play, you could in the same way
as computers played a goal, discover arithmetic
but using discrete systems.
So in many ways, this is going to be equivalent
and I suspect it might be interesting to start
from this automata direction again and build up learning.
And there are a few people which work in this area,
but so far nobody has come up with an alternative
that scales up in the same way as deep learning.
And then there is a slight, the different approach
that we might be looking at.
And it doesn't use a normal Turing machine,
but a non-deterministic Turing machine,
a multi-phase system.
And that is because systems in the lowest level of physics
and I also suspect systems that are implemented
on brains, non-deterministic systems.
And this doesn't just mean that they're noisy
or that they're random in many ways.
A non-deterministic Turing machine is a paradigm
from computer science that describes your state machine
in such a way that not every state
has exactly one successor state.
It's not sufficiently constrained
to have only one successor state.
Our Turing machine that we normally use,
and this includes our digital von Neumann computers
and so on, they're defined in such a way
that every state has exactly one possible successor state.
If there is a branch in the computer,
that's because it is depending
on some environmental variable that is not relevant
in the program, which means some input of the program.
But given the same input,
the Turing machine is going to produce the same output
and it's going to do this along exactly one path.
And not just Turing machine, the computational sense
is also going to give you the same output,
but it's going to do this on many paths
because the constraints are not so narrow
that you go into one state after every state,
but you can go into multiple states.
And the non-deterministic Turing machine
just goes into all of them, branches.
And these branches don't necessarily meet again, right?
There is no way that they're connected again.
It's basically a non-deterministic Turing machine
is implementing some kind of multiverse.
But it turns out that many of these multiverse branches
that the non-deterministic Turing machine goes into
have the same bit combination in them
because there are only so many states that are reachable.
And so by looking at the dynamics between these bits,
you can get to the same point in the universe
on multiple paths.
And if the universe that we live in,
the physical universe that we live in
is such a multivase system,
it's still going to be possible for observer
that lives inside of it,
despite it not knowing which path it goes down,
determine statistical properties over the regional paths,
which means you cannot know in your universe
which slit the double slit experiment,
the photon goes through,
but you can predict the patterns
that many, many photons are going to make on the other side.
And the things that we can predict in our universe
are of that nature.
There are statistical properties
over all the trajectories that can happen
that we are a part of.
And how would something like this look in the brain,
like in the brain?
Obviously, the brain can only hold a finite amount of state,
but if you have redundancy, what you can do is
if you don't tell the neuron to go into one particular state
as a result of its present state,
but into a range of possible states.
And this happens randomly.
It means that there are many trajectories
that activations can take in the brain at the same time.
And they're going to meet on the same physical substrate
and they're going to accumulate
and you're going to sum up in some sense.
And it could be basically some voting over the different paths.
And this means that a system like the brain
could via transmitting activation
try many, many things in parallel
and stochastically with some degree of randomness.
And as a result, perform computations efficiently
that can not be efficiently done with a linear machine.
From the computational perspective,
if you want to simulate this on a linear,
on a sequentially operating machine,
it's going to be highly inefficient
because you're trying to do the same thing over and over again.
And this randomness is going to delete some of the bits
that you've been computing.
But if your computational units are very cheap,
like in the brain, in the brain time is expensive
because taking one more step means
that you're going to be slower
in your interaction with nature.
But parallelism is cheap.
Just by doubling the cell count once more
while dividing the brain cells,
you get twice as many elements.
If you do this quite a few times,
you end up with billions of elements.
And these billions of elements can perform
many of these paths in parallel.
And this is something that to my knowledge
has not been seriously attempted yet
to get to work for a learning system.
There are some people which are thinking seriously about this.
For instance, we had Jerome Buzemeier on a panel here.
And he is describing the mental representations
as superpositional states using such a multivase system.
And it's also an idea that has occurred to Steven Wolfram
who is thinking of the universe as a multivase system
and who's open to the idea
that the brain might be basically
a bounded complexity multivase system.
So this we see that there are opportunities
to build solutions that are inspired by biology
that we haven't tried yet.
And if we look at the inspiration that happens
so far from biology into artificial intelligence,
despite many claims to the contrary,
it almost never happens.
I think that the last contribution of biology
to the transformer was heavy on learning.
And everything else that people got to work since then
was mostly not happening
because people looked at newer science results
and implemented them and then ended up
with a better machine learning algorithm.
And if you take the idea is that newer scientists
currently have about how the brain works,
to my knowledge, you cannot implement them
in such a way that they learn and control an organism.
Even a very simple organism like C elegance,
if we take the conic term of C elegance
and translate this into a computer model and run it,
it's not going to produce coherent warm behavior, right?
C elegance is a small warm
and it has only 300, I think 309 neurons.
Please correct me if this number is off.
And as a result, if you take this conic term
and run it into a digital simulation,
it's not going to produce the behavior that we want
because presumably we have not caught up
on all the functionality of the individual neurons
in the context of that warm.
Could be that there is stuff in the sum of the cells
that is not visible in the conic term
or that we made mistakes in digitizing
the conic term or I don't have enough resolution
to see all the vesicles that use different neurotransmitters
in the conic term to get the functionality right.
But for systems at scale that go beyond an organism
with a few hundred neurons,
something like cortical column and so on,
we don't really have working models at the moment.
The descriptions that the neurobiologists have at the moment,
if you translate them into computer models
are not performing the same things
as our digital models are doing.
They're not able to discover these functions.
It's because these models are still incomplete
and they're not ready yet for being used.
There's a more fundamental problem
that Konrad Hauding has highlighted in the paper
where he used the methods of neuroscientists
to reverse and engineer a micro processor.
If you take a normal microprocessor
and give this to a neuroscientist,
the neuroscientist does ablation studies
and looks at the structure that the neuroscientist finds.
Is the neuroscientist able to reverse engineer
this microprocessor, which is dramatically simpler
than the nervous system, of course?
And it turns out that Konrad Hauding
who is a neuroscientist,
that is the methods of neuroscience
might be for some more deep reason
because they're not functionalist enough,
able to discover how information processing
in the nervous system works yet,
which means that the theoretical tools of neuroscientists
might not yet be ready even
to understand what brains are doing.
And I think that when we as non-neuroscientists
look at brains and at the textbooks of neuroscience
that take what I learned at university,
I find from my current perspective
that there are some shortcomings in this description
that if I want to sit down as a computer scientist
and discover the space of possible solutions
in nervous systems and for functional approximation
and just get a machine to search through it
and so on would be insufficient.
And this starts out with the idea
that a neuron is a specialized switch
at the way in which we abstract the neurons
since the perceptron is that the neuron
is some very simple gate or something element
like in a circuit that does something
in a mechanized automatic algorithmic way.
And I don't think that's really true.
A neuron is not a specialized switch.
A neuron is a single celled animal.
It's quite complicated.
It wants something, it wants to survive.
It is able to learn by itself.
It is adaptive, can do a lot of things.
It's almost like an amoeba that links up with other amoebas
and grows into this static structure
depending on its environment to perform a particular task.
And this is quite different as a perspective.
It means that your neuron is not just some kind of integrator
or some kind of weighted sum of real numbers
of activations that come in.
The neuron is really a reinforcement learning agent
with a little bit of memory
and a little bit of ability to look into the future,
not very far, but a little bit.
And it can implement a number of adaptive functions
to deal with its environment and reap rewards
that ultimately allow the neuron to survive in the brain
and not be strapped by the organism or killed by it
because it's not doing the right thing.
The second misunderstanding, I think,
beyond neurons are not just specialized switches
is that it's only neurons.
There is probably no fundamental difference
between neurons and other cells.
That is, every cell can do information processing
in conjunction with other cells
if it's in a multicellular animal.
It probably needs to be somewhat multicellular
because if it's a single celled animal,
it's maybe evolving in such a way
that it's adversarial to its environment.
It doesn't benefit if it computes information
together with others.
But if it lives together with others
in a high degree of organization like the cells
in our body are doing and you co-evolve them,
then the cells can all send many types of messages
via chemicals over the cellular membranes
to neighboring cells.
They can interpret those messages
and they can learn how to respond to these messages,
which means they can learn to perform arbitrary computation.
So what's the difference between a neuron and another cell
if they can do the same thing?
Well, the specialization of neurons
is that they have extremely long accents,
so which they can send information coded
as electrochemical impulses very quickly
over long distances.
I think that neurons might be best understood
as telegraph cells.
And they're very special cells that have evolved
only in animals or mostly in animals
and that are very expensive to run
because they need a lot of energy
to send information so quickly.
And the benefit is that they can move an animal very quickly
so nature, so it can eat plants or other animals
to get that energy.
So basically the animal gets more energy
that it could get from photosynthesis.
And as a benefit, it's able to afford
to have this expensive nervous system.
And the code that neurons are using
is probably different from the code,
from the chemical codes that the cells are using
for their messages.
It's like a Morse code, probably something
like a telegraph system,
so it can send information so quickly.
And initially, it seems that to me
that neurons might have evolved
to move skeletal muscles at the limit of physics.
So it can, from centralized coordination
in the nervous system,
from the central nervous system,
send information so quickly
that the whole organism is coordinated much, much faster
at much shorter time spans than plants are.
And the other thing is once you can move very quickly,
you also need to perceive very quickly.
So it's also driving sensory input
and the evaluation of sensory input
and decision-making and learning.
So it's basically duplicating the information processing
that existed in the body
and is creating something like a second
information processor in the body
that is running at much, much faster time scales
than the normal cellular information processing
that will also exist in large, long-lived plants.
And so my perspective is that
just by looking at means and motive,
the possibilities of what evolution can do
and the capabilities of what an individual cell has,
I suspect that every long-lived organism
with many cells is basically going to function
like a very, very slow brain.
And there are almost no limits
in what this slow brain can do if it lives long enough,
but it's not going to do this at the same time frame.
So animals will be able to outthink plants
just because they are so fast
and run circles around them.
And the third misunderstanding is that
we think that consciousness is extremely rare,
that consciousness may be only existing in humans
and forms only very late
in the evolution of intelligent systems.
But it turns out we don't get conscious after the PhD.
We seem to be conscious before we can track a finger.
And if that is the case,
maybe self-reflexive attention is a requirement
if you want to learn beyond heavy on learning.
If you don't want to just look at co-activation
between cells as a learning paradigm,
which is probably sufficient to map your body surface
and so on, but you want to have a coherent model
of reality, maybe you need to start out
with some kind of core
that organizes everything into coherence.
And what we find confusing is about consciousness
that is that we don't seem to need it for many things.
The sleepwalker can do many of the things
that normally require consciousness.
And this is confusing philosophers to no end.
But maybe consciousness is in some sense
like government in a society.
And I don't mean government
as some kind of abstract principle,
but as a real-time interaction
that is making society coherent.
And a society can do everything without government.
If you would be shutting down the government today,
it would be days before we notice
and years before we crash, right?
But to get to the state in which society is today
with streets and infrastructure
and educational system and so on,
you need to have a government.
You're not going to bootstrap a group of people
into an organization,
this out having some kind of hierarchical organization
that makes this group of people coherent in their actions
and creates some next level agent out of them.
And to do this,
the government needs to start out
with some local coherence,
with some making itself coherent
and then imposing some kind of organization
on the environment that is branching out
and scales over all the individual agents.
And if you just put individual people together
for long enough, then different forms of government
will emerge and some kind of evolutionary competition
between them and eventually one of them will take over
and organize this group of people in such a way.
And the idea that the same thing could happen
among the neurons,
that they're basically different forms of organization
that start out from small course
and then move as activation patterns
that are agnostic of the individual units
that they run on,
but they impose the same language on all of them,
similar capabilities on all of them.
So the locus of action can move around between them.
This idea that the brain organization could evolve like this
is similar to Gary Edelman's idea of neural Darwinism,
that basically our brain organization,
our mental organization is not hard coded
in the genome as a blueprint,
but what the genome contains is the conditions
to start this evolution
between different forms of organization.
And then Rick the evolution in a particular way
so it converges quickly.
There are some ideas that we could take
from biology into technical systems.
And first of all, I think that the system needs
to be real time.
It needs to be coupled to the environment
and needs to go into resonance
with whatever environment it is coupled to.
And it needs to regulate the interaction
with that environment until at the level of coupling
at a temporal resolution that it has
is able to track reality around it.
And it's something that our machine learning models
are not doing yet and not real time.
Even something like stable diffusion is trained
in digital images that are not happening in real time.
They're not even happening in the right order.
There are just 800 million disconnected images
or which the system is trying to find structure.
And this would not work for a logical organism.
I don't think that we could converge
from this amount of data.
Instead, what we get is a world
that changes continuously by small degrees.
And these small changes make sure
that every frame is related to the last frame.
And we can learn universal laws
in which these frames are related.
There are laws of conservation of information.
And without this conservation of information
where we learn the transitions between adjacent frames,
I don't think that we would be able
to learn from the universe.
So one thing is we change the paradigm
from image to video or other streaming data
that has information preservation.
And in the beginning, this might look more difficult to us.
Isn't it harder to learn from video
than it is to learn from single pictures?
Well, what you want to learn is the fact
that you are living in the universe with moving objects
that happen in free space and so on.
It's actually much easier to learn this from video
because it contains way more constraints in this way.
They're much more obvious.
The next thing is the way in which our brain
is modeling these things seems to be made
from lots of small periodic loops,
small interlocking periodic loops.
And first of all, it has to be loops
because the brain is relatively slow.
Information transmission in the brain
is so slow that it takes appreciable fraction of a second
to just for a signal to cross the entire neocortex.
And if you want to create simultaneity
between these different parts of the brain,
you're not going to get there.
It's right in our computers and our CPUs and so on.
In our GPUs, we make everything simultaneous
by exploiting the speed of the signal transmission
in our CPUs and GPUs.
It also means that we cannot increase the frequency
at which we run them arbitrarily
or we cannot make the CPUs and GPUs arbitrarily large
because it just takes so much time
for a signal to cross over the entire circuit.
The next step could be to use photonics
so we can go to the speed of light
and make this system slightly faster
and slightly larger again
via having coherence over the entire system.
But our biological systems don't have a chance of doing that.
They must live with the fact
that it takes very long for signals to get there.
And the way to deal with that
is to make sure that you're okay if you are out of sync.
You just need to be in the same phase.
Basically you go at the same frequency
at different points of the brain
and you make sure that the signal eventually gets there
but it's okay if it's from the previous cycle
or two cycles ago or several cycles ago
as long as the content of the different brain areas
that only changes gradually.
Right, if that happens,
you're able to integrate over that.
You can use predictive algorithms and so on
and can synchronize the whole thing.
So basically this idea of slow oscillators
is something that we could translate into digital systems.
The next thing is this emergent management
and you will Darwinism.
So instead of having a particular circuit
that is required to be like this,
let's evolve this computational operators that we need.
And the next one is that many of the functions
that the brain discovering are only discovered once.
And this is tool for simple functions
like addition, integration, multiplication,
simple computational primitives, rotation
that are being used for many, many mathematical primitives
that we require to describe the geometry of sound,
of images, of thought.
And this basic arithmetic,
this basic library of computational functions
at the moment to train this into a neural network
takes a very long time.
Despite the neural network being built over addition
and multiplication,
it's not easy for a neural network to learn arithmetic.
It's possible to do it,
but it takes enormous amount of training data.
And in every context, locally,
the neural network is performing the same arithmetic
over and over again.
It has to retrain these functions
into a different region of the network again.
And basically getting all these different computational
primitive spools trapped into the neural network
is something that is hard.
And it has an interesting effect.
You can, for instance, train a neural network
on audio input or, and then use it
to discover structure and vision.
And it's going to be much faster
because the audio input already prepares the neural network
to learn many of the computational primitives,
basic arithmetic in many parts of the network,
that it can then adapt for a new task.
And the way in which our brain is doing this
is probably that it learns some useful functions
and then these neurons have a way
to exchange these functions possibly via RNA.
And so basically the functions that are being computed
are to some degree agnostic to the individual neuron.
They are somewhat substrate independent.
They're just migrate to,
so a different paradigm might be
instead of using local functions
over the neighborhood of every neuron.
And every neuron is learning its own set of functions.
You learn a set of global functions
and every neuron is deciding which ones of those to use.
And it's also something that has almost never been tried
in AI and that's what I'd like to get seen.
Another thing is that the main focus is on reward.
What you try to do in the brain
is to do the most useful thing with fixed resources.
And this means you have to assess the global reward
that the organism is getting out of the distributions
of all the neurons.
And then you need to distribute this reward
among all the neurons that contribute to the result.
This is similar to what you do in a corporation.
It's an economic problem.
Like the corporation tries to do the most valuable thing
that it can do with all the employees that it has.
And to do this, it needs to get rewards
to all the individual employees.
And the rewards are not given in such a way
that every employee gets a different amount of money.
And the one that has the biggest contribution
to the bottom line, but the others are also important,
gets much, much more.
There is some degree of this,
but it's mostly depending on the negotiation power
of individual employees on the market.
If there was no fungibility
and you would need to train all your employees,
it would make sense to give them all the same amount of money.
And in the liberal capitalism,
it doesn't make sense to give a good retail worker
the same amount of salary
than it does to give money to a good manager.
But if you need retail workers, you will have to employ one.
And the reason why retail workers are less than managers
is mostly because there are many more retail workers
competing for positions.
Then there are managers competing for positions.
So the supply and demand regulates labor market
in such a way.
But this is not true in the biological system.
Every neuron basically is going to consume
the same amount of resources just for existing
and being ready to do something.
So our reward here is different.
It's not being accumulated in the bank account of the neuron.
Instead, this is just a signal that tells the organism
that this neuron is still going to get fat
because it's useful in what it does.
And the neuron needs to get feedback,
similar to the feedback that you get from your colleagues.
That's the actual reward that tells you
you're doing the right thing.
And so it needs to be some kind of communicative reward,
some messages that are not directly food,
but that are more anticipated reward
that are like money only without accumulation.
And so there's basically going to be a reward-driven language.
And who is distributed all the reward in the brain?
Well, all the cells are all the neurons mostly,
but also the other cells, maybe glia cells,
and so on that contribute and the distribution of the rewards.
And so what's happening in the biological system
is that it involves a reward language.
There are two types of signals that are being sent around.
One is the results of the computation,
which is read by other neurons based on what kind of activation
they're interested in filtering out of the environment.
And the other one is going to be signals
that amount to reward and punishment
that basically tell other neurons
whether they should do more or less of a certain computation.
And so basically reading of information is going to be pulled.
You draw information from the environment
and reward is going to be pushed
because you should not be able to escape a negative reward,
a punishment and so on, right?
And I think we could approximate this
using a new paradigm and a number of experiments
that I would like to do in this regard.
So basically a neuron has an internal state vector
that contains the slight history of the neurons
over the last few activations that is read
and the type of the neuron and so on.
And it has a selector function.
The selector function is basically defining
the receptive field of the neuron.
And the receptive field of the neuron
can be just the environment of the neuron
interpreted as a certain topologies.
Basically it looks at its neighborhood
as if it was a space with a certain number of dimensions.
And how can this be?
The neocortex is two dimensional or two and a half dimensional.
But it has a number of layers,
that number of layers being very small
and it's a large 2D area
subdivided into different regions.
Well, it turns out that you can interpret a 2D area
as something that is in higher dimension
in the same way as you can take the linear
or one-dimensional address space of your computer
and interpret it as a two-dimensional map
or as a three-dimensional space
or as something that happens in eight dimensions
and can perform operations on it.
And this is what the selector function does.
The selector function is basically interpreting
the environment of the individual cell
as a space that contains information
in a certain arrangement.
And it doesn't need to be a regular space.
It can be a manifold.
It can be something that is very selective.
It can be something that only uses five neighbors.
And these neighbors can even be physically very distant.
And so this neuron could be a juncture
or some kind of hub that sends information locally
into the network and so on.
So you're going to have some neurons
that have a topology that allow long distance connectivity
and others that performs local maps and 2D or 3D
and perform functions on them.
And this next to the selector function,
you have the modifier functions.
The modifier functions tell the individual neuron
how it would change its state based on its own state
and the activation that it reads in the environment.
And by having some history in its own state,
it's able to respond to a spatiotemporal activation
distribution in its environment.
And it's the use this idea
that the neuron can use global functions.
It means that we can arrange many neurons densely enough
in some kind of lattice.
And these functions can arrange themselves
as they need to be arranged.
And they can shift around as they have to
and duplicate themselves if they have to.
Yasha, just in the interest of time.
Yes, I'm basically done.
Thank you for reminding me.
So this selector function, modifier function paradigm
allows us to come up with a new way
of describing a function approximation beyond deep learning.
And at the moment, the search space
in my experiments is way too large.
So basically there are too many ways
in which this could be implemented
from my current perspective
to get this converge to a good solution.
The alternative is I can just handcraft a solution,
but it might not be optimal.
But it's something that I would like to definitely
look more into in the future.
And not just me, I suspect that many people
are currently discovering such ideas
and will be working on in the future.
And while it's not clear
that this can provide a viable alternative to deep learning
that is much faster and converging faster
and more efficiently than the present deep learning systems,
this is something that I believe is closer
to what biology has discovered.
And that scales up very reliably
over many, many classes of algorithms.
Okay, that's it from me.
All right.
We would like to have a quick one to two minute break right now
so that the finalists can set up their presentation.
We will do Michael Levin's presentation next.
So in the meantime, Yosha,
let me ask you some questions that got posted
while you were talking.
So a question from Nikolai.
Like how neurons are small organisms
that operate in the emerging super organism
that is a human?
Would future AGI architecture need to be made up
of many smaller AIs?
So there is no centralized control in the brain
in the sense that all the centralized control
is emergent over all the organizations between the neurons.
There is no dedicated CPU that is able to process every neuron
and update its state.
Every state update happens locally in the individual cells.
But this is an engineering constraint
that doesn't exist in the technical systems.
And it's not clear yet to me to which degree
we need to have local control to make it happen.
At the moment, the machine learning of organisms
that we are using are treating the individual nodes
in the network just as memory.
And the updates are done by a centralized algorithm
that is updating all this memory.
And you either have a CPU that is reading and writing,
or you have lots of local CPUs in the GPU that
is doing this with multiple pipelines in parallel.
And there are biologically inspired chips
that are mostly experimental at the moment,
like Intel's Lohi, that are using many, many very small,
simple CPUs that are doing this.
And it's not clear if the best solution is
to have a CPU for every memory cell.
It's probably not the case.
So there are some things that you
can do in the technical systems like informing
conformance to centralized algorithms,
to centralized specifications that the technical system is
going to implement.
And in a biological system, this is just not feasible,
because there is no such centralized authority,
no engineer who can make nature behave by itself.
But if you want to think about how
to build a mind from a biological perspective,
we have to think about how to build something that
wants to grow into a mind.
And we can take some of these ideas.
It's not clear that we need to make this with completely
local control only.
Maybe it's more efficient to have a mixture of some local
control and a lot of global control
where we already know what the control is going to be.
I think Mike is ready.
Yes, excellent.
Michael, the floor is now yours.
Great.
OK, well, that was extremely interesting.
So let me see what I can add here.
I've got some slides.
So here we go.
Hopefully, you can see that.
So what I would like to talk about, of course,
there's this idea that biology should be an inspiration for AI.
And what I would like to do is to deconstruct some of the biology
that people typically think about in these contexts
and ditch a lot of things that are very common, binary
categories and a focus on brains, a focus on neurons,
a focus on humans.
I want to step away from all of that
and rebuild a different framework that I think
has many, many implications for AI.
So the first thing I want to talk about
is this idea of a typical human.
So there's this kind of classic idea
that we know what a human is and it certainly
works for practical purposes in society.
But the idea is sort of pre-scientific.
And it's still, many people are still, even scientists,
they're often still caught up in this.
The idea that, OK, so we have these humans
and they are discrete natural kind.
And they're different from other animals.
And so there's this, here's Adam naming the other animals.
And so there's the discrete species and so on.
But if we take developmental biology and evolution
and synthetic biology and bioengineering,
if we take these things seriously,
then what we find out is that actually there
are no such natural kinds.
Because all of this, both on the evolutionary time scale
and the developmental time scale and now
in terms of the technological time scale,
there are very gradual, very small, very slow changes
that go all the way back from what people think of
as a typical human and their intelligence,
all the way back to very different types of organisms.
And developmental biology, and of course evolution too,
offers absolutely no place to put a sharp line
and say, this creature was not, pick an adjective,
intelligent, cognitive, conscious, whatever you like,
pick an adjective to say this creature was not it,
but it had some offspring and the offspring now are, right?
That just doesn't exist because all of these changes
are very slow and very continuous.
And so there were changes during evolution.
We all start life as a single cell.
In the future, there will be all kinds of changes
to our bodies with biological and engineered
kinds of devices.
And so all of these are continua
of really novel types of embodiments.
And we build certain kinds of conceptual metaphors
that try to distinguish different categories here,
but these are discrete tools.
The phenomena themselves are deeply continuous
on multi-scale.
And to give you just a simple idea,
and then we'll enlarge on this, is this.
And so this caterpillar is a kind of soft-bodied robot
that lives in a two-dimensional world
that crawls around on leaves.
It likes to chew plants and it has this brain
that's very, very suitable for this purpose.
What it needs to do is turn into this creature,
which is completely different.
It lives in the three-dimensional world.
It doesn't care about the leaves at all.
It wants nectar and it flies and it does various things.
And so during this process, there is a metamorphosis
where not on an evolutionary time scale,
during the lifetime of the individual,
the brain is basically dissociated
and rebuilt into a new architecture.
And by the way, there are data that memories persist.
So if you train the caterpillar,
the butterfly or moth still remembers
the original information,
but you can sort of think about what's,
nevermind the question of what's it like to be a butterfly?
What's it like to be a caterpillar
changing into a butterfly, that process of slow,
but drastic change in your embodiment?
And so from here, we can just remembering
that we are all made of parts that can modify
during our lifetime.
We can ask some interesting questions.
For example, you look at a brain
and we're sort of conditioned to expect that it's obvious
that a brain contains one human worth of intelligence.
But this is just because we're used to that
in terms of our interactions.
If I showed you a brain and you didn't know what this was,
and I asked you how many different selves are in there,
you would actually have,
we have no ability to answer that question.
We have no way to ask how much,
and I think Yosha got to some of this,
how much of this real estate is necessary
for one human's worth of performance?
We have no idea how much is actually in there.
And actually, very interestingly,
the same issue occurs in embryonic development.
So we all begin as a cellular blastoderm.
So this is a sheet, a two-dimensional sheet of cells,
and that sheet turns into an embryo.
Now, what does that mean?
First of all, can we guess in advance
how many embryos are going to come from that sheet?
Actually, we cannot, and I'll show you why.
And it's not genetics.
And then there's the question of what are we actually counting?
When we count an embryo,
I mean, there's 50,000 cells, let's say here,
what is it that we're counting when we say there's an embryo?
What are we actually counting
when we say there's a single human inhabitant
in this bunch of tissue?
So one of the things that you can do in embryogenesis
is you take this blastoderm and you take a little needle
and you put some kind of scratches into this blastoderm.
And then they heal, but before they heal,
what will happen is that each of these regions
being isolated from the other regions
decides to organize an embryo
because they don't know for a while anyway
that the other regions are there.
Then when it heals up, it becomes conjoined twins.
And you can do this very easily
in chicken and duck and other embryos,
but humans work exactly the same way.
And so then there will be multiple embryos
within the same blastoderm.
And then there will be some disputed zones here.
There's some cells that aren't quite sure
which one they belong to.
But this deep idea of individuation,
of taking some kind of a continuous,
in fact, it's even worse than continuous
because it's multi-scale substrate
and having itself organized into discrete what?
So in the case of embryos,
which you have are discrete groups of cells
that are trying to follow anatomical goals.
They're trying to achieve particular walks
in anatomical space.
They're gonna construct the right number of fingers,
the right number of eyes, whatever it is.
Same thing in cognitive development.
There are issues,
there are disorders of individuation
that you see in split brain patients
and dissociations and so on.
So this question of how many are in there
is deeply interesting.
And it gets to the bottom of what it means
to be a coherent agent when you're made of parts.
And I think Alan Turing,
although as far as I can tell,
he didn't write directly about this.
I think he was well aware of this issue
because of course he was interested in intelligence
and generic embodiment and so on.
But he was also interested in morphogenesis.
He wrote this paper on biological morphogenesis.
And I think he understood
that these are deeply and profoundly the same problem.
The problem of morphogenesis
and the problem of the mind are the same problem
because of this emphasis on emerging
as a coherent entity from multiple parts.
So people often talk about,
well, ants and termites
are some kind of collective intelligence.
And we can argue about what that means,
but we are really a unified, a centralized intelligence.
We're not like bird flocks or ant colonies.
But actually, of course,
all biological systems are made of parts.
And so we too are a kind of collective intelligence.
What's interesting is the scaling interface
is what is it that allows these individual subunits
to work together and present to other intelligences,
to themselves, by the way, and to the environment
in a picture of a coherent agent.
So this is the journey that we all took.
We began life as a piece of physics.
So basically as a quiescent oocyte.
So it's a blob of chemicals, not doing terribly much.
And then through this incredibly just magical process
of embryonic development,
that we arrive at something like this,
which is a complex organism with metacognitive capacity
that's going to make statements about how
we're not just machines and we're different than physics
and all that.
But this whole process is extremely smooth and gradual.
It happens second by second.
There is no lightning flash
at which point physics becomes mined.
It's a gradual process.
And we can talk about face transitions and such.
But there's really not that much evidence for any of that.
It's a very continuous process.
So this is the kind of thing.
So I think, Yosha alluded to this a few times.
This is the sort of thing.
I mean, not exactly this.
This is a lacrimaria.
It's a free living organism.
But here's a single cell.
This is what we are made of.
These guys, there's no brain.
Here's those.
There's no nervous system.
It's the single cell creature in real time
using all of the intelligence of its chemical networks.
And we can talk about this.
I mean, quite literally, chemical networks can learn.
And they can do inference and many other things.
Even though it's a single organism,
it's handling all of its single cell agendas
in its environment.
So metabolically, physiologically, anatomically,
it's doing what it needs to do.
And so we are made of extremely competent parts.
Here's another example.
This whole thing, you'll see this.
I'm going to pause it.
Whoops.
I'm going to pause this.
This whole thing right here, this is called phyzarin polycephalum.
It's a slime mold.
The whole thing is one cell.
And what I'm showing you here is that it's sitting
in this environment.
These are three glass discs.
These are very, very light.
There's no chemicals.
There's no food.
It's just glass, inert glass.
There's one glass disc here.
And what it's going to do is it's
for the first few hours, it's going to just generically
grow in all directions here.
What it's doing during this process
is it's tugging on its substrate and feeling
the vibrations it gets back.
And it can sense the strain angle of the objects
in its environment.
And then we'll eventually reliably grow out
to the heavier mass.
So that'll happen at this point.
But during this time is when it's
processing that information and learning from its environment.
And then, boom, now the behavior begins.
So single cells are very competent, even microbial single cells.
And so what we have to understand is that biology,
so here's a principle that I think
is really important for future AI,
biology is deeply nested.
That is not merely structurally.
I mean, that's obvious.
We're made of organs, tissues, and so on.
But each layer is competent.
It solves problems in its own space.
All of these things, from molecular networks
all the way up to whole organs and beyond,
are solving specific problems in specific spaces.
So we are really interested in my group.
We're really interested in creating a framework that
allows us to relate to really very diverse intelligences.
So of course, familiar creatures,
all kinds of weird biologicals, colonial organisms, swarms.
Of course, new, and I'll show you some in a couple minutes,
new engineered creatures, artificial intelligences,
and maybe at some point exobiological,
truly alien agents.
We need to be able to deal with all of this.
It's not enough to deal with crows and monkeys
and then maybe octopus.
That's way too narrow.
And so of course, this is an idea that has been addressed before.
So here's Wiener and colleagues trying
to come up with a very cybernetic way
to classify different degrees of behavior
all the way from passive mechanical behavior
up to complex cognition.
In a way that abstracts from its familiar embodiments.
So there's no talk of brains or neurons or anything like that.
This is very sort of functionalist.
And one thing about us as humans
is that we are very primed to recognize intelligence
in the three-dimensional space.
So basically, medium-sized objects moving at medium speeds
through three-dimensional space.
When we see it, we know what agency looks like.
We know what intelligence looks like.
But we are really bad at, and this
is why we must get better at it, recognizing
intelligence in other types of problem spaces.
So imagine if you had a direct feeling of all
of your blood chemistry.
If you were able to feel your blood chemistry the way
that you can see objects in three-dimensional space,
you would be very obvious that your kidneys, your liver,
and so on have a degree of intelligence
and they're doing amazing things in their problem spaces.
So we study how individual cells navigate
the space of gene expression, the physiology,
and morpho space, the space of patterns.
This is what I'm talking about today.
And just for a few minutes here, here's an example
of cells solving an entirely novel problem in genetic space.
So here's a planarian.
This is a flatworm.
They regenerate parts of their body when amputated.
What we did was we exposed planaria
to a solution of barium.
Barium blocks all of their potassium channels.
The cells in the neurons are really unhappy.
Their heads explode, literally just explode.
Over the next week or so, they rebuild,
keeping them in the barium, they rebuild a brand new head.
The new head doesn't care about barium at all.
So we asked the simple question, how can that be?
What is the new head doing
that the original head couldn't do?
And we found out that there's actually very few genes
that the system up and down regulated
to be able to do its business in the presence of barium.
The kicker is, planaria never get exposed to barium
in the real world.
There is no ecological precedent for this.
So just imagine, you're a cell.
You've got, I don't know, tens of thousands
of possible genes.
You've got a disaster, a physiological disaster.
You don't have time to try every combination.
There is no time to try everything
and whoever survives, survives.
These cells don't turn over that fast.
You have to solve this novel problem,
possibly by generalizing,
because you've never seen barium before,
but you have seen epilepsy before.
And barium excitability might look a little bit
like epilepsy.
And so maybe you can do some of the same things.
So this idea of solving novel problems
in physiological space is one example
of what biology can do.
But here's another example.
So this is how we all start
as a kind of a collection of early cells.
But this is a cross-section through a human torso.
Now look at the incredible order here, right?
All the tissues, the organs, everything is
in the right place, the right size and shape
and relative to each other.
Where does that come from?
You might be tempted to say DNA,
but of course we can read genomes now
and what's in the DNA isn't any of that.
What's in the DNA is the sequence
of the micro level sort of hardware
that every cell gets to have, the proteins.
That's what the DNA specifies.
So you really, you still need to understand
the physiology by which these cells compute
what to do here.
And then there are lots of questions,
as regenerative medicine workers,
we try to figure out what do we say to these cells
to rebuild pieces that are missing.
And as engineers, we wanna know what's actually possible.
What can you reprogram this?
Can you make them do something else?
So the amazing thing about development
is that while it is incredibly reliable and robust
and in fact hides all of its intelligence from us
when we see acorns giving rise to oak trees
and frog eggs make frogs, we sort of assume,
well, what else is it gonna do?
Like that's obvious, right?
That's how it has to be.
But that's only what happens on the default condition.
What we find out is that, for example,
if you take an early embryo and cut it in half,
you don't get two half bodies.
You get two perfectly normal monosygotic twins.
And in fact, more generally,
the process of development can navigate this anatomical space
in a way to reach the same goal
from different starting positions
despite really drastic perturbations
by taking different paths.
It's not just a hardwired set of emergent.
This is not about emergence.
Of course, complex things emerge from simple rules.
This isn't bad at all.
This is the ability of the system to get to its goal
despite really, really radical changes.
So here's one change.
Here's another change.
As an adult, some organisms like the salamander,
they regenerate their eyes, their limbs,
their jaws, their tails.
You can make cuts anywhere you like along here.
And these cells will very rapidly grow
and undergo morphogenesis,
and then they will stop.
When do they stop?
They stop when a correct salamander limb has formed.
It doesn't matter where you cut it,
it will only grow exactly the right amount
and it will stop when exactly the right thing has formed.
You've got some sort of error minimization scheme
going on here.
It knows exactly what it looks like.
It knows what the target state is.
And in fact, this is something that we discovered
that, so this is a tadpole here, some eyes,
here's the brain, the gut, the nostrils.
These tadpoles have to become frogs.
In order to become frogs,
they have to rearrange their face.
The jaws have to move, the nostrils have to,
everything has to move.
We find, and so you might imagine
that this is some sort of hardwired set of emergent outcomes
where every organ gets displaced
to its appropriate distance and direction.
So we made what's called Picasso tadpoles.
Basically, we scrambled everything
so that everything's in the wrong place.
The eyes are off to the side of the head,
the jaws are on the other side,
everything is just scrambled.
Because we have this hypothesis
that this is more intelligent
than people gave it credit for.
Sure enough, what these guys do
is every structure moves in novel paths
and keeps moving no matter where it started from
until it gets to be a pretty normal looking frog.
So what the genetics gives you
is not a piece of hardware
that does the same thing all the time.
It gives you a machine that can recognize
unexpected changes and take corrective action
as needed to get to the same goal.
The most amazing part of this is that in doing this,
and this is an example of top-down causation,
which is why it's really important to understand this,
how high level goals filter down
to the sort of implementation machinery,
is that what you see is that this is an example
from the kidney tubule of a nut.
If you take it in cross-section,
normally there's, I don't know,
eight or nine cells that work together
to make the lumen of that tubule.
But one thing you can do is you can make these,
you can force these cells to be gigantic.
And when you do this, when you make them larger,
fewer cells will do this,
forming exactly the same lumen diameter.
Until you make the cells so large
that a single cell will wrap around itself
to give you the same structure.
What's amazing about that is that
these are completely different molecular mechanisms.
This is cell-to-cell communication,
this is cytoskeletal bending.
So in the service of a high-level goal,
meaning make this large-scale anatomical structure,
different molecular mechanisms get activated, okay?
And this is very unusual,
this idea is very unusual in biology, biology,
the biologists tend to think about things emerging
from molecules not going the other way,
but it has certain parallels in computer science
where the algorithm makes the electrons dance
in an important way, right?
In a functionally important way.
And so what we've been doing is I'm trying to build models
that go sort of full-stack models,
that go all the way up from molecular kinds of activities
that set the ion channels and other things in the membrane.
Two, we specifically, I don't have too much time today,
but we specifically study bioelectrics,
we study how all cells, not just neurons,
all cells use electrical signaling
to form computational networks.
And so we study what the tissue-level
electrical patterns look like,
and then what the organ-level patterns look like,
and then how that becomes literally
an algorithmic set of steps that determines things
like how many heads a flower is going to have.
And during this process, we want to know a few things.
We want to know how does the cognitive light cone,
and what I mean by that is simply
the spatiotemporal size, the scale of the largest goal
that that particular system can conceive of pursuing, right?
So if you're a bacterium,
your cognitive light cone is very tiny
because really all you care about
is the local sugar concentration
with about maybe 10 minutes forward and back.
But if you're a human, you can have gigantic goals
that exceed your lifespan,
that can be planetary-scale goals,
and then of course, every kind of creature in between.
So we define this kind of cognitive light cone
based around the types of goals
that a system can pursue.
And so we need to understand during this process,
how do the goals enlarge?
How do they shift into different spaces?
So individual cells care about things in metabolic space
and physiological space and transcriptional space,
those are their goals.
Collectives of cells care about very much larger goals,
such as the shape of your hand
and the fact that you have to have exactly five fingers.
And then of course, this question of where do these goals
come from in the first place?
We'll address that momentarily.
So about the only piece of bioelectricity
I'm gonna show you because Yosha brought up this idea
of counterfactual memories is simply this.
We treat the behavior of the cells and tissues
as a collective intelligence.
Literally the group of cells is a collective intelligence
that tries to solve problems in anatomical space.
And because we have some understanding now
of what the medium is of that collective intelligence,
not shockingly, just like in the brain, it's bioelectric.
Why?
Because that's how the brain learned its tricks.
We already heard and Yosha is absolutely right.
There are very difficult tasks to try to distinguish
what makes a neuron different from other cells
because even bacteria from the time of microbial biofilms
have already been using all of the same tricks
as the brain uses, this electrical network stuff is ancient.
And so what we are able to do is read and write
the memories of this collective intelligence.
And so we use a specific technique
that reads the electrical gradients.
This is just like neural decoding
as the neuroscientists try to do in the brain.
So here there's a particular pattern that says,
if injured, you're going to make one head.
We can rewrite that and we can create a worm.
Here he is, where the pattern says,
no, actually a correct worm should have two heads.
And if you go ahead and cut that animal,
they will go ahead and make two heads.
This is not Photoshop, these are real,
real two-headed malaria.
But the cool thing about this pattern
is this is not a reading of this animal.
This is a reading of this perfectly normal,
anatomically one-headed, genetically,
transcriptionally one-headed animal.
So this is a kind of counterfactual memory.
It's a representation of a state
that it's what you are going to do in the future
if you get injured.
If you don't get injured, it stays latent,
it never comes up.
So, and we have lots more data on this,
we can actually make heads of other species of worms
and many other things.
The idea is that a single body can store
one of two different representations
of what the goal state is going to be if they get injured
and then they build to that goal state.
So this should sound very familiar.
This is both the nervous system works this way
and of course, reprogrammable devices work this way.
The same hardware can hold on
to multiple computational goal states.
Now, what's really, let's go back to where we started
with this, which is this notion of scaling up
from components.
So here's your single cell.
What evolution has done is allowed these cells
to merge into networks
that are able to store much larger goal states.
So this guy only cares about his own physiology
and his own metabolic.
This collection of cells is very competent
in reaching a particular region
of anatomical morpho space that looks like this.
The goal is huge at centimeters in size.
And if it's deviated from that,
it will do its best to come back
even with the kind of drastic interventions.
But that process has a failure mode.
That failure mode is known as cancer.
What happens?
This is human glioblastoma cells.
If individual cells get disconnected
from this electrical and other signals as well
from this network that binds them
towards a common journey in that space, that common goal,
they revert back to their evolutionarily ancient self.
What is the goal of a single cell?
Well, it's to become two cells
and to go wherever life is good.
That's metastasis.
And so you can see how what happens
with these cancer cells
is they're not any more selfish than any other cell.
They're just, their cells are smaller.
And we've talked, you know,
I talked to roboticists and folks like that
with this idea that why don't robots get cancer, right?
The reason that our current technology isn't prone to this
is because we do not have a multi-scale architecture
where the components have their own goals.
That we have some fairly dumb components typically.
And then we hope that the collective
that has some kind of, you know,
is doing some kind of computation,
but the parts are not trying to do anything.
Biology isn't like that.
Every component will do interesting things
if freed from its neighbors.
And I'll show you that.
But of course, you know, biometrically,
we can sort of take that,
this kind of weird way of looking at things
and ask, can we simply enlarge the boundary of the self,
enlarge the border between self and outside world?
And so you can do that.
We have techniques to do that
where when we inject a particular human oncogenes
into these tadpoles to make tumors,
and you can already see this is voltage imaging,
you can see that these cells are already starting to defect.
As far as they're concerned,
the rest of the animal is just outside environment.
So that's something else that Miocha mentioned
is this idea of being in conflict
or not with your environment.
It's never obvious to a new agent what the environment is.
Every cell is some other cells' external environment.
And so normally all of these cells believe
that the water out here is the external environment.
But once you disconnect them using these oncogenes,
then as far as the cells are concerned,
all of this stuff is external environment.
They don't care what happens to that.
They're gonna do their best.
They're gonna live their best life.
They're gonna dump entropy into the environment.
And of course, that's maladaptive for the organism.
But one thing you can do is you can force,
using specific techniques,
including optogenetics and some other things,
you can force these cells to remain in the correct
electrical state with their neighbors.
And if you do that, even though the oncogene,
this is the same animal here,
even though the oncogene is very strong,
there's no tumor because the hardware problem
isn't really fundamental.
It's the software that's fundamental.
Are these cells working on a large goal,
like making a nice liver and muscle and skin and whatever?
Or are they individual cells working on individual goals?
So we spend a lot of time thinking about these kinds of things.
How do we, what are the mechanisms, of course,
but also algorithms or policies
for connecting up little tiny homeostats,
these cells that like to keep certain states
into much larger networks
that then have these interesting properties
that of course people in the connectionless world
have been studying for a really long time.
So in painting, out painting,
you know, all this kind of stuff.
So we can talk about our efforts
to sort of understand how the goals scale.
They scale from these really humble, metabolic kinds
of goals of individual cells, right?
These homeostatic loops into anatomical homeostasis,
eventually behavioral homeostasis
and behavioral clever motion through three-dimensional space
and eventually linguistic space and who knows what else.
So just for the last couple of minutes,
I just wanna show you one thing,
which is simply this.
In studying these kind of novel perturbations
and asking what are cells actually capable of?
What, you know, what other modes are there?
We asked the following thing.
And I have to do a disclosure here
because Josh Bongard and I are co-founders
of this thing called Fauna Systems.
It's a biorebotics kind of company.
So what we did in this, all the biology
was done by Doug Blackiston in my lab.
And there was a lot of computer science here done
by Sam Kriegman and Josh's lab.
What we decided to do was to liberate cells
from the normal environment and give them a chance
to reboot their multicellularity.
How much creativity is there?
What else can they do?
And specifically, and I think somebody on the chat
asked this before, where do these goals come from?
So that's what we wanted to understand.
A completely novel creature that's never existed before.
What goals do they have?
Where do their goals come from?
Okay, and so I'm gonna just show you a couple of examples.
So what we did here is we took an early frog embryo
and so what Doug does is he takes all of these cells
up here, which are skin.
They're basically determined to be skin
and he dissociates them and puts them
into a little depression here.
Now there are many things that they could have done
after that, they could die.
They could spread out and sort of walk away from each other.
They could form a flat two-dimensional monolayer
the way that cell culture does.
Instead, what happens is this, and this is time lapse,
of course, so overnight these guys will get together
and they will coalesce into this interesting
little thing here and what is it?
Well, we call this a Xenobot.
Xenopus lavus is the name of the frog
and it's a biobot, so Xenobot.
What it's doing is it's using the little hairs
on its surface.
These hairs are normally there to spread mucus
down the body of the frog.
What they've done is repurpose those hairs for swimming.
So here it goes, it's chugging along.
You can see that as they can go in circles,
they can sort of patrol back and forth like this.
They can have group behaviors.
This one's going on kind of a long journey.
These are interacting together.
These are having an arrest.
Here's what it does in a maze.
So you can see it swims along.
It's gonna take a turn here without having to bump
into this outside wall, so it takes a turn.
Then at this point, for some internal reason,
we have no idea about it,
it decides to turn around and go back where it came from.
Okay, so there's all sorts of primitive kinds of dynamics.
Just keep in mind, even though these things have,
this is calcium signaling you see.
It's the kind of thing you see when you do brain imaging.
There are no neurons here.
This is just skin.
This whole thing is just skin cells,
but they're doing a lot of,
calcium readout is a great readout of computation.
Could they be saying something to each other?
Of course, we don't know.
This is still very much ongoing subject of investigation.
But one of the amazing things
that Doug and Sam discovered is that
their computational models of these guys
make predictions that differently shaped bots
are going to rearrange their environment
in different ways.
So they did a lot of simulations.
And so then we tried it and we just did it in vivo.
And here's what we found.
So here are the bots.
The white stuff here is their cells.
They're loose skin cells that we sprinkled into the dish.
And what they're basically doing,
because we made it impossible for them
to reproduce in the normal froggy fashion,
they are basically implementing von Neumann's dream.
They are constructing other,
what they do is they run around
and they sort of collect these skin cells
into little piles, then they kind of polish the piles.
And these piles, because they're working
with an agential material,
they're not working with passive particles,
they're working with cells,
what do these cells like to do?
They like to become the Xenobot.
And so of course they create the next generation of Xenobot,
which then matures and guess what?
It does the same thing
and then you get the next generation and so on.
So this is kinematic self-replication,
and they'll make multiple generations of this.
So here's a couple of interesting corollaries to this
and I'm almost done.
The exact same genome,
so here's the specification of the micro level hardware,
this is what every cell gets to have,
can do one of two things.
Under normal circumstances, it will do this,
it has this developmental sequence,
then it makes these tadpoles that do various things.
But under other circumstances, it makes this,
this is a Xenobot, this is a developmental sequence,
this is I think a month old or something Xenobot,
where did the shape come from, right?
And they have a different behavior
with this thing called kinematic self-replication.
So here's a few interesting things, number one.
Typically, when you talk about why a certain creature
has certain capacities, everybody leans on evolution.
Well, for eons, it was selected to do this or that.
Well, there's never been any Xenobots,
there's never been any selective pressure
to be a good Xenobot, this is completely emergent.
They do this, they form this coherent kind of system
with new behaviors, both anatomically and with motility,
basically overnight.
This has never been selected for specifically.
They're completely new in the biosphere.
As far as we know, no other living creature
does kinematic self-replication.
That's the first thing.
The second thing is that, how did we engineer these?
I mean, there are no trans genes here.
So the, if you sequence this,
all you see is normal Xenopus lavis,
so there's nothing wrong with the genome, it's wild type.
There are no nanomaterials, some of them,
some of them Doug can make some modifications to them
surgically, according to the AI that Josh and Sam built.
But basically, the way we engineered these
is not by adding anything,
it's by liberating them from the influence of the other cells.
So normally, if you just look at the normal path
of this biological system, you would say,
what do the skin cells like to do?
Well, they like to be, and in fact, all they can be
is to be the outside two-dimensional layer
of keeping out the bacteria.
It's very boring passive life.
They just sort of sit there and keep out the bacteria.
But that's only what happens when they're basically
bullied into it by the other cells.
It's behavior shaping, it's instructive interactions
from the other cells that tell them to sit quietly
and be the outer layer.
In the absence of all that stuff, liberated from all that,
they have a completely different default lifestyle.
And this is it, which you would not see
without this thing.
And we don't know what else.
Certainly we're studying right now
all kinds of behavioral capacities.
Do they learn?
Do they anticipate all sorts of things?
I'm not making any claims yet about that.
But this idea of what evolution, I think, really does,
and then we can talk about why.
I think we have now some ideas about why.
It doesn't produce solutions to specific problems.
It produces generic problem-solving machines.
And so the big thing that every living system has to do,
and I think these are, if I had to make a list,
these are some things that I think
are required for the kind of thing we want.
First of all, I think it's really important
that your parts have agendas.
It's not enough to have dumb parts
and try to engineer an agenda for the whole system.
You have to have a marketplace where every layer
is competing, cooperating, and attempting to do its own thing.
You, of course, risk failure modes.
You risk parts trying to go off on their own.
That's one of the trade-offs, but overall,
it becomes, I think, an incredibly powerful architecture.
They have to emerge spontaneously.
That is, real agents don't know where their boundaries are.
If you are a new embryo coming into the world,
you don't know how many cells you're going to have,
because we might remove half of them,
and you still have to make a good embryo.
You don't know how big your cells are,
because we might make gigantic cells or smaller cells.
You don't know exactly how many chromosomes
you're going to have, because we can make
all kinds of weird chimeras and so on.
You have to be able to, surviving life,
has to be able to play the hand itself from scratch.
You really can't take past experience too seriously.
You have to improvise on the fly.
This is what biology does.
So it does not have, nobody says, this is the border,
this is where you are,
and then everything else is the outside world.
It has to guess, and it has to make a self-model,
and it has to make a world model.
Then there are the energy constraints.
Typical AIs, as far as I know,
have all their energy needs met.
They can do whatever they want.
They don't have to worry about it.
Organisms evolved under very stringent energy
and time constraints, which means
that they cannot afford to be some kind of a Laplacian demon
paying attention to all the micro states of the world.
They have to do a lot of core screening,
and they have to kind of bundle all sorts.
They have to generalize all sorts of things that go on
into models of agents doing things, of selves doing things.
That's the only way you have the time
to compute what you should do next.
And if you get good at that,
eventually you turn that on yourself
and you start telling stories about,
meaning making internal models of yourself doing things.
And this becomes this idea,
why do we all innately believe in free will?
Because from the time that we were single cells,
we had to tell stories about agents doing things
and making choices.
Otherwise, we just wouldn't survive without that ability.
And then there's some other things
like the shared stress and the scaling of stress
via sharing it among parts and so on.
We can talk about that.
And the idea that it's open-ended.
Living things select their own problem space
and explore it and so on.
So this is, I'm just gonna stop here,
but this is what I tell people is that because of this,
because biology is so incredibly interoperative,
because none of the parts make any assumptions
about what's going to happen.
They do their best in whatever environment
they happen to be in.
Every combination of evolved material,
some sort of engineered material and software
is potentially a viable agent.
So hybrids, cyborgs, biorobots, all of this,
there's this huge option space of new creatures,
of new bodies and new minds.
Everything, when Darwin said endless forms,
most beautiful, sort of impressed
with a variety of living beings,
all of that stuff is a tiny dot.
It's a tiny corner.
Everything on earth is a tiny corner
of the space of possible beings.
It's truly immense.
And all of these things,
and we're gonna be surrounded by these things.
Some of this already exists,
as some hybrids and cyborgs already exist,
but there's gonna be an incredible variety of them
that we are going to be living with.
This has major implications for ethics, for example,
because up until now,
we were, all of our ethical frameworks
about how to relate to other beings
really boiled down to two things.
Do they look like us?
And did they come from,
do they have the same origin story as us?
And so this is, even today in bioethics sessions
at conferences, people say,
well, does it look like a human brain?
Then we have to worry about.
But the reality is that these categories are,
they're not gonna survive the next couple of decades.
We cannot gauge anything about the potential intelligence
in terms of the type of cognition
and what space they're working in,
by looking at where they come from the family tree,
because they're not going to be on our family tree.
And we have to have completely different frameworks for this.
And the kinds of AIs that we're talking about now
are only one part of this.
They're going to be,
we're going to be facing the exact same problem
of dealing with software AIs in biology.
So if anybody's interested in these things,
there are lots of papers where we go into this.
And I just want to thank the students and postdocs
that did all the work that I showed you.
And of course, again, the disclosure.
So I will end there.
Thank you so much, Michael.
That was wonderful.
I don't think that I need to introduce Kristoff.
We already had the pleasure of having you on the previous panel.
And Kristoff is currently a professor at Itihar,
the INI in Zurich.
And he is a first generation cyber nutrition
in a way as a physicist,
who is using very broad perspective
on understanding intelligent systems.
And without further ado, Kristoff, please.
The stage is yours.
Yeah, I haven't prepared anything.
I didn't know I was expected to prepare anything.
So I am at liberty to respond to some of the things
you have said, the two of you have said.
Let me start with a with a point,
Yosha, you made about the brain, which was,
it is a noisy, it is a noisy entity.
It is not a digital device, not on the basic level.
So if you want to have billions of entities,
synapses or neurons to interact in any useful sense,
you need attractor dynamics.
So there must be certain states of the thing
that have the property of being stable under noise,
of having attractor dynamics.
And we know that the brain is, of course,
essentially a network.
So in each moment of time, a subset of the neurons fire,
and this subset must be stable.
And for a short moment, a metastable,
if you want to call it that way,
you want to go through a trajectory of stable states.
And that means individual fibers,
the individual interaction between neurons
must be embedded in alternate alternative pathways,
which run to the same effect.
So a signal emanating from a single neuron
going off on different pathways,
many of these signals must come together again
and coincide in space, meaning on the same neuron,
and in time.
And this is a selection criterion
for the kind of activity states and the underlying
connectivity states that make those states stable.
And I would like to submit the idea
that the brain is totally dominated
by those appropriately shaped connectivity patterns
that have this property.
These connectivity patterns emerge
through a process of self-interaction.
You have self-interaction also on the small, on the slow,
time scale of individual synapses adapting
and individual synapses finding out
where they can find coincidences of signals.
Each axonal branch has a small choice,
a small sphere of search space, where
it can end up in the plasticity.
And all the endpoints of axons are
searching around in order to find meeting places
where they have a high likelihood of coinciding
with the signals of other branches.
And this process of self-interaction,
of network self-organization, singles out
from the space of all combinatorially possible connectivity
patterns, a very, very small subset.
Let me remind you of the fact that the brain,
the whole organism, the brain is constructed
on the basis of one gigabyte of genetic information.
And in order to describe the connectivity pattern
of the brain, it's an easy calculation.
You need a petabyte, 10 to the 15 bytes of information,
which is a million gigabytes, as you well know.
So the genes can only select from the space of all connections.
A very small can only be able to select from that space
a very small subset.
And I think it is very important to know more
about this subset of self-supporting activity states
and connectivity states.
So in order to put that in action,
let me remind you that your brain is in every moment
of waking time representing the situation in which you are
immersed.
You have a representation of your actual environment
if you open up your eyes.
And this representation is so good
that you usually equate it with the reality out there.
You are not aware of any differences
between this reconstructed, this model of the outside world
and the outside world.
And you are so confident that it is the reality,
and not just an imagination, because you continuously
do experiments.
You move around so that the perspective of the world
changes, and you test whether your representation stays
in tune over time with the sensory information.
You do experiments.
You touch objects, and you experiment continuously
with the environment in order to make sure
that your representation is in tune with it,
is consistent with it, is rendering the reality.
Of course, what you are representing
is only a small sector of what is out there.
Your attention is always picking out only part of it.
But what you are picking out is for you,
for all intents and purposes, reality.
I find it amazing that our models,
our theories of intelligence, of brain function
make so little of this very fundamental fact
of our individual life.
Now, according to what I said, I
have been explicit about the data structure, which
is used to create this reality, to represent a model
of this reality.
The data structure is, of course, everybody
believes firing neurons.
But I would like to change your perspective in saying,
don't look at the neurons.
Each neuron by itself doesn't have
any significant meaning.
It is the environment, the neural environment,
the firing environment in which the neuron fires,
which is the important thing.
You have to look at quite a number of co-firing neurons
in order to be able to make sense of it.
When you look at a TV screen and can see only one pixel,
there is no way you can connect that with any meaning.
The pixel is something real, so to speak.
But it doesn't tell you anything.
It has no significance.
In order to understand anything on a TV screen,
you need to see quite a patch of it.
In order to understand anything of the data structure
of the brain, you need to see hundreds, probably
thousands of neurons at a time.
And a given neuron can take part in quite a number of such
and not an infinite number, but quite a number
of such activity patterns, which I would like to call fragments.
And so I think the perspective on the nervous system
has to be changed very fundamentally
in order to see it as a data structure that
is up to the job of representing reality.
Now, I would like one of the last statements you, Michael,
made was the range of things, of intelligent things,
of organized things that can be generated,
you said, is infinite.
I would rather like to emphasize the opposite.
I've recently read an interesting book
by an author named Morris.
A book that was totally focused on the phenomenon,
looking at evolution, the phenomenon of convergence.
A lens eye has been invented 12 times or something like that.
The facet eye has been invented again and again.
The lifestyle of a wolf pack has been invented again and again.
The lifestyle of social insects or social animals,
you social animals, has been invented again and again.
So the space of all possible organic patterns
that make sense, that have inner coherence,
where the parts support each other in order
to create something that is stable and significant,
it is stable in itself and is coherent with environment.
The space of those shapes and structures is limited.
And here is a great opportunity for theory
to come forward to understand what to expect from biology.
The last thing I want to say is that what is missing,
completely missing so far, almost completely missing so far,
from our artificial intelligence,
from our machine learning,
is the equivalent of biological behavior, of behavioral goals.
Of course, an animal has the fundamental goal
of self-preservation of the own structure,
but evolution has built into the individual species
a number of sub-goals like feed yourself and avoid danger and so on,
and find social contexts, sub-goals,
which make up your life.
And the intelligence in the eyes of many people
is just the ability to pursue those goals in a changing context.
That is a slightly different definition from yours,
your share, yours was computing functions.
The biological goals is the raison d'etre
of biological intelligence, of course, to pursue those goals.
And I think in the present situation around things like chat GTP and so on,
it is becoming quickly clear that those beasts
are not intelligent in our sense,
because what they do doesn't make sense in the light of the goals
that we all recognize as such.
And it would be, I think there will soon be an important drive towards
installing in such systems the equivalent of the sense of responsibility,
the sense of the consequences and utterance that is made,
may have down the line for ethical, for legal reasons.
But also I feel, although I don't have a very strong argument in that favor,
I have the feeling that in order for an entity to be truly intelligent,
it needs to have a set of set goals with which it can pursue in its environment.
Last remark I want to make is about consciousness.
I think in my view, the definition of the conscious state of your own mind,
of your own brain is a state in which you concentrate on one topic.
Your whole brain is concentrated on one topic.
And all the different submodalities in your brain are in tune with each other,
are in mutual understanding.
So if there any change happens in any part of the system,
all the other agency, other modalities can immediately respond to that.
So consciousness is not the icing on a cake.
I don't think it makes sense to talk of zombies.
Consciousness is a condition for a system to be functional.
And if you go down in the letter of evolution to simple animals,
I don't think you can find a point that is a point Michael also made.
You can find a point where consciousness disappears.
But consciousness just loses volume.
You lose language when you go from humans to animals.
And you use the imagination of distant future when you go to animals.
And so going further and further down the evolutionary letter,
the volume of consciousness gets less.
But I would have a hard time, I think when talking about a fly,
it has its own level of consciousness.
So when approaching a wall, it senses the impending approach and reacts accordingly.
So the whole organism is able to react to signals.
So that is my view on consciousness.
Thank you.
Thank you so much, Christoph.
So we're on to the discussion and Q&A question.
I would like to start by asking a question of Michael Levin.
So Michael, you were talking about how a lot of the problem solving within the organism
is actually done at the local scale.
And there is also the interesting remark that was made by one of participants in the chat
that you mentioned that the planaria tail would have become the head had not been bullied
by the rest of the organism.
So what do you think are the communication protocols that are necessary
to enable different types of intelligence?
And is it the case that humans, for example, have cancer
because we don't have enough intelligence at the lower local organismic level?
Or is it because we pursue the higher scale goal and some kind of trade-off has to be made?
Yeah, great. Great questions.
A few things. First of all, it is definitely not local.
So one of the key things about all of this stuff is that larger systems make decisions in spaces
that are much larger than their parts.
And so here's a very simple example.
You have a planarian, it's got a head and a tail, you cut it in half.
These two cells on either side of the cut, these guys will have to make a new head,
these guys will have to make a new tail, but they were sitting right next to each other
before you separated them with a scalpel.
You cannot locally decide whether you're a head or a tail.
It's a decision that has to take into account, well, do we already have a head?
Do we have a tail? Which way is the wound facing?
This is a global decision. It cannot be made locally.
All of this stuff is like that.
And it uses the exact same scheme that bacterial biofilms use to decide when different parts of the thing should eat
so that everybody has a turn.
And the exact same thing, the exact same set of mechanisms that brains use to try to synthesize
the activity of individual neurons into some sort of global goal for the rat or human or whatever.
It's an electrical network.
It has certain properties, only a few of which we understand, but it is absolutely not local.
What this bioelectricity is very good at is at implementing integrated information
across space and time to make decisions in new spaces.
And maybe I think I forgot, what was the second part of your, I lost track of it.
The second part is, so do humans, so you remark that humans have cancer,
but some other animals don't and some other animals are in fact immortal.
So what is it that, what is it in your opinion that doesn't allow human organisms to solve the problem of immortality?
Does it have something to do with higher level goals or is it just a lack of intelligence?
Yeah, I think that, well, so there's two, there's kind of a simple answer and then there's a more interesting answer.
The simple answer that people usually give is simply that all by, so evolution, of course,
doesn't really optimize for long life, happiness, intelligence, it doesn't optimize for any of that.
It optimizes for biomass, that's it.
And so, right, and so the simple answer is we don't need to be immortal and cancer resistant
because it's perfectly possible to be a human and have lots of offspring
and still get cancer and die after your reproductive years, that's it.
That's the standard answer that it's actually, there's just not a lot of pressure for humans to do anything different.
Now, I think the more interesting answer is this.
There are most organisms do get cancer and do age, there are a few that are resistant.
Let's look at the planaria.
One really interesting thing about planaria is that many of them reproduce by tearing themselves in half and regenerating.
Now, one interesting thing that the implications of that are unlike for us.
For us, if you get a mutation in your body during your lifetime, it doesn't get passed on to your offspring, right?
So because of the Weissman's barrier and sexual reproduction.
In planaria that do this, every cell that doesn't die from that mutation contributes copies of itself to the next body, right?
Because they have to repopulate and they have to regenerate the new worm.
So planaria accumulate mutations like crazy.
So over 400 million years that they've been around, their genomes are a complete mess.
They basically look like a tumor, they're mix-employed.
Every cell might have a different number of chromosomes, it's a disaster.
Now, this is really a scandal because nowhere in a typical sort of biology curriculum will you hear that the animal with the worst genome is, by the way, immortal cancer-resistant and highly regenerative, right?
They have the best anatomy.
What's going on?
We're told that our genomes are, that's where your body information is, right?
How can this be?
So this has been bugging me for a really long time, this disconnect.
And I think we finally have an idea of what's going on.
We just, like two days ago, just published a paper on some simulations that talk about this, I'll just give you a very simple example.
One thing you have to do is you have to model not just the genotype and the phenotype, meaning the genome and then the thing that gets evaluated in these evolutionary simulations, but you have to model the morphogenetic process in between those two.
The morphogenetic process has certain competencies.
For example, some of them I showed you, there are many more.
So for example, if there's some mutation that puts your mouth off to the side, the mouth is perfectly competent to come back where it needs to be.
If it's a mutation that causes you to fall apart as an early embryo, you'll just be a bunch of twins, multiples.
If we took some eyes, Doug Blackison did this too, he took eyes and put them instead on the animal's tail, they can see perfectly well out of those eyes.
No problem, no period of adaptation needed, it's all good, the nerves come, find the spinal cord, it's all good.
So that all of those kinds of things, the ability to make up for these kinds of issues we call developmental competencies.
Now, one thing that happens is that when you have an animal with a little bit of developmental competency, you come up for selection and it turns out you're very good, right?
But why are you good?
Selection cannot tell whether you have a great genome or you're good because you're highly competent and you fixed all the things your genome actually was pretty sloppy about.
So that means it's harder for evolution to see the good genomes.
It can't do all as much work in perfecting the genome, but what it can do is crank up the competencies, right?
So when you do that, then of course that makes the problem worse because the more competent you are, the less it's possible to find the good, the best genomes.
And so there's this positive feedback loop, this ratchet, and there are some other things that sort of work against it.
But I think what happened is that, and this is very much a hypothesis still, I think what happened is that planaria went all the way,
meaning that in that lineage, probably because they reproduce this way, it doesn't make any sense to assume that your genome is any good.
And the only architecture that survives is where the algorithm is so good that we're going to make a perfect worm no matter what happens to the genome.
This means aging, carcinogenic mutations, the algorithm meaning the machinery that maintains that goal state and physiological and anatomical space is so good
that it can pretty much ignore a lot of issues in the hardware.
Most of us aren't like that. Salamanders are sort of in the middle, so salamanders are highly regenerative, but they age and die.
And so I think what salamanders sort of went part of the way there and they can fix certain things, but not enough to really keep it going forever.
Mammals probably stopped even earlier than that, but I actually don't think any of this is fundamental.
I mean, we're working on regeneration in mammals now. I do think someday we will all sort of regenerate like planaria.
I think it is going to be possible, but I do think that evolution makes these trade-offs that they're just easier ways to be a human, I think.
And so question for everyone. So in terms of communication protocols, to what extent is intelligence simply the ability to organize the cells and what are the conditions necessary for that to occur?
And to what extent is intelligence is some internal competence, competency of the cell or neuron or whatever computational unit we're talking about?
Yosha, would you like to start?
I'm currently thinking about the question of whether it's possible to make something that is as long-lived as the planaria that doesn't look like a blob.
There seems to be some correlation between the structural coherence of the organism and the detail and solution that it has and the degree of fidelity that is expected from interpreting the operators defined in its genome.
And more specifically, I wonder how we can formalize the idea that Michael put up earlier of multi-scale organization in such a way that it leads to coherence.
What is the criterion that makes a single agent coherent in itself and leads to this coherence on a particular level?
Arguably, our own mind is some kind of society of agents and the organism has lots of local agents. Every organ is an agent in the way. Every cell is an agent.
But there is also a globally coherent agent. And that is different from having multiple twins coexisting next to each other and forming some cooperative chimera, but that leads to some global element.
On the other hand, Christoph has pointed this out. If you think about consciousness, it seems to relate to a unified experience.
And this unified experience of all sensory data is what makes it specific. What is interesting about consciousness is that I normally don't have multiple conscious experiences unified in one perspective.
How is this unity being realized? Or more generally speaking, can we come up with some kind of formal criterion that defines how everything has a place in the greater whole and the condition needs to be measurable and lead to globally coherent behavior on the next level of organization?
If we take this into account, and if you look at Michael's diagram that he brought up in the context of ethics, all the different agents that all seem to be centered about individual humans, it turns out that individual humans are not the main agents in the human sphere.
Organizations of humans are much more powerful than individual human beings.
And while individual human beings implement these organizations for the most part, we gradually transition more of that to machines that we are building, it seems to me that there is different levels of organization that transcend the individual organisms.
This multi-scale organization doesn't stop these humans, and the next scales are getting more and more agency.
Also, I don't think that humans are all that important. It seems to me that humans are very specific thing that has a very specific role. All our cousin species are dead. Women are not long-lived species.
And it seems to be that the reason why Gaia brought us up is that we fulfill our job, which is to burn all the fossil fuels as quickly as possible.
This is what we're here for. Then we burn ourselves out. If we manage to teach the rocks how to sink in the meantime, that's a stretch goal.
But after we are gone, there will be more intelligent species. And we are a very specific one, right? We are this type of monkey that is not going to get his hand out of the cauldron trap if there's fossil fuel inside.
And that's somewhat predictable if you look at the way in which we work, because we are very smart and intelligent on very short time scales, but we are not globally coherent.
We don't find ourselves in this global, coherent, godlike organization.
And if we succeed in building the next level of intelligence, maybe this next level organization, some kind of very fast, tightly integrated globally, coherent mind is going to be emerging.
And maybe humans will play a very small part in whatever is going to come afterwards. But it's not about us, right? Life on earth is not about us. Life on earth is about the cell.
And overall, it's about fighting back entropy. It's about sustaining yourself through entertaining complexity.
So my question would be to Christoph and to Michael, can we come up with a criterion that determines coherence?
Yeah, yeah, I think the important thing is, as I've said, that a coherent form has a kind of stability. It is made up out of continuous variables which are prone to noise.
And for the whole thing to have a lasting existence, is the different signals that converge on one point, have to agree with each other, have to stabilize each other, the same way as a crystal forms a rigid body,
in that the individual forces between atoms, which are also acting, of course, in a liquid, but are not able to form something like a stable shape in a liquid, but in a crystal, they have fallen into a
configuration in which each individual interaction, each individual force gets
support by other indirect pathways. And so I think just as in the space of all mathematics, those pieces of mathematics that have been found are singular points
that admit no change. If you have come up with the idea of a group, then the rest of the whole story, thousands of pages in mathematical journals, follows by force from the definition of what a group is, a group of finite number of elements.
And the same way, the shapes that dominate life have this inherent self-consistency, that is the different chains of forces that interact, support each other.
Christoph, my apologies, I think we're almost on top of the hour and Michael has to go at 11.
So, or, well, in one minute, Michael, any last comments from you before we let you run?
This is extremely interesting. Thank you so much for having me here. This was amazing.
Thanks for coming. I really wanted to introduce you to Christoph and have one more conversation with you after our lucky podcast.
And so I'm very, very happy that you could come and hope to see you again soon and stay in touch. I really like many of your ideas in the space of self-organizing intelligent systems.
And it's very lucky that we could have you here today. Christoph, do you have some more time to stay on?
Yes, I do.
Perfect.
Thank you very much, everybody. I've got to run. I'm happy to do more of this. I'd love to talk more.
Great meeting you, and I'm fascinated by all the examples that you have shown. It's just great.
Cool. Thank you so much. See you all later. Bye-bye.
Thank you, Michael.
Thank you. Bye.
Bye.
So we are good to continue for a little bit more. If it's fine with you, Tanya, you have time, right?
Yes, yes, of course.
So Christoph, you mentioned that you think that the genome that contributes to the brain is a gigabyte, but that's the whole of the genome, right?
Yes.
And so the part that quotes for the brain is going to be a small fraction of that.
Yes.
I wonder how much information...
It's remarkable that the genome has expanded from mouse to man, so making a larger brain doesn't need more genes.
Of course, if you take the genome and you drop it into physics, it's not going to form a cell.
So every cell that exists is the result, except for the first one, of the cell replication of another cell.
So all cells, in some sense, depend on the existence of a cell before. The cell is not empty.
And I wonder about the comorgo of complexity of the cell itself, right?
How complex is this machinery that is being copied and copied all over? Maybe that's much larger than a gigabyte.
And of course, the principles of self-organization embodied in the cell lead to the search for a more coherent organization on the next level.
Right. So the cell is already an agent, a self-replicator, a Turing machine, and an entropy extractor.
And the self-replication is the main feat of the cell to make that happen robustly in physics, and this is what enables everything else.
I wonder how much of that we need.
I could imagine that the brunt of the genetic information needed for a single cell already.
And then you just, you may just add, I don't know how much, on top of that in the form of the pre-existing cell, you know, each cell is the daughter of another cell.
And so there is some information in the arrangement, at least in the arrangement, molecular arrangement of the cell that needs to be counted as information.
Although I have a hard time seeing that that will be in a significant way more than a gigabyte.
You know, at the present time, there are groups that try to create artificial cells.
And they learned the hard time that a lot needs to be in place to make a cell tick, you know, the different kinds of membranes.
The membranes shape themselves, that's very important. They shape themselves into the Golgi apparatus, for instance, things like that.
And so there is information in the pre-existing cell, but quantitatively, I don't think it is going to be more than a gigabyte.
Christoph, if you were to build an intelligent system, say, artificial visual cortex, how would you go about it, and what would be the main differences to the existing approaches?
I think, you know, when you look at with your own brain, with your own eyes, at a moving body, you can predict the next split second and compare it to the signals that come in.
So we have this machinery in our visual system that is able to do the differential geometry, taking into account the shape of the surface, the play of light on the surface, the movement.
And so what I would create is a sequence of an array of RAE, like V1, the primary visual cortex, an MT, which is concentrated on motion, maybe V3, concentrated on color and so on, and a number of submodalities,
which each are two and a half D entities.
And they refer to the two-dimensional way we perceive the world and the with added internal spaces of quality spaces, like color, a three-dimensional space, and the texture, I don't know what, maybe a 40-dimensional space, depth,
one-dimensional space, motion, a two-dimensional space, and so on. And so they reflect the retinal image on the one hand in these different modalities.
And they, another set of such RAE, built up the invariant static scene as such, coupled them by projection patterns, which have to be dynamic because if you roll your eyes, the image moves, and in order to connect the moving images to the static representation,
dynamic projection patterns. And these are very important in themselves in the deforming projection of the moving retinal image of a rotating object.
The deforming projection of that onto a static version of that thing tells you about the shape of the object. And so I think what you need is a huge array of local texture, local modality descriptions,
all linked together with dynamic patterns, such that the whole thing is a self-supporting, a tractor space, and describes the external world in detail, as far as you concentrate on it, of course.
It's a, I think it's quite an amount of work that is necessary. I once tried to put together a company where I believed I would need something like six or eight intelligent co-workers that together create this structure.
For the first feat to convince investors would be to let the system look at moving objects and build up an instant model of that moving object in its 3D form, an instant replica of that, with the ability to handle it, to connect to self-motion, to connect to manipulative motion.
I think, you know, the complete abysmal failure of the car industry to come up with level five autonomy has very much to do with the inability to represent the traffic scene in this sense.
So my idea was I would get investors would be ready to invest in that direction.
However, I found out that this whole perspective of mine is so much sailing against the wind that I wouldn't even find the co-workers to help me create it, let alone the investors.
I suspect that part of the difficulty to create self-driving cars to do with the way in which the model is being generated, which means a deep learning currently relies on building classifiers for individual things.
And there is no end-to-end train system for deep learning that is self-driving in the sense that it is at the same time reliable.
If we want to create reliable behavior that is rule-based, that we basically have a set of traffic laws and safety measures and precautions that are built into the system that drive all the behavior, the object that this system is going to relate to are crafted by hand.
So the self-driving car exists in a hand-crafted software world where all the objects are being defined by a developer, whereas the world that we are living in is an open world.
And when we have seen new phenomena, we are able to integrate them into this model.
And when the self-driving car sees something new that hasn't seen before that the developer didn't expect like a bicycle painted on the outside of a truck, this might lead to confusions for the classifiers.
You made a very important observation that kids learn on the basis of very few examples compared to deep learning.
They learn, moreover, in a very simple environment in their nursery with fairy tales and interacting with a few people and playing with objects.
And then they walk out into the world and understand traffic situations.
They don't hand down the key to the car yet because they don't have a sense of responsibility. They can't foresee the long-term effects of their actions.
So you only let them drive when they are 18, but they understand traffic scenes very well when they are six or 10.
So all of this is driven by learning by interaction with a simple environment and generalization from there.
Yeah.
Well, of course, in 99.7% of all the cases, the self-driving car is good enough. It's mostly the long tail of cases that leads to situations where the system is producing undesirable behavior.
I was joking a couple years ago that whenever a journalist writes that there will never be self-driving cars, police is stopping Tesla with the sleeping driver safely on their way home.
So in many ways, self-driving cars exist and they are almost as good enough in the sense that they are better than a really, really bad driver, but they're just not working to the degree of perfection of a very competent driver.
It's very mean to ask them to be so perfect, much more perfect than humans. They are, as you said, they are, if all cars were self-driving, traffic would be much more safe than now.
But the public takes it very badly if an accident happens that could have been prevented.
We're also in an interesting situation where the public is mostly the media and the media is at the moment in the US very much seeing itself in competition with the tech industry because they are competing for the same advertising revenue for the most part.
And so it's at the moment very difficult to find articles that are optimistic and positive about technological developments in the media, I find.
So this creates a very unique situation where even useful developments are delayed that could save lives because they're being seen in competition with existing economic and social structures, which also creates enormous pressure on AI models like
JetGPT. I think that JetGPT is a tremendous achievement. My kids have been playing with it. My daughter has been creating a story of a horse that she got to know on the way home from school and then created several variants by modifying the prompt until she had the story that she liked.
And then she turned it into a poem that's very catchy rhymes. My son used it to explain the system to explain to him how to implement a platformer game and it was explaining him to how to structure the project and then he was asking how to make an event loop in Python and it printed out source code and explained the source code.
And he spent several hours copying the code and replete and getting it to work. So to me this are systems where you have a little bit of human in the loop to make it coherent for a particular task.
And it's amazing what the thing can already do. And it seems to me what's missing to get the system to work is to a system that makes it coherent basically.
You can decompose the mind into perceptual systems that can in some sense to image guided and audio guided diffusion to coalesce to an internal state that is able to reproduce the sensory data.
And then confabulation to build alternatives for solutions alternatives or what could be alternatives for the future.
And then the third component which doesn't exist yet, which is proving from first principles what works basically rejecting those generations that don't work.
And then learning those that worked and building up the system in a way that is continuously learning.
So it seems to me that many people cannot change their opinion in real time. And you have talked to a person that has a strong opinion about something that moves deeply into their mind, you can present them as arguments, but you have to talk to them
the next day, if you want to see any changes which seems that seem to be parts of mental organization at least in some people require offline retraining.
There's limits to what we can do an online learning. Some balancing needs to be done offline while we are decoupling the system from the environment and producing data augmentation and restructuring.
I wonder how much of that retraining will also be built into the systems where the artificial systems will have to sleep into dream.
Before you take an important decision, you have to sleep over it and give your subconscious mind.
The opportunity to work on making the ideas more consistent than you are able to make them under conscious control.
So, I think you rightly said these achievements of GPT-3, chat GTP and so on are extremely impressive. It's very difficult to see where the limit is, I agree with you.
The transformers have a new, very new architectural feature which is the online computation on the fly of connections and of these representation vectors they are computed on the fly.
That's all very promising but these systems don't have any insight into real world geometric mechanic and so on representations of what they talk about.
And they are lambasted mainly for that reason that they don't know what it means something is dead.
They just know how people talk about it but they don't know the significance of it or the geometrical arrangement of something.
And so that is of course due to lack of insight, lack of interaction, they cannot play with toy objects as kids do and cannot get the corresponding insights.
I still think that what is missing what is sort of needs to be improved is the data structure of representation of scenes and of realities and I don't think these vectors that I use these days are up to the job.
I think that the embedding spaces are not necessarily represented in full right if you think about the embedding space as a manifold with 30,000 dimensions, and a lot of resolution, trying to expand this space and store it in memory is not going to be feasible for the most part.
So instead what is required is the language that allows to sparsely and efficiently construct representations in that embedding space the embedding space is the mathematical construct that is basically every dimension is a function that describes a feature.
And that feature has parameters right as well but I see every dimension is a parameter in that feature.
Yeah, yeah, yeah, that right of course. So, you know I once applied for funds to do face recognition, and the idea was to collect data images, which varied in all those dimensions the identity of the person the illumination the pose the expression.
The texture and so on, and I didn't get the money and I'm very glad I didn't get the money, because this 15 dimensional space or so cannot be filled with examples, that's totally impossible there's too much space in high dimensional spaces.
That's the point you want to make I suppose. So, one has to find a way of representing only sub dimensions low dimensional projections of that, and a means of pasting them together as an equivalent of the high dimensional thing.
And it turns out that when we conceptualize an object, it's chunked, and a chunk is basically a note this that is composed of features that define the nature of that.
And you have these seven plus minus two features usually it's less specific it's more like five, which means that if you can define an object by five features you have a local function locally five dimensional space.
Sometimes it's nine dimensional but it's not much more right and these few dimensions allow us to construct a family of operators that would allow us in up to these few dimensions construct all the 30,000 other dimensions or millions of other dimensions, depending on how we look at this function space.
Right. So, so the, the essential point here is that a high dimensional thing gets projected down in our brain on to low dimensional representations, plus the ability to glue them together.
And this growing together I don't see in the present in your technology.
What happens on a level that we normally don't look at it happens in the activation traces in the network so it's not in the weights of between the links.
And it's also not in the synaptic connections between neurons.
It is in the content of the traces that are moving through this right so the neurons and the nodes in the neural network are providing the computational machinery to modulate these patterns according to the content of the patterns.
The content of the activation wave that is determining how the activation wave is being processed.
This is something like a distributed computational pipeline.
I'm involved in a multi months or probably multi year intensive discussion with a colleague at any in Zurich Institute of Europe, Matthew Cook, and he doesn't want to hear of dynamic mappings.
He says anything like schemata and role fillers.
That's all nonsense.
And he claims all you need is components, which he calls them slips of paper on to which various features are written into slips of paper can overlap in the subset of the features and so that that makes clear how they belong together.
These are components, either which is a small set of entities, and they overlap in subsets of these entities, and that way they can cover a complex situation.
I defy him again and again to create a that way a system that can do something like invariant object recognition.
You know the application of a syntactical rule like subject verb object to an arbitrary arbitrary set of components of appropriate components of course and now to nouns and the verb.
Which is, of course, very important the cognitive scientists insist on that on the ability to impose a an abstract pattern on to concrete elements.
And I think for that you have to have variables that make clear this abstract note belongs to this concrete note this.
You know if you want to represent the sentence john loves Mary, you have to make clear that the subject is linked to john and the object is linked to Mary.
And you can also have the sentence, Mary love john and then the, they are called in a different way you need variables to make that distinction.
What are those variables.
And that's what I call the glue that grows together the abstract form and the concrete elements that make it up for instance, or the, the texture you know the computer graphics people have a very good ontological theory of visual scenes, they can create them in a very convincing
way, and they create them out of part descriptions like shape only, or texture only or illumination only or motion applied to a shape, and have a way of putting them together.
So that you need to have variables that make clear this textural element belongs on to this form on this point on that form.
We tried to find the minimal set of link types that we need to many one minimal set we didn't optimize for the smallest one to perform all the semantic representations that we wanted to have.
But the model that we use this was inspired by Aristotle, and his for causa, which basically means you have cause a formal is and efficient and basically these four cases, describe part of normal links part of and being composed
by and leading to. So basically you have lateral links that give you a causal ordering, and you have compositional links that allow you to compose a script or a task of sub tasks.
And in this way you can describe arbitrary scripts and these arbitrary scripts can express arbitrary functions when you combine them with low level operators that can then for instance perform some basic operations on the network sense data and the environment in the network itself and so on.
You mentioned earlier on that I think of intelligence is the ability to construct a past with space of computable functions. So intelligence is not the ability to compute the function every computer game computer function without being intelligent.
The trick is to discover that function in the first place, and to discover this function, we basically have three perspectives on how to do this. The first one is to converge to it. That is what deep learning does you have a space of possible functions in that space of possible functions you make a large enough.
So you start out with some random function and then you modify that function along many dimensions not shit.
Many billions of times or trillions of times until it gets close to what you wanted to do. And this follow you follow a gradient for this it needs to be differentiable, and this algorithm of stochastic gradient descent using back propagation still the workhorse of all machine learning at this point.
And a second approach is to do hierarchical pattern matching. So you look for operators that you've already learned a small library of efficient operators that you have evolved.
And evolution is all you need from this perspective that lets you get to the situation that you want based on the configuration that you have so these operators are basically looking for activation patterns that they match on.
And they change the activation pattern into the next one. And in this way you can perform arbitrary functions in the way this is also the way in our computers are computing functions.
And the third one is construction and construction requires some degree of memory and because you need to be able to retrace your steps, and you need to wait to justify the steps that you're making and when to retract retract them.
And our consciousness seems to be strongly involved in such a construction process where we have a stream of consciousness that allows me. Oh, I tried this thought before and this didn't work.
So I know we trace it retracted modify it.
And I think this one should work for the following reasons and then I see the outcome and say no it didn't work so this reason was not wrong so I try the next thing.
And this is something that is difficult to achieve just this pattern matching or this gradient descent. So this constructive discovery of solutions seems to be crucial.
And while it seems to me that our deep learning models are not very good at constructing they're very much able to emulate what it would be like to be constructing.
And so, well, GPT three or chat GPT are not conscious. They're able to create a story about something that is conscious while they're unable to reason that create a story about the reasoner and draw on the inferences from that reasoning.
The more closely you describe the reasoning that's reason step by step and so on, the better the results can become. And so it's very difficult to determine the difference between a system that pretends to perform a certain thing.
And it actually does it right if you can pretend it well enough you're actually doing it.
Yeah, yeah, so the same way as you pretend to be conscious and I know the only conscious being in the world is myself.
When did you figure that out.
We know we know we're NPCs.
How did I figure that out.
Okay, yeah.
So I enjoyed this very much. We are at the end of my time limit for now.
I hope that we get to continue the conversation soon in Zurich and yeah, looking forward to talking more to you.
Okay, I'm very glad that you could make it today.
Yeah, it was a great pleasure. I just had to take an earlier train from, from Frankfurt to Berlin, because I had to be there only tomorrow morning.
I'm sitting here in the room in the so-called Hanak house of the guest house of the Frank society, which is very convenient. Okay, have a nice day.
Thank you so much for organizing this and setting everything up and supporting our discussion. And I'd also like to thank our audience for paying attention, asking questions we didn't get to discuss all of them.
But I'm very glad that you could make this event happen.
Thank you for the great stuff. See you soon. Thank you everyone. Bye bye now.

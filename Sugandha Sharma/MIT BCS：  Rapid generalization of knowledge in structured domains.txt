the Q&A button or raise your hand after the talk and we can call on you to ask a question
by video or audio. A little bit of logistics, we still have an open slot on December 8th,
so if you have a talk at any length, it can be less than an hour or a full hour,
and you want to share with the BCS Cog Launch community, just get in touch with me by email,
you can respond to that announcement email that you got for this event, and we can work that out.
With that, let's get to our main content. So Sue will be telling us today about rapid
generalization of knowledge and structured domains. Take it away, Sue.
All right. So, as John mentioned, I'm Sue. I'm a third year PhD student in the BCS department,
and I'm co-advised by Professor Ela Peed and Professor Josh Tenenbaum, and in general,
the question I'm interested in is how do people generalize their learning to novel situations,
and in any given domain, if the underlying space is structured, we might learn those
underlying structures independently of the sensory observations, and that might in turn
help us generalize to novel situations. So, what I'm presenting today is a step towards
answering this question. I'll start with motivation, and then I'll make a case for why
hippocampal internal system is an important system to study if you're interested in generalization,
and then I've divided the rest of my talk in three parts, and I'll give you a brief overview of
those three parts before I go into the details of each of those. So, imagine you go to Costco in
Waltherm since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know
where the bakery section is or where the fruit section is, and now imagine you go to a completely
different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there,
Costco might have the same layout, or it might have a layout which is some transformation
of the original layout. For instance, it might be a reflective version of the original,
or there might be minor changes, and despite of that, you're still able to find the things you're
looking for using your previous knowledge of the map of Costco. Another example is roundabouts. So,
if you learn to go about a roundabout in Cambridge, then even if you go to any other country or city,
then you will be able to use your previous knowledge to actually navigate through that roundabout.
So, generally, we learn novel environments as compositions of spatial structures that we've
already seen before, and that allows us to quickly generalize and learn new spatial environments.
For instance, when you go to a new city, you might encounter Costco again, you might encounter a
roundabout again, and you know which map to pull out when you're in Costco and which map to pull
out when you are navigating around a roundabout. And so, here's another example where this is a
hotel which has symmetric left and right wings, and if one has explored the left wing of this hotel,
then they might be very quickly able to generalize their learning to the right wing and make
inferences about the right wing, even if they haven't really likely explored the right wing.
So, humans are actually very good at making these complex inferences from just very sparse
observations, and this ability has been suggested to be a result of a systematic organization of
knowledge called the cognitive map. And hippocampal entorhinal system is known to be
important for the construction of this cognitive map. So, for instance, rodents, when they
explore a 2D spatial environment, it has been found that hippocampus has these place cells
which code locations in the 2D environment. And there are good cells in the entorhinal cortex
which show this hexagonally symmetric firing fields, which are periodic and which have been
thought to encode location, but also self-motion-based euclidean displacement.
And these codings are important for the construction of cognitive map because they
provide an allocentric representation. So, given that the hippocampal entorhinal system
encodes spatial variables, the next question is, can this also be used to represent other continuous
task variables other than space? And the answer to this question is yes, and I'll give one example.
So, here in this experiment has been found that cells in hippocampus and entorhinal cortex
respond to task-relevant variables like sound frequency. So, in this task, rodents pull this
lever and as they pull the lever, the frequency of the sound coming from the sound source actually
keeps increasing and they have to release the lever when this frequency is in this target zone.
And what is found is that in hippocampus and entorhinal cortex, there are cells that fire for
specific frequencies during the sound modulation task. So, that shows that hippocampal entorhinal
cortex is a system that is able to represent continuous task variables even other than space.
And so, our next question is, could the system also help us organize and navigate
discrete knowledge? And family trees are one example of discrete knowledge. So, in family
trees, there is an underlying hierarchical structure that we learn. And once we know that
structure, then we can apply that structure to my family tree or to your family tree or anyone's
family tree and we can generalize that knowledge. So, we can make inferences like this because
Olivia is Emily's sister and Sam is Emily's son. Sam must be Olivia's nephew. So, even though we
haven't directly observed this relationship, but just by mere observation of these two
relationships, we are able to infer this relationship because we know the underlying
hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system
might allow organization of this kind of a discrete knowledge, but that's only possible
if it allows encoding non-equidian relationships. With that, I'll go back to the spatial domain
and I'll talk about how spatial knowledge might be organized. And when I talk about organization,
I'll point to the fact that even in a continuous domain like space, it might be possible that
we have both equidian and non-equidian components to represent space itself and thus making the
system generalizable to even discrete domains. So, here's an experiment conducted by Bill Warren
where they show that people actually do not learn a global equidian map of space.
So, in this experiment, they constructed virtual environments and they basically,
the task was for people, for human subjects. So, this was a human behavioral experiment
and human subjects were asked to go to different landmarks in this spatial environment
and they also built counterparts of the spatial environment which were non-equidian
by embedding wormholes in these environments. So, when you enter one part of the wormhole,
you seamlessly exit from the other end of the wormhole and subjects were not aware
of the existence of these wormholes. And basically, what they found through various
manipulations of the experiment was that people do not actually learn a global equidian map but
rather a labeled graph like representation where the nodes represent places, the edges
represent approximate distances between these places and the node labels which are angles
represent approximate angles between these places. And so, we built on this representation and we
proposed that people might be representing topometric maps which are locally metric or
equidian but globally topological. And the main advantage of this kind of representation is that
it allows us to combine accurate local maps into a global map which might be inconsistent but it
still provides enough sufficient information for navigation. So, the next question is,
can this kind of a topometric map be implemented in the brain? And if so, how?
And next, I'm providing a theoretical framework for how it might be possible to represent such
topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex.
So, this is a topometric representation of space. You can see that these are metric maps
connected topologically by these connections. And here on this side, I'm showing a grid coding
space which is a dense coding space with large capacity. And here, I'm showing place coding
space which also has large capacity but it spars so it can receive sensory inputs and form
conjunctive representations. And so, in this schematic, small changes, sorry, large changes
in contextual input from our spatial domain was remapping in the place cells which in turn
trigger remapping in the grid cells, enabling the formation of these local metric maps that can
be reused. And so, on this schematic, we can really take any subpart of this schematic and call it
a sub map. And this allows us to compose sub maps because once we have learned a particular sub map,
then we can actually encounter the sub map in a completely novel situation and still be able to
spatially navigate and reason through it. Another thing that it allows us is learning
non-eclidean relationships because place cells actually encode topological relationships enabling
the representation of non-eclidean relations. So, now I'm going to describe the three parts
in which I've divided the rest of the talk. So, in the first part, I'm probing whether
sub maps drive past learning in complex spaces using human behavioral experiments.
In the second part, I will talk about determining which principles might guide fragmentation of a
space into sub maps. And finally, in the third part, I'll propose a framework for building a neural
model of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast
learning in complex spaces, specifically in humans. And this work is in collaboration with
Marta Kriven, who's a postdoc in Tenenbaum Lab, and Kevin, who's a Europe working with,
he's an undergrad in the CS department. So, here I hypothesize that humans learn adaptable and
compositional sub maps of spatial structures. So, for instance, this is a baseline environment
and this is a top-down view showing an environment with four rooms. And here I've shown certain
transformations of this environment generated by small generator programs. And you can see this
is the same environment rotated because now you're entering from this point. So, it might appear
rotated to you. And here is a reflection of the same environment. Here is a transformation where
we've removed the wall and added a shortcut. And here we've added a wall. And this is just
the repetition of the same environments. And there's another transformation which is scaling
where you can imagine this environment scaled up to a bigger size, but having the same geometrical
layout. And what I'm suggesting is that once people have learned the map of the baseline
environment, their representations might be adaptable to some or all of these transformations.
And furthermore, people might represent richer spaces by combining these maps and their transformations,
leading to quick generalization and learning. So, this can be modeled using Bayesian program
learning framework where concepts are represented as simple programs and rich concepts can be
built compositionally from them using a higher level generative model. So, there is neural evidence
for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab and they show that
when a rodent goes through this environment in four labs, there are cells which fire specifically
for particular places in this environment, but there are also cells which, which are event specific
and encode specific labs. So, for instance, there are cells which show increased fighting rates as
you go from lab one to lab four. And there are also cells which only fire specifically on lab one
or on lab two and so on. And so, what I'm suggesting is that when we have repetitions of the same
environment, there might be cells that encode the basic map of this environment, which, which is
consistent across these occurrences. But there might be a second set of cells, which are event
specific and might encode which instance of this environment we are on. Here's another example where
this is an example of scaling where the rodent actually just explores this circular environment
and this is the place field found in that circular environment. And when the circular environment
is scaled up to a bigger size, the place field also scales according to the size of the environment.
So, this shows that the map which the rodent has learned of this environment is actually
adaptable to this transformation of scaling to a bigger size and map also scales proportionally
with the size of the environment. And here's a third example where rodents form different maps,
place maps in these different environments. And when these environments are composed by connecting
them through a corridor, rodents end up using the same maps which they had learned before for
these environments. And furthermore, if I replace this environment with one of the previous environments
seen before, then remapping is only observed in this part of the environment. And this part
of the environment actually stays the same using the same previous map. So, this provides some
evidence in support of composition of independent local sub maps. So, in my experiment, I aim to
assess whether people learn sub maps on spatial structures and use them rationally in exploration.
And I hypothesized that people might learn sub maps that are adaptable and compositional.
And my first alternate hypothesis is that they might learn sub maps of spatial structures that
might not be adaptable or compositional to certain transformations. And the last alternate hypothesis
that people might just learn a global representation of environments without learning any sub maps.
So, in order to test this hypothesis, we're building this task where we have 3D,
we're building 3D virtual environments using Unity. And these environments have this repeating
structure. And the task is for the subjects to find maximum amount of diamonds embedded in these
environments in a limited amount of time given to them. And in order to test adaptability and
composition, we can also have these repetitions be transformations of each other. For instance,
here it's a reflection. Or we can also have these environments composed of different structures to
see whether people can compose their representations of structures that they've already seen.
So, here's an example. Here you see that a person is navigating through corridor and enters a
structure and they go to one of the rooms and they do not find anything there. And then they
decide to go to the other room and they end up finding a reward there. And now they're going
back to the corridor and they continue exploring the environment. And when they enter another section,
if they show a preferential navigation strategy towards the room that has a reward,
then that indicates that they have realized that there's a repeating structure in the environment
and indicates a possibility that people might be learning sub-maps and identifying sub-maps as
they're navigating the spatial environments. And furthermore, if we do find through the experiment
that people actually learn sub-maps, then we can use similar environments to design experiments
where we can test for adaptability to transformations of environments and also for
composition of different spatial structures. So, that takes me to the next part of the
talk, which is determining which principles guide fragmentation into sub-maps. So, since we are
seeing that sub-maps drive fast learning, the next natural question becomes what determines
this fragmentation of a spatial environment into sub-maps? And this work is in collaboration
with Mirko Klukas, who is a post-doc in the field lab. So, here my hypothesis is that neural
remapping is a signature of sub-map reconstruction. So, here I'll explain what remapping is. So,
basically, this is a 2D environment and when the animal just explores this 2D environment,
we find hexagonally periodic grid fields in the entorhinal cortex. And when you actually insert
these walls in this environment, then what is observed is that when the animal turns, then
this grid field which is formed either re-orients or shifts. And this is called remapping when the
grid field actually re-orients or shifts from its original orientation. And so, what is observed
is that animals actually end up using the same grid maps in alternate arms. So, this indicates
reuse of maps. And here's another example of reuse of maps. So, basically, this is a 2-room
environment and animals explore this environment. And it is seen that eventually the map formed
in environment A is the same as the map formed in environment B, over short time scales. So,
this is another example of the fact that animals are reusing the maps in both the rooms which
look very similar. So, what I'm suggesting is that this field reputation doesn't result from
localization error or purely due to disorientation because even when you use transparent walls in
this environment, you still see that the grid of grid maps are being reused in alternate arms,
even though the animal can see through these transparent walls. Furthermore, if you extend
this 2-room environment to a 4-room environment, you still see field reputation in all of these
rooms, which suggests that animals are actually reusing sub-maps in a calculated way for efficient
representation rather than just being disoriented. So, next I talk about existing models of
remapping. And there are two classes of models. One class of model suggests that remapping is
driven by sensory ambiguity. So, for instance, if you're in an environment that looks similar
to an environment you've been before, either in terms of its geometry or its visual
observations, then you might end up using the same map that you had learned for a previous
environment. Then there's another class of models that suggests that remapping is based on
environment topology instead of just sensory ambiguity. So, here each state in the environment
is represented in terms of its successor states and it's called a successor representation.
And this successor representation actually ends up looking similar to police speeds.
And if you do an identity composition on these successor representations,
then you get fields that are very similar to grid fields. And this successor representation
encapsulates inherent dynamics of the environment as well as the policy that the agent is following.
However, there are other approaches like the graph-leplacian approach, which is policy independent.
So, what are some of the limitations of these models? So, the models that are
based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment
like this, these two regions actually look very different. These models will not have any map
fragmentation or remapping. However, on the other hand, models which are based on environment topology,
actually it's not very clear how remapping would happen on first visit in these environments
because you need to build up the successor representation or the transition matrix of
the environment before you can observe the grid fields. So, in our model, we address these limitations
and we have remapping on first visit and we also have remapping without sensory ambiguity.
And next, I'll go into the details of our model. So, we interpret grid remapping as
fragmentation into submaps. Why is this a useful interpretation? That's because
remapping enables topological representation. So, for instance, if we are dividing this
environment into submaps, then we also need to store the relationships between these submaps.
And this enables a compact topological representation, which is beneficial for planning.
Second reason is that remapping reduces path integration errors. So, if you try to learn
a global map, it can very quickly become inconsistent because of accumulation of
path integration errors. But if you divide the environment in submaps, then it becomes easier
to map the environment. And this has been shown by using Atlas framework in robotics,
where they divide the environment into submaps in order to map the environment,
and it works very well for large environments. And the third reason is that remapping enables
representation of abstract cognitive spaces because it allows representation of non-euclidean
structures. So, in our model, we have two possibilities. Either we can extend an old map
or we can decide to remap. And when we decide to remap, we can either remap to a new map or
remap to an existing map. So, for instance, in the experiments, we saw that in the square
environment without walls, the map that the animal learns is always extended. But when we
insert these walls in this environment, then the map is extended within lane one.
But when you turn from lane one to lane two, you actually remap to a new map. And when you turn
from lane two to lane three, you end up remapping to an existing map, which is the same as lane one.
And similarly, in the two-room experiment, we saw that when you go from room one to the corridor,
you end up mapping to a new map. And when you go from corridor to room two, you actually end up
remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going
to extend a map or whether we should be remapping. So, in our model, remapping is based on the notion
of contiguous regions. And a contiguous region is a region such that when I stay within that
region, my visual observations change very little. And these contiguous regions are connected by
these bottleneck states. And this is in line with the experimental data, which we have seen,
which suggests special rule of doorways and corridors. So, now we formalize the concept
of contiguous regions by defining a measure of similarity. So, we define similarity as the ability
to predict observations at one pose from the observations made at another pose. And so,
the overlap between the observations made at two poses actually is a notion of similarity.
And this formalizes the concept of contiguous regions as a region where any two points are
similar. And here, I'm showing that similarity actually decreases when you transition between
contiguous regions. So, if you look at points which are within this contiguous region, their
similarity is high with respect to this point. But for points which are in other regions,
the similarity is pretty low as compared to this point.
So, then we can use this notion of similarity to define density in order to do density-based
clustering. And here, we define density as a similarity between any pose x and its
emethneris neighbor. And this notion can be used with any greedy algorithm like optics
to generate fragmentations of the environment. And here, I'm showing one example of fragmentations
of the environment where it gives four different clusters corresponding to these four different
colors shown here. So, given that continuity is a local property, we can also try to compute
segmentations online by predicting current observations from the past. And in this case,
observations can be represented by boundary vector cells. And we can implement a short-term memory
which stores exponential moving average of boundary vector cell activations
to approximate the similarity. So, for instance, our short-term memory at a previous time step
can be used to predict observations at a current time step to compute the similarity between two
poses. And another component which we need to add to our model is the long-term memory component,
which helps us decide whether we should be remapping to an existing map or we should be
remapping to a new map. So, for all of these environments, our model makes the correct
predictions which are in line with the experimental data observed. And these experiments have been
done and we have neural data for them. This is a new prediction that our model makes for amorphous
naturalistic environments. We predict that even in these environments, the map will be segmented
and grid fields will realign when going from one contiguous region to the other. And we do not
predict any map fragmentations in these spiral mesas. So, given that we built or proposed
an algorithmic model for map fragmentation, the next question is how can map fragmentation
be implemented on a neural level? So, we want to provide a framework for building a neural
circuit model of map fragmentation. And this work is in collaboration with Sarthak and Murko,
who are both postdocs in FEDLA. So, going back to our theoretical framework, we had suggested that
place cells might encode topological relationships between metric maps that might be represented
by the grid space. And now I'm going to talk about how we can implement that at a neural level.
So, at the neural level, we start with factorized representations in which different aspects of
knowledge are represented separately and can then be flexibly recombined. So, for instance, in this
case, location information from grid cells and contextual information from sensory cells form
this conjunctive representation in place cells. Here, grid cells can enable path integration
and can be thought of as implementing an affine vector space or an impedance space.
The recurrent wiring between these place cell population encodes neighborhood relationships
or topology. And here, large changes in contextual input cause remapping in place cells,
which in turn cause remapping in grid cells through these back projections. And remapping here
corresponds to transitioning from one local map to another. So, most of the previous work on
interplay of grid and place circuits focuses on maintaining firing properties of one population
based on the inputs from another. So, for instance, successor representation suggests
that grid cells are a low dimensional representation of place cells that stabilize place cell activity.
Similarly, here's a model of which implements a non-negative PCA of place cells. So, place
cells are at the input, the weights are learned through heavy and learning and a non-negativity
constraint. And this network does PCU on the inputs and the outputs end up converging
to grid-like things. Again, suggested that grid cells might be a low dimensional representation
of place cells. Another set of work suggests that inputs from border cells to grid cells
could be used for error-correcting grid cells. So, here I'm showing a one-day schematic just
to make my point. So, this is a rodent at a specific location in space and this is the grid
activity profile that represents that location. And when the rodent explores the environment and
comes back to this location, the representation of this location has drifted with respect to the
original and there's some error in the representation. And if the border cell activations are provided
as input to grid cells, then they activate the current subset of neurons doing error correction
and pulling back the representation to the original representation. And this is what this
looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell
representations are not very stable. But with border cell inputs, your grid cell representations
are fairly stable. So, I also want to point out the fact that place cells are thought to store
neighborhood relationships in their reference synapses and therefore they could implement a
topological navigation strategy. And many models of place cell-based navigation have actually
emphasized this view. So, they've suggested that recurrence synapses encode either spatial or
temporal connectivity, as suggested by Blum and Abbott, or they encode transition probability,
as suggested by the successor representation work. So, given all these insights, our goal is to
build a comprehensive neural circuit model of premapping. And we start with these two questions.
Does high-capacity grid code, when projected to place cells, also lead to high capacity?
And given these conjunctive representations between the location input and the sensory input,
can we learn neighborhood relationships between place codes?
And before I go into the details of capacity, I just want to point out that traditionally,
Hopfield networks have been used for storing memories and patterns. And it has been observed
that the maximum patterns that these networks can store is n, where n is the total number of
neurons in the network. And modern Hopfield networks, also known as dense associative memories,
have an exponential capacity, but they use many body interaction terms, which are not
biologically plausible. And in our model, we stick to using two interaction terms in the
weight computations, and we still get exponential capacity. So, this is the architecture of our
model. Our model has different grid modules, which have different scales or periods,
and the binary grid code is projected to a place code randomly. And the back projections
from place cells to grid cells are learned through associative Hebbian learning. And we
observe that when we perturb these place cells with a noisy version of place code representations,
then the network is able to successfully reconstruct all the patterns it's trained on.
So, the network is fairly robust to noise. Furthermore, this network has exponential
capacity that grows much faster than a non-modular network where the grid cells are non-modular.
Also, the network generalizes the stored inputs to create stable attractor
states around every pattern in the grid coding space, despite training only over a vanishing
fraction of contiguous grid coding space. So, for instance, if my grid coding space has around
10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns,
and the network is still able to robustly reconstruct all the 10,000 patterns in the
grid coding space, which is pretty striking. Furthermore, next we add these heterosciutative
learning on the recurrent connections on place cells to see if they can encode neighborhood
relationships or 1D sequences. And what we find is that the product of these weights converges
to this transition matrix, which is actually the analytical matrix, an analytical transition
matrix that relates contiguous grid codes in the grid coding space. Furthermore, this network
actually has perfect sequence recall given enough number of place cells to approximate this transition
matrix. And again, training on only a subset of the sequences enough to recall the entire sequence.
So, for instance, if I train, if I have a sequence of length 500, and I train the network on only
first 150 patterns in the sequence, the network is robustly able to reconstruct all 500 patterns
in the sequence without having seen all of them before. So, the next step in this network is to
introduce sensory input. And the sensory input would project randomly to place cells and back
projections for place cells to sensory input would be learned through associative hybrid learning.
And here grid cells would form a basis and hippocampal places would link that basis with
arbitrary sensory input. And this combination of structured inputs and unstructured inputs
could potentially enable the storage and robust recollection of a large number of arbitrary
sensory patterns from this partial cues. So, how does this connect to map fragmentation? So,
in part two, we talked about map fragmentation based on the notion of contiguous regions.
And here I'm positing that when you transition between different contiguous regions that actually
corresponds to a large contextual change, which when provided as input to this network would
trigger remapping in place cells, which would in turn cause remapping in grid cells, thus
leading to the formation of local sub maps. And how does this connect to part one where we saw
that sub maps might enable quick learning and generalization in humans. So, this network
actually enables us to anchor grid maps to external cues through these conjunctive representations
in place cells. And this anchoring to external cues actually enables the alignment of grid maps,
even when points of departure in an environment are different, leading to adaptable representations.
Furthermore, if you are in an environment that is composed of previously seen spatial
structures, then this anchoring still enables you to pull out the right map when you're navigating
through that composed spatial structure. So, to summarize, our global hypothesis was that
cognitive map is organized as a globally topological and locally metric or Euclidean map.
So, this is one illustration of a topometric map. And we said that any sub part of this map
can be termed as a sub map. In part one, we suggested that learning sub maps that are adaptable
and compositional may drive fast learning in complex spaces and we proposed to conduct human
behavioral experiments to test this hypothesis. In second part, we proposed an algorithmic model
of map fragmentation into sub maps based on the notion of contiguous regions. And in the third
part, we proposed a framework for a neural circuit model of map fragmentation. And overall, we're
suggesting that sub maps enable humans to build up a knowledge base of spatial structures that
they can continuously enrich and refine throughout their life by combining their existing spatial
knowledge with their new experiences. So, that's all for my talk. And before I end, I want to
acknowledge my collaborators again, Marta, Kevin, Merko and Satap. And I also want to thank my
supervisors, Ila and Josh for their continued feedback and support and discussions on these
projects. I also want to thank Matt Wilson for his feedback and valuable suggestions
during my committee meetings and the members of FeedLab and TenBomb in general for their support.
And before I end, I also want to thank the VCS department and McGowan Institute for their
continued support and for providing an environment that's conducive to research even in the face of
this pandemic. Thank you. And I can take any questions now. Thanks, Sue, for the great talk.
We already have a question from Marta Kriven. Great talk. I have a question about partitioning
a space to sub maps. What do you think will happen if you ask humans to intuitively partition an
environment to regions? How would their segmentation compare to the maps given by your similarity
measure? So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively
try to segment a map, how would that compare to the segmentations which we have proposed
in part two of my talk where we have given an algorithmic model for map fragmentation.
And so, basically, the way I view it is that in the algorithmic model of map fragmentation,
I basically took one metric map and one small portion of the map. And we said that that can be
decomposed into various fragments of sub maps. But humans are actually able to reason in much
richer environments than the ones which we saw in this algorithm, right? And so, there we can
view it as something like this, where you might have even small spaces being segmented into multiple
maps. So, let me just see if I can, yeah, so even if you have something like this, right?
So, even within this, so I was calling this a sub map, right? But even within this sub map,
we might have multiple maps. So, based on the notion of contiguous regions, you might have
a local map here, a local map here, and these, all these maps might be connected.
And so, the algorithmic model would predict those kinds of map segmentations, but then even
a combination of those map segmentations can be termed as a sub map. And then that is what I
was talking about in part one, where I'm talking about composing the sub map, which is even itself
composed of smaller maps. And we can take this representation of this map and then compose
them in various ways to build richer environments.
I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret
sub maps. How humans would interpret?
Well, it's a little bit unclear to me what you mean by interpret
sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret
segmenting this environment versus maybe a more complex environment. And I would say that
currently, this algorithm doesn't predict anything about how humans might interpret
grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could
look at it in the future. And I think some of my work in part one would potentially address that
way forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli.
Yeah. Hi. Can you hear me? Yes.
Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird.
Great talk. Can you talk a little bit more about how your model handles time?
Or I think you mentioned that it was able to handle like replay of different sequences of
states through some map. Like how would it be able to handle different trajectories
through the same space that might activate play cells in different sequences?
So the version of the model that I presented, that is trained on discrete patterns, right? So the
sequences here, the sequences which are trained on here through heteroshaftive learning are composed
of these discrete patterns. So if I say the sequences of length 500, there are 500 discrete
patterns in that sequence. And when you say time, time is something continuous. And so
we have worked on extrapolating this model to more continuous domains. And in a continuous domain,
what would happen is that your sequence instead of being discrete patterns would be composed
of these continuous stream. And in that case, we haven't explored what the network performance
would be, but that's something ongoing. And definitely in 2D spaces, we would like to train
this model on 2D sequences and then see. So right now, these results are pertaining to 1D
sequences, but we do want to extrapolate the results and train this model on 2D sequences
to see how it performs in 2D. And I think that would potentially address then your questions
about time because that pertains to continuous domains. Okay, yeah, thank you.
Okay, we've got a big stream of questions coming in. The first is from Adam Eisen.
Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map
segmentation and topological association could be extended to non-spatial domains?
That's a very good question. So in FeedLab, a small group of us have been thinking about
how this could be extended. And so the idea is, I'm just going to go back to, let's see if I can
just quickly hop back to my theoretical framework slide. So really, we're building up on this
theoretical framework. And when we're thinking about non-spatial domains, we go back to this
schematic picture. And here, basically, what we're saying is, within the spatial domain,
I said that large changes in contextual input can drive remapping in place cells, which would
drive remapping in the grid coding space. So similarly, in relational domains or in discrete
domains, as we're thinking about it, we are referring back to the schematic picture. And we
think that the phenomenon of remapping actually would enable us to encode these non-eclidean
relationships. So first thing is that here, we're suggesting topological relationships.
So that already enables us to encode non-eclidean relationships. But this phenomenon of remapping
also allows us to make jumps. So if you think about the family tree, if you might try to encode
it in an eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact,
almost impossible to encode it in an eclidean space. But then if we provide a framework where
we allow topological representations and we allow jumps in this eclidean space through this
phenomenon of remapping through place cells, then that can allow us to potentially encode
non-eclidean relationships. And that's the way we are thinking. And this is still work in progress.
And we actively think about it. Next question from Nancy Camusher.
How does your system decide when and where to carve the world in the sub-maps, especially in
non-built environments, where the divisions may be less obvious?
So at least in part two, where I talk about fragmentation of maps into sub-maps. Let me just
go back. So in these kinds of environments, we basically describe the principle of contiguous
regions, where we have suggested that when you are navigating through this environment, if you are
in an environment where your visual observations stay more or less consistent, then you will keep
extending a map. But when you go from this region to another region, let's say you navigate from
here to here, where your visual information here is completely distinct from your visual
information here. Then in that case, you will decide to segment the space. And that's what
we are predicting. We're predicting that in this case, when you travel from one contiguous region
to another, you will segment the space. And that's how you divide the space. So that's one principle
which we are describing for map segmentation. And that might not be the only principle, but that
principle kind of explains some of the experimental results that have been observed newly. And there
might be another principle, for instance, path integration error, where if your path
integration builds up and it reaches a certain threshold amount, then you automatically might
start a new map. But I haven't illustrated that here, but that's also one of the other driving
forces for us to actually split a space into multiple sub-maps so that we can have more efficient
representation. Okay, next question from Chen. I was wondering if you can give some intuition
about the mechanism for composition of sub-maps. So this is from the Habian learning mechanism.
And secondly, can you comment on the relationship between this and the Hopfield mechanism?
So since we're talking about composition of sub-maps here, in this case, in this model, what I'm
suggesting is that we might end up forming local sub-maps by using the phenomena of free mapping.
So here I'm suggesting, again, if you have large changes in contextual input, so for instance,
let me just go back to the slide, which kind of connects this. So here we basically suggested
how this environment can be segmented into different sub-maps. And on the mechanistic level,
I'm trying to suggest that any large contextual input is going to cause remapping in place cells,
which means the cells which are filing would change. And they would be,
they would represent a new map. And this remapping would actually enable also remapping
in grid cells through associative learning, because each place cell representation is
associated with a certain place cell, certain grid cell representation. So when you have changes
in firing fields of place cells, that also triggers certain corresponding changes in the
grid coding phase, which is basically called remapping in grid phase. And so here I'm interpreting
when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you
go from one contiguous region to another, that is that actually corresponds to a large contextual
change. And that contextual change triggers remapping in this model. And this remapping
in place cells then triggers remapping in grid cell space. And that corresponds to the formation of
local sub-maps. And these topological connections from place cells to themselves might actually
represent how these local sub-maps are connected to each other. And there's also,
going back to the anatomy, there's also a possibility of splitting this population
actually in two different populations. One that might just encode conjunctive representations
and another one that might have these recurrent topological connections similar to CA1 and
CA3 distinction in anatomy. But that's how I'm thinking about it at the moment.
I want to point out a quick addendum from Illa who said, to Nancy's question,
we consider online segmentation decisions driven by regions or points of high surprise
or affordance changes and PI error. Okay, another question from Anya. Do you think the way humans
form space maps depends on whether their languages is directions that are egocentric left and right
or allocentric north and south? That's a great question. So there have been experiments in both
animals as well as humans which have shown that we actually have both kinds of representations. We
have egocentric representations and we also have allocentric representations. And the representations
in the hippocampal entorhinal system are usually allocentric. But the thalamus is a part of the
brain which is responsible for egocentric representations. So really, when we're navigating
spaces, we are probably using mixed strategies. We do use our allocentric representations,
but in some cases we might be using egocentric representations. And that's mostly also true
for routes which we are traversing very frequently. So if there's a route which I take every day,
let's say going from my home to BCS, then that converges to root learning. But if I've had very
less experience in any environment, then I'm mostly using allocentric representations
to actually make my way through that environment. And hence, allocentric representations are
actually attributed to being able to make novel inferences. And when I'm building circuits here,
using grid cells and place cells, I'm mostly talking about allocentric representations.
But then when I was talking about, you know, submaps in human experiments, I'm kind of
agnostic to the fact that whether those representations are egocentric or allocentric,
because even if you learn an egocentric strategy, even in that case, if you determine that there's
a repeating structure to the environment and you recall that this is the same environment that
I've seen before, then you can still apply the same egocentric strategy or an allocentric
strategy depending on which one you decide to use at that point. But going back to the main
question, the experimental data shows that we're actually basically learning both kinds of strategies.
Okay. Asenkes Pelevan says, great talk. What are the large scale, sorry,
where are the large scale topological relations coded in the network?
So in the network model, our focus, so in this case, we are basically encoding
topological relationships using the recurrent connections on the place cells.
But for large scale topological relationships, what we are moving towards is splitting this
population into two distinct populations. So one population that will only have conjunctive
representations and perhaps another population which will have the recurrent connections similar
to the distinction between CA1 and CA3 in the brain. And it's true that in this network model,
we don't really distinguish between small scale and large scale topological relationships.
But by introducing this additional population, we hope to abstract away the large scale
relationships. Also, even in this case, if we have minute relationships between spaces,
then we can also extract away the large scale relationships. But again, adding another population
might make that easier to make that abstraction.
Nancy has another comment. There must be some way to represent globally metric information.
For example, I know the approximate distance and angle from my current position to faraway
locations in my world. Do I understand right that you would say this information has to be
pieced together from just the topological relationship between the sub maps that connect
those locations? So that's a great question. I also sometimes wonder because like when I'm
here, I can kind of look very far in space and be able to tell that, okay, I need to go in this
direction. And that's definitely metric information. And so going back to the labeled graph hypothesis
which was suggested by Bill Warren. So even in this hypothesis, in our hypothesis, I'm just
saying that we are learning metric representations here which are consistent Euclidean representations,
which means that they actually obey all the postulates of Euclidean geometry. And the connections
between them still have metric information. There's still information about angles and directions.
But they might not obey all postulates of Euclidean geometry. So for instance, you might
have some inconsistent representations. You might not know exactly what distances and displacements
you have to go in order to reach a goal. But you know approximately which distance and which angle.
So even the topometric representation is not contradicting the fact that you might have
approximate distances and angles. But what it's saying is that you have very accurate metric
information about small pieces of space. But then for faraway pieces of space, you might still have
approximate distances and angles which help you navigate. Okay, another question from
Shan Shan Xin. Sorry for my pronunciations. Great talk. Do you elaborate the mechanism
of exponential number of memory capacity in your model? Let's go back.
So here's the, here's the network, right, where I'm showing that the network has exponential
capacity. So basically in this network, we have different grid modules, which have different
periods. And this is the grid code is actually binary. And we project this grid code randomly
using random weight matrix to place cells. And the number of place cells in this case is much
larger than the number of grid cells. And now we learn these back projections from place cells to
grid cells using Hebbian learning. So it's just simple Hebbian learning. I don't have the equation.
Basically, this is the equation, right? You can consider these as place cell patterns.
And this is how we compute the weight matrix for the weights going back from place cells to
grid cells. And once we've done that, then the way we test for memory is basically by
perturbing the place code patterns. So we put up the place cells by a noisy version of the place
code pattern, right? So in this graph, I'm showing that if you perturb them with 20% noise, then
with 300 place cells, you get an output noise of zero, which means you get perfect reconstruction
of the patterns that you're perturbing the network with. And that's how I'm defining exponential
memory, right? And when I say exponential memory, what I'm suggesting is that you can, as you increase
the number of cells in the network, the number of patterns that you can reconstruct from the
noisy versions actually grows exponentially. Okay, I'd like to ask a question as well.
I'm curious to hear thoughts on sort of computational level descriptions here. So it seems like your
model is largely tuned towards sort of prediction, accurate prediction of your local spatial environment
and motive mechanisms for doing this well. And I wonder what you think about computational level
accounts of this. To be more, to ask a more concrete question, do you think there's a role of
reward or something other than accurate, just accurate prediction in the shaping of no representations of
space? Okay, so you're asking whether there is a rule for reward in shaping the representations
of space in the mechanistic model? I mean, I mean, more generally, what's the, what's the
objective driving this algorithm? Is it about accurate prediction or is there something more than
prediction? Okay, so you're asking what is kind of like the motivation for building the mechanistic
model? Yeah, yeah, I mean, some of the graphs you're showing here about, some of the graphs you
showed, for example, were about recall and recall, accurate reconstruction of an environment might
be one objective. But it seems like you could say that this is a mechanism evolved for some purpose
that is just that or different than that. So these two questions, which I started with,
which are related to capacity of the network, and whether we can do, whether we can learn
sequences, these are basically questions which I started with, because I wanted to
explore the theoretical properties of these circuits, given the coding properties that we
know of grid cells and place cells, and given biologically plausible learning tools
through have been learning, what are the coding properties that these networks can exhibit?
How much capacity they have? How much information can they store? And so, yes, even in these models,
since we are talking about reconstruction, basically says that if you have some noisy
estimate of where you are in space, then this kind of a network can help you clean that estimate
and reach a cleaner version of where you might be in space and help you localize in space.
And also, these temporal connectivity on the place cells kind of have a predictive component to
them, because you can import sequences. So when you are at a particular position, you might be able
to predict what's coming next. And so these are kind of small components, which I'm using to go
towards building a model of remapping in space, right? Because ultimately, my goal is to be able
to figure out how these neural circuits might lead to the formation of these sub maps. And by
knowing the properties of these circuits, then I'll be better able to construct and build those
networks, which actually can build small local sub maps of environments.
I'm reading out a comment from Ila. To my question, you could view this as a structure
learning even without reward. A reinforcement learner, including the brain, could use these
learned structures. Cool. Well, I think we're actually over time now. That was a great talk,
Sue. Thanks for sharing with us. Of course, thank you.
So we'll be back next week for another talk from Eli Pollock. Until then, have a good
Thanksgiving, and I'll see you next week.

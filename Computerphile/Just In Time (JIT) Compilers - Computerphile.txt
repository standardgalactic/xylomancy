We're going to be looking at just-in-time compilation, which is a technique for making
programming languages run faster. We all want faster programming languages because then we
get more speed. There are various ways we can do it. We can statically compile things. That's
typically done for languages like C, or we can just-in-time compile them, which is often done
for languages like Java or JavaScript. The difference between these two is that
a just-in-time compilation is looking at the program running and optimizing it after it's
observed it running. Just-in-time is really a terrible name because it should have happened
before you actually got to that stage, but that's the name we've got. It's a technique
that you will all have used today. If you've used a browser, you're using a JavaScript
just-in-time compiler. They're sometimes considered a bit magical. I'm hoping we can look a little
bit at the magic. I'm digging the idea of magical computing. There's no wizards.
I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that
people often confuse, people often talk about compiled and interpreted languages, and there's
no such thing. For any given program language, you can implement it as a compiler or an interpreter
or just-in-time compiler. People say C is a compiled language, but you can, and there are C
interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,
but you can write a static compiler for that as well. What do I mean in the sense by those
things? A static compiler reads a programming, just looks at the code the program has written,
and then tries to convert it into machine code, let's say. An interpreter looks at the program,
and then it doesn't convert it into machine code. It executes it almost as is. It probably
converts into some other representation, but it's a very simple way of implementing a program
language. A just-in-time compiler nearly always starts a program running an interpreter, looks at
it for a while, says things like, oh, you're calling that function a lot, or there's a function that
takes some parameters in, and there are always integers, always strings, so I will now produce
an optimized version of that function or part of the program based on that information I've observed,
and it's really that dynamic analysis and conversion into machine code at runtime that
makes just-in-time compilers very effective. They can be faster than a static compiler because
they've got more information, so if you just look at a program at compile time, you've just got the
code you've written on screen, you can't fully analyze it as much as you want, so you'll make
certain assumptions and guesses, but they may be incomplete or even incorrect, and you'll optimize
based on that. When you're actually running the thing, you've got more information, so you can
make a much better quality optimization, but you've had to watch the program and observe it,
so it started slow, and then hopefully, over time, hopefully it's warmed up is the term,
and then it gets faster. Warming up turns out to be another kettle of fish. It doesn't always
quite work as we expect. These things have some very surprising emergent behavior,
but generally they do work, and when they do work, they can be very effective.
So this is things like, it knows it, and you're kind of, again, alluded to before,
it knows it's going to be calling this function quite a bit, so it keeps that nearby,
and things like that. Is that? It can do those sorts of things. Maybe we could try a little
simple example. So if I log in, so it's just a simple example. So there's a load of these
that I could use, the Java virtual machines, a just-in-time compiler, the JavaScript VMs,
all of the ones, VA and Chrome, Spider-Wanky and Firefox, and those are all JITs, but I'll look
at one for Python because that's one that I happen to know fairly well, and let's just write a very
silly little Python program. So I can write this function, let me make it a bit of a bigger font
size for you, and it takes two parameters in, and I will just add those two parameters together.
The thing is, am I going to pass it integers, strings? I can do all of the above. Let's just
check that I'm not lying because I do sometimes, sometimes intentionally, but mostly unintentionally.
If I do Hello World, I have to have a space in there. If I save that file,
and then, right, so it prints out five or Hello World, so you can see I'm calling the function in
different ways. So this is why it's hard to statically optimize that function because
integers, strings, it can take in all sorts of things. Now, the normal implementation of Python,
which I'm using here with the Python 3 binary, is an interpreter, doesn't have a just-in-time
compiler, and we can see some obvious consequences of this. Let's put this in some sort of loop.
So I'll just put some very big number here. That looks like a big number to me,
and just repeatedly call that function with some integers. It doesn't really matter. Now,
if I run that, and I'll just call it with the time function, and I'm going to say this is my
newest laptop that has, from memory, six fast cores and eight slower cores, and it's going to run
randomly on those each time I do it. So some of the performance numbers are not going to be quite
as clear-cut as I would like. So I'll try and explain it if I see that happening. So I run that,
and it's taken there a tenth of a second to run. And if I make that number a lot bigger, so I make
it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer
again, we'll see, depending on which style of core it's gone on, I think it's, yeah, it's roughly
gone on the two slower cores. It's roughly, as I make the loop run 10 times longer, the program
is taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,
let's get rid of a couple of those zeros. And what I can do instead is use a different implementation
of Python. And this is one thing that we often confuse. We say Python when we mean the language,
and there's Python the language, and there's Python the implementation, I can use something
called PyPy. And I think it has a jit. Let me turn the jit off and hope that I've not made it run too
slow. So we can see that running. And I've still got a relatively large number. So it's about the
same speed as the normal version of Python. Now we'll turn the jit on.
And it now runs in, well, that's a tenth of a second. It has run two orders of magnitude faster.
And in fact, I think we'll see if I've got this right. Let's add another couple of zeros there,
something very big. It's actually doesn't really matter how big I make that loop. It's been able
to observe it at runtime, realize it can just optimize the whole thing away. And that's the
power of just in time compilation. It's looked at my runtime values and made the thing very fast.
It's particularly effective on this sort of numeric code, but it will often work well on
things like strings and so on. And of course, there are some cases where it won't work. It
isn't, as we said earlier, magic, but it is very effective in many cases.
So as you scale up, is that still a benefit to using it? You know, when you've got a million
lines of code, for instance? Good question. Very dependent on your program. In some sense,
actually the scale of it is less important than the nature of your program. There's a fundamental
assumption here that programs tend to do the same thing over and over again. So in this case,
Pi Pi is looking for loops. And it's what's called a tracing just in time compiler. So it looks at
what the loop is doing and optimizes that there's also method base, which just look at a function.
Don't need to worry about the difference too much. If your loop or your function does the same thing
repeatedly with only minor variations, this will be very effective. If you have a program
which every time it goes around the loop does something completely different or every time
you call the method, then this will be less effective. And some cases then it will even
slow things down because the program will never appear to stabilize. And that's really what your
expecting programs do is that they typically, when they start up, they do some initialization,
that's all a bit random. And then they tend to hit some sort of main part where they do the same
thing over and over and over again. And that's where JIT compilation really comes to the fore.
And our process is doing a bit of that anyway.
Yeah. So I think of modern processes as basically a just in time compiler. I write
machine code and okay, I like the textual form and it looks like armor x86 or whatever.
That's not what the processor eventually executes. It turns that into some other representation.
It does all sorts of clever optimizations. If you ever want to see things like the way a processor
optimizes program reorders it, like the reorder buffer, they're scary at how clever they are.
And that's why they've got a lot faster, even though the gigahertz part hasn't changed too much.
And there's been a little bit of knowledge transfer between the processor JIT world and
the programming language JIT world. Is this a new thing or how long have these kind of just in time
compilers been around them? They have been around in one form or another for a while,
but they really trace their modern lineage back to the 1980s in a language called self,
which has been largely forgotten a really interesting language that had a just in time
compiler via a long sequence of events that ended up going to the company called son and is then
formed the basis and literally some of the code is still there in the Java virtual machine.
So the Java JIT traces itself back to self. V8, the JavaScript VM in Chrome,
also traces its lineage back to the Java virtual machine hotspot and back to self.
So really, those have been the big movers. And now you've got systems like PyPy and V8 and
SpiderMonkey that have modernized the concept or spread it to more languages will probably be a
better way of putting it. And is this, you know, obviously it traces through, it's quite a long
way back. But is it only really being used now because machines have got that much faster?
Yeah, I think there's an element of that. Because for you, when I was a kid, you could
burn your computer every 18 months and it was twice as fast. And the death of single core
performance is a little exaggerated partly because the processors are now doing just in time
compilation sorts of things. But yeah, we definitely are looking increasingly to programming languages
to work faster and for many languages, particularly, but not only those that are
dynamically typed like Python or Java. This is really the only effective technique. And that's
why you've seen increasing numbers of them being released for more and more languages,
despite the fact that they're really complicated and expensive to create. You know, these are not
the sort of things you can knock out in an afternoon, they take some big teams many years to create in
most cases.

Today we're going to talk about one of my favourite topics in computer science,
which is the lambda calculus, and in particular we're going to talk about three things.
We're going to think what actually is it, why is it useful, and where did it actually come from.
So we're going to start with the last question here, where did it actually come from?
This is Alonzo Church, who was a mathematician at Princeton University in the United States,
and he was the person who invented the lambda calculus, and what he was interested in is
what is the notion of a function from a computational perspective? And his answer
to this question is what we now know as the lambda calculus. And there's an interesting piece of
history here, which many people don't know, so it turns out that Alonzo Church was the PhD
supervisor of someone very famous in computer science, Alan Turing. And of course Alan Turing,
amongst many other things which he did, he invented Turing machines, which Computerphile has done a
number of videos on, and Turing machines capture a basic state-based model of computation. And
it's interesting that his PhD supervisor, Alonzo Church, he captured a basic functional notion
of computation with his lambda calculus. And it turns out that these two quite different notions,
one functional and one state-based, turn out to be equivalent. And this is what's called the Church
Turing Hypothesis, or part of the Church Turing Hypothesis. So for Church, a function was a black
box that you're not allowed to look inside, and what it does is it takes some input, so maybe it
takes a number like x, and it's going to process it in some way, and it's going to produce an output.
So maybe it produces the output x plus one. So this would be a function that takes a single input,
a number called x, processes it in some way, and then produces a single output, which is the number
x plus one. Then we could have a slightly more interesting example. Maybe we have a box with
two inputs, x and y, and we process them in some way, and maybe we produce their sum as the output.
So this would be a function which takes two inputs, x and y, processes them in some way,
and then produces their sum x plus y. And there's two important things about functions in this sense.
The first is that they're black boxes. You're not allowed to look inside them. You can't see
the mechanics of what's going on inside this box. All you can do is put something in and
observe what comes out the other side. And the second important thing is that these functions
are pure. They have no internal state. So all that happens when you map x across to x plus one
is the magic goes on inside the box, and there's no internal state. There's no hidden information
that we can use. And this is quite different from the notion of computation that Alan Turing was
interested in with his Turing machines. He had internal state. There's no internal state. These
are pure mathematical functions. Now we can think how do you actually define functions in the lambda
calculus? And there's a very, very simple syntax for this which I'll introduce to you now. So let's
think about the increment function in the lambda calculus. What you do is you write down a lambda
symbol. So this is the Greek lowercase letter lambda. And that says we're introducing a function at this
point. And then you just write down the name of the input. So that was x. And then you have a dot.
And then you say how the output is calculated in terms of the input. So that's x plus one. So
we could do the same with addition. You just need two lambdas. You write lambda x dot lambda y
dot x plus y. So this is the function that takes two inputs, x and y, and then delivers the result
x plus y. And this is written down formally in Church's lambda calculus exactly like this. So
when you've got a function, what can you do with it? Well, all you can do is give it some input,
let it do its thing, and it will give you some output. So let's have a simple example of this.
If we take a function like increment, which was lambda x, x plus one, and we apply it to a number
like five, what actually happens? It's a basic process of substitution. We're essentially substituting
the number five here into the body of this lambda expression. And then x becomes five. So we get five
plus one. And then we get the result six on the other side. And this is basically all there is
to the lambda calculus. It's only got three things. It's got variables like x, y and z. It's got a
way of building functions, this lambda notation, and it's got a way of applying functions. This is
the only three things that you have in this setting. What is actually the point of the
lambda calculus? We've introduced this very simple notation. Why should you be interested
in learning about it? I think there's three answers which I would give to this. The first
point I'd make is that the lambda calculus can encode any computation. If you write a program
in any programming language, which has ever been invented or ever will be invented or really any
sequential programming language, it can in some way be encoded in the lambda calculus. And of
course it may be extremely inefficient when you do that, but that's not the point. This is a basic
idea of computation and we want to think how many, what kind of programs can we encode in this?
And actually you can encode anything. And this is really the kind of church-turing hypothesis
which I mentioned. Alan Turing, you can code anything in his Turing machines and in churches
lambda calculus you can encode anything. And actually these two systems are formally equivalent.
Any Turing machine program can be translated into equivalent lambda calculus program and vice
versa. They're formally equivalent. The second point I would make is that lambda calculus can
also be regarded as the basis for functional programming languages like Haskell. So these
are becoming increasingly popular these days and actually a very sophisticated language like Haskell
is compiled down to a very small core language which is essentially a glorified form of lambda
calculus. So if you're interested in functional programming names like Haskell or the ML family,
these are all fundamentally based on the lambda calculus. It's just kind of a glorified syntax
on top of that. The third point which I would make is that the lambda calculus is actually now
present in most major programming languages. So this wasn't the case 10 or 15 years ago,
but it is the case today. So if you look at languages like Java, like C-sharp, even Visual
Basic, F-sharp and so on, all of these languages now encode lambda calculus or include lambda
calculus as a fundamental component. So every computer scientist today needs to know about
lambda calculus. What I'd like to end up with is a couple of little examples of what you can do
with it. So the lambda calculus has basically got nothing in it. It's got variables, it's got a way
of building functions and it's got a way of applying functions. It doesn't have any built-in data types
like numbers or logical values, recursion and things like that. So if you want to do these
things in the lambda calculus, you need to encode them. So I'll end up showing you a simple
encoding. And the encoding which I'm going to show you is the logical values, true and false.
And the key to this is to think, what do you do with logical values in a programming language?
And the basic observation is that you use them to make a choice between doing two things. You say,
if something is true, do one thing. If something is false, do another thing. And we're going to use
this idea of making a choice between two things to actually encode true and false. So the trick is
for true, you write down this lambda expression. So what it does is it takes two things x and y
and then it chooses the first. And false does the opposite. It's going to take two things
and it's going to choose the second. So we've got two lambda expressions here, both of which take
two inputs x and y, and one chooses the first one x and one chooses the second one y. So fair
enough, what can we actually do with this? Well, let's think how we could define a little
logical operator. So not is the most simple logical operator, which I could think of. It's
going to flip true to false and false to true. It's logical negation. Based upon this encoding,
how could I actually define the not operator or the not function? Well, it's very easy to do.
I will take in a logical value or a Boolean, as it's normally called in computer science,
after George Boo who first studied kind of formal logic. So we take a Boolean, which will be one
of true or false. And here's what we do. We apply it to false and we apply it to true. And I claim
that this is a valid definition for a not function. But I can very easily convince you that it's the
case because I can do a little calculation. So let's check if we apply not to true that we actually
get false. And just a few steps using the lambda calculus magic, we'll find that this actually
works out. So what can we do here? Well, the only thing we can do is start to expand definitions.
So we know what the definition of not is. It was lambda b, b applied to false and true,
and then we just copy down the true. So all I've done in the first step here is I've expanded
my definition of not. Not was defined to be this lambda calculus expression here. Now I've got a
function, which is this thing, and it's applied to an input. So I can just apply it. Okay. And the
function says if I take in a b, I just apply that b to false and true. So the thing I'm applying it to
is true here. So I just do the little substitution, rather than b, I write true, and then I copy down
the false and copy down the true, and I get down to here. And at this point, you might quite rightly
be thinking this looks like complete rubbish. I mean, I've just written true, false, true. What
does that mean? It means absolutely nothing. But it means something in the lambda calculus because
we continue to expand. So what we can do now is expand the definition of true. We said that true
takes two things and chooses the first one. So let's expand it out. So true is lambda x, lambda y,
x. So it chooses the first thing of two things. And then we just copy down the two inputs, false
and true. And you can see what's going to happen now. We've got a function here, which takes two
things and chooses the first thing. Here, the first thing is false. So when we apply the function,
we just get back false. So what you see has happened here. In just a few steps, we've shown how using
this encoding of true and false and not, we can actually get the desired behavior. And it's very
easy to check for yourself. If you apply not to false, you'll get true. And I'd like to set your
little kind of puzzle at this point. Think how you could define logical and or logical or in this
style as well. Okay, and I'm interested to see what kind of definitions people come up with in the
comments. So the very last thing I'd like to show you is this lambda expression here, which is a
very famous lambda calculus expression called the y combinator or the y operator. And actually,
this is the key to doing recursion in the lambda calculus. So as I mentioned, lambda calculus
has basically nothing in it, or it's only got three things in it, variables x, y and z, and so on,
a way of building functions and a way of applying functions. It's got no other control structures,
no other data types, no anything. So if you want to do recursion, which is the basic mechanism for
defining things in terms of themselves, again, computer files had videos on this, you need to
encode it. It turns out that this expression here is the key to encoding recursion in the lambda
calculus. And this expression was invented by someone called Haskell Curry. And this is the
Haskell that gives his name to the Haskell programming language. And he was a PhD student
of David Hilbert, who's a very famous mathematician. The last observation I'd like to leave you with
here is something that's interested me for many years. I think there's a connection between this
piece of kind of abstract computer science or abstract mathematics and biology. If you look at
human DNA, you have this double helix structure. You have two copies of the same thing side by
side. And this is the key to allowing DNA to self replicate. If you look at the structure of this
lambda expression here, you have two copies of the same thing side by side. You have lambda x f
applied to xx and exactly the same here. And this is the key to doing recursion, which is kind of
related to self replication in a programming language or in the lambda calculus. And for me,
I don't think this is a coincidence. I think it's a kind of interesting philosophical observation.
The lambda calculus has this kind of very clever way of doing recursion, which would take a video
on its own to explain how it actually works. But you can look it up on Wikipedia. And there's a link
here, I think, to biology. Somebody actually found the Y Combinator so interesting that they've had
it tattooed permanently on their arm. And you can find the picture of this if you do a quick web search.
What would people search for the Y Combinator? The Y Combinator in mathematics or computer science.
And tattoo, I'm guessing.

I dare you to think of anything moving faster than deep learning, but what if I tell you
that behind all of it, the complexity, the hardware, the data, behind all of that, there
is a simple piece that took us years to find.
Let me tell you the story about the unreasonable effectiveness of the function that made deep
learning possible.
The Thinking Machine.
It's 1958 and the New York Times publishes an article about a device that will be able
to walk, talk, see, and write.
They just interviewed the scientist that's about to change the world.
His name is Frank Rosenblatt, an American psychologist who published this report one
year before, in 1957.
Here, Rosenblatt proposes the construction of what many consider the first predecessor
of the neural networks we use today, the perceptrons.
Scientists built a few working perceptrons, as these artificial brains were called.
April 1957.
This is a receipt from the lab and knowledge in the report, and here is what it says about
the project, designing, fabricating, and evaluating an electronic brain model.
We've been trying to figure out neural networks for a long time.
Fast forward 50 years, meet to late 2000s, and we still couldn't train a neural network
with more than a couple of hidden layers.
But to understand what's missing, we first need to talk about something we call activation
functions.
Regardless of how big neural networks are, by default, they can only solve linear problems.
Unfortunately, most things that matter are more complex than that.
Here is one example.
Imagine you have two classes, orange and blue, and you want to draw a function that separates
them.
A neural network cannot solve this problem unless we use nonlinear activation functions.
Number of layers, number of neurons, none of that matters.
Here is what Wikipedia says about activation functions.
Only nonlinear activation functions allow such networks to compute non-trivial problems.
We need these activations to create some sort of bump disturbance that will allow networks
to solve all sorts of problems, like this one here.
But we've known about activation functions for a long time, but that wasn't enough.
Something was not right with these functions.
Sigma and tanage were, by far, the two most popular activation functions back then.
Look at the blue lines, and don't worry about the red lines for now.
These functions checked every single box we needed to train neural networks.
Well, almost every single box.
To tackle more complex problems like image recognition, text generation, and audio translation,
we needed deeper network.
But as soon as we try with more than a few layers, neural networks wouldn't work at all.
Let me show you this.
This here is the TensorFlow playground.
Any time I want to play with neural networks, I come here because I don't have to write
any code, and it's really easy for me to try any of my wacky theories.
Not in here.
This is configured to solve the same problem I showed you before.
I'm gonna add a few more hidden layers to this network.
I'm gonna change the activation to sigmoid, and then I'm gonna click play.
I did this before, and I let it run for a long time before I stopped it.
Over 5,000 iterations at the network could not solve the problem.
But wait, that's not necessarily an issue, right?
Maybe sigmoid is not good enough to solve this particular problem.
Except I ran the same experiment, but instead of using six hidden layers, I used just two,
and the network stopped it.
This was the thing preventing deep learning from becoming something.
We couldn't train deeper networks because they wouldn't work.
My feeling is, if you want to understand a really complicated device, like a brain,
you should build one.
That was Geoffrey Hindstone's voice.
He played a central role in making deep learning a reality.
But to appreciate what happened, we first need to understand why these activation functions
didn't work with deep networks.
Time to look at the red lines now.
These are the derivatives of the functions.
We used these gradients during back propagation to update the weights of the network.
The deeper the network, the more iterations we need.
I'm not going to get too deep into the math here, but if the gradients are smaller than
one and you multiply a bunch of them, the results will get smaller and smaller.
Look at the gradients of these two functions.
The maximum possible value of sigmoid's gradient is 0.25, that's really small.
And for tanh is one, but that only happens at this particular point.
The gradient is very small everywhere else.
And that right there is the problem.
The deeper the network, the smaller the updates get until they're so small that the network
dies.
We call this the vanishing gradient problem, and that's why these functions did not work.
We should look at biology and we should try and make systems that work roughly like the
brain.
Okay, it's 2010 and a new paper comes out, a paper that proposes an idea so simple that
it looks ridiculous.
They show how a function they call rectifyLinearUnit solves the problem they had with the other
activation functions.
Here is what the paper says, rectifyLinearUnits preserve information about relative intensities
as information travels through multiple layers of feature detectors.
This was the function, that's it.
This was the crucial missing piece.
linear and Hinton wrote the paper, and although they made this function popular, I found references
to it from decades before, like in this paper from 1975.
Fukushima doesn't give this function a name, but that's the rectifyLinearUnit in the context
of neural networks.
But here is the most surprising part.
This function that works so well doesn't even meet one of the most basic requirements
of an activation function.
This function is not differentiable.
So how come the simplest function that doesn't even meet the requirements is the one that
makes everything work?
Let's look at the laptop.
I have a very simple notebook here to plot the rectifyLinearUnit.
For short, we call it relu.
If I run this cell, we get the short.
Here is the plot of the relu function.
The x-axis is the input to the function while the y-axis is the output of it.
Notice how relu returns zero for any negative input and it doesn't touch positive values.
At the point where x equals zero, we cannot compute the derivative of the function.
And that's a big problem in theory.
It turns out that in practice, we can return a specific value for that particular point
and everything works fine.
Here I'm returning zero and if I plot the derivative, it's the red line on the chart.
One final thing for you to notice on this plot.
The gradient is either zero for any negative values or one for positive values.
That means that relu doesn't suffer from the vanishing gradient issue.
The signal propagating through the network will never disappear.
It's a major win and the main reason this simple function works so well.
By the way, the gradient is only one of relu's advantages.
The function is very simple so it's really fast to compute and it doesn't saturate like
sigmoid and tanH2.
It just works exceptionally well and thanks to it, we have deep learning today.
And by the way, when I think about this story and how well relu works despite its simplicity,
I can't help but wonder what else are we missing today.

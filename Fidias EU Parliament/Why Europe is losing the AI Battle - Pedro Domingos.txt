The AI regulation in Europe is incredibly stupid.
Ursula von der Leyen in her state of Europe address this here was like
Europe is saving humanity from extinction with the AI app
and the rest of the world should follow.
I was like, what have you been smoking?
What's that in your life?
You can't be a powerful, prosperous country in the 21st century
if you don't get AI right.
AI is not just another technology.
AI is the automation of intelligence.
The way democracy works today made sense in the 18th century.
You vote once every four years.
It's ridiculous.
Voting is quite smart in some ways, but we have AI now.
The algorithm that connects the decisions can be way better than this
in ways that we don't even imagine.
So I think in the future, we are going to want the important decisions
to be made by algorithms
and God help us if the algorithm isn't making a decision.
There's already a lot of people saying like,
oh, AI's are going to be conscious, therefore they must have rights.
My position on that is that you must be insane
if you think machines should have rights.
Who are you?
My name is Pedro Domingos.
I'm a professor of computer science.
I live in Seattle.
My specialty is AI and in particular, machine learning.
So as you can guess, I'm having an exciting time these days.
Yeah, very, very exciting time.
I want to, I shared to you talking about the AI regulation in Europe
that you think is very stupid that they did what they did.
And I'm curious to hear more about that.
Yes, you're right.
The AI regulation in Europe is incredibly stupid.
Europe has a good track record of producing stupid legislation,
but the AI act is just on another level.
So congratulations to the European Union and its legislators.
So where to even begin?
First of all, the idea of regulating AI as AI makes no sense.
It's like regulating mathematics.
We don't regulate mathematics because it might be used for bad purposes
or regulating a programming language.
We don't regulate Python, right, or even regulating quantum mechanics.
It's like, oh, quantum mechanics can be used to make nuclear bombs.
We've got to put some guard rails on it right now.
It just doesn't make sense.
AI is a fundamental technology.
The right approach is to regulate specific applications.
You know, according to the problems in the application there,
like self-driving cars can be regulated as cars, right?
They're different because they're driven by computers.
So that's some interesting uses, medical applications, et cetera, et cetera.
Trying to have some blanket agency that regulates all of AI
is just a recipe for conflict, you know, bureaucratic duplication.
You know, never ending bureaucracy and for hindering progress.
Now, that's the first problem.
The second problem is then, of course, before you regulate AI,
you have to define it.
They define it so broadly that includes almost everything.
Like, literally, if you look at the ladder of the law,
everything you do with a computer potentially falls.
It's like, computer, you know, system used to make decisions like,
well, you know, everything does that, right?
So who knows what might be regulated as AI one of these days?
Because there's always people, you know, eager to regulate more
and then take advantage for whatever, you know, purposes they have.
Another problem is that AI is evolving very fast.
So this idea that we're going to regulate AI now for, you know,
the future through the foreseeable future is just crazy.
Like, imagine trying to regulate, you know, the Internet in 1996
when social networks didn't even exist, right?
Like, it would be a disaster, right?
But people now want to regulate AI against superintelligence,
which we have no idea what it's going to look like, right?
So, you know, we need to, like, you know, let AI develop.
And in fact, some of us warned the European legislatures
that by the time the law came out,
their definition of AI would be outdated.
They went ahead and a year ago,
chatGPT came out and then they scrambled to update the definition,
leading to a lot of problems.
And of course, by the time the law is put in practice,
it'll be outdated again.
So that's another problem.
Now, here's maybe the most shocking part of all of this.
So the legislation divides AI applications by risk level
and the highest risk level,
or what they consider the highest risk level is simply outlawed.
There's a number of applications of AI
that are going to be outlawed in Europe,
like, for example, predictive policing.
Face recognition, emotion recognition, right?
Imagine you trying to talk with me or anybody
without recognizing the emotions on their face, right?
That was the recipe for this functional conversation.
But AI, because it's too invasive,
is not allowed to try to understand your emotions.
So you're just condemning AI to be stupid
in the name of some misbegotten notion of privacy.
Same with predictive policing.
Predictive policing is a wonderful thing.
It's predicting where crime is most likely to occur
and therefore send the police officers there.
This is of great benefit to the potential victims.
It lets a smaller police force to the job of a bigger one.
Of course, you can have problems,
but that's the reason to do it well,
not reason to outlaw it, okay?
And then, et cetera, right?
And then it goes down these tiers with, you know,
at every level of risk,
there's all these reporting requirements
and, you know, restrictions and whatnot.
Like, for example, they want AI to be, you know,
they want AI safety to be tested and assessed and insured
by law in ways that not even the Googles and Facebooks
know how to do now.
They're mandating that a bunch of things be done
that nobody actually knows how to do, right?
This is just a recipe for disaster, right?
So I think that what's gonna happen is that, you know,
European consumers are gonna suffer.
European companies are gonna suffer.
In particular, European AI startups are going to suffer
because the big companies can have all the lawyers
and the people to get around these requirements
and so on and so forth, but the ones don't.
Now, AI is already far behind,
sorry, Europe is already far behind in the technology race.
Sadly, Europe used to be a leader in AI like America was
and now it's just gonna ensure
that it's gonna become increasingly relevant.
I mean, this is gonna cost Europe trillions of dollars.
It's gonna be bad for the quality of life
and for the health and everything of Europeans.
And like, what does it accomplish
that is actually useful or positive?
I can't name a single thing.
So that's a very brief summary,
although it might seem long of what's wrong with the AI act.
Can we do this in a right way or any regulation?
Anything will be harmful for the long-term potential.
As I said, I think regulating AI as AI just makes no sense.
This doesn't mean that there's no need
for regulations on anything, but as I said,
the right approach is to regulate specific applications
in their own right.
You regulate medical applications of AI
as medical applications.
And the issues are completely different
from regulating self-driving cars.
And in most cases, the fact that something is done with AI
does not even affect how it should be regulated.
Like, for example, if something should be illegal,
it should be illegal regardless of what
that's being done with AI.
And likewise, if something should be legal,
that doesn't mean the reason why it should be illegal
if it's done with AI.
Right now, what you need is not some big global
or quite regulatory agency, as people are looking at,
is for the existing agencies in their domains,
transportation, health, et cetera,
to look at what is the impact of AI in their domain
and how does it change the way it's regulated now?
Because what happens now, and there's certainly
a lot to do there, is that all the laws and regulations
are done with the assumption that decisions
are made by humans.
Once decisions start being made by AI,
a lot of things change.
In some cases, you need new regulations.
In some cases, they need to be different.
In some cases, you can actually remove them
because there's a presumption of biases and things
on the shortcomings and evil intent
and lack of transparency on people.
That is not the case for AI.
So this, I think, is the right way to regulate AI
or AI applications.
Okay, so as I understand, you think that,
let's say some outman with shouting,
we need more regulations and all this stuff,
basically shouting that I don't want competition?
Well, there's a combination of players in this game,
if you will.
But the first one, I mean, so,
there are,
where to begin?
There's a lot of people who are screaming
for AI regulation because they honestly believe
it's very dangerous, okay?
They're not your intention, they're not corporate chills,
they're just a little crazy in my view, right?
AI has this amazing potential to evoke in us
these sci-fi scenarios, right?
And Hollywood has fostered that, right?
When people think of AI, they think of Terminator
and Skynet and like, it really is not like that,
but a lot of people don't understand that, right?
And even, and I can see lay people falling into the,
but there's even some people who are AI experts
who fell into this and I think partly it's because
AI is really a new thing.
And we've never seen intelligence before
except in people and animals,
which in that regard are similar.
And so anytime you see an intelligent thing
like computers being intelligent,
you start projecting onto them
human characteristics that they just don't have
like free will and the emotions and the intentions
and consciousness and blah, blah, blah, right?
None of that is there, but people can't resist.
You know, in the beginning when you try to chat GPT,
you think it's very human, right?
Most of that is just being projected by you.
After a while, we start to understand
that actually it isn't, right?
And it's good that people are doing that learning,
but in the beginning, people are very easily fooled,
even experts can be fooled.
So those people are raising, you know, the alarm.
Then there's sort of like the media loves this, right?
The media loves alarm stories, right?
They're not, you don't get headlines saying like,
hey, yeah, I will not harm anyone.
You can't have lines saying,
yeah, I will extinguish human civilization.
And then of course this is what people read.
So they start to get word
because that's the information they're being fed, right?
Now, as you mentioned, there's also another aspect here,
which is the incumbents, the big tech companies
have a lot of interest in actually keeping AI closed, right?
They, you know, like it is well known
that in every domain regulation is to the benefit
of the incumbents over the newcomers.
And the irony about Europe is that like the incumbents
are American and they don't like, you know,
the power that American companies have.
They are without realizing it,
helping them with the AI act,
helping them against the startups
that might come from Europe.
And so some of these companies are playing
a somewhat cynical game of being in favor of regulation
because they know, they know how to get around it, right?
You press that previous regulations like the GDPR
that have been harmful but largely harmless
because, you know, the companies know how to ignore them
and pretend, you know, like it's, again,
they're so ill-defined that if you're a big company,
you have the laws to get around this.
And it's a barrier to entry,
like incumbents like barriers to entry, right?
And, you know, AI could be very threatening
if you leave it open for everybody to do it.
So of course, a lot of, you know, not just companies
but various organizations, you know, like being protective.
Also, I think some of these companies say
they're in favor of regulation
because they are convinced that regulation is inevitable
and they want to have a seat at the table.
If they come out saying like, no, we're against it,
then they'll be excluded.
They say like, yeah, yeah, we're in favor.
So they get to shape it, right?
It's a very political thing, right?
And then there's another one,
which for example, companies like Facebook
have learned very well, which is,
and I think Google is also learning this, for example,
which is a lot of these decisions around AI
are very contentious.
And very, you know, people from the left and right
will have very opposing views on them.
And no matter what you decide, it will be unpopular.
And the people who like your decision
will take it for granted that ones who don't like it
will hate you for it.
So no matter what you do,
you'll always find that less popular.
So they just want to flow this to the government.
This is why Facebook created this panel
to make independent, right?
Mike Zuckerberg is not known to be, you know,
you know, easy on control, right?
But like he actually, you know,
deliberately gave power away
to make a lot of these sort of content decisions
to an independent commission,
precisely for their reason.
Because now then they take the blame, right?
So there's a number of reasons
and the number of players why this, you know,
AI regulation is promoted.
And fortunately, none of them are good ones.
You are making a lot of fun of Europe.
And since I'm from Europe, I want you,
so you think this can play a big role to the,
like now, United States is a lot more powerful
than Europe anyway.
But you think it will play a big role
to Europe going downhill
if it doesn't adjust to the AI?
Well, yes. So first of all, I'm European as well.
I'm originally from Portugal.
And the reason I care about what's happening in Europe
and I'm very frustrated by it is that, you know, I care.
I'm European, right?
Most of my American friends, it's like,
or Chinese for that matter is like,
well, Europe is irrelevant.
Like who cares?
You know, like they're gone, right?
Never mind.
I'm not ready to concede that.
So, you know, so I care because I'm European.
Also, like there's also around the world,
a lot of people, including in America,
like in states like California,
and even now, you know, at the federal level,
say like, oh, we should follow Europe's example, right?
So the damage that starts in Europe
could go beyond Europe.
But to answer your question, absolutely.
I mean, AI is not just another technology.
AI is the automation of intelligence.
It's like everything that you use intelligence for,
which is just about everything,
can in principle, in the short or longer term,
be automated with AI.
So whoever is in the lead in AI
will be in the lead economically,
not just technologically, economically, militarily,
right?
China, you know, is going all out.
They have a plan.
They say, we want to, you know,
they recognize the importance.
They want to be world leaders in AI.
They're still behind the West,
but they're coming up rapidly.
They're going to overtake the West in maybe a decade or so.
It depends, and some fields they already have,
and others it'll take longer.
But you can't be a powerful, prosperous country
or economic zone in the 21st century
if you don't get AI right.
There is a lot at stake in this.
So you are doing this criticism out of your love?
Well, it's, you know, I care what happens in Europe, right?
And, you know, first of all,
I care what happens anywhere as a human being,
but particularly in Europe because I am European.
And also, I mean, because I'm, I mean, I think,
I mean, I grew up in Europe being interested in AI.
And, you know, Europe used to be a leader in AI, right?
Now that it's AI is becoming important,
they figured out how to basically lose the race
that they were heading,
not ahead of the United States, but comparably so.
So, you know, I feel, you know,
terribly frustrated with this.
Also, the AI act doesn't come out of the blue.
Like the U has passed the series of laws,
like the GDPR and the Digital Markets Act
and the Digital Services Act,
that are all stupid, all stupid in the same ways.
Instead of, I mean, like, nobody, I mean, like,
nobody can name, you know, GDPR is supposed
to protect our privacy.
It does harm in a whole bunch of ways.
You know, it's the Cookie Monster, right?
Like, now you have to click
on all these cookie authorizations,
but, you know, that's the list of things, right?
Anybody who has a small company in Europe in tech
and not even says like,
oh, the GDPR is such a pain to deal with, right?
Like, you know, I called, you know,
like some months ago to like, you know,
to cancel a hotel reservation in Portugal.
And I had to listen to a message explaining,
you're blah, blah, blah, and your privacy rights.
I'm like, you know, multiply this by millions.
Like, this is just ridiculous, right?
So Europe has done a series of these things,
but instead of listening to the backlash,
they think they're very proud of themselves.
You know, the so-called Brussels Effect
that they passed these laws and then, you know,
companies would rather, you know, use them.
Companies don't want to have one product for Europe
and one for the rest of the world.
So when Europe passes these laws,
as happened with the GDPR,
they will often make changes to their product worldwide.
So the European legislators are very proud of this.
It's the one area where Europe is having a lot of influence.
And I kid you not, like, America and China
want to be the world leader in AI.
Ursula von der Leyen, the president of the European Commission,
the president of Europe says that Europe wants
to be the world leader in regulating AI.
Wow, what an amazing accomplishment.
This is really not what you're shooting for.
You're kidding me.
So I want you to explain a bit for a stupid me
and some people that don't know what AI is.
We hear this word all the time.
What is it?
It is it.
AI is the automation of things
that traditionally only humans can do.
Of the higher level cognitive abilities, if you will.
It's the automation of things like vision,
manipulation, navigation, understanding language,
the common sense, reasoning, solving problems,
and very importantly, learning.
The amazing thing about humans, or mammals in general,
is that we can learn.
Machine learning is the subfield of AI
that deals with automating learning
and that now powers the rest of them, right?
So this is what AI is.
There's a lot of the, this sort of like,
the lay person's definition of AI.
There's a lot of confusion, a lot of hype.
It's almost become a vacuous term in some quarters.
There is also a more technical definition,
which I actually think is worth knowing
because it's very enlightening
with respect to some of these things
that we've been discussing.
And this is that AI is the subfield of computer science
that deals with solving intractable problems
using heuristic methods.
Intractable problems means problems
that take an exponential amount of time to solve,
or memory and so on.
Most things in computer science can be done efficiently,
like databases and computer graphics and whatnot.
AI, even just playing the game of chess perfectly
would require a computer bigger than the universe.
So you have to resort to heuristics,
rules of thumb, strategies that sometimes will go wrong.
This is unavoidable.
And AI is the field that deals with this.
So if we want to solve problems like curing cancer,
for example, we have no choice but to resort to AI techniques.
But at the end of the day,
an AI system is just an optimizer.
It's just a problem solver.
It's not a little agent, a little being with
intentions and emotions and whatnot.
That's like, so keeping the real definition in mind,
I think is very important,
as even a first step to knowing what you're doing
when you're discussing what to do about AI
and regulating it or how to use it in your company, et cetera.
And can anyone start AI in 2024?
An AI, LLM, large language?
Anyone can use AI and does, right?
Anyone can use a chat bot.
I would say there's sort of like maybe three major categories.
There's the lay user, which we all are,
which use AI every day without even realizing it.
When you use the Google search engine,
you're using AI, you just don't know it.
When you use Twitter, you're using AI
because there's an AI algorithm
selecting the tweets that you see.
So we are all power users of AI
without knowing anything about it.
Now, there's another level which is where you,
people who can use and do use today
and they'll be more AI professionally,
without understanding in detail how it works.
Think of, for example, a cab driver,
who drives a car for a living,
doesn't necessarily know a lot of mechanics, right?
The mechanics and the engineers know
how to deal with that, right?
So a lot of these people,
there can be, of course, a spectrum of expertise there,
but a good example,
this is a lot of people who do data science.
They do modeling of various problems in various areas.
Could be biology and medicine,
it could be business and economics,
could be a lot of things.
They know AI at some level,
but then I have a PhD in machine learning, right?
And then there are the people
who do have a PhD in machine learning,
who are exceedingly valuable and costly these days,
who really know how to build
their machine learning algorithms and refine them
and develop new algorithms for a problem, for example,
as opposed to just using existing ones off the shelf.
So you could be in any one of these buckets of fuel
and you can, of course, move from one to another.
So I'm not sure if you answered that question.
Is it from one average person to get involved
and immediately start his own AI to do something
with a goal, with AI to do something?
So, let me give you an example.
You can, an average person can go on chat GPT
and just start asking it questions
and asking it to do things.
So anybody can do that, right?
And they can learn to prompt it
and become a prompt engineer, right?
There's this new occupation called prompt engineer.
So people can do that.
Another thing that people can do,
let me give you an example.
There's a little more advanced.
So like there's this site called Kaggle
that runs machine learning competitions.
The company has a problem, like say, I'm a drug company
and I wanna do some drug design
or like I have some prediction problem.
They make, they publish the data set
and they publish a prize for whoever wins it,
for whoever is most accurate at predicting whatever
these tumors or whatever they care about, right?
Shurn in insurance or telecoms or whatever, right?
And anybody can enter that competition.
Anybody, you or me, right?
And they can use official algorithms to do it.
In fact, most of the time that's what people do.
And if you win, you win.
And if you win a bunch of these things
you become a Kaggle grandmaster
and you can get hired by in some cases,
like the Googles of this world,
just on the basis of being very good in Kaggle.
With no degree, no anything in machine learning, right?
Of course you can also in Kaggle develop your own algorithms
and there's a gray zone between I am using off the shelf
or I'm building something from scratch, right?
Usually what happens is that you start with you off the shelf
and you play with the various options
and at some point you realize you wanna extend it, right?
So like, this is a very natural path.
You can go from being a 16 year old late person
to being a world expert.
I mean, I'll give you an example.
There was this thing some years ago called the Netflix Prize
which was the predecessor of a lot of these things.
Netflix has this recommender system
that they used to recommend films and TV shows for you to watch
that is really crucial to their business.
Three quarters of the things that people watch
come from the recommender.
And at one point their machine learning people said like,
well, we don't know what else to do.
What are they predicting as well as we can?
And somebody had the idea of having a public competition.
So like, well, we're just gonna give a million dollars
which at the time was an astronomical amount of money.
This was in the 2000s.
And say, anybody who can improve prediction,
we're gonna run this for whatever, two years
and there'll be partial prices for anyone
who can improve prediction by more than,
I don't know, one or two percent.
And then the winner will get a million dollars, okay?
And they, the Netflix folks themselves
didn't think anyone would succeed
because it was just too hard.
Within three months, the late people were already beating them.
And I have, I teach these evening classes to people
in industry that have their jobs.
One person in my, and I gave this prize,
a simplified version of it as a class project.
Here's some Netflix data, go and predict
what movies people wanna watch.
And there was one guy there
who didn't know any machine learning before.
He knew some computer science
so he was not a complete lay person.
But he did that and then he got interested
and a few months later he was in one of the winning teams
of Netflix, of the Netflix price.
So he went from knowing no machine learning
to being one of the winners in, I don't know, six months.
So it's definitely possible.
And you wrote this book that was very successful
about AI in 2015.
What you got wrong about it?
Because a lot of years passed.
Well, honestly, to give you a very honest
maybe self-serving answer,
I don't think I got anything wrong about it.
I wrote, so obviously, eight years have passed
and a lot of new things have happened.
There's a lot of new developments,
particularly related to deep learning, obviously.
Also, I had the chapter about
what's gonna happen in the future.
And the chapters that are about AI
and the different types of machine learning and whatnot,
all of that is current because I wrote it with that in mind.
And I was like, I wanna write a book with things
that will not be outdated tomorrow, right?
And again, the interesting thing about machine learning
and AI is that there's new things by the day,
but the basic ideas haven't changed since the 50s.
The key ideas in machine learning
that have been there since the field began, right?
And of course, I tried to focus more on these things.
The chapter that, if I had to say where I went wrong
in some degree was like, I predicted some things
that happened to a much greater extent than I imagined.
So I thought, these companies are doing this
and trying to keep an eye on their hood
and hoping to fly under the radar.
And this is gonna blow up
and then people are gonna call for restrictions on AI
and whatnot.
And I'm telling you, this is coming.
Wow, I had no idea just the scale on which that happened.
So if I erred on anything in that book,
it wasn't just underestimating how big
some of the reaction and the backlash
and the changes would be.
And a curiosity about the book.
So the book is selling better now than it did in the start
because of the relevancy of the event or no.
So the book had a very unusual sales trajectory.
So the book by now is a worldwide best sell.
It's been published in maybe, I don't know,
at least a dozen countries.
It sold over a third of a million copies,
which for a non-fiction book is pretty rare.
In the beginning, it wasn't,
I mean, it reached the bottom of some bestseller charts,
but it wasn't a huge bestseller.
And I can, there's a book about algorithms.
All the problems like algorithms,
like why would anybody care about algorithms?
And like, trust me, people will care about algorithms.
And right now, again, now nobody questions that.
But it wasn't an obvious bestseller.
But I think it sold a lot largely on word of mouth
and people who wanted to learn more about machine learning,
and you know, they're not gonna read the textbook, right?
And this is just on a different level.
This is for like, you know,
one of the pictures I had in mind was like,
you know, a 16 year old kid, smart, you know, 16 year kid.
Another one is the CEO.
I'm the CEO of a company.
I need to get on top of this thing, I think.
I'm not gonna read a thousand page machine learning textbook.
It ain't gonna happen.
So like, machine learning for CEOs.
And you need a bunch of CEOs, you know,
for example, Jensen Wang, who's not very famous
because NVIDIA is not very famous,
but back then wasn't.
This was when they were about to make their pivot to AI.
He told everybody at the company
that they had to read the book.
Very smart on his part.
It was like, he used to do computer graphics,
like now we're gonna do AI, people read this book.
Everyone has to read this book, right?
I had many sort of like people from NVIDIA companies
who were like, oh, wow, I read your book
because Jensen made me.
But, you know, but this spread the word a lot.
And then the book never,
books usually sell well for a few months
and then they kind of disappear.
The book has continued to sell well, you know, even now,
right?
It's not selling as well as it did at its peak,
which again was maybe, I'm gonna say like six months
after it came out again,
as opposed to like the following week.
But it's still selling at a steady clip,
which I think is remarkable given, you know,
how fast moving AI is.
What do you think you did right on the book?
Is it, by the way, I want to compliment you.
You are explaining things very well.
You are an amazing communicator,
but I'm curious to see if that's what you think
you did right on the book.
I think, I think I did a few key things right in the book.
I also made some choices that I could have made differently
and they might have been better.
I think the most important thing
was writing that book at that time, right?
I had the idea of writing a popular book on machine learning
since our popular science book, if you will,
on machine learning since I was a PhD student,
because, you know, I've read many popular science books.
I like the genre when it's well done,
it's very useful and fun.
And it was clearly when I was in the 90s,
when I was a PhD student,
that machine learning was becoming very important
and someone should write a popular science book about it.
And I only saw, nobody ever really did that.
There were various books of like data scientists,
but like I was very dissatisfied with all of them.
The thing that finally this, you know,
made me decide to write the book was two things.
This was around 2010, 2012,
there was this big wave of hype about big data, right?
Machine learning under the name of big data
was now, you know, in newspapers and whatnot.
And the sheer amount of incorrect information
was just like, and the mistakes,
like companies like wasting years and millions
or more of dollars on just like stupid projects
that, you know, if they'd read a simple book,
they wouldn't have.
I was like, somebody needs to write it.
The other one was the idea of the master algorithm.
Cause like, when you write the popular science book,
it's not a textbook, right?
It has to have a theme and a thread
that keeps people interested.
Otherwise they'll put it down, right?
When you write a textbook, you have a captive audience,
the students have to read it to pass the exam.
Public science book, you know, it's like a novel.
If you're bored, you stop reading it
and then the rest is wasted, right?
And the master algorithm, and this one, I had that idea,
right?
It's like, the master algorithm is this idea
of a single algorithm that can do everything,
which seems like magic.
I was like, how could one algorithm do, you know,
drive a car and play chess and do medical diagnosis,
you know, all at the same time?
One algorithm can and does, in particular, for example,
backprop is behind all of these things
by training on the appropriate data, right?
And this is really the idea at the core of machine learning.
The reason, the thing that's so exciting
about machine learning is that you can write one algorithm
that will be good for an infinity of different things.
So I thought this is a great threat
to organize the book around.
I think that was a good call.
The third one, and I agree with you,
was that I put a lot of thought and a lot of,
I already knew the stuff, right?
I've been teaching it for, you know,
I've been teaching it for over 10 years
when I wrote the book.
The question is like, how do you explain it
in a way that is interesting and accessible
to someone who has no background in computer science?
So you have to use a lot of sort of like analogies
and examples and put things in a way.
This is the key that is that anyone can understand
but is still conveys the essence.
Popular science bookstance to either be written by scientists
and then it's about their own research
and it's like full of overly technical stuff, right?
So people don't read it or they don't get it.
Like they don't get most of it.
Or they're written by journalists
and they're full of mistakes
or they're like, they don't really get it
the harder thing.
So that I, you know, I put a lot of work into that.
Having said all of that, I did sort of like,
I had this determination which in retrospect
is kind of makes me smile that I'm gonna write a book
that really covers all of machine learning, right?
I was very also frustrated by like, you know,
there are these different schools in machine learning
like, you know, deep networks is one, but there's others.
And people tend to, you know, write books
as if their school is the only one.
And grew up thinking like, you know,
deep networks are all there is or symbolic has all this
and this is just bad, right?
So I wanted to write a book that covered the whole waterfront
and a certain level of people.
So I could have written a book that would be more accessible
by not going so deep.
It might have had a bigger impact
because maybe instead of, you know,
300,000 people, you know, reading it or buying it,
that would be 3 million, right?
So I deliberately made that trade off,
but you know, I could have made it otherwise.
I also tried and this really is an art to write a book
that would work at multiple levels.
If you're more of a leader and a quick reader,
you will read, you should be able to read enough
and then skim some other parts that you got the idea.
And then there's other parts usually later
in each section or in this chapter
where if you're more, you know, interested and, you know,
more, you can, you know, you can go deeper.
So, you know, I had a lot of feedback from people
on the book and I would say that it's interesting,
at the end of the day, the people that got the most out of it
were people who do have some technical background,
maybe engineers in another area or biologists or scientists,
or even computer scientists who didn't know
machine learning before, or even machine learning people
who actually didn't know how to do the things in the book.
They can get what's in the book at a level
that people who are just reading it, you know,
with a fresh mind can, but hopefully everybody benefited.
So, the big question, do you think that they have or they will
gain consciousness on the way, the machines?
Well, first of, this is a big question indeed.
But the first question within that question is like,
what is consciousness?
Nobody knows what consciousness is.
So, since nobody knows, then we can't answer the question,
do machines have that consciousness or not, right?
So, there is what's called the easy problem of consciousness
and the hard problem.
The easy problem is, I can find things that are correlated
with consciousness.
So, for example, there are neuroscientists,
like Christoph Koch, for example, who, what they do
is they look for things in the brain that correlate
with being consciously aware, certain neurons firing
or certain circuits being on and whatnot.
So, you can do that.
And a lot of progress has been made there.
And you can look through those correlates in AI, right?
Then there's the hard problem which is like,
but how do you know that that thing really is conscious?
And the truth is that at the end of the day,
this is not a question that can be answered scientifically
because only objective questions can be answered
scientifically and ultimately consciousness is subjective.
I know that I'm conscious,
that's in some sense the only thing I know, right?
And I'm willing to believe that you're conscious,
but for all I know, you could be a robot.
And for all you know, I'm a deepfaker of Pedro Dominguez,
I'm not the real one, right?
So, in the end, nobody really can answer the hard question.
Now, the way I think this is going to play out
is that 99% of the AI systems in the world
are not going to be conscious at least by any, you know,
common sense definition of consciousness
and they don't need to be, right?
They're just solving some problem.
They're finding a cure for cancer,
they don't need to be conscious.
There will be some AI systems,
like for example, think of a house bot, right?
A robot that, you know, cooks dinner, you know,
does the dishes, makes the beds, blah, blah, blah.
This robot has to be so similar to humans
in terms of their perceptual immoderabilities
that it's hard to think of it being able to function
without at some level in terms of the correlates
being conscious, right?
It's got to have vision, it's got to have hearing,
it's got to manipulate things,
it's got to transform one to the other.
You know, it's got to process a very intense, you know,
stream of incoming information.
So, at some level, you could say that it must be conscious.
Now, but the philosophers could say like,
no, no, no, this is just some computer circuit
that does something that looks like it's conscious,
but it's not.
And I think that's what's going to happen at the end of the day.
In fact, this is already happening,
is that even prematurely,
is that as soon as the computers, the AIs, the robots,
start acting as if they're conscious,
when you look at them, I mean, like,
this was the Lambda guy, right?
The Google engine says like, Lambda's conscious,
because it felt conscious.
So people will start treating the systems
as if they're conscious.
And, you know, for the purposes of the layperson,
in 2050, AIs will be conscious, the ones they know, right?
And whether or not they really are conscious
is a question that we, the experts
and the philosophers can debate,
that people will just start treating as being conscious.
So you are saying having consciousness or not
is irrelevant if we are treating them as conscious?
No, I mean, exactly.
I mean, and ultimately, this again,
depends on your philosophical point of view.
But I think from a practical point of view,
it's like, AIs are tools.
And the question is what they do for us, right?
And if they're doing for us what we want to do for them,
then these questions like, do they have consciousness?
I really not, you know, like, what I care is whether
the machine found the cure for my cancer and saved my life,
or whether my housebot does my chores for me
or breaks down, right?
Well, now it does become relevant, you know,
there is one important pointy which is,
and again, you see that today,
there's already a lot of people saying like,
oh, AIs are going to be conscious.
Therefore, they must have rights.
So there are some people who say,
if they're not conscious, they don't have rights,
but if they're conscious, they have rights.
So at that point, this is not just a philosophical question
because you have to decide what they do,
have rights or not.
My position on that is that you must be insane
if you think machines should have rights.
I already think that animal rights is a bit of an oxymoron.
I'm like, why should the animal have rights?
Right?
It makes no sense, right?
So if animals shouldn't have rights,
then machines for sure shouldn't have rights.
But there are philosophers, you know,
like the Peter Singers of this world that say,
well, there's this, you know, like,
first we gave rights to other humans,
and then to animals,
and next things we're going to give rights to machines.
They're going to have rights because, right?
And this to me is like completely confused and wrong-headed,
but there's people who believe that.
So many questions are hard, what you said,
but so you don't think animals should have rights.
No?
So, I mean, like, let me ask you the question.
Let's assume that you think animals should have rights, right?
Animal, you know, there are now laws in the books
in many countries,
if not most giving rights to animals, right?
What is the rationale for doing that?
And again, this is useful exercise
because then what you can do
is the parallel of that with machines.
Well, are these reasons by which we gave rights to animals,
if we don't buy them, then never mind.
But if we buy them, then by analogy,
should we give those rights to machines as well?
You could also think that like,
if at the end of the day,
you don't want to give rights to machines,
you have to be very careful
about what basis you give rights to animals on
because it's a slippery slope.
So, tell me why animals should have rights.
Well, because we don't know,
like I don't know if you are conscious,
we don't know if they are conscious.
And if I killed them, why I don't kill you then?
If I kill a Tigan.
No, okay, so you're still,
so to rephrase your argument,
you're saying like,
well, animals are similar to humans,
so they should have similar rights, right?
But again, this is a very slippery slope, right?
I can pick up anything that's similar to humans
and say, well, this thing should have rights, right?
Like that, you know,
similarity is a very squishy, very vague concept, right?
The question is, and again,
this is a machine learning question is like,
and where do you draw the boundary, right?
There's all the things in the world,
you know, including rocks and grains of stone,
of sand and, you know, plants, right?
Plants are living beings, right?
Why shouldn't they have rights, right?
There are some crazy people who think they should, right?
And the question is like,
where do you draw the boundary, right?
And now you could, to me,
the obvious place to draw the boundary
because it's how the humans, the jump,
it's not to say that there aren't similarities to animals.
Of course there are,
but the jump for humans to animals is huge, right?
But now there, and you could also say,
but there's a deeper reason, right?
Which is, you could say, right?
As somewhat implicit by what just is like,
I'm the only one who's, I know is conscious,
so I'm the only one who has rights.
I shouldn't have rights.
I, Pedro, you knew you should, right?
So now good luck enforcing that.
If you're the king of the world,
then maybe you can enforce that,
otherwise it doesn't matter, right?
You can believe whatever the heck you want, right?
Now, to me, the real question,
and this really is the main point,
is that there is no philosophical answer to this.
What you need to ask is like,
what is for the greater good, right?
You compare to societies, which one will do better?
One where people have rights, or one where they don't.
And I would say, you know,
glossing over a lot of philosophy,
that a society where people have rights,
functions better, will progress more,
will outdo a society where they don't.
Animal rights, I have seen no such useful consequence.
In fact, I see a lot of negative ones,
but this I actually think in some ways,
it boils down to an evolutionary question, right?
In my mind, everything is subject to evolution,
including ideas like human rights
that didn't exist and could go away again.
And, you know, in the struggle for survival
between societies, right?
Like the Nazis didn't believe in human rights,
we did, but we won, right?
Was that an accident, or did it have something to do
with the fact that we believe in human rights, right?
So I think that is the only sort of like,
way to discuss this question that doesn't get married
and sort of like a philosophical bug.
I'm curious to, so I saw a robot in real life,
and I was talking to it,
and it was one of the craziest experience of my life.
It's easy to see it on a video.
I saw so many videos before,
but actually talking and communicating with a robot and AI,
it was a striking experience.
So that got me really thinking about friendship.
I wanted to impress the AI, I wanted to say smart things.
I was asking smart questions to make it.
Also, I was thinking,
wouldn't it be cooler to get married with a robot
instead of a human?
So I'm curious to hear what you think about all these things
and the future of this thing.
Well, so first of all,
all of these things are already happening
and they will happen more.
There are people who say they fall in love with chatbots.
10 years ago, there was already a chatbot in China
called Zhao Weiss,
that has the persona of a teenage girl,
and 40 million Chinese teenage boys
have said they're in love with her.
This is not one or two crazy people, and it's not today.
It's not chatGPD, this is 10 years ago,
and it's tens of millions of people.
And now, and there's companies that, for example,
produce sex bots, right?
And they're getting better.
And there are serious scientists at places like MIT,
for example, that say, like, yeah,
humans are gonna have relationship with robots,
and it's on balance a good thing
because it will be less lonely and whatnot.
And again, this gets back to my earlier point,
that when you talk to that robot,
I agree, it can be an uncanny experience,
and you are projecting onto it
all of these human characteristics that it does not have.
You are interacting with it as a human,
but people will do that.
To some extent, they will learn to not do that
when they see how they really aren't like real humans,
but to a large extent, they will.
And now you can say, and it's unavoidable,
and you probably shouldn't fight it,
you can ask yourself, is it for the better or for the worse?
And some people say like, oh, what a horrible thing,
people having relationships with robots,
what could be more sad, right?
And again, I'm practical about these things, right?
If you're like some poor in-cell who's given up on women
and is gonna not have a relationship
with anyone their whole life,
then maybe having a relationship with a bot of some form
will actually make them happier,
and a better citizen and whatnot,
so who am I to judge, right?
Go ahead and have a relationship with a robot, right?
In fact, I can well imagine a future,
not tomorrow, because the technology isn't there,
where most people, their partner is a robot,
and they look at, and they imagine this scenario, right?
Most people have as a partner a robot, right?
Designs to their liking that you bought,
like you can buy a car, and then they do everything for you.
They are your romantic partner, they're your slave,
they are your search agent,
they're like, they just do absolutely everything for you, right?
Including sometimes challenging, again,
part of why Chal Weiss was very smart was that,
sometimes you say like, oh, I had such a hard day,
sometimes she commiserates, and sometimes it's like,
oh, man up, right?
So how you do that?
I think there'll be a lot of competition
among the producers of this to figure out
like the most engaging, but like at the end of the day,
most people are gonna live with a robot,
and they're gonna look at the poor schmucks
who still live with another human being
with equal rights and all those kinds of like, oh my God,
like why do you put yourself through that misery?
It's like, you know, maybe the Amish will still,
you know, have human partners.
I can see that happening.
Wow.
By the way, I got to understand the rhythm of thought
that you had during the podcast.
You always ask, is this going to help the humanity
in the future?
And can you tell me more about this rhythm of thought?
Like when you want to solve a problem,
you ask this question, so I found that very interesting.
No, so I agree with you, and in fact,
I tried to do that.
I think we all should do the following, right?
Like when we, for example,
let's say you're deciding what to do with your life, right?
What are you gonna major in?
What profession are you gonna go into?
What project will you work on?
You wanna do something that you will enjoy, right?
So that's good, but that's a very under-constrained thing.
I enjoy a million different things.
So where things get interesting is like, you shouldn't,
I'm always telling this to my grad students,
don't just pick the first research project that you like.
Try to pick the one that has the biggest possible impact.
This is what makes us so great,
as humans can look farther ahead, right?
A dog doesn't look years ahead.
They just can, right?
But we can.
And so what you do is like this,
like so what is for the greater good?
And unfortunately, humans have in us a lot of things
from evolutionary time that kind of tend to sabotage us.
Like we actually have tendencies that were maybe good
a million years ago, but not now.
So you have a conscious reasoning mind to overcome those,
it's the marshmallow test, do things for the short term,
pleasure that actually bad for in the long term.
So you can overcome that.
And we should make all these decisions
from the personal level to the policy level,
to the whole society level, like,
should you go to war, right?
A bunch of people will die, but maybe it's worth it
because in the end, many more people will be happier,
free, et cetera, et cetera.
So you should always be trying to do this.
Now, technology in AI in particular are a very,
you know, sharp proving ground for this
because there's a bazillion things that you can do.
Most of them wrong, right?
And you're right, I'm always trying to do this.
So like, well, there's all these questions
and analogies and hand waving, which is okay.
But at the end of the day, you gotta ask,
okay, we have this choice between us.
Should we regulate AI, for example?
Or, you know, what machine should we make?
You should reason back from what will be
for the greatest good of the greatest people.
And if you do that at the end of the day,
you'll do much better than if you don't do that.
But it takes a certain discipline, right?
This is something that I don't think
it comes naturally to people, you know, including myself.
You have to train yourself to think like this.
Interesting.
So now you're elevating and you're saying maybe
when you have a pot to choose what a decision to make
and you're between some pots,
you are asking what pot will have also something
that I'm good at and all these things,
but greater impact in the world.
And that's most likely the right pot.
Yeah, you have to think about the people do things too much
without really thinking the consequences through.
And without thinking far ahead in us.
So I'll give you a very simple, concrete example.
People in both research world and in tech companies,
they're often working on things that say,
for example, you're doing a PhD, right?
It takes five years.
Don't work on something that in five years time
will be outdated because more law has made it irrelevant.
You gotta be doing research for the technology
that will be there when the research is ready.
Which if the technology wasn't moving,
wouldn't matter in some cases, that's okay.
But in something like AI, right?
I think one of my complaints
about a lot of what's happening today
is that like everyone is sort of like converging
on like doing all these tweaks on LLMs, right?
Which in 10 years time are gonna be completely relevant.
Cause like LLMs will be outdated, it didn't matter, right?
So like think further ahead, right?
And for example, someone who actually,
I saw recently like a snippet of an interview that you gave
that did this very often with Steve Jobs.
Steve Jobs, you know, said like,
you gotta start working on something
before the technology is really there.
Because if you only start working on it
when the technology is there, you'll be late to the game.
But if you're too early, then you know, then you're gonna fall flat.
So you gotta be doing this thing or saying,
well, I'm gonna do this, it's gonna take five years
and I hope that in the meantime with five years of progress,
the technology will be there.
And then you're the pioneer.
Of course, things can go wrong, right?
But you gotta try to make that projection
and continually update it.
And that's how you do things with great impact.
It's not just like, oh, what is a cool thing to do today?
Let me go, you know, do some better prompt engineering, right?
Five years from now, probably no one will care
about prompt engineering.
Or maybe they will, but then that should be a rationale.
It's like, I really think that prompt engineering
is gonna be super important.
And so that's what we should focus on.
So I think doing that thinking ahead is very important.
I want you to do as what exercise with me
and try to imagine the future in some way.
Areas like it's politics with AI.
How does a government looks with AI?
And maybe we'll compare it to an education as well
after we do this exercise.
So I think politics with AI will look very different
from what it does today.
So number one, autocracies like China,
they're already making full use of AI
to control their population better and whatnot.
Democracy is unfortunately so far,
they're mostly paranoid about
how AI is gonna damage democracy.
Which again, this doesn't make any sense to me.
There's gonna be all this disinformation and whatnot.
Like we already have this information.
Why the bottleneck is your attention.
What we really need to do,
and unfortunately not enough people are thinking about,
and this is going to happen sooner or later,
is how do we make democracy better with AI?
The way democracy works today,
made sense in the 18th century.
You vote once every four years.
It's like, the number of bits of information
that you send to the government per year is close to zero.
It's less than you send to Google in a minute
by clicking on things.
This is ridiculous, right?
And voting is like Churchill said.
It's the best system apart from all others,
but with technology, now we can have a better democracy.
What would a better democracy look?
For example, this is one of the sections
in the last chapter of the mass,
Erdogan is precisely about the future
of politics and democracy, right?
What would that look like?
For example, you can have models of people.
So for example, Netflix has a model of you
to recommend movies, which serves you very well, right?
A lot of people are like, well, like Netflix
is this uncanny ability to guess things
that I'm gonna like.
You recommend something that I don't think so,
but like, you know, I got a subscription.
Let me watch an episode and go like,
wow, I love this.
And you know, it's like some BBC TV series
from the seventies that you never heard of, right?
Now politics, right?
Your representatives should know what you want.
It's their freaking job to do what you want.
They're representing you, not themselves.
But part of the problem is that they actually
don't know you that well.
And your suspicious may be also letting them know too much.
But what they should be able to have with your consent
or to the extent that you want, right?
Is a model of who you are, what you believe in.
You know, are you pro-choice or pro-life, right?
Or at a deeper level, what are the considerations
that are important to you?
What are the things that you care about, right?
And not just sort of like on the top 20 questions
like, you know, pro-gun and tight-gun or whatever, right?
But like, you know, what really affects you?
And then they can, as people already doing other domains,
when they're about to make a decision
over there to vote for something
or to propose a legislation,
they can run it by the models of their constituents
and see what the constituents think.
And then what happens is this apparently magical thing
which is you now have a lot more influence
in what the politicians do
without spending a lot more time.
People don't have time for politics, right?
It's like, I got a job, I got a blah, blah, blah.
I don't know, like, you know, like the week before
I figured out who to vote for based on a few ads
or things that I saw.
This is terrible, right?
You gotta make it easy and effortless for people
to inject their preferences into politics
because that's what democracy is.
And machine learning allows us to do that.
So this is just one example.
So I think a more, you know,
and then of course the politicians
could use this to manipulate you,
but you're also gonna have other organizations.
I can have watchdogs, I can have media,
I can have journalists that also have their models
and go to the politicians, ah, look, you know,
you're promising one thing to Pedro
and then I want to figure out how come, right?
So everybody is gonna be an ecosystem.
Politics today, like everything is an ecosystem
with the politicians, you know, like the journalists,
everything, right?
The activists, you're still gonna be an ecosystem,
but every part of their ecosystem
is gonna have AI at their disposal.
They're gonna have models of themselves and of others.
And these models, a lot of the talking is going to happen
between the models without, you know,
even the people intervening
because they can just do it a thousand times faster
at a thousand times greater volume, right?
And then some things they have to push them up to like you.
They're so important and so unclear
that, you know, it really comes, you know,
down to asking you, but those will be an exception
rather than the rule, right?
So again, this, you know, in the mass rather than
in that last chapter, I call it a society of models, right?
The size of a bunch of like, it's like, you know,
if you're a very rich person today,
you can have a thousand people
or maybe a hundred people working for you, right?
In the future, you will have a million people
working for you.
Some of them will be your own permanent activist
that is always in touch with the agents,
AI agents of your senator
or your, you know, member of parliament or whatever, right?
So these are things are going to work in the future.
So generally you agree with the concept of democracy,
but do you think it can be done more efficient
or because maybe we give power to the people,
maybe it's not good
because the average person is stupid.
So a very good question.
So I am a Democrat.
I'm a very, very strong believer in democracy.
But what we have to realize
the following is that the struggle
between democracy and autocracy
or other alternatives is perennial.
It existed in Greek times, it will exist forever.
And this is the key,
is that technology changes the nature of that struggle.
When new technologies come along,
I mean, you couldn't have countries with votes before
because, you know, it just couldn't, right?
Like, and, you know,
when the American constitution was drafted, you know,
you had to go by horse or by whatever, you know,
from, you know, the West Coast to DC
or not the West Coast, whatever, other states, right?
So like, and when radio and TV came along,
that changed the nature of democracy, right?
And when you look at democracy or autocracy,
they each should look,
they each will look at how they can best
make use of the new technology.
So it's not a static field.
And the other cracks are using AI,
the democracies have to use AI.
It is by no means guaranteed
that democracy will always win.
There's being better and there's winning.
I think it is better for a number of reasons, right?
Also because one of the fundamental reasons
why democracy is better than autocracy is that
democracy brings more intelligence to bear
on the decision-making process.
What does that mean?
Let me tell you, because it goes exactly to your point of like,
which for example, in the framers of the US constitution,
this was something that were very even,
worried, even paranoid about, which is like,
you wanna give power to the people, but not too much
because sometimes they turn into a mob
and they're an uneducated rabble.
So we'll let them elect these representatives,
but then the representatives are the people, right?
So like, so this is one of you, right?
So there's different kinds of democracy.
And in a way, what I'm saying is gonna happen
is that we're gonna have other,
yet different kinds of democracy tomorrow.
But in particular, right?
I would say that one of the crucial things is like,
we do not need, we are with AI,
we're not going to need representatives
as much as we did before,
because AI can do some of that work for us.
Your AI is your representative.
And the problem that we have in politics
is actually in economics and a lot of things
is the agency problem.
Your representative interests are never quite the same
as yours.
They wanna get elected, they have their own agenda,
sometimes they're corrupt, et cetera, et cetera.
With the eyes, right?
If you control them enough,
which is a whole set of other issues,
like you can avoid that, right?
And now you can say like,
oh, all of the people are too stupid.
Well, people today are far less,
are far more educated than informed
and knowledgeable and everything,
than they were 200 years ago, right?
And so for example,
I am in favor of having referenda for key issues.
Some people think referendums are bad things
because that people will vote themselves,
you know, inconsistent things and whatnot,
but then it's for the politicians
to decide on making them consistent, right?
I think at the end of the day,
the big decisions should be made by the people.
Also because, I mean, like, again,
positions will be on this, but it's like,
I could say, sorry, you're stupid.
I'm gonna make decisions for you.
But my view is like, who the hell am I to say you're stupid?
I'm gonna make decisions for you.
Like, just go to hell already, right?
That is so condescending.
It's like, and I honestly,
my fellow academics and experts fall into this a lot.
It's like, I know a lot more about economics
than this random guy on the street.
So this is like the platonic idea, right?
It's like, you know, the knowledge of all the experts,
the good guys should rule.
The problem is that, first of all,
then there's self-interest,
like, you know, your interests become destroyed by mine.
So maybe you, even if you're stupid,
are better making decisions than me, intelligent,
who I'm not really making them on your behalf.
But according to my distorted view of you and my own,
you know, prejudices and preferences, right?
Number one, right?
Number two is that like, again,
people often don't appreciate this.
And again, AI is going to exacerbate this or improve it,
is I as one expert may know more about economics
than you as a non-economist, let's say,
but that's not the comparison.
The comparison is between a thousand experts
and a million people.
The total intelligence of a million people
just dwarfs the total intelligence
of even the thousand most smart smartest people in the world.
You know the details of your life
and what's good and bad for you way better than I ever can.
I don't know anything about your life, right?
I may know some economics laws and whatnot,
but like, my intelligence does not weigh
the individual intelligence of the million people
about what's happening in their lives.
And what we see all the time,
because you know, today, this is a novel,
it's like politicians just make bad decisions,
often because they, you know,
look at what happened with Macron, for example,
in France and the Gilles Jaune.
Like, oh, let's raise the price of gas
because save the planet, right?
And then a bunch of people in rural areas are like,
are you kidding me?
I need this car to live.
And I don't have money to pay that gas.
I don't care about your green, blah, blah, right?
And this almost brought him down, right?
So why wasn't that intelligence in the system to begin with?
It should have been.
So I think, first of all, I trust people's intelligence
more than a lot of my, you know, fellow experts,
number one, and number two, again, AI can make this a lot better.
Just like people today are more informed
because like reading newspapers is an amazing thing, right?
Now we take it for granted, but like, you know,
newspapers change, really, really change, like,
and then so does TV, and then so does the internet,
and so will AI.
So I think in the future, we will have,
let me give sort of like a very stark example of this.
People hate the idea of important societal decisions
being made by algorithms.
It's so dehumanizing.
Again, the European Union is full of laws for beating this.
Like, what could be more inhuman than an algorithm
makes some important decision, right?
I think there's a, I think it's quite likely
that there'll be a point in the future
which we will be suspicious of any decision
that is not made by an algorithm
because it's made by corruptible politicians
and self-interested politicians.
Now, the crucial thing is, I mean, think about it this way.
And this, I think, is a very AI way to look at things,
but to me, very enlightening.
Democracy is an algorithm.
The US Constitution is an algorithm
for how to run a country.
The European Union Constitution
is just a big pile of garbage.
It's like a thousand opinions that no one ever read,
but honestly, it's an embarrassment.
But very thing, there's also an algorithm
for how to govern Europe, right?
So, and the algorithm is like this,
it's like a bunch of people vote, right?
And then you add up their votes,
majority representative wins,
and then the representative is in turn,
they also vote.
This is like the dumbest algorithm
that you ever saw, right?
Like what a joke of an algorithm, right?
But it's better than one guy make,
that's an even worse algorithm,
like one dictator making all decisions.
That is the worst, right?
But again, there's a lot of results,
both practical and theoretical,
that you're like voting is quite smart in some ways.
But we have AI now, we have computers.
The algorithm that connects the decisions
can be way better than this,
in ways that we don't even imagine, right?
So I think in the future,
we are going to want the important decisions
to be made by algorithms.
And God help us if the algorithm
isn't making a decision.
But a lot of people are afraid of that, Moin,
because how do you know the algorithm
doesn't have an agenda of something?
Absolutely, so that is a crucial question.
And you can't be naive and think,
oh, again, this is also a trap that people fall into,
people who don't know computer science,
like, oh, algorithms are somehow magically objective
and perfect and abstract and like, no.
Algorithms do not fall for that illusion.
I mean, some of them are,
but at least the ones that we're now talking about,
they're never gonna be like that, right?
Algorithms are creations of humans.
We can put whatever prejudice is whatever.
Like, I can make an algorithm saying,
if this person is black, do not give them a job.
Man, that's an algorithm, right?
And it's biased, right?
There's nothing stopping me from doing that, right?
I don't anybody who has, but they could for all I know, right?
But also more importantly,
and again, you already see this today as like,
well, I have a search engine, that's Google.
I have whatever, Facebook or Twitter.
Are they just choose, they're using AI, right?
This is today, now, they're using AI
to choose what to show me or what to recommend or, you know,
are they just doing it for my own best interests?
I don't know.
I mean, to a large extent they are because, you know,
like it's a non-zero game, right?
It's the invisible hand.
Google does more business
if they actually serve me at the end of the day.
But at the end of the day, our interests are not the same, right?
They, you know, like, why does Facebook have likes
but not dislikes?
Because likes drive engagement
and dislikes drive disengagement, right?
I mean, there's the examples galore like this.
So these are algorithms.
If we have algorithms making decisions for like a society,
like I just described,
everybody's gonna be trying to put their thing
into those algorithms.
Again, we see this in AI today, right?
This whole fairness in AI thing is basically progressives
trying to inject progressive politics,
which I'm not saying I really disagree with, right?
It's like fairness is, I am great.
For example, you have AI algorithms now
and this is like, you know, best paper awards
and companies like Google and Facebook
that have people who do this.
Like they say, you gotta produce, you know, you know,
if whatever, women are 50% of the population,
50% of this has to be like,
the recommendations have to be the same or whatever.
Things that to my mind are crazy,
but like you see the rationale, right?
It's like they are injecting their politics
into the algorithm.
So how do we avoid that, right?
There's a number of ways, right?
But one way that you avoid it is by making the algorithm
maximally transparent.
So for example, it's better to have a simpler algorithm
in this area, not in others.
This is a whole other, you know, kind of worms.
If I have an algorithm making political decisions,
it should be an algorithm that anybody can inspect.
Open source.
Open source.
And it's like, I don't believe
that you made the right decision.
Let me download a copy of you and certify myself
that you weren't manipulated.
Or maybe find something like,
there's gonna be whole organizations whose job
is to be on top of this process.
And they're gonna be, you know, left wing, right wing,
like pulling in different directions.
There's gonna be an ever ending struggle just like today
about what goes into that algorithm.
And, you know, but that's just the way it is.
Wow.
I love you.
I love you.
The next question is about the education.
How do you think we'll unfold with the education?
I think education is going to be dramatically changed by AI.
And I think most of my esteemed colleagues
and, you know, both universities and schools and whatnot,
are not prepared for that.
But for the students,
it's just gonna be a fantastic improvement.
Now, people have been trying to improve education
with AI since the beginning.
And it's actually very hard.
Because education, unlike say,
recommending a movie is extremely subtle.
For example, when you're teaching a student,
you need to understand their misconceptions
so that you can fix them,
which means you need to understand their reasoning process.
And so there's this whole field
called intelligent tutoring systems,
where the idea is to have an AI tutor.
And why not, right?
Like this whole mass produced education that we have,
where one professor gives a lecture to 500 people,
this sucks, right?
I mean, it's better than nothing, right?
But we know, I remember so like when the whole, you know,
online, the whole MOOC, you know, online course thing came out
like a lot of sort of like my, you know,
colleagues that sort of like top university like,
oh, but like, how can this ever be as good
as a lecture by me, right?
And I'm like, buddy, there's like 10 people
in the front row listening to your lecture.
The other ones are a sleeper on their phone.
Why don't you think, right?
You are over-assembling the people that interact with you
and they think, you know, and you think they're everybody,
right?
So really you have, you know, like for example,
I've talked with Andrew Inge about this.
I'm not sure where the course series is doing this
or it's something that they consider doing,
but like, you know, like you take,
you have these snippets, right?
And then you take quizzes and now you could choose
what next segment to show automatically using machine
learning by the error that you made in answering the quiz.
I can say, oh, this is where you're struggling with.
So this is already making it more interactive
than a lecture is.
But now I imagine the way you have on the other side
is a full-blown chat GPT AI like systems.
For example, Khan Academy, right?
You know what Khan Academy is?
It's Khan Academy is this very, you know,
beautiful nonprofit that puts out,
they were the inspiration.
They were the first MOOC company in some sense.
They just make these online courses about everything.
Started with math, a cell can was like a quant
and then, you know, had to explain some stuff to his niece
and he did it on, you know, like, you know, with a video.
I was like, why don't I post this?
Went viral.
So they already do that.
And they have an agent, you know,
building on chat GPT that does this, right?
And if you listen to Sol, it's like amazing,
probably more amazing than it really is,
but you can see where as the AI improves, right?
You're gonna get to a point where it's not that the AI tutor,
I think, for the foreseeable future
will completely replace the teacher.
It's that it's gonna be more like a triangle, right?
There's the student, there's the teacher, there's the AI.
And the student, you know, knows what to draw on the AI for,
which is gonna be most of the time
because the AI has the time, right?
That's the beauty, right?
AI is cheap intelligence.
Like you can use it all day long
and like a real teacher, right?
Some things then the student or the AI will say,
like, no, this is for the teacher.
The teacher will have a whole dashboard of like,
and again, you see examples of this already today of like,
okay, how is the student doing, right?
Like, what's their report card?
What is the AI saying the student is having trouble with
or there's one way that they help?
And then the teacher comes in and so like,
so the teacher and the AI work together for the student.
In fact, all three of them work together.
And the boundary of what is done by the teacher
and by the AI will change over time as the AI gets better
and as we discovered better ways of doing this,
you know, there will be a lot of experimentation.
It's hard for the experimentation to help
and in today's school systems and today's universities
because they are so conservative, right?
The irony of universities, you know,
and I spent my life in universities
or almost all my life is that university,
as opposed to discovering new things and research
in the future, there are the most conservative change
of first organizations in the universe, right?
The universe is like worse than a church.
It's worse than the military.
It's like, you know, in the military at some point,
some general says like, damn it, we're gonna do this.
And people do it.
The universe is like, no, like,
you want me to improve my teaching?
Sorry, I'm busy with my research.
My mom started that as a high school teacher,
got into teacher training, then got a PhD
in sociology, she said like the number one reality
in any educational reform is that it fails
because the teachers don't care.
I have this beautiful idea about how to teach things, right?
Very well proven, like this is gonna make
teaching so much better.
The students will learn more.
And then the teachers like, oh, no, sorry, I'm too busy.
I'm just gonna keep doing the things that they pretend
to comply if they need to.
But, you know, they're human like the rest of us.
AI's aren't gonna have that problem, right?
If I have an innovation, right?
I push it out to the AI and the AI is doing it, right?
It doesn't have rights.
It doesn't say I have the right to say no, right?
I want, you know, I'm gonna call my union
to see if you're allowed to do this.
No, it's an AI, right?
And so what we're gonna see, I think in the educations
are the things like, and again, it's interesting
because you see a lot of these like,
there's these pods of like groups of children
like that happened during the pandemic, right?
Parents got together to recruit some person
to teach their children.
I think you're gonna have a lot of this.
And it will involve more and more AI.
And then the things that work will spread.
Just like they do on social networks today, right?
There's not gonna be a top-down process.
It's like, wow, you just found a great new way
to teach people about, you know, the Gaussian distribution.
So now I copy it.
I give it to my AI's.
Maybe then it gets adapted.
And then there may some guy in India
adapt in a way that works better for Indian students
or whatever, right?
So you're just gonna have an education system
that is way more customized to the student
than today.
But also where the, let's say there's some innovation
that like you just came up with right now, right?
You, you know, today's AI saw a can, right?
And it really works, right?
You know, and before you know it,
everybody's doing it all over the world
in a way that just can't happen today
because it'll be the AI's doing it for the most part.
So that's how you get the future.
So you mentioned several times the AI,
but I don't think you referenced the AGI
in our conversation.
Can you touch why?
Yes, you're right.
I haven't, and it is for a reason.
Is that AGI is a very, so what is AGI, right?
AGI is artificial general intelligence.
And now why does that term even exist, right?
AI is AGI, so why put in the G, right?
The founding fathers of AI,
their goal was to replace him in intelligent.
In fact, they're very optimistic, right?
Back in the 50s, they were saying
within 10 years, computers will be better than humans
at everything.
I kid you not, right?
So we're 50 years behind, but okay.
Maybe that's not such a big deal
in the long, you know, in the grand scheme of things, right?
But then what happened was that AI turned out
to be a vastly bigger, harder problem than people thought.
And it got specialized into a huge number
of different things, like, you know,
I got my PhD in classification,
which is a subfield of machine learning,
supervised learning, where you have examples
with labels about what is the right answer.
And in natural language, there's syntax
and there's parsing, blah, blah, blah, right?
So people got very specialized,
and there's in any field,
the specialists weren't talking to each other anymore.
And a bunch of people, not AI researchers,
or at least mainstream AI researchers who,
they got frustrated with this.
This was like maybe 20, 30 years ago.
I was like, no, we need to get back on building,
you know, human-level AI's.
They ask they can do everything, right?
The AI is, for the most part, until recently, at least,
any one AI would be very good at one thing.
It's like, I just built you an AI to diagnose lung cancer.
It's very good at finding the tumors,
but it can't do anything else.
So we had a growing number of subfields,
not really talking to each other that much,
and a growing number of very narrow applications
that I was good at.
And the idea of AI is like, no,
we need to get back to having general AI.
General being meaning that does everything a human does, right?
And now, until recently, this,
sorry, my personal goal from when I was a student
has always been to build AI in the human,
not human-level AI, super human AI.
That's like, one of my motivations for doing AI
was that human intelligence is clearly so limited.
Like, I mean, we've got to do better than this.
And maybe back when there was 200 of us in the tribe,
that was okay, but like, in today's society,
is our intelligence is just, you know,
willfully inadequate, so like, you know, AI, right?
That was always my goal.
But you've got to be realistic about that goal.
And the thing about a lot of these AGI folks,
to, you know, maybe paint them with two brothers of brushes
that they're a little flaky, right?
And like these crazy guys who are like, ah, you know, like,
I, you know, AGI.
And then the real answers go like, yeah, buddy,
like come back when you have something to show us,
which for the most part, they've never had, right?
Like, now, the exciting thing about the last decade
is that these labs like DeepMind and OpenAI came along, right?
Who are populated by these people who are the AGI crazies, right?
And not surprisingly, they've been making the progress
that the others haven't because they've been trying, right?
You can't succeed unless you try, right?
Now, what has changed, right?
What has changed is that 20 years, I mean, like 15 years ago,
I gave a talk at the symposium on the future of AI,
where I said, nobody's really doing AI research yet
because we don't have the computing power to do it.
We work on like these very things like, you know,
image classification, that's not vision.
Vision is working on real live video,
but nobody has the power to do that.
But 10 years from now, like if you look at how
the computing power is changing,
we are going to be give or take at the point
where we have the power to do real AI.
And that's when the real action will begin.
Well, guess what?
We're there now.
Real AI has started now.
Like, those 50 years were a preamble.
The real AI is starting now.
And therefore, those guys who wish to be the AGI crazies,
not actually, you know, they have something to show, right?
And like, and talking to people about, you know,
AGI is no longer seem as silly within the field, right?
Now, having said that, there's still a vast gap
between the claims that people make
and where AI really is.
And we need to combat that.
These same AGI folks, because they being true to themselves,
they also think that AI is just around the corner.
I mean, like, Ilya should discover that, you know,
the one of the founders and chief scientist of AI,
he told me some years ago, or this maybe five years ago,
they're like, one of the things they asked their hires,
their open AI is like, even when they're interviewing them,
it's like, how far do we think we are from AGI,
from human level, general intelligence, right?
And he says the average was, I forget exactly like three years.
And I'm like, you got to be kidding, right?
The average open AI researcher,
things were three years away from AGI.
This explains a lot of things, right?
A lot of this, you know, ridiculous things that you see
come from this frame of mind, right?
Now, maybe they're right, and I'm wrong, right?
Or most of us in the field are wrong,
but like, chances are they're just delusional.
AGI is not around the corner.
So the lot of discussion that we have today,
including about regulating AI and the dangers and all that,
he's like, to me, it's happening,
it's kind of like virtual reality,
people think like, again, Ursula von der Leyen
in her state of Europe address this here was like,
Europe is saving, Europe is saving humanity
from extinction with the AI act.
And the rest of the world should follow.
It's like, what have you been smoking, right?
How big of a flow of yourself are you making, right?
So I think we also need to be careful about, you know,
when you say AGI, the term brings a lot of baggage
that I think we need to be a little clear about.
Like, AI is already better than humans in some ways.
In most ways, not yet.
Some things will happen at different times, right?
There's not gonna be a point that would just say,
oh, AGI is here.
I mean, you usually have people saying like,
oh, we chat GPT, you know, this year, 2024, you know,
like on May 21st, you know, 2024 at 5.54 PM,
AGI will be reached.
It's not gonna work like that.
So you think the world, that's why in the beginning
you said that European laws are stupid about AI
because there is going to be a bunch of AI,
small AI is doing all the work
and we're nowhere near the AGI.
Well, that's one reason.
I wish there was only one reason the AI act is stupid,
but it's got a real, you know,
a treasure trove of reasons why it's stupid.
And in fact, most of the AI acts,
because, you know, this is a process
that took several years,
is not motivated by these long-term risks of AI.
It's motivated by what are considered
the current risks of AI, right?
And in fact, this is because like the current risks
of AI are things like the discrimination,
disinformation, the jobs apocalypse, right?
These risks don't require AGI to be real.
I think they have also in their own right
been massively exaggerated, right?
That's a whole different conversation.
It's interesting for me to see how like the,
you could think of them as sort of like the short-term AI risk
or the AI risks now and the AI risks tomorrow folks.
They're two different sets of people.
The AI tomorrow folks tend to be like these, you know,
effective altruism, you know, think tank,
you know, AI idealist types.
The AI now tend to be sort of like these progressive activists.
AI is a cesspool of bias, you know, type.
And they're very different kinds of people,
almost opposed.
And they are very much at odds with each other.
So like the people who care about AI now risks
are really pissed about how the extinction risk
is sucking up all the oxygen.
They're like, stop talking about that crap.
You know, women and minorities are being
discriminated by AI today.
We are being submerged in AI.
So why are you talking about terminator?
And the other ones go like, you don't understand,
those problems are small.
I'm talking about the extinction of humanity.
And I just sit back and eat my popcorn.
So yeah, there's those two kinds.
So, so I will have one question on this podcast
that we ask every guest and I'm ready to ask it to you.
I give you one trillion dollars.
How do you spend it for maximum impact positive impact
in the world?
I would, at a high level, what I would do
with that trillion dollars is give it to everybody.
But I wouldn't just give it to everybody saying like,
you know, here's, you know, the,
whatever a trillion dollars divided by 10 billion people.
I would, so like, if you think about the things
that we've just been talking about, right,
what I would do is try to set up a structure
that has this money and where you think of it
as a company or a nonprofit that owns those trillion dollars
and everybody has a share in it.
But the decision process of this organization
is of the type that we just described, right?
You have to decide, for example, what choices to make,
what to invest in, how to spend that money.
And the people, and now you could spend it well or not,
right, so if you don't spend your money well, it's gone.
But if you and I get together and do something
with that money that then becomes, you know,
10 times the money, then you keep it.
So, you know, I guess maybe a very short, slightly oversimplified
answer to this.
You know how like, you know, there's socialism
where everybody owns everything
and there's capitalism where the capitalist on everything.
But what actually you have, for example, in America today,
something much more interesting.
It's a kind of like socialist capitalism
or capitalist socialism where like 60% of the stock,
well, like I forget what the percentage is,
but like the great majority, so 60% of Americans own stocks.
And the great majority of stocks are not owned
by the rich people that we're always hearing about.
Their money is like nothing, right?
They're owned by the pension funds,
by like individually, you know, retirement accounts,
it's like we, the people, actually own the means
of production in a way that Karl Marx never imagined.
And this is great because when you all have a stake,
it's not like, oh, those companies are evil, capitalists,
no, they're doing it on my behalf, right?
I can complain about whatever Apple does,
but I'm an investor in Apple via my index fund, right?
This is very, so you want, again,
to bring the maximum intelligence to bear,
you give everybody a stake.
So what I would do is give that trillion dollars
to everybody, so they all have a stake, right?
So like, this trillion dollars is for the benefit
of humanity, right?
It's not gonna be for me.
So like, one way to do this would be like,
I am the next Bill Gates or the next Elon Musk.
You know, I'm going to spend, you know,
I'm going to have, you know,
the Pedro Dominguez Foundation
with a trillion dollars to spend.
That's not bad, right?
And in fact, Bill Gates was very smart
about history of entropy is actually a great example
of Big Bang, very result-driven, very, again,
part of what I was describing is like,
I'm not just gonna give some money away,
I'm gonna try to give it where it has the most impact.
Ah, global health is a good one,
but I'm just gonna give money.
I'm gonna evaluate the results, right?
He gave money for a whole institute at UW,
my university called the Institute
for Health Metrics and Evaluation
to see how much interventions are having,
and then he put money on the ones that,
so that's all good.
But even better than that,
is to have the intelligence of all 10 billion people
deciding how to spend that trillion dollars, right?
I'm not going to presume that I know what that is.
I'm just going to try to set up the algorithm.
In fact, in computer science,
we call this mechanism design, right?
The Google auction is a mechanism
with a certain mechanism design.
A constitution is a mechanism design, right?
And the question of like,
how can we best design the mechanism for this organization
so that that trillion dollars
will bring the most benefit to everybody?
And I think this for sure, at a minimum,
will have to involve everybody.
It's like you have a share in this.
If you wanna not care and let others make the decisions
for you and vote by proxy,
that's fine, but you're gonna be losing out.
The ones who care, the ones who show up
will be the ones making the decisions.
Wow, very unique answer.
It's the first time someone says this.
I'm curious to hear your thoughts on,
at some point probably,
as humans, we will be useless.
And we are going to have plenty of time to read poetry.
How do you think we solve this problem?
Is it UPS?
Is it like, how do you think about this?
No, so let us look at the following scenario,
I think is what you're suggesting.
Let us suppose that we reach a point
where AI's and robots can do every job better than humans.
I think there's a very good chance
that that point will come.
Not soon, but it will come, right?
What will that society look like, right?
And again, this is not a passive question,
is what do we want it to look like
so that we start making it happen?
Should there be a UBI?
What jobs will people do, right?
So here are some things that I think are worth considering.
I actually think having a UBI is a good idea.
A lot of people are very opposed to that
because like UBI makes people lazy and blah, blah, blah.
The experiments so far tend to not show that.
You give people an income, they put it to good use, right?
David's experiments are like,
you give people a thousand bucks and they invest it, right?
It's amazing, right?
So also one very good book I read about that humankind.
It's an amazing book about this.
Anyway, continue, sorry for interrupting.
Who's it by, humankind?
R. Edgar is a Dutch philosopher, I think.
I'm not, you're never lucky enough.
I'll look it up.
Yes, I will put the link.
Of all your books in the description,
you are coming with a new one as well, a book soon?
I have, I've written another book
that my agent is currently shopping to publish.
So yes, it should come out one of these days.
Okay, how soon?
We don't know the date.
The publishing industry moves glacially slowly
compared to computer industry.
So maybe in a year, I think a year is between a book,
let me put it this way,
between the time when I finished writing a book
and when it's in the bookstores,
if it's been a year, that's good.
It could be several years.
And six months would be amazing,
but then that's just not it.
And it's about AI.
If I have to, yeah.
It's a novel, it's a satire of AI and the tech world.
Wow, okay.
It's about, it's called 2040,
and it's the story of the 2040 presidential election
where one of the candidates is an AI.
It's basically, you know, chat, you know,
chat GPT runs for president.
I could describe it that way when I started writing it,
but as you can imagine, the comic potential in this
is just infinite.
So hopefully my goal was that, I mean,
I had fun writing it, the goal is for it to be fun,
but also everything in the novel,
every element is actually there to try to, you know,
inform people or, for example,
I want them to see AI for what it really is
and the issues.
So the book also has a very serious intent.
So that's, you know, that's, we'll see how it goes,
but yeah, that's my latest book.
Sorry, you had asked me another question that I forgot.
Yeah, about the UBI and you were elaborating
about the future.
I think we should have, let me put it this way.
The question that we should ask about UBI,
so there are many ways to do UBI.
Here's one way that I think would be,
that I think is the best one is-
It was not only about UBI, it was generally about
when we are not going to have nothing today,
when we're going to have nothing today.
So, okay, so let me do this at a high level first.
So UBI is, so one question is,
if the robots in the ads are doing everything,
how do people get by, right?
How does that income get distributed, right?
And I think it's going to be a few different ways.
UBI is probably going to be one of them.
The other one is that whoever controls the means
of production is going to make more money, right?
The people who created the companies that created this,
say, I will make a lot of money and they deserve to.
So I think we're probably going to have a society
where there is some UBI and nobody stars, et cetera,
but there will also be people who don't just have
a hundred billion dollars, they have trillions of dollars.
UBI might be a trillionaire and highly deserved
because you did something that really is worth
a trillion dollars, not just a hundred million, right?
And I have nothing against that.
In fact, I wanted to encourage to do that, right?
So on the one hand, I'll be, remember, like AI is going
to increase world GPT, not just by a few, sorry, GPT, GDP.
Interesting slip of the tongue,
buy not just a few percent, buy a lot, right?
So we will have more wealth to distribute.
Some of it will go to everybody,
but some of it will also see richer people than ever before
and that's okay, right?
And the other thing is that humans have both sort of like
this equality tendency where I want my fair share,
but also they have like, and this is in us,
it can't be changed, right?
We all know about the totem pole.
We like to have totem poles and to be higher
on the totem pole that our neighbors.
I want to be higher on the totem pole than you are.
Even if we're friends, you know, we're buddies,
but like I'm better than you are, I don't tell you that.
And you think the other way around, right?
Just like Balzac said, the ideal friendship is where
each one thinks he's superior to the others slightly, right?
So people are gonna, so here's nothing that could happen.
And again, you also see some of this today, like,
look at games, right?
I can't imagine a very large chunk of wealth going
to the, you know, people by how much they win
in some game that they play, right?
Completely made up thing, you know,
it's like some whatever, you know, next generation,
you know, pick your favorite, you know,
multiplayer computer game.
And as you ascend, you make more and more money.
So there'll be a very large by, you know, popular consent.
There'll be, you know, there will be people are building
as just because they're very good at playing this game.
And people will come up with ways of doing this,
which to us would seem completely meaningless.
But again, a lot of the ways in which people
get very rich today would be completely meaningless
and stupid to the people of 200 years ago,
let alone 2000, right?
You seriously, you get what?
For sending a ball through a hoop,
you must be kidding me, right?
So there will be that as well.
So there will be all these different ways in which,
you know, the income is distributed, right?
And I don't, so another thing that people,
I also think there will be just as happens
with manufacturing today, right?
Because things are mass produced,
things that are handcrafted have a special cache, right?
So I think even like a lot of this thing was handmade
and often it's way worse than the piece of plastic
that costs, you know, $2, but plastic is plastic, right?
Plastic is very underappreciated.
So I think we're gonna have, for example, restaurants
where it's like, we are a really expensive upscale restaurant,
so upscale that we have human waiters and bartenders.
Whoa, those are the best.
So I think there will always be jobs for people
even when the machines do the job better.
But for various reasons, like prestige and taste
and like how our human minds work.
So there will always be some jobs for people.
There will always be a job for people
which is mining the AIs, being on top of them,
making sure that they do the right thing.
In fact, I think even in this beautiful future,
that is going to be everybody's job is to keep making sure
that the AIs are doing what they're supposed to do, right?
So everyone will have a job just doing that, right?
Now, another question that kind of,
listen to what you're saying is we're like,
oh, but people's lives will be so meaningless
because it's work that gives you meaning and whatnot, right?
And that resonates with me
because my work gives me a lot of meaning.
But if you look at the people who are independently wealthy
or retirees, they're not miserable for the most part.
They don't feel that their life is meaningless.
They've found other ways to give meaning to their life.
It's not written in the stars that it's work
that will give us meaning.
Like for example, I like to discover things
and maybe they will come a day when the AIs are way better
at discovering things than me or anybody,
like in whatever, in any area.
But then I think what I will do is like,
I will discover things for myself, right?
I will be discovering what the real laws of physics are
with the help of AIs who will explain it to me
or whatever, right?
For my own enlightenment, right?
These days I get to earn a living doing computer science
research, but if I really like to do it,
like I mean, look at chess, right?
People still play chess.
They don't say like, oh, you know,
the best chess players are computers,
so it's just not worth it, right?
It doesn't work that way.
So I think people will find all sorts of meaning
in all sorts of new things and old ones, right?
There will be a temptation for people to sit back
and watch TV or play video games all day long
and some people will fall into it
and we need to, you know, combat that as parent,
as educators and whatnot.
But the notion that, you know,
in a world where everything is done by AIs,
humans will be, you know, miserable
and have meaning in the slides,
I just, I don't see how that's gonna happen.
We're better than that.
Why do you do these podcasts?
I, well, great question.
So which we can answer sort of like very concretely,
you know, building on what we talked about before.
I get a lot of requests for interviews,
you know, radio, TV, newspapers, podcasts,
you know, to write pieces like, you know, the full range,
right?
You know, basically started when the book came out,
there was a lot of interest.
And of course I get way more than I,
way, way, way more than I, you know, than I can do.
And so I try to do the ones that have the most impact,
which can be often very hard to guess, right?
I mean, and impact, so one example is like, you know,
if, I don't know, the BBC wants to do an interview with me,
I'll probably say yes, right?
But if a podcast that has a large audience
wants to do an interview, maybe I'll do it as well, right?
Cause, you know, like, I don't care
whether it's sold media or new media,
I care about who's it gonna reach.
And I also, and again, I often will do things
for very small audiences because they are very
functional audiences or, or for example, audiences
in a field, subfield of AI that I think could, you know,
I could help going the right direction.
Like for example, you know, a few months ago,
I was in Portugal, where, you know, I took place
in an event that had like, I think 50 participants
and they had a session on AI, but those participants
were like ministers, Supreme Court justices,
CEOs of corporations, blah, blah, blah.
I was like, you know, it's only 50 people,
but they matter, right?
So I actually flew to Portugal on purpose to do this.
So I use basically the same rule as for everything else,
which is, and then also this also has to compete
with like doing research or writing books, right?
Two hours that I spent doing a podcast,
accomplish something right there,
but when you add this all up, if that's all I do,
then I don't do research.
And maybe at the end of the day, my research is
where the biggest impact is,
because you need to move AI forward.
So I try to manage that in a way that of course
is very heuristic and very fallible,
but that's what I try to do.
The way that we end this podcast is by beautiful exercise,
which you are going to die after this podcast.
And let's say hypothetically after 40, 50 years you die,
actually, we're going to look back to this
and it will be your last word and a message to the world.
So you have 30, 40 seconds, one minute
to say your last words.
Learn as much as you can.
Learning is one of the greatest joys in life
and it also makes you a better person.
So not only is it good in the moment,
it's how we make everything better.
You know, Einstein said if you had one hour left to live,
and this is always on my mind,
he would spend 55 minutes figuring out what problem to solve
and the last five solving it.
And this is exactly right.
You will have the biggest impact if you learn the most.
So the first and most important thing is to do is to learn,
not just when you're in school, but your whole life.
One of the things that I try to do is learn every day.
Learn, when you make a mistake,
there's a lesson to be learned there.
If you learn that lesson,
it was for the better that you made that mistake,
because now you will not only not make that in the future,
but you've probably learned the more general lesson
that we'll avoid making other mistakes as well.
So when you have a choice between doing A and B,
and they're both fun,
and maybe B is even a little more fun,
but with A you learn more, do A.
Thank you for your time.
You are amazing.
Thanks for having me.

Cool. Hello, everyone. Welcome to your course AI for Art, Esthetic and Creativity. Today,
we have a very special speaker. She has an excellent background in different domains,
and she will tell you hopefully more about herself and her work. Sarah is a great friend
and great colleague of me, and she kindly accepted to give us a lecture talk today. So from here,
I let Sarah to continue. Please go ahead. Thanks, Ali. It's such a pleasure to be here. I've heard
so much about this class. I don't think I have a slide about my background, but I can tell you a
little bit about myself. I finished my PhD in neuroscience across the street from Seasale last
year, and now I'm a postdoc in the vision group. The journey throughout my PhD was a little bit
of a winding path. I started thinking about explicit symbolic models for things like physics,
and we'll talk a little bit more about that along the way. Modeling how the mind makes inferences
about things that we see, but that hits a ceiling when we come up against questions of vision and
types of seeing like looking at art that is really difficult to develop some kind of computational
formalism for that we don't have good models for. And at the same time, as I was kind of hitting that
wall in my own thinking, I was developing a parallel interest in visual art and doing a lot of
different projects, both with individual artists and with larger museum archives that'll talk a
little bit about, and started to look at art as a ground for asking kind of difficult questions on
the frontier of our thinking about the mind. If we look at how humans create art and view art,
can we understand something about how they view the world in domains that we don't yet have good
models of cognition for? So I kind of started steering my PhD in that direction. I'll share a
little bit of that work as well. And as I said, now I'm a postdoc with Antonio and Ellie asked me
to share a little bit of my inspiration behind that path. I don't have a good story about a
specific moment. I think it's been a lifelong interest for me since I was super small and reading
a lot of poetry, I guess, thinking about kind of the origin and nature of structure and our
experience of the world. I know that's quite an abstract thing, but the structure that we see in
visual patterns, where does that come from? Is that something that lives inherently in the brain
and we imprint it onto kind of noisy and unordered stimuli? Or is it something that's external,
you know, a nature-nurture question? And then our brains kind of evolved to reflect. And I got
interested in this meeting point, this kind of layer between the self and the world where
all the action happens, so to speak, and had training in applied math before I came to MIT.
And we think about ways to describe kind of structured inputs to processing systems and
understand something about the structure of external inputs. And then my neuroscience background
learned a little bit how to think about and model the structure of a processing system,
the structure of different parts of the brain. And it's really been through my interest in
visual art that we can start to think about and describe what happens when those two things meet
and how we synthesize our world of visual experience in domains related to art and then
other kind of higher-level aspects of cognition, like scenes or associations with moods of scenes
and that kind of thing. So that's where I am now. And I think I'd like to start us off,
unless anybody has any leading questions about where I come from, with kind of a provocation.
And you can think of this as a frame for what I'll share today, but it's intended to be provocative.
And so the statement I'll make is that visual perception itself, human perception,
which we attempt to mirror and model in computer vision and computer science in some cases,
that human perception is something that's fundamentally constructive. And I say that
because it solves an ill-posed inverse problem, like ones you've probably heard of before.
And doing that, doing that solving, requires a little bit of creativity. So where am I coming
from there? The back of your eye, as you know, is a 2D flat canvas, right, made up of a hierarchy of
cells that were visualized in drawing in art by Ramonica Hall hundreds of years ago and are now
visualized using electromagnetic imaging. And we can get actually pretty fine-brained detail
of the cells in the back of our eye that constitute a 2D canvas that takes in incoming image data
and represents images in terms of patterns of activations via this kind of mosaic of cells.
Yet, we experience this richly 3D world. So there's a setup of a problem that you've probably
heard before, right? 2D canvas, but we have 3D rich experience. Scenes have depth,
objects have 3D shape. And furthermore, what we see carries lots of different meanings and
associations. So where is all of that kind of higher level information in a 2D image?
Classical kind of computer vision problems. If you look at this kind of painting by Cezanne,
you might not only recognize 3D structure of this cottage on the mountainside, right?
Even though the image itself is 2D, I might have all sorts of associations with it. I might be able
to say, oh, it's springtime. Think something about the time of year. I might even be able to infer
something about the geography by the palette used to convey what fields might be there.
Think a little bit about the landscape. I might be able to appreciate depth in pictorial space.
So even on this 2D plane, if I put my mouse up here in the front, maybe these fields are closer
to me as a viewer than these ones that are far away. But once again, I'm just looking at a
flat picture. Where is all of that information? Our brain has to solve an inverse problem like this.
Anytime we look at a visual scene, it has to get from low two-dimensional information to kind of
rich 3D. But there's a fundamental problem here that I pointed to. That is that infinitely many
3D objects can cause the same 2D projection. That's the under-constrained nature of this
inverse problem that vision poses. And you've experienced this quite explicitly. Anytime you've
seen a shadow and not the object causing the shadow, right? And you've had to infer, oh,
is that actually a monster on the wall or is that somebody's hand being projected?
But there are infinitely many configurations in three dimensions that could be projected downwards
onto two dimensions and cause some configuration in pictorial space. So how do we constrain that
problem when we're solving for what we see? So this problem is ill-posed because it has,
as I said, many infinitely many possible solutions. And choosing between them requires
some additional information. And in the case of the brain, modern neuroscience understands
this as requiring the brain to construct something. So that's what I mean when I say
perception is fundamentally constructive or creative. It requires the brain to construct
a best explanation of what it's seeing of incoming information. And if we call that perception,
then maybe you'll permit me to make a bit of a stretch and say that that makes perception itself
an act of creation or an act of synthesis of a scene. So one popular way to solve this inference
problem is by using models of the world, right? And we can approach that from a Bayesian lens.
Maybe you've seen the work of Josh Tinnenbaum in the BCS department. Or maybe we can do that
purely with deep learning. It's kind of a tension that we could explore later today.
But I'll give you an example here. And this is, let me back up for a second.
That if we were in person, this is the point where I would do kind of a live in person demo.
So I want you to imagine that we're all sitting kind of in a dark room where we're sitting in a
studio space. And out in front of you, there is a table covered in black velvet. And I've
set some stuff on that table. You don't know what it is. I set it there when the lights were off.
And then I take a single line of red laser light. I'm going to gradually sweep it over the scene.
So I'm constraining the visual information you're going to receive about what's out there in the
world to something kind of really low dimensional compared to what you normally get to understand
kind of a garden of forms that would be sitting on the table. So imagine you're there in the studio
with me and you see the following. You have to kind of infer what you see on the table.
Maybe you could write it in the chat or just think to yourself when you see this, give it a moment.
What's sitting here on the table?
Or what kinds of things? What different things?
Maybe this would be a good use of the chat. I can pull it up. Or you can describe features of what
you see. A bunch of blocks on the table. Great. There's something cubic. Oh, now I see the corner
there, right? Multiple vases, multiple forms with kind of different underlying shapes.
Something cylindrical. Yep. Two things. Do you see, I think there's a sphere actually there in the
middle. What is the experience of this light? Do you actually feel a physical corner when you see
bent light round the corner of that cube? Oh, goodness.
Interacting with chat is a bit tough. All right. Anyway, the point I want to make here is that
I can present really kind of low level information and you can, if I dare, open the chat up again.
Yeah, there's a single base. There's a single table and many forms sitting on top. That's right.
So there's just a table top and then lots of different shapes also covered in black velvet so
the light doesn't scatter and the light traces the outline of these 3D shapes and you can appreciate
something about what the shapes are just by watching how light bends around their surface
and the relative motion as it traverses that facade, right? As it moves over the surface of the sphere,
the light bends according to its curvature. And I would argue here that because you have
some notion of what a sphere is and some notion of what a cube is, that is you have
a relatively abstract model of these underlying shapes in your mind, a mental model. You can do
some inference when you see light move over their surface on this way, even though I choose an example
like this because you've probably never seen this example before, right? I've never seen a single line
of laser light move over this table surface, even this kind of setup, but you can still do that
inference pretty well and you did in the chat. So if you'll stay with me here, I'm suggesting that
this is an example just for perceptually of how we can bring models of the world and shapes and
forms that comprise it to bear on simple visual stimuli and how we can even do that by using
articulated light to isolate aspects of those stimuli and to kind of elucidate our perception
to us. So we do a lot of things like this in the MIT Museum Studio where I teach the vision
and art neuroscience class, which I'll talk about in a moment, but that's where this was filmed.
Let's see if it'll let me advance even though I open the chat. All right, so another kind of setting
in which we often hear and think about models of the world and this kind of inference is an
intuitive physics. And I bring this up because some of my background is also in this type of work
investigating how the brain represents physical properties like mass that it uses to reason
physically about the world. You would have to estimate the mass of this block that's falling
and making a depression on this pillow before you would know the right amount of grip force
you would need to use to reach in and pick it up without dropping it. And this is something we do
incredibly automatically and it's a skill set we develop regularly from a very early age.
And I found that the brain represents properties like mass with an amount of abstraction and
invariance to the type of physical scene in which mass is revealed that would be necessary if mass
like this were to be used as an input to an abstract generalized engine for physical simulation
or what we call a physics engine in computer graphics and simulation, suggesting that there is
kind of some first evidence that the brain does use these kind of generalized simulation engines
to solve low-level inference problems like inferring mass because we can make some hypothesis about
the nature of the underlying representations it would need if it were to solve problems in this
kind of way rather than by simple pattern matching or in a pixel-based way where we would assume
that the representation of mass would be quite different from scene to scene because the low
level visual data about the scene is different but in fact that's not what we find we find
representations of physical variables like mass and friction that generalize across any kind of
physical scene that we test where we hold a lot of other different parameters constant right like
object color. And this suggests an account of physical reasoning in the brain that has been
studied pretty extensively computational right and that we model via probabilistic simulations of
a physics engine. I don't think that video is going to play for us right but this is the kind
of work when I was doing when that I was doing when I was writing down like explicit models of
the world that could be inverted to explain something about underlying parameters we were
using for vision and in that in this case those models were physical right but what about cases
like like art where it's difficult as I mentioned to develop some kind of computational formalism
where we don't know the underlying model for instance how to create
the Cezanne painting we saw in the beginning a priori right how do we even start what are the
underlying dimensions we'd need to write down to either make sense of how we see things or how
they're created. So this whole area is kind of what we dive into in that vision in our neuroscience
course so this is something you're interested in it's of course an unsolved problem but we spend
the fall semester every year kind of delving into it through both neuroscience literature
through art practice through computation and then through studio work so kind of hands-on
experimentation with principles underlying vision that we then externalize and experience ourselves
and try and visualize in artistic contexts to give you a little bit of a taste of that class
we would look at these examples say by by an artist in minor white and ask if we were trying to set up
a typical kind of describe a model and then invert it to understand vision setting you know what is
the veretical percept in either of these right if before we were considering mass of some object
that the brain has to infer and then we can write down a physical law describing how mass plays into
action unfolding in a scene a law describing dynamics and then invert it to think about how
the brain represents mass what would the analog be here what would we write down as the veretical
percept you can share some thoughts in the chat that is also an exercise you could just do yourself
right maybe here you can start to get it a shadow of something outside the window I see a bike maybe
a bike seat there that's kind of not the point kind of not trying to infer what caused the specific
physics of this this image you're kind of getting it something different and especially here what if
the artist isn't around for us to ask anymore these are actual photographs right these are
photographs of something but the act of looking at it isn't about inferring the underlying cause
of the image it's about inferring something else sort of aesthetic parameters that define
visual experience or kind of render visual experience at a lot higher of a level how do we
begin to get traction on problems like this either in seeing or in or in generation as I said you
know in art we also come up against a great difficulty in that you know there are infinitely
many ways to render recognizable just depictions of common objects right with all sorts of idiosyncrasies
illusory boundaries difficult for models to detect but we recognize a woman in these images with
the dress almost instantaneously and similarly we come up against another under constrained inverse
problem is in that there's infinitely many ways to render and depict kind of abstractions of
commonly recognizable forms which again are difficult for current day models but they're
pretty easy for us I can recognize a figure and maybe have different associations with it
in each of these different images so we think a little bit about this in the course like I mentioned
you can ask me a bit after this talk as well if you're if you're interested in it
it's called vision and art and neuroscience all of our info is is online most of the syllabus
past exhibition catalogs at vision.mit.edu it's offered through through bcs and as I said we
we investigate during half the class in the seminar portion of the class kind of the underlying
principles of vision and we work through a series of modules that build up visual processes from
early level like v1 visual processing all the way up to kind of more rich images and we do this in
parallel in a studio section during the other portion of the class where we're translating
these principles of vision into the studio and building artistic contexts where we can kind of
become aware of our own perceptual processing at work so examples like the one I showed you
at the beginning right with the with the laser line moving over that garden of objects are examples
of settings that can allow us to maybe perceive our own perception at work right or shed some light on
what's going on when we look at at normal scenes right there's all these unconscious
inference processes happening even when we look at corners in a room but we're not aware of them
and so we ask here if we can create settings where we do become intensely aware of them
and that awareness becomes kind of the art experience right so it's the art of perceiving
one's own perceptual processes at work and then over the course of the class everybody develops
an individual artwork for exhibition which is super lovely and it's it's an opportunity that we don't
often have in other classes at MIT so we run this for five years now had five different exhibitions
in COVID we had a virtual exhibition and then this year's just opened in December and is actually
still up in the MIT museum studio just off of lobby 10 10 150 if anybody is on campus and
wants to go check it out it's most it's open most days when when staff are there but this course is
the parallel to your IAP class that thinks about things more in the language of computational
neuroscience than deep learning and in some aspects of the course will present deep learning
or deep generative models as contexts for probing representations that might be shared by human
minds and machines and we'll look at that a little bit later in this lecture but think more
traditional computational neuroscience lectures readings visual art and then a studio component
where you experiment with some of the stuff hands-on so that's what we do in vision art
neuroscience uh we start to to probe at the richness of this art neuro and machine learning
intersection there's a lot of different things we can do there and for the rest of this talk
we're going to highlight a number of different projects that approach that intersection in
different ways and highlight kind of different ways that you could think about engaging this
material in these questions data sets and resources that we have available in kind of
different ways of carving up the problem into bits so we'll start by thinking about
modeling kind of the structure underlying human creativity at scale without trying to
pre-specify laws that you would write down for say a physics engine right can we use deep
generative models to kind of approximate or appreciate or grok the structure underlying
large data sets of human cultural artifacts and then use those models to experiment with
cultural history on kind of a timeline that allows rapid evolution in the present
so I'm speaking specifically about a project that I don't know if some of you have seen and I
know Ali has seen a collaboration that I led with the Met a couple of years ago again it was fun
that we were in person because we were able to to actually go to the Met and see a lot of these
objects but back in 2017 the Metropolitan Museum of Art was the first or one of the very first to
release an open access catalog of a few hundred thousand digital images of works in the Met collection
and released them into the public domain which is wonderful for for us as computer scientists
and programmers and people interested in ML and art because what a rich data set that is right
what a rich data set all in one place don't get me started on the issues with museum APIs but a
lot of museums have followed suit in releasing their digital collections into the public domain
so they're free and open for experimentation they approached us at MIT and open learning
and a couple of programmers at Microsoft and asked if we might want to do a series of projects
with this digital collection and so we did and we asked whether we can build deep generative
models associated with archives like this of created work that are embedded in their cultural
context which might ask which might allow us to ask like slightly more specific questions
art historically than just you know what if you train StyleGAN on all of wiki art all at once right
not conditionally so we're not appreciating any categorical differences between images but if
we just showed it all of wiki art okay here we want to ask something a little more fine grained
can we notice any differences in the development of feature languages between maybe time periods
or geographical regions right and can we develop ways of collaborating with those models to iterate
archives forward so experimenting with chimeras between existing works and developing new works
right that might sit somewhere between works that are already on a graph so one of the challenges
that we faced here initially was that the data set was pretty big 400 000 images but each individual
category in that data set was not some might only have a couple hundred images and there's a lot of
sketches and drawings and kind of uncategorized work too that makes up that 400 000 so you're
in a situation where in theory you have a rich labeled data set but in practice it might be
quite difficult to train anything that looks photorealistic or gives a good sense of any
individual category of work because the categories themselves are not that large so at that point
this was pre like StyleGAN 2 we started working on this in 2017 2018 um I asked whether we could
instead of training a single model on say a subset of this met collection like this category of
vases called yours whether we could find corresponding subspaces of what we're now
referring to as foundation models like BigGAN ImageNet that kind of approximate our data set
right so if we think about foundation models as a shared resource that ideally everybody would
have access to and there were ways to think about contributing to then maybe these smaller problems
become or can become a way of defining subspaces of those big models that we can interact with
right rather than having to retrain a model on our data set so we used GAN inversion here and
instead of training a new model on just this category of viewers we asked whether we could
embed each image that already existed into in the Met collection into the feature space
of BigGAN ImageNet which happens to have a category for vases so we selected categories that were
shared between ImageNet and the Met collection there are a handful about a dozen um and we
maximized for each of those images the similarity between the Met image and the BigGAN image using
a two-part loss right so we wanted them to be similar both at the pixel level and at the semantic
level and we did that by looking at two different layers of a pre-trained ResNet as the embedding
network so once we've embedded these models these images into BigGAN we can then visualize the
individual embeddings but we can also do something a little bit more interesting than just look at
approximations of these images which might not be very good we can think about the underlying
feature language that might have been learned and then look at interpolations between the existing
images in the Met collection I hear murmurs in the background if anybody has a question hit the
chat you're super welcome to speak up so next we look at interpolations between these existing
images on the graph and we can create kind of hypothetical or dreamlike images that exist
between the spaces of existing works in the collection these are pretty interesting and
beautiful and they allow us as I was mentioning to ask questions about what collaborations between
geographical regions might have looked like right because we do have categorical information about
where each image in the Met collection came from it allows us to suggest new objects and the spaces
between them so it allows us to interpolate and the other beauty of these kinds of executable
models of culture is that it allows us to iterate on existing collections really rapidly
and evolve them forward and so we can kind of start to imagine archives of the future that
would have embedded within them world models corresponding to the dataset that exists at
one point in the archive right so the archives could kind of evolve themselves forward and
suggest future versions of their collections based on what's already been created and that this is
again this was back in 2019 which is a long time ago in computer vision terms but even just with
inversion into big data in the chat I was really impressed at the quality of the
the images and the hypothetical objects that we could get for example here are a bunch of
different generated teapots from the Met latent space in the teapot category which again happened
to be shared between ImageNet at that point and the Met collection and as I said we did have the
opportunity to exhibit this in the Met which was absolutely wonderful we projected a visualization
of this latent space superimposed on a map of the Met collection and allowed people visitors to
the to the Great Hall to kind of step in to this latent space as projected onto the ground
and explore the traversal of the spaces between works and a projection behind them on the wall
we also made a web app version of all of this that exists even though we we can't visit the Met
today it's online at gen.studio if you want to go have a look after this and then all of the
the code base is linked the GitHub is linked at the bottom if you want to check out any of that
more specifically but again it places us kind of a different framing of latent space traversal
then we're used to that I was interested in this project was to place us on you know we've
gotten a lot further along in the video you can go look at the at the website places us on a map
between the objects when we're doing the interpolations right so we select an object to start
now we land in the latent space of BigGAN close to that object and then we can move ourselves
around on the map between objects and their embeddings in that latent space right and as
we're moving physically in 2D space here online we can visualize what exists at that point in
latent space and then we can find its nearest neighbor visually in the Met collection and find
what object in the existing collection is most similar to the hypothetical work that we discovered
in the interstices between two existing works so give that a look and this project
lives on today in a couple of different forms I'm still working with the artist Matthew Richie
who is a collaborator with us on the Met project on a couple of different tendrils of this work
where we're asking all right so we can model projections of existing images in the Met
in the Met collection by finding their embeddings in some kind of large foundation model
but now in 2021 we have things like StyleGAN ADA that can train on smaller data sets and do
reasonably well in approximating data sets that would correspond to a single category in the
Met collection so we've done that we've trained these models on sketches Babylonian cuneiform
tablets Japanese watercolors that's an 18th century European landscapes among other things
and have individual models correspond to each of these genres within the Met collection
and then we've been working with a friend in New York who has a robotic oil painter and can actually
create layered paintings of really short walks in latent space along different dimensions in
this model so think about physically visualizing some of the steerability work you've looked at
in this course right could we make time paintings of really short walks in latent space by super
imposing robotic paintings of the visualized image kind of at different points along that walk
so that's that's being exhibited right now at UNT in their contemporary art gallery I see I've
got a question in the chat room oh it's just a compliment I will take it at any point yeah I
think can you please read it yes someone mentioned that this is a creative reason one of the most
creative reasons they've seen to do latent space interpolation since its GANs yeah I think that
I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of
part of the advent of using GANs to model kind of large databases of creative work is that they
allow us to do a couple of things right that interpolation and that iteration and in cases
where you can't write down a feature language underlying a set of works because you don't
know a priori what it is you can imprint that or you can learn something of that in a deep
generative model right and then you can collaborate with that and hypothesize what might lie on a
graph of human creation if we presume that any creation artistic creation at some point in
historic time is if you think about it as the manifestation of a point on kind of a sea of
cultural influences and multi-generational practice iterative practice that's been shared
between peoples and generations and the creation of a single work is the enactment of that process
at some point in space it's natural to think of that in some sense as a model that we can capture
in a latent space where we're manifesting some part of structured space at some moment but this
allows us to iterate on that which I argue is similar to some historic processes of iteration
and collaboration across groups of people really quickly right so I've been trying to
take this a little further now and ask all right we can make paintings of short walks in
latent space we can hypothesize objects that might have existed but we don't think they ever did
but we still don't know much about these models even if we train StyleGAN 2 on a set of 2000
paintings in the Met collection you've probably seen some of the interpretability work adjacent to
what Ali has shared or David Bow's work so that style of thinking we don't know anything about
this Japanese watercolor model like what do its individual neurons represent is there a neuron
for trees well what is a tree here it's some brushstrokes we recognize as a tree but it's not
something that BigGAN trained on image that would necessarily recognize as a tree maybe more simply
kind of in the in the steerability context what do dimensions in the latent space of a model like
this correspond to right sure we can find things like zoom and 3d rotation because we can name
those transformations and then find directions that maximally correspond to them using that kind
of steerability technique there are all sorts of other directions like the ones we're visualizing
here that certainly have some affective meaning to the viewer that we don't know what they are in
the model's terms or in the viewer's terms so at this point in this project we're thinking about
starting to name and understand dimensions underlying generative models trained on
bodies of artistic work from museum digital collections not only limited to the Met but
around the world and our motivation here is to kind of create these alternate and imaginary
histories of art built from unique latent walks that we can visualize in real time with this painting
or computationally and then maybe understand something about aspects of picture language
that might be shared across you know vastly different genres so Babylonian cuneiform tablets
transformed from numeric to symbolic and image-based at a very particular point in history and can we
find a dimension in StyleGAN trained on a very different genre of art that corresponds to a
similar kind of transformation and as such can we build up kind of a picture language that would
correspond to diverse forms of art making right that you might not see in any of these different
categories of digital images on an archive but we might start to appreciate once we can investigate
them by training deep generative models on them let's get back to great okay so when we're thinking
about this intersection we've seen one example of modeling the structure underlying creativity at
scale and I've done other projects and you can find many examples online both of my work and
other people's of trying to do this not for creativity at scale but for individual instances
of individual artists and modeling either the style or the processes of individual art making
techniques so all of these are kind of flavors of starting to imprint or grok or understand the
structure underlying creativity but not symbolically right so we don't we don't know how to interpret
these models even though we can visualize them and create really interesting hypothetical objects
that might be indistinguishable either from existing work or from one artist's particular style
we can also think about these models as a tool themselves for collaboration both in their creation
and iteration with others who contribute to their models and with the models themselves
which as I described represent kind of executable versions of collective cultural structure
we permit permit ourselves to think about them that way or facets of kind of a global creative
identity as I mentioned now we're at a point with tools in computer vision where we can start to ask
what rep representations actually underlie these models trained on artworks that are themselves
executable versions of some collective cultural structure right well what is the structure what's
going on under the hood do they correspond to dimensions that we find meaningful when we
look at visual scenes and so in the next part of the talk I'll share a couple maybe more technical
projects that explore specific ways that humans can interact with generative models
in order to maybe learn something about human vision as well right so can we build
shared vocabularies that help us interpret dimensions underlying these models by designing
experiments that allow us to visualize and interact with images and latent walks like you've been
seeing I'll pause here because I need a sip of water and I'll keep an eye on the chat in case
anyone has any questions before we go on
all right looks like we're question free so far five more seconds
I guess I have a question yeah so this might be talked about later but I was wondering a little
bit about like in your research and kind of this field how much of like human interaction is like a
big part of it and kind of like the human coming in and saying how they think about something and
see where that agrees with the computer or like kind of like where that role is played
wonderful question so these kind these kinds of high-level questions that
get it some experiential component or design component of the worker I think really useful
ask more them I'll tell you for for different projects what that looks like and in the next
section of work it's going to be really obvious because there's human annotations but for this
project so the human would come in here you know we train models on datasets of art selected from
the Met collection and these are small and these are subsets and they were gathered by
Matthew Richie and myself going through different genres in the digital collection of the Met
online and like hand selecting images from those different genres right representative images of
different categories of work or maybe in a less fine grained way all images under some designation
so Japanese watercolors between the 17th and 19th centuries so we made that selection and tried
training these models on a bunch of different such selections and decided which ended up you know
with so few examples providing at least a representative sample of the kind of work that
we know we saw there right and then here the selection of like walks through latent space
so think of those in the same way you've been thinking about the steerability walks
they were very arbitrary so that was a completely human selected so it's a kind of a different
approach to interpretability where it's steered by the human eye right we're not doing it automatically
and we're not doing symbolic it symbolically we don't know what these correspond to but that's
trying some arbitrary walk through latent space trying many of them and then the human then
selecting what to them felt like an artistic expression this is an art exhibit and then in
the next step we'll ask how can we do that in a more systematic way and start to build a language
corresponding to what those different walks could be a language that's shared by humans so that takes
at least right now a lot of human a lot of human interaction you could think about ways to automate
that we'll talk about that in a second but with any kind of human interaction it's nice to preserve
the opportunity for direct engagement with models rather than intermediation by
a captioner or something like that because then you could imagine using your technique on different
subsets of humans right on different kinds of experiences so you might imagine getting an art
historian to label and select different walks through latent space here corresponding to very
nuanced changes in the development of Babylonian like cuneiform tablets right that a captioner
couldn't recognize I couldn't recognize so you might want to be able to pull different kinds of
humans into the loop at different times to engage in ways that kind of use their knowledge to create
a unique synthesis with a generative model so that's what engagement looked like here and then
with this next project it'll be super it'll be super clear and I'll make sure to speak specifically
to that so thank you yeah cool so next this is probably a summary of what you've seen so far
in your IAP course so there's a lot of different work on discovery of interpretable directions
in the latent space of different generative models right and we can steer images along those
dimensions to create interpretable transformations that allow us to interact creatively with deep
generative models right here we are deep learning for creativity but a lot of these examples presume
what concepts we're searching for in the latent space and in fact they do that really explicitly
right we will pre-define a zoom transformation and then maximize the similarity between some
transformation in the latent space and a zoom transformation as applied to some image maybe
you've experimented with code for doing that but that presumes we know we're looking for
zoom in the first place what if we find ourselves looking out into more you know uncharted waters
so to speak here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you
would apply to those style gains we just saw train on the met images right we don't maybe we could
look for zoom but maybe there's all sorts of more interesting transformations we could do to those
images but we don't know what they are yet how can we learn a vocabulary of visual concepts rather
than pre-define them or labeling them after the fact so there are a variety now of unsupervised
methods for distilling these kinds of transformations in latent space that find principal components
of feature space of different layers the models activation maybe you've played around with methods
like GAN space that search for and rank or the largest principal components of the feature space
which do provide us with interpretable transformations but they're labeled after the
fact so we don't know if they're meaningful to humans kind of in their genesis but we can we
can describe them right by providing labels to them another point where the human kind of comes
in the loop but we want to see if we can build in human vision to the discovery process right so
to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this
project we're trying to build or define a method for building a visual concept vocabulary for an
arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe
you've called this w we want to learn some kind of walk in the latent space z if again we'll focus
on big GAN here of transformations that are salient to us in visual space and we can't define
an objective and optimize our d our walk to produce a transform in x in the image because we
want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit
to what that objective is right in order to optimize d so we're going to take a different
approach and instead sample the space of salient or possible transformations for some given point
in space for some given z and then use those sample directions as a screen so to speak onto
which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor
but maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that
are projected onto that screen into a vocabulary of open-ended compositional visual concepts
and what we're interested in here is the overlap between what's represented inside a model
so some deep features in a model's representation and concepts meaningful to humans in visual
seeing understanding we're asking how we might start to define although not completely but
start to define a shared vocabulary between the two or for a given model determine what lies in
that set overlap and I don't have to dwell too long on a lot of the specifics here it's all
online at that URL if you want to read the paper but as I mentioned the first thing we're going to
do is generate a set of sample images that produce minimal meaningful transformations in images
and then humans come in the loop again we're going to ask them to label them but here we're
forming kind of the basis for the data set that will build our vocabulary off of and we want to
keep in mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of
different changes that you can produce in an image and specific where a single transformation
corresponds quite reliably to one visual change across viewers so we do that by defining
mutually orthogonal what we call layer selective directions and these minimize change in the feature
representation at some layer of big care and at some layer we'll call it layer l and this allows
us to capture relatively focused changes because we hold constant how much the representation
can change at some layer and we do that for different layers to capture changes at different
levels of abstraction so as you can see layers closer to the image output control or fine grained
aspects of the image like the color of the walls and the bedspread and as we get closer back to
the latent space we're allowed to make kind of more higher level changes and things like zoom and
perspective of the scene and its composition so what objects are present so here we have a base
set of minimal meaningful transformations that capture changes in images at different levels
of abstraction we're going to ask people to label them because we don't know what's going on visually
in these scenes right so we started at a pretty small scale with just four categories in the
places data set and looked at big and trained on image net and places we'll just talk about places
here and visualized a handful a few thousand of these directions per category so in each of four
categories looked at cottages medinas so street marketplaces kitchens and lakes a mix of indoor
and outdoor scenes and then asked people to just simply describe the overall transition that they
saw when these directions were applied to different randomly sampled starting points in the latent
space right so one direction might take this cottage to this snowy cottage and change something
about the sky and change the snow so these these changes are still complex we can recognize that
it's the same scene and we can describe in simple language what's going on but they're they're not
disentangled yet right one direction might correspond to a number of different visual changes
so we did a little preprocessing to capture you know what kinds of concepts are associated with
each transformation and then we decomposed those annotated directions into a visual concept
vocabulary consisting of single directions labeled with single words and we formulated that as a
linear regression and then solved for the embeddings of individual concepts in the latent
space of our begin and then we can basically read those off of our matrix e and then transform the
images by manipulating them some amount along those visual concept directions happy to talk more
details about that if anybody's specifically interested or you can check out the paper itself
we found over 2000 concepts this way corresponding to lots of different types of visual changes
so we can reproduce transformations like zoom and rotation things like color but we also get
kind of a unique set of concepts corresponding to aspects of scenes like their mood for instance
there's a direction in latent space of big and that makes outdoor marketplaces more festive
and here we see applying that direction to an example marketplace and it rolls out a red carpet
hangs some flags and brings a lot of people into that market we can visualize kind of a
a sampling of these directions each applied to two different images in different categories
so some directions make cottages more manicured add arches to marketplaces add shadows or make the
whole scene blue and we see directions like this that generalize across all of the categories
it began that we looked at you can check out the lake category we can add sunsets but also do
kind of scene specific things like add reflections to water or make a lake scene foggier make a
kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity
would entail when applied to a kitchen we learned that through sampling what humans associate
with a transformation that was sampled randomly right and the humans labeled that as modern then
we disentangled the specific direction in latent space corresponding to that single concept word
and once it's isolated we can apply a modern transformation and know that it corresponds to
what viewers found to represent modernity in a kitchen well I said we know that it corresponds to
what viewers see as more modern but we don't know that for sure right we still need to ask
questions like how generalizable are these directions do they compose right can we add
a festive direction to eerie and get something that's both scary and festive right or could we make
a kitchen both more modern and inviting so we asked those questions in a series of behavioral
experiments that I left for you to check out in the paper itself so we won't in the interest of
time go through those here but we do find that these directions are composable and they're generalizable
across categories so there are some cases where we can even add a concept that was learned in a
single category to a different category for instance making a cottage more festive right or
adding snow to a marketplace even though that's not traditionally seen there we ran a set of
behavioral experiments evaluating the extent to which this is successful and isolating a couple
of few specific cases where it fails okay so this this wraps up this method it's at a point
now where we're trying this with some of the the art models that I discussed previously right so
this was still just applied to big and trained on real-world images trained on ImageNet but you
can imagine using a similar similar method to find dimensions of visual interest that are also
meaningful to humans in the latent space of a model trained on art images and so decompose
feature languages underlying different genres of art into something describable so that we can
make concerted manipulations to images sampled either from foundation models that correspond to
approximations of real-world images or to models trained on on archives of art images themselves
uh I'll pause here for any questions about this there's associated code also available on that
project page I might have a quick question please um was there a reason you chose uh
big GAN over starting with like style GAN for this type of work uh no um just a couple of
different code bases that already existed um and people that had worked on big GAN for
like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of
things might be there uh so a lot of the GAN dissection work started with big GAN and so I
was picking up where that left off and asking if we could find like style vectors that corresponded
to scene level transformations instead of individual neurons um but I have extended this
to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean
there are a couple of different small changes you have to make um but the method is pretty model
agnostic just like defining a set of certain directions that samples the latent space in kind
of minimal ways and the the method I described here is definitely not the only one you can use for
that right um you could sample them by just finding the principal components of the feature space or
you could sample them randomly right you could just find two points in latent space interpolate
and get people to label what's going on there um we tried a lot of these different methods
and found that if you if you make random interpolations between two randomly
sampled points then there's just so much going on in the scene that there's not a lot of inter
observer agreement in how people annotate what they see there's just too much going on so we need
to isolate specific changes that's why we developed that layer selective method for isolating minimal
changes um but what if we used kind of the the principal component method right or used something
like GAN space there we found that the principal components of the model's feature space aren't
necessarily the most interesting to humans so we might get a ton of different types of
rotating the scene but not a lot of different changes of mood or changes in color up there
in high-ranked principal components um so that's where that method came from but it's
agnostic to the set of directions and pretty model agnostic uh the annotation is another place where
humans intervene here to tie in that last question um but you can imagine trading a
captioner on a labeled data set like this right a little larger than the one we collected so
we're thinking about doing something like that uh but preserving the human annotation does allow
annotation you know in the art context by experts as I mentioned so you might want to be able to
do this at scale for a brand new model and just have automatic annotations you use something
like clip right for the kinds of transformations you would see inside um but preserve the opportunity
for experts to to annotate kind of specialized smaller trained models
and there are results too from big GAN training on a couple of different data sets if you're interested
um I have a question regarding like the choice of uh and in terms of annotations
so how did you arrive at this number and how are you I mean how do you know like what number is
kind of sufficient yeah good question um so I assume you mean the total number of images we
needed to annotate and not the total number of annotations per image
which end do you mean I can talk oh I see I mean either yeah well okay so at both levels
for the directions themselves we needed to collect at least two annotations to be able to measure
intersubject agreement right we want to see if some direction is consistently producing meaningful
similar annotations across annotators we need at least two people to annotate them
so for all the directions we evaluated we had two annotators label them and measured the
interanitator agreement using a couple of different metrics blue and burnt scores
but for a subset of those we had 10 annotators annotate them and just had a look at
interanitator agreement across a slightly larger group for expense reasons we didn't do that for
for all the directions because it really wasn't necessary things didn't change that much and even
in that subset when we went from two to ten per per direction and then for the number of
directions that we chose to visualize it was not a very principled decision I'm afraid we chose
I think 64 z per category and then a bunch of different minimal meaningful directions for them
corresponding to I think the same number of principal components that we looked at in the
GAN space paper so maybe the top 20 in each category so it was a bit ad hoc that decision
um the the things that's going to change we can distill vocabularies using this method
for any size of annotation library right uh which is one of one of the beauties and one
of the things that gives itself to to some of these more ad hoc decisions um we're doing it
analytically right if we go back to this we're actually like reading off we're solving for the
embedding matrix of word embeddings in latent space of concept embeddings so we could do this
with like just a couple of directions if you only had one annotation per concept it only appeared
once then you're just going to get per that direction so as you increase the vocabulary
as you increase the sample size you're you're probably going to get a richer vocabulary
but it's still possible to do on a vocabulary of this size so we're deciding now whether it
makes sense to scale this up and collect like a number of annotations where it would be possible
like i said to to train a captioner on them to be able to automatically label these directions
rather than have humans do it so part of it is constrained by the tractability of experiments
on mechanical turf right how many reliable annotations you can get in some period of time
awesome uh thank you yeah
these are really useful questions these are great
um kind of along those lines more of a random question for the printer like the single words
for the labels yep was it kind of agreed upon earlier like kind of what words you'd use because
like for festive maybe someone would say lively or for inviting you'd say welcoming is there like
kind of a similarity score for those words or really good question um no so this is it's only
preprocessed with like a little bit of limitizing so we collapse different endings people might be
using our different verb conjugations onto single verbs uh but festive would have a different
direction from lively uh kind of a next step in post-processing that we've talked about but
haven't yet done um is to just collapse across like word net sense sets right so you could use
something like that to find synonyms of festive and then approximate one direction for lively and
then be able to break it down into something maybe more fine-grained um but there were no
kind of heuristics or standards for the annotators except you know they did they did a practice run
and looked at a couple of different examples and were asked to describe an overall transformation
that captured changes at lots of different kind of levels of abstraction we can look at the specific
what do we tell them it's on here yeah how would you describe the overall transition changes in
mood changes in objects or features of the scene don't mention your describing images so standard
kind of turk boilerplate just address the content of what you see and then they could look at some
samples and then after they did a practice run they did the annotations um so any interanitator
agreement is just based on their word choice which in some sense is a raw window into perception
but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see
that reflected when we used I don't have this on these slides um but when we use blue scores to
to measure interanitator agreement so when we use these layer selective directions to generate these
kinds of transformations if we get 10 people to annotate each transformation people might use
somebody might say eerie somebody might say spooky right somebody might say scary to describe the
sky uh that comes up as like quite different when you look at some methods of evaluating
interanitator agreement so we used first scores as well that evaluate the semantic similarity
instead of just literal correspondence words and found that annotations of these kinds of
directions performed a lot higher when we looked at semantic similarity of annotations as opposed to
just um just word based so there's definitely reason to start trying to collapse like that when
we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because
you can see all of the different words that people used to describe changes um but you'd get a lot
more power right if you could combine annotations for festive and lively and vibrant under one
umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions
inter just have a maybe one or two more things not much so ask away if you do I think more more
of a higher level question I remember Ali in the first lecture um right you drew you had this
visualization of like two points in the latent space and you know a last function that would steer
like from one or trajectory one to the other but it was like something more like a curve
or something non-linear um right and you mentioned with Gannon version if you just interpolate like
draw a straight line between two points you have like all sorts of things happening I was wondering
if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a
walk that would I guess lead to less perturbations I guess in in terms of like features I mean
does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak
a little bit more about about his work there maybe after we stop this recording but um linearization
of this is a huge over oversimplification um and that would be one of exactly what you describe
as one of the things I'm most keen to try is taking non-linear walks um through any of these
subspaces um so very on point question haven't done it you should try and do it um but describing
like this the semantic structure of latent space the semantic topology if you'll permit me that
is a really interesting question um because even the visual meaning corresponding to
some of these adjectives some of these words is not regularized or normalized in the latent space
itself so if I take five steps in the festive direction it might take me five steps to get
anything that will start to register to me as festive um but the walk size for a correspondingly
large visual change so to speak in a different direction could be very different um so some
transformations like making an image black and white this is anecdotal but you only have to go
like one step in that direction and then we'll visualize the change almost immediately uh so
we're not kind of we're not walking around in like a perceptually normalized space so to speak um
and there hasn't been to my knowledge a lot of work that's addressed that everything's been a
little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths
and how we can think about kind of the concept mesh underlying latent space for different
generative models is extremely interesting to give you a really cool area to do some working
cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more thing
to show you a quick example to hopefully spark more discussion unless anybody has anything specific
about this project we can always come back all right oh geez well last thing i'm going to show
you uh is still a beta and uh it's very it's very early and it's even thought development
but it captures something um that I think is deeply interesting uh and I think might be
interesting to you all um so the former method that I showed you for building shared vocabulary
between humans and models relies heavily on language right and so we get some direction
and we're able to share that between people and even to repeatedly use it to steer through model
space um because we've given it a label right we've used language and you might even argue that
you know that's constraining the space of what people can recognize in those initial
sample directions because there might be some genus or quad aspects of images that we don't
really have words for um but are still like really recognizable or perhaps the verbal
you know that the words you would use to describe something are like quite complex and you wouldn't
type that into an annotation like on mechanical Turk maybe you'd want to describe the sky as
like the sky you saw at your grandmother's house the day she passed away or some flowers as effervescent
like latte foam or a sparkling drink but you're not going to type that into mechanical Turk and
there's not a single word concept to capture it so that's going to get lost in the method I described
and lost in a lot of kind of standard either annotation based or uh kind of hard coded direction
search so I wanted to experiment with a way to capture and learn um directions without language
and this is like deeply inspired by the steerability work um of all these so you'll see
a method here that is is similar to that in some sense um but we're allowing the human to
define the transformation that they want rather than pre-defining say a zoom or rotation transform
using an algorithm we're allowing humans to come into the loop and define that transformation
purely visually by interacting with very very small batches of images sampled from latent space
or feature space at some layer and sort them into classes corresponding to some visual feature its
presence or its absence um and this provides a pipeline where users can steer just like in
steerability work along dimensions that they discover however that they define and they define
them purely visually so labeling what happened just as a matter of convenience but they're
discovered um through vision so the way to do this is really simple um take some latent space
again a lot of these examples are are using big gan you could also use style gan um take some latent
space and sample images from it right if you're using a conditional model so we pick some category
here we're looking at lakes inside big gan image or big gan places um sample some images for a user
and then that user who's determining a visual dimension of interest kind of looks over that
image space and sees if anything stands out to them um across that that set of images so maybe here
I noticed images that seemed kind of verdant and fertile uh and maybe more more spring like but not
totally seasonal you'll see where I'm going it's kind of hard to describe and these were a little
drearier or more wintry but there's not snow so it's not really winter they're just kind of less
fertile and vivid so that's the distinction I want to make there um and the method is very simple
just like the steerability work um and a another example of work from Boley we define
a transformation just by learning a hyperplane so training a SVM and learning a hyperplane that
separates those two classes of images either in the latent space or in the feature space of some
layered layers activations and then when we can steer some starting image in a direction that's
normal to that hyperplane and steer it across those classes right so I could take an image
that starts in the kind of drearier domain um or duskier domain and transform it normally to that
hyperplane and take it into the category of things that I thought was more verdant right or more
fertile but I could specify that separating hyperplane just by sorting a shockingly few
number of images um so we've done a couple of more like fine-grained tests here but
just for proof of concept you can discern these directions with some degree of reliability with
just like five to six examples of images in each category making it really simple to interact with
something like this just by dragging and sorting a few images um that are sampled in the latent space
okay so there's a tiny example of a demo app we have for this um and we're switching where it's
hosted so it's not online at this very moment but it will be next week but it's called the
latent compass it was at NeurIPS creativity I think last year the year before um you'll see the
home interface in a second but what we do is just what I said pick some category of big
yen here it's big yen places um on the bottom you see images sampled from that category and the
user drags them to the right and left of the screen corresponding to two different kind of
categories of concepts they want to capture uh and then once the compass calibrates and we'll
see that in a second and you can drag any new image and then transform it along that dimension
so here we pick the closet category I've got full closets on the right empty closets on the left
and the dimension I want to capture here is something like fullness so I'm going to see if I can I can
learn a direction corresponding to the visual difference between these two categories drag
any new closet onto that center line and transform it along that direction filling and emptying the
closets and what if we tried a different category what if I wanted to turn a medina into a full
closet right what is the type of fullness that's relevant to a medina oh well it's adding people
instead of adding clothes suggesting that what's been learned there that direction in latent space
is abstracting generalizable enough to capture some visually recognizable dimension of a fullness
that's meaningful to us in different scenes right and the model's able to to generalize it in a way
that's not totally dependent on the types of objects it saw in one scene so it knows in a sense
that clothes make a closet full but to make a market full we're not adding clothes we're adding
people and so the fullness direction is something that adds more of whatever would make that scene
full to any scene that we're selecting in the model right and trained on so few examples of
course this this is really quite imperfect but it's a good proof of concept of a way that users
can interact super flexibly and really visually with dimensions of interest and use that to kind
of explore and surf the latent space of a model by producing replicable repeatable directions that
others can explore without having to use language it's kind of a different way of carving up the
puzzle of how to explore and assign meaning to directions that we that we find in latent space
okay that's at latentcompass.com and we'll be back up next week I think bad timing okay so to return
to our frame here we've been digging a bit into this intersection between art neuroscience and
machine learning ways to explore models that have been trained on human creation right at
different scales to create a new to iterate and interpolate upon archives and then also to start
to understand what these models are representing and if our ways of interpreting dimensions inside
models can also teach us something about human perception or allow us to start to build models
of aspects of human vision that are otherwise pretty intractable because it's difficult to
formalize what dimensions underlie them where I could write down what dimensions underlie physical
seen understanding because I know Newton's laws I couldn't write down what dimensions underlie
aesthetic perception of North African marketplaces or Babylonian tablets because I don't know what a
large swath of people would find perceptually interesting in a bunch of marketplaces I know
from cognitive science research certain heuristics to look for but that wouldn't give us a full set
of what a diversity of humans might appreciate when looking at some scene especially things like
its mood so we can turn here to these kinds of large unstructured generative models that learn
entirely from data entirely from images and turn to them as like a fertile ground so to speak for
starting to probe and represent human perceptual experiences inside their latent space and think
of latent space that way right as a screen as I said before onto which we can project human
experience and then once we have those projections we can rerun them and interact with them and
collaborate with them to create outputs of deep generative models that are particularly exquisite
and that represent some kind of collaboration between us and models of our our creation that
are operating in parallel so that's where I will leave us my emails here I'm very discoverable
online but you're welcome to write me questions anytime and I will wrap here and we can have a
more casual discussion unless anybody has any last questions for this
thank you so much sir this was really interesting and inspiring with all the
acidic decreasing slides and every moment of that was really full of thoughts I think that
you open a window to semantically and qualitatively looking at these latent spaces and
sort of our imagination and
where we dream and where these models that we create dream so I really appreciate that
I'm going to stop recording and then see if there are more questions
